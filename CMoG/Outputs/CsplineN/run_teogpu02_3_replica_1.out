2023-10-23 12:33:31.381000: Importing os...
2023-10-23 12:33:31.381030: Importing sys...
2023-10-23 12:33:31.381035: Importing and initializing argparse...
Visible devices: [3]
2023-10-23 12:33:31.387695: Importing timer from timeit...
2023-10-23 12:33:31.387943: Setting env variables for tf import (only device [3] will be available)...
2023-10-23 12:33:31.387963: Importing numpy...
2023-10-23 12:33:31.533386: Importing pandas...
2023-10-23 12:33:31.692574: Importing shutil...
2023-10-23 12:33:31.692604: Importing subprocess...
2023-10-23 12:33:31.692614: Importing tensorflow...
Tensorflow version: 2.12.0
2023-10-23 12:33:33.724886: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-10-23 12:33:34.062341: Importing textwrap...
2023-10-23 12:33:34.062371: Importing timeit...
2023-10-23 12:33:34.062381: Importing traceback...
2023-10-23 12:33:34.062387: Importing typing...
2023-10-23 12:33:34.062397: Setting tf configs...
2023-10-23 12:33:34.322662: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-10-23 12:33:35.561112: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

===========
Generating train data for run 263.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_263
self.data_kwargs: {'seed': 377}
self.x_data: [[5.416377   7.151266   6.5657053  ... 3.613302   2.6304417  8.037386  ]
 [4.297167   5.832213   0.12875912 ... 1.4317983  6.415448   1.3533646 ]
 [6.2353234  7.1795187  6.219724   ... 4.5891814  2.66546    7.551563  ]
 ...
 [1.8237456  4.102903   7.5608215  ... 7.136859   2.564185   1.5539532 ]
 [2.2305896  4.6568003  8.321339   ... 7.0577617  2.6189713  1.518045  ]
 [5.653481   7.1263475  6.929123   ... 4.292566   2.643216   7.404443  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  1074400   
 r)                                                              
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7feac0136d40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fea40799570>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fea40799570>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fea407c8640>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fea40633520>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fea40633a90>, <keras.callbacks.ModelCheckpoint object at 0x7fea40633be0>, <keras.callbacks.EarlyStopping object at 0x7fea40633df0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fea40633e20>, <keras.callbacks.TerminateOnNaN object at 0x7fea40633b50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_263/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 263/720 with hyperparameters:
timestamp = 2023-10-23 12:33:45.418204
ndims = 32
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.416377    7.151266    6.5657053   5.44807     3.870332    6.1800294
  2.9008856   8.441634    9.391678    4.1002183   7.382605    5.3625755
  5.5868373   9.949682    0.82923275  1.9832962  -0.10069095  8.219267
  8.539603    9.221239    9.5321455   8.239499    4.945965    8.152974
 -0.16393006  6.303131    0.6036788   9.50227     6.1976576   3.613302
  2.6304417   8.037386  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 13: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-23 12:36:28.255 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2715.4319, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 163s - loss: nan - MinusLogProbMetric: 2715.4319 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 163s/epoch - 829ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 263.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_263
self.data_kwargs: {'seed': 377}
self.x_data: [[5.416377   7.151266   6.5657053  ... 3.613302   2.6304417  8.037386  ]
 [4.297167   5.832213   0.12875912 ... 1.4317983  6.415448   1.3533646 ]
 [6.2353234  7.1795187  6.219724   ... 4.5891814  2.66546    7.551563  ]
 ...
 [1.8237456  4.102903   7.5608215  ... 7.136859   2.564185   1.5539532 ]
 [2.2305896  4.6568003  8.321339   ... 7.0577617  2.6189713  1.518045  ]
 [5.653481   7.1263475  6.929123   ... 4.292566   2.643216   7.404443  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7fee5968f100>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fee58d59ba0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fee58d59ba0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fee59d564a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fee5877a920>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fee5877ae90>, <keras.callbacks.ModelCheckpoint object at 0x7fee5877af50>, <keras.callbacks.EarlyStopping object at 0x7fee5877b1c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fee5877b1f0>, <keras.callbacks.TerminateOnNaN object at 0x7fee5877ae30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_263/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 263/720 with hyperparameters:
timestamp = 2023-10-23 12:36:37.035068
ndims = 32
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.416377    7.151266    6.5657053   5.44807     3.870332    6.1800294
  2.9008856   8.441634    9.391678    4.1002183   7.382605    5.3625755
  5.5868373   9.949682    0.82923275  1.9832962  -0.10069095  8.219267
  8.539603    9.221239    9.5321455   8.239499    4.945965    8.152974
 -0.16393006  6.303131    0.6036788   9.50227     6.1976576   3.613302
  2.6304417   8.037386  ]
Epoch 1/1000
2023-10-23 12:40:19.176 
Epoch 1/1000 
	 loss: 1239.3098, MinusLogProbMetric: 1239.3098, val_loss: 1075.5774, val_MinusLogProbMetric: 1075.5774

Epoch 1: val_loss improved from inf to 1075.57739, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 223s - loss: 1239.3098 - MinusLogProbMetric: 1239.3098 - val_loss: 1075.5774 - val_MinusLogProbMetric: 1075.5774 - lr: 3.3333e-04 - 223s/epoch - 1s/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 140: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-23 12:41:09.418 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 617.4393, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 1075.57739
196/196 - 49s - loss: nan - MinusLogProbMetric: 617.4393 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 49s/epoch - 249ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0001111111111111111.
===========
Generating train data for run 263.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_263
self.data_kwargs: {'seed': 377}
self.x_data: [[5.416377   7.151266   6.5657053  ... 3.613302   2.6304417  8.037386  ]
 [4.297167   5.832213   0.12875912 ... 1.4317983  6.415448   1.3533646 ]
 [6.2353234  7.1795187  6.219724   ... 4.5891814  2.66546    7.551563  ]
 ...
 [1.8237456  4.102903   7.5608215  ... 7.136859   2.564185   1.5539532 ]
 [2.2305896  4.6568003  8.321339   ... 7.0577617  2.6189713  1.518045  ]
 [5.653481   7.1263475  6.929123   ... 4.292566   2.643216   7.404443  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7fe8803a5150>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe6dc72eb60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe6dc72eb60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe92a0facb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe92034eec0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe92034f430>, <keras.callbacks.ModelCheckpoint object at 0x7fe92034f4f0>, <keras.callbacks.EarlyStopping object at 0x7fe92034f760>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe92034f790>, <keras.callbacks.TerminateOnNaN object at 0x7fe92034f3d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 263/720 with hyperparameters:
timestamp = 2023-10-23 12:41:18.511887
ndims = 32
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.416377    7.151266    6.5657053   5.44807     3.870332    6.1800294
  2.9008856   8.441634    9.391678    4.1002183   7.382605    5.3625755
  5.5868373   9.949682    0.82923275  1.9832962  -0.10069095  8.219267
  8.539603    9.221239    9.5321455   8.239499    4.945965    8.152974
 -0.16393006  6.303131    0.6036788   9.50227     6.1976576   3.613302
  2.6304417   8.037386  ]
Epoch 1/1000
2023-10-23 12:45:12.249 
Epoch 1/1000 
	 loss: 379.5732, MinusLogProbMetric: 379.5732, val_loss: 236.8631, val_MinusLogProbMetric: 236.8631

Epoch 1: val_loss improved from inf to 236.86310, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 235s - loss: 379.5732 - MinusLogProbMetric: 379.5732 - val_loss: 236.8631 - val_MinusLogProbMetric: 236.8631 - lr: 1.1111e-04 - 235s/epoch - 1s/step
Epoch 2/1000
2023-10-23 12:46:20.011 
Epoch 2/1000 
	 loss: 271.6324, MinusLogProbMetric: 271.6324, val_loss: 196.7158, val_MinusLogProbMetric: 196.7158

Epoch 2: val_loss improved from 236.86310 to 196.71576, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 271.6324 - MinusLogProbMetric: 271.6324 - val_loss: 196.7158 - val_MinusLogProbMetric: 196.7158 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 3/1000
2023-10-23 12:47:29.480 
Epoch 3/1000 
	 loss: 159.7747, MinusLogProbMetric: 159.7747, val_loss: 138.1609, val_MinusLogProbMetric: 138.1609

Epoch 3: val_loss improved from 196.71576 to 138.16087, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 69s - loss: 159.7747 - MinusLogProbMetric: 159.7747 - val_loss: 138.1609 - val_MinusLogProbMetric: 138.1609 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 4/1000
2023-10-23 12:48:36.457 
Epoch 4/1000 
	 loss: 121.5444, MinusLogProbMetric: 121.5444, val_loss: 105.0462, val_MinusLogProbMetric: 105.0462

Epoch 4: val_loss improved from 138.16087 to 105.04615, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 121.5444 - MinusLogProbMetric: 121.5444 - val_loss: 105.0462 - val_MinusLogProbMetric: 105.0462 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 5/1000
2023-10-23 12:49:47.512 
Epoch 5/1000 
	 loss: 101.0169, MinusLogProbMetric: 101.0169, val_loss: 107.2819, val_MinusLogProbMetric: 107.2819

Epoch 5: val_loss did not improve from 105.04615
196/196 - 70s - loss: 101.0169 - MinusLogProbMetric: 101.0169 - val_loss: 107.2819 - val_MinusLogProbMetric: 107.2819 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 6/1000
2023-10-23 12:50:55.640 
Epoch 6/1000 
	 loss: 88.8717, MinusLogProbMetric: 88.8717, val_loss: 82.3696, val_MinusLogProbMetric: 82.3696

Epoch 6: val_loss improved from 105.04615 to 82.36960, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 69s - loss: 88.8717 - MinusLogProbMetric: 88.8717 - val_loss: 82.3696 - val_MinusLogProbMetric: 82.3696 - lr: 1.1111e-04 - 69s/epoch - 353ms/step
Epoch 7/1000
2023-10-23 12:52:02.473 
Epoch 7/1000 
	 loss: 150.6762, MinusLogProbMetric: 150.6762, val_loss: 204.7557, val_MinusLogProbMetric: 204.7557

Epoch 7: val_loss did not improve from 82.36960
196/196 - 66s - loss: 150.6762 - MinusLogProbMetric: 150.6762 - val_loss: 204.7557 - val_MinusLogProbMetric: 204.7557 - lr: 1.1111e-04 - 66s/epoch - 336ms/step
Epoch 8/1000
2023-10-23 12:53:11.895 
Epoch 8/1000 
	 loss: 151.4961, MinusLogProbMetric: 151.4961, val_loss: 122.4404, val_MinusLogProbMetric: 122.4404

Epoch 8: val_loss did not improve from 82.36960
196/196 - 69s - loss: 151.4961 - MinusLogProbMetric: 151.4961 - val_loss: 122.4404 - val_MinusLogProbMetric: 122.4404 - lr: 1.1111e-04 - 69s/epoch - 354ms/step
Epoch 9/1000
2023-10-23 12:54:21.482 
Epoch 9/1000 
	 loss: 101.2853, MinusLogProbMetric: 101.2853, val_loss: 96.0277, val_MinusLogProbMetric: 96.0277

Epoch 9: val_loss did not improve from 82.36960
196/196 - 70s - loss: 101.2853 - MinusLogProbMetric: 101.2853 - val_loss: 96.0277 - val_MinusLogProbMetric: 96.0277 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 10/1000
2023-10-23 12:55:32.437 
Epoch 10/1000 
	 loss: 85.1601, MinusLogProbMetric: 85.1601, val_loss: 77.7040, val_MinusLogProbMetric: 77.7040

Epoch 10: val_loss improved from 82.36960 to 77.70399, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 72s - loss: 85.1601 - MinusLogProbMetric: 85.1601 - val_loss: 77.7040 - val_MinusLogProbMetric: 77.7040 - lr: 1.1111e-04 - 72s/epoch - 368ms/step
Epoch 11/1000
2023-10-23 12:56:40.021 
Epoch 11/1000 
	 loss: 101.7383, MinusLogProbMetric: 101.7383, val_loss: 82.8057, val_MinusLogProbMetric: 82.8057

Epoch 11: val_loss did not improve from 77.70399
196/196 - 66s - loss: 101.7383 - MinusLogProbMetric: 101.7383 - val_loss: 82.8057 - val_MinusLogProbMetric: 82.8057 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 12/1000
2023-10-23 12:57:46.135 
Epoch 12/1000 
	 loss: 81.3832, MinusLogProbMetric: 81.3832, val_loss: 72.0780, val_MinusLogProbMetric: 72.0780

Epoch 12: val_loss improved from 77.70399 to 72.07796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 81.3832 - MinusLogProbMetric: 81.3832 - val_loss: 72.0780 - val_MinusLogProbMetric: 72.0780 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 13/1000
2023-10-23 12:58:52.496 
Epoch 13/1000 
	 loss: 66.8601, MinusLogProbMetric: 66.8601, val_loss: 62.5104, val_MinusLogProbMetric: 62.5104

Epoch 13: val_loss improved from 72.07796 to 62.51038, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 66.8601 - MinusLogProbMetric: 66.8601 - val_loss: 62.5104 - val_MinusLogProbMetric: 62.5104 - lr: 1.1111e-04 - 67s/epoch - 339ms/step
Epoch 14/1000
2023-10-23 13:00:01.347 
Epoch 14/1000 
	 loss: 60.0813, MinusLogProbMetric: 60.0813, val_loss: 56.9231, val_MinusLogProbMetric: 56.9231

Epoch 14: val_loss improved from 62.51038 to 56.92308, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 69s - loss: 60.0813 - MinusLogProbMetric: 60.0813 - val_loss: 56.9231 - val_MinusLogProbMetric: 56.9231 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 15/1000
2023-10-23 13:01:09.331 
Epoch 15/1000 
	 loss: 55.8862, MinusLogProbMetric: 55.8862, val_loss: 54.7815, val_MinusLogProbMetric: 54.7815

Epoch 15: val_loss improved from 56.92308 to 54.78148, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 55.8862 - MinusLogProbMetric: 55.8862 - val_loss: 54.7815 - val_MinusLogProbMetric: 54.7815 - lr: 1.1111e-04 - 68s/epoch - 346ms/step
Epoch 16/1000
2023-10-23 13:02:25.193 
Epoch 16/1000 
	 loss: 58.3148, MinusLogProbMetric: 58.3148, val_loss: 90.6957, val_MinusLogProbMetric: 90.6957

Epoch 16: val_loss did not improve from 54.78148
196/196 - 75s - loss: 58.3148 - MinusLogProbMetric: 58.3148 - val_loss: 90.6957 - val_MinusLogProbMetric: 90.6957 - lr: 1.1111e-04 - 75s/epoch - 381ms/step
Epoch 17/1000
2023-10-23 13:03:34.777 
Epoch 17/1000 
	 loss: 60.5270, MinusLogProbMetric: 60.5270, val_loss: 58.5309, val_MinusLogProbMetric: 58.5309

Epoch 17: val_loss did not improve from 54.78148
196/196 - 70s - loss: 60.5270 - MinusLogProbMetric: 60.5270 - val_loss: 58.5309 - val_MinusLogProbMetric: 58.5309 - lr: 1.1111e-04 - 70s/epoch - 355ms/step
Epoch 18/1000
2023-10-23 13:04:44.162 
Epoch 18/1000 
	 loss: 60.2640, MinusLogProbMetric: 60.2640, val_loss: 50.5454, val_MinusLogProbMetric: 50.5454

Epoch 18: val_loss improved from 54.78148 to 50.54540, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 70s - loss: 60.2640 - MinusLogProbMetric: 60.2640 - val_loss: 50.5454 - val_MinusLogProbMetric: 50.5454 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 19/1000
2023-10-23 13:05:54.673 
Epoch 19/1000 
	 loss: 49.3216, MinusLogProbMetric: 49.3216, val_loss: 47.1810, val_MinusLogProbMetric: 47.1810

Epoch 19: val_loss improved from 50.54540 to 47.18099, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 71s - loss: 49.3216 - MinusLogProbMetric: 49.3216 - val_loss: 47.1810 - val_MinusLogProbMetric: 47.1810 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 20/1000
2023-10-23 13:07:04.835 
Epoch 20/1000 
	 loss: 46.6428, MinusLogProbMetric: 46.6428, val_loss: 45.8825, val_MinusLogProbMetric: 45.8825

Epoch 20: val_loss improved from 47.18099 to 45.88245, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 70s - loss: 46.6428 - MinusLogProbMetric: 46.6428 - val_loss: 45.8825 - val_MinusLogProbMetric: 45.8825 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 21/1000
2023-10-23 13:08:13.604 
Epoch 21/1000 
	 loss: 43.7826, MinusLogProbMetric: 43.7826, val_loss: 42.7585, val_MinusLogProbMetric: 42.7585

Epoch 21: val_loss improved from 45.88245 to 42.75850, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 69s - loss: 43.7826 - MinusLogProbMetric: 43.7826 - val_loss: 42.7585 - val_MinusLogProbMetric: 42.7585 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 22/1000
2023-10-23 13:09:20.298 
Epoch 22/1000 
	 loss: 42.0284, MinusLogProbMetric: 42.0284, val_loss: 41.3019, val_MinusLogProbMetric: 41.3019

Epoch 22: val_loss improved from 42.75850 to 41.30186, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 42.0284 - MinusLogProbMetric: 42.0284 - val_loss: 41.3019 - val_MinusLogProbMetric: 41.3019 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 23/1000
2023-10-23 13:10:32.713 
Epoch 23/1000 
	 loss: 40.5653, MinusLogProbMetric: 40.5653, val_loss: 40.1471, val_MinusLogProbMetric: 40.1471

Epoch 23: val_loss improved from 41.30186 to 40.14712, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 73s - loss: 40.5653 - MinusLogProbMetric: 40.5653 - val_loss: 40.1471 - val_MinusLogProbMetric: 40.1471 - lr: 1.1111e-04 - 73s/epoch - 370ms/step
Epoch 24/1000
2023-10-23 13:11:42.830 
Epoch 24/1000 
	 loss: 39.3251, MinusLogProbMetric: 39.3251, val_loss: 38.3037, val_MinusLogProbMetric: 38.3037

Epoch 24: val_loss improved from 40.14712 to 38.30365, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 70s - loss: 39.3251 - MinusLogProbMetric: 39.3251 - val_loss: 38.3037 - val_MinusLogProbMetric: 38.3037 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 25/1000
2023-10-23 13:12:57.010 
Epoch 25/1000 
	 loss: 38.0517, MinusLogProbMetric: 38.0517, val_loss: 38.3122, val_MinusLogProbMetric: 38.3122

Epoch 25: val_loss did not improve from 38.30365
196/196 - 73s - loss: 38.0517 - MinusLogProbMetric: 38.0517 - val_loss: 38.3122 - val_MinusLogProbMetric: 38.3122 - lr: 1.1111e-04 - 73s/epoch - 373ms/step
Epoch 26/1000
2023-10-23 13:14:12.624 
Epoch 26/1000 
	 loss: 36.8103, MinusLogProbMetric: 36.8103, val_loss: 37.2893, val_MinusLogProbMetric: 37.2893

Epoch 26: val_loss improved from 38.30365 to 37.28927, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 77s - loss: 36.8103 - MinusLogProbMetric: 36.8103 - val_loss: 37.2893 - val_MinusLogProbMetric: 37.2893 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 27/1000
2023-10-23 13:15:29.352 
Epoch 27/1000 
	 loss: 35.6566, MinusLogProbMetric: 35.6566, val_loss: 38.9441, val_MinusLogProbMetric: 38.9441

Epoch 27: val_loss did not improve from 37.28927
196/196 - 75s - loss: 35.6566 - MinusLogProbMetric: 35.6566 - val_loss: 38.9441 - val_MinusLogProbMetric: 38.9441 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 28/1000
2023-10-23 13:16:46.023 
Epoch 28/1000 
	 loss: 34.6735, MinusLogProbMetric: 34.6735, val_loss: 34.1613, val_MinusLogProbMetric: 34.1613

Epoch 28: val_loss improved from 37.28927 to 34.16125, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 78s - loss: 34.6735 - MinusLogProbMetric: 34.6735 - val_loss: 34.1613 - val_MinusLogProbMetric: 34.1613 - lr: 1.1111e-04 - 78s/epoch - 397ms/step
Epoch 29/1000
2023-10-23 13:18:00.094 
Epoch 29/1000 
	 loss: 33.9933, MinusLogProbMetric: 33.9933, val_loss: 33.6366, val_MinusLogProbMetric: 33.6366

Epoch 29: val_loss improved from 34.16125 to 33.63655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 74s - loss: 33.9933 - MinusLogProbMetric: 33.9933 - val_loss: 33.6366 - val_MinusLogProbMetric: 33.6366 - lr: 1.1111e-04 - 74s/epoch - 378ms/step
Epoch 30/1000
2023-10-23 13:19:15.721 
Epoch 30/1000 
	 loss: 33.1800, MinusLogProbMetric: 33.1800, val_loss: 33.8938, val_MinusLogProbMetric: 33.8938

Epoch 30: val_loss did not improve from 33.63655
196/196 - 75s - loss: 33.1800 - MinusLogProbMetric: 33.1800 - val_loss: 33.8938 - val_MinusLogProbMetric: 33.8938 - lr: 1.1111e-04 - 75s/epoch - 380ms/step
Epoch 31/1000
2023-10-23 13:20:23.402 
Epoch 31/1000 
	 loss: 32.7759, MinusLogProbMetric: 32.7759, val_loss: 32.9085, val_MinusLogProbMetric: 32.9085

Epoch 31: val_loss improved from 33.63655 to 32.90850, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 69s - loss: 32.7759 - MinusLogProbMetric: 32.7759 - val_loss: 32.9085 - val_MinusLogProbMetric: 32.9085 - lr: 1.1111e-04 - 69s/epoch - 351ms/step
Epoch 32/1000
2023-10-23 13:21:37.989 
Epoch 32/1000 
	 loss: 32.2610, MinusLogProbMetric: 32.2610, val_loss: 31.7317, val_MinusLogProbMetric: 31.7317

Epoch 32: val_loss improved from 32.90850 to 31.73173, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 75s - loss: 32.2610 - MinusLogProbMetric: 32.2610 - val_loss: 31.7317 - val_MinusLogProbMetric: 31.7317 - lr: 1.1111e-04 - 75s/epoch - 382ms/step
Epoch 33/1000
2023-10-23 13:22:50.364 
Epoch 33/1000 
	 loss: 31.6705, MinusLogProbMetric: 31.6705, val_loss: 31.2374, val_MinusLogProbMetric: 31.2374

Epoch 33: val_loss improved from 31.73173 to 31.23736, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 72s - loss: 31.6705 - MinusLogProbMetric: 31.6705 - val_loss: 31.2374 - val_MinusLogProbMetric: 31.2374 - lr: 1.1111e-04 - 72s/epoch - 368ms/step
Epoch 34/1000
2023-10-23 13:24:01.957 
Epoch 34/1000 
	 loss: 31.1840, MinusLogProbMetric: 31.1840, val_loss: 31.0043, val_MinusLogProbMetric: 31.0043

Epoch 34: val_loss improved from 31.23736 to 31.00434, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 71s - loss: 31.1840 - MinusLogProbMetric: 31.1840 - val_loss: 31.0043 - val_MinusLogProbMetric: 31.0043 - lr: 1.1111e-04 - 71s/epoch - 365ms/step
Epoch 35/1000
2023-10-23 13:25:10.793 
Epoch 35/1000 
	 loss: 30.5350, MinusLogProbMetric: 30.5350, val_loss: 31.0979, val_MinusLogProbMetric: 31.0979

Epoch 35: val_loss did not improve from 31.00434
196/196 - 68s - loss: 30.5350 - MinusLogProbMetric: 30.5350 - val_loss: 31.0979 - val_MinusLogProbMetric: 31.0979 - lr: 1.1111e-04 - 68s/epoch - 346ms/step
Epoch 36/1000
2023-10-23 13:26:23.406 
Epoch 36/1000 
	 loss: 30.2454, MinusLogProbMetric: 30.2454, val_loss: 30.1337, val_MinusLogProbMetric: 30.1337

Epoch 36: val_loss improved from 31.00434 to 30.13371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 74s - loss: 30.2454 - MinusLogProbMetric: 30.2454 - val_loss: 30.1337 - val_MinusLogProbMetric: 30.1337 - lr: 1.1111e-04 - 74s/epoch - 376ms/step
Epoch 37/1000
2023-10-23 13:27:36.800 
Epoch 37/1000 
	 loss: 29.7624, MinusLogProbMetric: 29.7624, val_loss: 29.8939, val_MinusLogProbMetric: 29.8939

Epoch 37: val_loss improved from 30.13371 to 29.89391, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 74s - loss: 29.7624 - MinusLogProbMetric: 29.7624 - val_loss: 29.8939 - val_MinusLogProbMetric: 29.8939 - lr: 1.1111e-04 - 74s/epoch - 375ms/step
Epoch 38/1000
2023-10-23 13:28:52.045 
Epoch 38/1000 
	 loss: 29.3896, MinusLogProbMetric: 29.3896, val_loss: 29.7758, val_MinusLogProbMetric: 29.7758

Epoch 38: val_loss improved from 29.89391 to 29.77579, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 75s - loss: 29.3896 - MinusLogProbMetric: 29.3896 - val_loss: 29.7758 - val_MinusLogProbMetric: 29.7758 - lr: 1.1111e-04 - 75s/epoch - 384ms/step
Epoch 39/1000
2023-10-23 13:30:06.959 
Epoch 39/1000 
	 loss: 29.0810, MinusLogProbMetric: 29.0810, val_loss: 28.7547, val_MinusLogProbMetric: 28.7547

Epoch 39: val_loss improved from 29.77579 to 28.75465, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 75s - loss: 29.0810 - MinusLogProbMetric: 29.0810 - val_loss: 28.7547 - val_MinusLogProbMetric: 28.7547 - lr: 1.1111e-04 - 75s/epoch - 381ms/step
Epoch 40/1000
2023-10-23 13:31:14.326 
Epoch 40/1000 
	 loss: 28.6516, MinusLogProbMetric: 28.6516, val_loss: 28.4433, val_MinusLogProbMetric: 28.4433

Epoch 40: val_loss improved from 28.75465 to 28.44328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 28.6516 - MinusLogProbMetric: 28.6516 - val_loss: 28.4433 - val_MinusLogProbMetric: 28.4433 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 41/1000
2023-10-23 13:32:20.657 
Epoch 41/1000 
	 loss: 28.3370, MinusLogProbMetric: 28.3370, val_loss: 28.0613, val_MinusLogProbMetric: 28.0613

Epoch 41: val_loss improved from 28.44328 to 28.06128, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 66s - loss: 28.3370 - MinusLogProbMetric: 28.3370 - val_loss: 28.0613 - val_MinusLogProbMetric: 28.0613 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 42/1000
2023-10-23 13:33:30.883 
Epoch 42/1000 
	 loss: 28.1739, MinusLogProbMetric: 28.1739, val_loss: 27.9587, val_MinusLogProbMetric: 27.9587

Epoch 42: val_loss improved from 28.06128 to 27.95866, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 70s - loss: 28.1739 - MinusLogProbMetric: 28.1739 - val_loss: 27.9587 - val_MinusLogProbMetric: 27.9587 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 43/1000
2023-10-23 13:34:45.022 
Epoch 43/1000 
	 loss: 28.0173, MinusLogProbMetric: 28.0173, val_loss: 27.5388, val_MinusLogProbMetric: 27.5388

Epoch 43: val_loss improved from 27.95866 to 27.53881, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 74s - loss: 28.0173 - MinusLogProbMetric: 28.0173 - val_loss: 27.5388 - val_MinusLogProbMetric: 27.5388 - lr: 1.1111e-04 - 74s/epoch - 377ms/step
Epoch 44/1000
2023-10-23 13:35:56.573 
Epoch 44/1000 
	 loss: 27.5019, MinusLogProbMetric: 27.5019, val_loss: 27.6605, val_MinusLogProbMetric: 27.6605

Epoch 44: val_loss did not improve from 27.53881
196/196 - 70s - loss: 27.5019 - MinusLogProbMetric: 27.5019 - val_loss: 27.6605 - val_MinusLogProbMetric: 27.6605 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 45/1000
2023-10-23 13:37:03.822 
Epoch 45/1000 
	 loss: 27.2754, MinusLogProbMetric: 27.2754, val_loss: 27.4495, val_MinusLogProbMetric: 27.4495

Epoch 45: val_loss improved from 27.53881 to 27.44947, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 27.2754 - MinusLogProbMetric: 27.2754 - val_loss: 27.4495 - val_MinusLogProbMetric: 27.4495 - lr: 1.1111e-04 - 68s/epoch - 349ms/step
Epoch 46/1000
2023-10-23 13:38:12.628 
Epoch 46/1000 
	 loss: 27.1625, MinusLogProbMetric: 27.1625, val_loss: 28.0861, val_MinusLogProbMetric: 28.0861

Epoch 46: val_loss did not improve from 27.44947
196/196 - 68s - loss: 27.1625 - MinusLogProbMetric: 27.1625 - val_loss: 28.0861 - val_MinusLogProbMetric: 28.0861 - lr: 1.1111e-04 - 68s/epoch - 345ms/step
Epoch 47/1000
2023-10-23 13:39:21.204 
Epoch 47/1000 
	 loss: 26.8426, MinusLogProbMetric: 26.8426, val_loss: 27.2482, val_MinusLogProbMetric: 27.2482

Epoch 47: val_loss improved from 27.44947 to 27.24825, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 70s - loss: 26.8426 - MinusLogProbMetric: 26.8426 - val_loss: 27.2482 - val_MinusLogProbMetric: 27.2482 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 48/1000
2023-10-23 13:40:32.858 
Epoch 48/1000 
	 loss: 26.6962, MinusLogProbMetric: 26.6962, val_loss: 26.3807, val_MinusLogProbMetric: 26.3807

Epoch 48: val_loss improved from 27.24825 to 26.38068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 72s - loss: 26.6962 - MinusLogProbMetric: 26.6962 - val_loss: 26.3807 - val_MinusLogProbMetric: 26.3807 - lr: 1.1111e-04 - 72s/epoch - 365ms/step
Epoch 49/1000
2023-10-23 13:41:46.281 
Epoch 49/1000 
	 loss: 26.3952, MinusLogProbMetric: 26.3952, val_loss: 26.2242, val_MinusLogProbMetric: 26.2242

Epoch 49: val_loss improved from 26.38068 to 26.22417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 73s - loss: 26.3952 - MinusLogProbMetric: 26.3952 - val_loss: 26.2242 - val_MinusLogProbMetric: 26.2242 - lr: 1.1111e-04 - 73s/epoch - 374ms/step
Epoch 50/1000
2023-10-23 13:43:00.604 
Epoch 50/1000 
	 loss: 26.1450, MinusLogProbMetric: 26.1450, val_loss: 26.1013, val_MinusLogProbMetric: 26.1013

Epoch 50: val_loss improved from 26.22417 to 26.10127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 75s - loss: 26.1450 - MinusLogProbMetric: 26.1450 - val_loss: 26.1013 - val_MinusLogProbMetric: 26.1013 - lr: 1.1111e-04 - 75s/epoch - 380ms/step
Epoch 51/1000
2023-10-23 13:44:13.520 
Epoch 51/1000 
	 loss: 26.0602, MinusLogProbMetric: 26.0602, val_loss: 25.6306, val_MinusLogProbMetric: 25.6306

Epoch 51: val_loss improved from 26.10127 to 25.63057, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 73s - loss: 26.0602 - MinusLogProbMetric: 26.0602 - val_loss: 25.6306 - val_MinusLogProbMetric: 25.6306 - lr: 1.1111e-04 - 73s/epoch - 372ms/step
Epoch 52/1000
2023-10-23 13:45:28.445 
Epoch 52/1000 
	 loss: 25.7426, MinusLogProbMetric: 25.7426, val_loss: 25.5650, val_MinusLogProbMetric: 25.5650

Epoch 52: val_loss improved from 25.63057 to 25.56497, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 75s - loss: 25.7426 - MinusLogProbMetric: 25.7426 - val_loss: 25.5650 - val_MinusLogProbMetric: 25.5650 - lr: 1.1111e-04 - 75s/epoch - 381ms/step
Epoch 53/1000
2023-10-23 13:46:37.365 
Epoch 53/1000 
	 loss: 25.5457, MinusLogProbMetric: 25.5457, val_loss: 25.5467, val_MinusLogProbMetric: 25.5467

Epoch 53: val_loss improved from 25.56497 to 25.54669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 69s - loss: 25.5457 - MinusLogProbMetric: 25.5457 - val_loss: 25.5467 - val_MinusLogProbMetric: 25.5467 - lr: 1.1111e-04 - 69s/epoch - 351ms/step
Epoch 54/1000
2023-10-23 13:47:49.804 
Epoch 54/1000 
	 loss: 25.5003, MinusLogProbMetric: 25.5003, val_loss: 25.5410, val_MinusLogProbMetric: 25.5410

Epoch 54: val_loss improved from 25.54669 to 25.54097, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 72s - loss: 25.5003 - MinusLogProbMetric: 25.5003 - val_loss: 25.5410 - val_MinusLogProbMetric: 25.5410 - lr: 1.1111e-04 - 72s/epoch - 370ms/step
Epoch 55/1000
2023-10-23 13:49:02.178 
Epoch 55/1000 
	 loss: 25.1605, MinusLogProbMetric: 25.1605, val_loss: 25.1889, val_MinusLogProbMetric: 25.1889

Epoch 55: val_loss improved from 25.54097 to 25.18891, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 73s - loss: 25.1605 - MinusLogProbMetric: 25.1605 - val_loss: 25.1889 - val_MinusLogProbMetric: 25.1889 - lr: 1.1111e-04 - 73s/epoch - 370ms/step
Epoch 56/1000
2023-10-23 13:50:15.966 
Epoch 56/1000 
	 loss: 25.1655, MinusLogProbMetric: 25.1655, val_loss: 24.9829, val_MinusLogProbMetric: 24.9829

Epoch 56: val_loss improved from 25.18891 to 24.98290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 74s - loss: 25.1655 - MinusLogProbMetric: 25.1655 - val_loss: 24.9829 - val_MinusLogProbMetric: 24.9829 - lr: 1.1111e-04 - 74s/epoch - 377ms/step
Epoch 57/1000
2023-10-23 13:51:30.649 
Epoch 57/1000 
	 loss: 24.9533, MinusLogProbMetric: 24.9533, val_loss: 24.8786, val_MinusLogProbMetric: 24.8786

Epoch 57: val_loss improved from 24.98290 to 24.87864, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 74s - loss: 24.9533 - MinusLogProbMetric: 24.9533 - val_loss: 24.8786 - val_MinusLogProbMetric: 24.8786 - lr: 1.1111e-04 - 74s/epoch - 380ms/step
Epoch 58/1000
2023-10-23 13:52:45.382 
Epoch 58/1000 
	 loss: 24.8124, MinusLogProbMetric: 24.8124, val_loss: 24.8903, val_MinusLogProbMetric: 24.8903

Epoch 58: val_loss did not improve from 24.87864
196/196 - 74s - loss: 24.8124 - MinusLogProbMetric: 24.8124 - val_loss: 24.8903 - val_MinusLogProbMetric: 24.8903 - lr: 1.1111e-04 - 74s/epoch - 375ms/step
Epoch 59/1000
2023-10-23 13:53:54.149 
Epoch 59/1000 
	 loss: 24.5137, MinusLogProbMetric: 24.5137, val_loss: 24.2883, val_MinusLogProbMetric: 24.2883

Epoch 59: val_loss improved from 24.87864 to 24.28829, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 70s - loss: 24.5137 - MinusLogProbMetric: 24.5137 - val_loss: 24.2883 - val_MinusLogProbMetric: 24.2883 - lr: 1.1111e-04 - 70s/epoch - 356ms/step
Epoch 60/1000
2023-10-23 13:55:07.872 
Epoch 60/1000 
	 loss: 24.4533, MinusLogProbMetric: 24.4533, val_loss: 24.7940, val_MinusLogProbMetric: 24.7940

Epoch 60: val_loss did not improve from 24.28829
196/196 - 73s - loss: 24.4533 - MinusLogProbMetric: 24.4533 - val_loss: 24.7940 - val_MinusLogProbMetric: 24.7940 - lr: 1.1111e-04 - 73s/epoch - 371ms/step
Epoch 61/1000
2023-10-23 13:56:19.876 
Epoch 61/1000 
	 loss: 24.2664, MinusLogProbMetric: 24.2664, val_loss: 24.1960, val_MinusLogProbMetric: 24.1960

Epoch 61: val_loss improved from 24.28829 to 24.19603, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 73s - loss: 24.2664 - MinusLogProbMetric: 24.2664 - val_loss: 24.1960 - val_MinusLogProbMetric: 24.1960 - lr: 1.1111e-04 - 73s/epoch - 373ms/step
Epoch 62/1000
2023-10-23 13:57:32.375 
Epoch 62/1000 
	 loss: 24.2929, MinusLogProbMetric: 24.2929, val_loss: 24.1039, val_MinusLogProbMetric: 24.1039

Epoch 62: val_loss improved from 24.19603 to 24.10393, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 73s - loss: 24.2929 - MinusLogProbMetric: 24.2929 - val_loss: 24.1039 - val_MinusLogProbMetric: 24.1039 - lr: 1.1111e-04 - 73s/epoch - 370ms/step
Epoch 63/1000
2023-10-23 13:58:46.345 
Epoch 63/1000 
	 loss: 23.9765, MinusLogProbMetric: 23.9765, val_loss: 23.9215, val_MinusLogProbMetric: 23.9215

Epoch 63: val_loss improved from 24.10393 to 23.92153, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 74s - loss: 23.9765 - MinusLogProbMetric: 23.9765 - val_loss: 23.9215 - val_MinusLogProbMetric: 23.9215 - lr: 1.1111e-04 - 74s/epoch - 377ms/step
Epoch 64/1000
2023-10-23 13:59:58.971 
Epoch 64/1000 
	 loss: 23.8379, MinusLogProbMetric: 23.8379, val_loss: 24.8206, val_MinusLogProbMetric: 24.8206

Epoch 64: val_loss did not improve from 23.92153
196/196 - 71s - loss: 23.8379 - MinusLogProbMetric: 23.8379 - val_loss: 24.8206 - val_MinusLogProbMetric: 24.8206 - lr: 1.1111e-04 - 71s/epoch - 365ms/step
Epoch 65/1000
2023-10-23 14:01:10.187 
Epoch 65/1000 
	 loss: 23.6964, MinusLogProbMetric: 23.6964, val_loss: 23.6953, val_MinusLogProbMetric: 23.6953

Epoch 65: val_loss improved from 23.92153 to 23.69532, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 72s - loss: 23.6964 - MinusLogProbMetric: 23.6964 - val_loss: 23.6953 - val_MinusLogProbMetric: 23.6953 - lr: 1.1111e-04 - 72s/epoch - 369ms/step
Epoch 66/1000
2023-10-23 14:02:20.275 
Epoch 66/1000 
	 loss: 23.5677, MinusLogProbMetric: 23.5677, val_loss: 23.9304, val_MinusLogProbMetric: 23.9304

Epoch 66: val_loss did not improve from 23.69532
196/196 - 69s - loss: 23.5677 - MinusLogProbMetric: 23.5677 - val_loss: 23.9304 - val_MinusLogProbMetric: 23.9304 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 67/1000
2023-10-23 14:03:27.219 
Epoch 67/1000 
	 loss: 23.5012, MinusLogProbMetric: 23.5012, val_loss: 24.2547, val_MinusLogProbMetric: 24.2547

Epoch 67: val_loss did not improve from 23.69532
196/196 - 67s - loss: 23.5012 - MinusLogProbMetric: 23.5012 - val_loss: 24.2547 - val_MinusLogProbMetric: 24.2547 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 68/1000
2023-10-23 14:04:35.666 
Epoch 68/1000 
	 loss: 23.3274, MinusLogProbMetric: 23.3274, val_loss: 23.8032, val_MinusLogProbMetric: 23.8032

Epoch 68: val_loss did not improve from 23.69532
196/196 - 68s - loss: 23.3274 - MinusLogProbMetric: 23.3274 - val_loss: 23.8032 - val_MinusLogProbMetric: 23.8032 - lr: 1.1111e-04 - 68s/epoch - 349ms/step
Epoch 69/1000
2023-10-23 14:05:46.037 
Epoch 69/1000 
	 loss: 23.2331, MinusLogProbMetric: 23.2331, val_loss: 23.5194, val_MinusLogProbMetric: 23.5194

Epoch 69: val_loss improved from 23.69532 to 23.51937, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 72s - loss: 23.2331 - MinusLogProbMetric: 23.2331 - val_loss: 23.5194 - val_MinusLogProbMetric: 23.5194 - lr: 1.1111e-04 - 72s/epoch - 367ms/step
Epoch 70/1000
2023-10-23 14:06:59.348 
Epoch 70/1000 
	 loss: 23.1152, MinusLogProbMetric: 23.1152, val_loss: 23.2377, val_MinusLogProbMetric: 23.2377

Epoch 70: val_loss improved from 23.51937 to 23.23767, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 73s - loss: 23.1152 - MinusLogProbMetric: 23.1152 - val_loss: 23.2377 - val_MinusLogProbMetric: 23.2377 - lr: 1.1111e-04 - 73s/epoch - 372ms/step
Epoch 71/1000
2023-10-23 14:08:05.754 
Epoch 71/1000 
	 loss: 23.0404, MinusLogProbMetric: 23.0404, val_loss: 22.9481, val_MinusLogProbMetric: 22.9481

Epoch 71: val_loss improved from 23.23767 to 22.94812, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 23.0404 - MinusLogProbMetric: 23.0404 - val_loss: 22.9481 - val_MinusLogProbMetric: 22.9481 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 72/1000
2023-10-23 14:09:13.453 
Epoch 72/1000 
	 loss: 22.8347, MinusLogProbMetric: 22.8347, val_loss: 23.3699, val_MinusLogProbMetric: 23.3699

Epoch 72: val_loss did not improve from 22.94812
196/196 - 66s - loss: 22.8347 - MinusLogProbMetric: 22.8347 - val_loss: 23.3699 - val_MinusLogProbMetric: 23.3699 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 73/1000
2023-10-23 14:10:27.941 
Epoch 73/1000 
	 loss: 22.7655, MinusLogProbMetric: 22.7655, val_loss: 22.9800, val_MinusLogProbMetric: 22.9800

Epoch 73: val_loss did not improve from 22.94812
196/196 - 74s - loss: 22.7655 - MinusLogProbMetric: 22.7655 - val_loss: 22.9800 - val_MinusLogProbMetric: 22.9800 - lr: 1.1111e-04 - 74s/epoch - 380ms/step
Epoch 74/1000
2023-10-23 14:11:40.622 
Epoch 74/1000 
	 loss: 22.6845, MinusLogProbMetric: 22.6845, val_loss: 22.8737, val_MinusLogProbMetric: 22.8737

Epoch 74: val_loss improved from 22.94812 to 22.87374, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 74s - loss: 22.6845 - MinusLogProbMetric: 22.6845 - val_loss: 22.8737 - val_MinusLogProbMetric: 22.8737 - lr: 1.1111e-04 - 74s/epoch - 377ms/step
Epoch 75/1000
2023-10-23 14:12:56.196 
Epoch 75/1000 
	 loss: 22.5582, MinusLogProbMetric: 22.5582, val_loss: 22.4479, val_MinusLogProbMetric: 22.4479

Epoch 75: val_loss improved from 22.87374 to 22.44790, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 76s - loss: 22.5582 - MinusLogProbMetric: 22.5582 - val_loss: 22.4479 - val_MinusLogProbMetric: 22.4479 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 76/1000
2023-10-23 14:14:12.641 
Epoch 76/1000 
	 loss: 22.4476, MinusLogProbMetric: 22.4476, val_loss: 23.0278, val_MinusLogProbMetric: 23.0278

Epoch 76: val_loss did not improve from 22.44790
196/196 - 75s - loss: 22.4476 - MinusLogProbMetric: 22.4476 - val_loss: 23.0278 - val_MinusLogProbMetric: 23.0278 - lr: 1.1111e-04 - 75s/epoch - 384ms/step
Epoch 77/1000
2023-10-23 14:15:28.339 
Epoch 77/1000 
	 loss: 22.3643, MinusLogProbMetric: 22.3643, val_loss: 22.3853, val_MinusLogProbMetric: 22.3853

Epoch 77: val_loss improved from 22.44790 to 22.38535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 77s - loss: 22.3643 - MinusLogProbMetric: 22.3643 - val_loss: 22.3853 - val_MinusLogProbMetric: 22.3853 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 78/1000
2023-10-23 14:16:34.601 
Epoch 78/1000 
	 loss: 22.2773, MinusLogProbMetric: 22.2773, val_loss: 22.3249, val_MinusLogProbMetric: 22.3249

Epoch 78: val_loss improved from 22.38535 to 22.32490, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 66s - loss: 22.2773 - MinusLogProbMetric: 22.2773 - val_loss: 22.3249 - val_MinusLogProbMetric: 22.3249 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 79/1000
2023-10-23 14:17:39.788 
Epoch 79/1000 
	 loss: 22.2141, MinusLogProbMetric: 22.2141, val_loss: 22.1687, val_MinusLogProbMetric: 22.1687

Epoch 79: val_loss improved from 22.32490 to 22.16866, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 22.2141 - MinusLogProbMetric: 22.2141 - val_loss: 22.1687 - val_MinusLogProbMetric: 22.1687 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 80/1000
2023-10-23 14:18:43.344 
Epoch 80/1000 
	 loss: 22.1926, MinusLogProbMetric: 22.1926, val_loss: 22.5075, val_MinusLogProbMetric: 22.5075

Epoch 80: val_loss did not improve from 22.16866
196/196 - 62s - loss: 22.1926 - MinusLogProbMetric: 22.1926 - val_loss: 22.5075 - val_MinusLogProbMetric: 22.5075 - lr: 1.1111e-04 - 62s/epoch - 319ms/step
Epoch 81/1000
2023-10-23 14:19:46.476 
Epoch 81/1000 
	 loss: 22.0804, MinusLogProbMetric: 22.0804, val_loss: 22.6821, val_MinusLogProbMetric: 22.6821

Epoch 81: val_loss did not improve from 22.16866
196/196 - 63s - loss: 22.0804 - MinusLogProbMetric: 22.0804 - val_loss: 22.6821 - val_MinusLogProbMetric: 22.6821 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 82/1000
2023-10-23 14:20:53.736 
Epoch 82/1000 
	 loss: 22.0123, MinusLogProbMetric: 22.0123, val_loss: 22.1360, val_MinusLogProbMetric: 22.1360

Epoch 82: val_loss improved from 22.16866 to 22.13603, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 69s - loss: 22.0123 - MinusLogProbMetric: 22.0123 - val_loss: 22.1360 - val_MinusLogProbMetric: 22.1360 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 83/1000
2023-10-23 14:22:01.833 
Epoch 83/1000 
	 loss: 21.8769, MinusLogProbMetric: 21.8769, val_loss: 22.0104, val_MinusLogProbMetric: 22.0104

Epoch 83: val_loss improved from 22.13603 to 22.01037, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 21.8769 - MinusLogProbMetric: 21.8769 - val_loss: 22.0104 - val_MinusLogProbMetric: 22.0104 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 84/1000
2023-10-23 14:23:16.480 
Epoch 84/1000 
	 loss: 21.8307, MinusLogProbMetric: 21.8307, val_loss: 21.8678, val_MinusLogProbMetric: 21.8678

Epoch 84: val_loss improved from 22.01037 to 21.86780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 75s - loss: 21.8307 - MinusLogProbMetric: 21.8307 - val_loss: 21.8678 - val_MinusLogProbMetric: 21.8678 - lr: 1.1111e-04 - 75s/epoch - 381ms/step
Epoch 85/1000
2023-10-23 14:24:26.397 
Epoch 85/1000 
	 loss: 21.7342, MinusLogProbMetric: 21.7342, val_loss: 21.9188, val_MinusLogProbMetric: 21.9188

Epoch 85: val_loss did not improve from 21.86780
196/196 - 69s - loss: 21.7342 - MinusLogProbMetric: 21.7342 - val_loss: 21.9188 - val_MinusLogProbMetric: 21.9188 - lr: 1.1111e-04 - 69s/epoch - 351ms/step
Epoch 86/1000
2023-10-23 14:25:37.021 
Epoch 86/1000 
	 loss: 22.9887, MinusLogProbMetric: 22.9887, val_loss: 37.0104, val_MinusLogProbMetric: 37.0104

Epoch 86: val_loss did not improve from 21.86780
196/196 - 71s - loss: 22.9887 - MinusLogProbMetric: 22.9887 - val_loss: 37.0104 - val_MinusLogProbMetric: 37.0104 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 87/1000
2023-10-23 14:26:47.791 
Epoch 87/1000 
	 loss: 24.8264, MinusLogProbMetric: 24.8264, val_loss: 21.7620, val_MinusLogProbMetric: 21.7620

Epoch 87: val_loss improved from 21.86780 to 21.76200, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 72s - loss: 24.8264 - MinusLogProbMetric: 24.8264 - val_loss: 21.7620 - val_MinusLogProbMetric: 21.7620 - lr: 1.1111e-04 - 72s/epoch - 368ms/step
Epoch 88/1000
2023-10-23 14:27:59.837 
Epoch 88/1000 
	 loss: 21.5371, MinusLogProbMetric: 21.5371, val_loss: 21.6394, val_MinusLogProbMetric: 21.6394

Epoch 88: val_loss improved from 21.76200 to 21.63938, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 72s - loss: 21.5371 - MinusLogProbMetric: 21.5371 - val_loss: 21.6394 - val_MinusLogProbMetric: 21.6394 - lr: 1.1111e-04 - 72s/epoch - 368ms/step
Epoch 89/1000
2023-10-23 14:29:17.094 
Epoch 89/1000 
	 loss: 21.4665, MinusLogProbMetric: 21.4665, val_loss: 21.6460, val_MinusLogProbMetric: 21.6460

Epoch 89: val_loss did not improve from 21.63938
196/196 - 76s - loss: 21.4665 - MinusLogProbMetric: 21.4665 - val_loss: 21.6460 - val_MinusLogProbMetric: 21.6460 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 90/1000
2023-10-23 14:30:28.834 
Epoch 90/1000 
	 loss: 32.8772, MinusLogProbMetric: 32.8772, val_loss: 27.7108, val_MinusLogProbMetric: 27.7108

Epoch 90: val_loss did not improve from 21.63938
196/196 - 72s - loss: 32.8772 - MinusLogProbMetric: 32.8772 - val_loss: 27.7108 - val_MinusLogProbMetric: 27.7108 - lr: 1.1111e-04 - 72s/epoch - 366ms/step
Epoch 91/1000
2023-10-23 14:31:33.664 
Epoch 91/1000 
	 loss: 22.9996, MinusLogProbMetric: 22.9996, val_loss: 22.5535, val_MinusLogProbMetric: 22.5535

Epoch 91: val_loss did not improve from 21.63938
196/196 - 65s - loss: 22.9996 - MinusLogProbMetric: 22.9996 - val_loss: 22.5535 - val_MinusLogProbMetric: 22.5535 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 92/1000
2023-10-23 14:32:38.100 
Epoch 92/1000 
	 loss: 21.9203, MinusLogProbMetric: 21.9203, val_loss: 21.7725, val_MinusLogProbMetric: 21.7725

Epoch 92: val_loss did not improve from 21.63938
196/196 - 64s - loss: 21.9203 - MinusLogProbMetric: 21.9203 - val_loss: 21.7725 - val_MinusLogProbMetric: 21.7725 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 93/1000
2023-10-23 14:33:42.472 
Epoch 93/1000 
	 loss: 21.7471, MinusLogProbMetric: 21.7471, val_loss: 21.5420, val_MinusLogProbMetric: 21.5420

Epoch 93: val_loss improved from 21.63938 to 21.54202, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 21.7471 - MinusLogProbMetric: 21.7471 - val_loss: 21.5420 - val_MinusLogProbMetric: 21.5420 - lr: 1.1111e-04 - 65s/epoch - 334ms/step
Epoch 94/1000
2023-10-23 14:34:49.694 
Epoch 94/1000 
	 loss: 21.5302, MinusLogProbMetric: 21.5302, val_loss: 21.5519, val_MinusLogProbMetric: 21.5519

Epoch 94: val_loss did not improve from 21.54202
196/196 - 66s - loss: 21.5302 - MinusLogProbMetric: 21.5302 - val_loss: 21.5519 - val_MinusLogProbMetric: 21.5519 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 95/1000
2023-10-23 14:35:55.532 
Epoch 95/1000 
	 loss: 21.4890, MinusLogProbMetric: 21.4890, val_loss: 21.3600, val_MinusLogProbMetric: 21.3600

Epoch 95: val_loss improved from 21.54202 to 21.36000, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 21.4890 - MinusLogProbMetric: 21.4890 - val_loss: 21.3600 - val_MinusLogProbMetric: 21.3600 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 96/1000
2023-10-23 14:37:06.996 
Epoch 96/1000 
	 loss: 21.4124, MinusLogProbMetric: 21.4124, val_loss: 21.3459, val_MinusLogProbMetric: 21.3459

Epoch 96: val_loss improved from 21.36000 to 21.34591, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 71s - loss: 21.4124 - MinusLogProbMetric: 21.4124 - val_loss: 21.3459 - val_MinusLogProbMetric: 21.3459 - lr: 1.1111e-04 - 71s/epoch - 364ms/step
Epoch 97/1000
2023-10-23 14:38:22.254 
Epoch 97/1000 
	 loss: 21.3506, MinusLogProbMetric: 21.3506, val_loss: 21.2405, val_MinusLogProbMetric: 21.2405

Epoch 97: val_loss improved from 21.34591 to 21.24047, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 75s - loss: 21.3506 - MinusLogProbMetric: 21.3506 - val_loss: 21.2405 - val_MinusLogProbMetric: 21.2405 - lr: 1.1111e-04 - 75s/epoch - 384ms/step
Epoch 98/1000
2023-10-23 14:39:30.809 
Epoch 98/1000 
	 loss: 21.2422, MinusLogProbMetric: 21.2422, val_loss: 21.5407, val_MinusLogProbMetric: 21.5407

Epoch 98: val_loss did not improve from 21.24047
196/196 - 67s - loss: 21.2422 - MinusLogProbMetric: 21.2422 - val_loss: 21.5407 - val_MinusLogProbMetric: 21.5407 - lr: 1.1111e-04 - 67s/epoch - 344ms/step
Epoch 99/1000
2023-10-23 14:40:42.842 
Epoch 99/1000 
	 loss: 21.1464, MinusLogProbMetric: 21.1464, val_loss: 21.3934, val_MinusLogProbMetric: 21.3934

Epoch 99: val_loss did not improve from 21.24047
196/196 - 72s - loss: 21.1464 - MinusLogProbMetric: 21.1464 - val_loss: 21.3934 - val_MinusLogProbMetric: 21.3934 - lr: 1.1111e-04 - 72s/epoch - 368ms/step
Epoch 100/1000
2023-10-23 14:41:57.218 
Epoch 100/1000 
	 loss: 21.1213, MinusLogProbMetric: 21.1213, val_loss: 21.1063, val_MinusLogProbMetric: 21.1063

Epoch 100: val_loss improved from 21.24047 to 21.10631, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 76s - loss: 21.1213 - MinusLogProbMetric: 21.1213 - val_loss: 21.1063 - val_MinusLogProbMetric: 21.1063 - lr: 1.1111e-04 - 76s/epoch - 386ms/step
Epoch 101/1000
2023-10-23 14:43:07.439 
Epoch 101/1000 
	 loss: 21.0814, MinusLogProbMetric: 21.0814, val_loss: 21.2189, val_MinusLogProbMetric: 21.2189

Epoch 101: val_loss did not improve from 21.10631
196/196 - 69s - loss: 21.0814 - MinusLogProbMetric: 21.0814 - val_loss: 21.2189 - val_MinusLogProbMetric: 21.2189 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 102/1000
2023-10-23 14:44:16.315 
Epoch 102/1000 
	 loss: 23.2694, MinusLogProbMetric: 23.2694, val_loss: 42.3859, val_MinusLogProbMetric: 42.3859

Epoch 102: val_loss did not improve from 21.10631
196/196 - 69s - loss: 23.2694 - MinusLogProbMetric: 23.2694 - val_loss: 42.3859 - val_MinusLogProbMetric: 42.3859 - lr: 1.1111e-04 - 69s/epoch - 351ms/step
Epoch 103/1000
2023-10-23 14:45:32.946 
Epoch 103/1000 
	 loss: 28.4033, MinusLogProbMetric: 28.4033, val_loss: 36.6563, val_MinusLogProbMetric: 36.6563

Epoch 103: val_loss did not improve from 21.10631
196/196 - 77s - loss: 28.4033 - MinusLogProbMetric: 28.4033 - val_loss: 36.6563 - val_MinusLogProbMetric: 36.6563 - lr: 1.1111e-04 - 77s/epoch - 391ms/step
Epoch 104/1000
2023-10-23 14:46:48.073 
Epoch 104/1000 
	 loss: 22.8821, MinusLogProbMetric: 22.8821, val_loss: 21.3833, val_MinusLogProbMetric: 21.3833

Epoch 104: val_loss did not improve from 21.10631
196/196 - 75s - loss: 22.8821 - MinusLogProbMetric: 22.8821 - val_loss: 21.3833 - val_MinusLogProbMetric: 21.3833 - lr: 1.1111e-04 - 75s/epoch - 383ms/step
Epoch 105/1000
2023-10-23 14:47:58.305 
Epoch 105/1000 
	 loss: 21.0853, MinusLogProbMetric: 21.0853, val_loss: 21.0647, val_MinusLogProbMetric: 21.0647

Epoch 105: val_loss improved from 21.10631 to 21.06470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 71s - loss: 21.0853 - MinusLogProbMetric: 21.0853 - val_loss: 21.0647 - val_MinusLogProbMetric: 21.0647 - lr: 1.1111e-04 - 71s/epoch - 364ms/step
Epoch 106/1000
2023-10-23 14:49:05.011 
Epoch 106/1000 
	 loss: 20.9521, MinusLogProbMetric: 20.9521, val_loss: 21.1203, val_MinusLogProbMetric: 21.1203

Epoch 106: val_loss did not improve from 21.06470
196/196 - 66s - loss: 20.9521 - MinusLogProbMetric: 20.9521 - val_loss: 21.1203 - val_MinusLogProbMetric: 21.1203 - lr: 1.1111e-04 - 66s/epoch - 334ms/step
Epoch 107/1000
2023-10-23 14:50:08.296 
Epoch 107/1000 
	 loss: 20.8395, MinusLogProbMetric: 20.8395, val_loss: 20.8192, val_MinusLogProbMetric: 20.8192

Epoch 107: val_loss improved from 21.06470 to 20.81922, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 20.8395 - MinusLogProbMetric: 20.8395 - val_loss: 20.8192 - val_MinusLogProbMetric: 20.8192 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 108/1000
2023-10-23 14:51:18.420 
Epoch 108/1000 
	 loss: 20.7877, MinusLogProbMetric: 20.7877, val_loss: 20.8290, val_MinusLogProbMetric: 20.8290

Epoch 108: val_loss did not improve from 20.81922
196/196 - 69s - loss: 20.7877 - MinusLogProbMetric: 20.7877 - val_loss: 20.8290 - val_MinusLogProbMetric: 20.8290 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 109/1000
2023-10-23 14:52:28.563 
Epoch 109/1000 
	 loss: 20.7593, MinusLogProbMetric: 20.7593, val_loss: 20.9578, val_MinusLogProbMetric: 20.9578

Epoch 109: val_loss did not improve from 20.81922
196/196 - 70s - loss: 20.7593 - MinusLogProbMetric: 20.7593 - val_loss: 20.9578 - val_MinusLogProbMetric: 20.9578 - lr: 1.1111e-04 - 70s/epoch - 358ms/step
Epoch 110/1000
2023-10-23 14:53:39.161 
Epoch 110/1000 
	 loss: 20.6999, MinusLogProbMetric: 20.6999, val_loss: 20.9996, val_MinusLogProbMetric: 20.9996

Epoch 110: val_loss did not improve from 20.81922
196/196 - 71s - loss: 20.6999 - MinusLogProbMetric: 20.6999 - val_loss: 20.9996 - val_MinusLogProbMetric: 20.9996 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 111/1000
2023-10-23 14:54:41.379 
Epoch 111/1000 
	 loss: 20.6425, MinusLogProbMetric: 20.6425, val_loss: 20.9407, val_MinusLogProbMetric: 20.9407

Epoch 111: val_loss did not improve from 20.81922
196/196 - 62s - loss: 20.6425 - MinusLogProbMetric: 20.6425 - val_loss: 20.9407 - val_MinusLogProbMetric: 20.9407 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 112/1000
2023-10-23 14:55:43.312 
Epoch 112/1000 
	 loss: 21.3472, MinusLogProbMetric: 21.3472, val_loss: 20.8148, val_MinusLogProbMetric: 20.8148

Epoch 112: val_loss improved from 20.81922 to 20.81484, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 21.3472 - MinusLogProbMetric: 21.3472 - val_loss: 20.8148 - val_MinusLogProbMetric: 20.8148 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 113/1000
2023-10-23 14:56:50.881 
Epoch 113/1000 
	 loss: 22.9907, MinusLogProbMetric: 22.9907, val_loss: 26.1013, val_MinusLogProbMetric: 26.1013

Epoch 113: val_loss did not improve from 20.81484
196/196 - 66s - loss: 22.9907 - MinusLogProbMetric: 22.9907 - val_loss: 26.1013 - val_MinusLogProbMetric: 26.1013 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 114/1000
2023-10-23 14:57:56.087 
Epoch 114/1000 
	 loss: 21.8163, MinusLogProbMetric: 21.8163, val_loss: 21.1502, val_MinusLogProbMetric: 21.1502

Epoch 114: val_loss did not improve from 20.81484
196/196 - 65s - loss: 21.8163 - MinusLogProbMetric: 21.8163 - val_loss: 21.1502 - val_MinusLogProbMetric: 21.1502 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 115/1000
2023-10-23 14:59:01.034 
Epoch 115/1000 
	 loss: 20.7500, MinusLogProbMetric: 20.7500, val_loss: 20.7523, val_MinusLogProbMetric: 20.7523

Epoch 115: val_loss improved from 20.81484 to 20.75228, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 66s - loss: 20.7500 - MinusLogProbMetric: 20.7500 - val_loss: 20.7523 - val_MinusLogProbMetric: 20.7523 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 116/1000
2023-10-23 15:00:04.250 
Epoch 116/1000 
	 loss: 20.6374, MinusLogProbMetric: 20.6374, val_loss: 20.5976, val_MinusLogProbMetric: 20.5976

Epoch 116: val_loss improved from 20.75228 to 20.59755, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 20.6374 - MinusLogProbMetric: 20.6374 - val_loss: 20.5976 - val_MinusLogProbMetric: 20.5976 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 117/1000
2023-10-23 15:01:11.388 
Epoch 117/1000 
	 loss: 20.5732, MinusLogProbMetric: 20.5732, val_loss: 20.6606, val_MinusLogProbMetric: 20.6606

Epoch 117: val_loss did not improve from 20.59755
196/196 - 66s - loss: 20.5732 - MinusLogProbMetric: 20.5732 - val_loss: 20.6606 - val_MinusLogProbMetric: 20.6606 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 118/1000
2023-10-23 15:02:15.098 
Epoch 118/1000 
	 loss: 20.4495, MinusLogProbMetric: 20.4495, val_loss: 21.3207, val_MinusLogProbMetric: 21.3207

Epoch 118: val_loss did not improve from 20.59755
196/196 - 64s - loss: 20.4495 - MinusLogProbMetric: 20.4495 - val_loss: 21.3207 - val_MinusLogProbMetric: 21.3207 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 119/1000
2023-10-23 15:03:22.815 
Epoch 119/1000 
	 loss: 20.4368, MinusLogProbMetric: 20.4368, val_loss: 20.6191, val_MinusLogProbMetric: 20.6191

Epoch 119: val_loss did not improve from 20.59755
196/196 - 68s - loss: 20.4368 - MinusLogProbMetric: 20.4368 - val_loss: 20.6191 - val_MinusLogProbMetric: 20.6191 - lr: 1.1111e-04 - 68s/epoch - 345ms/step
Epoch 120/1000
2023-10-23 15:04:27.321 
Epoch 120/1000 
	 loss: 20.3755, MinusLogProbMetric: 20.3755, val_loss: 20.3196, val_MinusLogProbMetric: 20.3196

Epoch 120: val_loss improved from 20.59755 to 20.31958, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 66s - loss: 20.3755 - MinusLogProbMetric: 20.3755 - val_loss: 20.3196 - val_MinusLogProbMetric: 20.3196 - lr: 1.1111e-04 - 66s/epoch - 334ms/step
Epoch 121/1000
2023-10-23 15:05:30.856 
Epoch 121/1000 
	 loss: 20.3390, MinusLogProbMetric: 20.3390, val_loss: 20.3388, val_MinusLogProbMetric: 20.3388

Epoch 121: val_loss did not improve from 20.31958
196/196 - 62s - loss: 20.3390 - MinusLogProbMetric: 20.3390 - val_loss: 20.3388 - val_MinusLogProbMetric: 20.3388 - lr: 1.1111e-04 - 62s/epoch - 319ms/step
Epoch 122/1000
2023-10-23 15:06:35.942 
Epoch 122/1000 
	 loss: 20.3246, MinusLogProbMetric: 20.3246, val_loss: 20.2769, val_MinusLogProbMetric: 20.2769

Epoch 122: val_loss improved from 20.31958 to 20.27692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 66s - loss: 20.3246 - MinusLogProbMetric: 20.3246 - val_loss: 20.2769 - val_MinusLogProbMetric: 20.2769 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 123/1000
2023-10-23 15:07:40.083 
Epoch 123/1000 
	 loss: 20.2315, MinusLogProbMetric: 20.2315, val_loss: 20.1203, val_MinusLogProbMetric: 20.1203

Epoch 123: val_loss improved from 20.27692 to 20.12033, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 20.2315 - MinusLogProbMetric: 20.2315 - val_loss: 20.1203 - val_MinusLogProbMetric: 20.1203 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 124/1000
2023-10-23 15:08:44.732 
Epoch 124/1000 
	 loss: 20.1937, MinusLogProbMetric: 20.1937, val_loss: 20.2272, val_MinusLogProbMetric: 20.2272

Epoch 124: val_loss did not improve from 20.12033
196/196 - 64s - loss: 20.1937 - MinusLogProbMetric: 20.1937 - val_loss: 20.2272 - val_MinusLogProbMetric: 20.2272 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 125/1000
2023-10-23 15:09:51.373 
Epoch 125/1000 
	 loss: 20.1654, MinusLogProbMetric: 20.1654, val_loss: 20.1681, val_MinusLogProbMetric: 20.1681

Epoch 125: val_loss did not improve from 20.12033
196/196 - 67s - loss: 20.1654 - MinusLogProbMetric: 20.1654 - val_loss: 20.1681 - val_MinusLogProbMetric: 20.1681 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 126/1000
2023-10-23 15:10:57.338 
Epoch 126/1000 
	 loss: 20.0402, MinusLogProbMetric: 20.0402, val_loss: 20.7960, val_MinusLogProbMetric: 20.7960

Epoch 126: val_loss did not improve from 20.12033
196/196 - 66s - loss: 20.0402 - MinusLogProbMetric: 20.0402 - val_loss: 20.7960 - val_MinusLogProbMetric: 20.7960 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 127/1000
2023-10-23 15:12:02.204 
Epoch 127/1000 
	 loss: 20.1111, MinusLogProbMetric: 20.1111, val_loss: 19.9565, val_MinusLogProbMetric: 19.9565

Epoch 127: val_loss improved from 20.12033 to 19.95653, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 66s - loss: 20.1111 - MinusLogProbMetric: 20.1111 - val_loss: 19.9565 - val_MinusLogProbMetric: 19.9565 - lr: 1.1111e-04 - 66s/epoch - 336ms/step
Epoch 128/1000
2023-10-23 15:13:07.619 
Epoch 128/1000 
	 loss: 20.0968, MinusLogProbMetric: 20.0968, val_loss: 20.1980, val_MinusLogProbMetric: 20.1980

Epoch 128: val_loss did not improve from 19.95653
196/196 - 64s - loss: 20.0968 - MinusLogProbMetric: 20.0968 - val_loss: 20.1980 - val_MinusLogProbMetric: 20.1980 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 129/1000
2023-10-23 15:14:14.575 
Epoch 129/1000 
	 loss: 19.9924, MinusLogProbMetric: 19.9924, val_loss: 19.9364, val_MinusLogProbMetric: 19.9364

Epoch 129: val_loss improved from 19.95653 to 19.93636, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 19.9924 - MinusLogProbMetric: 19.9924 - val_loss: 19.9364 - val_MinusLogProbMetric: 19.9364 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 130/1000
2023-10-23 15:15:21.373 
Epoch 130/1000 
	 loss: 20.0500, MinusLogProbMetric: 20.0500, val_loss: 20.0080, val_MinusLogProbMetric: 20.0080

Epoch 130: val_loss did not improve from 19.93636
196/196 - 66s - loss: 20.0500 - MinusLogProbMetric: 20.0500 - val_loss: 20.0080 - val_MinusLogProbMetric: 20.0080 - lr: 1.1111e-04 - 66s/epoch - 336ms/step
Epoch 131/1000
2023-10-23 15:16:24.916 
Epoch 131/1000 
	 loss: 19.9154, MinusLogProbMetric: 19.9154, val_loss: 19.9224, val_MinusLogProbMetric: 19.9224

Epoch 131: val_loss improved from 19.93636 to 19.92239, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 19.9154 - MinusLogProbMetric: 19.9154 - val_loss: 19.9224 - val_MinusLogProbMetric: 19.9224 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 132/1000
2023-10-23 15:17:29.251 
Epoch 132/1000 
	 loss: 19.9321, MinusLogProbMetric: 19.9321, val_loss: 19.9510, val_MinusLogProbMetric: 19.9510

Epoch 132: val_loss did not improve from 19.92239
196/196 - 63s - loss: 19.9321 - MinusLogProbMetric: 19.9321 - val_loss: 19.9510 - val_MinusLogProbMetric: 19.9510 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 133/1000
2023-10-23 15:18:36.498 
Epoch 133/1000 
	 loss: 19.9382, MinusLogProbMetric: 19.9382, val_loss: 19.8873, val_MinusLogProbMetric: 19.8873

Epoch 133: val_loss improved from 19.92239 to 19.88729, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 19.9382 - MinusLogProbMetric: 19.9382 - val_loss: 19.8873 - val_MinusLogProbMetric: 19.8873 - lr: 1.1111e-04 - 68s/epoch - 349ms/step
Epoch 134/1000
2023-10-23 15:19:42.356 
Epoch 134/1000 
	 loss: 19.8191, MinusLogProbMetric: 19.8191, val_loss: 19.9879, val_MinusLogProbMetric: 19.9879

Epoch 134: val_loss did not improve from 19.88729
196/196 - 65s - loss: 19.8191 - MinusLogProbMetric: 19.8191 - val_loss: 19.9879 - val_MinusLogProbMetric: 19.9879 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 135/1000
2023-10-23 15:20:45.408 
Epoch 135/1000 
	 loss: 19.8461, MinusLogProbMetric: 19.8461, val_loss: 20.0342, val_MinusLogProbMetric: 20.0342

Epoch 135: val_loss did not improve from 19.88729
196/196 - 63s - loss: 19.8461 - MinusLogProbMetric: 19.8461 - val_loss: 20.0342 - val_MinusLogProbMetric: 20.0342 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 136/1000
2023-10-23 15:21:51.941 
Epoch 136/1000 
	 loss: 19.9927, MinusLogProbMetric: 19.9927, val_loss: 20.1255, val_MinusLogProbMetric: 20.1255

Epoch 136: val_loss did not improve from 19.88729
196/196 - 67s - loss: 19.9927 - MinusLogProbMetric: 19.9927 - val_loss: 20.1255 - val_MinusLogProbMetric: 20.1255 - lr: 1.1111e-04 - 67s/epoch - 339ms/step
Epoch 137/1000
2023-10-23 15:22:58.947 
Epoch 137/1000 
	 loss: 19.8379, MinusLogProbMetric: 19.8379, val_loss: 19.8361, val_MinusLogProbMetric: 19.8361

Epoch 137: val_loss improved from 19.88729 to 19.83614, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 19.8379 - MinusLogProbMetric: 19.8379 - val_loss: 19.8361 - val_MinusLogProbMetric: 19.8361 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 138/1000
2023-10-23 15:24:06.606 
Epoch 138/1000 
	 loss: 19.7870, MinusLogProbMetric: 19.7870, val_loss: 19.8389, val_MinusLogProbMetric: 19.8389

Epoch 138: val_loss did not improve from 19.83614
196/196 - 67s - loss: 19.7870 - MinusLogProbMetric: 19.7870 - val_loss: 19.8389 - val_MinusLogProbMetric: 19.8389 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 139/1000
2023-10-23 15:25:11.911 
Epoch 139/1000 
	 loss: 19.8243, MinusLogProbMetric: 19.8243, val_loss: 20.0775, val_MinusLogProbMetric: 20.0775

Epoch 139: val_loss did not improve from 19.83614
196/196 - 65s - loss: 19.8243 - MinusLogProbMetric: 19.8243 - val_loss: 20.0775 - val_MinusLogProbMetric: 20.0775 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 140/1000
2023-10-23 15:26:17.598 
Epoch 140/1000 
	 loss: 19.6880, MinusLogProbMetric: 19.6880, val_loss: 19.6484, val_MinusLogProbMetric: 19.6484

Epoch 140: val_loss improved from 19.83614 to 19.64843, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 19.6880 - MinusLogProbMetric: 19.6880 - val_loss: 19.6484 - val_MinusLogProbMetric: 19.6484 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 141/1000
2023-10-23 15:27:24.533 
Epoch 141/1000 
	 loss: 19.6452, MinusLogProbMetric: 19.6452, val_loss: 19.6008, val_MinusLogProbMetric: 19.6008

Epoch 141: val_loss improved from 19.64843 to 19.60077, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 19.6452 - MinusLogProbMetric: 19.6452 - val_loss: 19.6008 - val_MinusLogProbMetric: 19.6008 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 142/1000
2023-10-23 15:28:31.784 
Epoch 142/1000 
	 loss: 19.6810, MinusLogProbMetric: 19.6810, val_loss: 19.7794, val_MinusLogProbMetric: 19.7794

Epoch 142: val_loss did not improve from 19.60077
196/196 - 66s - loss: 19.6810 - MinusLogProbMetric: 19.6810 - val_loss: 19.7794 - val_MinusLogProbMetric: 19.7794 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 143/1000
2023-10-23 15:29:38.095 
Epoch 143/1000 
	 loss: 19.6569, MinusLogProbMetric: 19.6569, val_loss: 19.6832, val_MinusLogProbMetric: 19.6832

Epoch 143: val_loss did not improve from 19.60077
196/196 - 66s - loss: 19.6569 - MinusLogProbMetric: 19.6569 - val_loss: 19.6832 - val_MinusLogProbMetric: 19.6832 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 144/1000
2023-10-23 15:30:39.821 
Epoch 144/1000 
	 loss: 19.6233, MinusLogProbMetric: 19.6233, val_loss: 19.6680, val_MinusLogProbMetric: 19.6680

Epoch 144: val_loss did not improve from 19.60077
196/196 - 62s - loss: 19.6233 - MinusLogProbMetric: 19.6233 - val_loss: 19.6680 - val_MinusLogProbMetric: 19.6680 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 145/1000
2023-10-23 15:31:43.446 
Epoch 145/1000 
	 loss: 19.7055, MinusLogProbMetric: 19.7055, val_loss: 19.5705, val_MinusLogProbMetric: 19.5705

Epoch 145: val_loss improved from 19.60077 to 19.57051, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 19.7055 - MinusLogProbMetric: 19.7055 - val_loss: 19.5705 - val_MinusLogProbMetric: 19.5705 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 146/1000
2023-10-23 15:32:48.393 
Epoch 146/1000 
	 loss: 19.5239, MinusLogProbMetric: 19.5239, val_loss: 19.5193, val_MinusLogProbMetric: 19.5193

Epoch 146: val_loss improved from 19.57051 to 19.51931, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 19.5239 - MinusLogProbMetric: 19.5239 - val_loss: 19.5193 - val_MinusLogProbMetric: 19.5193 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 147/1000
2023-10-23 15:33:51.916 
Epoch 147/1000 
	 loss: 19.5278, MinusLogProbMetric: 19.5278, val_loss: 19.3430, val_MinusLogProbMetric: 19.3430

Epoch 147: val_loss improved from 19.51931 to 19.34295, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 19.5278 - MinusLogProbMetric: 19.5278 - val_loss: 19.3430 - val_MinusLogProbMetric: 19.3430 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 148/1000
2023-10-23 15:34:54.967 
Epoch 148/1000 
	 loss: 19.4808, MinusLogProbMetric: 19.4808, val_loss: 20.0858, val_MinusLogProbMetric: 20.0858

Epoch 148: val_loss did not improve from 19.34295
196/196 - 62s - loss: 19.4808 - MinusLogProbMetric: 19.4808 - val_loss: 20.0858 - val_MinusLogProbMetric: 20.0858 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 149/1000
2023-10-23 15:35:55.637 
Epoch 149/1000 
	 loss: 19.5539, MinusLogProbMetric: 19.5539, val_loss: 19.5157, val_MinusLogProbMetric: 19.5157

Epoch 149: val_loss did not improve from 19.34295
196/196 - 61s - loss: 19.5539 - MinusLogProbMetric: 19.5539 - val_loss: 19.5157 - val_MinusLogProbMetric: 19.5157 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 150/1000
2023-10-23 15:37:01.284 
Epoch 150/1000 
	 loss: 19.4488, MinusLogProbMetric: 19.4488, val_loss: 19.4589, val_MinusLogProbMetric: 19.4589

Epoch 150: val_loss did not improve from 19.34295
196/196 - 66s - loss: 19.4488 - MinusLogProbMetric: 19.4488 - val_loss: 19.4589 - val_MinusLogProbMetric: 19.4589 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 151/1000
2023-10-23 15:38:07.805 
Epoch 151/1000 
	 loss: 19.5257, MinusLogProbMetric: 19.5257, val_loss: 19.4087, val_MinusLogProbMetric: 19.4087

Epoch 151: val_loss did not improve from 19.34295
196/196 - 67s - loss: 19.5257 - MinusLogProbMetric: 19.5257 - val_loss: 19.4087 - val_MinusLogProbMetric: 19.4087 - lr: 1.1111e-04 - 67s/epoch - 339ms/step
Epoch 152/1000
2023-10-23 15:39:08.275 
Epoch 152/1000 
	 loss: 19.5040, MinusLogProbMetric: 19.5040, val_loss: 19.4596, val_MinusLogProbMetric: 19.4596

Epoch 152: val_loss did not improve from 19.34295
196/196 - 60s - loss: 19.5040 - MinusLogProbMetric: 19.5040 - val_loss: 19.4596 - val_MinusLogProbMetric: 19.4596 - lr: 1.1111e-04 - 60s/epoch - 309ms/step
Epoch 153/1000
2023-10-23 15:40:09.175 
Epoch 153/1000 
	 loss: 19.4953, MinusLogProbMetric: 19.4953, val_loss: 19.4346, val_MinusLogProbMetric: 19.4346

Epoch 153: val_loss did not improve from 19.34295
196/196 - 61s - loss: 19.4953 - MinusLogProbMetric: 19.4953 - val_loss: 19.4346 - val_MinusLogProbMetric: 19.4346 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 154/1000
2023-10-23 15:41:11.550 
Epoch 154/1000 
	 loss: 19.3371, MinusLogProbMetric: 19.3371, val_loss: 19.4184, val_MinusLogProbMetric: 19.4184

Epoch 154: val_loss did not improve from 19.34295
196/196 - 62s - loss: 19.3371 - MinusLogProbMetric: 19.3371 - val_loss: 19.4184 - val_MinusLogProbMetric: 19.4184 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 155/1000
2023-10-23 15:42:14.311 
Epoch 155/1000 
	 loss: 19.3768, MinusLogProbMetric: 19.3768, val_loss: 19.6117, val_MinusLogProbMetric: 19.6117

Epoch 155: val_loss did not improve from 19.34295
196/196 - 63s - loss: 19.3768 - MinusLogProbMetric: 19.3768 - val_loss: 19.6117 - val_MinusLogProbMetric: 19.6117 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 156/1000
2023-10-23 15:43:16.433 
Epoch 156/1000 
	 loss: 19.3187, MinusLogProbMetric: 19.3187, val_loss: 19.5250, val_MinusLogProbMetric: 19.5250

Epoch 156: val_loss did not improve from 19.34295
196/196 - 62s - loss: 19.3187 - MinusLogProbMetric: 19.3187 - val_loss: 19.5250 - val_MinusLogProbMetric: 19.5250 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 157/1000
2023-10-23 15:44:20.376 
Epoch 157/1000 
	 loss: 19.3771, MinusLogProbMetric: 19.3771, val_loss: 19.3554, val_MinusLogProbMetric: 19.3554

Epoch 157: val_loss did not improve from 19.34295
196/196 - 64s - loss: 19.3771 - MinusLogProbMetric: 19.3771 - val_loss: 19.3554 - val_MinusLogProbMetric: 19.3554 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 158/1000
2023-10-23 15:45:22.262 
Epoch 158/1000 
	 loss: 19.3316, MinusLogProbMetric: 19.3316, val_loss: 19.3504, val_MinusLogProbMetric: 19.3504

Epoch 158: val_loss did not improve from 19.34295
196/196 - 62s - loss: 19.3316 - MinusLogProbMetric: 19.3316 - val_loss: 19.3504 - val_MinusLogProbMetric: 19.3504 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 159/1000
2023-10-23 15:46:24.311 
Epoch 159/1000 
	 loss: 19.3085, MinusLogProbMetric: 19.3085, val_loss: 19.7607, val_MinusLogProbMetric: 19.7607

Epoch 159: val_loss did not improve from 19.34295
196/196 - 62s - loss: 19.3085 - MinusLogProbMetric: 19.3085 - val_loss: 19.7607 - val_MinusLogProbMetric: 19.7607 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 160/1000
2023-10-23 15:47:26.605 
Epoch 160/1000 
	 loss: 19.2178, MinusLogProbMetric: 19.2178, val_loss: 19.1852, val_MinusLogProbMetric: 19.1852

Epoch 160: val_loss improved from 19.34295 to 19.18520, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 19.2178 - MinusLogProbMetric: 19.2178 - val_loss: 19.1852 - val_MinusLogProbMetric: 19.1852 - lr: 1.1111e-04 - 63s/epoch - 324ms/step
Epoch 161/1000
2023-10-23 15:48:31.112 
Epoch 161/1000 
	 loss: 19.3269, MinusLogProbMetric: 19.3269, val_loss: 19.1918, val_MinusLogProbMetric: 19.1918

Epoch 161: val_loss did not improve from 19.18520
196/196 - 63s - loss: 19.3269 - MinusLogProbMetric: 19.3269 - val_loss: 19.1918 - val_MinusLogProbMetric: 19.1918 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 162/1000
2023-10-23 15:49:32.877 
Epoch 162/1000 
	 loss: 19.2018, MinusLogProbMetric: 19.2018, val_loss: 19.3298, val_MinusLogProbMetric: 19.3298

Epoch 162: val_loss did not improve from 19.18520
196/196 - 62s - loss: 19.2018 - MinusLogProbMetric: 19.2018 - val_loss: 19.3298 - val_MinusLogProbMetric: 19.3298 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 163/1000
2023-10-23 15:50:35.244 
Epoch 163/1000 
	 loss: 19.2070, MinusLogProbMetric: 19.2070, val_loss: 19.1701, val_MinusLogProbMetric: 19.1701

Epoch 163: val_loss improved from 19.18520 to 19.17012, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 19.2070 - MinusLogProbMetric: 19.2070 - val_loss: 19.1701 - val_MinusLogProbMetric: 19.1701 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 164/1000
2023-10-23 15:51:40.667 
Epoch 164/1000 
	 loss: 19.2494, MinusLogProbMetric: 19.2494, val_loss: 19.4403, val_MinusLogProbMetric: 19.4403

Epoch 164: val_loss did not improve from 19.17012
196/196 - 64s - loss: 19.2494 - MinusLogProbMetric: 19.2494 - val_loss: 19.4403 - val_MinusLogProbMetric: 19.4403 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 165/1000
2023-10-23 15:52:45.860 
Epoch 165/1000 
	 loss: 19.2760, MinusLogProbMetric: 19.2760, val_loss: 19.2694, val_MinusLogProbMetric: 19.2694

Epoch 165: val_loss did not improve from 19.17012
196/196 - 65s - loss: 19.2760 - MinusLogProbMetric: 19.2760 - val_loss: 19.2694 - val_MinusLogProbMetric: 19.2694 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 166/1000
2023-10-23 15:53:50.854 
Epoch 166/1000 
	 loss: 19.2258, MinusLogProbMetric: 19.2258, val_loss: 19.0529, val_MinusLogProbMetric: 19.0529

Epoch 166: val_loss improved from 19.17012 to 19.05292, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 66s - loss: 19.2258 - MinusLogProbMetric: 19.2258 - val_loss: 19.0529 - val_MinusLogProbMetric: 19.0529 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 167/1000
2023-10-23 15:54:56.084 
Epoch 167/1000 
	 loss: 19.1060, MinusLogProbMetric: 19.1060, val_loss: 19.2875, val_MinusLogProbMetric: 19.2875

Epoch 167: val_loss did not improve from 19.05292
196/196 - 64s - loss: 19.1060 - MinusLogProbMetric: 19.1060 - val_loss: 19.2875 - val_MinusLogProbMetric: 19.2875 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 168/1000
2023-10-23 15:56:01.681 
Epoch 168/1000 
	 loss: 19.1605, MinusLogProbMetric: 19.1605, val_loss: 19.7262, val_MinusLogProbMetric: 19.7262

Epoch 168: val_loss did not improve from 19.05292
196/196 - 66s - loss: 19.1605 - MinusLogProbMetric: 19.1605 - val_loss: 19.7262 - val_MinusLogProbMetric: 19.7262 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 169/1000
2023-10-23 15:57:05.699 
Epoch 169/1000 
	 loss: 19.2147, MinusLogProbMetric: 19.2147, val_loss: 19.0405, val_MinusLogProbMetric: 19.0405

Epoch 169: val_loss improved from 19.05292 to 19.04048, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 19.2147 - MinusLogProbMetric: 19.2147 - val_loss: 19.0405 - val_MinusLogProbMetric: 19.0405 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 170/1000
2023-10-23 15:58:13.828 
Epoch 170/1000 
	 loss: 19.0797, MinusLogProbMetric: 19.0797, val_loss: 19.3349, val_MinusLogProbMetric: 19.3349

Epoch 170: val_loss did not improve from 19.04048
196/196 - 67s - loss: 19.0797 - MinusLogProbMetric: 19.0797 - val_loss: 19.3349 - val_MinusLogProbMetric: 19.3349 - lr: 1.1111e-04 - 67s/epoch - 341ms/step
Epoch 171/1000
2023-10-23 15:59:17.884 
Epoch 171/1000 
	 loss: 19.1278, MinusLogProbMetric: 19.1278, val_loss: 19.3063, val_MinusLogProbMetric: 19.3063

Epoch 171: val_loss did not improve from 19.04048
196/196 - 64s - loss: 19.1278 - MinusLogProbMetric: 19.1278 - val_loss: 19.3063 - val_MinusLogProbMetric: 19.3063 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 172/1000
2023-10-23 16:00:26.088 
Epoch 172/1000 
	 loss: 19.1572, MinusLogProbMetric: 19.1572, val_loss: 19.4044, val_MinusLogProbMetric: 19.4044

Epoch 172: val_loss did not improve from 19.04048
196/196 - 68s - loss: 19.1572 - MinusLogProbMetric: 19.1572 - val_loss: 19.4044 - val_MinusLogProbMetric: 19.4044 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 173/1000
2023-10-23 16:01:28.462 
Epoch 173/1000 
	 loss: 19.0353, MinusLogProbMetric: 19.0353, val_loss: 19.6145, val_MinusLogProbMetric: 19.6145

Epoch 173: val_loss did not improve from 19.04048
196/196 - 62s - loss: 19.0353 - MinusLogProbMetric: 19.0353 - val_loss: 19.6145 - val_MinusLogProbMetric: 19.6145 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 174/1000
2023-10-23 16:02:35.754 
Epoch 174/1000 
	 loss: 19.0417, MinusLogProbMetric: 19.0417, val_loss: 19.2991, val_MinusLogProbMetric: 19.2991

Epoch 174: val_loss did not improve from 19.04048
196/196 - 67s - loss: 19.0417 - MinusLogProbMetric: 19.0417 - val_loss: 19.2991 - val_MinusLogProbMetric: 19.2991 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 175/1000
2023-10-23 16:03:38.238 
Epoch 175/1000 
	 loss: 19.0375, MinusLogProbMetric: 19.0375, val_loss: 18.9202, val_MinusLogProbMetric: 18.9202

Epoch 175: val_loss improved from 19.04048 to 18.92021, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 19.0375 - MinusLogProbMetric: 19.0375 - val_loss: 18.9202 - val_MinusLogProbMetric: 18.9202 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 176/1000
2023-10-23 16:04:47.369 
Epoch 176/1000 
	 loss: 19.1117, MinusLogProbMetric: 19.1117, val_loss: 19.2156, val_MinusLogProbMetric: 19.2156

Epoch 176: val_loss did not improve from 18.92021
196/196 - 68s - loss: 19.1117 - MinusLogProbMetric: 19.1117 - val_loss: 19.2156 - val_MinusLogProbMetric: 19.2156 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 177/1000
2023-10-23 16:05:49.636 
Epoch 177/1000 
	 loss: 19.0192, MinusLogProbMetric: 19.0192, val_loss: 19.4632, val_MinusLogProbMetric: 19.4632

Epoch 177: val_loss did not improve from 18.92021
196/196 - 62s - loss: 19.0192 - MinusLogProbMetric: 19.0192 - val_loss: 19.4632 - val_MinusLogProbMetric: 19.4632 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 178/1000
2023-10-23 16:06:55.954 
Epoch 178/1000 
	 loss: 19.0200, MinusLogProbMetric: 19.0200, val_loss: 18.9950, val_MinusLogProbMetric: 18.9950

Epoch 178: val_loss did not improve from 18.92021
196/196 - 66s - loss: 19.0200 - MinusLogProbMetric: 19.0200 - val_loss: 18.9950 - val_MinusLogProbMetric: 18.9950 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 179/1000
2023-10-23 16:08:01.602 
Epoch 179/1000 
	 loss: 18.9722, MinusLogProbMetric: 18.9722, val_loss: 18.7613, val_MinusLogProbMetric: 18.7613

Epoch 179: val_loss improved from 18.92021 to 18.76125, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 18.9722 - MinusLogProbMetric: 18.9722 - val_loss: 18.7613 - val_MinusLogProbMetric: 18.7613 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 180/1000
2023-10-23 16:09:05.391 
Epoch 180/1000 
	 loss: 18.9679, MinusLogProbMetric: 18.9679, val_loss: 18.9651, val_MinusLogProbMetric: 18.9651

Epoch 180: val_loss did not improve from 18.76125
196/196 - 63s - loss: 18.9679 - MinusLogProbMetric: 18.9679 - val_loss: 18.9651 - val_MinusLogProbMetric: 18.9651 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 181/1000
2023-10-23 16:10:08.088 
Epoch 181/1000 
	 loss: 18.9473, MinusLogProbMetric: 18.9473, val_loss: 19.5018, val_MinusLogProbMetric: 19.5018

Epoch 181: val_loss did not improve from 18.76125
196/196 - 63s - loss: 18.9473 - MinusLogProbMetric: 18.9473 - val_loss: 19.5018 - val_MinusLogProbMetric: 19.5018 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 182/1000
2023-10-23 16:11:16.161 
Epoch 182/1000 
	 loss: 18.9128, MinusLogProbMetric: 18.9128, val_loss: 18.9545, val_MinusLogProbMetric: 18.9545

Epoch 182: val_loss did not improve from 18.76125
196/196 - 68s - loss: 18.9128 - MinusLogProbMetric: 18.9128 - val_loss: 18.9545 - val_MinusLogProbMetric: 18.9545 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 183/1000
2023-10-23 16:12:20.512 
Epoch 183/1000 
	 loss: 19.3144, MinusLogProbMetric: 19.3144, val_loss: 19.3216, val_MinusLogProbMetric: 19.3216

Epoch 183: val_loss did not improve from 18.76125
196/196 - 64s - loss: 19.3144 - MinusLogProbMetric: 19.3144 - val_loss: 19.3216 - val_MinusLogProbMetric: 19.3216 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 184/1000
2023-10-23 16:13:26.056 
Epoch 184/1000 
	 loss: 18.9103, MinusLogProbMetric: 18.9103, val_loss: 19.0927, val_MinusLogProbMetric: 19.0927

Epoch 184: val_loss did not improve from 18.76125
196/196 - 66s - loss: 18.9103 - MinusLogProbMetric: 18.9103 - val_loss: 19.0927 - val_MinusLogProbMetric: 19.0927 - lr: 1.1111e-04 - 66s/epoch - 334ms/step
Epoch 185/1000
2023-10-23 16:14:30.864 
Epoch 185/1000 
	 loss: 18.9649, MinusLogProbMetric: 18.9649, val_loss: 19.7267, val_MinusLogProbMetric: 19.7267

Epoch 185: val_loss did not improve from 18.76125
196/196 - 65s - loss: 18.9649 - MinusLogProbMetric: 18.9649 - val_loss: 19.7267 - val_MinusLogProbMetric: 19.7267 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 186/1000
2023-10-23 16:15:34.432 
Epoch 186/1000 
	 loss: 18.9924, MinusLogProbMetric: 18.9924, val_loss: 19.0662, val_MinusLogProbMetric: 19.0662

Epoch 186: val_loss did not improve from 18.76125
196/196 - 64s - loss: 18.9924 - MinusLogProbMetric: 18.9924 - val_loss: 19.0662 - val_MinusLogProbMetric: 19.0662 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 187/1000
2023-10-23 16:16:42.047 
Epoch 187/1000 
	 loss: 18.8313, MinusLogProbMetric: 18.8313, val_loss: 18.9902, val_MinusLogProbMetric: 18.9902

Epoch 187: val_loss did not improve from 18.76125
196/196 - 68s - loss: 18.8313 - MinusLogProbMetric: 18.8313 - val_loss: 18.9902 - val_MinusLogProbMetric: 18.9902 - lr: 1.1111e-04 - 68s/epoch - 345ms/step
Epoch 188/1000
2023-10-23 16:17:48.704 
Epoch 188/1000 
	 loss: 18.8383, MinusLogProbMetric: 18.8383, val_loss: 19.5847, val_MinusLogProbMetric: 19.5847

Epoch 188: val_loss did not improve from 18.76125
196/196 - 67s - loss: 18.8383 - MinusLogProbMetric: 18.8383 - val_loss: 19.5847 - val_MinusLogProbMetric: 19.5847 - lr: 1.1111e-04 - 67s/epoch - 340ms/step
Epoch 189/1000
2023-10-23 16:18:53.697 
Epoch 189/1000 
	 loss: 18.8284, MinusLogProbMetric: 18.8284, val_loss: 18.9905, val_MinusLogProbMetric: 18.9905

Epoch 189: val_loss did not improve from 18.76125
196/196 - 65s - loss: 18.8284 - MinusLogProbMetric: 18.8284 - val_loss: 18.9905 - val_MinusLogProbMetric: 18.9905 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 190/1000
2023-10-23 16:20:00.572 
Epoch 190/1000 
	 loss: 18.8938, MinusLogProbMetric: 18.8938, val_loss: 19.1855, val_MinusLogProbMetric: 19.1855

Epoch 190: val_loss did not improve from 18.76125
196/196 - 67s - loss: 18.8938 - MinusLogProbMetric: 18.8938 - val_loss: 19.1855 - val_MinusLogProbMetric: 19.1855 - lr: 1.1111e-04 - 67s/epoch - 341ms/step
Epoch 191/1000
2023-10-23 16:21:03.938 
Epoch 191/1000 
	 loss: 18.8142, MinusLogProbMetric: 18.8142, val_loss: 18.7964, val_MinusLogProbMetric: 18.7964

Epoch 191: val_loss did not improve from 18.76125
196/196 - 63s - loss: 18.8142 - MinusLogProbMetric: 18.8142 - val_loss: 18.7964 - val_MinusLogProbMetric: 18.7964 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 192/1000
2023-10-23 16:22:09.475 
Epoch 192/1000 
	 loss: 19.0815, MinusLogProbMetric: 19.0815, val_loss: 18.8134, val_MinusLogProbMetric: 18.8134

Epoch 192: val_loss did not improve from 18.76125
196/196 - 66s - loss: 19.0815 - MinusLogProbMetric: 19.0815 - val_loss: 18.8134 - val_MinusLogProbMetric: 18.8134 - lr: 1.1111e-04 - 66s/epoch - 334ms/step
Epoch 193/1000
2023-10-23 16:23:14.224 
Epoch 193/1000 
	 loss: 18.8264, MinusLogProbMetric: 18.8264, val_loss: 19.5416, val_MinusLogProbMetric: 19.5416

Epoch 193: val_loss did not improve from 18.76125
196/196 - 65s - loss: 18.8264 - MinusLogProbMetric: 18.8264 - val_loss: 19.5416 - val_MinusLogProbMetric: 19.5416 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 194/1000
2023-10-23 16:24:20.439 
Epoch 194/1000 
	 loss: 18.9100, MinusLogProbMetric: 18.9100, val_loss: 18.8876, val_MinusLogProbMetric: 18.8876

Epoch 194: val_loss did not improve from 18.76125
196/196 - 66s - loss: 18.9100 - MinusLogProbMetric: 18.9100 - val_loss: 18.8876 - val_MinusLogProbMetric: 18.8876 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 195/1000
2023-10-23 16:25:23.368 
Epoch 195/1000 
	 loss: 18.7488, MinusLogProbMetric: 18.7488, val_loss: 18.8213, val_MinusLogProbMetric: 18.8213

Epoch 195: val_loss did not improve from 18.76125
196/196 - 63s - loss: 18.7488 - MinusLogProbMetric: 18.7488 - val_loss: 18.8213 - val_MinusLogProbMetric: 18.8213 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 196/1000
2023-10-23 16:26:28.714 
Epoch 196/1000 
	 loss: 18.9332, MinusLogProbMetric: 18.9332, val_loss: 18.9639, val_MinusLogProbMetric: 18.9639

Epoch 196: val_loss did not improve from 18.76125
196/196 - 65s - loss: 18.9332 - MinusLogProbMetric: 18.9332 - val_loss: 18.9639 - val_MinusLogProbMetric: 18.9639 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 197/1000
2023-10-23 16:27:36.252 
Epoch 197/1000 
	 loss: 18.9561, MinusLogProbMetric: 18.9561, val_loss: 18.6942, val_MinusLogProbMetric: 18.6942

Epoch 197: val_loss improved from 18.76125 to 18.69418, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 69s - loss: 18.9561 - MinusLogProbMetric: 18.9561 - val_loss: 18.6942 - val_MinusLogProbMetric: 18.6942 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 198/1000
2023-10-23 16:28:41.893 
Epoch 198/1000 
	 loss: 18.7748, MinusLogProbMetric: 18.7748, val_loss: 19.0214, val_MinusLogProbMetric: 19.0214

Epoch 198: val_loss did not improve from 18.69418
196/196 - 65s - loss: 18.7748 - MinusLogProbMetric: 18.7748 - val_loss: 19.0214 - val_MinusLogProbMetric: 19.0214 - lr: 1.1111e-04 - 65s/epoch - 329ms/step
Epoch 199/1000
2023-10-23 16:29:46.574 
Epoch 199/1000 
	 loss: 18.7729, MinusLogProbMetric: 18.7729, val_loss: 18.9312, val_MinusLogProbMetric: 18.9312

Epoch 199: val_loss did not improve from 18.69418
196/196 - 65s - loss: 18.7729 - MinusLogProbMetric: 18.7729 - val_loss: 18.9312 - val_MinusLogProbMetric: 18.9312 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 200/1000
2023-10-23 16:30:49.803 
Epoch 200/1000 
	 loss: 18.7386, MinusLogProbMetric: 18.7386, val_loss: 19.1306, val_MinusLogProbMetric: 19.1306

Epoch 200: val_loss did not improve from 18.69418
196/196 - 63s - loss: 18.7386 - MinusLogProbMetric: 18.7386 - val_loss: 19.1306 - val_MinusLogProbMetric: 19.1306 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 201/1000
2023-10-23 16:31:56.327 
Epoch 201/1000 
	 loss: 18.7683, MinusLogProbMetric: 18.7683, val_loss: 18.7528, val_MinusLogProbMetric: 18.7528

Epoch 201: val_loss did not improve from 18.69418
196/196 - 67s - loss: 18.7683 - MinusLogProbMetric: 18.7683 - val_loss: 18.7528 - val_MinusLogProbMetric: 18.7528 - lr: 1.1111e-04 - 67s/epoch - 339ms/step
Epoch 202/1000
2023-10-23 16:33:01.860 
Epoch 202/1000 
	 loss: 18.7647, MinusLogProbMetric: 18.7647, val_loss: 19.0276, val_MinusLogProbMetric: 19.0276

Epoch 202: val_loss did not improve from 18.69418
196/196 - 66s - loss: 18.7647 - MinusLogProbMetric: 18.7647 - val_loss: 19.0276 - val_MinusLogProbMetric: 19.0276 - lr: 1.1111e-04 - 66s/epoch - 334ms/step
Epoch 203/1000
2023-10-23 16:34:06.253 
Epoch 203/1000 
	 loss: 18.7225, MinusLogProbMetric: 18.7225, val_loss: 18.6721, val_MinusLogProbMetric: 18.6721

Epoch 203: val_loss improved from 18.69418 to 18.67208, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 66s - loss: 18.7225 - MinusLogProbMetric: 18.7225 - val_loss: 18.6721 - val_MinusLogProbMetric: 18.6721 - lr: 1.1111e-04 - 66s/epoch - 334ms/step
Epoch 204/1000
2023-10-23 16:35:12.319 
Epoch 204/1000 
	 loss: 18.7108, MinusLogProbMetric: 18.7108, val_loss: 18.9611, val_MinusLogProbMetric: 18.9611

Epoch 204: val_loss did not improve from 18.67208
196/196 - 65s - loss: 18.7108 - MinusLogProbMetric: 18.7108 - val_loss: 18.9611 - val_MinusLogProbMetric: 18.9611 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 205/1000
2023-10-23 16:36:15.545 
Epoch 205/1000 
	 loss: 18.6591, MinusLogProbMetric: 18.6591, val_loss: 18.6861, val_MinusLogProbMetric: 18.6861

Epoch 205: val_loss did not improve from 18.67208
196/196 - 63s - loss: 18.6591 - MinusLogProbMetric: 18.6591 - val_loss: 18.6861 - val_MinusLogProbMetric: 18.6861 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 206/1000
2023-10-23 16:37:17.786 
Epoch 206/1000 
	 loss: 18.7124, MinusLogProbMetric: 18.7124, val_loss: 18.6421, val_MinusLogProbMetric: 18.6421

Epoch 206: val_loss improved from 18.67208 to 18.64209, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 18.7124 - MinusLogProbMetric: 18.7124 - val_loss: 18.6421 - val_MinusLogProbMetric: 18.6421 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 207/1000
2023-10-23 16:38:24.005 
Epoch 207/1000 
	 loss: 18.9073, MinusLogProbMetric: 18.9073, val_loss: 19.1254, val_MinusLogProbMetric: 19.1254

Epoch 207: val_loss did not improve from 18.64209
196/196 - 65s - loss: 18.9073 - MinusLogProbMetric: 18.9073 - val_loss: 19.1254 - val_MinusLogProbMetric: 19.1254 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 208/1000
2023-10-23 16:39:32.602 
Epoch 208/1000 
	 loss: 18.6441, MinusLogProbMetric: 18.6441, val_loss: 18.8413, val_MinusLogProbMetric: 18.8413

Epoch 208: val_loss did not improve from 18.64209
196/196 - 69s - loss: 18.6441 - MinusLogProbMetric: 18.6441 - val_loss: 18.8413 - val_MinusLogProbMetric: 18.8413 - lr: 1.1111e-04 - 69s/epoch - 350ms/step
Epoch 209/1000
2023-10-23 16:40:35.978 
Epoch 209/1000 
	 loss: 18.6351, MinusLogProbMetric: 18.6351, val_loss: 18.6610, val_MinusLogProbMetric: 18.6610

Epoch 209: val_loss did not improve from 18.64209
196/196 - 63s - loss: 18.6351 - MinusLogProbMetric: 18.6351 - val_loss: 18.6610 - val_MinusLogProbMetric: 18.6610 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 210/1000
2023-10-23 16:41:40.406 
Epoch 210/1000 
	 loss: 19.1279, MinusLogProbMetric: 19.1279, val_loss: 18.9754, val_MinusLogProbMetric: 18.9754

Epoch 210: val_loss did not improve from 18.64209
196/196 - 64s - loss: 19.1279 - MinusLogProbMetric: 19.1279 - val_loss: 18.9754 - val_MinusLogProbMetric: 18.9754 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 211/1000
2023-10-23 16:42:47.260 
Epoch 211/1000 
	 loss: 18.6168, MinusLogProbMetric: 18.6168, val_loss: 18.5033, val_MinusLogProbMetric: 18.5033

Epoch 211: val_loss improved from 18.64209 to 18.50327, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 18.6168 - MinusLogProbMetric: 18.6168 - val_loss: 18.5033 - val_MinusLogProbMetric: 18.5033 - lr: 1.1111e-04 - 68s/epoch - 347ms/step
Epoch 212/1000
2023-10-23 16:43:54.796 
Epoch 212/1000 
	 loss: 18.5910, MinusLogProbMetric: 18.5910, val_loss: 18.6616, val_MinusLogProbMetric: 18.6616

Epoch 212: val_loss did not improve from 18.50327
196/196 - 66s - loss: 18.5910 - MinusLogProbMetric: 18.5910 - val_loss: 18.6616 - val_MinusLogProbMetric: 18.6616 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 213/1000
2023-10-23 16:44:58.565 
Epoch 213/1000 
	 loss: 18.6009, MinusLogProbMetric: 18.6009, val_loss: 18.4396, val_MinusLogProbMetric: 18.4396

Epoch 213: val_loss improved from 18.50327 to 18.43961, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 18.6009 - MinusLogProbMetric: 18.6009 - val_loss: 18.4396 - val_MinusLogProbMetric: 18.4396 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 214/1000
2023-10-23 16:46:07.869 
Epoch 214/1000 
	 loss: 18.6383, MinusLogProbMetric: 18.6383, val_loss: 19.0439, val_MinusLogProbMetric: 19.0439

Epoch 214: val_loss did not improve from 18.43961
196/196 - 68s - loss: 18.6383 - MinusLogProbMetric: 18.6383 - val_loss: 19.0439 - val_MinusLogProbMetric: 19.0439 - lr: 1.1111e-04 - 68s/epoch - 348ms/step
Epoch 215/1000
2023-10-23 16:47:11.959 
Epoch 215/1000 
	 loss: 18.6582, MinusLogProbMetric: 18.6582, val_loss: 18.7777, val_MinusLogProbMetric: 18.7777

Epoch 215: val_loss did not improve from 18.43961
196/196 - 64s - loss: 18.6582 - MinusLogProbMetric: 18.6582 - val_loss: 18.7777 - val_MinusLogProbMetric: 18.7777 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 216/1000
2023-10-23 16:48:12.341 
Epoch 216/1000 
	 loss: 18.5623, MinusLogProbMetric: 18.5623, val_loss: 18.5176, val_MinusLogProbMetric: 18.5176

Epoch 216: val_loss did not improve from 18.43961
196/196 - 60s - loss: 18.5623 - MinusLogProbMetric: 18.5623 - val_loss: 18.5176 - val_MinusLogProbMetric: 18.5176 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 217/1000
2023-10-23 16:49:14.700 
Epoch 217/1000 
	 loss: 18.5383, MinusLogProbMetric: 18.5383, val_loss: 18.5181, val_MinusLogProbMetric: 18.5181

Epoch 217: val_loss did not improve from 18.43961
196/196 - 62s - loss: 18.5383 - MinusLogProbMetric: 18.5383 - val_loss: 18.5181 - val_MinusLogProbMetric: 18.5181 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 218/1000
2023-10-23 16:50:17.338 
Epoch 218/1000 
	 loss: 18.6411, MinusLogProbMetric: 18.6411, val_loss: 20.2525, val_MinusLogProbMetric: 20.2525

Epoch 218: val_loss did not improve from 18.43961
196/196 - 63s - loss: 18.6411 - MinusLogProbMetric: 18.6411 - val_loss: 20.2525 - val_MinusLogProbMetric: 20.2525 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 219/1000
2023-10-23 16:51:16.751 
Epoch 219/1000 
	 loss: 18.5746, MinusLogProbMetric: 18.5746, val_loss: 18.4178, val_MinusLogProbMetric: 18.4178

Epoch 219: val_loss improved from 18.43961 to 18.41779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 60s - loss: 18.5746 - MinusLogProbMetric: 18.5746 - val_loss: 18.4178 - val_MinusLogProbMetric: 18.4178 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 220/1000
2023-10-23 16:52:20.144 
Epoch 220/1000 
	 loss: 18.4707, MinusLogProbMetric: 18.4707, val_loss: 18.7970, val_MinusLogProbMetric: 18.7970

Epoch 220: val_loss did not improve from 18.41779
196/196 - 62s - loss: 18.4707 - MinusLogProbMetric: 18.4707 - val_loss: 18.7970 - val_MinusLogProbMetric: 18.7970 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 221/1000
2023-10-23 16:53:22.895 
Epoch 221/1000 
	 loss: 18.8017, MinusLogProbMetric: 18.8017, val_loss: 18.5054, val_MinusLogProbMetric: 18.5054

Epoch 221: val_loss did not improve from 18.41779
196/196 - 63s - loss: 18.8017 - MinusLogProbMetric: 18.8017 - val_loss: 18.5054 - val_MinusLogProbMetric: 18.5054 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 222/1000
2023-10-23 16:54:23.538 
Epoch 222/1000 
	 loss: 18.5755, MinusLogProbMetric: 18.5755, val_loss: 18.4583, val_MinusLogProbMetric: 18.4583

Epoch 222: val_loss did not improve from 18.41779
196/196 - 61s - loss: 18.5755 - MinusLogProbMetric: 18.5755 - val_loss: 18.4583 - val_MinusLogProbMetric: 18.4583 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 223/1000
2023-10-23 16:55:23.490 
Epoch 223/1000 
	 loss: 18.6575, MinusLogProbMetric: 18.6575, val_loss: 18.7027, val_MinusLogProbMetric: 18.7027

Epoch 223: val_loss did not improve from 18.41779
196/196 - 60s - loss: 18.6575 - MinusLogProbMetric: 18.6575 - val_loss: 18.7027 - val_MinusLogProbMetric: 18.7027 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 224/1000
2023-10-23 16:56:24.534 
Epoch 224/1000 
	 loss: 18.5314, MinusLogProbMetric: 18.5314, val_loss: 18.4897, val_MinusLogProbMetric: 18.4897

Epoch 224: val_loss did not improve from 18.41779
196/196 - 61s - loss: 18.5314 - MinusLogProbMetric: 18.5314 - val_loss: 18.4897 - val_MinusLogProbMetric: 18.4897 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 225/1000
2023-10-23 16:57:24.882 
Epoch 225/1000 
	 loss: 18.7264, MinusLogProbMetric: 18.7264, val_loss: 18.5864, val_MinusLogProbMetric: 18.5864

Epoch 225: val_loss did not improve from 18.41779
196/196 - 60s - loss: 18.7264 - MinusLogProbMetric: 18.7264 - val_loss: 18.5864 - val_MinusLogProbMetric: 18.5864 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 226/1000
2023-10-23 16:58:26.721 
Epoch 226/1000 
	 loss: 18.4685, MinusLogProbMetric: 18.4685, val_loss: 18.6564, val_MinusLogProbMetric: 18.6564

Epoch 226: val_loss did not improve from 18.41779
196/196 - 62s - loss: 18.4685 - MinusLogProbMetric: 18.4685 - val_loss: 18.6564 - val_MinusLogProbMetric: 18.6564 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 227/1000
2023-10-23 16:59:27.794 
Epoch 227/1000 
	 loss: 18.4605, MinusLogProbMetric: 18.4605, val_loss: 18.6317, val_MinusLogProbMetric: 18.6317

Epoch 227: val_loss did not improve from 18.41779
196/196 - 61s - loss: 18.4605 - MinusLogProbMetric: 18.4605 - val_loss: 18.6317 - val_MinusLogProbMetric: 18.6317 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 228/1000
2023-10-23 17:00:30.173 
Epoch 228/1000 
	 loss: 18.5045, MinusLogProbMetric: 18.5045, val_loss: 18.4149, val_MinusLogProbMetric: 18.4149

Epoch 228: val_loss improved from 18.41779 to 18.41486, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 18.5045 - MinusLogProbMetric: 18.5045 - val_loss: 18.4149 - val_MinusLogProbMetric: 18.4149 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 229/1000
2023-10-23 17:01:33.113 
Epoch 229/1000 
	 loss: 18.3772, MinusLogProbMetric: 18.3772, val_loss: 18.5069, val_MinusLogProbMetric: 18.5069

Epoch 229: val_loss did not improve from 18.41486
196/196 - 62s - loss: 18.3772 - MinusLogProbMetric: 18.3772 - val_loss: 18.5069 - val_MinusLogProbMetric: 18.5069 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 230/1000
2023-10-23 17:02:34.230 
Epoch 230/1000 
	 loss: 18.4148, MinusLogProbMetric: 18.4148, val_loss: 18.3816, val_MinusLogProbMetric: 18.3816

Epoch 230: val_loss improved from 18.41486 to 18.38164, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 62s - loss: 18.4148 - MinusLogProbMetric: 18.4148 - val_loss: 18.3816 - val_MinusLogProbMetric: 18.3816 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 231/1000
2023-10-23 17:03:35.177 
Epoch 231/1000 
	 loss: 18.5202, MinusLogProbMetric: 18.5202, val_loss: 18.3668, val_MinusLogProbMetric: 18.3668

Epoch 231: val_loss improved from 18.38164 to 18.36678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 61s - loss: 18.5202 - MinusLogProbMetric: 18.5202 - val_loss: 18.3668 - val_MinusLogProbMetric: 18.3668 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 232/1000
2023-10-23 17:04:36.983 
Epoch 232/1000 
	 loss: 18.4802, MinusLogProbMetric: 18.4802, val_loss: 18.4118, val_MinusLogProbMetric: 18.4118

Epoch 232: val_loss did not improve from 18.36678
196/196 - 61s - loss: 18.4802 - MinusLogProbMetric: 18.4802 - val_loss: 18.4118 - val_MinusLogProbMetric: 18.4118 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 233/1000
2023-10-23 17:05:39.264 
Epoch 233/1000 
	 loss: 18.5764, MinusLogProbMetric: 18.5764, val_loss: 22.3610, val_MinusLogProbMetric: 22.3610

Epoch 233: val_loss did not improve from 18.36678
196/196 - 62s - loss: 18.5764 - MinusLogProbMetric: 18.5764 - val_loss: 22.3610 - val_MinusLogProbMetric: 22.3610 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 234/1000
2023-10-23 17:06:40.305 
Epoch 234/1000 
	 loss: 18.6562, MinusLogProbMetric: 18.6562, val_loss: 18.8558, val_MinusLogProbMetric: 18.8558

Epoch 234: val_loss did not improve from 18.36678
196/196 - 61s - loss: 18.6562 - MinusLogProbMetric: 18.6562 - val_loss: 18.8558 - val_MinusLogProbMetric: 18.8558 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 235/1000
2023-10-23 17:07:43.699 
Epoch 235/1000 
	 loss: 18.4431, MinusLogProbMetric: 18.4431, val_loss: 18.6680, val_MinusLogProbMetric: 18.6680

Epoch 235: val_loss did not improve from 18.36678
196/196 - 63s - loss: 18.4431 - MinusLogProbMetric: 18.4431 - val_loss: 18.6680 - val_MinusLogProbMetric: 18.6680 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 236/1000
2023-10-23 17:08:44.920 
Epoch 236/1000 
	 loss: 18.3901, MinusLogProbMetric: 18.3901, val_loss: 18.4706, val_MinusLogProbMetric: 18.4706

Epoch 236: val_loss did not improve from 18.36678
196/196 - 61s - loss: 18.3901 - MinusLogProbMetric: 18.3901 - val_loss: 18.4706 - val_MinusLogProbMetric: 18.4706 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 237/1000
2023-10-23 17:09:45.422 
Epoch 237/1000 
	 loss: 18.4407, MinusLogProbMetric: 18.4407, val_loss: 18.3065, val_MinusLogProbMetric: 18.3065

Epoch 237: val_loss improved from 18.36678 to 18.30647, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 62s - loss: 18.4407 - MinusLogProbMetric: 18.4407 - val_loss: 18.3065 - val_MinusLogProbMetric: 18.3065 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 238/1000
2023-10-23 17:10:48.354 
Epoch 238/1000 
	 loss: 18.4298, MinusLogProbMetric: 18.4298, val_loss: 18.3511, val_MinusLogProbMetric: 18.3511

Epoch 238: val_loss did not improve from 18.30647
196/196 - 62s - loss: 18.4298 - MinusLogProbMetric: 18.4298 - val_loss: 18.3511 - val_MinusLogProbMetric: 18.3511 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 239/1000
2023-10-23 17:11:46.975 
Epoch 239/1000 
	 loss: 18.3819, MinusLogProbMetric: 18.3819, val_loss: 18.5758, val_MinusLogProbMetric: 18.5758

Epoch 239: val_loss did not improve from 18.30647
196/196 - 59s - loss: 18.3819 - MinusLogProbMetric: 18.3819 - val_loss: 18.5758 - val_MinusLogProbMetric: 18.5758 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 240/1000
2023-10-23 17:12:48.472 
Epoch 240/1000 
	 loss: 18.6453, MinusLogProbMetric: 18.6453, val_loss: 18.2056, val_MinusLogProbMetric: 18.2056

Epoch 240: val_loss improved from 18.30647 to 18.20560, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 62s - loss: 18.6453 - MinusLogProbMetric: 18.6453 - val_loss: 18.2056 - val_MinusLogProbMetric: 18.2056 - lr: 1.1111e-04 - 62s/epoch - 319ms/step
Epoch 241/1000
2023-10-23 17:13:48.933 
Epoch 241/1000 
	 loss: 18.3833, MinusLogProbMetric: 18.3833, val_loss: 18.6286, val_MinusLogProbMetric: 18.6286

Epoch 241: val_loss did not improve from 18.20560
196/196 - 59s - loss: 18.3833 - MinusLogProbMetric: 18.3833 - val_loss: 18.6286 - val_MinusLogProbMetric: 18.6286 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 242/1000
2023-10-23 17:14:47.219 
Epoch 242/1000 
	 loss: 18.3962, MinusLogProbMetric: 18.3962, val_loss: 18.2847, val_MinusLogProbMetric: 18.2847

Epoch 242: val_loss did not improve from 18.20560
196/196 - 58s - loss: 18.3962 - MinusLogProbMetric: 18.3962 - val_loss: 18.2847 - val_MinusLogProbMetric: 18.2847 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 243/1000
2023-10-23 17:15:48.472 
Epoch 243/1000 
	 loss: 18.4172, MinusLogProbMetric: 18.4172, val_loss: 18.4702, val_MinusLogProbMetric: 18.4702

Epoch 243: val_loss did not improve from 18.20560
196/196 - 61s - loss: 18.4172 - MinusLogProbMetric: 18.4172 - val_loss: 18.4702 - val_MinusLogProbMetric: 18.4702 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 244/1000
2023-10-23 17:16:47.540 
Epoch 244/1000 
	 loss: 18.3570, MinusLogProbMetric: 18.3570, val_loss: 18.3371, val_MinusLogProbMetric: 18.3371

Epoch 244: val_loss did not improve from 18.20560
196/196 - 59s - loss: 18.3570 - MinusLogProbMetric: 18.3570 - val_loss: 18.3371 - val_MinusLogProbMetric: 18.3371 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 245/1000
2023-10-23 17:17:48.051 
Epoch 245/1000 
	 loss: 18.3667, MinusLogProbMetric: 18.3667, val_loss: 18.5298, val_MinusLogProbMetric: 18.5298

Epoch 245: val_loss did not improve from 18.20560
196/196 - 61s - loss: 18.3667 - MinusLogProbMetric: 18.3667 - val_loss: 18.5298 - val_MinusLogProbMetric: 18.5298 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 246/1000
2023-10-23 17:18:48.736 
Epoch 246/1000 
	 loss: 18.3923, MinusLogProbMetric: 18.3923, val_loss: 18.4058, val_MinusLogProbMetric: 18.4058

Epoch 246: val_loss did not improve from 18.20560
196/196 - 61s - loss: 18.3923 - MinusLogProbMetric: 18.3923 - val_loss: 18.4058 - val_MinusLogProbMetric: 18.4058 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 247/1000
2023-10-23 17:19:49.648 
Epoch 247/1000 
	 loss: 18.5405, MinusLogProbMetric: 18.5405, val_loss: 20.0098, val_MinusLogProbMetric: 20.0098

Epoch 247: val_loss did not improve from 18.20560
196/196 - 61s - loss: 18.5405 - MinusLogProbMetric: 18.5405 - val_loss: 20.0098 - val_MinusLogProbMetric: 20.0098 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 248/1000
2023-10-23 17:20:51.690 
Epoch 248/1000 
	 loss: 18.5358, MinusLogProbMetric: 18.5358, val_loss: 18.2285, val_MinusLogProbMetric: 18.2285

Epoch 248: val_loss did not improve from 18.20560
196/196 - 62s - loss: 18.5358 - MinusLogProbMetric: 18.5358 - val_loss: 18.2285 - val_MinusLogProbMetric: 18.2285 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 249/1000
2023-10-23 17:21:51.911 
Epoch 249/1000 
	 loss: 18.2951, MinusLogProbMetric: 18.2951, val_loss: 18.6440, val_MinusLogProbMetric: 18.6440

Epoch 249: val_loss did not improve from 18.20560
196/196 - 60s - loss: 18.2951 - MinusLogProbMetric: 18.2951 - val_loss: 18.6440 - val_MinusLogProbMetric: 18.6440 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 250/1000
2023-10-23 17:22:50.894 
Epoch 250/1000 
	 loss: 18.3805, MinusLogProbMetric: 18.3805, val_loss: 18.4018, val_MinusLogProbMetric: 18.4018

Epoch 250: val_loss did not improve from 18.20560
196/196 - 59s - loss: 18.3805 - MinusLogProbMetric: 18.3805 - val_loss: 18.4018 - val_MinusLogProbMetric: 18.4018 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 251/1000
2023-10-23 17:23:52.658 
Epoch 251/1000 
	 loss: 18.3359, MinusLogProbMetric: 18.3359, val_loss: 18.2378, val_MinusLogProbMetric: 18.2378

Epoch 251: val_loss did not improve from 18.20560
196/196 - 62s - loss: 18.3359 - MinusLogProbMetric: 18.3359 - val_loss: 18.2378 - val_MinusLogProbMetric: 18.2378 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 252/1000
2023-10-23 17:24:55.881 
Epoch 252/1000 
	 loss: 18.4383, MinusLogProbMetric: 18.4383, val_loss: 18.5506, val_MinusLogProbMetric: 18.5506

Epoch 252: val_loss did not improve from 18.20560
196/196 - 63s - loss: 18.4383 - MinusLogProbMetric: 18.4383 - val_loss: 18.5506 - val_MinusLogProbMetric: 18.5506 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 253/1000
2023-10-23 17:25:55.556 
Epoch 253/1000 
	 loss: 18.5145, MinusLogProbMetric: 18.5145, val_loss: 18.5110, val_MinusLogProbMetric: 18.5110

Epoch 253: val_loss did not improve from 18.20560
196/196 - 60s - loss: 18.5145 - MinusLogProbMetric: 18.5145 - val_loss: 18.5110 - val_MinusLogProbMetric: 18.5110 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 254/1000
2023-10-23 17:26:58.437 
Epoch 254/1000 
	 loss: 18.2468, MinusLogProbMetric: 18.2468, val_loss: 18.2333, val_MinusLogProbMetric: 18.2333

Epoch 254: val_loss did not improve from 18.20560
196/196 - 63s - loss: 18.2468 - MinusLogProbMetric: 18.2468 - val_loss: 18.2333 - val_MinusLogProbMetric: 18.2333 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 255/1000
2023-10-23 17:27:59.653 
Epoch 255/1000 
	 loss: 18.2899, MinusLogProbMetric: 18.2899, val_loss: 18.4841, val_MinusLogProbMetric: 18.4841

Epoch 255: val_loss did not improve from 18.20560
196/196 - 61s - loss: 18.2899 - MinusLogProbMetric: 18.2899 - val_loss: 18.4841 - val_MinusLogProbMetric: 18.4841 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 256/1000
2023-10-23 17:28:59.196 
Epoch 256/1000 
	 loss: 18.2532, MinusLogProbMetric: 18.2532, val_loss: 18.2118, val_MinusLogProbMetric: 18.2118

Epoch 256: val_loss did not improve from 18.20560
196/196 - 60s - loss: 18.2532 - MinusLogProbMetric: 18.2532 - val_loss: 18.2118 - val_MinusLogProbMetric: 18.2118 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 257/1000
2023-10-23 17:30:00.278 
Epoch 257/1000 
	 loss: 18.2691, MinusLogProbMetric: 18.2691, val_loss: 18.2226, val_MinusLogProbMetric: 18.2226

Epoch 257: val_loss did not improve from 18.20560
196/196 - 61s - loss: 18.2691 - MinusLogProbMetric: 18.2691 - val_loss: 18.2226 - val_MinusLogProbMetric: 18.2226 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 258/1000
2023-10-23 17:30:59.763 
Epoch 258/1000 
	 loss: 18.3376, MinusLogProbMetric: 18.3376, val_loss: 18.6708, val_MinusLogProbMetric: 18.6708

Epoch 258: val_loss did not improve from 18.20560
196/196 - 59s - loss: 18.3376 - MinusLogProbMetric: 18.3376 - val_loss: 18.6708 - val_MinusLogProbMetric: 18.6708 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 259/1000
2023-10-23 17:32:00.257 
Epoch 259/1000 
	 loss: 18.2127, MinusLogProbMetric: 18.2127, val_loss: 18.3172, val_MinusLogProbMetric: 18.3172

Epoch 259: val_loss did not improve from 18.20560
196/196 - 60s - loss: 18.2127 - MinusLogProbMetric: 18.2127 - val_loss: 18.3172 - val_MinusLogProbMetric: 18.3172 - lr: 1.1111e-04 - 60s/epoch - 309ms/step
Epoch 260/1000
2023-10-23 17:33:01.587 
Epoch 260/1000 
	 loss: 18.2077, MinusLogProbMetric: 18.2077, val_loss: 18.3793, val_MinusLogProbMetric: 18.3793

Epoch 260: val_loss did not improve from 18.20560
196/196 - 61s - loss: 18.2077 - MinusLogProbMetric: 18.2077 - val_loss: 18.3793 - val_MinusLogProbMetric: 18.3793 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 261/1000
2023-10-23 17:34:02.848 
Epoch 261/1000 
	 loss: 18.2142, MinusLogProbMetric: 18.2142, val_loss: 18.3149, val_MinusLogProbMetric: 18.3149

Epoch 261: val_loss did not improve from 18.20560
196/196 - 61s - loss: 18.2142 - MinusLogProbMetric: 18.2142 - val_loss: 18.3149 - val_MinusLogProbMetric: 18.3149 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 262/1000
2023-10-23 17:35:01.809 
Epoch 262/1000 
	 loss: 18.3413, MinusLogProbMetric: 18.3413, val_loss: 18.2823, val_MinusLogProbMetric: 18.2823

Epoch 262: val_loss did not improve from 18.20560
196/196 - 59s - loss: 18.3413 - MinusLogProbMetric: 18.3413 - val_loss: 18.2823 - val_MinusLogProbMetric: 18.2823 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 263/1000
2023-10-23 17:36:03.414 
Epoch 263/1000 
	 loss: 18.3170, MinusLogProbMetric: 18.3170, val_loss: 18.2478, val_MinusLogProbMetric: 18.2478

Epoch 263: val_loss did not improve from 18.20560
196/196 - 62s - loss: 18.3170 - MinusLogProbMetric: 18.3170 - val_loss: 18.2478 - val_MinusLogProbMetric: 18.2478 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 264/1000
2023-10-23 17:37:02.282 
Epoch 264/1000 
	 loss: 18.2113, MinusLogProbMetric: 18.2113, val_loss: 18.7587, val_MinusLogProbMetric: 18.7587

Epoch 264: val_loss did not improve from 18.20560
196/196 - 59s - loss: 18.2113 - MinusLogProbMetric: 18.2113 - val_loss: 18.7587 - val_MinusLogProbMetric: 18.7587 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 265/1000
2023-10-23 17:38:05.329 
Epoch 265/1000 
	 loss: 18.2717, MinusLogProbMetric: 18.2717, val_loss: 18.7441, val_MinusLogProbMetric: 18.7441

Epoch 265: val_loss did not improve from 18.20560
196/196 - 63s - loss: 18.2717 - MinusLogProbMetric: 18.2717 - val_loss: 18.7441 - val_MinusLogProbMetric: 18.7441 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 266/1000
2023-10-23 17:39:05.014 
Epoch 266/1000 
	 loss: 18.1468, MinusLogProbMetric: 18.1468, val_loss: 18.2494, val_MinusLogProbMetric: 18.2494

Epoch 266: val_loss did not improve from 18.20560
196/196 - 60s - loss: 18.1468 - MinusLogProbMetric: 18.1468 - val_loss: 18.2494 - val_MinusLogProbMetric: 18.2494 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 267/1000
2023-10-23 17:40:06.036 
Epoch 267/1000 
	 loss: 18.2781, MinusLogProbMetric: 18.2781, val_loss: 18.1282, val_MinusLogProbMetric: 18.1282

Epoch 267: val_loss improved from 18.20560 to 18.12818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 62s - loss: 18.2781 - MinusLogProbMetric: 18.2781 - val_loss: 18.1282 - val_MinusLogProbMetric: 18.1282 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 268/1000
2023-10-23 17:41:07.946 
Epoch 268/1000 
	 loss: 18.1878, MinusLogProbMetric: 18.1878, val_loss: 18.1198, val_MinusLogProbMetric: 18.1198

Epoch 268: val_loss improved from 18.12818 to 18.11975, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 62s - loss: 18.1878 - MinusLogProbMetric: 18.1878 - val_loss: 18.1198 - val_MinusLogProbMetric: 18.1198 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 269/1000
2023-10-23 17:42:08.550 
Epoch 269/1000 
	 loss: 18.2335, MinusLogProbMetric: 18.2335, val_loss: 18.4929, val_MinusLogProbMetric: 18.4929

Epoch 269: val_loss did not improve from 18.11975
196/196 - 60s - loss: 18.2335 - MinusLogProbMetric: 18.2335 - val_loss: 18.4929 - val_MinusLogProbMetric: 18.4929 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 270/1000
2023-10-23 17:43:08.983 
Epoch 270/1000 
	 loss: 18.2439, MinusLogProbMetric: 18.2439, val_loss: 18.2461, val_MinusLogProbMetric: 18.2461

Epoch 270: val_loss did not improve from 18.11975
196/196 - 60s - loss: 18.2439 - MinusLogProbMetric: 18.2439 - val_loss: 18.2461 - val_MinusLogProbMetric: 18.2461 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 271/1000
2023-10-23 17:44:09.062 
Epoch 271/1000 
	 loss: 18.7506, MinusLogProbMetric: 18.7506, val_loss: 18.0950, val_MinusLogProbMetric: 18.0950

Epoch 271: val_loss improved from 18.11975 to 18.09495, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 61s - loss: 18.7506 - MinusLogProbMetric: 18.7506 - val_loss: 18.0950 - val_MinusLogProbMetric: 18.0950 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 272/1000
2023-10-23 17:45:08.441 
Epoch 272/1000 
	 loss: 18.1501, MinusLogProbMetric: 18.1501, val_loss: 18.1536, val_MinusLogProbMetric: 18.1536

Epoch 272: val_loss did not improve from 18.09495
196/196 - 58s - loss: 18.1501 - MinusLogProbMetric: 18.1501 - val_loss: 18.1536 - val_MinusLogProbMetric: 18.1536 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 273/1000
2023-10-23 17:46:08.420 
Epoch 273/1000 
	 loss: 18.2066, MinusLogProbMetric: 18.2066, val_loss: 18.2749, val_MinusLogProbMetric: 18.2749

Epoch 273: val_loss did not improve from 18.09495
196/196 - 60s - loss: 18.2066 - MinusLogProbMetric: 18.2066 - val_loss: 18.2749 - val_MinusLogProbMetric: 18.2749 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 274/1000
2023-10-23 17:47:10.676 
Epoch 274/1000 
	 loss: 18.2634, MinusLogProbMetric: 18.2634, val_loss: 18.8852, val_MinusLogProbMetric: 18.8852

Epoch 274: val_loss did not improve from 18.09495
196/196 - 62s - loss: 18.2634 - MinusLogProbMetric: 18.2634 - val_loss: 18.8852 - val_MinusLogProbMetric: 18.8852 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 275/1000
2023-10-23 17:48:11.248 
Epoch 275/1000 
	 loss: 18.5716, MinusLogProbMetric: 18.5716, val_loss: 18.3475, val_MinusLogProbMetric: 18.3475

Epoch 275: val_loss did not improve from 18.09495
196/196 - 61s - loss: 18.5716 - MinusLogProbMetric: 18.5716 - val_loss: 18.3475 - val_MinusLogProbMetric: 18.3475 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 276/1000
2023-10-23 17:49:13.848 
Epoch 276/1000 
	 loss: 18.1022, MinusLogProbMetric: 18.1022, val_loss: 18.2827, val_MinusLogProbMetric: 18.2827

Epoch 276: val_loss did not improve from 18.09495
196/196 - 63s - loss: 18.1022 - MinusLogProbMetric: 18.1022 - val_loss: 18.2827 - val_MinusLogProbMetric: 18.2827 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 277/1000
2023-10-23 17:50:12.458 
Epoch 277/1000 
	 loss: 18.2350, MinusLogProbMetric: 18.2350, val_loss: 18.1137, val_MinusLogProbMetric: 18.1137

Epoch 277: val_loss did not improve from 18.09495
196/196 - 59s - loss: 18.2350 - MinusLogProbMetric: 18.2350 - val_loss: 18.1137 - val_MinusLogProbMetric: 18.1137 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 278/1000
2023-10-23 17:51:15.491 
Epoch 278/1000 
	 loss: 18.1751, MinusLogProbMetric: 18.1751, val_loss: 18.1443, val_MinusLogProbMetric: 18.1443

Epoch 278: val_loss did not improve from 18.09495
196/196 - 63s - loss: 18.1751 - MinusLogProbMetric: 18.1751 - val_loss: 18.1443 - val_MinusLogProbMetric: 18.1443 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 279/1000
2023-10-23 17:52:15.599 
Epoch 279/1000 
	 loss: 18.1571, MinusLogProbMetric: 18.1571, val_loss: 17.9631, val_MinusLogProbMetric: 17.9631

Epoch 279: val_loss improved from 18.09495 to 17.96309, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 61s - loss: 18.1571 - MinusLogProbMetric: 18.1571 - val_loss: 17.9631 - val_MinusLogProbMetric: 17.9631 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 280/1000
2023-10-23 17:53:20.648 
Epoch 280/1000 
	 loss: 18.1531, MinusLogProbMetric: 18.1531, val_loss: 18.3588, val_MinusLogProbMetric: 18.3588

Epoch 280: val_loss did not improve from 17.96309
196/196 - 64s - loss: 18.1531 - MinusLogProbMetric: 18.1531 - val_loss: 18.3588 - val_MinusLogProbMetric: 18.3588 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 281/1000
2023-10-23 17:54:20.533 
Epoch 281/1000 
	 loss: 18.1167, MinusLogProbMetric: 18.1167, val_loss: 18.0760, val_MinusLogProbMetric: 18.0760

Epoch 281: val_loss did not improve from 17.96309
196/196 - 60s - loss: 18.1167 - MinusLogProbMetric: 18.1167 - val_loss: 18.0760 - val_MinusLogProbMetric: 18.0760 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 282/1000
2023-10-23 17:55:23.025 
Epoch 282/1000 
	 loss: 18.1300, MinusLogProbMetric: 18.1300, val_loss: 18.1720, val_MinusLogProbMetric: 18.1720

Epoch 282: val_loss did not improve from 17.96309
196/196 - 62s - loss: 18.1300 - MinusLogProbMetric: 18.1300 - val_loss: 18.1720 - val_MinusLogProbMetric: 18.1720 - lr: 1.1111e-04 - 62s/epoch - 319ms/step
Epoch 283/1000
2023-10-23 17:56:25.527 
Epoch 283/1000 
	 loss: 18.4719, MinusLogProbMetric: 18.4719, val_loss: 18.2519, val_MinusLogProbMetric: 18.2519

Epoch 283: val_loss did not improve from 17.96309
196/196 - 62s - loss: 18.4719 - MinusLogProbMetric: 18.4719 - val_loss: 18.2519 - val_MinusLogProbMetric: 18.2519 - lr: 1.1111e-04 - 62s/epoch - 319ms/step
Epoch 284/1000
2023-10-23 17:57:25.274 
Epoch 284/1000 
	 loss: 18.0924, MinusLogProbMetric: 18.0924, val_loss: 17.9096, val_MinusLogProbMetric: 17.9096

Epoch 284: val_loss improved from 17.96309 to 17.90958, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 61s - loss: 18.0924 - MinusLogProbMetric: 18.0924 - val_loss: 17.9096 - val_MinusLogProbMetric: 17.9096 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 285/1000
2023-10-23 17:58:26.709 
Epoch 285/1000 
	 loss: 18.2420, MinusLogProbMetric: 18.2420, val_loss: 18.4429, val_MinusLogProbMetric: 18.4429

Epoch 285: val_loss did not improve from 17.90958
196/196 - 60s - loss: 18.2420 - MinusLogProbMetric: 18.2420 - val_loss: 18.4429 - val_MinusLogProbMetric: 18.4429 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 286/1000
2023-10-23 17:59:29.731 
Epoch 286/1000 
	 loss: 18.1467, MinusLogProbMetric: 18.1467, val_loss: 18.0417, val_MinusLogProbMetric: 18.0417

Epoch 286: val_loss did not improve from 17.90958
196/196 - 63s - loss: 18.1467 - MinusLogProbMetric: 18.1467 - val_loss: 18.0417 - val_MinusLogProbMetric: 18.0417 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 287/1000
2023-10-23 18:00:31.665 
Epoch 287/1000 
	 loss: 18.3603, MinusLogProbMetric: 18.3603, val_loss: 18.2349, val_MinusLogProbMetric: 18.2349

Epoch 287: val_loss did not improve from 17.90958
196/196 - 62s - loss: 18.3603 - MinusLogProbMetric: 18.3603 - val_loss: 18.2349 - val_MinusLogProbMetric: 18.2349 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 288/1000
2023-10-23 18:01:31.970 
Epoch 288/1000 
	 loss: 18.1329, MinusLogProbMetric: 18.1329, val_loss: 18.0718, val_MinusLogProbMetric: 18.0718

Epoch 288: val_loss did not improve from 17.90958
196/196 - 60s - loss: 18.1329 - MinusLogProbMetric: 18.1329 - val_loss: 18.0718 - val_MinusLogProbMetric: 18.0718 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 289/1000
2023-10-23 18:02:35.280 
Epoch 289/1000 
	 loss: 18.1725, MinusLogProbMetric: 18.1725, val_loss: 18.2025, val_MinusLogProbMetric: 18.2025

Epoch 289: val_loss did not improve from 17.90958
196/196 - 63s - loss: 18.1725 - MinusLogProbMetric: 18.1725 - val_loss: 18.2025 - val_MinusLogProbMetric: 18.2025 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 290/1000
2023-10-23 18:03:36.160 
Epoch 290/1000 
	 loss: 18.0930, MinusLogProbMetric: 18.0930, val_loss: 17.9619, val_MinusLogProbMetric: 17.9619

Epoch 290: val_loss did not improve from 17.90958
196/196 - 61s - loss: 18.0930 - MinusLogProbMetric: 18.0930 - val_loss: 17.9619 - val_MinusLogProbMetric: 17.9619 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 291/1000
2023-10-23 18:04:38.049 
Epoch 291/1000 
	 loss: 18.1141, MinusLogProbMetric: 18.1141, val_loss: 18.1910, val_MinusLogProbMetric: 18.1910

Epoch 291: val_loss did not improve from 17.90958
196/196 - 62s - loss: 18.1141 - MinusLogProbMetric: 18.1141 - val_loss: 18.1910 - val_MinusLogProbMetric: 18.1910 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 292/1000
2023-10-23 18:05:39.222 
Epoch 292/1000 
	 loss: 18.1311, MinusLogProbMetric: 18.1311, val_loss: 18.4566, val_MinusLogProbMetric: 18.4566

Epoch 292: val_loss did not improve from 17.90958
196/196 - 61s - loss: 18.1311 - MinusLogProbMetric: 18.1311 - val_loss: 18.4566 - val_MinusLogProbMetric: 18.4566 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 293/1000
2023-10-23 18:06:40.243 
Epoch 293/1000 
	 loss: 18.3507, MinusLogProbMetric: 18.3507, val_loss: 18.0774, val_MinusLogProbMetric: 18.0774

Epoch 293: val_loss did not improve from 17.90958
196/196 - 61s - loss: 18.3507 - MinusLogProbMetric: 18.3507 - val_loss: 18.0774 - val_MinusLogProbMetric: 18.0774 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 294/1000
2023-10-23 18:07:41.599 
Epoch 294/1000 
	 loss: 18.0608, MinusLogProbMetric: 18.0608, val_loss: 18.1759, val_MinusLogProbMetric: 18.1759

Epoch 294: val_loss did not improve from 17.90958
196/196 - 61s - loss: 18.0608 - MinusLogProbMetric: 18.0608 - val_loss: 18.1759 - val_MinusLogProbMetric: 18.1759 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 295/1000
2023-10-23 18:08:42.959 
Epoch 295/1000 
	 loss: 18.2331, MinusLogProbMetric: 18.2331, val_loss: 19.0857, val_MinusLogProbMetric: 19.0857

Epoch 295: val_loss did not improve from 17.90958
196/196 - 61s - loss: 18.2331 - MinusLogProbMetric: 18.2331 - val_loss: 19.0857 - val_MinusLogProbMetric: 19.0857 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 296/1000
2023-10-23 18:09:44.396 
Epoch 296/1000 
	 loss: 18.0419, MinusLogProbMetric: 18.0419, val_loss: 18.5491, val_MinusLogProbMetric: 18.5491

Epoch 296: val_loss did not improve from 17.90958
196/196 - 61s - loss: 18.0419 - MinusLogProbMetric: 18.0419 - val_loss: 18.5491 - val_MinusLogProbMetric: 18.5491 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 297/1000
2023-10-23 18:10:44.752 
Epoch 297/1000 
	 loss: 18.1184, MinusLogProbMetric: 18.1184, val_loss: 18.0516, val_MinusLogProbMetric: 18.0516

Epoch 297: val_loss did not improve from 17.90958
196/196 - 60s - loss: 18.1184 - MinusLogProbMetric: 18.1184 - val_loss: 18.0516 - val_MinusLogProbMetric: 18.0516 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 298/1000
2023-10-23 18:11:44.359 
Epoch 298/1000 
	 loss: 18.0634, MinusLogProbMetric: 18.0634, val_loss: 18.6325, val_MinusLogProbMetric: 18.6325

Epoch 298: val_loss did not improve from 17.90958
196/196 - 60s - loss: 18.0634 - MinusLogProbMetric: 18.0634 - val_loss: 18.6325 - val_MinusLogProbMetric: 18.6325 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 299/1000
2023-10-23 18:12:46.182 
Epoch 299/1000 
	 loss: 18.1838, MinusLogProbMetric: 18.1838, val_loss: 17.9813, val_MinusLogProbMetric: 17.9813

Epoch 299: val_loss did not improve from 17.90958
196/196 - 62s - loss: 18.1838 - MinusLogProbMetric: 18.1838 - val_loss: 17.9813 - val_MinusLogProbMetric: 17.9813 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 300/1000
2023-10-23 18:13:47.459 
Epoch 300/1000 
	 loss: 18.0944, MinusLogProbMetric: 18.0944, val_loss: 18.4523, val_MinusLogProbMetric: 18.4523

Epoch 300: val_loss did not improve from 17.90958
196/196 - 61s - loss: 18.0944 - MinusLogProbMetric: 18.0944 - val_loss: 18.4523 - val_MinusLogProbMetric: 18.4523 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 301/1000
2023-10-23 18:14:47.482 
Epoch 301/1000 
	 loss: 18.0159, MinusLogProbMetric: 18.0159, val_loss: 18.0614, val_MinusLogProbMetric: 18.0614

Epoch 301: val_loss did not improve from 17.90958
196/196 - 60s - loss: 18.0159 - MinusLogProbMetric: 18.0159 - val_loss: 18.0614 - val_MinusLogProbMetric: 18.0614 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 302/1000
2023-10-23 18:15:50.197 
Epoch 302/1000 
	 loss: 18.0135, MinusLogProbMetric: 18.0135, val_loss: 18.2992, val_MinusLogProbMetric: 18.2992

Epoch 302: val_loss did not improve from 17.90958
196/196 - 63s - loss: 18.0135 - MinusLogProbMetric: 18.0135 - val_loss: 18.2992 - val_MinusLogProbMetric: 18.2992 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 303/1000
2023-10-23 18:16:51.532 
Epoch 303/1000 
	 loss: 18.0134, MinusLogProbMetric: 18.0134, val_loss: 17.9103, val_MinusLogProbMetric: 17.9103

Epoch 303: val_loss did not improve from 17.90958
196/196 - 61s - loss: 18.0134 - MinusLogProbMetric: 18.0134 - val_loss: 17.9103 - val_MinusLogProbMetric: 17.9103 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 304/1000
2023-10-23 18:17:51.293 
Epoch 304/1000 
	 loss: 18.0246, MinusLogProbMetric: 18.0246, val_loss: 18.1143, val_MinusLogProbMetric: 18.1143

Epoch 304: val_loss did not improve from 17.90958
196/196 - 60s - loss: 18.0246 - MinusLogProbMetric: 18.0246 - val_loss: 18.1143 - val_MinusLogProbMetric: 18.1143 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 305/1000
2023-10-23 18:18:52.091 
Epoch 305/1000 
	 loss: 18.5891, MinusLogProbMetric: 18.5891, val_loss: 18.2937, val_MinusLogProbMetric: 18.2937

Epoch 305: val_loss did not improve from 17.90958
196/196 - 61s - loss: 18.5891 - MinusLogProbMetric: 18.5891 - val_loss: 18.2937 - val_MinusLogProbMetric: 18.2937 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 306/1000
2023-10-23 18:19:51.902 
Epoch 306/1000 
	 loss: 18.0131, MinusLogProbMetric: 18.0131, val_loss: 18.0908, val_MinusLogProbMetric: 18.0908

Epoch 306: val_loss did not improve from 17.90958
196/196 - 60s - loss: 18.0131 - MinusLogProbMetric: 18.0131 - val_loss: 18.0908 - val_MinusLogProbMetric: 18.0908 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 307/1000
2023-10-23 18:20:50.715 
Epoch 307/1000 
	 loss: 18.0770, MinusLogProbMetric: 18.0770, val_loss: 17.9164, val_MinusLogProbMetric: 17.9164

Epoch 307: val_loss did not improve from 17.90958
196/196 - 59s - loss: 18.0770 - MinusLogProbMetric: 18.0770 - val_loss: 17.9164 - val_MinusLogProbMetric: 17.9164 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 308/1000
2023-10-23 18:21:53.715 
Epoch 308/1000 
	 loss: 18.0069, MinusLogProbMetric: 18.0069, val_loss: 18.2294, val_MinusLogProbMetric: 18.2294

Epoch 308: val_loss did not improve from 17.90958
196/196 - 63s - loss: 18.0069 - MinusLogProbMetric: 18.0069 - val_loss: 18.2294 - val_MinusLogProbMetric: 18.2294 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 309/1000
2023-10-23 18:22:55.692 
Epoch 309/1000 
	 loss: 18.0405, MinusLogProbMetric: 18.0405, val_loss: 18.3978, val_MinusLogProbMetric: 18.3978

Epoch 309: val_loss did not improve from 17.90958
196/196 - 62s - loss: 18.0405 - MinusLogProbMetric: 18.0405 - val_loss: 18.3978 - val_MinusLogProbMetric: 18.3978 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 310/1000
2023-10-23 18:23:55.624 
Epoch 310/1000 
	 loss: 18.2697, MinusLogProbMetric: 18.2697, val_loss: 18.4363, val_MinusLogProbMetric: 18.4363

Epoch 310: val_loss did not improve from 17.90958
196/196 - 60s - loss: 18.2697 - MinusLogProbMetric: 18.2697 - val_loss: 18.4363 - val_MinusLogProbMetric: 18.4363 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 311/1000
2023-10-23 18:24:57.997 
Epoch 311/1000 
	 loss: 18.0269, MinusLogProbMetric: 18.0269, val_loss: 18.0822, val_MinusLogProbMetric: 18.0822

Epoch 311: val_loss did not improve from 17.90958
196/196 - 62s - loss: 18.0269 - MinusLogProbMetric: 18.0269 - val_loss: 18.0822 - val_MinusLogProbMetric: 18.0822 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 312/1000
2023-10-23 18:25:59.517 
Epoch 312/1000 
	 loss: 17.9849, MinusLogProbMetric: 17.9849, val_loss: 18.0758, val_MinusLogProbMetric: 18.0758

Epoch 312: val_loss did not improve from 17.90958
196/196 - 62s - loss: 17.9849 - MinusLogProbMetric: 17.9849 - val_loss: 18.0758 - val_MinusLogProbMetric: 18.0758 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 313/1000
2023-10-23 18:27:01.545 
Epoch 313/1000 
	 loss: 18.0260, MinusLogProbMetric: 18.0260, val_loss: 17.9802, val_MinusLogProbMetric: 17.9802

Epoch 313: val_loss did not improve from 17.90958
196/196 - 62s - loss: 18.0260 - MinusLogProbMetric: 18.0260 - val_loss: 17.9802 - val_MinusLogProbMetric: 17.9802 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 314/1000
2023-10-23 18:28:04.418 
Epoch 314/1000 
	 loss: 18.2833, MinusLogProbMetric: 18.2833, val_loss: 18.0539, val_MinusLogProbMetric: 18.0539

Epoch 314: val_loss did not improve from 17.90958
196/196 - 63s - loss: 18.2833 - MinusLogProbMetric: 18.2833 - val_loss: 18.0539 - val_MinusLogProbMetric: 18.0539 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 315/1000
2023-10-23 18:29:03.863 
Epoch 315/1000 
	 loss: 17.9418, MinusLogProbMetric: 17.9418, val_loss: 18.3841, val_MinusLogProbMetric: 18.3841

Epoch 315: val_loss did not improve from 17.90958
196/196 - 59s - loss: 17.9418 - MinusLogProbMetric: 17.9418 - val_loss: 18.3841 - val_MinusLogProbMetric: 18.3841 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 316/1000
2023-10-23 18:30:05.706 
Epoch 316/1000 
	 loss: 18.0227, MinusLogProbMetric: 18.0227, val_loss: 17.9375, val_MinusLogProbMetric: 17.9375

Epoch 316: val_loss did not improve from 17.90958
196/196 - 62s - loss: 18.0227 - MinusLogProbMetric: 18.0227 - val_loss: 17.9375 - val_MinusLogProbMetric: 17.9375 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 317/1000
2023-10-23 18:31:07.669 
Epoch 317/1000 
	 loss: 17.9811, MinusLogProbMetric: 17.9811, val_loss: 18.4877, val_MinusLogProbMetric: 18.4877

Epoch 317: val_loss did not improve from 17.90958
196/196 - 62s - loss: 17.9811 - MinusLogProbMetric: 17.9811 - val_loss: 18.4877 - val_MinusLogProbMetric: 18.4877 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 318/1000
2023-10-23 18:32:09.259 
Epoch 318/1000 
	 loss: 18.2303, MinusLogProbMetric: 18.2303, val_loss: 18.0041, val_MinusLogProbMetric: 18.0041

Epoch 318: val_loss did not improve from 17.90958
196/196 - 62s - loss: 18.2303 - MinusLogProbMetric: 18.2303 - val_loss: 18.0041 - val_MinusLogProbMetric: 18.0041 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 319/1000
2023-10-23 18:33:14.222 
Epoch 319/1000 
	 loss: 17.9734, MinusLogProbMetric: 17.9734, val_loss: 18.0211, val_MinusLogProbMetric: 18.0211

Epoch 319: val_loss did not improve from 17.90958
196/196 - 65s - loss: 17.9734 - MinusLogProbMetric: 17.9734 - val_loss: 18.0211 - val_MinusLogProbMetric: 18.0211 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 320/1000
2023-10-23 18:34:15.883 
Epoch 320/1000 
	 loss: 17.9387, MinusLogProbMetric: 17.9387, val_loss: 18.7633, val_MinusLogProbMetric: 18.7633

Epoch 320: val_loss did not improve from 17.90958
196/196 - 62s - loss: 17.9387 - MinusLogProbMetric: 17.9387 - val_loss: 18.7633 - val_MinusLogProbMetric: 18.7633 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 321/1000
2023-10-23 18:35:17.295 
Epoch 321/1000 
	 loss: 19.1887, MinusLogProbMetric: 19.1887, val_loss: 18.0038, val_MinusLogProbMetric: 18.0038

Epoch 321: val_loss did not improve from 17.90958
196/196 - 61s - loss: 19.1887 - MinusLogProbMetric: 19.1887 - val_loss: 18.0038 - val_MinusLogProbMetric: 18.0038 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 322/1000
2023-10-23 18:36:21.255 
Epoch 322/1000 
	 loss: 17.9330, MinusLogProbMetric: 17.9330, val_loss: 17.9917, val_MinusLogProbMetric: 17.9917

Epoch 322: val_loss did not improve from 17.90958
196/196 - 64s - loss: 17.9330 - MinusLogProbMetric: 17.9330 - val_loss: 17.9917 - val_MinusLogProbMetric: 17.9917 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 323/1000
2023-10-23 18:37:23.781 
Epoch 323/1000 
	 loss: 17.9779, MinusLogProbMetric: 17.9779, val_loss: 18.1922, val_MinusLogProbMetric: 18.1922

Epoch 323: val_loss did not improve from 17.90958
196/196 - 63s - loss: 17.9779 - MinusLogProbMetric: 17.9779 - val_loss: 18.1922 - val_MinusLogProbMetric: 18.1922 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 324/1000
2023-10-23 18:38:25.742 
Epoch 324/1000 
	 loss: 18.1309, MinusLogProbMetric: 18.1309, val_loss: 18.3052, val_MinusLogProbMetric: 18.3052

Epoch 324: val_loss did not improve from 17.90958
196/196 - 62s - loss: 18.1309 - MinusLogProbMetric: 18.1309 - val_loss: 18.3052 - val_MinusLogProbMetric: 18.3052 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 325/1000
2023-10-23 18:39:27.281 
Epoch 325/1000 
	 loss: 17.9770, MinusLogProbMetric: 17.9770, val_loss: 18.0931, val_MinusLogProbMetric: 18.0931

Epoch 325: val_loss did not improve from 17.90958
196/196 - 62s - loss: 17.9770 - MinusLogProbMetric: 17.9770 - val_loss: 18.0931 - val_MinusLogProbMetric: 18.0931 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 326/1000
2023-10-23 18:40:27.266 
Epoch 326/1000 
	 loss: 17.9833, MinusLogProbMetric: 17.9833, val_loss: 18.3271, val_MinusLogProbMetric: 18.3271

Epoch 326: val_loss did not improve from 17.90958
196/196 - 60s - loss: 17.9833 - MinusLogProbMetric: 17.9833 - val_loss: 18.3271 - val_MinusLogProbMetric: 18.3271 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 327/1000
2023-10-23 18:41:29.441 
Epoch 327/1000 
	 loss: 18.1191, MinusLogProbMetric: 18.1191, val_loss: 18.0510, val_MinusLogProbMetric: 18.0510

Epoch 327: val_loss did not improve from 17.90958
196/196 - 62s - loss: 18.1191 - MinusLogProbMetric: 18.1191 - val_loss: 18.0510 - val_MinusLogProbMetric: 18.0510 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 328/1000
2023-10-23 18:42:31.542 
Epoch 328/1000 
	 loss: 17.9699, MinusLogProbMetric: 17.9699, val_loss: 18.2129, val_MinusLogProbMetric: 18.2129

Epoch 328: val_loss did not improve from 17.90958
196/196 - 62s - loss: 17.9699 - MinusLogProbMetric: 17.9699 - val_loss: 18.2129 - val_MinusLogProbMetric: 18.2129 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 329/1000
2023-10-23 18:43:34.349 
Epoch 329/1000 
	 loss: 17.8910, MinusLogProbMetric: 17.8910, val_loss: 18.5725, val_MinusLogProbMetric: 18.5725

Epoch 329: val_loss did not improve from 17.90958
196/196 - 63s - loss: 17.8910 - MinusLogProbMetric: 17.8910 - val_loss: 18.5725 - val_MinusLogProbMetric: 18.5725 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 330/1000
2023-10-23 18:44:35.017 
Epoch 330/1000 
	 loss: 17.9934, MinusLogProbMetric: 17.9934, val_loss: 18.0716, val_MinusLogProbMetric: 18.0716

Epoch 330: val_loss did not improve from 17.90958
196/196 - 61s - loss: 17.9934 - MinusLogProbMetric: 17.9934 - val_loss: 18.0716 - val_MinusLogProbMetric: 18.0716 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 331/1000
2023-10-23 18:45:35.961 
Epoch 331/1000 
	 loss: 18.0354, MinusLogProbMetric: 18.0354, val_loss: 17.8250, val_MinusLogProbMetric: 17.8250

Epoch 331: val_loss improved from 17.90958 to 17.82505, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 62s - loss: 18.0354 - MinusLogProbMetric: 18.0354 - val_loss: 17.8250 - val_MinusLogProbMetric: 17.8250 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 332/1000
2023-10-23 18:46:39.951 
Epoch 332/1000 
	 loss: 17.9929, MinusLogProbMetric: 17.9929, val_loss: 17.9417, val_MinusLogProbMetric: 17.9417

Epoch 332: val_loss did not improve from 17.82505
196/196 - 63s - loss: 17.9929 - MinusLogProbMetric: 17.9929 - val_loss: 17.9417 - val_MinusLogProbMetric: 17.9417 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 333/1000
2023-10-23 18:47:42.807 
Epoch 333/1000 
	 loss: 18.1343, MinusLogProbMetric: 18.1343, val_loss: 17.8850, val_MinusLogProbMetric: 17.8850

Epoch 333: val_loss did not improve from 17.82505
196/196 - 63s - loss: 18.1343 - MinusLogProbMetric: 18.1343 - val_loss: 17.8850 - val_MinusLogProbMetric: 17.8850 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 334/1000
2023-10-23 18:48:47.078 
Epoch 334/1000 
	 loss: 17.9415, MinusLogProbMetric: 17.9415, val_loss: 18.0470, val_MinusLogProbMetric: 18.0470

Epoch 334: val_loss did not improve from 17.82505
196/196 - 64s - loss: 17.9415 - MinusLogProbMetric: 17.9415 - val_loss: 18.0470 - val_MinusLogProbMetric: 18.0470 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 335/1000
2023-10-23 18:49:51.131 
Epoch 335/1000 
	 loss: 17.9743, MinusLogProbMetric: 17.9743, val_loss: 18.0183, val_MinusLogProbMetric: 18.0183

Epoch 335: val_loss did not improve from 17.82505
196/196 - 64s - loss: 17.9743 - MinusLogProbMetric: 17.9743 - val_loss: 18.0183 - val_MinusLogProbMetric: 18.0183 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 336/1000
2023-10-23 18:50:57.979 
Epoch 336/1000 
	 loss: 18.7607, MinusLogProbMetric: 18.7607, val_loss: 17.9235, val_MinusLogProbMetric: 17.9235

Epoch 336: val_loss did not improve from 17.82505
196/196 - 67s - loss: 18.7607 - MinusLogProbMetric: 18.7607 - val_loss: 17.9235 - val_MinusLogProbMetric: 17.9235 - lr: 1.1111e-04 - 67s/epoch - 341ms/step
Epoch 337/1000
2023-10-23 18:52:05.817 
Epoch 337/1000 
	 loss: 17.8783, MinusLogProbMetric: 17.8783, val_loss: 17.8711, val_MinusLogProbMetric: 17.8711

Epoch 337: val_loss did not improve from 17.82505
196/196 - 68s - loss: 17.8783 - MinusLogProbMetric: 17.8783 - val_loss: 17.8711 - val_MinusLogProbMetric: 17.8711 - lr: 1.1111e-04 - 68s/epoch - 346ms/step
Epoch 338/1000
2023-10-23 18:53:10.704 
Epoch 338/1000 
	 loss: 17.9165, MinusLogProbMetric: 17.9165, val_loss: 17.8751, val_MinusLogProbMetric: 17.8751

Epoch 338: val_loss did not improve from 17.82505
196/196 - 65s - loss: 17.9165 - MinusLogProbMetric: 17.9165 - val_loss: 17.8751 - val_MinusLogProbMetric: 17.8751 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 339/1000
2023-10-23 18:54:15.306 
Epoch 339/1000 
	 loss: 17.9373, MinusLogProbMetric: 17.9373, val_loss: 18.0270, val_MinusLogProbMetric: 18.0270

Epoch 339: val_loss did not improve from 17.82505
196/196 - 65s - loss: 17.9373 - MinusLogProbMetric: 17.9373 - val_loss: 18.0270 - val_MinusLogProbMetric: 18.0270 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 340/1000
2023-10-23 18:55:19.665 
Epoch 340/1000 
	 loss: 17.8982, MinusLogProbMetric: 17.8982, val_loss: 18.0630, val_MinusLogProbMetric: 18.0630

Epoch 340: val_loss did not improve from 17.82505
196/196 - 64s - loss: 17.8982 - MinusLogProbMetric: 17.8982 - val_loss: 18.0630 - val_MinusLogProbMetric: 18.0630 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 341/1000
2023-10-23 18:56:22.381 
Epoch 341/1000 
	 loss: 17.9159, MinusLogProbMetric: 17.9159, val_loss: 18.2819, val_MinusLogProbMetric: 18.2819

Epoch 341: val_loss did not improve from 17.82505
196/196 - 63s - loss: 17.9159 - MinusLogProbMetric: 17.9159 - val_loss: 18.2819 - val_MinusLogProbMetric: 18.2819 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 342/1000
2023-10-23 18:57:25.328 
Epoch 342/1000 
	 loss: 17.9085, MinusLogProbMetric: 17.9085, val_loss: 17.9534, val_MinusLogProbMetric: 17.9534

Epoch 342: val_loss did not improve from 17.82505
196/196 - 63s - loss: 17.9085 - MinusLogProbMetric: 17.9085 - val_loss: 17.9534 - val_MinusLogProbMetric: 17.9534 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 343/1000
2023-10-23 18:58:28.185 
Epoch 343/1000 
	 loss: 18.9849, MinusLogProbMetric: 18.9849, val_loss: 20.0688, val_MinusLogProbMetric: 20.0688

Epoch 343: val_loss did not improve from 17.82505
196/196 - 63s - loss: 18.9849 - MinusLogProbMetric: 18.9849 - val_loss: 20.0688 - val_MinusLogProbMetric: 20.0688 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 344/1000
2023-10-23 18:59:32.177 
Epoch 344/1000 
	 loss: 18.1257, MinusLogProbMetric: 18.1257, val_loss: 18.1702, val_MinusLogProbMetric: 18.1702

Epoch 344: val_loss did not improve from 17.82505
196/196 - 64s - loss: 18.1257 - MinusLogProbMetric: 18.1257 - val_loss: 18.1702 - val_MinusLogProbMetric: 18.1702 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 345/1000
2023-10-23 19:00:34.133 
Epoch 345/1000 
	 loss: 18.0041, MinusLogProbMetric: 18.0041, val_loss: 18.1488, val_MinusLogProbMetric: 18.1488

Epoch 345: val_loss did not improve from 17.82505
196/196 - 62s - loss: 18.0041 - MinusLogProbMetric: 18.0041 - val_loss: 18.1488 - val_MinusLogProbMetric: 18.1488 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 346/1000
2023-10-23 19:01:35.894 
Epoch 346/1000 
	 loss: 17.8872, MinusLogProbMetric: 17.8872, val_loss: 17.9167, val_MinusLogProbMetric: 17.9167

Epoch 346: val_loss did not improve from 17.82505
196/196 - 62s - loss: 17.8872 - MinusLogProbMetric: 17.8872 - val_loss: 17.9167 - val_MinusLogProbMetric: 17.9167 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 347/1000
2023-10-23 19:02:38.279 
Epoch 347/1000 
	 loss: 17.8741, MinusLogProbMetric: 17.8741, val_loss: 18.0545, val_MinusLogProbMetric: 18.0545

Epoch 347: val_loss did not improve from 17.82505
196/196 - 62s - loss: 17.8741 - MinusLogProbMetric: 17.8741 - val_loss: 18.0545 - val_MinusLogProbMetric: 18.0545 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 348/1000
2023-10-23 19:03:41.919 
Epoch 348/1000 
	 loss: 18.0459, MinusLogProbMetric: 18.0459, val_loss: 18.1308, val_MinusLogProbMetric: 18.1308

Epoch 348: val_loss did not improve from 17.82505
196/196 - 64s - loss: 18.0459 - MinusLogProbMetric: 18.0459 - val_loss: 18.1308 - val_MinusLogProbMetric: 18.1308 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 349/1000
2023-10-23 19:04:45.780 
Epoch 349/1000 
	 loss: 17.8579, MinusLogProbMetric: 17.8579, val_loss: 18.1737, val_MinusLogProbMetric: 18.1737

Epoch 349: val_loss did not improve from 17.82505
196/196 - 64s - loss: 17.8579 - MinusLogProbMetric: 17.8579 - val_loss: 18.1737 - val_MinusLogProbMetric: 18.1737 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 350/1000
2023-10-23 19:05:50.047 
Epoch 350/1000 
	 loss: 17.8278, MinusLogProbMetric: 17.8278, val_loss: 18.1487, val_MinusLogProbMetric: 18.1487

Epoch 350: val_loss did not improve from 17.82505
196/196 - 64s - loss: 17.8278 - MinusLogProbMetric: 17.8278 - val_loss: 18.1487 - val_MinusLogProbMetric: 18.1487 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 351/1000
2023-10-23 19:06:53.354 
Epoch 351/1000 
	 loss: 17.9221, MinusLogProbMetric: 17.9221, val_loss: 17.7433, val_MinusLogProbMetric: 17.7433

Epoch 351: val_loss improved from 17.82505 to 17.74335, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 17.9221 - MinusLogProbMetric: 17.9221 - val_loss: 17.7433 - val_MinusLogProbMetric: 17.7433 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 352/1000
2023-10-23 19:07:57.512 
Epoch 352/1000 
	 loss: 17.8709, MinusLogProbMetric: 17.8709, val_loss: 18.3574, val_MinusLogProbMetric: 18.3574

Epoch 352: val_loss did not improve from 17.74335
196/196 - 63s - loss: 17.8709 - MinusLogProbMetric: 17.8709 - val_loss: 18.3574 - val_MinusLogProbMetric: 18.3574 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 353/1000
2023-10-23 19:08:59.971 
Epoch 353/1000 
	 loss: 17.8312, MinusLogProbMetric: 17.8312, val_loss: 17.8298, val_MinusLogProbMetric: 17.8298

Epoch 353: val_loss did not improve from 17.74335
196/196 - 62s - loss: 17.8312 - MinusLogProbMetric: 17.8312 - val_loss: 17.8298 - val_MinusLogProbMetric: 17.8298 - lr: 1.1111e-04 - 62s/epoch - 319ms/step
Epoch 354/1000
2023-10-23 19:10:04.381 
Epoch 354/1000 
	 loss: 17.8615, MinusLogProbMetric: 17.8615, val_loss: 18.4808, val_MinusLogProbMetric: 18.4808

Epoch 354: val_loss did not improve from 17.74335
196/196 - 64s - loss: 17.8615 - MinusLogProbMetric: 17.8615 - val_loss: 18.4808 - val_MinusLogProbMetric: 18.4808 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 355/1000
2023-10-23 19:11:07.419 
Epoch 355/1000 
	 loss: 17.8841, MinusLogProbMetric: 17.8841, val_loss: 17.9433, val_MinusLogProbMetric: 17.9433

Epoch 355: val_loss did not improve from 17.74335
196/196 - 63s - loss: 17.8841 - MinusLogProbMetric: 17.8841 - val_loss: 17.9433 - val_MinusLogProbMetric: 17.9433 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 356/1000
2023-10-23 19:12:10.052 
Epoch 356/1000 
	 loss: 17.8974, MinusLogProbMetric: 17.8974, val_loss: 18.3786, val_MinusLogProbMetric: 18.3786

Epoch 356: val_loss did not improve from 17.74335
196/196 - 63s - loss: 17.8974 - MinusLogProbMetric: 17.8974 - val_loss: 18.3786 - val_MinusLogProbMetric: 18.3786 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 357/1000
2023-10-23 19:13:13.282 
Epoch 357/1000 
	 loss: 17.9116, MinusLogProbMetric: 17.9116, val_loss: 17.9006, val_MinusLogProbMetric: 17.9006

Epoch 357: val_loss did not improve from 17.74335
196/196 - 63s - loss: 17.9116 - MinusLogProbMetric: 17.9116 - val_loss: 17.9006 - val_MinusLogProbMetric: 17.9006 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 358/1000
2023-10-23 19:14:16.862 
Epoch 358/1000 
	 loss: 17.8477, MinusLogProbMetric: 17.8477, val_loss: 17.9010, val_MinusLogProbMetric: 17.9010

Epoch 358: val_loss did not improve from 17.74335
196/196 - 64s - loss: 17.8477 - MinusLogProbMetric: 17.8477 - val_loss: 17.9010 - val_MinusLogProbMetric: 17.9010 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 359/1000
2023-10-23 19:15:19.129 
Epoch 359/1000 
	 loss: 17.8300, MinusLogProbMetric: 17.8300, val_loss: 18.1642, val_MinusLogProbMetric: 18.1642

Epoch 359: val_loss did not improve from 17.74335
196/196 - 62s - loss: 17.8300 - MinusLogProbMetric: 17.8300 - val_loss: 18.1642 - val_MinusLogProbMetric: 18.1642 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 360/1000
2023-10-23 19:16:21.115 
Epoch 360/1000 
	 loss: 17.8560, MinusLogProbMetric: 17.8560, val_loss: 17.8090, val_MinusLogProbMetric: 17.8090

Epoch 360: val_loss did not improve from 17.74335
196/196 - 62s - loss: 17.8560 - MinusLogProbMetric: 17.8560 - val_loss: 17.8090 - val_MinusLogProbMetric: 17.8090 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 361/1000
2023-10-23 19:17:24.278 
Epoch 361/1000 
	 loss: 17.8301, MinusLogProbMetric: 17.8301, val_loss: 17.9676, val_MinusLogProbMetric: 17.9676

Epoch 361: val_loss did not improve from 17.74335
196/196 - 63s - loss: 17.8301 - MinusLogProbMetric: 17.8301 - val_loss: 17.9676 - val_MinusLogProbMetric: 17.9676 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 362/1000
2023-10-23 19:18:28.500 
Epoch 362/1000 
	 loss: 17.7722, MinusLogProbMetric: 17.7722, val_loss: 17.7768, val_MinusLogProbMetric: 17.7768

Epoch 362: val_loss did not improve from 17.74335
196/196 - 64s - loss: 17.7722 - MinusLogProbMetric: 17.7722 - val_loss: 17.7768 - val_MinusLogProbMetric: 17.7768 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 363/1000
2023-10-23 19:19:31.939 
Epoch 363/1000 
	 loss: 17.8784, MinusLogProbMetric: 17.8784, val_loss: 17.8563, val_MinusLogProbMetric: 17.8563

Epoch 363: val_loss did not improve from 17.74335
196/196 - 63s - loss: 17.8784 - MinusLogProbMetric: 17.8784 - val_loss: 17.8563 - val_MinusLogProbMetric: 17.8563 - lr: 1.1111e-04 - 63s/epoch - 324ms/step
Epoch 364/1000
2023-10-23 19:20:35.874 
Epoch 364/1000 
	 loss: 17.8228, MinusLogProbMetric: 17.8228, val_loss: 17.9269, val_MinusLogProbMetric: 17.9269

Epoch 364: val_loss did not improve from 17.74335
196/196 - 64s - loss: 17.8228 - MinusLogProbMetric: 17.8228 - val_loss: 17.9269 - val_MinusLogProbMetric: 17.9269 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 365/1000
2023-10-23 19:21:40.311 
Epoch 365/1000 
	 loss: 17.8071, MinusLogProbMetric: 17.8071, val_loss: 17.8668, val_MinusLogProbMetric: 17.8668

Epoch 365: val_loss did not improve from 17.74335
196/196 - 64s - loss: 17.8071 - MinusLogProbMetric: 17.8071 - val_loss: 17.8668 - val_MinusLogProbMetric: 17.8668 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 366/1000
2023-10-23 19:22:43.867 
Epoch 366/1000 
	 loss: 17.7850, MinusLogProbMetric: 17.7850, val_loss: 17.9441, val_MinusLogProbMetric: 17.9441

Epoch 366: val_loss did not improve from 17.74335
196/196 - 64s - loss: 17.7850 - MinusLogProbMetric: 17.7850 - val_loss: 17.9441 - val_MinusLogProbMetric: 17.9441 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 367/1000
2023-10-23 19:23:47.966 
Epoch 367/1000 
	 loss: 18.3455, MinusLogProbMetric: 18.3455, val_loss: 18.2009, val_MinusLogProbMetric: 18.2009

Epoch 367: val_loss did not improve from 17.74335
196/196 - 64s - loss: 18.3455 - MinusLogProbMetric: 18.3455 - val_loss: 18.2009 - val_MinusLogProbMetric: 18.2009 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 368/1000
2023-10-23 19:24:51.218 
Epoch 368/1000 
	 loss: 17.8031, MinusLogProbMetric: 17.8031, val_loss: 17.8718, val_MinusLogProbMetric: 17.8718

Epoch 368: val_loss did not improve from 17.74335
196/196 - 63s - loss: 17.8031 - MinusLogProbMetric: 17.8031 - val_loss: 17.8718 - val_MinusLogProbMetric: 17.8718 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 369/1000
2023-10-23 19:25:54.761 
Epoch 369/1000 
	 loss: 17.8368, MinusLogProbMetric: 17.8368, val_loss: 17.8430, val_MinusLogProbMetric: 17.8430

Epoch 369: val_loss did not improve from 17.74335
196/196 - 64s - loss: 17.8368 - MinusLogProbMetric: 17.8368 - val_loss: 17.8430 - val_MinusLogProbMetric: 17.8430 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 370/1000
2023-10-23 19:26:57.061 
Epoch 370/1000 
	 loss: 17.8033, MinusLogProbMetric: 17.8033, val_loss: 17.8006, val_MinusLogProbMetric: 17.8006

Epoch 370: val_loss did not improve from 17.74335
196/196 - 62s - loss: 17.8033 - MinusLogProbMetric: 17.8033 - val_loss: 17.8006 - val_MinusLogProbMetric: 17.8006 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 371/1000
2023-10-23 19:28:01.122 
Epoch 371/1000 
	 loss: 17.7902, MinusLogProbMetric: 17.7902, val_loss: 17.8110, val_MinusLogProbMetric: 17.8110

Epoch 371: val_loss did not improve from 17.74335
196/196 - 64s - loss: 17.7902 - MinusLogProbMetric: 17.7902 - val_loss: 17.8110 - val_MinusLogProbMetric: 17.8110 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 372/1000
2023-10-23 19:29:03.766 
Epoch 372/1000 
	 loss: 17.9513, MinusLogProbMetric: 17.9513, val_loss: 17.8927, val_MinusLogProbMetric: 17.8927

Epoch 372: val_loss did not improve from 17.74335
196/196 - 63s - loss: 17.9513 - MinusLogProbMetric: 17.9513 - val_loss: 17.8927 - val_MinusLogProbMetric: 17.8927 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 373/1000
2023-10-23 19:30:07.220 
Epoch 373/1000 
	 loss: 17.8379, MinusLogProbMetric: 17.8379, val_loss: 17.9254, val_MinusLogProbMetric: 17.9254

Epoch 373: val_loss did not improve from 17.74335
196/196 - 63s - loss: 17.8379 - MinusLogProbMetric: 17.8379 - val_loss: 17.9254 - val_MinusLogProbMetric: 17.9254 - lr: 1.1111e-04 - 63s/epoch - 324ms/step
Epoch 374/1000
2023-10-23 19:31:09.985 
Epoch 374/1000 
	 loss: 17.7868, MinusLogProbMetric: 17.7868, val_loss: 18.1688, val_MinusLogProbMetric: 18.1688

Epoch 374: val_loss did not improve from 17.74335
196/196 - 63s - loss: 17.7868 - MinusLogProbMetric: 17.7868 - val_loss: 18.1688 - val_MinusLogProbMetric: 18.1688 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 375/1000
2023-10-23 19:32:12.671 
Epoch 375/1000 
	 loss: 17.8640, MinusLogProbMetric: 17.8640, val_loss: 17.8353, val_MinusLogProbMetric: 17.8353

Epoch 375: val_loss did not improve from 17.74335
196/196 - 63s - loss: 17.8640 - MinusLogProbMetric: 17.8640 - val_loss: 17.8353 - val_MinusLogProbMetric: 17.8353 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 376/1000
2023-10-23 19:33:15.601 
Epoch 376/1000 
	 loss: 17.7794, MinusLogProbMetric: 17.7794, val_loss: 17.9977, val_MinusLogProbMetric: 17.9977

Epoch 376: val_loss did not improve from 17.74335
196/196 - 63s - loss: 17.7794 - MinusLogProbMetric: 17.7794 - val_loss: 17.9977 - val_MinusLogProbMetric: 17.9977 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 377/1000
2023-10-23 19:34:19.321 
Epoch 377/1000 
	 loss: 17.7698, MinusLogProbMetric: 17.7698, val_loss: 17.8752, val_MinusLogProbMetric: 17.8752

Epoch 377: val_loss did not improve from 17.74335
196/196 - 64s - loss: 17.7698 - MinusLogProbMetric: 17.7698 - val_loss: 17.8752 - val_MinusLogProbMetric: 17.8752 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 378/1000
2023-10-23 19:35:21.526 
Epoch 378/1000 
	 loss: 17.9230, MinusLogProbMetric: 17.9230, val_loss: 20.5638, val_MinusLogProbMetric: 20.5638

Epoch 378: val_loss did not improve from 17.74335
196/196 - 62s - loss: 17.9230 - MinusLogProbMetric: 17.9230 - val_loss: 20.5638 - val_MinusLogProbMetric: 20.5638 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 379/1000
2023-10-23 19:36:24.300 
Epoch 379/1000 
	 loss: 17.9268, MinusLogProbMetric: 17.9268, val_loss: 17.9603, val_MinusLogProbMetric: 17.9603

Epoch 379: val_loss did not improve from 17.74335
196/196 - 63s - loss: 17.9268 - MinusLogProbMetric: 17.9268 - val_loss: 17.9603 - val_MinusLogProbMetric: 17.9603 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 380/1000
2023-10-23 19:37:26.126 
Epoch 380/1000 
	 loss: 17.8663, MinusLogProbMetric: 17.8663, val_loss: 17.7210, val_MinusLogProbMetric: 17.7210

Epoch 380: val_loss improved from 17.74335 to 17.72096, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 17.8663 - MinusLogProbMetric: 17.8663 - val_loss: 17.7210 - val_MinusLogProbMetric: 17.7210 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 381/1000
2023-10-23 19:38:30.855 
Epoch 381/1000 
	 loss: 17.7419, MinusLogProbMetric: 17.7419, val_loss: 17.7090, val_MinusLogProbMetric: 17.7090

Epoch 381: val_loss improved from 17.72096 to 17.70904, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 17.7419 - MinusLogProbMetric: 17.7419 - val_loss: 17.7090 - val_MinusLogProbMetric: 17.7090 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 382/1000
2023-10-23 19:39:35.034 
Epoch 382/1000 
	 loss: 17.7569, MinusLogProbMetric: 17.7569, val_loss: 18.1516, val_MinusLogProbMetric: 18.1516

Epoch 382: val_loss did not improve from 17.70904
196/196 - 63s - loss: 17.7569 - MinusLogProbMetric: 17.7569 - val_loss: 18.1516 - val_MinusLogProbMetric: 18.1516 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 383/1000
2023-10-23 19:40:38.633 
Epoch 383/1000 
	 loss: 17.7622, MinusLogProbMetric: 17.7622, val_loss: 17.7618, val_MinusLogProbMetric: 17.7618

Epoch 383: val_loss did not improve from 17.70904
196/196 - 64s - loss: 17.7622 - MinusLogProbMetric: 17.7622 - val_loss: 17.7618 - val_MinusLogProbMetric: 17.7618 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 384/1000
2023-10-23 19:41:42.278 
Epoch 384/1000 
	 loss: 17.7561, MinusLogProbMetric: 17.7561, val_loss: 17.7310, val_MinusLogProbMetric: 17.7310

Epoch 384: val_loss did not improve from 17.70904
196/196 - 64s - loss: 17.7561 - MinusLogProbMetric: 17.7561 - val_loss: 17.7310 - val_MinusLogProbMetric: 17.7310 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 385/1000
2023-10-23 19:42:47.424 
Epoch 385/1000 
	 loss: 17.7543, MinusLogProbMetric: 17.7543, val_loss: 17.8340, val_MinusLogProbMetric: 17.8340

Epoch 385: val_loss did not improve from 17.70904
196/196 - 65s - loss: 17.7543 - MinusLogProbMetric: 17.7543 - val_loss: 17.8340 - val_MinusLogProbMetric: 17.8340 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 386/1000
2023-10-23 19:43:51.570 
Epoch 386/1000 
	 loss: 17.8083, MinusLogProbMetric: 17.8083, val_loss: 17.7870, val_MinusLogProbMetric: 17.7870

Epoch 386: val_loss did not improve from 17.70904
196/196 - 64s - loss: 17.8083 - MinusLogProbMetric: 17.8083 - val_loss: 17.7870 - val_MinusLogProbMetric: 17.7870 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 387/1000
2023-10-23 19:44:53.711 
Epoch 387/1000 
	 loss: 17.7224, MinusLogProbMetric: 17.7224, val_loss: 17.7608, val_MinusLogProbMetric: 17.7608

Epoch 387: val_loss did not improve from 17.70904
196/196 - 62s - loss: 17.7224 - MinusLogProbMetric: 17.7224 - val_loss: 17.7608 - val_MinusLogProbMetric: 17.7608 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 388/1000
2023-10-23 19:45:57.281 
Epoch 388/1000 
	 loss: 18.7236, MinusLogProbMetric: 18.7236, val_loss: 17.9925, val_MinusLogProbMetric: 17.9925

Epoch 388: val_loss did not improve from 17.70904
196/196 - 64s - loss: 18.7236 - MinusLogProbMetric: 18.7236 - val_loss: 17.9925 - val_MinusLogProbMetric: 17.9925 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 389/1000
2023-10-23 19:47:00.260 
Epoch 389/1000 
	 loss: 17.7524, MinusLogProbMetric: 17.7524, val_loss: 17.9402, val_MinusLogProbMetric: 17.9402

Epoch 389: val_loss did not improve from 17.70904
196/196 - 63s - loss: 17.7524 - MinusLogProbMetric: 17.7524 - val_loss: 17.9402 - val_MinusLogProbMetric: 17.9402 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 390/1000
2023-10-23 19:48:04.008 
Epoch 390/1000 
	 loss: 17.7597, MinusLogProbMetric: 17.7597, val_loss: 17.9502, val_MinusLogProbMetric: 17.9502

Epoch 390: val_loss did not improve from 17.70904
196/196 - 64s - loss: 17.7597 - MinusLogProbMetric: 17.7597 - val_loss: 17.9502 - val_MinusLogProbMetric: 17.9502 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 391/1000
2023-10-23 19:49:08.018 
Epoch 391/1000 
	 loss: 17.7926, MinusLogProbMetric: 17.7926, val_loss: 17.9229, val_MinusLogProbMetric: 17.9229

Epoch 391: val_loss did not improve from 17.70904
196/196 - 64s - loss: 17.7926 - MinusLogProbMetric: 17.7926 - val_loss: 17.9229 - val_MinusLogProbMetric: 17.9229 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 392/1000
2023-10-23 19:50:11.906 
Epoch 392/1000 
	 loss: 17.7483, MinusLogProbMetric: 17.7483, val_loss: 17.8365, val_MinusLogProbMetric: 17.8365

Epoch 392: val_loss did not improve from 17.70904
196/196 - 64s - loss: 17.7483 - MinusLogProbMetric: 17.7483 - val_loss: 17.8365 - val_MinusLogProbMetric: 17.8365 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 393/1000
2023-10-23 19:51:17.160 
Epoch 393/1000 
	 loss: 17.8374, MinusLogProbMetric: 17.8374, val_loss: 18.2036, val_MinusLogProbMetric: 18.2036

Epoch 393: val_loss did not improve from 17.70904
196/196 - 65s - loss: 17.8374 - MinusLogProbMetric: 17.8374 - val_loss: 18.2036 - val_MinusLogProbMetric: 18.2036 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 394/1000
2023-10-23 19:52:19.719 
Epoch 394/1000 
	 loss: 17.7438, MinusLogProbMetric: 17.7438, val_loss: 17.8778, val_MinusLogProbMetric: 17.8778

Epoch 394: val_loss did not improve from 17.70904
196/196 - 63s - loss: 17.7438 - MinusLogProbMetric: 17.7438 - val_loss: 17.8778 - val_MinusLogProbMetric: 17.8778 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 395/1000
2023-10-23 19:53:24.039 
Epoch 395/1000 
	 loss: 17.7698, MinusLogProbMetric: 17.7698, val_loss: 17.8817, val_MinusLogProbMetric: 17.8817

Epoch 395: val_loss did not improve from 17.70904
196/196 - 64s - loss: 17.7698 - MinusLogProbMetric: 17.7698 - val_loss: 17.8817 - val_MinusLogProbMetric: 17.8817 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 396/1000
2023-10-23 19:54:27.154 
Epoch 396/1000 
	 loss: 17.7576, MinusLogProbMetric: 17.7576, val_loss: 17.7191, val_MinusLogProbMetric: 17.7191

Epoch 396: val_loss did not improve from 17.70904
196/196 - 63s - loss: 17.7576 - MinusLogProbMetric: 17.7576 - val_loss: 17.7191 - val_MinusLogProbMetric: 17.7191 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 397/1000
2023-10-23 19:55:33.597 
Epoch 397/1000 
	 loss: 17.9531, MinusLogProbMetric: 17.9531, val_loss: 18.0883, val_MinusLogProbMetric: 18.0883

Epoch 397: val_loss did not improve from 17.70904
196/196 - 66s - loss: 17.9531 - MinusLogProbMetric: 17.9531 - val_loss: 18.0883 - val_MinusLogProbMetric: 18.0883 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 398/1000
2023-10-23 19:56:37.173 
Epoch 398/1000 
	 loss: 17.7413, MinusLogProbMetric: 17.7413, val_loss: 17.8798, val_MinusLogProbMetric: 17.8798

Epoch 398: val_loss did not improve from 17.70904
196/196 - 64s - loss: 17.7413 - MinusLogProbMetric: 17.7413 - val_loss: 17.8798 - val_MinusLogProbMetric: 17.8798 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 399/1000
2023-10-23 19:57:40.482 
Epoch 399/1000 
	 loss: 17.8030, MinusLogProbMetric: 17.8030, val_loss: 17.6491, val_MinusLogProbMetric: 17.6491

Epoch 399: val_loss improved from 17.70904 to 17.64914, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 17.8030 - MinusLogProbMetric: 17.8030 - val_loss: 17.6491 - val_MinusLogProbMetric: 17.6491 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 400/1000
2023-10-23 19:58:42.911 
Epoch 400/1000 
	 loss: 17.7159, MinusLogProbMetric: 17.7159, val_loss: 18.0585, val_MinusLogProbMetric: 18.0585

Epoch 400: val_loss did not improve from 17.64914
196/196 - 61s - loss: 17.7159 - MinusLogProbMetric: 17.7159 - val_loss: 18.0585 - val_MinusLogProbMetric: 18.0585 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 401/1000
2023-10-23 19:59:44.848 
Epoch 401/1000 
	 loss: 17.8088, MinusLogProbMetric: 17.8088, val_loss: 18.0914, val_MinusLogProbMetric: 18.0914

Epoch 401: val_loss did not improve from 17.64914
196/196 - 62s - loss: 17.8088 - MinusLogProbMetric: 17.8088 - val_loss: 18.0914 - val_MinusLogProbMetric: 18.0914 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 402/1000
2023-10-23 20:00:47.928 
Epoch 402/1000 
	 loss: 17.7289, MinusLogProbMetric: 17.7289, val_loss: 17.6584, val_MinusLogProbMetric: 17.6584

Epoch 402: val_loss did not improve from 17.64914
196/196 - 63s - loss: 17.7289 - MinusLogProbMetric: 17.7289 - val_loss: 17.6584 - val_MinusLogProbMetric: 17.6584 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 403/1000
2023-10-23 20:01:51.459 
Epoch 403/1000 
	 loss: 17.7246, MinusLogProbMetric: 17.7246, val_loss: 17.6900, val_MinusLogProbMetric: 17.6900

Epoch 403: val_loss did not improve from 17.64914
196/196 - 64s - loss: 17.7246 - MinusLogProbMetric: 17.7246 - val_loss: 17.6900 - val_MinusLogProbMetric: 17.6900 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 404/1000
2023-10-23 20:02:55.922 
Epoch 404/1000 
	 loss: 17.9509, MinusLogProbMetric: 17.9509, val_loss: 18.0106, val_MinusLogProbMetric: 18.0106

Epoch 404: val_loss did not improve from 17.64914
196/196 - 64s - loss: 17.9509 - MinusLogProbMetric: 17.9509 - val_loss: 18.0106 - val_MinusLogProbMetric: 18.0106 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 405/1000
2023-10-23 20:04:00.361 
Epoch 405/1000 
	 loss: 17.7470, MinusLogProbMetric: 17.7470, val_loss: 17.7743, val_MinusLogProbMetric: 17.7743

Epoch 405: val_loss did not improve from 17.64914
196/196 - 64s - loss: 17.7470 - MinusLogProbMetric: 17.7470 - val_loss: 17.7743 - val_MinusLogProbMetric: 17.7743 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 406/1000
2023-10-23 20:05:03.902 
Epoch 406/1000 
	 loss: 17.7334, MinusLogProbMetric: 17.7334, val_loss: 17.6674, val_MinusLogProbMetric: 17.6674

Epoch 406: val_loss did not improve from 17.64914
196/196 - 64s - loss: 17.7334 - MinusLogProbMetric: 17.7334 - val_loss: 17.6674 - val_MinusLogProbMetric: 17.6674 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 407/1000
2023-10-23 20:06:07.931 
Epoch 407/1000 
	 loss: 17.7249, MinusLogProbMetric: 17.7249, val_loss: 17.7979, val_MinusLogProbMetric: 17.7979

Epoch 407: val_loss did not improve from 17.64914
196/196 - 64s - loss: 17.7249 - MinusLogProbMetric: 17.7249 - val_loss: 17.7979 - val_MinusLogProbMetric: 17.7979 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 408/1000
2023-10-23 20:07:12.241 
Epoch 408/1000 
	 loss: 17.7353, MinusLogProbMetric: 17.7353, val_loss: 18.0494, val_MinusLogProbMetric: 18.0494

Epoch 408: val_loss did not improve from 17.64914
196/196 - 64s - loss: 17.7353 - MinusLogProbMetric: 17.7353 - val_loss: 18.0494 - val_MinusLogProbMetric: 18.0494 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 409/1000
2023-10-23 20:08:16.764 
Epoch 409/1000 
	 loss: 17.7274, MinusLogProbMetric: 17.7274, val_loss: 18.1590, val_MinusLogProbMetric: 18.1590

Epoch 409: val_loss did not improve from 17.64914
196/196 - 65s - loss: 17.7274 - MinusLogProbMetric: 17.7274 - val_loss: 18.1590 - val_MinusLogProbMetric: 18.1590 - lr: 1.1111e-04 - 65s/epoch - 329ms/step
Epoch 410/1000
2023-10-23 20:09:21.433 
Epoch 410/1000 
	 loss: 17.7816, MinusLogProbMetric: 17.7816, val_loss: 17.6998, val_MinusLogProbMetric: 17.6998

Epoch 410: val_loss did not improve from 17.64914
196/196 - 65s - loss: 17.7816 - MinusLogProbMetric: 17.7816 - val_loss: 17.6998 - val_MinusLogProbMetric: 17.6998 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 411/1000
2023-10-23 20:10:23.932 
Epoch 411/1000 
	 loss: 17.7234, MinusLogProbMetric: 17.7234, val_loss: 17.7972, val_MinusLogProbMetric: 17.7972

Epoch 411: val_loss did not improve from 17.64914
196/196 - 62s - loss: 17.7234 - MinusLogProbMetric: 17.7234 - val_loss: 17.7972 - val_MinusLogProbMetric: 17.7972 - lr: 1.1111e-04 - 62s/epoch - 319ms/step
Epoch 412/1000
2023-10-23 20:11:27.271 
Epoch 412/1000 
	 loss: 17.6974, MinusLogProbMetric: 17.6974, val_loss: 17.7977, val_MinusLogProbMetric: 17.7977

Epoch 412: val_loss did not improve from 17.64914
196/196 - 63s - loss: 17.6974 - MinusLogProbMetric: 17.6974 - val_loss: 17.7977 - val_MinusLogProbMetric: 17.7977 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 413/1000
2023-10-23 20:12:30.337 
Epoch 413/1000 
	 loss: 17.6876, MinusLogProbMetric: 17.6876, val_loss: 17.7883, val_MinusLogProbMetric: 17.7883

Epoch 413: val_loss did not improve from 17.64914
196/196 - 63s - loss: 17.6876 - MinusLogProbMetric: 17.6876 - val_loss: 17.7883 - val_MinusLogProbMetric: 17.7883 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 414/1000
2023-10-23 20:13:32.850 
Epoch 414/1000 
	 loss: 17.7868, MinusLogProbMetric: 17.7868, val_loss: 18.1146, val_MinusLogProbMetric: 18.1146

Epoch 414: val_loss did not improve from 17.64914
196/196 - 63s - loss: 17.7868 - MinusLogProbMetric: 17.7868 - val_loss: 18.1146 - val_MinusLogProbMetric: 18.1146 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 415/1000
2023-10-23 20:14:36.265 
Epoch 415/1000 
	 loss: 17.7088, MinusLogProbMetric: 17.7088, val_loss: 17.7661, val_MinusLogProbMetric: 17.7661

Epoch 415: val_loss did not improve from 17.64914
196/196 - 63s - loss: 17.7088 - MinusLogProbMetric: 17.7088 - val_loss: 17.7661 - val_MinusLogProbMetric: 17.7661 - lr: 1.1111e-04 - 63s/epoch - 324ms/step
Epoch 416/1000
2023-10-23 20:15:40.082 
Epoch 416/1000 
	 loss: 17.7773, MinusLogProbMetric: 17.7773, val_loss: 17.9189, val_MinusLogProbMetric: 17.9189

Epoch 416: val_loss did not improve from 17.64914
196/196 - 64s - loss: 17.7773 - MinusLogProbMetric: 17.7773 - val_loss: 17.9189 - val_MinusLogProbMetric: 17.9189 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 417/1000
2023-10-23 20:16:44.719 
Epoch 417/1000 
	 loss: 17.7366, MinusLogProbMetric: 17.7366, val_loss: 17.7904, val_MinusLogProbMetric: 17.7904

Epoch 417: val_loss did not improve from 17.64914
196/196 - 65s - loss: 17.7366 - MinusLogProbMetric: 17.7366 - val_loss: 17.7904 - val_MinusLogProbMetric: 17.7904 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 418/1000
2023-10-23 20:17:48.360 
Epoch 418/1000 
	 loss: 17.6866, MinusLogProbMetric: 17.6866, val_loss: 17.6833, val_MinusLogProbMetric: 17.6833

Epoch 418: val_loss did not improve from 17.64914
196/196 - 64s - loss: 17.6866 - MinusLogProbMetric: 17.6866 - val_loss: 17.6833 - val_MinusLogProbMetric: 17.6833 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 419/1000
2023-10-23 20:18:52.112 
Epoch 419/1000 
	 loss: 17.6667, MinusLogProbMetric: 17.6667, val_loss: 17.9683, val_MinusLogProbMetric: 17.9683

Epoch 419: val_loss did not improve from 17.64914
196/196 - 64s - loss: 17.6667 - MinusLogProbMetric: 17.6667 - val_loss: 17.9683 - val_MinusLogProbMetric: 17.9683 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 420/1000
2023-10-23 20:19:56.341 
Epoch 420/1000 
	 loss: 17.7086, MinusLogProbMetric: 17.7086, val_loss: 18.0530, val_MinusLogProbMetric: 18.0530

Epoch 420: val_loss did not improve from 17.64914
196/196 - 64s - loss: 17.7086 - MinusLogProbMetric: 17.7086 - val_loss: 18.0530 - val_MinusLogProbMetric: 18.0530 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 421/1000
2023-10-23 20:21:00.655 
Epoch 421/1000 
	 loss: 17.7531, MinusLogProbMetric: 17.7531, val_loss: 17.8326, val_MinusLogProbMetric: 17.8326

Epoch 421: val_loss did not improve from 17.64914
196/196 - 64s - loss: 17.7531 - MinusLogProbMetric: 17.7531 - val_loss: 17.8326 - val_MinusLogProbMetric: 17.8326 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 422/1000
2023-10-23 20:22:04.245 
Epoch 422/1000 
	 loss: 17.6968, MinusLogProbMetric: 17.6968, val_loss: 17.7757, val_MinusLogProbMetric: 17.7757

Epoch 422: val_loss did not improve from 17.64914
196/196 - 64s - loss: 17.6968 - MinusLogProbMetric: 17.6968 - val_loss: 17.7757 - val_MinusLogProbMetric: 17.7757 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 423/1000
2023-10-23 20:23:07.880 
Epoch 423/1000 
	 loss: 17.7550, MinusLogProbMetric: 17.7550, val_loss: 17.6368, val_MinusLogProbMetric: 17.6368

Epoch 423: val_loss improved from 17.64914 to 17.63685, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 17.7550 - MinusLogProbMetric: 17.7550 - val_loss: 17.6368 - val_MinusLogProbMetric: 17.6368 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 424/1000
2023-10-23 20:24:11.083 
Epoch 424/1000 
	 loss: 17.6874, MinusLogProbMetric: 17.6874, val_loss: 17.6886, val_MinusLogProbMetric: 17.6886

Epoch 424: val_loss did not improve from 17.63685
196/196 - 62s - loss: 17.6874 - MinusLogProbMetric: 17.6874 - val_loss: 17.6886 - val_MinusLogProbMetric: 17.6886 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 425/1000
2023-10-23 20:25:13.399 
Epoch 425/1000 
	 loss: 17.6960, MinusLogProbMetric: 17.6960, val_loss: 17.9060, val_MinusLogProbMetric: 17.9060

Epoch 425: val_loss did not improve from 17.63685
196/196 - 62s - loss: 17.6960 - MinusLogProbMetric: 17.6960 - val_loss: 17.9060 - val_MinusLogProbMetric: 17.9060 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 426/1000
2023-10-23 20:26:16.141 
Epoch 426/1000 
	 loss: 17.7034, MinusLogProbMetric: 17.7034, val_loss: 17.7237, val_MinusLogProbMetric: 17.7237

Epoch 426: val_loss did not improve from 17.63685
196/196 - 63s - loss: 17.7034 - MinusLogProbMetric: 17.7034 - val_loss: 17.7237 - val_MinusLogProbMetric: 17.7237 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 427/1000
2023-10-23 20:27:19.396 
Epoch 427/1000 
	 loss: 17.7137, MinusLogProbMetric: 17.7137, val_loss: 17.8541, val_MinusLogProbMetric: 17.8541

Epoch 427: val_loss did not improve from 17.63685
196/196 - 63s - loss: 17.7137 - MinusLogProbMetric: 17.7137 - val_loss: 17.8541 - val_MinusLogProbMetric: 17.8541 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 428/1000
2023-10-23 20:28:24.442 
Epoch 428/1000 
	 loss: 17.8164, MinusLogProbMetric: 17.8164, val_loss: 17.8412, val_MinusLogProbMetric: 17.8412

Epoch 428: val_loss did not improve from 17.63685
196/196 - 65s - loss: 17.8164 - MinusLogProbMetric: 17.8164 - val_loss: 17.8412 - val_MinusLogProbMetric: 17.8412 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 429/1000
2023-10-23 20:29:28.148 
Epoch 429/1000 
	 loss: 17.7525, MinusLogProbMetric: 17.7525, val_loss: 17.8797, val_MinusLogProbMetric: 17.8797

Epoch 429: val_loss did not improve from 17.63685
196/196 - 64s - loss: 17.7525 - MinusLogProbMetric: 17.7525 - val_loss: 17.8797 - val_MinusLogProbMetric: 17.8797 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 430/1000
2023-10-23 20:30:31.962 
Epoch 430/1000 
	 loss: 17.7027, MinusLogProbMetric: 17.7027, val_loss: 17.7993, val_MinusLogProbMetric: 17.7993

Epoch 430: val_loss did not improve from 17.63685
196/196 - 64s - loss: 17.7027 - MinusLogProbMetric: 17.7027 - val_loss: 17.7993 - val_MinusLogProbMetric: 17.7993 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 431/1000
2023-10-23 20:31:36.078 
Epoch 431/1000 
	 loss: 17.6927, MinusLogProbMetric: 17.6927, val_loss: 17.9092, val_MinusLogProbMetric: 17.9092

Epoch 431: val_loss did not improve from 17.63685
196/196 - 64s - loss: 17.6927 - MinusLogProbMetric: 17.6927 - val_loss: 17.9092 - val_MinusLogProbMetric: 17.9092 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 432/1000
2023-10-23 20:32:39.453 
Epoch 432/1000 
	 loss: 17.6303, MinusLogProbMetric: 17.6303, val_loss: 17.7501, val_MinusLogProbMetric: 17.7501

Epoch 432: val_loss did not improve from 17.63685
196/196 - 63s - loss: 17.6303 - MinusLogProbMetric: 17.6303 - val_loss: 17.7501 - val_MinusLogProbMetric: 17.7501 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 433/1000
2023-10-23 20:33:44.114 
Epoch 433/1000 
	 loss: 17.6831, MinusLogProbMetric: 17.6831, val_loss: 17.9052, val_MinusLogProbMetric: 17.9052

Epoch 433: val_loss did not improve from 17.63685
196/196 - 65s - loss: 17.6831 - MinusLogProbMetric: 17.6831 - val_loss: 17.9052 - val_MinusLogProbMetric: 17.9052 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 434/1000
2023-10-23 20:34:46.522 
Epoch 434/1000 
	 loss: 17.8657, MinusLogProbMetric: 17.8657, val_loss: 17.8681, val_MinusLogProbMetric: 17.8681

Epoch 434: val_loss did not improve from 17.63685
196/196 - 62s - loss: 17.8657 - MinusLogProbMetric: 17.8657 - val_loss: 17.8681 - val_MinusLogProbMetric: 17.8681 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 435/1000
2023-10-23 20:35:49.526 
Epoch 435/1000 
	 loss: 17.6808, MinusLogProbMetric: 17.6808, val_loss: 17.6557, val_MinusLogProbMetric: 17.6557

Epoch 435: val_loss did not improve from 17.63685
196/196 - 63s - loss: 17.6808 - MinusLogProbMetric: 17.6808 - val_loss: 17.6557 - val_MinusLogProbMetric: 17.6557 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 436/1000
2023-10-23 20:36:52.280 
Epoch 436/1000 
	 loss: 17.6661, MinusLogProbMetric: 17.6661, val_loss: 18.0314, val_MinusLogProbMetric: 18.0314

Epoch 436: val_loss did not improve from 17.63685
196/196 - 63s - loss: 17.6661 - MinusLogProbMetric: 17.6661 - val_loss: 18.0314 - val_MinusLogProbMetric: 18.0314 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 437/1000
2023-10-23 20:37:58.126 
Epoch 437/1000 
	 loss: 17.6789, MinusLogProbMetric: 17.6789, val_loss: 17.5065, val_MinusLogProbMetric: 17.5065

Epoch 437: val_loss improved from 17.63685 to 17.50645, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 17.6789 - MinusLogProbMetric: 17.6789 - val_loss: 17.5065 - val_MinusLogProbMetric: 17.5065 - lr: 1.1111e-04 - 67s/epoch - 341ms/step
Epoch 438/1000
2023-10-23 20:39:06.331 
Epoch 438/1000 
	 loss: 17.6098, MinusLogProbMetric: 17.6098, val_loss: 17.7387, val_MinusLogProbMetric: 17.7387

Epoch 438: val_loss did not improve from 17.50645
196/196 - 67s - loss: 17.6098 - MinusLogProbMetric: 17.6098 - val_loss: 17.7387 - val_MinusLogProbMetric: 17.7387 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 439/1000
2023-10-23 20:40:13.364 
Epoch 439/1000 
	 loss: 17.6421, MinusLogProbMetric: 17.6421, val_loss: 17.6519, val_MinusLogProbMetric: 17.6519

Epoch 439: val_loss did not improve from 17.50645
196/196 - 67s - loss: 17.6421 - MinusLogProbMetric: 17.6421 - val_loss: 17.6519 - val_MinusLogProbMetric: 17.6519 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 440/1000
2023-10-23 20:41:17.859 
Epoch 440/1000 
	 loss: 17.6520, MinusLogProbMetric: 17.6520, val_loss: 17.6157, val_MinusLogProbMetric: 17.6157

Epoch 440: val_loss did not improve from 17.50645
196/196 - 64s - loss: 17.6520 - MinusLogProbMetric: 17.6520 - val_loss: 17.6157 - val_MinusLogProbMetric: 17.6157 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 441/1000
2023-10-23 20:42:23.362 
Epoch 441/1000 
	 loss: 17.6606, MinusLogProbMetric: 17.6606, val_loss: 17.8864, val_MinusLogProbMetric: 17.8864

Epoch 441: val_loss did not improve from 17.50645
196/196 - 65s - loss: 17.6606 - MinusLogProbMetric: 17.6606 - val_loss: 17.8864 - val_MinusLogProbMetric: 17.8864 - lr: 1.1111e-04 - 65s/epoch - 334ms/step
Epoch 442/1000
2023-10-23 20:43:28.394 
Epoch 442/1000 
	 loss: 17.8291, MinusLogProbMetric: 17.8291, val_loss: 17.8448, val_MinusLogProbMetric: 17.8448

Epoch 442: val_loss did not improve from 17.50645
196/196 - 65s - loss: 17.8291 - MinusLogProbMetric: 17.8291 - val_loss: 17.8448 - val_MinusLogProbMetric: 17.8448 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 443/1000
2023-10-23 20:44:30.178 
Epoch 443/1000 
	 loss: 17.6534, MinusLogProbMetric: 17.6534, val_loss: 17.7228, val_MinusLogProbMetric: 17.7228

Epoch 443: val_loss did not improve from 17.50645
196/196 - 62s - loss: 17.6534 - MinusLogProbMetric: 17.6534 - val_loss: 17.7228 - val_MinusLogProbMetric: 17.7228 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 444/1000
2023-10-23 20:45:34.579 
Epoch 444/1000 
	 loss: 17.7126, MinusLogProbMetric: 17.7126, val_loss: 17.5191, val_MinusLogProbMetric: 17.5191

Epoch 444: val_loss did not improve from 17.50645
196/196 - 64s - loss: 17.7126 - MinusLogProbMetric: 17.7126 - val_loss: 17.5191 - val_MinusLogProbMetric: 17.5191 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 445/1000
2023-10-23 20:46:41.055 
Epoch 445/1000 
	 loss: 17.6862, MinusLogProbMetric: 17.6862, val_loss: 17.9480, val_MinusLogProbMetric: 17.9480

Epoch 445: val_loss did not improve from 17.50645
196/196 - 66s - loss: 17.6862 - MinusLogProbMetric: 17.6862 - val_loss: 17.9480 - val_MinusLogProbMetric: 17.9480 - lr: 1.1111e-04 - 66s/epoch - 339ms/step
Epoch 446/1000
2023-10-23 20:47:45.893 
Epoch 446/1000 
	 loss: 17.6502, MinusLogProbMetric: 17.6502, val_loss: 17.6424, val_MinusLogProbMetric: 17.6424

Epoch 446: val_loss did not improve from 17.50645
196/196 - 65s - loss: 17.6502 - MinusLogProbMetric: 17.6502 - val_loss: 17.6424 - val_MinusLogProbMetric: 17.6424 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 447/1000
2023-10-23 20:48:49.841 
Epoch 447/1000 
	 loss: 17.6189, MinusLogProbMetric: 17.6189, val_loss: 17.8039, val_MinusLogProbMetric: 17.8039

Epoch 447: val_loss did not improve from 17.50645
196/196 - 64s - loss: 17.6189 - MinusLogProbMetric: 17.6189 - val_loss: 17.8039 - val_MinusLogProbMetric: 17.8039 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 448/1000
2023-10-23 20:49:56.174 
Epoch 448/1000 
	 loss: 17.6209, MinusLogProbMetric: 17.6209, val_loss: 17.5756, val_MinusLogProbMetric: 17.5756

Epoch 448: val_loss did not improve from 17.50645
196/196 - 66s - loss: 17.6209 - MinusLogProbMetric: 17.6209 - val_loss: 17.5756 - val_MinusLogProbMetric: 17.5756 - lr: 1.1111e-04 - 66s/epoch - 338ms/step
Epoch 449/1000
2023-10-23 20:51:00.988 
Epoch 449/1000 
	 loss: 17.6074, MinusLogProbMetric: 17.6074, val_loss: 17.4642, val_MinusLogProbMetric: 17.4642

Epoch 449: val_loss improved from 17.50645 to 17.46418, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 66s - loss: 17.6074 - MinusLogProbMetric: 17.6074 - val_loss: 17.4642 - val_MinusLogProbMetric: 17.4642 - lr: 1.1111e-04 - 66s/epoch - 337ms/step
Epoch 450/1000
2023-10-23 20:52:05.365 
Epoch 450/1000 
	 loss: 17.6338, MinusLogProbMetric: 17.6338, val_loss: 17.8191, val_MinusLogProbMetric: 17.8191

Epoch 450: val_loss did not improve from 17.46418
196/196 - 63s - loss: 17.6338 - MinusLogProbMetric: 17.6338 - val_loss: 17.8191 - val_MinusLogProbMetric: 17.8191 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 451/1000
2023-10-23 20:53:09.992 
Epoch 451/1000 
	 loss: 17.6385, MinusLogProbMetric: 17.6385, val_loss: 17.9354, val_MinusLogProbMetric: 17.9354

Epoch 451: val_loss did not improve from 17.46418
196/196 - 65s - loss: 17.6385 - MinusLogProbMetric: 17.6385 - val_loss: 17.9354 - val_MinusLogProbMetric: 17.9354 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 452/1000
2023-10-23 20:54:13.281 
Epoch 452/1000 
	 loss: 17.7234, MinusLogProbMetric: 17.7234, val_loss: 17.8407, val_MinusLogProbMetric: 17.8407

Epoch 452: val_loss did not improve from 17.46418
196/196 - 63s - loss: 17.7234 - MinusLogProbMetric: 17.7234 - val_loss: 17.8407 - val_MinusLogProbMetric: 17.8407 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 453/1000
2023-10-23 20:55:15.937 
Epoch 453/1000 
	 loss: 17.6513, MinusLogProbMetric: 17.6513, val_loss: 17.8327, val_MinusLogProbMetric: 17.8327

Epoch 453: val_loss did not improve from 17.46418
196/196 - 63s - loss: 17.6513 - MinusLogProbMetric: 17.6513 - val_loss: 17.8327 - val_MinusLogProbMetric: 17.8327 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 454/1000
2023-10-23 20:56:21.129 
Epoch 454/1000 
	 loss: 17.6718, MinusLogProbMetric: 17.6718, val_loss: 18.0066, val_MinusLogProbMetric: 18.0066

Epoch 454: val_loss did not improve from 17.46418
196/196 - 65s - loss: 17.6718 - MinusLogProbMetric: 17.6718 - val_loss: 18.0066 - val_MinusLogProbMetric: 18.0066 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 455/1000
2023-10-23 20:57:25.613 
Epoch 455/1000 
	 loss: 17.6290, MinusLogProbMetric: 17.6290, val_loss: 18.0947, val_MinusLogProbMetric: 18.0947

Epoch 455: val_loss did not improve from 17.46418
196/196 - 64s - loss: 17.6290 - MinusLogProbMetric: 17.6290 - val_loss: 18.0947 - val_MinusLogProbMetric: 18.0947 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 456/1000
2023-10-23 20:58:30.307 
Epoch 456/1000 
	 loss: 17.6928, MinusLogProbMetric: 17.6928, val_loss: 17.7176, val_MinusLogProbMetric: 17.7176

Epoch 456: val_loss did not improve from 17.46418
196/196 - 65s - loss: 17.6928 - MinusLogProbMetric: 17.6928 - val_loss: 17.7176 - val_MinusLogProbMetric: 17.7176 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 457/1000
2023-10-23 20:59:37.365 
Epoch 457/1000 
	 loss: 17.6228, MinusLogProbMetric: 17.6228, val_loss: 17.6119, val_MinusLogProbMetric: 17.6119

Epoch 457: val_loss did not improve from 17.46418
196/196 - 67s - loss: 17.6228 - MinusLogProbMetric: 17.6228 - val_loss: 17.6119 - val_MinusLogProbMetric: 17.6119 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 458/1000
2023-10-23 21:00:38.209 
Epoch 458/1000 
	 loss: 17.7026, MinusLogProbMetric: 17.7026, val_loss: 17.8345, val_MinusLogProbMetric: 17.8345

Epoch 458: val_loss did not improve from 17.46418
196/196 - 61s - loss: 17.7026 - MinusLogProbMetric: 17.7026 - val_loss: 17.8345 - val_MinusLogProbMetric: 17.8345 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 459/1000
2023-10-23 21:01:42.103 
Epoch 459/1000 
	 loss: 17.6706, MinusLogProbMetric: 17.6706, val_loss: 17.6632, val_MinusLogProbMetric: 17.6632

Epoch 459: val_loss did not improve from 17.46418
196/196 - 64s - loss: 17.6706 - MinusLogProbMetric: 17.6706 - val_loss: 17.6632 - val_MinusLogProbMetric: 17.6632 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 460/1000
2023-10-23 21:02:44.914 
Epoch 460/1000 
	 loss: 17.5992, MinusLogProbMetric: 17.5992, val_loss: 17.5637, val_MinusLogProbMetric: 17.5637

Epoch 460: val_loss did not improve from 17.46418
196/196 - 63s - loss: 17.5992 - MinusLogProbMetric: 17.5992 - val_loss: 17.5637 - val_MinusLogProbMetric: 17.5637 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 461/1000
2023-10-23 21:03:52.182 
Epoch 461/1000 
	 loss: 17.6472, MinusLogProbMetric: 17.6472, val_loss: 17.6876, val_MinusLogProbMetric: 17.6876

Epoch 461: val_loss did not improve from 17.46418
196/196 - 67s - loss: 17.6472 - MinusLogProbMetric: 17.6472 - val_loss: 17.6876 - val_MinusLogProbMetric: 17.6876 - lr: 1.1111e-04 - 67s/epoch - 343ms/step
Epoch 462/1000
2023-10-23 21:04:57.515 
Epoch 462/1000 
	 loss: 17.5898, MinusLogProbMetric: 17.5898, val_loss: 17.9187, val_MinusLogProbMetric: 17.9187

Epoch 462: val_loss did not improve from 17.46418
196/196 - 65s - loss: 17.5898 - MinusLogProbMetric: 17.5898 - val_loss: 17.9187 - val_MinusLogProbMetric: 17.9187 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 463/1000
2023-10-23 21:06:01.189 
Epoch 463/1000 
	 loss: 17.8343, MinusLogProbMetric: 17.8343, val_loss: 17.6553, val_MinusLogProbMetric: 17.6553

Epoch 463: val_loss did not improve from 17.46418
196/196 - 64s - loss: 17.8343 - MinusLogProbMetric: 17.8343 - val_loss: 17.6553 - val_MinusLogProbMetric: 17.6553 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 464/1000
2023-10-23 21:07:05.616 
Epoch 464/1000 
	 loss: 17.6101, MinusLogProbMetric: 17.6101, val_loss: 17.6107, val_MinusLogProbMetric: 17.6107

Epoch 464: val_loss did not improve from 17.46418
196/196 - 64s - loss: 17.6101 - MinusLogProbMetric: 17.6101 - val_loss: 17.6107 - val_MinusLogProbMetric: 17.6107 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 465/1000
2023-10-23 21:08:10.316 
Epoch 465/1000 
	 loss: 17.6956, MinusLogProbMetric: 17.6956, val_loss: 17.6424, val_MinusLogProbMetric: 17.6424

Epoch 465: val_loss did not improve from 17.46418
196/196 - 65s - loss: 17.6956 - MinusLogProbMetric: 17.6956 - val_loss: 17.6424 - val_MinusLogProbMetric: 17.6424 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 466/1000
2023-10-23 21:09:15.791 
Epoch 466/1000 
	 loss: 17.6138, MinusLogProbMetric: 17.6138, val_loss: 17.7225, val_MinusLogProbMetric: 17.7225

Epoch 466: val_loss did not improve from 17.46418
196/196 - 65s - loss: 17.6138 - MinusLogProbMetric: 17.6138 - val_loss: 17.7225 - val_MinusLogProbMetric: 17.7225 - lr: 1.1111e-04 - 65s/epoch - 334ms/step
Epoch 467/1000
2023-10-23 21:10:20.212 
Epoch 467/1000 
	 loss: 17.6043, MinusLogProbMetric: 17.6043, val_loss: 17.7178, val_MinusLogProbMetric: 17.7178

Epoch 467: val_loss did not improve from 17.46418
196/196 - 64s - loss: 17.6043 - MinusLogProbMetric: 17.6043 - val_loss: 17.7178 - val_MinusLogProbMetric: 17.7178 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 468/1000
2023-10-23 21:11:26.073 
Epoch 468/1000 
	 loss: 17.6133, MinusLogProbMetric: 17.6133, val_loss: 17.8212, val_MinusLogProbMetric: 17.8212

Epoch 468: val_loss did not improve from 17.46418
196/196 - 66s - loss: 17.6133 - MinusLogProbMetric: 17.6133 - val_loss: 17.8212 - val_MinusLogProbMetric: 17.8212 - lr: 1.1111e-04 - 66s/epoch - 336ms/step
Epoch 469/1000
2023-10-23 21:12:31.290 
Epoch 469/1000 
	 loss: 17.6288, MinusLogProbMetric: 17.6288, val_loss: 17.6139, val_MinusLogProbMetric: 17.6139

Epoch 469: val_loss did not improve from 17.46418
196/196 - 65s - loss: 17.6288 - MinusLogProbMetric: 17.6288 - val_loss: 17.6139 - val_MinusLogProbMetric: 17.6139 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 470/1000
2023-10-23 21:13:35.681 
Epoch 470/1000 
	 loss: 17.6022, MinusLogProbMetric: 17.6022, val_loss: 17.6084, val_MinusLogProbMetric: 17.6084

Epoch 470: val_loss did not improve from 17.46418
196/196 - 64s - loss: 17.6022 - MinusLogProbMetric: 17.6022 - val_loss: 17.6084 - val_MinusLogProbMetric: 17.6084 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 471/1000
2023-10-23 21:14:38.186 
Epoch 471/1000 
	 loss: 17.5826, MinusLogProbMetric: 17.5826, val_loss: 17.6431, val_MinusLogProbMetric: 17.6431

Epoch 471: val_loss did not improve from 17.46418
196/196 - 63s - loss: 17.5826 - MinusLogProbMetric: 17.5826 - val_loss: 17.6431 - val_MinusLogProbMetric: 17.6431 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 472/1000
2023-10-23 21:15:41.559 
Epoch 472/1000 
	 loss: 17.5971, MinusLogProbMetric: 17.5971, val_loss: 17.9435, val_MinusLogProbMetric: 17.9435

Epoch 472: val_loss did not improve from 17.46418
196/196 - 63s - loss: 17.5971 - MinusLogProbMetric: 17.5971 - val_loss: 17.9435 - val_MinusLogProbMetric: 17.9435 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 473/1000
2023-10-23 21:16:42.413 
Epoch 473/1000 
	 loss: 17.5960, MinusLogProbMetric: 17.5960, val_loss: 17.5740, val_MinusLogProbMetric: 17.5740

Epoch 473: val_loss did not improve from 17.46418
196/196 - 61s - loss: 17.5960 - MinusLogProbMetric: 17.5960 - val_loss: 17.5740 - val_MinusLogProbMetric: 17.5740 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 474/1000
2023-10-23 21:17:44.399 
Epoch 474/1000 
	 loss: 17.5829, MinusLogProbMetric: 17.5829, val_loss: 17.4989, val_MinusLogProbMetric: 17.4989

Epoch 474: val_loss did not improve from 17.46418
196/196 - 62s - loss: 17.5829 - MinusLogProbMetric: 17.5829 - val_loss: 17.4989 - val_MinusLogProbMetric: 17.4989 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 475/1000
2023-10-23 21:18:46.023 
Epoch 475/1000 
	 loss: 17.6233, MinusLogProbMetric: 17.6233, val_loss: 17.7049, val_MinusLogProbMetric: 17.7049

Epoch 475: val_loss did not improve from 17.46418
196/196 - 62s - loss: 17.6233 - MinusLogProbMetric: 17.6233 - val_loss: 17.7049 - val_MinusLogProbMetric: 17.7049 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 476/1000
2023-10-23 21:19:47.854 
Epoch 476/1000 
	 loss: 17.6001, MinusLogProbMetric: 17.6001, val_loss: 17.5152, val_MinusLogProbMetric: 17.5152

Epoch 476: val_loss did not improve from 17.46418
196/196 - 62s - loss: 17.6001 - MinusLogProbMetric: 17.6001 - val_loss: 17.5152 - val_MinusLogProbMetric: 17.5152 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 477/1000
2023-10-23 21:20:52.322 
Epoch 477/1000 
	 loss: 17.6231, MinusLogProbMetric: 17.6231, val_loss: 17.8054, val_MinusLogProbMetric: 17.8054

Epoch 477: val_loss did not improve from 17.46418
196/196 - 64s - loss: 17.6231 - MinusLogProbMetric: 17.6231 - val_loss: 17.8054 - val_MinusLogProbMetric: 17.8054 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 478/1000
2023-10-23 21:21:53.952 
Epoch 478/1000 
	 loss: 17.5807, MinusLogProbMetric: 17.5807, val_loss: 17.5715, val_MinusLogProbMetric: 17.5715

Epoch 478: val_loss did not improve from 17.46418
196/196 - 62s - loss: 17.5807 - MinusLogProbMetric: 17.5807 - val_loss: 17.5715 - val_MinusLogProbMetric: 17.5715 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 479/1000
2023-10-23 21:22:56.949 
Epoch 479/1000 
	 loss: 17.5892, MinusLogProbMetric: 17.5892, val_loss: 17.7011, val_MinusLogProbMetric: 17.7011

Epoch 479: val_loss did not improve from 17.46418
196/196 - 63s - loss: 17.5892 - MinusLogProbMetric: 17.5892 - val_loss: 17.7011 - val_MinusLogProbMetric: 17.7011 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 480/1000
2023-10-23 21:24:00.158 
Epoch 480/1000 
	 loss: 17.5531, MinusLogProbMetric: 17.5531, val_loss: 17.5657, val_MinusLogProbMetric: 17.5657

Epoch 480: val_loss did not improve from 17.46418
196/196 - 63s - loss: 17.5531 - MinusLogProbMetric: 17.5531 - val_loss: 17.5657 - val_MinusLogProbMetric: 17.5657 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 481/1000
2023-10-23 21:25:05.390 
Epoch 481/1000 
	 loss: 17.8101, MinusLogProbMetric: 17.8101, val_loss: 17.8983, val_MinusLogProbMetric: 17.8983

Epoch 481: val_loss did not improve from 17.46418
196/196 - 65s - loss: 17.8101 - MinusLogProbMetric: 17.8101 - val_loss: 17.8983 - val_MinusLogProbMetric: 17.8983 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 482/1000
2023-10-23 21:26:20.750 
Epoch 482/1000 
	 loss: 17.6159, MinusLogProbMetric: 17.6159, val_loss: 18.1080, val_MinusLogProbMetric: 18.1080

Epoch 482: val_loss did not improve from 17.46418
196/196 - 75s - loss: 17.6159 - MinusLogProbMetric: 17.6159 - val_loss: 18.1080 - val_MinusLogProbMetric: 18.1080 - lr: 1.1111e-04 - 75s/epoch - 385ms/step
Epoch 483/1000
2023-10-23 21:27:36.699 
Epoch 483/1000 
	 loss: 17.6583, MinusLogProbMetric: 17.6583, val_loss: 18.0125, val_MinusLogProbMetric: 18.0125

Epoch 483: val_loss did not improve from 17.46418
196/196 - 76s - loss: 17.6583 - MinusLogProbMetric: 17.6583 - val_loss: 18.0125 - val_MinusLogProbMetric: 18.0125 - lr: 1.1111e-04 - 76s/epoch - 387ms/step
Epoch 484/1000
2023-10-23 21:28:49.582 
Epoch 484/1000 
	 loss: 17.5422, MinusLogProbMetric: 17.5422, val_loss: 17.5718, val_MinusLogProbMetric: 17.5718

Epoch 484: val_loss did not improve from 17.46418
196/196 - 73s - loss: 17.5422 - MinusLogProbMetric: 17.5422 - val_loss: 17.5718 - val_MinusLogProbMetric: 17.5718 - lr: 1.1111e-04 - 73s/epoch - 372ms/step
Epoch 485/1000
2023-10-23 21:29:59.846 
Epoch 485/1000 
	 loss: 17.5769, MinusLogProbMetric: 17.5769, val_loss: 17.8489, val_MinusLogProbMetric: 17.8489

Epoch 485: val_loss did not improve from 17.46418
196/196 - 70s - loss: 17.5769 - MinusLogProbMetric: 17.5769 - val_loss: 17.8489 - val_MinusLogProbMetric: 17.8489 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 486/1000
2023-10-23 21:31:10.382 
Epoch 486/1000 
	 loss: 17.5935, MinusLogProbMetric: 17.5935, val_loss: 17.7449, val_MinusLogProbMetric: 17.7449

Epoch 486: val_loss did not improve from 17.46418
196/196 - 71s - loss: 17.5935 - MinusLogProbMetric: 17.5935 - val_loss: 17.7449 - val_MinusLogProbMetric: 17.7449 - lr: 1.1111e-04 - 71s/epoch - 360ms/step
Epoch 487/1000
2023-10-23 21:32:22.640 
Epoch 487/1000 
	 loss: 17.5853, MinusLogProbMetric: 17.5853, val_loss: 17.9500, val_MinusLogProbMetric: 17.9500

Epoch 487: val_loss did not improve from 17.46418
196/196 - 72s - loss: 17.5853 - MinusLogProbMetric: 17.5853 - val_loss: 17.9500 - val_MinusLogProbMetric: 17.9500 - lr: 1.1111e-04 - 72s/epoch - 369ms/step
Epoch 488/1000
2023-10-23 21:33:35.333 
Epoch 488/1000 
	 loss: 17.5626, MinusLogProbMetric: 17.5626, val_loss: 17.5920, val_MinusLogProbMetric: 17.5920

Epoch 488: val_loss did not improve from 17.46418
196/196 - 73s - loss: 17.5626 - MinusLogProbMetric: 17.5626 - val_loss: 17.5920 - val_MinusLogProbMetric: 17.5920 - lr: 1.1111e-04 - 73s/epoch - 371ms/step
Epoch 489/1000
2023-10-23 21:34:47.019 
Epoch 489/1000 
	 loss: 17.5628, MinusLogProbMetric: 17.5628, val_loss: 17.5519, val_MinusLogProbMetric: 17.5519

Epoch 489: val_loss did not improve from 17.46418
196/196 - 72s - loss: 17.5628 - MinusLogProbMetric: 17.5628 - val_loss: 17.5519 - val_MinusLogProbMetric: 17.5519 - lr: 1.1111e-04 - 72s/epoch - 366ms/step
Epoch 490/1000
2023-10-23 21:35:57.423 
Epoch 490/1000 
	 loss: 17.5503, MinusLogProbMetric: 17.5503, val_loss: 17.6953, val_MinusLogProbMetric: 17.6953

Epoch 490: val_loss did not improve from 17.46418
196/196 - 70s - loss: 17.5503 - MinusLogProbMetric: 17.5503 - val_loss: 17.6953 - val_MinusLogProbMetric: 17.6953 - lr: 1.1111e-04 - 70s/epoch - 359ms/step
Epoch 491/1000
2023-10-23 21:37:05.309 
Epoch 491/1000 
	 loss: 17.5953, MinusLogProbMetric: 17.5953, val_loss: 17.5755, val_MinusLogProbMetric: 17.5755

Epoch 491: val_loss did not improve from 17.46418
196/196 - 68s - loss: 17.5953 - MinusLogProbMetric: 17.5953 - val_loss: 17.5755 - val_MinusLogProbMetric: 17.5755 - lr: 1.1111e-04 - 68s/epoch - 346ms/step
Epoch 492/1000
2023-10-23 21:38:12.408 
Epoch 492/1000 
	 loss: 17.5677, MinusLogProbMetric: 17.5677, val_loss: 17.6792, val_MinusLogProbMetric: 17.6792

Epoch 492: val_loss did not improve from 17.46418
196/196 - 67s - loss: 17.5677 - MinusLogProbMetric: 17.5677 - val_loss: 17.6792 - val_MinusLogProbMetric: 17.6792 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 493/1000
2023-10-23 21:39:25.810 
Epoch 493/1000 
	 loss: 17.6312, MinusLogProbMetric: 17.6312, val_loss: 17.6135, val_MinusLogProbMetric: 17.6135

Epoch 493: val_loss did not improve from 17.46418
196/196 - 73s - loss: 17.6312 - MinusLogProbMetric: 17.6312 - val_loss: 17.6135 - val_MinusLogProbMetric: 17.6135 - lr: 1.1111e-04 - 73s/epoch - 374ms/step
Epoch 494/1000
2023-10-23 21:40:37.966 
Epoch 494/1000 
	 loss: 17.5915, MinusLogProbMetric: 17.5915, val_loss: 17.6629, val_MinusLogProbMetric: 17.6629

Epoch 494: val_loss did not improve from 17.46418
196/196 - 72s - loss: 17.5915 - MinusLogProbMetric: 17.5915 - val_loss: 17.6629 - val_MinusLogProbMetric: 17.6629 - lr: 1.1111e-04 - 72s/epoch - 368ms/step
Epoch 495/1000
2023-10-23 21:41:51.868 
Epoch 495/1000 
	 loss: 17.6385, MinusLogProbMetric: 17.6385, val_loss: 17.6822, val_MinusLogProbMetric: 17.6822

Epoch 495: val_loss did not improve from 17.46418
196/196 - 74s - loss: 17.6385 - MinusLogProbMetric: 17.6385 - val_loss: 17.6822 - val_MinusLogProbMetric: 17.6822 - lr: 1.1111e-04 - 74s/epoch - 377ms/step
Epoch 496/1000
2023-10-23 21:43:06.031 
Epoch 496/1000 
	 loss: 17.5601, MinusLogProbMetric: 17.5601, val_loss: 17.7088, val_MinusLogProbMetric: 17.7088

Epoch 496: val_loss did not improve from 17.46418
196/196 - 74s - loss: 17.5601 - MinusLogProbMetric: 17.5601 - val_loss: 17.7088 - val_MinusLogProbMetric: 17.7088 - lr: 1.1111e-04 - 74s/epoch - 378ms/step
Epoch 497/1000
2023-10-23 21:44:17.737 
Epoch 497/1000 
	 loss: 17.5226, MinusLogProbMetric: 17.5226, val_loss: 17.5798, val_MinusLogProbMetric: 17.5798

Epoch 497: val_loss did not improve from 17.46418
196/196 - 72s - loss: 17.5226 - MinusLogProbMetric: 17.5226 - val_loss: 17.5798 - val_MinusLogProbMetric: 17.5798 - lr: 1.1111e-04 - 72s/epoch - 366ms/step
Epoch 498/1000
2023-10-23 21:45:31.280 
Epoch 498/1000 
	 loss: 17.5213, MinusLogProbMetric: 17.5213, val_loss: 17.7196, val_MinusLogProbMetric: 17.7196

Epoch 498: val_loss did not improve from 17.46418
196/196 - 74s - loss: 17.5213 - MinusLogProbMetric: 17.5213 - val_loss: 17.7196 - val_MinusLogProbMetric: 17.7196 - lr: 1.1111e-04 - 74s/epoch - 375ms/step
Epoch 499/1000
2023-10-23 21:46:46.227 
Epoch 499/1000 
	 loss: 17.5404, MinusLogProbMetric: 17.5404, val_loss: 17.6466, val_MinusLogProbMetric: 17.6466

Epoch 499: val_loss did not improve from 17.46418
196/196 - 75s - loss: 17.5404 - MinusLogProbMetric: 17.5404 - val_loss: 17.6466 - val_MinusLogProbMetric: 17.6466 - lr: 1.1111e-04 - 75s/epoch - 382ms/step
Epoch 500/1000
2023-10-23 21:48:02.559 
Epoch 500/1000 
	 loss: 17.3343, MinusLogProbMetric: 17.3343, val_loss: 17.3815, val_MinusLogProbMetric: 17.3815

Epoch 500: val_loss improved from 17.46418 to 17.38148, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 78s - loss: 17.3343 - MinusLogProbMetric: 17.3343 - val_loss: 17.3815 - val_MinusLogProbMetric: 17.3815 - lr: 5.5556e-05 - 78s/epoch - 396ms/step
Epoch 501/1000
2023-10-23 21:49:14.425 
Epoch 501/1000 
	 loss: 17.3242, MinusLogProbMetric: 17.3242, val_loss: 17.3581, val_MinusLogProbMetric: 17.3581

Epoch 501: val_loss improved from 17.38148 to 17.35807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 72s - loss: 17.3242 - MinusLogProbMetric: 17.3242 - val_loss: 17.3581 - val_MinusLogProbMetric: 17.3581 - lr: 5.5556e-05 - 72s/epoch - 367ms/step
Epoch 502/1000
2023-10-23 21:50:26.611 
Epoch 502/1000 
	 loss: 17.3256, MinusLogProbMetric: 17.3256, val_loss: 17.3188, val_MinusLogProbMetric: 17.3188

Epoch 502: val_loss improved from 17.35807 to 17.31876, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 72s - loss: 17.3256 - MinusLogProbMetric: 17.3256 - val_loss: 17.3188 - val_MinusLogProbMetric: 17.3188 - lr: 5.5556e-05 - 72s/epoch - 367ms/step
Epoch 503/1000
2023-10-23 21:51:38.445 
Epoch 503/1000 
	 loss: 17.3233, MinusLogProbMetric: 17.3233, val_loss: 17.3621, val_MinusLogProbMetric: 17.3621

Epoch 503: val_loss did not improve from 17.31876
196/196 - 71s - loss: 17.3233 - MinusLogProbMetric: 17.3233 - val_loss: 17.3621 - val_MinusLogProbMetric: 17.3621 - lr: 5.5556e-05 - 71s/epoch - 360ms/step
Epoch 504/1000
2023-10-23 21:52:53.410 
Epoch 504/1000 
	 loss: 17.3432, MinusLogProbMetric: 17.3432, val_loss: 17.3416, val_MinusLogProbMetric: 17.3416

Epoch 504: val_loss did not improve from 17.31876
196/196 - 75s - loss: 17.3432 - MinusLogProbMetric: 17.3432 - val_loss: 17.3416 - val_MinusLogProbMetric: 17.3416 - lr: 5.5556e-05 - 75s/epoch - 382ms/step
Epoch 505/1000
2023-10-23 21:54:07.152 
Epoch 505/1000 
	 loss: 17.3469, MinusLogProbMetric: 17.3469, val_loss: 17.3497, val_MinusLogProbMetric: 17.3497

Epoch 505: val_loss did not improve from 17.31876
196/196 - 74s - loss: 17.3469 - MinusLogProbMetric: 17.3469 - val_loss: 17.3497 - val_MinusLogProbMetric: 17.3497 - lr: 5.5556e-05 - 74s/epoch - 376ms/step
Epoch 506/1000
2023-10-23 21:55:18.454 
Epoch 506/1000 
	 loss: 17.3266, MinusLogProbMetric: 17.3266, val_loss: 17.4823, val_MinusLogProbMetric: 17.4823

Epoch 506: val_loss did not improve from 17.31876
196/196 - 71s - loss: 17.3266 - MinusLogProbMetric: 17.3266 - val_loss: 17.4823 - val_MinusLogProbMetric: 17.4823 - lr: 5.5556e-05 - 71s/epoch - 364ms/step
Epoch 507/1000
2023-10-23 21:56:28.266 
Epoch 507/1000 
	 loss: 17.3321, MinusLogProbMetric: 17.3321, val_loss: 17.4089, val_MinusLogProbMetric: 17.4089

Epoch 507: val_loss did not improve from 17.31876
196/196 - 70s - loss: 17.3321 - MinusLogProbMetric: 17.3321 - val_loss: 17.4089 - val_MinusLogProbMetric: 17.4089 - lr: 5.5556e-05 - 70s/epoch - 356ms/step
Epoch 508/1000
2023-10-23 21:57:40.351 
Epoch 508/1000 
	 loss: 17.3372, MinusLogProbMetric: 17.3372, val_loss: 17.3639, val_MinusLogProbMetric: 17.3639

Epoch 508: val_loss did not improve from 17.31876
196/196 - 72s - loss: 17.3372 - MinusLogProbMetric: 17.3372 - val_loss: 17.3639 - val_MinusLogProbMetric: 17.3639 - lr: 5.5556e-05 - 72s/epoch - 368ms/step
Epoch 509/1000
2023-10-23 21:58:52.262 
Epoch 509/1000 
	 loss: 17.3137, MinusLogProbMetric: 17.3137, val_loss: 17.4291, val_MinusLogProbMetric: 17.4291

Epoch 509: val_loss did not improve from 17.31876
196/196 - 72s - loss: 17.3137 - MinusLogProbMetric: 17.3137 - val_loss: 17.4291 - val_MinusLogProbMetric: 17.4291 - lr: 5.5556e-05 - 72s/epoch - 367ms/step
Epoch 510/1000
2023-10-23 22:00:04.614 
Epoch 510/1000 
	 loss: 17.3134, MinusLogProbMetric: 17.3134, val_loss: 17.3245, val_MinusLogProbMetric: 17.3245

Epoch 510: val_loss did not improve from 17.31876
196/196 - 72s - loss: 17.3134 - MinusLogProbMetric: 17.3134 - val_loss: 17.3245 - val_MinusLogProbMetric: 17.3245 - lr: 5.5556e-05 - 72s/epoch - 369ms/step
Epoch 511/1000
2023-10-23 22:01:16.684 
Epoch 511/1000 
	 loss: 17.3167, MinusLogProbMetric: 17.3167, val_loss: 17.3661, val_MinusLogProbMetric: 17.3661

Epoch 511: val_loss did not improve from 17.31876
196/196 - 72s - loss: 17.3167 - MinusLogProbMetric: 17.3167 - val_loss: 17.3661 - val_MinusLogProbMetric: 17.3661 - lr: 5.5556e-05 - 72s/epoch - 368ms/step
Epoch 512/1000
2023-10-23 22:02:30.810 
Epoch 512/1000 
	 loss: 17.3197, MinusLogProbMetric: 17.3197, val_loss: 17.3144, val_MinusLogProbMetric: 17.3144

Epoch 512: val_loss improved from 17.31876 to 17.31441, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 75s - loss: 17.3197 - MinusLogProbMetric: 17.3197 - val_loss: 17.3144 - val_MinusLogProbMetric: 17.3144 - lr: 5.5556e-05 - 75s/epoch - 385ms/step
Epoch 513/1000
2023-10-23 22:03:42.550 
Epoch 513/1000 
	 loss: 17.3080, MinusLogProbMetric: 17.3080, val_loss: 17.3728, val_MinusLogProbMetric: 17.3728

Epoch 513: val_loss did not improve from 17.31441
196/196 - 70s - loss: 17.3080 - MinusLogProbMetric: 17.3080 - val_loss: 17.3728 - val_MinusLogProbMetric: 17.3728 - lr: 5.5556e-05 - 70s/epoch - 359ms/step
Epoch 514/1000
2023-10-23 22:04:55.471 
Epoch 514/1000 
	 loss: 17.3073, MinusLogProbMetric: 17.3073, val_loss: 17.4940, val_MinusLogProbMetric: 17.4940

Epoch 514: val_loss did not improve from 17.31441
196/196 - 73s - loss: 17.3073 - MinusLogProbMetric: 17.3073 - val_loss: 17.4940 - val_MinusLogProbMetric: 17.4940 - lr: 5.5556e-05 - 73s/epoch - 372ms/step
Epoch 515/1000
2023-10-23 22:06:06.114 
Epoch 515/1000 
	 loss: 17.3367, MinusLogProbMetric: 17.3367, val_loss: 17.3384, val_MinusLogProbMetric: 17.3384

Epoch 515: val_loss did not improve from 17.31441
196/196 - 71s - loss: 17.3367 - MinusLogProbMetric: 17.3367 - val_loss: 17.3384 - val_MinusLogProbMetric: 17.3384 - lr: 5.5556e-05 - 71s/epoch - 360ms/step
Epoch 516/1000
2023-10-23 22:07:21.107 
Epoch 516/1000 
	 loss: 17.3028, MinusLogProbMetric: 17.3028, val_loss: 17.3283, val_MinusLogProbMetric: 17.3283

Epoch 516: val_loss did not improve from 17.31441
196/196 - 75s - loss: 17.3028 - MinusLogProbMetric: 17.3028 - val_loss: 17.3283 - val_MinusLogProbMetric: 17.3283 - lr: 5.5556e-05 - 75s/epoch - 383ms/step
Epoch 517/1000
2023-10-23 22:08:35.743 
Epoch 517/1000 
	 loss: 17.3133, MinusLogProbMetric: 17.3133, val_loss: 17.3983, val_MinusLogProbMetric: 17.3983

Epoch 517: val_loss did not improve from 17.31441
196/196 - 75s - loss: 17.3133 - MinusLogProbMetric: 17.3133 - val_loss: 17.3983 - val_MinusLogProbMetric: 17.3983 - lr: 5.5556e-05 - 75s/epoch - 381ms/step
Epoch 518/1000
2023-10-23 22:09:51.750 
Epoch 518/1000 
	 loss: 17.3069, MinusLogProbMetric: 17.3069, val_loss: 17.3660, val_MinusLogProbMetric: 17.3660

Epoch 518: val_loss did not improve from 17.31441
196/196 - 76s - loss: 17.3069 - MinusLogProbMetric: 17.3069 - val_loss: 17.3660 - val_MinusLogProbMetric: 17.3660 - lr: 5.5556e-05 - 76s/epoch - 388ms/step
Epoch 519/1000
2023-10-23 22:11:07.751 
Epoch 519/1000 
	 loss: 17.3062, MinusLogProbMetric: 17.3062, val_loss: 17.3062, val_MinusLogProbMetric: 17.3062

Epoch 519: val_loss improved from 17.31441 to 17.30619, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 77s - loss: 17.3062 - MinusLogProbMetric: 17.3062 - val_loss: 17.3062 - val_MinusLogProbMetric: 17.3062 - lr: 5.5556e-05 - 77s/epoch - 394ms/step
Epoch 520/1000
2023-10-23 22:12:24.473 
Epoch 520/1000 
	 loss: 17.3058, MinusLogProbMetric: 17.3058, val_loss: 17.4120, val_MinusLogProbMetric: 17.4120

Epoch 520: val_loss did not improve from 17.30619
196/196 - 76s - loss: 17.3058 - MinusLogProbMetric: 17.3058 - val_loss: 17.4120 - val_MinusLogProbMetric: 17.4120 - lr: 5.5556e-05 - 76s/epoch - 385ms/step
Epoch 521/1000
2023-10-23 22:13:36.937 
Epoch 521/1000 
	 loss: 17.3130, MinusLogProbMetric: 17.3130, val_loss: 17.3374, val_MinusLogProbMetric: 17.3374

Epoch 521: val_loss did not improve from 17.30619
196/196 - 72s - loss: 17.3130 - MinusLogProbMetric: 17.3130 - val_loss: 17.3374 - val_MinusLogProbMetric: 17.3374 - lr: 5.5556e-05 - 72s/epoch - 370ms/step
Epoch 522/1000
2023-10-23 22:14:53.250 
Epoch 522/1000 
	 loss: 17.2892, MinusLogProbMetric: 17.2892, val_loss: 17.4269, val_MinusLogProbMetric: 17.4269

Epoch 522: val_loss did not improve from 17.30619
196/196 - 76s - loss: 17.2892 - MinusLogProbMetric: 17.2892 - val_loss: 17.4269 - val_MinusLogProbMetric: 17.4269 - lr: 5.5556e-05 - 76s/epoch - 389ms/step
Epoch 523/1000
2023-10-23 22:16:08.631 
Epoch 523/1000 
	 loss: 17.3281, MinusLogProbMetric: 17.3281, val_loss: 17.3882, val_MinusLogProbMetric: 17.3882

Epoch 523: val_loss did not improve from 17.30619
196/196 - 75s - loss: 17.3281 - MinusLogProbMetric: 17.3281 - val_loss: 17.3882 - val_MinusLogProbMetric: 17.3882 - lr: 5.5556e-05 - 75s/epoch - 385ms/step
Epoch 524/1000
2023-10-23 22:17:24.696 
Epoch 524/1000 
	 loss: 17.3046, MinusLogProbMetric: 17.3046, val_loss: 17.3919, val_MinusLogProbMetric: 17.3919

Epoch 524: val_loss did not improve from 17.30619
196/196 - 76s - loss: 17.3046 - MinusLogProbMetric: 17.3046 - val_loss: 17.3919 - val_MinusLogProbMetric: 17.3919 - lr: 5.5556e-05 - 76s/epoch - 388ms/step
Epoch 525/1000
2023-10-23 22:18:38.408 
Epoch 525/1000 
	 loss: 17.3178, MinusLogProbMetric: 17.3178, val_loss: 17.3640, val_MinusLogProbMetric: 17.3640

Epoch 525: val_loss did not improve from 17.30619
196/196 - 74s - loss: 17.3178 - MinusLogProbMetric: 17.3178 - val_loss: 17.3640 - val_MinusLogProbMetric: 17.3640 - lr: 5.5556e-05 - 74s/epoch - 376ms/step
Epoch 526/1000
2023-10-23 22:19:52.036 
Epoch 526/1000 
	 loss: 17.3347, MinusLogProbMetric: 17.3347, val_loss: 17.4094, val_MinusLogProbMetric: 17.4094

Epoch 526: val_loss did not improve from 17.30619
196/196 - 74s - loss: 17.3347 - MinusLogProbMetric: 17.3347 - val_loss: 17.4094 - val_MinusLogProbMetric: 17.4094 - lr: 5.5556e-05 - 74s/epoch - 376ms/step
Epoch 527/1000
2023-10-23 22:21:10.552 
Epoch 527/1000 
	 loss: 17.3358, MinusLogProbMetric: 17.3358, val_loss: 17.3928, val_MinusLogProbMetric: 17.3928

Epoch 527: val_loss did not improve from 17.30619
196/196 - 79s - loss: 17.3358 - MinusLogProbMetric: 17.3358 - val_loss: 17.3928 - val_MinusLogProbMetric: 17.3928 - lr: 5.5556e-05 - 79s/epoch - 401ms/step
Epoch 528/1000
2023-10-23 22:22:25.945 
Epoch 528/1000 
	 loss: 17.2940, MinusLogProbMetric: 17.2940, val_loss: 17.4565, val_MinusLogProbMetric: 17.4565

Epoch 528: val_loss did not improve from 17.30619
196/196 - 75s - loss: 17.2940 - MinusLogProbMetric: 17.2940 - val_loss: 17.4565 - val_MinusLogProbMetric: 17.4565 - lr: 5.5556e-05 - 75s/epoch - 385ms/step
Epoch 529/1000
2023-10-23 22:23:44.968 
Epoch 529/1000 
	 loss: 17.3021, MinusLogProbMetric: 17.3021, val_loss: 17.4252, val_MinusLogProbMetric: 17.4252

Epoch 529: val_loss did not improve from 17.30619
196/196 - 79s - loss: 17.3021 - MinusLogProbMetric: 17.3021 - val_loss: 17.4252 - val_MinusLogProbMetric: 17.4252 - lr: 5.5556e-05 - 79s/epoch - 403ms/step
Epoch 530/1000
2023-10-23 22:24:58.508 
Epoch 530/1000 
	 loss: 17.3005, MinusLogProbMetric: 17.3005, val_loss: 17.4078, val_MinusLogProbMetric: 17.4078

Epoch 530: val_loss did not improve from 17.30619
196/196 - 74s - loss: 17.3005 - MinusLogProbMetric: 17.3005 - val_loss: 17.4078 - val_MinusLogProbMetric: 17.4078 - lr: 5.5556e-05 - 74s/epoch - 375ms/step
Epoch 531/1000
2023-10-23 22:26:12.803 
Epoch 531/1000 
	 loss: 17.3084, MinusLogProbMetric: 17.3084, val_loss: 17.2939, val_MinusLogProbMetric: 17.2939

Epoch 531: val_loss improved from 17.30619 to 17.29393, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 75s - loss: 17.3084 - MinusLogProbMetric: 17.3084 - val_loss: 17.2939 - val_MinusLogProbMetric: 17.2939 - lr: 5.5556e-05 - 75s/epoch - 385ms/step
Epoch 532/1000
2023-10-23 22:27:27.645 
Epoch 532/1000 
	 loss: 17.3126, MinusLogProbMetric: 17.3126, val_loss: 17.3706, val_MinusLogProbMetric: 17.3706

Epoch 532: val_loss did not improve from 17.29393
196/196 - 74s - loss: 17.3126 - MinusLogProbMetric: 17.3126 - val_loss: 17.3706 - val_MinusLogProbMetric: 17.3706 - lr: 5.5556e-05 - 74s/epoch - 376ms/step
Epoch 533/1000
2023-10-23 22:28:42.863 
Epoch 533/1000 
	 loss: 17.3161, MinusLogProbMetric: 17.3161, val_loss: 17.3754, val_MinusLogProbMetric: 17.3754

Epoch 533: val_loss did not improve from 17.29393
196/196 - 75s - loss: 17.3161 - MinusLogProbMetric: 17.3161 - val_loss: 17.3754 - val_MinusLogProbMetric: 17.3754 - lr: 5.5556e-05 - 75s/epoch - 384ms/step
Epoch 534/1000
2023-10-23 22:29:59.625 
Epoch 534/1000 
	 loss: 17.3013, MinusLogProbMetric: 17.3013, val_loss: 17.3957, val_MinusLogProbMetric: 17.3957

Epoch 534: val_loss did not improve from 17.29393
196/196 - 77s - loss: 17.3013 - MinusLogProbMetric: 17.3013 - val_loss: 17.3957 - val_MinusLogProbMetric: 17.3957 - lr: 5.5556e-05 - 77s/epoch - 392ms/step
Epoch 535/1000
2023-10-23 22:31:18.156 
Epoch 535/1000 
	 loss: 17.3317, MinusLogProbMetric: 17.3317, val_loss: 17.3234, val_MinusLogProbMetric: 17.3234

Epoch 535: val_loss did not improve from 17.29393
196/196 - 79s - loss: 17.3317 - MinusLogProbMetric: 17.3317 - val_loss: 17.3234 - val_MinusLogProbMetric: 17.3234 - lr: 5.5556e-05 - 79s/epoch - 401ms/step
Epoch 536/1000
2023-10-23 22:32:31.456 
Epoch 536/1000 
	 loss: 17.2974, MinusLogProbMetric: 17.2974, val_loss: 17.3501, val_MinusLogProbMetric: 17.3501

Epoch 536: val_loss did not improve from 17.29393
196/196 - 73s - loss: 17.2974 - MinusLogProbMetric: 17.2974 - val_loss: 17.3501 - val_MinusLogProbMetric: 17.3501 - lr: 5.5556e-05 - 73s/epoch - 374ms/step
Epoch 537/1000
2023-10-23 22:33:47.776 
Epoch 537/1000 
	 loss: 17.3258, MinusLogProbMetric: 17.3258, val_loss: 17.5363, val_MinusLogProbMetric: 17.5363

Epoch 537: val_loss did not improve from 17.29393
196/196 - 76s - loss: 17.3258 - MinusLogProbMetric: 17.3258 - val_loss: 17.5363 - val_MinusLogProbMetric: 17.5363 - lr: 5.5556e-05 - 76s/epoch - 389ms/step
Epoch 538/1000
2023-10-23 22:35:03.139 
Epoch 538/1000 
	 loss: 17.3222, MinusLogProbMetric: 17.3222, val_loss: 17.3058, val_MinusLogProbMetric: 17.3058

Epoch 538: val_loss did not improve from 17.29393
196/196 - 75s - loss: 17.3222 - MinusLogProbMetric: 17.3222 - val_loss: 17.3058 - val_MinusLogProbMetric: 17.3058 - lr: 5.5556e-05 - 75s/epoch - 384ms/step
Epoch 539/1000
2023-10-23 22:36:15.401 
Epoch 539/1000 
	 loss: 17.2966, MinusLogProbMetric: 17.2966, val_loss: 17.3830, val_MinusLogProbMetric: 17.3830

Epoch 539: val_loss did not improve from 17.29393
196/196 - 72s - loss: 17.2966 - MinusLogProbMetric: 17.2966 - val_loss: 17.3830 - val_MinusLogProbMetric: 17.3830 - lr: 5.5556e-05 - 72s/epoch - 369ms/step
Epoch 540/1000
2023-10-23 22:37:27.392 
Epoch 540/1000 
	 loss: 17.2785, MinusLogProbMetric: 17.2785, val_loss: 17.4075, val_MinusLogProbMetric: 17.4075

Epoch 540: val_loss did not improve from 17.29393
196/196 - 72s - loss: 17.2785 - MinusLogProbMetric: 17.2785 - val_loss: 17.4075 - val_MinusLogProbMetric: 17.4075 - lr: 5.5556e-05 - 72s/epoch - 367ms/step
Epoch 541/1000
2023-10-23 22:38:34.159 
Epoch 541/1000 
	 loss: 17.3055, MinusLogProbMetric: 17.3055, val_loss: 17.3265, val_MinusLogProbMetric: 17.3265

Epoch 541: val_loss did not improve from 17.29393
196/196 - 67s - loss: 17.3055 - MinusLogProbMetric: 17.3055 - val_loss: 17.3265 - val_MinusLogProbMetric: 17.3265 - lr: 5.5556e-05 - 67s/epoch - 341ms/step
Epoch 542/1000
2023-10-23 22:39:41.021 
Epoch 542/1000 
	 loss: 17.3018, MinusLogProbMetric: 17.3018, val_loss: 17.3418, val_MinusLogProbMetric: 17.3418

Epoch 542: val_loss did not improve from 17.29393
196/196 - 67s - loss: 17.3018 - MinusLogProbMetric: 17.3018 - val_loss: 17.3418 - val_MinusLogProbMetric: 17.3418 - lr: 5.5556e-05 - 67s/epoch - 341ms/step
Epoch 543/1000
2023-10-23 22:40:49.477 
Epoch 543/1000 
	 loss: 17.2807, MinusLogProbMetric: 17.2807, val_loss: 17.4355, val_MinusLogProbMetric: 17.4355

Epoch 543: val_loss did not improve from 17.29393
196/196 - 68s - loss: 17.2807 - MinusLogProbMetric: 17.2807 - val_loss: 17.4355 - val_MinusLogProbMetric: 17.4355 - lr: 5.5556e-05 - 68s/epoch - 349ms/step
Epoch 544/1000
2023-10-23 22:42:00.220 
Epoch 544/1000 
	 loss: 17.3056, MinusLogProbMetric: 17.3056, val_loss: 17.3278, val_MinusLogProbMetric: 17.3278

Epoch 544: val_loss did not improve from 17.29393
196/196 - 71s - loss: 17.3056 - MinusLogProbMetric: 17.3056 - val_loss: 17.3278 - val_MinusLogProbMetric: 17.3278 - lr: 5.5556e-05 - 71s/epoch - 361ms/step
Epoch 545/1000
2023-10-23 22:43:09.756 
Epoch 545/1000 
	 loss: 17.3054, MinusLogProbMetric: 17.3054, val_loss: 17.3262, val_MinusLogProbMetric: 17.3262

Epoch 545: val_loss did not improve from 17.29393
196/196 - 70s - loss: 17.3054 - MinusLogProbMetric: 17.3054 - val_loss: 17.3262 - val_MinusLogProbMetric: 17.3262 - lr: 5.5556e-05 - 70s/epoch - 355ms/step
Epoch 546/1000
2023-10-23 22:44:19.779 
Epoch 546/1000 
	 loss: 17.2900, MinusLogProbMetric: 17.2900, val_loss: 17.5074, val_MinusLogProbMetric: 17.5074

Epoch 546: val_loss did not improve from 17.29393
196/196 - 70s - loss: 17.2900 - MinusLogProbMetric: 17.2900 - val_loss: 17.5074 - val_MinusLogProbMetric: 17.5074 - lr: 5.5556e-05 - 70s/epoch - 357ms/step
Epoch 547/1000
2023-10-23 22:45:28.970 
Epoch 547/1000 
	 loss: 17.2838, MinusLogProbMetric: 17.2838, val_loss: 17.5022, val_MinusLogProbMetric: 17.5022

Epoch 547: val_loss did not improve from 17.29393
196/196 - 69s - loss: 17.2838 - MinusLogProbMetric: 17.2838 - val_loss: 17.5022 - val_MinusLogProbMetric: 17.5022 - lr: 5.5556e-05 - 69s/epoch - 353ms/step
Epoch 548/1000
2023-10-23 22:46:39.806 
Epoch 548/1000 
	 loss: 17.3097, MinusLogProbMetric: 17.3097, val_loss: 17.4613, val_MinusLogProbMetric: 17.4613

Epoch 548: val_loss did not improve from 17.29393
196/196 - 71s - loss: 17.3097 - MinusLogProbMetric: 17.3097 - val_loss: 17.4613 - val_MinusLogProbMetric: 17.4613 - lr: 5.5556e-05 - 71s/epoch - 361ms/step
Epoch 549/1000
2023-10-23 22:47:51.454 
Epoch 549/1000 
	 loss: 17.2895, MinusLogProbMetric: 17.2895, val_loss: 17.4053, val_MinusLogProbMetric: 17.4053

Epoch 549: val_loss did not improve from 17.29393
196/196 - 72s - loss: 17.2895 - MinusLogProbMetric: 17.2895 - val_loss: 17.4053 - val_MinusLogProbMetric: 17.4053 - lr: 5.5556e-05 - 72s/epoch - 366ms/step
Epoch 550/1000
2023-10-23 22:49:05.260 
Epoch 550/1000 
	 loss: 17.3150, MinusLogProbMetric: 17.3150, val_loss: 17.2915, val_MinusLogProbMetric: 17.2915

Epoch 550: val_loss improved from 17.29393 to 17.29153, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 75s - loss: 17.3150 - MinusLogProbMetric: 17.3150 - val_loss: 17.2915 - val_MinusLogProbMetric: 17.2915 - lr: 5.5556e-05 - 75s/epoch - 382ms/step
Epoch 551/1000
2023-10-23 22:50:22.745 
Epoch 551/1000 
	 loss: 17.2970, MinusLogProbMetric: 17.2970, val_loss: 17.3585, val_MinusLogProbMetric: 17.3585

Epoch 551: val_loss did not improve from 17.29153
196/196 - 76s - loss: 17.2970 - MinusLogProbMetric: 17.2970 - val_loss: 17.3585 - val_MinusLogProbMetric: 17.3585 - lr: 5.5556e-05 - 76s/epoch - 390ms/step
Epoch 552/1000
2023-10-23 22:51:35.361 
Epoch 552/1000 
	 loss: 17.2958, MinusLogProbMetric: 17.2958, val_loss: 17.3705, val_MinusLogProbMetric: 17.3705

Epoch 552: val_loss did not improve from 17.29153
196/196 - 73s - loss: 17.2958 - MinusLogProbMetric: 17.2958 - val_loss: 17.3705 - val_MinusLogProbMetric: 17.3705 - lr: 5.5556e-05 - 73s/epoch - 370ms/step
Epoch 553/1000
2023-10-23 22:52:44.191 
Epoch 553/1000 
	 loss: 17.2799, MinusLogProbMetric: 17.2799, val_loss: 17.3890, val_MinusLogProbMetric: 17.3890

Epoch 553: val_loss did not improve from 17.29153
196/196 - 69s - loss: 17.2799 - MinusLogProbMetric: 17.2799 - val_loss: 17.3890 - val_MinusLogProbMetric: 17.3890 - lr: 5.5556e-05 - 69s/epoch - 351ms/step
Epoch 554/1000
2023-10-23 22:53:58.642 
Epoch 554/1000 
	 loss: 17.2792, MinusLogProbMetric: 17.2792, val_loss: 17.3071, val_MinusLogProbMetric: 17.3071

Epoch 554: val_loss did not improve from 17.29153
196/196 - 74s - loss: 17.2792 - MinusLogProbMetric: 17.2792 - val_loss: 17.3071 - val_MinusLogProbMetric: 17.3071 - lr: 5.5556e-05 - 74s/epoch - 380ms/step
Epoch 555/1000
2023-10-23 22:55:11.836 
Epoch 555/1000 
	 loss: 17.2726, MinusLogProbMetric: 17.2726, val_loss: 17.3563, val_MinusLogProbMetric: 17.3563

Epoch 555: val_loss did not improve from 17.29153
196/196 - 73s - loss: 17.2726 - MinusLogProbMetric: 17.2726 - val_loss: 17.3563 - val_MinusLogProbMetric: 17.3563 - lr: 5.5556e-05 - 73s/epoch - 373ms/step
Epoch 556/1000
2023-10-23 22:56:26.111 
Epoch 556/1000 
	 loss: 17.2805, MinusLogProbMetric: 17.2805, val_loss: 17.3487, val_MinusLogProbMetric: 17.3487

Epoch 556: val_loss did not improve from 17.29153
196/196 - 74s - loss: 17.2805 - MinusLogProbMetric: 17.2805 - val_loss: 17.3487 - val_MinusLogProbMetric: 17.3487 - lr: 5.5556e-05 - 74s/epoch - 379ms/step
Epoch 557/1000
2023-10-23 22:57:40.234 
Epoch 557/1000 
	 loss: 17.2889, MinusLogProbMetric: 17.2889, val_loss: 17.3319, val_MinusLogProbMetric: 17.3319

Epoch 557: val_loss did not improve from 17.29153
196/196 - 74s - loss: 17.2889 - MinusLogProbMetric: 17.2889 - val_loss: 17.3319 - val_MinusLogProbMetric: 17.3319 - lr: 5.5556e-05 - 74s/epoch - 378ms/step
Epoch 558/1000
2023-10-23 22:58:52.932 
Epoch 558/1000 
	 loss: 17.2933, MinusLogProbMetric: 17.2933, val_loss: 17.3242, val_MinusLogProbMetric: 17.3242

Epoch 558: val_loss did not improve from 17.29153
196/196 - 73s - loss: 17.2933 - MinusLogProbMetric: 17.2933 - val_loss: 17.3242 - val_MinusLogProbMetric: 17.3242 - lr: 5.5556e-05 - 73s/epoch - 371ms/step
Epoch 559/1000
2023-10-23 23:00:05.509 
Epoch 559/1000 
	 loss: 17.2744, MinusLogProbMetric: 17.2744, val_loss: 17.2998, val_MinusLogProbMetric: 17.2998

Epoch 559: val_loss did not improve from 17.29153
196/196 - 73s - loss: 17.2744 - MinusLogProbMetric: 17.2744 - val_loss: 17.2998 - val_MinusLogProbMetric: 17.2998 - lr: 5.5556e-05 - 73s/epoch - 370ms/step
Epoch 560/1000
2023-10-23 23:01:13.089 
Epoch 560/1000 
	 loss: 17.2948, MinusLogProbMetric: 17.2948, val_loss: 17.3291, val_MinusLogProbMetric: 17.3291

Epoch 560: val_loss did not improve from 17.29153
196/196 - 68s - loss: 17.2948 - MinusLogProbMetric: 17.2948 - val_loss: 17.3291 - val_MinusLogProbMetric: 17.3291 - lr: 5.5556e-05 - 68s/epoch - 345ms/step
Epoch 561/1000
2023-10-23 23:02:22.159 
Epoch 561/1000 
	 loss: 17.3132, MinusLogProbMetric: 17.3132, val_loss: 17.3383, val_MinusLogProbMetric: 17.3383

Epoch 561: val_loss did not improve from 17.29153
196/196 - 69s - loss: 17.3132 - MinusLogProbMetric: 17.3132 - val_loss: 17.3383 - val_MinusLogProbMetric: 17.3383 - lr: 5.5556e-05 - 69s/epoch - 352ms/step
Epoch 562/1000
2023-10-23 23:03:30.235 
Epoch 562/1000 
	 loss: 17.2760, MinusLogProbMetric: 17.2760, val_loss: 17.2782, val_MinusLogProbMetric: 17.2782

Epoch 562: val_loss improved from 17.29153 to 17.27818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 69s - loss: 17.2760 - MinusLogProbMetric: 17.2760 - val_loss: 17.2782 - val_MinusLogProbMetric: 17.2782 - lr: 5.5556e-05 - 69s/epoch - 353ms/step
Epoch 563/1000
2023-10-23 23:04:38.438 
Epoch 563/1000 
	 loss: 17.2800, MinusLogProbMetric: 17.2800, val_loss: 17.4007, val_MinusLogProbMetric: 17.4007

Epoch 563: val_loss did not improve from 17.27818
196/196 - 67s - loss: 17.2800 - MinusLogProbMetric: 17.2800 - val_loss: 17.4007 - val_MinusLogProbMetric: 17.4007 - lr: 5.5556e-05 - 67s/epoch - 342ms/step
Epoch 564/1000
2023-10-23 23:05:48.538 
Epoch 564/1000 
	 loss: 17.3155, MinusLogProbMetric: 17.3155, val_loss: 17.5063, val_MinusLogProbMetric: 17.5063

Epoch 564: val_loss did not improve from 17.27818
196/196 - 70s - loss: 17.3155 - MinusLogProbMetric: 17.3155 - val_loss: 17.5063 - val_MinusLogProbMetric: 17.5063 - lr: 5.5556e-05 - 70s/epoch - 358ms/step
Epoch 565/1000
2023-10-23 23:06:54.585 
Epoch 565/1000 
	 loss: 17.2794, MinusLogProbMetric: 17.2794, val_loss: 17.2875, val_MinusLogProbMetric: 17.2875

Epoch 565: val_loss did not improve from 17.27818
196/196 - 66s - loss: 17.2794 - MinusLogProbMetric: 17.2794 - val_loss: 17.2875 - val_MinusLogProbMetric: 17.2875 - lr: 5.5556e-05 - 66s/epoch - 337ms/step
Epoch 566/1000
2023-10-23 23:08:01.297 
Epoch 566/1000 
	 loss: 17.2801, MinusLogProbMetric: 17.2801, val_loss: 17.3011, val_MinusLogProbMetric: 17.3011

Epoch 566: val_loss did not improve from 17.27818
196/196 - 67s - loss: 17.2801 - MinusLogProbMetric: 17.2801 - val_loss: 17.3011 - val_MinusLogProbMetric: 17.3011 - lr: 5.5556e-05 - 67s/epoch - 340ms/step
Epoch 567/1000
2023-10-23 23:09:04.918 
Epoch 567/1000 
	 loss: 17.4144, MinusLogProbMetric: 17.4144, val_loss: 17.6674, val_MinusLogProbMetric: 17.6674

Epoch 567: val_loss did not improve from 17.27818
196/196 - 64s - loss: 17.4144 - MinusLogProbMetric: 17.4144 - val_loss: 17.6674 - val_MinusLogProbMetric: 17.6674 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 568/1000
2023-10-23 23:10:07.168 
Epoch 568/1000 
	 loss: 17.6003, MinusLogProbMetric: 17.6003, val_loss: 17.3987, val_MinusLogProbMetric: 17.3987

Epoch 568: val_loss did not improve from 17.27818
196/196 - 62s - loss: 17.6003 - MinusLogProbMetric: 17.6003 - val_loss: 17.3987 - val_MinusLogProbMetric: 17.3987 - lr: 5.5556e-05 - 62s/epoch - 318ms/step
Epoch 569/1000
2023-10-23 23:11:16.149 
Epoch 569/1000 
	 loss: 17.2870, MinusLogProbMetric: 17.2870, val_loss: 17.3210, val_MinusLogProbMetric: 17.3210

Epoch 569: val_loss did not improve from 17.27818
196/196 - 69s - loss: 17.2870 - MinusLogProbMetric: 17.2870 - val_loss: 17.3210 - val_MinusLogProbMetric: 17.3210 - lr: 5.5556e-05 - 69s/epoch - 352ms/step
Epoch 570/1000
2023-10-23 23:12:18.374 
Epoch 570/1000 
	 loss: 17.2886, MinusLogProbMetric: 17.2886, val_loss: 17.3895, val_MinusLogProbMetric: 17.3895

Epoch 570: val_loss did not improve from 17.27818
196/196 - 62s - loss: 17.2886 - MinusLogProbMetric: 17.2886 - val_loss: 17.3895 - val_MinusLogProbMetric: 17.3895 - lr: 5.5556e-05 - 62s/epoch - 317ms/step
Epoch 571/1000
2023-10-23 23:13:23.997 
Epoch 571/1000 
	 loss: 17.2665, MinusLogProbMetric: 17.2665, val_loss: 17.4925, val_MinusLogProbMetric: 17.4925

Epoch 571: val_loss did not improve from 17.27818
196/196 - 66s - loss: 17.2665 - MinusLogProbMetric: 17.2665 - val_loss: 17.4925 - val_MinusLogProbMetric: 17.4925 - lr: 5.5556e-05 - 66s/epoch - 335ms/step
Epoch 572/1000
2023-10-23 23:14:28.131 
Epoch 572/1000 
	 loss: 17.2907, MinusLogProbMetric: 17.2907, val_loss: 17.5944, val_MinusLogProbMetric: 17.5944

Epoch 572: val_loss did not improve from 17.27818
196/196 - 64s - loss: 17.2907 - MinusLogProbMetric: 17.2907 - val_loss: 17.5944 - val_MinusLogProbMetric: 17.5944 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 573/1000
2023-10-23 23:15:30.974 
Epoch 573/1000 
	 loss: 17.2961, MinusLogProbMetric: 17.2961, val_loss: 17.3110, val_MinusLogProbMetric: 17.3110

Epoch 573: val_loss did not improve from 17.27818
196/196 - 63s - loss: 17.2961 - MinusLogProbMetric: 17.2961 - val_loss: 17.3110 - val_MinusLogProbMetric: 17.3110 - lr: 5.5556e-05 - 63s/epoch - 321ms/step
Epoch 574/1000
2023-10-23 23:16:34.841 
Epoch 574/1000 
	 loss: 17.2711, MinusLogProbMetric: 17.2711, val_loss: 17.3669, val_MinusLogProbMetric: 17.3669

Epoch 574: val_loss did not improve from 17.27818
196/196 - 64s - loss: 17.2711 - MinusLogProbMetric: 17.2711 - val_loss: 17.3669 - val_MinusLogProbMetric: 17.3669 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 575/1000
2023-10-23 23:17:38.386 
Epoch 575/1000 
	 loss: 17.2809, MinusLogProbMetric: 17.2809, val_loss: 17.2791, val_MinusLogProbMetric: 17.2791

Epoch 575: val_loss did not improve from 17.27818
196/196 - 64s - loss: 17.2809 - MinusLogProbMetric: 17.2809 - val_loss: 17.2791 - val_MinusLogProbMetric: 17.2791 - lr: 5.5556e-05 - 64s/epoch - 324ms/step
Epoch 576/1000
2023-10-23 23:18:42.954 
Epoch 576/1000 
	 loss: 17.2743, MinusLogProbMetric: 17.2743, val_loss: 17.4017, val_MinusLogProbMetric: 17.4017

Epoch 576: val_loss did not improve from 17.27818
196/196 - 65s - loss: 17.2743 - MinusLogProbMetric: 17.2743 - val_loss: 17.4017 - val_MinusLogProbMetric: 17.4017 - lr: 5.5556e-05 - 65s/epoch - 329ms/step
Epoch 577/1000
2023-10-23 23:19:50.649 
Epoch 577/1000 
	 loss: 17.2828, MinusLogProbMetric: 17.2828, val_loss: 17.4178, val_MinusLogProbMetric: 17.4178

Epoch 577: val_loss did not improve from 17.27818
196/196 - 68s - loss: 17.2828 - MinusLogProbMetric: 17.2828 - val_loss: 17.4178 - val_MinusLogProbMetric: 17.4178 - lr: 5.5556e-05 - 68s/epoch - 345ms/step
Epoch 578/1000
2023-10-23 23:20:56.717 
Epoch 578/1000 
	 loss: 17.2750, MinusLogProbMetric: 17.2750, val_loss: 17.2791, val_MinusLogProbMetric: 17.2791

Epoch 578: val_loss did not improve from 17.27818
196/196 - 66s - loss: 17.2750 - MinusLogProbMetric: 17.2750 - val_loss: 17.2791 - val_MinusLogProbMetric: 17.2791 - lr: 5.5556e-05 - 66s/epoch - 337ms/step
Epoch 579/1000
2023-10-23 23:22:01.032 
Epoch 579/1000 
	 loss: 17.2708, MinusLogProbMetric: 17.2708, val_loss: 17.2211, val_MinusLogProbMetric: 17.2211

Epoch 579: val_loss improved from 17.27818 to 17.22112, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 17.2708 - MinusLogProbMetric: 17.2708 - val_loss: 17.2211 - val_MinusLogProbMetric: 17.2211 - lr: 5.5556e-05 - 65s/epoch - 334ms/step
Epoch 580/1000
2023-10-23 23:23:02.143 
Epoch 580/1000 
	 loss: 17.2737, MinusLogProbMetric: 17.2737, val_loss: 17.3197, val_MinusLogProbMetric: 17.3197

Epoch 580: val_loss did not improve from 17.22112
196/196 - 60s - loss: 17.2737 - MinusLogProbMetric: 17.2737 - val_loss: 17.3197 - val_MinusLogProbMetric: 17.3197 - lr: 5.5556e-05 - 60s/epoch - 306ms/step
Epoch 581/1000
2023-10-23 23:24:07.493 
Epoch 581/1000 
	 loss: 17.2912, MinusLogProbMetric: 17.2912, val_loss: 17.2922, val_MinusLogProbMetric: 17.2922

Epoch 581: val_loss did not improve from 17.22112
196/196 - 65s - loss: 17.2912 - MinusLogProbMetric: 17.2912 - val_loss: 17.2922 - val_MinusLogProbMetric: 17.2922 - lr: 5.5556e-05 - 65s/epoch - 333ms/step
Epoch 582/1000
2023-10-23 23:25:14.582 
Epoch 582/1000 
	 loss: 17.2678, MinusLogProbMetric: 17.2678, val_loss: 17.4398, val_MinusLogProbMetric: 17.4398

Epoch 582: val_loss did not improve from 17.22112
196/196 - 67s - loss: 17.2678 - MinusLogProbMetric: 17.2678 - val_loss: 17.4398 - val_MinusLogProbMetric: 17.4398 - lr: 5.5556e-05 - 67s/epoch - 342ms/step
Epoch 583/1000
2023-10-23 23:26:19.020 
Epoch 583/1000 
	 loss: 17.3058, MinusLogProbMetric: 17.3058, val_loss: 17.3339, val_MinusLogProbMetric: 17.3339

Epoch 583: val_loss did not improve from 17.22112
196/196 - 64s - loss: 17.3058 - MinusLogProbMetric: 17.3058 - val_loss: 17.3339 - val_MinusLogProbMetric: 17.3339 - lr: 5.5556e-05 - 64s/epoch - 329ms/step
Epoch 584/1000
2023-10-23 23:27:25.344 
Epoch 584/1000 
	 loss: 17.2947, MinusLogProbMetric: 17.2947, val_loss: 17.5637, val_MinusLogProbMetric: 17.5637

Epoch 584: val_loss did not improve from 17.22112
196/196 - 66s - loss: 17.2947 - MinusLogProbMetric: 17.2947 - val_loss: 17.5637 - val_MinusLogProbMetric: 17.5637 - lr: 5.5556e-05 - 66s/epoch - 338ms/step
Epoch 585/1000
2023-10-23 23:28:32.986 
Epoch 585/1000 
	 loss: 17.3033, MinusLogProbMetric: 17.3033, val_loss: 17.3217, val_MinusLogProbMetric: 17.3217

Epoch 585: val_loss did not improve from 17.22112
196/196 - 68s - loss: 17.3033 - MinusLogProbMetric: 17.3033 - val_loss: 17.3217 - val_MinusLogProbMetric: 17.3217 - lr: 5.5556e-05 - 68s/epoch - 345ms/step
Epoch 586/1000
2023-10-23 23:29:34.469 
Epoch 586/1000 
	 loss: 17.3021, MinusLogProbMetric: 17.3021, val_loss: 17.3329, val_MinusLogProbMetric: 17.3329

Epoch 586: val_loss did not improve from 17.22112
196/196 - 61s - loss: 17.3021 - MinusLogProbMetric: 17.3021 - val_loss: 17.3329 - val_MinusLogProbMetric: 17.3329 - lr: 5.5556e-05 - 61s/epoch - 314ms/step
Epoch 587/1000
2023-10-23 23:30:37.197 
Epoch 587/1000 
	 loss: 17.2982, MinusLogProbMetric: 17.2982, val_loss: 17.3020, val_MinusLogProbMetric: 17.3020

Epoch 587: val_loss did not improve from 17.22112
196/196 - 63s - loss: 17.2982 - MinusLogProbMetric: 17.2982 - val_loss: 17.3020 - val_MinusLogProbMetric: 17.3020 - lr: 5.5556e-05 - 63s/epoch - 320ms/step
Epoch 588/1000
2023-10-23 23:31:47.986 
Epoch 588/1000 
	 loss: 17.3021, MinusLogProbMetric: 17.3021, val_loss: 17.3258, val_MinusLogProbMetric: 17.3258

Epoch 588: val_loss did not improve from 17.22112
196/196 - 71s - loss: 17.3021 - MinusLogProbMetric: 17.3021 - val_loss: 17.3258 - val_MinusLogProbMetric: 17.3258 - lr: 5.5556e-05 - 71s/epoch - 361ms/step
Epoch 589/1000
2023-10-23 23:32:52.062 
Epoch 589/1000 
	 loss: 17.2644, MinusLogProbMetric: 17.2644, val_loss: 17.5002, val_MinusLogProbMetric: 17.5002

Epoch 589: val_loss did not improve from 17.22112
196/196 - 64s - loss: 17.2644 - MinusLogProbMetric: 17.2644 - val_loss: 17.5002 - val_MinusLogProbMetric: 17.5002 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 590/1000
2023-10-23 23:33:58.050 
Epoch 590/1000 
	 loss: 17.3339, MinusLogProbMetric: 17.3339, val_loss: 17.4281, val_MinusLogProbMetric: 17.4281

Epoch 590: val_loss did not improve from 17.22112
196/196 - 66s - loss: 17.3339 - MinusLogProbMetric: 17.3339 - val_loss: 17.4281 - val_MinusLogProbMetric: 17.4281 - lr: 5.5556e-05 - 66s/epoch - 337ms/step
Epoch 591/1000
2023-10-23 23:35:06.700 
Epoch 591/1000 
	 loss: 17.2811, MinusLogProbMetric: 17.2811, val_loss: 17.3213, val_MinusLogProbMetric: 17.3213

Epoch 591: val_loss did not improve from 17.22112
196/196 - 69s - loss: 17.2811 - MinusLogProbMetric: 17.2811 - val_loss: 17.3213 - val_MinusLogProbMetric: 17.3213 - lr: 5.5556e-05 - 69s/epoch - 350ms/step
Epoch 592/1000
2023-10-23 23:36:14.955 
Epoch 592/1000 
	 loss: 17.2796, MinusLogProbMetric: 17.2796, val_loss: 17.3239, val_MinusLogProbMetric: 17.3239

Epoch 592: val_loss did not improve from 17.22112
196/196 - 68s - loss: 17.2796 - MinusLogProbMetric: 17.2796 - val_loss: 17.3239 - val_MinusLogProbMetric: 17.3239 - lr: 5.5556e-05 - 68s/epoch - 348ms/step
Epoch 593/1000
2023-10-23 23:37:22.931 
Epoch 593/1000 
	 loss: 19.6031, MinusLogProbMetric: 19.6031, val_loss: 17.7519, val_MinusLogProbMetric: 17.7519

Epoch 593: val_loss did not improve from 17.22112
196/196 - 68s - loss: 19.6031 - MinusLogProbMetric: 19.6031 - val_loss: 17.7519 - val_MinusLogProbMetric: 17.7519 - lr: 5.5556e-05 - 68s/epoch - 347ms/step
Epoch 594/1000
2023-10-23 23:38:33.755 
Epoch 594/1000 
	 loss: 17.3974, MinusLogProbMetric: 17.3974, val_loss: 17.3949, val_MinusLogProbMetric: 17.3949

Epoch 594: val_loss did not improve from 17.22112
196/196 - 71s - loss: 17.3974 - MinusLogProbMetric: 17.3974 - val_loss: 17.3949 - val_MinusLogProbMetric: 17.3949 - lr: 5.5556e-05 - 71s/epoch - 361ms/step
Epoch 595/1000
2023-10-23 23:39:46.125 
Epoch 595/1000 
	 loss: 17.3032, MinusLogProbMetric: 17.3032, val_loss: 17.3186, val_MinusLogProbMetric: 17.3186

Epoch 595: val_loss did not improve from 17.22112
196/196 - 72s - loss: 17.3032 - MinusLogProbMetric: 17.3032 - val_loss: 17.3186 - val_MinusLogProbMetric: 17.3186 - lr: 5.5556e-05 - 72s/epoch - 369ms/step
Epoch 596/1000
2023-10-23 23:41:02.026 
Epoch 596/1000 
	 loss: 17.2893, MinusLogProbMetric: 17.2893, val_loss: 17.3205, val_MinusLogProbMetric: 17.3205

Epoch 596: val_loss did not improve from 17.22112
196/196 - 76s - loss: 17.2893 - MinusLogProbMetric: 17.2893 - val_loss: 17.3205 - val_MinusLogProbMetric: 17.3205 - lr: 5.5556e-05 - 76s/epoch - 387ms/step
Epoch 597/1000
2023-10-23 23:42:17.072 
Epoch 597/1000 
	 loss: 17.2779, MinusLogProbMetric: 17.2779, val_loss: 17.3165, val_MinusLogProbMetric: 17.3165

Epoch 597: val_loss did not improve from 17.22112
196/196 - 75s - loss: 17.2779 - MinusLogProbMetric: 17.2779 - val_loss: 17.3165 - val_MinusLogProbMetric: 17.3165 - lr: 5.5556e-05 - 75s/epoch - 383ms/step
Epoch 598/1000
2023-10-23 23:43:37.279 
Epoch 598/1000 
	 loss: 17.2830, MinusLogProbMetric: 17.2830, val_loss: 17.3582, val_MinusLogProbMetric: 17.3582

Epoch 598: val_loss did not improve from 17.22112
196/196 - 80s - loss: 17.2830 - MinusLogProbMetric: 17.2830 - val_loss: 17.3582 - val_MinusLogProbMetric: 17.3582 - lr: 5.5556e-05 - 80s/epoch - 409ms/step
Epoch 599/1000
2023-10-23 23:44:54.656 
Epoch 599/1000 
	 loss: 17.2464, MinusLogProbMetric: 17.2464, val_loss: 17.3138, val_MinusLogProbMetric: 17.3138

Epoch 599: val_loss did not improve from 17.22112
196/196 - 77s - loss: 17.2464 - MinusLogProbMetric: 17.2464 - val_loss: 17.3138 - val_MinusLogProbMetric: 17.3138 - lr: 5.5556e-05 - 77s/epoch - 395ms/step
Epoch 600/1000
2023-10-23 23:46:10.682 
Epoch 600/1000 
	 loss: 17.2722, MinusLogProbMetric: 17.2722, val_loss: 17.3783, val_MinusLogProbMetric: 17.3783

Epoch 600: val_loss did not improve from 17.22112
196/196 - 76s - loss: 17.2722 - MinusLogProbMetric: 17.2722 - val_loss: 17.3783 - val_MinusLogProbMetric: 17.3783 - lr: 5.5556e-05 - 76s/epoch - 388ms/step
Epoch 601/1000
2023-10-23 23:47:25.469 
Epoch 601/1000 
	 loss: 17.2496, MinusLogProbMetric: 17.2496, val_loss: 17.3158, val_MinusLogProbMetric: 17.3158

Epoch 601: val_loss did not improve from 17.22112
196/196 - 75s - loss: 17.2496 - MinusLogProbMetric: 17.2496 - val_loss: 17.3158 - val_MinusLogProbMetric: 17.3158 - lr: 5.5556e-05 - 75s/epoch - 382ms/step
Epoch 602/1000
2023-10-23 23:48:39.977 
Epoch 602/1000 
	 loss: 17.2698, MinusLogProbMetric: 17.2698, val_loss: 17.3040, val_MinusLogProbMetric: 17.3040

Epoch 602: val_loss did not improve from 17.22112
196/196 - 75s - loss: 17.2698 - MinusLogProbMetric: 17.2698 - val_loss: 17.3040 - val_MinusLogProbMetric: 17.3040 - lr: 5.5556e-05 - 75s/epoch - 380ms/step
Epoch 603/1000
2023-10-23 23:49:55.860 
Epoch 603/1000 
	 loss: 17.2603, MinusLogProbMetric: 17.2603, val_loss: 17.2297, val_MinusLogProbMetric: 17.2297

Epoch 603: val_loss did not improve from 17.22112
196/196 - 76s - loss: 17.2603 - MinusLogProbMetric: 17.2603 - val_loss: 17.2297 - val_MinusLogProbMetric: 17.2297 - lr: 5.5556e-05 - 76s/epoch - 387ms/step
Epoch 604/1000
2023-10-23 23:51:14.583 
Epoch 604/1000 
	 loss: 17.2614, MinusLogProbMetric: 17.2614, val_loss: 17.4069, val_MinusLogProbMetric: 17.4069

Epoch 604: val_loss did not improve from 17.22112
196/196 - 79s - loss: 17.2614 - MinusLogProbMetric: 17.2614 - val_loss: 17.4069 - val_MinusLogProbMetric: 17.4069 - lr: 5.5556e-05 - 79s/epoch - 402ms/step
Epoch 605/1000
2023-10-23 23:52:31.495 
Epoch 605/1000 
	 loss: 17.2462, MinusLogProbMetric: 17.2462, val_loss: 17.2999, val_MinusLogProbMetric: 17.2999

Epoch 605: val_loss did not improve from 17.22112
196/196 - 77s - loss: 17.2462 - MinusLogProbMetric: 17.2462 - val_loss: 17.2999 - val_MinusLogProbMetric: 17.2999 - lr: 5.5556e-05 - 77s/epoch - 392ms/step
Epoch 606/1000
2023-10-23 23:53:49.024 
Epoch 606/1000 
	 loss: 17.2780, MinusLogProbMetric: 17.2780, val_loss: 17.2319, val_MinusLogProbMetric: 17.2319

Epoch 606: val_loss did not improve from 17.22112
196/196 - 78s - loss: 17.2780 - MinusLogProbMetric: 17.2780 - val_loss: 17.2319 - val_MinusLogProbMetric: 17.2319 - lr: 5.5556e-05 - 78s/epoch - 396ms/step
Epoch 607/1000
2023-10-23 23:55:02.699 
Epoch 607/1000 
	 loss: 17.2671, MinusLogProbMetric: 17.2671, val_loss: 17.3798, val_MinusLogProbMetric: 17.3798

Epoch 607: val_loss did not improve from 17.22112
196/196 - 74s - loss: 17.2671 - MinusLogProbMetric: 17.2671 - val_loss: 17.3798 - val_MinusLogProbMetric: 17.3798 - lr: 5.5556e-05 - 74s/epoch - 376ms/step
Epoch 608/1000
2023-10-23 23:56:20.496 
Epoch 608/1000 
	 loss: 17.2552, MinusLogProbMetric: 17.2552, val_loss: 17.5401, val_MinusLogProbMetric: 17.5401

Epoch 608: val_loss did not improve from 17.22112
196/196 - 78s - loss: 17.2552 - MinusLogProbMetric: 17.2552 - val_loss: 17.5401 - val_MinusLogProbMetric: 17.5401 - lr: 5.5556e-05 - 78s/epoch - 397ms/step
Epoch 609/1000
2023-10-23 23:57:37.353 
Epoch 609/1000 
	 loss: 17.2655, MinusLogProbMetric: 17.2655, val_loss: 17.2206, val_MinusLogProbMetric: 17.2206

Epoch 609: val_loss improved from 17.22112 to 17.22059, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 78s - loss: 17.2655 - MinusLogProbMetric: 17.2655 - val_loss: 17.2206 - val_MinusLogProbMetric: 17.2206 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 610/1000
2023-10-23 23:58:54.820 
Epoch 610/1000 
	 loss: 17.2676, MinusLogProbMetric: 17.2676, val_loss: 17.3195, val_MinusLogProbMetric: 17.3195

Epoch 610: val_loss did not improve from 17.22059
196/196 - 76s - loss: 17.2676 - MinusLogProbMetric: 17.2676 - val_loss: 17.3195 - val_MinusLogProbMetric: 17.3195 - lr: 5.5556e-05 - 76s/epoch - 390ms/step
Epoch 611/1000
2023-10-24 00:00:10.800 
Epoch 611/1000 
	 loss: 17.2852, MinusLogProbMetric: 17.2852, val_loss: 17.6330, val_MinusLogProbMetric: 17.6330

Epoch 611: val_loss did not improve from 17.22059
196/196 - 76s - loss: 17.2852 - MinusLogProbMetric: 17.2852 - val_loss: 17.6330 - val_MinusLogProbMetric: 17.6330 - lr: 5.5556e-05 - 76s/epoch - 388ms/step
Epoch 612/1000
2023-10-24 00:01:27.189 
Epoch 612/1000 
	 loss: 17.2932, MinusLogProbMetric: 17.2932, val_loss: 17.3299, val_MinusLogProbMetric: 17.3299

Epoch 612: val_loss did not improve from 17.22059
196/196 - 76s - loss: 17.2932 - MinusLogProbMetric: 17.2932 - val_loss: 17.3299 - val_MinusLogProbMetric: 17.3299 - lr: 5.5556e-05 - 76s/epoch - 390ms/step
Epoch 613/1000
2023-10-24 00:02:42.820 
Epoch 613/1000 
	 loss: 17.2645, MinusLogProbMetric: 17.2645, val_loss: 17.2634, val_MinusLogProbMetric: 17.2634

Epoch 613: val_loss did not improve from 17.22059
196/196 - 76s - loss: 17.2645 - MinusLogProbMetric: 17.2645 - val_loss: 17.2634 - val_MinusLogProbMetric: 17.2634 - lr: 5.5556e-05 - 76s/epoch - 386ms/step
Epoch 614/1000
2023-10-24 00:03:56.936 
Epoch 614/1000 
	 loss: 17.2464, MinusLogProbMetric: 17.2464, val_loss: 17.2951, val_MinusLogProbMetric: 17.2951

Epoch 614: val_loss did not improve from 17.22059
196/196 - 74s - loss: 17.2464 - MinusLogProbMetric: 17.2464 - val_loss: 17.2951 - val_MinusLogProbMetric: 17.2951 - lr: 5.5556e-05 - 74s/epoch - 378ms/step
Epoch 615/1000
2023-10-24 00:05:12.794 
Epoch 615/1000 
	 loss: 17.2460, MinusLogProbMetric: 17.2460, val_loss: 17.3516, val_MinusLogProbMetric: 17.3516

Epoch 615: val_loss did not improve from 17.22059
196/196 - 76s - loss: 17.2460 - MinusLogProbMetric: 17.2460 - val_loss: 17.3516 - val_MinusLogProbMetric: 17.3516 - lr: 5.5556e-05 - 76s/epoch - 387ms/step
Epoch 616/1000
2023-10-24 00:06:29.353 
Epoch 616/1000 
	 loss: 17.2665, MinusLogProbMetric: 17.2665, val_loss: 17.3062, val_MinusLogProbMetric: 17.3062

Epoch 616: val_loss did not improve from 17.22059
196/196 - 77s - loss: 17.2665 - MinusLogProbMetric: 17.2665 - val_loss: 17.3062 - val_MinusLogProbMetric: 17.3062 - lr: 5.5556e-05 - 77s/epoch - 391ms/step
Epoch 617/1000
2023-10-24 00:07:42.146 
Epoch 617/1000 
	 loss: 17.2377, MinusLogProbMetric: 17.2377, val_loss: 17.2573, val_MinusLogProbMetric: 17.2573

Epoch 617: val_loss did not improve from 17.22059
196/196 - 73s - loss: 17.2377 - MinusLogProbMetric: 17.2377 - val_loss: 17.2573 - val_MinusLogProbMetric: 17.2573 - lr: 5.5556e-05 - 73s/epoch - 371ms/step
Epoch 618/1000
2023-10-24 00:08:56.425 
Epoch 618/1000 
	 loss: 17.2416, MinusLogProbMetric: 17.2416, val_loss: 17.3300, val_MinusLogProbMetric: 17.3300

Epoch 618: val_loss did not improve from 17.22059
196/196 - 74s - loss: 17.2416 - MinusLogProbMetric: 17.2416 - val_loss: 17.3300 - val_MinusLogProbMetric: 17.3300 - lr: 5.5556e-05 - 74s/epoch - 379ms/step
Epoch 619/1000
2023-10-24 00:10:11.936 
Epoch 619/1000 
	 loss: 17.2563, MinusLogProbMetric: 17.2563, val_loss: 17.4836, val_MinusLogProbMetric: 17.4836

Epoch 619: val_loss did not improve from 17.22059
196/196 - 76s - loss: 17.2563 - MinusLogProbMetric: 17.2563 - val_loss: 17.4836 - val_MinusLogProbMetric: 17.4836 - lr: 5.5556e-05 - 76s/epoch - 385ms/step
Epoch 620/1000
2023-10-24 00:11:26.828 
Epoch 620/1000 
	 loss: 17.2429, MinusLogProbMetric: 17.2429, val_loss: 17.3727, val_MinusLogProbMetric: 17.3727

Epoch 620: val_loss did not improve from 17.22059
196/196 - 75s - loss: 17.2429 - MinusLogProbMetric: 17.2429 - val_loss: 17.3727 - val_MinusLogProbMetric: 17.3727 - lr: 5.5556e-05 - 75s/epoch - 382ms/step
Epoch 621/1000
2023-10-24 00:12:42.715 
Epoch 621/1000 
	 loss: 17.2647, MinusLogProbMetric: 17.2647, val_loss: 17.2757, val_MinusLogProbMetric: 17.2757

Epoch 621: val_loss did not improve from 17.22059
196/196 - 76s - loss: 17.2647 - MinusLogProbMetric: 17.2647 - val_loss: 17.2757 - val_MinusLogProbMetric: 17.2757 - lr: 5.5556e-05 - 76s/epoch - 387ms/step
Epoch 622/1000
2023-10-24 00:13:58.486 
Epoch 622/1000 
	 loss: 17.2437, MinusLogProbMetric: 17.2437, val_loss: 17.2316, val_MinusLogProbMetric: 17.2316

Epoch 622: val_loss did not improve from 17.22059
196/196 - 76s - loss: 17.2437 - MinusLogProbMetric: 17.2437 - val_loss: 17.2316 - val_MinusLogProbMetric: 17.2316 - lr: 5.5556e-05 - 76s/epoch - 387ms/step
Epoch 623/1000
2023-10-24 00:15:14.456 
Epoch 623/1000 
	 loss: 17.2528, MinusLogProbMetric: 17.2528, val_loss: 17.3882, val_MinusLogProbMetric: 17.3882

Epoch 623: val_loss did not improve from 17.22059
196/196 - 76s - loss: 17.2528 - MinusLogProbMetric: 17.2528 - val_loss: 17.3882 - val_MinusLogProbMetric: 17.3882 - lr: 5.5556e-05 - 76s/epoch - 388ms/step
Epoch 624/1000
2023-10-24 00:16:30.852 
Epoch 624/1000 
	 loss: 17.2673, MinusLogProbMetric: 17.2673, val_loss: 17.2730, val_MinusLogProbMetric: 17.2730

Epoch 624: val_loss did not improve from 17.22059
196/196 - 76s - loss: 17.2673 - MinusLogProbMetric: 17.2673 - val_loss: 17.2730 - val_MinusLogProbMetric: 17.2730 - lr: 5.5556e-05 - 76s/epoch - 390ms/step
Epoch 625/1000
2023-10-24 00:17:47.558 
Epoch 625/1000 
	 loss: 17.2429, MinusLogProbMetric: 17.2429, val_loss: 17.3361, val_MinusLogProbMetric: 17.3361

Epoch 625: val_loss did not improve from 17.22059
196/196 - 77s - loss: 17.2429 - MinusLogProbMetric: 17.2429 - val_loss: 17.3361 - val_MinusLogProbMetric: 17.3361 - lr: 5.5556e-05 - 77s/epoch - 391ms/step
Epoch 626/1000
2023-10-24 00:19:03.854 
Epoch 626/1000 
	 loss: 17.2552, MinusLogProbMetric: 17.2552, val_loss: 17.3372, val_MinusLogProbMetric: 17.3372

Epoch 626: val_loss did not improve from 17.22059
196/196 - 76s - loss: 17.2552 - MinusLogProbMetric: 17.2552 - val_loss: 17.3372 - val_MinusLogProbMetric: 17.3372 - lr: 5.5556e-05 - 76s/epoch - 389ms/step
Epoch 627/1000
2023-10-24 00:20:19.575 
Epoch 627/1000 
	 loss: 17.2455, MinusLogProbMetric: 17.2455, val_loss: 17.2439, val_MinusLogProbMetric: 17.2439

Epoch 627: val_loss did not improve from 17.22059
196/196 - 76s - loss: 17.2455 - MinusLogProbMetric: 17.2455 - val_loss: 17.2439 - val_MinusLogProbMetric: 17.2439 - lr: 5.5556e-05 - 76s/epoch - 386ms/step
Epoch 628/1000
2023-10-24 00:21:36.168 
Epoch 628/1000 
	 loss: 17.2325, MinusLogProbMetric: 17.2325, val_loss: 17.2707, val_MinusLogProbMetric: 17.2707

Epoch 628: val_loss did not improve from 17.22059
196/196 - 77s - loss: 17.2325 - MinusLogProbMetric: 17.2325 - val_loss: 17.2707 - val_MinusLogProbMetric: 17.2707 - lr: 5.5556e-05 - 77s/epoch - 391ms/step
Epoch 629/1000
2023-10-24 00:22:52.670 
Epoch 629/1000 
	 loss: 17.2449, MinusLogProbMetric: 17.2449, val_loss: 17.3671, val_MinusLogProbMetric: 17.3671

Epoch 629: val_loss did not improve from 17.22059
196/196 - 76s - loss: 17.2449 - MinusLogProbMetric: 17.2449 - val_loss: 17.3671 - val_MinusLogProbMetric: 17.3671 - lr: 5.5556e-05 - 76s/epoch - 390ms/step
Epoch 630/1000
2023-10-24 00:24:09.876 
Epoch 630/1000 
	 loss: 17.2341, MinusLogProbMetric: 17.2341, val_loss: 17.2849, val_MinusLogProbMetric: 17.2849

Epoch 630: val_loss did not improve from 17.22059
196/196 - 77s - loss: 17.2341 - MinusLogProbMetric: 17.2341 - val_loss: 17.2849 - val_MinusLogProbMetric: 17.2849 - lr: 5.5556e-05 - 77s/epoch - 394ms/step
Epoch 631/1000
2023-10-24 00:25:27.192 
Epoch 631/1000 
	 loss: 17.2689, MinusLogProbMetric: 17.2689, val_loss: 17.2864, val_MinusLogProbMetric: 17.2864

Epoch 631: val_loss did not improve from 17.22059
196/196 - 77s - loss: 17.2689 - MinusLogProbMetric: 17.2689 - val_loss: 17.2864 - val_MinusLogProbMetric: 17.2864 - lr: 5.5556e-05 - 77s/epoch - 394ms/step
Epoch 632/1000
2023-10-24 00:26:41.842 
Epoch 632/1000 
	 loss: 17.2484, MinusLogProbMetric: 17.2484, val_loss: 17.3633, val_MinusLogProbMetric: 17.3633

Epoch 632: val_loss did not improve from 17.22059
196/196 - 75s - loss: 17.2484 - MinusLogProbMetric: 17.2484 - val_loss: 17.3633 - val_MinusLogProbMetric: 17.3633 - lr: 5.5556e-05 - 75s/epoch - 381ms/step
Epoch 633/1000
2023-10-24 00:27:52.498 
Epoch 633/1000 
	 loss: 17.2360, MinusLogProbMetric: 17.2360, val_loss: 17.2036, val_MinusLogProbMetric: 17.2036

Epoch 633: val_loss improved from 17.22059 to 17.20357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 72s - loss: 17.2360 - MinusLogProbMetric: 17.2360 - val_loss: 17.2036 - val_MinusLogProbMetric: 17.2036 - lr: 5.5556e-05 - 72s/epoch - 366ms/step
Epoch 634/1000
2023-10-24 00:29:07.467 
Epoch 634/1000 
	 loss: 17.2492, MinusLogProbMetric: 17.2492, val_loss: 17.3330, val_MinusLogProbMetric: 17.3330

Epoch 634: val_loss did not improve from 17.20357
196/196 - 74s - loss: 17.2492 - MinusLogProbMetric: 17.2492 - val_loss: 17.3330 - val_MinusLogProbMetric: 17.3330 - lr: 5.5556e-05 - 74s/epoch - 377ms/step
Epoch 635/1000
2023-10-24 00:30:22.781 
Epoch 635/1000 
	 loss: 17.2430, MinusLogProbMetric: 17.2430, val_loss: 17.2590, val_MinusLogProbMetric: 17.2590

Epoch 635: val_loss did not improve from 17.20357
196/196 - 75s - loss: 17.2430 - MinusLogProbMetric: 17.2430 - val_loss: 17.2590 - val_MinusLogProbMetric: 17.2590 - lr: 5.5556e-05 - 75s/epoch - 384ms/step
Epoch 636/1000
2023-10-24 00:31:38.407 
Epoch 636/1000 
	 loss: 17.2191, MinusLogProbMetric: 17.2191, val_loss: 17.2712, val_MinusLogProbMetric: 17.2712

Epoch 636: val_loss did not improve from 17.20357
196/196 - 76s - loss: 17.2191 - MinusLogProbMetric: 17.2191 - val_loss: 17.2712 - val_MinusLogProbMetric: 17.2712 - lr: 5.5556e-05 - 76s/epoch - 386ms/step
Epoch 637/1000
2023-10-24 00:32:48.555 
Epoch 637/1000 
	 loss: 17.2513, MinusLogProbMetric: 17.2513, val_loss: 17.2749, val_MinusLogProbMetric: 17.2749

Epoch 637: val_loss did not improve from 17.20357
196/196 - 70s - loss: 17.2513 - MinusLogProbMetric: 17.2513 - val_loss: 17.2749 - val_MinusLogProbMetric: 17.2749 - lr: 5.5556e-05 - 70s/epoch - 358ms/step
Epoch 638/1000
2023-10-24 00:34:02.595 
Epoch 638/1000 
	 loss: 17.2327, MinusLogProbMetric: 17.2327, val_loss: 17.2951, val_MinusLogProbMetric: 17.2951

Epoch 638: val_loss did not improve from 17.20357
196/196 - 74s - loss: 17.2327 - MinusLogProbMetric: 17.2327 - val_loss: 17.2951 - val_MinusLogProbMetric: 17.2951 - lr: 5.5556e-05 - 74s/epoch - 378ms/step
Epoch 639/1000
2023-10-24 00:35:17.599 
Epoch 639/1000 
	 loss: 17.2454, MinusLogProbMetric: 17.2454, val_loss: 17.4180, val_MinusLogProbMetric: 17.4180

Epoch 639: val_loss did not improve from 17.20357
196/196 - 75s - loss: 17.2454 - MinusLogProbMetric: 17.2454 - val_loss: 17.4180 - val_MinusLogProbMetric: 17.4180 - lr: 5.5556e-05 - 75s/epoch - 383ms/step
Epoch 640/1000
2023-10-24 00:36:31.989 
Epoch 640/1000 
	 loss: 17.2443, MinusLogProbMetric: 17.2443, val_loss: 17.3239, val_MinusLogProbMetric: 17.3239

Epoch 640: val_loss did not improve from 17.20357
196/196 - 74s - loss: 17.2443 - MinusLogProbMetric: 17.2443 - val_loss: 17.3239 - val_MinusLogProbMetric: 17.3239 - lr: 5.5556e-05 - 74s/epoch - 380ms/step
Epoch 641/1000
2023-10-24 00:37:44.456 
Epoch 641/1000 
	 loss: 17.2298, MinusLogProbMetric: 17.2298, val_loss: 17.2461, val_MinusLogProbMetric: 17.2461

Epoch 641: val_loss did not improve from 17.20357
196/196 - 72s - loss: 17.2298 - MinusLogProbMetric: 17.2298 - val_loss: 17.2461 - val_MinusLogProbMetric: 17.2461 - lr: 5.5556e-05 - 72s/epoch - 370ms/step
Epoch 642/1000
2023-10-24 00:38:58.821 
Epoch 642/1000 
	 loss: 17.2495, MinusLogProbMetric: 17.2495, val_loss: 17.3229, val_MinusLogProbMetric: 17.3229

Epoch 642: val_loss did not improve from 17.20357
196/196 - 74s - loss: 17.2495 - MinusLogProbMetric: 17.2495 - val_loss: 17.3229 - val_MinusLogProbMetric: 17.3229 - lr: 5.5556e-05 - 74s/epoch - 379ms/step
Epoch 643/1000
2023-10-24 00:40:13.746 
Epoch 643/1000 
	 loss: 17.2642, MinusLogProbMetric: 17.2642, val_loss: 17.2402, val_MinusLogProbMetric: 17.2402

Epoch 643: val_loss did not improve from 17.20357
196/196 - 75s - loss: 17.2642 - MinusLogProbMetric: 17.2642 - val_loss: 17.2402 - val_MinusLogProbMetric: 17.2402 - lr: 5.5556e-05 - 75s/epoch - 382ms/step
Epoch 644/1000
2023-10-24 00:41:30.800 
Epoch 644/1000 
	 loss: 17.2512, MinusLogProbMetric: 17.2512, val_loss: 17.2269, val_MinusLogProbMetric: 17.2269

Epoch 644: val_loss did not improve from 17.20357
196/196 - 77s - loss: 17.2512 - MinusLogProbMetric: 17.2512 - val_loss: 17.2269 - val_MinusLogProbMetric: 17.2269 - lr: 5.5556e-05 - 77s/epoch - 393ms/step
Epoch 645/1000
2023-10-24 00:42:47.339 
Epoch 645/1000 
	 loss: 17.2520, MinusLogProbMetric: 17.2520, val_loss: 17.3085, val_MinusLogProbMetric: 17.3085

Epoch 645: val_loss did not improve from 17.20357
196/196 - 77s - loss: 17.2520 - MinusLogProbMetric: 17.2520 - val_loss: 17.3085 - val_MinusLogProbMetric: 17.3085 - lr: 5.5556e-05 - 77s/epoch - 391ms/step
Epoch 646/1000
2023-10-24 00:44:01.757 
Epoch 646/1000 
	 loss: 17.2477, MinusLogProbMetric: 17.2477, val_loss: 17.3617, val_MinusLogProbMetric: 17.3617

Epoch 646: val_loss did not improve from 17.20357
196/196 - 74s - loss: 17.2477 - MinusLogProbMetric: 17.2477 - val_loss: 17.3617 - val_MinusLogProbMetric: 17.3617 - lr: 5.5556e-05 - 74s/epoch - 380ms/step
Epoch 647/1000
2023-10-24 00:45:15.910 
Epoch 647/1000 
	 loss: 17.2474, MinusLogProbMetric: 17.2474, val_loss: 17.3066, val_MinusLogProbMetric: 17.3066

Epoch 647: val_loss did not improve from 17.20357
196/196 - 74s - loss: 17.2474 - MinusLogProbMetric: 17.2474 - val_loss: 17.3066 - val_MinusLogProbMetric: 17.3066 - lr: 5.5556e-05 - 74s/epoch - 378ms/step
Epoch 648/1000
2023-10-24 00:46:32.205 
Epoch 648/1000 
	 loss: 17.2326, MinusLogProbMetric: 17.2326, val_loss: 17.3157, val_MinusLogProbMetric: 17.3157

Epoch 648: val_loss did not improve from 17.20357
196/196 - 76s - loss: 17.2326 - MinusLogProbMetric: 17.2326 - val_loss: 17.3157 - val_MinusLogProbMetric: 17.3157 - lr: 5.5556e-05 - 76s/epoch - 389ms/step
Epoch 649/1000
2023-10-24 00:47:47.541 
Epoch 649/1000 
	 loss: 17.2257, MinusLogProbMetric: 17.2257, val_loss: 17.4985, val_MinusLogProbMetric: 17.4985

Epoch 649: val_loss did not improve from 17.20357
196/196 - 75s - loss: 17.2257 - MinusLogProbMetric: 17.2257 - val_loss: 17.4985 - val_MinusLogProbMetric: 17.4985 - lr: 5.5556e-05 - 75s/epoch - 384ms/step
Epoch 650/1000
2023-10-24 00:49:03.349 
Epoch 650/1000 
	 loss: 17.2298, MinusLogProbMetric: 17.2298, val_loss: 17.7257, val_MinusLogProbMetric: 17.7257

Epoch 650: val_loss did not improve from 17.20357
196/196 - 76s - loss: 17.2298 - MinusLogProbMetric: 17.2298 - val_loss: 17.7257 - val_MinusLogProbMetric: 17.7257 - lr: 5.5556e-05 - 76s/epoch - 387ms/step
Epoch 651/1000
2023-10-24 00:50:19.580 
Epoch 651/1000 
	 loss: 17.2383, MinusLogProbMetric: 17.2383, val_loss: 17.3080, val_MinusLogProbMetric: 17.3080

Epoch 651: val_loss did not improve from 17.20357
196/196 - 76s - loss: 17.2383 - MinusLogProbMetric: 17.2383 - val_loss: 17.3080 - val_MinusLogProbMetric: 17.3080 - lr: 5.5556e-05 - 76s/epoch - 389ms/step
Epoch 652/1000
2023-10-24 00:51:33.380 
Epoch 652/1000 
	 loss: 17.2607, MinusLogProbMetric: 17.2607, val_loss: 17.3166, val_MinusLogProbMetric: 17.3166

Epoch 652: val_loss did not improve from 17.20357
196/196 - 74s - loss: 17.2607 - MinusLogProbMetric: 17.2607 - val_loss: 17.3166 - val_MinusLogProbMetric: 17.3166 - lr: 5.5556e-05 - 74s/epoch - 377ms/step
Epoch 653/1000
2023-10-24 00:52:47.002 
Epoch 653/1000 
	 loss: 17.2568, MinusLogProbMetric: 17.2568, val_loss: 17.3940, val_MinusLogProbMetric: 17.3940

Epoch 653: val_loss did not improve from 17.20357
196/196 - 74s - loss: 17.2568 - MinusLogProbMetric: 17.2568 - val_loss: 17.3940 - val_MinusLogProbMetric: 17.3940 - lr: 5.5556e-05 - 74s/epoch - 376ms/step
Epoch 654/1000
2023-10-24 00:54:01.385 
Epoch 654/1000 
	 loss: 17.2548, MinusLogProbMetric: 17.2548, val_loss: 17.3000, val_MinusLogProbMetric: 17.3000

Epoch 654: val_loss did not improve from 17.20357
196/196 - 74s - loss: 17.2548 - MinusLogProbMetric: 17.2548 - val_loss: 17.3000 - val_MinusLogProbMetric: 17.3000 - lr: 5.5556e-05 - 74s/epoch - 379ms/step
Epoch 655/1000
2023-10-24 00:55:17.118 
Epoch 655/1000 
	 loss: 17.2431, MinusLogProbMetric: 17.2431, val_loss: 17.1925, val_MinusLogProbMetric: 17.1925

Epoch 655: val_loss improved from 17.20357 to 17.19248, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 77s - loss: 17.2431 - MinusLogProbMetric: 17.2431 - val_loss: 17.1925 - val_MinusLogProbMetric: 17.1925 - lr: 5.5556e-05 - 77s/epoch - 393ms/step
Epoch 656/1000
2023-10-24 00:56:34.923 
Epoch 656/1000 
	 loss: 17.2391, MinusLogProbMetric: 17.2391, val_loss: 17.4027, val_MinusLogProbMetric: 17.4027

Epoch 656: val_loss did not improve from 17.19248
196/196 - 76s - loss: 17.2391 - MinusLogProbMetric: 17.2391 - val_loss: 17.4027 - val_MinusLogProbMetric: 17.4027 - lr: 5.5556e-05 - 76s/epoch - 390ms/step
Epoch 657/1000
2023-10-24 00:57:52.305 
Epoch 657/1000 
	 loss: 17.2389, MinusLogProbMetric: 17.2389, val_loss: 17.2827, val_MinusLogProbMetric: 17.2827

Epoch 657: val_loss did not improve from 17.19248
196/196 - 77s - loss: 17.2389 - MinusLogProbMetric: 17.2389 - val_loss: 17.2827 - val_MinusLogProbMetric: 17.2827 - lr: 5.5556e-05 - 77s/epoch - 395ms/step
Epoch 658/1000
2023-10-24 00:59:07.119 
Epoch 658/1000 
	 loss: 17.2648, MinusLogProbMetric: 17.2648, val_loss: 17.3136, val_MinusLogProbMetric: 17.3136

Epoch 658: val_loss did not improve from 17.19248
196/196 - 75s - loss: 17.2648 - MinusLogProbMetric: 17.2648 - val_loss: 17.3136 - val_MinusLogProbMetric: 17.3136 - lr: 5.5556e-05 - 75s/epoch - 382ms/step
Epoch 659/1000
2023-10-24 01:00:23.859 
Epoch 659/1000 
	 loss: 17.2541, MinusLogProbMetric: 17.2541, val_loss: 17.2511, val_MinusLogProbMetric: 17.2511

Epoch 659: val_loss did not improve from 17.19248
196/196 - 77s - loss: 17.2541 - MinusLogProbMetric: 17.2541 - val_loss: 17.2511 - val_MinusLogProbMetric: 17.2511 - lr: 5.5556e-05 - 77s/epoch - 392ms/step
Epoch 660/1000
2023-10-24 01:01:41.333 
Epoch 660/1000 
	 loss: 17.2102, MinusLogProbMetric: 17.2102, val_loss: 17.2842, val_MinusLogProbMetric: 17.2842

Epoch 660: val_loss did not improve from 17.19248
196/196 - 77s - loss: 17.2102 - MinusLogProbMetric: 17.2102 - val_loss: 17.2842 - val_MinusLogProbMetric: 17.2842 - lr: 5.5556e-05 - 77s/epoch - 395ms/step
Epoch 661/1000
2023-10-24 01:03:00.541 
Epoch 661/1000 
	 loss: 17.2233, MinusLogProbMetric: 17.2233, val_loss: 17.5031, val_MinusLogProbMetric: 17.5031

Epoch 661: val_loss did not improve from 17.19248
196/196 - 79s - loss: 17.2233 - MinusLogProbMetric: 17.2233 - val_loss: 17.5031 - val_MinusLogProbMetric: 17.5031 - lr: 5.5556e-05 - 79s/epoch - 404ms/step
Epoch 662/1000
2023-10-24 01:04:16.258 
Epoch 662/1000 
	 loss: 17.2347, MinusLogProbMetric: 17.2347, val_loss: 17.3920, val_MinusLogProbMetric: 17.3920

Epoch 662: val_loss did not improve from 17.19248
196/196 - 76s - loss: 17.2347 - MinusLogProbMetric: 17.2347 - val_loss: 17.3920 - val_MinusLogProbMetric: 17.3920 - lr: 5.5556e-05 - 76s/epoch - 386ms/step
Epoch 663/1000
2023-10-24 01:05:32.280 
Epoch 663/1000 
	 loss: 17.2221, MinusLogProbMetric: 17.2221, val_loss: 17.3032, val_MinusLogProbMetric: 17.3032

Epoch 663: val_loss did not improve from 17.19248
196/196 - 76s - loss: 17.2221 - MinusLogProbMetric: 17.2221 - val_loss: 17.3032 - val_MinusLogProbMetric: 17.3032 - lr: 5.5556e-05 - 76s/epoch - 388ms/step
Epoch 664/1000
2023-10-24 01:06:48.844 
Epoch 664/1000 
	 loss: 17.2288, MinusLogProbMetric: 17.2288, val_loss: 17.2659, val_MinusLogProbMetric: 17.2659

Epoch 664: val_loss did not improve from 17.19248
196/196 - 77s - loss: 17.2288 - MinusLogProbMetric: 17.2288 - val_loss: 17.2659 - val_MinusLogProbMetric: 17.2659 - lr: 5.5556e-05 - 77s/epoch - 391ms/step
Epoch 665/1000
2023-10-24 01:08:08.851 
Epoch 665/1000 
	 loss: 17.2243, MinusLogProbMetric: 17.2243, val_loss: 17.3266, val_MinusLogProbMetric: 17.3266

Epoch 665: val_loss did not improve from 17.19248
196/196 - 80s - loss: 17.2243 - MinusLogProbMetric: 17.2243 - val_loss: 17.3266 - val_MinusLogProbMetric: 17.3266 - lr: 5.5556e-05 - 80s/epoch - 408ms/step
Epoch 666/1000
2023-10-24 01:09:26.944 
Epoch 666/1000 
	 loss: 17.2168, MinusLogProbMetric: 17.2168, val_loss: 17.2316, val_MinusLogProbMetric: 17.2316

Epoch 666: val_loss did not improve from 17.19248
196/196 - 78s - loss: 17.2168 - MinusLogProbMetric: 17.2168 - val_loss: 17.2316 - val_MinusLogProbMetric: 17.2316 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 667/1000
2023-10-24 01:10:44.650 
Epoch 667/1000 
	 loss: 17.2573, MinusLogProbMetric: 17.2573, val_loss: 17.2755, val_MinusLogProbMetric: 17.2755

Epoch 667: val_loss did not improve from 17.19248
196/196 - 78s - loss: 17.2573 - MinusLogProbMetric: 17.2573 - val_loss: 17.2755 - val_MinusLogProbMetric: 17.2755 - lr: 5.5556e-05 - 78s/epoch - 396ms/step
Epoch 668/1000
2023-10-24 01:11:59.757 
Epoch 668/1000 
	 loss: 17.2225, MinusLogProbMetric: 17.2225, val_loss: 17.3384, val_MinusLogProbMetric: 17.3384

Epoch 668: val_loss did not improve from 17.19248
196/196 - 75s - loss: 17.2225 - MinusLogProbMetric: 17.2225 - val_loss: 17.3384 - val_MinusLogProbMetric: 17.3384 - lr: 5.5556e-05 - 75s/epoch - 383ms/step
Epoch 669/1000
2023-10-24 01:13:16.777 
Epoch 669/1000 
	 loss: 17.2405, MinusLogProbMetric: 17.2405, val_loss: 17.3997, val_MinusLogProbMetric: 17.3997

Epoch 669: val_loss did not improve from 17.19248
196/196 - 77s - loss: 17.2405 - MinusLogProbMetric: 17.2405 - val_loss: 17.3997 - val_MinusLogProbMetric: 17.3997 - lr: 5.5556e-05 - 77s/epoch - 393ms/step
Epoch 670/1000
2023-10-24 01:14:32.568 
Epoch 670/1000 
	 loss: 17.2240, MinusLogProbMetric: 17.2240, val_loss: 17.3464, val_MinusLogProbMetric: 17.3464

Epoch 670: val_loss did not improve from 17.19248
196/196 - 76s - loss: 17.2240 - MinusLogProbMetric: 17.2240 - val_loss: 17.3464 - val_MinusLogProbMetric: 17.3464 - lr: 5.5556e-05 - 76s/epoch - 387ms/step
Epoch 671/1000
2023-10-24 01:15:47.701 
Epoch 671/1000 
	 loss: 17.2385, MinusLogProbMetric: 17.2385, val_loss: 17.2392, val_MinusLogProbMetric: 17.2392

Epoch 671: val_loss did not improve from 17.19248
196/196 - 75s - loss: 17.2385 - MinusLogProbMetric: 17.2385 - val_loss: 17.2392 - val_MinusLogProbMetric: 17.2392 - lr: 5.5556e-05 - 75s/epoch - 383ms/step
Epoch 672/1000
2023-10-24 01:17:02.221 
Epoch 672/1000 
	 loss: 17.2017, MinusLogProbMetric: 17.2017, val_loss: 17.2786, val_MinusLogProbMetric: 17.2786

Epoch 672: val_loss did not improve from 17.19248
196/196 - 75s - loss: 17.2017 - MinusLogProbMetric: 17.2017 - val_loss: 17.2786 - val_MinusLogProbMetric: 17.2786 - lr: 5.5556e-05 - 75s/epoch - 380ms/step
Epoch 673/1000
2023-10-24 01:18:16.022 
Epoch 673/1000 
	 loss: 17.2628, MinusLogProbMetric: 17.2628, val_loss: 17.4260, val_MinusLogProbMetric: 17.4260

Epoch 673: val_loss did not improve from 17.19248
196/196 - 74s - loss: 17.2628 - MinusLogProbMetric: 17.2628 - val_loss: 17.4260 - val_MinusLogProbMetric: 17.4260 - lr: 5.5556e-05 - 74s/epoch - 377ms/step
Epoch 674/1000
2023-10-24 01:19:31.138 
Epoch 674/1000 
	 loss: 17.2204, MinusLogProbMetric: 17.2204, val_loss: 17.3698, val_MinusLogProbMetric: 17.3698

Epoch 674: val_loss did not improve from 17.19248
196/196 - 75s - loss: 17.2204 - MinusLogProbMetric: 17.2204 - val_loss: 17.3698 - val_MinusLogProbMetric: 17.3698 - lr: 5.5556e-05 - 75s/epoch - 383ms/step
Epoch 675/1000
2023-10-24 01:20:46.508 
Epoch 675/1000 
	 loss: 17.2469, MinusLogProbMetric: 17.2469, val_loss: 17.2329, val_MinusLogProbMetric: 17.2329

Epoch 675: val_loss did not improve from 17.19248
196/196 - 75s - loss: 17.2469 - MinusLogProbMetric: 17.2469 - val_loss: 17.2329 - val_MinusLogProbMetric: 17.2329 - lr: 5.5556e-05 - 75s/epoch - 385ms/step
Epoch 676/1000
2023-10-24 01:22:02.708 
Epoch 676/1000 
	 loss: 17.2142, MinusLogProbMetric: 17.2142, val_loss: 17.2943, val_MinusLogProbMetric: 17.2943

Epoch 676: val_loss did not improve from 17.19248
196/196 - 76s - loss: 17.2142 - MinusLogProbMetric: 17.2142 - val_loss: 17.2943 - val_MinusLogProbMetric: 17.2943 - lr: 5.5556e-05 - 76s/epoch - 389ms/step
Epoch 677/1000
2023-10-24 01:23:16.835 
Epoch 677/1000 
	 loss: 17.2190, MinusLogProbMetric: 17.2190, val_loss: 17.2592, val_MinusLogProbMetric: 17.2592

Epoch 677: val_loss did not improve from 17.19248
196/196 - 74s - loss: 17.2190 - MinusLogProbMetric: 17.2190 - val_loss: 17.2592 - val_MinusLogProbMetric: 17.2592 - lr: 5.5556e-05 - 74s/epoch - 378ms/step
Epoch 678/1000
2023-10-24 01:24:32.505 
Epoch 678/1000 
	 loss: 17.2054, MinusLogProbMetric: 17.2054, val_loss: 17.2462, val_MinusLogProbMetric: 17.2462

Epoch 678: val_loss did not improve from 17.19248
196/196 - 76s - loss: 17.2054 - MinusLogProbMetric: 17.2054 - val_loss: 17.2462 - val_MinusLogProbMetric: 17.2462 - lr: 5.5556e-05 - 76s/epoch - 386ms/step
Epoch 679/1000
2023-10-24 01:25:45.146 
Epoch 679/1000 
	 loss: 17.2084, MinusLogProbMetric: 17.2084, val_loss: 17.2369, val_MinusLogProbMetric: 17.2369

Epoch 679: val_loss did not improve from 17.19248
196/196 - 73s - loss: 17.2084 - MinusLogProbMetric: 17.2084 - val_loss: 17.2369 - val_MinusLogProbMetric: 17.2369 - lr: 5.5556e-05 - 73s/epoch - 371ms/step
Epoch 680/1000
2023-10-24 01:26:59.807 
Epoch 680/1000 
	 loss: 17.2148, MinusLogProbMetric: 17.2148, val_loss: 17.4087, val_MinusLogProbMetric: 17.4087

Epoch 680: val_loss did not improve from 17.19248
196/196 - 75s - loss: 17.2148 - MinusLogProbMetric: 17.2148 - val_loss: 17.4087 - val_MinusLogProbMetric: 17.4087 - lr: 5.5556e-05 - 75s/epoch - 381ms/step
Epoch 681/1000
2023-10-24 01:28:17.299 
Epoch 681/1000 
	 loss: 17.2390, MinusLogProbMetric: 17.2390, val_loss: 17.3068, val_MinusLogProbMetric: 17.3068

Epoch 681: val_loss did not improve from 17.19248
196/196 - 77s - loss: 17.2390 - MinusLogProbMetric: 17.2390 - val_loss: 17.3068 - val_MinusLogProbMetric: 17.3068 - lr: 5.5556e-05 - 77s/epoch - 395ms/step
Epoch 682/1000
2023-10-24 01:29:35.152 
Epoch 682/1000 
	 loss: 17.2070, MinusLogProbMetric: 17.2070, val_loss: 17.3439, val_MinusLogProbMetric: 17.3439

Epoch 682: val_loss did not improve from 17.19248
196/196 - 78s - loss: 17.2070 - MinusLogProbMetric: 17.2070 - val_loss: 17.3439 - val_MinusLogProbMetric: 17.3439 - lr: 5.5556e-05 - 78s/epoch - 397ms/step
Epoch 683/1000
2023-10-24 01:30:50.091 
Epoch 683/1000 
	 loss: 17.2169, MinusLogProbMetric: 17.2169, val_loss: 17.3250, val_MinusLogProbMetric: 17.3250

Epoch 683: val_loss did not improve from 17.19248
196/196 - 75s - loss: 17.2169 - MinusLogProbMetric: 17.2169 - val_loss: 17.3250 - val_MinusLogProbMetric: 17.3250 - lr: 5.5556e-05 - 75s/epoch - 382ms/step
Epoch 684/1000
2023-10-24 01:32:04.069 
Epoch 684/1000 
	 loss: 17.2300, MinusLogProbMetric: 17.2300, val_loss: 17.3491, val_MinusLogProbMetric: 17.3491

Epoch 684: val_loss did not improve from 17.19248
196/196 - 74s - loss: 17.2300 - MinusLogProbMetric: 17.2300 - val_loss: 17.3491 - val_MinusLogProbMetric: 17.3491 - lr: 5.5556e-05 - 74s/epoch - 377ms/step
Epoch 685/1000
2023-10-24 01:33:20.303 
Epoch 685/1000 
	 loss: 17.2176, MinusLogProbMetric: 17.2176, val_loss: 17.2219, val_MinusLogProbMetric: 17.2219

Epoch 685: val_loss did not improve from 17.19248
196/196 - 76s - loss: 17.2176 - MinusLogProbMetric: 17.2176 - val_loss: 17.2219 - val_MinusLogProbMetric: 17.2219 - lr: 5.5556e-05 - 76s/epoch - 389ms/step
Epoch 686/1000
2023-10-24 01:34:35.081 
Epoch 686/1000 
	 loss: 17.2051, MinusLogProbMetric: 17.2051, val_loss: 17.1748, val_MinusLogProbMetric: 17.1748

Epoch 686: val_loss improved from 17.19248 to 17.17483, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 76s - loss: 17.2051 - MinusLogProbMetric: 17.2051 - val_loss: 17.1748 - val_MinusLogProbMetric: 17.1748 - lr: 5.5556e-05 - 76s/epoch - 388ms/step
Epoch 687/1000
2023-10-24 01:35:51.355 
Epoch 687/1000 
	 loss: 17.2357, MinusLogProbMetric: 17.2357, val_loss: 17.2912, val_MinusLogProbMetric: 17.2912

Epoch 687: val_loss did not improve from 17.17483
196/196 - 75s - loss: 17.2357 - MinusLogProbMetric: 17.2357 - val_loss: 17.2912 - val_MinusLogProbMetric: 17.2912 - lr: 5.5556e-05 - 75s/epoch - 383ms/step
Epoch 688/1000
2023-10-24 01:37:06.716 
Epoch 688/1000 
	 loss: 17.2077, MinusLogProbMetric: 17.2077, val_loss: 17.2034, val_MinusLogProbMetric: 17.2034

Epoch 688: val_loss did not improve from 17.17483
196/196 - 75s - loss: 17.2077 - MinusLogProbMetric: 17.2077 - val_loss: 17.2034 - val_MinusLogProbMetric: 17.2034 - lr: 5.5556e-05 - 75s/epoch - 384ms/step
Epoch 689/1000
2023-10-24 01:38:21.632 
Epoch 689/1000 
	 loss: 17.1983, MinusLogProbMetric: 17.1983, val_loss: 17.2184, val_MinusLogProbMetric: 17.2184

Epoch 689: val_loss did not improve from 17.17483
196/196 - 75s - loss: 17.1983 - MinusLogProbMetric: 17.1983 - val_loss: 17.2184 - val_MinusLogProbMetric: 17.2184 - lr: 5.5556e-05 - 75s/epoch - 382ms/step
Epoch 690/1000
2023-10-24 01:39:35.477 
Epoch 690/1000 
	 loss: 17.1971, MinusLogProbMetric: 17.1971, val_loss: 17.2359, val_MinusLogProbMetric: 17.2359

Epoch 690: val_loss did not improve from 17.17483
196/196 - 74s - loss: 17.1971 - MinusLogProbMetric: 17.1971 - val_loss: 17.2359 - val_MinusLogProbMetric: 17.2359 - lr: 5.5556e-05 - 74s/epoch - 377ms/step
Epoch 691/1000
2023-10-24 01:40:50.501 
Epoch 691/1000 
	 loss: 17.2201, MinusLogProbMetric: 17.2201, val_loss: 17.3635, val_MinusLogProbMetric: 17.3635

Epoch 691: val_loss did not improve from 17.17483
196/196 - 75s - loss: 17.2201 - MinusLogProbMetric: 17.2201 - val_loss: 17.3635 - val_MinusLogProbMetric: 17.3635 - lr: 5.5556e-05 - 75s/epoch - 383ms/step
Epoch 692/1000
2023-10-24 01:42:04.277 
Epoch 692/1000 
	 loss: 17.2016, MinusLogProbMetric: 17.2016, val_loss: 17.2567, val_MinusLogProbMetric: 17.2567

Epoch 692: val_loss did not improve from 17.17483
196/196 - 74s - loss: 17.2016 - MinusLogProbMetric: 17.2016 - val_loss: 17.2567 - val_MinusLogProbMetric: 17.2567 - lr: 5.5556e-05 - 74s/epoch - 376ms/step
Epoch 693/1000
2023-10-24 01:43:17.717 
Epoch 693/1000 
	 loss: 17.2144, MinusLogProbMetric: 17.2144, val_loss: 17.4105, val_MinusLogProbMetric: 17.4105

Epoch 693: val_loss did not improve from 17.17483
196/196 - 73s - loss: 17.2144 - MinusLogProbMetric: 17.2144 - val_loss: 17.4105 - val_MinusLogProbMetric: 17.4105 - lr: 5.5556e-05 - 73s/epoch - 375ms/step
Epoch 694/1000
2023-10-24 01:44:30.998 
Epoch 694/1000 
	 loss: 17.2092, MinusLogProbMetric: 17.2092, val_loss: 17.2829, val_MinusLogProbMetric: 17.2829

Epoch 694: val_loss did not improve from 17.17483
196/196 - 73s - loss: 17.2092 - MinusLogProbMetric: 17.2092 - val_loss: 17.2829 - val_MinusLogProbMetric: 17.2829 - lr: 5.5556e-05 - 73s/epoch - 374ms/step
Epoch 695/1000
2023-10-24 01:45:47.286 
Epoch 695/1000 
	 loss: 17.2085, MinusLogProbMetric: 17.2085, val_loss: 17.2323, val_MinusLogProbMetric: 17.2323

Epoch 695: val_loss did not improve from 17.17483
196/196 - 76s - loss: 17.2085 - MinusLogProbMetric: 17.2085 - val_loss: 17.2323 - val_MinusLogProbMetric: 17.2323 - lr: 5.5556e-05 - 76s/epoch - 389ms/step
Epoch 696/1000
2023-10-24 01:47:02.529 
Epoch 696/1000 
	 loss: 17.2058, MinusLogProbMetric: 17.2058, val_loss: 17.2407, val_MinusLogProbMetric: 17.2407

Epoch 696: val_loss did not improve from 17.17483
196/196 - 75s - loss: 17.2058 - MinusLogProbMetric: 17.2058 - val_loss: 17.2407 - val_MinusLogProbMetric: 17.2407 - lr: 5.5556e-05 - 75s/epoch - 384ms/step
Epoch 697/1000
2023-10-24 01:48:17.522 
Epoch 697/1000 
	 loss: 17.2130, MinusLogProbMetric: 17.2130, val_loss: 17.1951, val_MinusLogProbMetric: 17.1951

Epoch 697: val_loss did not improve from 17.17483
196/196 - 75s - loss: 17.2130 - MinusLogProbMetric: 17.2130 - val_loss: 17.1951 - val_MinusLogProbMetric: 17.1951 - lr: 5.5556e-05 - 75s/epoch - 383ms/step
Epoch 698/1000
2023-10-24 01:49:30.693 
Epoch 698/1000 
	 loss: 17.2356, MinusLogProbMetric: 17.2356, val_loss: 17.2510, val_MinusLogProbMetric: 17.2510

Epoch 698: val_loss did not improve from 17.17483
196/196 - 73s - loss: 17.2356 - MinusLogProbMetric: 17.2356 - val_loss: 17.2510 - val_MinusLogProbMetric: 17.2510 - lr: 5.5556e-05 - 73s/epoch - 373ms/step
Epoch 699/1000
2023-10-24 01:50:46.609 
Epoch 699/1000 
	 loss: 17.2068, MinusLogProbMetric: 17.2068, val_loss: 17.2567, val_MinusLogProbMetric: 17.2567

Epoch 699: val_loss did not improve from 17.17483
196/196 - 76s - loss: 17.2068 - MinusLogProbMetric: 17.2068 - val_loss: 17.2567 - val_MinusLogProbMetric: 17.2567 - lr: 5.5556e-05 - 76s/epoch - 387ms/step
Epoch 700/1000
2023-10-24 01:52:03.561 
Epoch 700/1000 
	 loss: 17.1970, MinusLogProbMetric: 17.1970, val_loss: 17.3103, val_MinusLogProbMetric: 17.3103

Epoch 700: val_loss did not improve from 17.17483
196/196 - 77s - loss: 17.1970 - MinusLogProbMetric: 17.1970 - val_loss: 17.3103 - val_MinusLogProbMetric: 17.3103 - lr: 5.5556e-05 - 77s/epoch - 393ms/step
Epoch 701/1000
2023-10-24 01:53:19.462 
Epoch 701/1000 
	 loss: 17.2136, MinusLogProbMetric: 17.2136, val_loss: 17.3043, val_MinusLogProbMetric: 17.3043

Epoch 701: val_loss did not improve from 17.17483
196/196 - 76s - loss: 17.2136 - MinusLogProbMetric: 17.2136 - val_loss: 17.3043 - val_MinusLogProbMetric: 17.3043 - lr: 5.5556e-05 - 76s/epoch - 387ms/step
Epoch 702/1000
2023-10-24 01:54:34.493 
Epoch 702/1000 
	 loss: 17.2054, MinusLogProbMetric: 17.2054, val_loss: 17.3280, val_MinusLogProbMetric: 17.3280

Epoch 702: val_loss did not improve from 17.17483
196/196 - 75s - loss: 17.2054 - MinusLogProbMetric: 17.2054 - val_loss: 17.3280 - val_MinusLogProbMetric: 17.3280 - lr: 5.5556e-05 - 75s/epoch - 383ms/step
Epoch 703/1000
2023-10-24 01:55:49.509 
Epoch 703/1000 
	 loss: 17.2230, MinusLogProbMetric: 17.2230, val_loss: 17.1787, val_MinusLogProbMetric: 17.1787

Epoch 703: val_loss did not improve from 17.17483
196/196 - 75s - loss: 17.2230 - MinusLogProbMetric: 17.2230 - val_loss: 17.1787 - val_MinusLogProbMetric: 17.1787 - lr: 5.5556e-05 - 75s/epoch - 383ms/step
Epoch 704/1000
2023-10-24 01:57:05.730 
Epoch 704/1000 
	 loss: 17.1989, MinusLogProbMetric: 17.1989, val_loss: 17.2204, val_MinusLogProbMetric: 17.2204

Epoch 704: val_loss did not improve from 17.17483
196/196 - 76s - loss: 17.1989 - MinusLogProbMetric: 17.1989 - val_loss: 17.2204 - val_MinusLogProbMetric: 17.2204 - lr: 5.5556e-05 - 76s/epoch - 389ms/step
Epoch 705/1000
2023-10-24 01:58:23.646 
Epoch 705/1000 
	 loss: 17.2005, MinusLogProbMetric: 17.2005, val_loss: 17.2512, val_MinusLogProbMetric: 17.2512

Epoch 705: val_loss did not improve from 17.17483
196/196 - 78s - loss: 17.2005 - MinusLogProbMetric: 17.2005 - val_loss: 17.2512 - val_MinusLogProbMetric: 17.2512 - lr: 5.5556e-05 - 78s/epoch - 398ms/step
Epoch 706/1000
2023-10-24 01:59:40.088 
Epoch 706/1000 
	 loss: 17.1963, MinusLogProbMetric: 17.1963, val_loss: 17.3235, val_MinusLogProbMetric: 17.3235

Epoch 706: val_loss did not improve from 17.17483
196/196 - 76s - loss: 17.1963 - MinusLogProbMetric: 17.1963 - val_loss: 17.3235 - val_MinusLogProbMetric: 17.3235 - lr: 5.5556e-05 - 76s/epoch - 390ms/step
Epoch 707/1000
2023-10-24 02:00:56.015 
Epoch 707/1000 
	 loss: 17.2063, MinusLogProbMetric: 17.2063, val_loss: 17.2877, val_MinusLogProbMetric: 17.2877

Epoch 707: val_loss did not improve from 17.17483
196/196 - 76s - loss: 17.2063 - MinusLogProbMetric: 17.2063 - val_loss: 17.2877 - val_MinusLogProbMetric: 17.2877 - lr: 5.5556e-05 - 76s/epoch - 387ms/step
Epoch 708/1000
2023-10-24 02:02:11.094 
Epoch 708/1000 
	 loss: 17.1979, MinusLogProbMetric: 17.1979, val_loss: 17.3421, val_MinusLogProbMetric: 17.3421

Epoch 708: val_loss did not improve from 17.17483
196/196 - 75s - loss: 17.1979 - MinusLogProbMetric: 17.1979 - val_loss: 17.3421 - val_MinusLogProbMetric: 17.3421 - lr: 5.5556e-05 - 75s/epoch - 383ms/step
Epoch 709/1000
2023-10-24 02:03:27.043 
Epoch 709/1000 
	 loss: 17.2036, MinusLogProbMetric: 17.2036, val_loss: 17.2005, val_MinusLogProbMetric: 17.2005

Epoch 709: val_loss did not improve from 17.17483
196/196 - 76s - loss: 17.2036 - MinusLogProbMetric: 17.2036 - val_loss: 17.2005 - val_MinusLogProbMetric: 17.2005 - lr: 5.5556e-05 - 76s/epoch - 387ms/step
Epoch 710/1000
2023-10-24 02:04:44.683 
Epoch 710/1000 
	 loss: 17.2423, MinusLogProbMetric: 17.2423, val_loss: 17.2280, val_MinusLogProbMetric: 17.2280

Epoch 710: val_loss did not improve from 17.17483
196/196 - 78s - loss: 17.2423 - MinusLogProbMetric: 17.2423 - val_loss: 17.2280 - val_MinusLogProbMetric: 17.2280 - lr: 5.5556e-05 - 78s/epoch - 396ms/step
Epoch 711/1000
2023-10-24 02:05:59.663 
Epoch 711/1000 
	 loss: 17.1987, MinusLogProbMetric: 17.1987, val_loss: 17.2884, val_MinusLogProbMetric: 17.2884

Epoch 711: val_loss did not improve from 17.17483
196/196 - 75s - loss: 17.1987 - MinusLogProbMetric: 17.1987 - val_loss: 17.2884 - val_MinusLogProbMetric: 17.2884 - lr: 5.5556e-05 - 75s/epoch - 383ms/step
Epoch 712/1000
2023-10-24 02:07:15.605 
Epoch 712/1000 
	 loss: 17.1904, MinusLogProbMetric: 17.1904, val_loss: 17.2837, val_MinusLogProbMetric: 17.2837

Epoch 712: val_loss did not improve from 17.17483
196/196 - 76s - loss: 17.1904 - MinusLogProbMetric: 17.1904 - val_loss: 17.2837 - val_MinusLogProbMetric: 17.2837 - lr: 5.5556e-05 - 76s/epoch - 387ms/step
Epoch 713/1000
2023-10-24 02:08:29.571 
Epoch 713/1000 
	 loss: 17.2130, MinusLogProbMetric: 17.2130, val_loss: 17.3262, val_MinusLogProbMetric: 17.3262

Epoch 713: val_loss did not improve from 17.17483
196/196 - 74s - loss: 17.2130 - MinusLogProbMetric: 17.2130 - val_loss: 17.3262 - val_MinusLogProbMetric: 17.3262 - lr: 5.5556e-05 - 74s/epoch - 377ms/step
Epoch 714/1000
2023-10-24 02:09:42.360 
Epoch 714/1000 
	 loss: 17.2084, MinusLogProbMetric: 17.2084, val_loss: 17.2227, val_MinusLogProbMetric: 17.2227

Epoch 714: val_loss did not improve from 17.17483
196/196 - 73s - loss: 17.2084 - MinusLogProbMetric: 17.2084 - val_loss: 17.2227 - val_MinusLogProbMetric: 17.2227 - lr: 5.5556e-05 - 73s/epoch - 371ms/step
Epoch 715/1000
2023-10-24 02:10:58.489 
Epoch 715/1000 
	 loss: 17.2008, MinusLogProbMetric: 17.2008, val_loss: 17.2306, val_MinusLogProbMetric: 17.2306

Epoch 715: val_loss did not improve from 17.17483
196/196 - 76s - loss: 17.2008 - MinusLogProbMetric: 17.2008 - val_loss: 17.2306 - val_MinusLogProbMetric: 17.2306 - lr: 5.5556e-05 - 76s/epoch - 388ms/step
Epoch 716/1000
2023-10-24 02:12:13.599 
Epoch 716/1000 
	 loss: 17.2123, MinusLogProbMetric: 17.2123, val_loss: 17.2987, val_MinusLogProbMetric: 17.2987

Epoch 716: val_loss did not improve from 17.17483
196/196 - 75s - loss: 17.2123 - MinusLogProbMetric: 17.2123 - val_loss: 17.2987 - val_MinusLogProbMetric: 17.2987 - lr: 5.5556e-05 - 75s/epoch - 383ms/step
Epoch 717/1000
2023-10-24 02:13:26.881 
Epoch 717/1000 
	 loss: 17.1922, MinusLogProbMetric: 17.1922, val_loss: 17.2446, val_MinusLogProbMetric: 17.2446

Epoch 717: val_loss did not improve from 17.17483
196/196 - 73s - loss: 17.1922 - MinusLogProbMetric: 17.1922 - val_loss: 17.2446 - val_MinusLogProbMetric: 17.2446 - lr: 5.5556e-05 - 73s/epoch - 374ms/step
Epoch 718/1000
2023-10-24 02:14:39.290 
Epoch 718/1000 
	 loss: 17.1926, MinusLogProbMetric: 17.1926, val_loss: 17.1994, val_MinusLogProbMetric: 17.1994

Epoch 718: val_loss did not improve from 17.17483
196/196 - 72s - loss: 17.1926 - MinusLogProbMetric: 17.1926 - val_loss: 17.1994 - val_MinusLogProbMetric: 17.1994 - lr: 5.5556e-05 - 72s/epoch - 369ms/step
Epoch 719/1000
2023-10-24 02:15:52.256 
Epoch 719/1000 
	 loss: 17.1891, MinusLogProbMetric: 17.1891, val_loss: 17.3563, val_MinusLogProbMetric: 17.3563

Epoch 719: val_loss did not improve from 17.17483
196/196 - 73s - loss: 17.1891 - MinusLogProbMetric: 17.1891 - val_loss: 17.3563 - val_MinusLogProbMetric: 17.3563 - lr: 5.5556e-05 - 73s/epoch - 372ms/step
Epoch 720/1000
2023-10-24 02:17:08.309 
Epoch 720/1000 
	 loss: 17.2033, MinusLogProbMetric: 17.2033, val_loss: 17.2892, val_MinusLogProbMetric: 17.2892

Epoch 720: val_loss did not improve from 17.17483
196/196 - 76s - loss: 17.2033 - MinusLogProbMetric: 17.2033 - val_loss: 17.2892 - val_MinusLogProbMetric: 17.2892 - lr: 5.5556e-05 - 76s/epoch - 388ms/step
Epoch 721/1000
2023-10-24 02:18:24.750 
Epoch 721/1000 
	 loss: 17.2110, MinusLogProbMetric: 17.2110, val_loss: 17.3234, val_MinusLogProbMetric: 17.3234

Epoch 721: val_loss did not improve from 17.17483
196/196 - 76s - loss: 17.2110 - MinusLogProbMetric: 17.2110 - val_loss: 17.3234 - val_MinusLogProbMetric: 17.3234 - lr: 5.5556e-05 - 76s/epoch - 390ms/step
Epoch 722/1000
2023-10-24 02:19:39.544 
Epoch 722/1000 
	 loss: 17.2233, MinusLogProbMetric: 17.2233, val_loss: 17.2777, val_MinusLogProbMetric: 17.2777

Epoch 722: val_loss did not improve from 17.17483
196/196 - 75s - loss: 17.2233 - MinusLogProbMetric: 17.2233 - val_loss: 17.2777 - val_MinusLogProbMetric: 17.2777 - lr: 5.5556e-05 - 75s/epoch - 382ms/step
Epoch 723/1000
2023-10-24 02:20:53.644 
Epoch 723/1000 
	 loss: 17.2320, MinusLogProbMetric: 17.2320, val_loss: 17.4680, val_MinusLogProbMetric: 17.4680

Epoch 723: val_loss did not improve from 17.17483
196/196 - 74s - loss: 17.2320 - MinusLogProbMetric: 17.2320 - val_loss: 17.4680 - val_MinusLogProbMetric: 17.4680 - lr: 5.5556e-05 - 74s/epoch - 378ms/step
Epoch 724/1000
2023-10-24 02:22:09.021 
Epoch 724/1000 
	 loss: 17.1950, MinusLogProbMetric: 17.1950, val_loss: 17.2116, val_MinusLogProbMetric: 17.2116

Epoch 724: val_loss did not improve from 17.17483
196/196 - 75s - loss: 17.1950 - MinusLogProbMetric: 17.1950 - val_loss: 17.2116 - val_MinusLogProbMetric: 17.2116 - lr: 5.5556e-05 - 75s/epoch - 385ms/step
Epoch 725/1000
2023-10-24 02:23:22.594 
Epoch 725/1000 
	 loss: 17.2038, MinusLogProbMetric: 17.2038, val_loss: 17.2498, val_MinusLogProbMetric: 17.2498

Epoch 725: val_loss did not improve from 17.17483
196/196 - 74s - loss: 17.2038 - MinusLogProbMetric: 17.2038 - val_loss: 17.2498 - val_MinusLogProbMetric: 17.2498 - lr: 5.5556e-05 - 74s/epoch - 375ms/step
Epoch 726/1000
2023-10-24 02:24:36.673 
Epoch 726/1000 
	 loss: 17.2318, MinusLogProbMetric: 17.2318, val_loss: 17.5460, val_MinusLogProbMetric: 17.5460

Epoch 726: val_loss did not improve from 17.17483
196/196 - 74s - loss: 17.2318 - MinusLogProbMetric: 17.2318 - val_loss: 17.5460 - val_MinusLogProbMetric: 17.5460 - lr: 5.5556e-05 - 74s/epoch - 378ms/step
Epoch 727/1000
2023-10-24 02:25:52.680 
Epoch 727/1000 
	 loss: 17.1827, MinusLogProbMetric: 17.1827, val_loss: 17.1880, val_MinusLogProbMetric: 17.1880

Epoch 727: val_loss did not improve from 17.17483
196/196 - 76s - loss: 17.1827 - MinusLogProbMetric: 17.1827 - val_loss: 17.1880 - val_MinusLogProbMetric: 17.1880 - lr: 5.5556e-05 - 76s/epoch - 388ms/step
Epoch 728/1000
2023-10-24 02:27:04.912 
Epoch 728/1000 
	 loss: 17.1843, MinusLogProbMetric: 17.1843, val_loss: 17.2120, val_MinusLogProbMetric: 17.2120

Epoch 728: val_loss did not improve from 17.17483
196/196 - 72s - loss: 17.1843 - MinusLogProbMetric: 17.1843 - val_loss: 17.2120 - val_MinusLogProbMetric: 17.2120 - lr: 5.5556e-05 - 72s/epoch - 369ms/step
Epoch 729/1000
2023-10-24 02:28:16.347 
Epoch 729/1000 
	 loss: 17.1924, MinusLogProbMetric: 17.1924, val_loss: 17.2845, val_MinusLogProbMetric: 17.2845

Epoch 729: val_loss did not improve from 17.17483
196/196 - 71s - loss: 17.1924 - MinusLogProbMetric: 17.1924 - val_loss: 17.2845 - val_MinusLogProbMetric: 17.2845 - lr: 5.5556e-05 - 71s/epoch - 364ms/step
Epoch 730/1000
2023-10-24 02:29:33.375 
Epoch 730/1000 
	 loss: 17.1996, MinusLogProbMetric: 17.1996, val_loss: 17.3292, val_MinusLogProbMetric: 17.3292

Epoch 730: val_loss did not improve from 17.17483
196/196 - 77s - loss: 17.1996 - MinusLogProbMetric: 17.1996 - val_loss: 17.3292 - val_MinusLogProbMetric: 17.3292 - lr: 5.5556e-05 - 77s/epoch - 393ms/step
Epoch 731/1000
2023-10-24 02:30:46.004 
Epoch 731/1000 
	 loss: 17.1812, MinusLogProbMetric: 17.1812, val_loss: 17.2541, val_MinusLogProbMetric: 17.2541

Epoch 731: val_loss did not improve from 17.17483
196/196 - 73s - loss: 17.1812 - MinusLogProbMetric: 17.1812 - val_loss: 17.2541 - val_MinusLogProbMetric: 17.2541 - lr: 5.5556e-05 - 73s/epoch - 371ms/step
Epoch 732/1000
2023-10-24 02:31:58.512 
Epoch 732/1000 
	 loss: 17.1785, MinusLogProbMetric: 17.1785, val_loss: 17.2084, val_MinusLogProbMetric: 17.2084

Epoch 732: val_loss did not improve from 17.17483
196/196 - 73s - loss: 17.1785 - MinusLogProbMetric: 17.1785 - val_loss: 17.2084 - val_MinusLogProbMetric: 17.2084 - lr: 5.5556e-05 - 73s/epoch - 370ms/step
Epoch 733/1000
2023-10-24 02:33:03.199 
Epoch 733/1000 
	 loss: 17.1910, MinusLogProbMetric: 17.1910, val_loss: 17.2744, val_MinusLogProbMetric: 17.2744

Epoch 733: val_loss did not improve from 17.17483
196/196 - 65s - loss: 17.1910 - MinusLogProbMetric: 17.1910 - val_loss: 17.2744 - val_MinusLogProbMetric: 17.2744 - lr: 5.5556e-05 - 65s/epoch - 330ms/step
Epoch 734/1000
2023-10-24 02:34:09.882 
Epoch 734/1000 
	 loss: 17.1778, MinusLogProbMetric: 17.1778, val_loss: 17.2320, val_MinusLogProbMetric: 17.2320

Epoch 734: val_loss did not improve from 17.17483
196/196 - 67s - loss: 17.1778 - MinusLogProbMetric: 17.1778 - val_loss: 17.2320 - val_MinusLogProbMetric: 17.2320 - lr: 5.5556e-05 - 67s/epoch - 340ms/step
Epoch 735/1000
2023-10-24 02:35:17.059 
Epoch 735/1000 
	 loss: 17.1919, MinusLogProbMetric: 17.1919, val_loss: 17.3194, val_MinusLogProbMetric: 17.3194

Epoch 735: val_loss did not improve from 17.17483
196/196 - 67s - loss: 17.1919 - MinusLogProbMetric: 17.1919 - val_loss: 17.3194 - val_MinusLogProbMetric: 17.3194 - lr: 5.5556e-05 - 67s/epoch - 343ms/step
Epoch 736/1000
2023-10-24 02:36:19.061 
Epoch 736/1000 
	 loss: 17.1995, MinusLogProbMetric: 17.1995, val_loss: 17.1692, val_MinusLogProbMetric: 17.1692

Epoch 736: val_loss improved from 17.17483 to 17.16923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 17.1995 - MinusLogProbMetric: 17.1995 - val_loss: 17.1692 - val_MinusLogProbMetric: 17.1692 - lr: 5.5556e-05 - 63s/epoch - 323ms/step
Epoch 737/1000
2023-10-24 02:37:26.841 
Epoch 737/1000 
	 loss: 17.2152, MinusLogProbMetric: 17.2152, val_loss: 17.2396, val_MinusLogProbMetric: 17.2396

Epoch 737: val_loss did not improve from 17.16923
196/196 - 66s - loss: 17.2152 - MinusLogProbMetric: 17.2152 - val_loss: 17.2396 - val_MinusLogProbMetric: 17.2396 - lr: 5.5556e-05 - 66s/epoch - 339ms/step
Epoch 738/1000
2023-10-24 02:38:33.752 
Epoch 738/1000 
	 loss: 17.2041, MinusLogProbMetric: 17.2041, val_loss: 17.2013, val_MinusLogProbMetric: 17.2013

Epoch 738: val_loss did not improve from 17.16923
196/196 - 67s - loss: 17.2041 - MinusLogProbMetric: 17.2041 - val_loss: 17.2013 - val_MinusLogProbMetric: 17.2013 - lr: 5.5556e-05 - 67s/epoch - 341ms/step
Epoch 739/1000
2023-10-24 02:39:34.683 
Epoch 739/1000 
	 loss: 17.1926, MinusLogProbMetric: 17.1926, val_loss: 17.3200, val_MinusLogProbMetric: 17.3200

Epoch 739: val_loss did not improve from 17.16923
196/196 - 61s - loss: 17.1926 - MinusLogProbMetric: 17.1926 - val_loss: 17.3200 - val_MinusLogProbMetric: 17.3200 - lr: 5.5556e-05 - 61s/epoch - 311ms/step
Epoch 740/1000
2023-10-24 02:40:42.101 
Epoch 740/1000 
	 loss: 17.1756, MinusLogProbMetric: 17.1756, val_loss: 17.2046, val_MinusLogProbMetric: 17.2046

Epoch 740: val_loss did not improve from 17.16923
196/196 - 67s - loss: 17.1756 - MinusLogProbMetric: 17.1756 - val_loss: 17.2046 - val_MinusLogProbMetric: 17.2046 - lr: 5.5556e-05 - 67s/epoch - 344ms/step
Epoch 741/1000
2023-10-24 02:41:43.154 
Epoch 741/1000 
	 loss: 17.2262, MinusLogProbMetric: 17.2262, val_loss: 17.2289, val_MinusLogProbMetric: 17.2289

Epoch 741: val_loss did not improve from 17.16923
196/196 - 61s - loss: 17.2262 - MinusLogProbMetric: 17.2262 - val_loss: 17.2289 - val_MinusLogProbMetric: 17.2289 - lr: 5.5556e-05 - 61s/epoch - 311ms/step
Epoch 742/1000
2023-10-24 02:42:44.908 
Epoch 742/1000 
	 loss: 17.2016, MinusLogProbMetric: 17.2016, val_loss: 17.2215, val_MinusLogProbMetric: 17.2215

Epoch 742: val_loss did not improve from 17.16923
196/196 - 62s - loss: 17.2016 - MinusLogProbMetric: 17.2016 - val_loss: 17.2215 - val_MinusLogProbMetric: 17.2215 - lr: 5.5556e-05 - 62s/epoch - 315ms/step
Epoch 743/1000
2023-10-24 02:43:45.041 
Epoch 743/1000 
	 loss: 17.1887, MinusLogProbMetric: 17.1887, val_loss: 17.2474, val_MinusLogProbMetric: 17.2474

Epoch 743: val_loss did not improve from 17.16923
196/196 - 60s - loss: 17.1887 - MinusLogProbMetric: 17.1887 - val_loss: 17.2474 - val_MinusLogProbMetric: 17.2474 - lr: 5.5556e-05 - 60s/epoch - 307ms/step
Epoch 744/1000
2023-10-24 02:44:48.081 
Epoch 744/1000 
	 loss: 17.1772, MinusLogProbMetric: 17.1772, val_loss: 17.1693, val_MinusLogProbMetric: 17.1693

Epoch 744: val_loss did not improve from 17.16923
196/196 - 63s - loss: 17.1772 - MinusLogProbMetric: 17.1772 - val_loss: 17.1693 - val_MinusLogProbMetric: 17.1693 - lr: 5.5556e-05 - 63s/epoch - 322ms/step
Epoch 745/1000
2023-10-24 02:45:51.658 
Epoch 745/1000 
	 loss: 17.1763, MinusLogProbMetric: 17.1763, val_loss: 17.2012, val_MinusLogProbMetric: 17.2012

Epoch 745: val_loss did not improve from 17.16923
196/196 - 64s - loss: 17.1763 - MinusLogProbMetric: 17.1763 - val_loss: 17.2012 - val_MinusLogProbMetric: 17.2012 - lr: 5.5556e-05 - 64s/epoch - 324ms/step
Epoch 746/1000
2023-10-24 02:46:55.085 
Epoch 746/1000 
	 loss: 17.1919, MinusLogProbMetric: 17.1919, val_loss: 17.2390, val_MinusLogProbMetric: 17.2390

Epoch 746: val_loss did not improve from 17.16923
196/196 - 63s - loss: 17.1919 - MinusLogProbMetric: 17.1919 - val_loss: 17.2390 - val_MinusLogProbMetric: 17.2390 - lr: 5.5556e-05 - 63s/epoch - 324ms/step
Epoch 747/1000
2023-10-24 02:47:56.702 
Epoch 747/1000 
	 loss: 17.1880, MinusLogProbMetric: 17.1880, val_loss: 17.3028, val_MinusLogProbMetric: 17.3028

Epoch 747: val_loss did not improve from 17.16923
196/196 - 62s - loss: 17.1880 - MinusLogProbMetric: 17.1880 - val_loss: 17.3028 - val_MinusLogProbMetric: 17.3028 - lr: 5.5556e-05 - 62s/epoch - 314ms/step
Epoch 748/1000
2023-10-24 02:48:56.938 
Epoch 748/1000 
	 loss: 17.1736, MinusLogProbMetric: 17.1736, val_loss: 17.2108, val_MinusLogProbMetric: 17.2108

Epoch 748: val_loss did not improve from 17.16923
196/196 - 60s - loss: 17.1736 - MinusLogProbMetric: 17.1736 - val_loss: 17.2108 - val_MinusLogProbMetric: 17.2108 - lr: 5.5556e-05 - 60s/epoch - 307ms/step
Epoch 749/1000
2023-10-24 02:50:01.343 
Epoch 749/1000 
	 loss: 17.1679, MinusLogProbMetric: 17.1679, val_loss: 17.1747, val_MinusLogProbMetric: 17.1747

Epoch 749: val_loss did not improve from 17.16923
196/196 - 64s - loss: 17.1679 - MinusLogProbMetric: 17.1679 - val_loss: 17.1747 - val_MinusLogProbMetric: 17.1747 - lr: 5.5556e-05 - 64s/epoch - 329ms/step
Epoch 750/1000
2023-10-24 02:51:04.439 
Epoch 750/1000 
	 loss: 17.2159, MinusLogProbMetric: 17.2159, val_loss: 17.4353, val_MinusLogProbMetric: 17.4353

Epoch 750: val_loss did not improve from 17.16923
196/196 - 63s - loss: 17.2159 - MinusLogProbMetric: 17.2159 - val_loss: 17.4353 - val_MinusLogProbMetric: 17.4353 - lr: 5.5556e-05 - 63s/epoch - 322ms/step
Epoch 751/1000
2023-10-24 02:52:04.956 
Epoch 751/1000 
	 loss: 17.1903, MinusLogProbMetric: 17.1903, val_loss: 17.2554, val_MinusLogProbMetric: 17.2554

Epoch 751: val_loss did not improve from 17.16923
196/196 - 61s - loss: 17.1903 - MinusLogProbMetric: 17.1903 - val_loss: 17.2554 - val_MinusLogProbMetric: 17.2554 - lr: 5.5556e-05 - 61s/epoch - 309ms/step
Epoch 752/1000
2023-10-24 02:53:08.743 
Epoch 752/1000 
	 loss: 17.1731, MinusLogProbMetric: 17.1731, val_loss: 17.2372, val_MinusLogProbMetric: 17.2372

Epoch 752: val_loss did not improve from 17.16923
196/196 - 64s - loss: 17.1731 - MinusLogProbMetric: 17.1731 - val_loss: 17.2372 - val_MinusLogProbMetric: 17.2372 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 753/1000
2023-10-24 02:54:10.662 
Epoch 753/1000 
	 loss: 17.1948, MinusLogProbMetric: 17.1948, val_loss: 17.1944, val_MinusLogProbMetric: 17.1944

Epoch 753: val_loss did not improve from 17.16923
196/196 - 62s - loss: 17.1948 - MinusLogProbMetric: 17.1948 - val_loss: 17.1944 - val_MinusLogProbMetric: 17.1944 - lr: 5.5556e-05 - 62s/epoch - 316ms/step
Epoch 754/1000
2023-10-24 02:55:18.207 
Epoch 754/1000 
	 loss: 17.1785, MinusLogProbMetric: 17.1785, val_loss: 17.3642, val_MinusLogProbMetric: 17.3642

Epoch 754: val_loss did not improve from 17.16923
196/196 - 68s - loss: 17.1785 - MinusLogProbMetric: 17.1785 - val_loss: 17.3642 - val_MinusLogProbMetric: 17.3642 - lr: 5.5556e-05 - 68s/epoch - 345ms/step
Epoch 755/1000
2023-10-24 02:56:23.863 
Epoch 755/1000 
	 loss: 17.1972, MinusLogProbMetric: 17.1972, val_loss: 17.3808, val_MinusLogProbMetric: 17.3808

Epoch 755: val_loss did not improve from 17.16923
196/196 - 66s - loss: 17.1972 - MinusLogProbMetric: 17.1972 - val_loss: 17.3808 - val_MinusLogProbMetric: 17.3808 - lr: 5.5556e-05 - 66s/epoch - 335ms/step
Epoch 756/1000
2023-10-24 02:57:28.548 
Epoch 756/1000 
	 loss: 17.1779, MinusLogProbMetric: 17.1779, val_loss: 17.2369, val_MinusLogProbMetric: 17.2369

Epoch 756: val_loss did not improve from 17.16923
196/196 - 65s - loss: 17.1779 - MinusLogProbMetric: 17.1779 - val_loss: 17.2369 - val_MinusLogProbMetric: 17.2369 - lr: 5.5556e-05 - 65s/epoch - 330ms/step
Epoch 757/1000
2023-10-24 02:58:30.330 
Epoch 757/1000 
	 loss: 17.1723, MinusLogProbMetric: 17.1723, val_loss: 17.2414, val_MinusLogProbMetric: 17.2414

Epoch 757: val_loss did not improve from 17.16923
196/196 - 62s - loss: 17.1723 - MinusLogProbMetric: 17.1723 - val_loss: 17.2414 - val_MinusLogProbMetric: 17.2414 - lr: 5.5556e-05 - 62s/epoch - 315ms/step
Epoch 758/1000
2023-10-24 02:59:32.577 
Epoch 758/1000 
	 loss: 17.1850, MinusLogProbMetric: 17.1850, val_loss: 17.1965, val_MinusLogProbMetric: 17.1965

Epoch 758: val_loss did not improve from 17.16923
196/196 - 62s - loss: 17.1850 - MinusLogProbMetric: 17.1850 - val_loss: 17.1965 - val_MinusLogProbMetric: 17.1965 - lr: 5.5556e-05 - 62s/epoch - 318ms/step
Epoch 759/1000
2023-10-24 03:00:34.992 
Epoch 759/1000 
	 loss: 17.1610, MinusLogProbMetric: 17.1610, val_loss: 17.1966, val_MinusLogProbMetric: 17.1966

Epoch 759: val_loss did not improve from 17.16923
196/196 - 62s - loss: 17.1610 - MinusLogProbMetric: 17.1610 - val_loss: 17.1966 - val_MinusLogProbMetric: 17.1966 - lr: 5.5556e-05 - 62s/epoch - 318ms/step
Epoch 760/1000
2023-10-24 03:01:36.309 
Epoch 760/1000 
	 loss: 17.1819, MinusLogProbMetric: 17.1819, val_loss: 17.1560, val_MinusLogProbMetric: 17.1560

Epoch 760: val_loss improved from 17.16923 to 17.15604, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 17.1819 - MinusLogProbMetric: 17.1819 - val_loss: 17.1560 - val_MinusLogProbMetric: 17.1560 - lr: 5.5556e-05 - 63s/epoch - 321ms/step
Epoch 761/1000
2023-10-24 03:02:38.353 
Epoch 761/1000 
	 loss: 17.1555, MinusLogProbMetric: 17.1555, val_loss: 17.2675, val_MinusLogProbMetric: 17.2675

Epoch 761: val_loss did not improve from 17.15604
196/196 - 60s - loss: 17.1555 - MinusLogProbMetric: 17.1555 - val_loss: 17.2675 - val_MinusLogProbMetric: 17.2675 - lr: 5.5556e-05 - 60s/epoch - 308ms/step
Epoch 762/1000
2023-10-24 03:03:37.173 
Epoch 762/1000 
	 loss: 17.1855, MinusLogProbMetric: 17.1855, val_loss: 17.2629, val_MinusLogProbMetric: 17.2629

Epoch 762: val_loss did not improve from 17.15604
196/196 - 59s - loss: 17.1855 - MinusLogProbMetric: 17.1855 - val_loss: 17.2629 - val_MinusLogProbMetric: 17.2629 - lr: 5.5556e-05 - 59s/epoch - 300ms/step
Epoch 763/1000
2023-10-24 03:04:41.183 
Epoch 763/1000 
	 loss: 17.1807, MinusLogProbMetric: 17.1807, val_loss: 17.2793, val_MinusLogProbMetric: 17.2793

Epoch 763: val_loss did not improve from 17.15604
196/196 - 64s - loss: 17.1807 - MinusLogProbMetric: 17.1807 - val_loss: 17.2793 - val_MinusLogProbMetric: 17.2793 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 764/1000
2023-10-24 03:05:43.063 
Epoch 764/1000 
	 loss: 17.1790, MinusLogProbMetric: 17.1790, val_loss: 17.1675, val_MinusLogProbMetric: 17.1675

Epoch 764: val_loss did not improve from 17.15604
196/196 - 62s - loss: 17.1790 - MinusLogProbMetric: 17.1790 - val_loss: 17.1675 - val_MinusLogProbMetric: 17.1675 - lr: 5.5556e-05 - 62s/epoch - 316ms/step
Epoch 765/1000
2023-10-24 03:06:43.303 
Epoch 765/1000 
	 loss: 17.1984, MinusLogProbMetric: 17.1984, val_loss: 17.2263, val_MinusLogProbMetric: 17.2263

Epoch 765: val_loss did not improve from 17.15604
196/196 - 60s - loss: 17.1984 - MinusLogProbMetric: 17.1984 - val_loss: 17.2263 - val_MinusLogProbMetric: 17.2263 - lr: 5.5556e-05 - 60s/epoch - 307ms/step
Epoch 766/1000
2023-10-24 03:07:43.244 
Epoch 766/1000 
	 loss: 17.1762, MinusLogProbMetric: 17.1762, val_loss: 17.3652, val_MinusLogProbMetric: 17.3652

Epoch 766: val_loss did not improve from 17.15604
196/196 - 60s - loss: 17.1762 - MinusLogProbMetric: 17.1762 - val_loss: 17.3652 - val_MinusLogProbMetric: 17.3652 - lr: 5.5556e-05 - 60s/epoch - 306ms/step
Epoch 767/1000
2023-10-24 03:08:44.013 
Epoch 767/1000 
	 loss: 17.2053, MinusLogProbMetric: 17.2053, val_loss: 17.2362, val_MinusLogProbMetric: 17.2362

Epoch 767: val_loss did not improve from 17.15604
196/196 - 61s - loss: 17.2053 - MinusLogProbMetric: 17.2053 - val_loss: 17.2362 - val_MinusLogProbMetric: 17.2362 - lr: 5.5556e-05 - 61s/epoch - 310ms/step
Epoch 768/1000
2023-10-24 03:09:45.710 
Epoch 768/1000 
	 loss: 17.1975, MinusLogProbMetric: 17.1975, val_loss: 17.4274, val_MinusLogProbMetric: 17.4274

Epoch 768: val_loss did not improve from 17.15604
196/196 - 62s - loss: 17.1975 - MinusLogProbMetric: 17.1975 - val_loss: 17.4274 - val_MinusLogProbMetric: 17.4274 - lr: 5.5556e-05 - 62s/epoch - 315ms/step
Epoch 769/1000
2023-10-24 03:10:48.878 
Epoch 769/1000 
	 loss: 17.1775, MinusLogProbMetric: 17.1775, val_loss: 17.2141, val_MinusLogProbMetric: 17.2141

Epoch 769: val_loss did not improve from 17.15604
196/196 - 63s - loss: 17.1775 - MinusLogProbMetric: 17.1775 - val_loss: 17.2141 - val_MinusLogProbMetric: 17.2141 - lr: 5.5556e-05 - 63s/epoch - 322ms/step
Epoch 770/1000
2023-10-24 03:11:50.460 
Epoch 770/1000 
	 loss: 17.1697, MinusLogProbMetric: 17.1697, val_loss: 17.2157, val_MinusLogProbMetric: 17.2157

Epoch 770: val_loss did not improve from 17.15604
196/196 - 62s - loss: 17.1697 - MinusLogProbMetric: 17.1697 - val_loss: 17.2157 - val_MinusLogProbMetric: 17.2157 - lr: 5.5556e-05 - 62s/epoch - 314ms/step
Epoch 771/1000
2023-10-24 03:12:52.421 
Epoch 771/1000 
	 loss: 17.1959, MinusLogProbMetric: 17.1959, val_loss: 17.3150, val_MinusLogProbMetric: 17.3150

Epoch 771: val_loss did not improve from 17.15604
196/196 - 62s - loss: 17.1959 - MinusLogProbMetric: 17.1959 - val_loss: 17.3150 - val_MinusLogProbMetric: 17.3150 - lr: 5.5556e-05 - 62s/epoch - 316ms/step
Epoch 772/1000
2023-10-24 03:13:54.422 
Epoch 772/1000 
	 loss: 17.1756, MinusLogProbMetric: 17.1756, val_loss: 17.2360, val_MinusLogProbMetric: 17.2360

Epoch 772: val_loss did not improve from 17.15604
196/196 - 62s - loss: 17.1756 - MinusLogProbMetric: 17.1756 - val_loss: 17.2360 - val_MinusLogProbMetric: 17.2360 - lr: 5.5556e-05 - 62s/epoch - 316ms/step
Epoch 773/1000
2023-10-24 03:14:56.144 
Epoch 773/1000 
	 loss: 17.1559, MinusLogProbMetric: 17.1559, val_loss: 17.2012, val_MinusLogProbMetric: 17.2012

Epoch 773: val_loss did not improve from 17.15604
196/196 - 62s - loss: 17.1559 - MinusLogProbMetric: 17.1559 - val_loss: 17.2012 - val_MinusLogProbMetric: 17.2012 - lr: 5.5556e-05 - 62s/epoch - 315ms/step
Epoch 774/1000
2023-10-24 03:15:59.094 
Epoch 774/1000 
	 loss: 17.1833, MinusLogProbMetric: 17.1833, val_loss: 17.1897, val_MinusLogProbMetric: 17.1897

Epoch 774: val_loss did not improve from 17.15604
196/196 - 63s - loss: 17.1833 - MinusLogProbMetric: 17.1833 - val_loss: 17.1897 - val_MinusLogProbMetric: 17.1897 - lr: 5.5556e-05 - 63s/epoch - 321ms/step
Epoch 775/1000
2023-10-24 03:16:58.867 
Epoch 775/1000 
	 loss: 17.1639, MinusLogProbMetric: 17.1639, val_loss: 17.2192, val_MinusLogProbMetric: 17.2192

Epoch 775: val_loss did not improve from 17.15604
196/196 - 60s - loss: 17.1639 - MinusLogProbMetric: 17.1639 - val_loss: 17.2192 - val_MinusLogProbMetric: 17.2192 - lr: 5.5556e-05 - 60s/epoch - 305ms/step
Epoch 776/1000
2023-10-24 03:17:56.811 
Epoch 776/1000 
	 loss: 17.1679, MinusLogProbMetric: 17.1679, val_loss: 17.1870, val_MinusLogProbMetric: 17.1870

Epoch 776: val_loss did not improve from 17.15604
196/196 - 58s - loss: 17.1679 - MinusLogProbMetric: 17.1679 - val_loss: 17.1870 - val_MinusLogProbMetric: 17.1870 - lr: 5.5556e-05 - 58s/epoch - 296ms/step
Epoch 777/1000
2023-10-24 03:19:02.620 
Epoch 777/1000 
	 loss: 17.1814, MinusLogProbMetric: 17.1814, val_loss: 17.2979, val_MinusLogProbMetric: 17.2979

Epoch 777: val_loss did not improve from 17.15604
196/196 - 66s - loss: 17.1814 - MinusLogProbMetric: 17.1814 - val_loss: 17.2979 - val_MinusLogProbMetric: 17.2979 - lr: 5.5556e-05 - 66s/epoch - 336ms/step
Epoch 778/1000
2023-10-24 03:20:09.122 
Epoch 778/1000 
	 loss: 17.1448, MinusLogProbMetric: 17.1448, val_loss: 17.2515, val_MinusLogProbMetric: 17.2515

Epoch 778: val_loss did not improve from 17.15604
196/196 - 66s - loss: 17.1448 - MinusLogProbMetric: 17.1448 - val_loss: 17.2515 - val_MinusLogProbMetric: 17.2515 - lr: 5.5556e-05 - 66s/epoch - 339ms/step
Epoch 779/1000
2023-10-24 03:21:09.705 
Epoch 779/1000 
	 loss: 17.1933, MinusLogProbMetric: 17.1933, val_loss: 17.2619, val_MinusLogProbMetric: 17.2619

Epoch 779: val_loss did not improve from 17.15604
196/196 - 61s - loss: 17.1933 - MinusLogProbMetric: 17.1933 - val_loss: 17.2619 - val_MinusLogProbMetric: 17.2619 - lr: 5.5556e-05 - 61s/epoch - 309ms/step
Epoch 780/1000
2023-10-24 03:22:11.809 
Epoch 780/1000 
	 loss: 17.1898, MinusLogProbMetric: 17.1898, val_loss: 17.3125, val_MinusLogProbMetric: 17.3125

Epoch 780: val_loss did not improve from 17.15604
196/196 - 62s - loss: 17.1898 - MinusLogProbMetric: 17.1898 - val_loss: 17.3125 - val_MinusLogProbMetric: 17.3125 - lr: 5.5556e-05 - 62s/epoch - 317ms/step
Epoch 781/1000
2023-10-24 03:23:12.256 
Epoch 781/1000 
	 loss: 17.1637, MinusLogProbMetric: 17.1637, val_loss: 17.2311, val_MinusLogProbMetric: 17.2311

Epoch 781: val_loss did not improve from 17.15604
196/196 - 60s - loss: 17.1637 - MinusLogProbMetric: 17.1637 - val_loss: 17.2311 - val_MinusLogProbMetric: 17.2311 - lr: 5.5556e-05 - 60s/epoch - 308ms/step
Epoch 782/1000
2023-10-24 03:24:14.450 
Epoch 782/1000 
	 loss: 17.1508, MinusLogProbMetric: 17.1508, val_loss: 17.2595, val_MinusLogProbMetric: 17.2595

Epoch 782: val_loss did not improve from 17.15604
196/196 - 62s - loss: 17.1508 - MinusLogProbMetric: 17.1508 - val_loss: 17.2595 - val_MinusLogProbMetric: 17.2595 - lr: 5.5556e-05 - 62s/epoch - 317ms/step
Epoch 783/1000
2023-10-24 03:25:14.089 
Epoch 783/1000 
	 loss: 17.1739, MinusLogProbMetric: 17.1739, val_loss: 17.2088, val_MinusLogProbMetric: 17.2088

Epoch 783: val_loss did not improve from 17.15604
196/196 - 60s - loss: 17.1739 - MinusLogProbMetric: 17.1739 - val_loss: 17.2088 - val_MinusLogProbMetric: 17.2088 - lr: 5.5556e-05 - 60s/epoch - 304ms/step
Epoch 784/1000
2023-10-24 03:26:13.210 
Epoch 784/1000 
	 loss: 17.1694, MinusLogProbMetric: 17.1694, val_loss: 17.2246, val_MinusLogProbMetric: 17.2246

Epoch 784: val_loss did not improve from 17.15604
196/196 - 59s - loss: 17.1694 - MinusLogProbMetric: 17.1694 - val_loss: 17.2246 - val_MinusLogProbMetric: 17.2246 - lr: 5.5556e-05 - 59s/epoch - 302ms/step
Epoch 785/1000
2023-10-24 03:27:12.133 
Epoch 785/1000 
	 loss: 17.1649, MinusLogProbMetric: 17.1649, val_loss: 17.2210, val_MinusLogProbMetric: 17.2210

Epoch 785: val_loss did not improve from 17.15604
196/196 - 59s - loss: 17.1649 - MinusLogProbMetric: 17.1649 - val_loss: 17.2210 - val_MinusLogProbMetric: 17.2210 - lr: 5.5556e-05 - 59s/epoch - 301ms/step
Epoch 786/1000
2023-10-24 03:28:13.569 
Epoch 786/1000 
	 loss: 17.1646, MinusLogProbMetric: 17.1646, val_loss: 17.2526, val_MinusLogProbMetric: 17.2526

Epoch 786: val_loss did not improve from 17.15604
196/196 - 61s - loss: 17.1646 - MinusLogProbMetric: 17.1646 - val_loss: 17.2526 - val_MinusLogProbMetric: 17.2526 - lr: 5.5556e-05 - 61s/epoch - 313ms/step
Epoch 787/1000
2023-10-24 03:29:10.999 
Epoch 787/1000 
	 loss: 17.1668, MinusLogProbMetric: 17.1668, val_loss: 17.3092, val_MinusLogProbMetric: 17.3092

Epoch 787: val_loss did not improve from 17.15604
196/196 - 57s - loss: 17.1668 - MinusLogProbMetric: 17.1668 - val_loss: 17.3092 - val_MinusLogProbMetric: 17.3092 - lr: 5.5556e-05 - 57s/epoch - 293ms/step
Epoch 788/1000
2023-10-24 03:30:08.895 
Epoch 788/1000 
	 loss: 17.1674, MinusLogProbMetric: 17.1674, val_loss: 17.2157, val_MinusLogProbMetric: 17.2157

Epoch 788: val_loss did not improve from 17.15604
196/196 - 58s - loss: 17.1674 - MinusLogProbMetric: 17.1674 - val_loss: 17.2157 - val_MinusLogProbMetric: 17.2157 - lr: 5.5556e-05 - 58s/epoch - 295ms/step
Epoch 789/1000
2023-10-24 03:31:08.526 
Epoch 789/1000 
	 loss: 17.1406, MinusLogProbMetric: 17.1406, val_loss: 17.3476, val_MinusLogProbMetric: 17.3476

Epoch 789: val_loss did not improve from 17.15604
196/196 - 60s - loss: 17.1406 - MinusLogProbMetric: 17.1406 - val_loss: 17.3476 - val_MinusLogProbMetric: 17.3476 - lr: 5.5556e-05 - 60s/epoch - 304ms/step
Epoch 790/1000
2023-10-24 03:32:08.637 
Epoch 790/1000 
	 loss: 17.1621, MinusLogProbMetric: 17.1621, val_loss: 17.1798, val_MinusLogProbMetric: 17.1798

Epoch 790: val_loss did not improve from 17.15604
196/196 - 60s - loss: 17.1621 - MinusLogProbMetric: 17.1621 - val_loss: 17.1798 - val_MinusLogProbMetric: 17.1798 - lr: 5.5556e-05 - 60s/epoch - 307ms/step
Epoch 791/1000
2023-10-24 03:33:08.893 
Epoch 791/1000 
	 loss: 17.1666, MinusLogProbMetric: 17.1666, val_loss: 17.2472, val_MinusLogProbMetric: 17.2472

Epoch 791: val_loss did not improve from 17.15604
196/196 - 60s - loss: 17.1666 - MinusLogProbMetric: 17.1666 - val_loss: 17.2472 - val_MinusLogProbMetric: 17.2472 - lr: 5.5556e-05 - 60s/epoch - 307ms/step
Epoch 792/1000
2023-10-24 03:34:07.762 
Epoch 792/1000 
	 loss: 17.1673, MinusLogProbMetric: 17.1673, val_loss: 17.1882, val_MinusLogProbMetric: 17.1882

Epoch 792: val_loss did not improve from 17.15604
196/196 - 59s - loss: 17.1673 - MinusLogProbMetric: 17.1673 - val_loss: 17.1882 - val_MinusLogProbMetric: 17.1882 - lr: 5.5556e-05 - 59s/epoch - 300ms/step
Epoch 793/1000
2023-10-24 03:35:06.489 
Epoch 793/1000 
	 loss: 17.1532, MinusLogProbMetric: 17.1532, val_loss: 17.1313, val_MinusLogProbMetric: 17.1313

Epoch 793: val_loss improved from 17.15604 to 17.13128, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 60s - loss: 17.1532 - MinusLogProbMetric: 17.1532 - val_loss: 17.1313 - val_MinusLogProbMetric: 17.1313 - lr: 5.5556e-05 - 60s/epoch - 305ms/step
Epoch 794/1000
2023-10-24 03:36:08.285 
Epoch 794/1000 
	 loss: 17.1630, MinusLogProbMetric: 17.1630, val_loss: 17.2541, val_MinusLogProbMetric: 17.2541

Epoch 794: val_loss did not improve from 17.13128
196/196 - 61s - loss: 17.1630 - MinusLogProbMetric: 17.1630 - val_loss: 17.2541 - val_MinusLogProbMetric: 17.2541 - lr: 5.5556e-05 - 61s/epoch - 310ms/step
Epoch 795/1000
2023-10-24 03:37:09.696 
Epoch 795/1000 
	 loss: 17.1738, MinusLogProbMetric: 17.1738, val_loss: 17.3914, val_MinusLogProbMetric: 17.3914

Epoch 795: val_loss did not improve from 17.13128
196/196 - 61s - loss: 17.1738 - MinusLogProbMetric: 17.1738 - val_loss: 17.3914 - val_MinusLogProbMetric: 17.3914 - lr: 5.5556e-05 - 61s/epoch - 313ms/step
Epoch 796/1000
2023-10-24 03:38:07.518 
Epoch 796/1000 
	 loss: 17.2187, MinusLogProbMetric: 17.2187, val_loss: 17.3338, val_MinusLogProbMetric: 17.3338

Epoch 796: val_loss did not improve from 17.13128
196/196 - 58s - loss: 17.2187 - MinusLogProbMetric: 17.2187 - val_loss: 17.3338 - val_MinusLogProbMetric: 17.3338 - lr: 5.5556e-05 - 58s/epoch - 295ms/step
Epoch 797/1000
2023-10-24 03:39:10.304 
Epoch 797/1000 
	 loss: 17.1544, MinusLogProbMetric: 17.1544, val_loss: 17.2110, val_MinusLogProbMetric: 17.2110

Epoch 797: val_loss did not improve from 17.13128
196/196 - 63s - loss: 17.1544 - MinusLogProbMetric: 17.1544 - val_loss: 17.2110 - val_MinusLogProbMetric: 17.2110 - lr: 5.5556e-05 - 63s/epoch - 320ms/step
Epoch 798/1000
2023-10-24 03:40:12.408 
Epoch 798/1000 
	 loss: 17.1623, MinusLogProbMetric: 17.1623, val_loss: 17.2435, val_MinusLogProbMetric: 17.2435

Epoch 798: val_loss did not improve from 17.13128
196/196 - 62s - loss: 17.1623 - MinusLogProbMetric: 17.1623 - val_loss: 17.2435 - val_MinusLogProbMetric: 17.2435 - lr: 5.5556e-05 - 62s/epoch - 317ms/step
Epoch 799/1000
2023-10-24 03:41:13.360 
Epoch 799/1000 
	 loss: 17.1696, MinusLogProbMetric: 17.1696, val_loss: 17.3597, val_MinusLogProbMetric: 17.3597

Epoch 799: val_loss did not improve from 17.13128
196/196 - 61s - loss: 17.1696 - MinusLogProbMetric: 17.1696 - val_loss: 17.3597 - val_MinusLogProbMetric: 17.3597 - lr: 5.5556e-05 - 61s/epoch - 311ms/step
Epoch 800/1000
2023-10-24 03:42:12.955 
Epoch 800/1000 
	 loss: 17.1745, MinusLogProbMetric: 17.1745, val_loss: 17.4819, val_MinusLogProbMetric: 17.4819

Epoch 800: val_loss did not improve from 17.13128
196/196 - 60s - loss: 17.1745 - MinusLogProbMetric: 17.1745 - val_loss: 17.4819 - val_MinusLogProbMetric: 17.4819 - lr: 5.5556e-05 - 60s/epoch - 304ms/step
Epoch 801/1000
2023-10-24 03:43:12.727 
Epoch 801/1000 
	 loss: 17.1684, MinusLogProbMetric: 17.1684, val_loss: 17.2161, val_MinusLogProbMetric: 17.2161

Epoch 801: val_loss did not improve from 17.13128
196/196 - 60s - loss: 17.1684 - MinusLogProbMetric: 17.1684 - val_loss: 17.2161 - val_MinusLogProbMetric: 17.2161 - lr: 5.5556e-05 - 60s/epoch - 305ms/step
Epoch 802/1000
2023-10-24 03:44:16.795 
Epoch 802/1000 
	 loss: 17.1566, MinusLogProbMetric: 17.1566, val_loss: 17.2728, val_MinusLogProbMetric: 17.2728

Epoch 802: val_loss did not improve from 17.13128
196/196 - 64s - loss: 17.1566 - MinusLogProbMetric: 17.1566 - val_loss: 17.2728 - val_MinusLogProbMetric: 17.2728 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 803/1000
2023-10-24 03:45:15.809 
Epoch 803/1000 
	 loss: 17.1708, MinusLogProbMetric: 17.1708, val_loss: 17.1812, val_MinusLogProbMetric: 17.1812

Epoch 803: val_loss did not improve from 17.13128
196/196 - 59s - loss: 17.1708 - MinusLogProbMetric: 17.1708 - val_loss: 17.1812 - val_MinusLogProbMetric: 17.1812 - lr: 5.5556e-05 - 59s/epoch - 301ms/step
Epoch 804/1000
2023-10-24 03:46:14.767 
Epoch 804/1000 
	 loss: 17.1784, MinusLogProbMetric: 17.1784, val_loss: 17.1858, val_MinusLogProbMetric: 17.1858

Epoch 804: val_loss did not improve from 17.13128
196/196 - 59s - loss: 17.1784 - MinusLogProbMetric: 17.1784 - val_loss: 17.1858 - val_MinusLogProbMetric: 17.1858 - lr: 5.5556e-05 - 59s/epoch - 301ms/step
Epoch 805/1000
2023-10-24 03:47:20.246 
Epoch 805/1000 
	 loss: 17.1706, MinusLogProbMetric: 17.1706, val_loss: 17.2357, val_MinusLogProbMetric: 17.2357

Epoch 805: val_loss did not improve from 17.13128
196/196 - 65s - loss: 17.1706 - MinusLogProbMetric: 17.1706 - val_loss: 17.2357 - val_MinusLogProbMetric: 17.2357 - lr: 5.5556e-05 - 65s/epoch - 334ms/step
Epoch 806/1000
2023-10-24 03:48:20.630 
Epoch 806/1000 
	 loss: 17.1706, MinusLogProbMetric: 17.1706, val_loss: 17.2458, val_MinusLogProbMetric: 17.2458

Epoch 806: val_loss did not improve from 17.13128
196/196 - 60s - loss: 17.1706 - MinusLogProbMetric: 17.1706 - val_loss: 17.2458 - val_MinusLogProbMetric: 17.2458 - lr: 5.5556e-05 - 60s/epoch - 308ms/step
Epoch 807/1000
2023-10-24 03:49:26.273 
Epoch 807/1000 
	 loss: 17.1607, MinusLogProbMetric: 17.1607, val_loss: 17.3547, val_MinusLogProbMetric: 17.3547

Epoch 807: val_loss did not improve from 17.13128
196/196 - 66s - loss: 17.1607 - MinusLogProbMetric: 17.1607 - val_loss: 17.3547 - val_MinusLogProbMetric: 17.3547 - lr: 5.5556e-05 - 66s/epoch - 335ms/step
Epoch 808/1000
2023-10-24 03:50:28.658 
Epoch 808/1000 
	 loss: 17.1736, MinusLogProbMetric: 17.1736, val_loss: 17.2363, val_MinusLogProbMetric: 17.2363

Epoch 808: val_loss did not improve from 17.13128
196/196 - 62s - loss: 17.1736 - MinusLogProbMetric: 17.1736 - val_loss: 17.2363 - val_MinusLogProbMetric: 17.2363 - lr: 5.5556e-05 - 62s/epoch - 318ms/step
Epoch 809/1000
2023-10-24 03:51:31.033 
Epoch 809/1000 
	 loss: 17.1626, MinusLogProbMetric: 17.1626, val_loss: 17.2171, val_MinusLogProbMetric: 17.2171

Epoch 809: val_loss did not improve from 17.13128
196/196 - 62s - loss: 17.1626 - MinusLogProbMetric: 17.1626 - val_loss: 17.2171 - val_MinusLogProbMetric: 17.2171 - lr: 5.5556e-05 - 62s/epoch - 318ms/step
Epoch 810/1000
2023-10-24 03:52:33.488 
Epoch 810/1000 
	 loss: 17.1665, MinusLogProbMetric: 17.1665, val_loss: 17.3371, val_MinusLogProbMetric: 17.3371

Epoch 810: val_loss did not improve from 17.13128
196/196 - 62s - loss: 17.1665 - MinusLogProbMetric: 17.1665 - val_loss: 17.3371 - val_MinusLogProbMetric: 17.3371 - lr: 5.5556e-05 - 62s/epoch - 319ms/step
Epoch 811/1000
2023-10-24 03:53:36.101 
Epoch 811/1000 
	 loss: 17.1725, MinusLogProbMetric: 17.1725, val_loss: 17.1074, val_MinusLogProbMetric: 17.1074

Epoch 811: val_loss improved from 17.13128 to 17.10742, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 17.1725 - MinusLogProbMetric: 17.1725 - val_loss: 17.1074 - val_MinusLogProbMetric: 17.1074 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 812/1000
2023-10-24 03:54:38.311 
Epoch 812/1000 
	 loss: 17.1751, MinusLogProbMetric: 17.1751, val_loss: 17.2092, val_MinusLogProbMetric: 17.2092

Epoch 812: val_loss did not improve from 17.10742
196/196 - 61s - loss: 17.1751 - MinusLogProbMetric: 17.1751 - val_loss: 17.2092 - val_MinusLogProbMetric: 17.2092 - lr: 5.5556e-05 - 61s/epoch - 310ms/step
Epoch 813/1000
2023-10-24 03:55:37.743 
Epoch 813/1000 
	 loss: 17.1463, MinusLogProbMetric: 17.1463, val_loss: 17.1600, val_MinusLogProbMetric: 17.1600

Epoch 813: val_loss did not improve from 17.10742
196/196 - 59s - loss: 17.1463 - MinusLogProbMetric: 17.1463 - val_loss: 17.1600 - val_MinusLogProbMetric: 17.1600 - lr: 5.5556e-05 - 59s/epoch - 303ms/step
Epoch 814/1000
2023-10-24 03:56:36.677 
Epoch 814/1000 
	 loss: 17.1584, MinusLogProbMetric: 17.1584, val_loss: 17.1500, val_MinusLogProbMetric: 17.1500

Epoch 814: val_loss did not improve from 17.10742
196/196 - 59s - loss: 17.1584 - MinusLogProbMetric: 17.1584 - val_loss: 17.1500 - val_MinusLogProbMetric: 17.1500 - lr: 5.5556e-05 - 59s/epoch - 301ms/step
Epoch 815/1000
2023-10-24 03:57:39.022 
Epoch 815/1000 
	 loss: 17.1493, MinusLogProbMetric: 17.1493, val_loss: 17.1658, val_MinusLogProbMetric: 17.1658

Epoch 815: val_loss did not improve from 17.10742
196/196 - 62s - loss: 17.1493 - MinusLogProbMetric: 17.1493 - val_loss: 17.1658 - val_MinusLogProbMetric: 17.1658 - lr: 5.5556e-05 - 62s/epoch - 318ms/step
Epoch 816/1000
2023-10-24 03:58:49.630 
Epoch 816/1000 
	 loss: 17.1523, MinusLogProbMetric: 17.1523, val_loss: 17.3985, val_MinusLogProbMetric: 17.3985

Epoch 816: val_loss did not improve from 17.10742
196/196 - 71s - loss: 17.1523 - MinusLogProbMetric: 17.1523 - val_loss: 17.3985 - val_MinusLogProbMetric: 17.3985 - lr: 5.5556e-05 - 71s/epoch - 360ms/step
Epoch 817/1000
2023-10-24 03:59:52.112 
Epoch 817/1000 
	 loss: 17.1509, MinusLogProbMetric: 17.1509, val_loss: 17.2382, val_MinusLogProbMetric: 17.2382

Epoch 817: val_loss did not improve from 17.10742
196/196 - 62s - loss: 17.1509 - MinusLogProbMetric: 17.1509 - val_loss: 17.2382 - val_MinusLogProbMetric: 17.2382 - lr: 5.5556e-05 - 62s/epoch - 319ms/step
Epoch 818/1000
2023-10-24 04:00:53.474 
Epoch 818/1000 
	 loss: 17.1713, MinusLogProbMetric: 17.1713, val_loss: 17.1961, val_MinusLogProbMetric: 17.1961

Epoch 818: val_loss did not improve from 17.10742
196/196 - 61s - loss: 17.1713 - MinusLogProbMetric: 17.1713 - val_loss: 17.1961 - val_MinusLogProbMetric: 17.1961 - lr: 5.5556e-05 - 61s/epoch - 313ms/step
Epoch 819/1000
2023-10-24 04:01:55.640 
Epoch 819/1000 
	 loss: 17.1462, MinusLogProbMetric: 17.1462, val_loss: 17.2241, val_MinusLogProbMetric: 17.2241

Epoch 819: val_loss did not improve from 17.10742
196/196 - 62s - loss: 17.1462 - MinusLogProbMetric: 17.1462 - val_loss: 17.2241 - val_MinusLogProbMetric: 17.2241 - lr: 5.5556e-05 - 62s/epoch - 317ms/step
Epoch 820/1000
2023-10-24 04:02:55.343 
Epoch 820/1000 
	 loss: 17.1556, MinusLogProbMetric: 17.1556, val_loss: 17.3028, val_MinusLogProbMetric: 17.3028

Epoch 820: val_loss did not improve from 17.10742
196/196 - 60s - loss: 17.1556 - MinusLogProbMetric: 17.1556 - val_loss: 17.3028 - val_MinusLogProbMetric: 17.3028 - lr: 5.5556e-05 - 60s/epoch - 305ms/step
Epoch 821/1000
2023-10-24 04:03:54.403 
Epoch 821/1000 
	 loss: 17.1516, MinusLogProbMetric: 17.1516, val_loss: 17.2484, val_MinusLogProbMetric: 17.2484

Epoch 821: val_loss did not improve from 17.10742
196/196 - 59s - loss: 17.1516 - MinusLogProbMetric: 17.1516 - val_loss: 17.2484 - val_MinusLogProbMetric: 17.2484 - lr: 5.5556e-05 - 59s/epoch - 301ms/step
Epoch 822/1000
2023-10-24 04:04:53.742 
Epoch 822/1000 
	 loss: 17.1482, MinusLogProbMetric: 17.1482, val_loss: 17.2573, val_MinusLogProbMetric: 17.2573

Epoch 822: val_loss did not improve from 17.10742
196/196 - 59s - loss: 17.1482 - MinusLogProbMetric: 17.1482 - val_loss: 17.2573 - val_MinusLogProbMetric: 17.2573 - lr: 5.5556e-05 - 59s/epoch - 303ms/step
Epoch 823/1000
2023-10-24 04:05:55.677 
Epoch 823/1000 
	 loss: 17.1453, MinusLogProbMetric: 17.1453, val_loss: 17.2276, val_MinusLogProbMetric: 17.2276

Epoch 823: val_loss did not improve from 17.10742
196/196 - 62s - loss: 17.1453 - MinusLogProbMetric: 17.1453 - val_loss: 17.2276 - val_MinusLogProbMetric: 17.2276 - lr: 5.5556e-05 - 62s/epoch - 316ms/step
Epoch 824/1000
2023-10-24 04:06:58.117 
Epoch 824/1000 
	 loss: 17.1611, MinusLogProbMetric: 17.1611, val_loss: 17.2984, val_MinusLogProbMetric: 17.2984

Epoch 824: val_loss did not improve from 17.10742
196/196 - 62s - loss: 17.1611 - MinusLogProbMetric: 17.1611 - val_loss: 17.2984 - val_MinusLogProbMetric: 17.2984 - lr: 5.5556e-05 - 62s/epoch - 319ms/step
Epoch 825/1000
2023-10-24 04:07:57.818 
Epoch 825/1000 
	 loss: 17.1441, MinusLogProbMetric: 17.1441, val_loss: 17.1677, val_MinusLogProbMetric: 17.1677

Epoch 825: val_loss did not improve from 17.10742
196/196 - 60s - loss: 17.1441 - MinusLogProbMetric: 17.1441 - val_loss: 17.1677 - val_MinusLogProbMetric: 17.1677 - lr: 5.5556e-05 - 60s/epoch - 305ms/step
Epoch 826/1000
2023-10-24 04:08:57.601 
Epoch 826/1000 
	 loss: 17.1475, MinusLogProbMetric: 17.1475, val_loss: 17.1240, val_MinusLogProbMetric: 17.1240

Epoch 826: val_loss did not improve from 17.10742
196/196 - 60s - loss: 17.1475 - MinusLogProbMetric: 17.1475 - val_loss: 17.1240 - val_MinusLogProbMetric: 17.1240 - lr: 5.5556e-05 - 60s/epoch - 305ms/step
Epoch 827/1000
2023-10-24 04:09:56.606 
Epoch 827/1000 
	 loss: 17.1441, MinusLogProbMetric: 17.1441, val_loss: 17.1971, val_MinusLogProbMetric: 17.1971

Epoch 827: val_loss did not improve from 17.10742
196/196 - 59s - loss: 17.1441 - MinusLogProbMetric: 17.1441 - val_loss: 17.1971 - val_MinusLogProbMetric: 17.1971 - lr: 5.5556e-05 - 59s/epoch - 301ms/step
Epoch 828/1000
2023-10-24 04:10:56.454 
Epoch 828/1000 
	 loss: 17.1796, MinusLogProbMetric: 17.1796, val_loss: 17.2765, val_MinusLogProbMetric: 17.2765

Epoch 828: val_loss did not improve from 17.10742
196/196 - 60s - loss: 17.1796 - MinusLogProbMetric: 17.1796 - val_loss: 17.2765 - val_MinusLogProbMetric: 17.2765 - lr: 5.5556e-05 - 60s/epoch - 305ms/step
Epoch 829/1000
2023-10-24 04:12:01.810 
Epoch 829/1000 
	 loss: 17.1403, MinusLogProbMetric: 17.1403, val_loss: 17.1126, val_MinusLogProbMetric: 17.1126

Epoch 829: val_loss did not improve from 17.10742
196/196 - 65s - loss: 17.1403 - MinusLogProbMetric: 17.1403 - val_loss: 17.1126 - val_MinusLogProbMetric: 17.1126 - lr: 5.5556e-05 - 65s/epoch - 333ms/step
Epoch 830/1000
2023-10-24 04:13:03.538 
Epoch 830/1000 
	 loss: 17.1356, MinusLogProbMetric: 17.1356, val_loss: 17.2406, val_MinusLogProbMetric: 17.2406

Epoch 830: val_loss did not improve from 17.10742
196/196 - 62s - loss: 17.1356 - MinusLogProbMetric: 17.1356 - val_loss: 17.2406 - val_MinusLogProbMetric: 17.2406 - lr: 5.5556e-05 - 62s/epoch - 315ms/step
Epoch 831/1000
2023-10-24 04:14:06.094 
Epoch 831/1000 
	 loss: 17.1651, MinusLogProbMetric: 17.1651, val_loss: 17.2479, val_MinusLogProbMetric: 17.2479

Epoch 831: val_loss did not improve from 17.10742
196/196 - 63s - loss: 17.1651 - MinusLogProbMetric: 17.1651 - val_loss: 17.2479 - val_MinusLogProbMetric: 17.2479 - lr: 5.5556e-05 - 63s/epoch - 319ms/step
Epoch 832/1000
2023-10-24 04:15:05.953 
Epoch 832/1000 
	 loss: 17.1754, MinusLogProbMetric: 17.1754, val_loss: 17.1910, val_MinusLogProbMetric: 17.1910

Epoch 832: val_loss did not improve from 17.10742
196/196 - 60s - loss: 17.1754 - MinusLogProbMetric: 17.1754 - val_loss: 17.1910 - val_MinusLogProbMetric: 17.1910 - lr: 5.5556e-05 - 60s/epoch - 305ms/step
Epoch 833/1000
2023-10-24 04:16:10.225 
Epoch 833/1000 
	 loss: 17.1345, MinusLogProbMetric: 17.1345, val_loss: 17.3027, val_MinusLogProbMetric: 17.3027

Epoch 833: val_loss did not improve from 17.10742
196/196 - 64s - loss: 17.1345 - MinusLogProbMetric: 17.1345 - val_loss: 17.3027 - val_MinusLogProbMetric: 17.3027 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 834/1000
2023-10-24 04:17:08.891 
Epoch 834/1000 
	 loss: 17.1534, MinusLogProbMetric: 17.1534, val_loss: 17.2046, val_MinusLogProbMetric: 17.2046

Epoch 834: val_loss did not improve from 17.10742
196/196 - 59s - loss: 17.1534 - MinusLogProbMetric: 17.1534 - val_loss: 17.2046 - val_MinusLogProbMetric: 17.2046 - lr: 5.5556e-05 - 59s/epoch - 299ms/step
Epoch 835/1000
2023-10-24 04:18:08.237 
Epoch 835/1000 
	 loss: 17.1837, MinusLogProbMetric: 17.1837, val_loss: 17.1745, val_MinusLogProbMetric: 17.1745

Epoch 835: val_loss did not improve from 17.10742
196/196 - 59s - loss: 17.1837 - MinusLogProbMetric: 17.1837 - val_loss: 17.1745 - val_MinusLogProbMetric: 17.1745 - lr: 5.5556e-05 - 59s/epoch - 303ms/step
Epoch 836/1000
2023-10-24 04:19:15.133 
Epoch 836/1000 
	 loss: 17.1570, MinusLogProbMetric: 17.1570, val_loss: 17.1483, val_MinusLogProbMetric: 17.1483

Epoch 836: val_loss did not improve from 17.10742
196/196 - 67s - loss: 17.1570 - MinusLogProbMetric: 17.1570 - val_loss: 17.1483 - val_MinusLogProbMetric: 17.1483 - lr: 5.5556e-05 - 67s/epoch - 341ms/step
Epoch 837/1000
2023-10-24 04:20:15.511 
Epoch 837/1000 
	 loss: 17.1429, MinusLogProbMetric: 17.1429, val_loss: 17.2858, val_MinusLogProbMetric: 17.2858

Epoch 837: val_loss did not improve from 17.10742
196/196 - 60s - loss: 17.1429 - MinusLogProbMetric: 17.1429 - val_loss: 17.2858 - val_MinusLogProbMetric: 17.2858 - lr: 5.5556e-05 - 60s/epoch - 308ms/step
Epoch 838/1000
2023-10-24 04:21:14.832 
Epoch 838/1000 
	 loss: 17.1716, MinusLogProbMetric: 17.1716, val_loss: 17.2574, val_MinusLogProbMetric: 17.2574

Epoch 838: val_loss did not improve from 17.10742
196/196 - 59s - loss: 17.1716 - MinusLogProbMetric: 17.1716 - val_loss: 17.2574 - val_MinusLogProbMetric: 17.2574 - lr: 5.5556e-05 - 59s/epoch - 303ms/step
Epoch 839/1000
2023-10-24 04:22:20.823 
Epoch 839/1000 
	 loss: 17.1318, MinusLogProbMetric: 17.1318, val_loss: 17.2613, val_MinusLogProbMetric: 17.2613

Epoch 839: val_loss did not improve from 17.10742
196/196 - 66s - loss: 17.1318 - MinusLogProbMetric: 17.1318 - val_loss: 17.2613 - val_MinusLogProbMetric: 17.2613 - lr: 5.5556e-05 - 66s/epoch - 337ms/step
Epoch 840/1000
2023-10-24 04:23:18.937 
Epoch 840/1000 
	 loss: 17.1292, MinusLogProbMetric: 17.1292, val_loss: 17.3192, val_MinusLogProbMetric: 17.3192

Epoch 840: val_loss did not improve from 17.10742
196/196 - 58s - loss: 17.1292 - MinusLogProbMetric: 17.1292 - val_loss: 17.3192 - val_MinusLogProbMetric: 17.3192 - lr: 5.5556e-05 - 58s/epoch - 296ms/step
Epoch 841/1000
2023-10-24 04:24:18.718 
Epoch 841/1000 
	 loss: 17.1536, MinusLogProbMetric: 17.1536, val_loss: 17.1594, val_MinusLogProbMetric: 17.1594

Epoch 841: val_loss did not improve from 17.10742
196/196 - 60s - loss: 17.1536 - MinusLogProbMetric: 17.1536 - val_loss: 17.1594 - val_MinusLogProbMetric: 17.1594 - lr: 5.5556e-05 - 60s/epoch - 305ms/step
Epoch 842/1000
2023-10-24 04:25:20.587 
Epoch 842/1000 
	 loss: 17.1866, MinusLogProbMetric: 17.1866, val_loss: 17.2718, val_MinusLogProbMetric: 17.2718

Epoch 842: val_loss did not improve from 17.10742
196/196 - 62s - loss: 17.1866 - MinusLogProbMetric: 17.1866 - val_loss: 17.2718 - val_MinusLogProbMetric: 17.2718 - lr: 5.5556e-05 - 62s/epoch - 316ms/step
Epoch 843/1000
2023-10-24 04:26:25.162 
Epoch 843/1000 
	 loss: 17.1469, MinusLogProbMetric: 17.1469, val_loss: 17.1856, val_MinusLogProbMetric: 17.1856

Epoch 843: val_loss did not improve from 17.10742
196/196 - 65s - loss: 17.1469 - MinusLogProbMetric: 17.1469 - val_loss: 17.1856 - val_MinusLogProbMetric: 17.1856 - lr: 5.5556e-05 - 65s/epoch - 329ms/step
Epoch 844/1000
2023-10-24 04:27:32.967 
Epoch 844/1000 
	 loss: 17.1589, MinusLogProbMetric: 17.1589, val_loss: 17.2239, val_MinusLogProbMetric: 17.2239

Epoch 844: val_loss did not improve from 17.10742
196/196 - 68s - loss: 17.1589 - MinusLogProbMetric: 17.1589 - val_loss: 17.2239 - val_MinusLogProbMetric: 17.2239 - lr: 5.5556e-05 - 68s/epoch - 346ms/step
Epoch 845/1000
2023-10-24 04:28:35.656 
Epoch 845/1000 
	 loss: 17.1291, MinusLogProbMetric: 17.1291, val_loss: 17.1029, val_MinusLogProbMetric: 17.1029

Epoch 845: val_loss improved from 17.10742 to 17.10287, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 17.1291 - MinusLogProbMetric: 17.1291 - val_loss: 17.1029 - val_MinusLogProbMetric: 17.1029 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 846/1000
2023-10-24 04:29:38.963 
Epoch 846/1000 
	 loss: 17.1512, MinusLogProbMetric: 17.1512, val_loss: 17.1639, val_MinusLogProbMetric: 17.1639

Epoch 846: val_loss did not improve from 17.10287
196/196 - 62s - loss: 17.1512 - MinusLogProbMetric: 17.1512 - val_loss: 17.1639 - val_MinusLogProbMetric: 17.1639 - lr: 5.5556e-05 - 62s/epoch - 316ms/step
Epoch 847/1000
2023-10-24 04:30:46.787 
Epoch 847/1000 
	 loss: 17.1419, MinusLogProbMetric: 17.1419, val_loss: 17.2854, val_MinusLogProbMetric: 17.2854

Epoch 847: val_loss did not improve from 17.10287
196/196 - 68s - loss: 17.1419 - MinusLogProbMetric: 17.1419 - val_loss: 17.2854 - val_MinusLogProbMetric: 17.2854 - lr: 5.5556e-05 - 68s/epoch - 346ms/step
Epoch 848/1000
2023-10-24 04:31:46.420 
Epoch 848/1000 
	 loss: 17.1326, MinusLogProbMetric: 17.1326, val_loss: 17.1199, val_MinusLogProbMetric: 17.1199

Epoch 848: val_loss did not improve from 17.10287
196/196 - 60s - loss: 17.1326 - MinusLogProbMetric: 17.1326 - val_loss: 17.1199 - val_MinusLogProbMetric: 17.1199 - lr: 5.5556e-05 - 60s/epoch - 304ms/step
Epoch 849/1000
2023-10-24 04:32:50.637 
Epoch 849/1000 
	 loss: 17.1490, MinusLogProbMetric: 17.1490, val_loss: 17.2225, val_MinusLogProbMetric: 17.2225

Epoch 849: val_loss did not improve from 17.10287
196/196 - 64s - loss: 17.1490 - MinusLogProbMetric: 17.1490 - val_loss: 17.2225 - val_MinusLogProbMetric: 17.2225 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 850/1000
2023-10-24 04:33:51.240 
Epoch 850/1000 
	 loss: 17.1476, MinusLogProbMetric: 17.1476, val_loss: 17.1819, val_MinusLogProbMetric: 17.1819

Epoch 850: val_loss did not improve from 17.10287
196/196 - 61s - loss: 17.1476 - MinusLogProbMetric: 17.1476 - val_loss: 17.1819 - val_MinusLogProbMetric: 17.1819 - lr: 5.5556e-05 - 61s/epoch - 309ms/step
Epoch 851/1000
2023-10-24 04:34:55.078 
Epoch 851/1000 
	 loss: 17.1298, MinusLogProbMetric: 17.1298, val_loss: 17.1912, val_MinusLogProbMetric: 17.1912

Epoch 851: val_loss did not improve from 17.10287
196/196 - 64s - loss: 17.1298 - MinusLogProbMetric: 17.1298 - val_loss: 17.1912 - val_MinusLogProbMetric: 17.1912 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 852/1000
2023-10-24 04:35:58.343 
Epoch 852/1000 
	 loss: 17.1322, MinusLogProbMetric: 17.1322, val_loss: 17.1644, val_MinusLogProbMetric: 17.1644

Epoch 852: val_loss did not improve from 17.10287
196/196 - 63s - loss: 17.1322 - MinusLogProbMetric: 17.1322 - val_loss: 17.1644 - val_MinusLogProbMetric: 17.1644 - lr: 5.5556e-05 - 63s/epoch - 323ms/step
Epoch 853/1000
2023-10-24 04:36:57.605 
Epoch 853/1000 
	 loss: 17.1457, MinusLogProbMetric: 17.1457, val_loss: 17.1898, val_MinusLogProbMetric: 17.1898

Epoch 853: val_loss did not improve from 17.10287
196/196 - 59s - loss: 17.1457 - MinusLogProbMetric: 17.1457 - val_loss: 17.1898 - val_MinusLogProbMetric: 17.1898 - lr: 5.5556e-05 - 59s/epoch - 302ms/step
Epoch 854/1000
2023-10-24 04:38:01.328 
Epoch 854/1000 
	 loss: 17.1619, MinusLogProbMetric: 17.1619, val_loss: 17.2832, val_MinusLogProbMetric: 17.2832

Epoch 854: val_loss did not improve from 17.10287
196/196 - 64s - loss: 17.1619 - MinusLogProbMetric: 17.1619 - val_loss: 17.2832 - val_MinusLogProbMetric: 17.2832 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 855/1000
2023-10-24 04:39:03.846 
Epoch 855/1000 
	 loss: 17.1578, MinusLogProbMetric: 17.1578, val_loss: 17.2156, val_MinusLogProbMetric: 17.2156

Epoch 855: val_loss did not improve from 17.10287
196/196 - 63s - loss: 17.1578 - MinusLogProbMetric: 17.1578 - val_loss: 17.2156 - val_MinusLogProbMetric: 17.2156 - lr: 5.5556e-05 - 63s/epoch - 319ms/step
Epoch 856/1000
2023-10-24 04:40:04.073 
Epoch 856/1000 
	 loss: 17.1347, MinusLogProbMetric: 17.1347, val_loss: 17.1596, val_MinusLogProbMetric: 17.1596

Epoch 856: val_loss did not improve from 17.10287
196/196 - 60s - loss: 17.1347 - MinusLogProbMetric: 17.1347 - val_loss: 17.1596 - val_MinusLogProbMetric: 17.1596 - lr: 5.5556e-05 - 60s/epoch - 307ms/step
Epoch 857/1000
2023-10-24 04:41:05.158 
Epoch 857/1000 
	 loss: 17.1505, MinusLogProbMetric: 17.1505, val_loss: 17.1577, val_MinusLogProbMetric: 17.1577

Epoch 857: val_loss did not improve from 17.10287
196/196 - 61s - loss: 17.1505 - MinusLogProbMetric: 17.1505 - val_loss: 17.1577 - val_MinusLogProbMetric: 17.1577 - lr: 5.5556e-05 - 61s/epoch - 312ms/step
Epoch 858/1000
2023-10-24 04:42:08.564 
Epoch 858/1000 
	 loss: 17.1626, MinusLogProbMetric: 17.1626, val_loss: 17.3066, val_MinusLogProbMetric: 17.3066

Epoch 858: val_loss did not improve from 17.10287
196/196 - 63s - loss: 17.1626 - MinusLogProbMetric: 17.1626 - val_loss: 17.3066 - val_MinusLogProbMetric: 17.3066 - lr: 5.5556e-05 - 63s/epoch - 324ms/step
Epoch 859/1000
2023-10-24 04:43:15.556 
Epoch 859/1000 
	 loss: 17.1399, MinusLogProbMetric: 17.1399, val_loss: 17.2529, val_MinusLogProbMetric: 17.2529

Epoch 859: val_loss did not improve from 17.10287
196/196 - 67s - loss: 17.1399 - MinusLogProbMetric: 17.1399 - val_loss: 17.2529 - val_MinusLogProbMetric: 17.2529 - lr: 5.5556e-05 - 67s/epoch - 342ms/step
Epoch 860/1000
2023-10-24 04:44:18.732 
Epoch 860/1000 
	 loss: 17.1447, MinusLogProbMetric: 17.1447, val_loss: 17.2244, val_MinusLogProbMetric: 17.2244

Epoch 860: val_loss did not improve from 17.10287
196/196 - 63s - loss: 17.1447 - MinusLogProbMetric: 17.1447 - val_loss: 17.2244 - val_MinusLogProbMetric: 17.2244 - lr: 5.5556e-05 - 63s/epoch - 322ms/step
Epoch 861/1000
2023-10-24 04:45:22.300 
Epoch 861/1000 
	 loss: 17.1540, MinusLogProbMetric: 17.1540, val_loss: 17.1179, val_MinusLogProbMetric: 17.1179

Epoch 861: val_loss did not improve from 17.10287
196/196 - 64s - loss: 17.1540 - MinusLogProbMetric: 17.1540 - val_loss: 17.1179 - val_MinusLogProbMetric: 17.1179 - lr: 5.5556e-05 - 64s/epoch - 324ms/step
Epoch 862/1000
2023-10-24 04:46:25.397 
Epoch 862/1000 
	 loss: 17.1303, MinusLogProbMetric: 17.1303, val_loss: 17.1657, val_MinusLogProbMetric: 17.1657

Epoch 862: val_loss did not improve from 17.10287
196/196 - 63s - loss: 17.1303 - MinusLogProbMetric: 17.1303 - val_loss: 17.1657 - val_MinusLogProbMetric: 17.1657 - lr: 5.5556e-05 - 63s/epoch - 322ms/step
Epoch 863/1000
2023-10-24 04:47:30.472 
Epoch 863/1000 
	 loss: 17.1698, MinusLogProbMetric: 17.1698, val_loss: 17.3262, val_MinusLogProbMetric: 17.3262

Epoch 863: val_loss did not improve from 17.10287
196/196 - 65s - loss: 17.1698 - MinusLogProbMetric: 17.1698 - val_loss: 17.3262 - val_MinusLogProbMetric: 17.3262 - lr: 5.5556e-05 - 65s/epoch - 332ms/step
Epoch 864/1000
2023-10-24 04:48:36.734 
Epoch 864/1000 
	 loss: 17.1522, MinusLogProbMetric: 17.1522, val_loss: 17.1907, val_MinusLogProbMetric: 17.1907

Epoch 864: val_loss did not improve from 17.10287
196/196 - 66s - loss: 17.1522 - MinusLogProbMetric: 17.1522 - val_loss: 17.1907 - val_MinusLogProbMetric: 17.1907 - lr: 5.5556e-05 - 66s/epoch - 338ms/step
Epoch 865/1000
2023-10-24 04:49:37.889 
Epoch 865/1000 
	 loss: 17.1279, MinusLogProbMetric: 17.1279, val_loss: 17.1260, val_MinusLogProbMetric: 17.1260

Epoch 865: val_loss did not improve from 17.10287
196/196 - 61s - loss: 17.1279 - MinusLogProbMetric: 17.1279 - val_loss: 17.1260 - val_MinusLogProbMetric: 17.1260 - lr: 5.5556e-05 - 61s/epoch - 312ms/step
Epoch 866/1000
2023-10-24 04:50:39.414 
Epoch 866/1000 
	 loss: 17.1193, MinusLogProbMetric: 17.1193, val_loss: 17.2066, val_MinusLogProbMetric: 17.2066

Epoch 866: val_loss did not improve from 17.10287
196/196 - 62s - loss: 17.1193 - MinusLogProbMetric: 17.1193 - val_loss: 17.2066 - val_MinusLogProbMetric: 17.2066 - lr: 5.5556e-05 - 62s/epoch - 314ms/step
Epoch 867/1000
2023-10-24 04:51:42.265 
Epoch 867/1000 
	 loss: 17.1311, MinusLogProbMetric: 17.1311, val_loss: 17.1999, val_MinusLogProbMetric: 17.1999

Epoch 867: val_loss did not improve from 17.10287
196/196 - 63s - loss: 17.1311 - MinusLogProbMetric: 17.1311 - val_loss: 17.1999 - val_MinusLogProbMetric: 17.1999 - lr: 5.5556e-05 - 63s/epoch - 321ms/step
Epoch 868/1000
2023-10-24 04:52:45.526 
Epoch 868/1000 
	 loss: 17.1237, MinusLogProbMetric: 17.1237, val_loss: 17.2012, val_MinusLogProbMetric: 17.2012

Epoch 868: val_loss did not improve from 17.10287
196/196 - 63s - loss: 17.1237 - MinusLogProbMetric: 17.1237 - val_loss: 17.2012 - val_MinusLogProbMetric: 17.2012 - lr: 5.5556e-05 - 63s/epoch - 323ms/step
Epoch 869/1000
2023-10-24 04:53:45.658 
Epoch 869/1000 
	 loss: 17.1182, MinusLogProbMetric: 17.1182, val_loss: 17.2299, val_MinusLogProbMetric: 17.2299

Epoch 869: val_loss did not improve from 17.10287
196/196 - 60s - loss: 17.1182 - MinusLogProbMetric: 17.1182 - val_loss: 17.2299 - val_MinusLogProbMetric: 17.2299 - lr: 5.5556e-05 - 60s/epoch - 307ms/step
Epoch 870/1000
2023-10-24 04:54:46.314 
Epoch 870/1000 
	 loss: 17.1352, MinusLogProbMetric: 17.1352, val_loss: 17.1426, val_MinusLogProbMetric: 17.1426

Epoch 870: val_loss did not improve from 17.10287
196/196 - 61s - loss: 17.1352 - MinusLogProbMetric: 17.1352 - val_loss: 17.1426 - val_MinusLogProbMetric: 17.1426 - lr: 5.5556e-05 - 61s/epoch - 309ms/step
Epoch 871/1000
2023-10-24 04:55:48.402 
Epoch 871/1000 
	 loss: 17.1307, MinusLogProbMetric: 17.1307, val_loss: 17.2613, val_MinusLogProbMetric: 17.2613

Epoch 871: val_loss did not improve from 17.10287
196/196 - 62s - loss: 17.1307 - MinusLogProbMetric: 17.1307 - val_loss: 17.2613 - val_MinusLogProbMetric: 17.2613 - lr: 5.5556e-05 - 62s/epoch - 317ms/step
Epoch 872/1000
2023-10-24 04:56:47.594 
Epoch 872/1000 
	 loss: 17.1417, MinusLogProbMetric: 17.1417, val_loss: 17.1970, val_MinusLogProbMetric: 17.1970

Epoch 872: val_loss did not improve from 17.10287
196/196 - 59s - loss: 17.1417 - MinusLogProbMetric: 17.1417 - val_loss: 17.1970 - val_MinusLogProbMetric: 17.1970 - lr: 5.5556e-05 - 59s/epoch - 302ms/step
Epoch 873/1000
2023-10-24 04:57:48.234 
Epoch 873/1000 
	 loss: 17.1510, MinusLogProbMetric: 17.1510, val_loss: 17.3109, val_MinusLogProbMetric: 17.3109

Epoch 873: val_loss did not improve from 17.10287
196/196 - 61s - loss: 17.1510 - MinusLogProbMetric: 17.1510 - val_loss: 17.3109 - val_MinusLogProbMetric: 17.3109 - lr: 5.5556e-05 - 61s/epoch - 309ms/step
Epoch 874/1000
2023-10-24 04:58:46.866 
Epoch 874/1000 
	 loss: 17.1503, MinusLogProbMetric: 17.1503, val_loss: 17.2456, val_MinusLogProbMetric: 17.2456

Epoch 874: val_loss did not improve from 17.10287
196/196 - 59s - loss: 17.1503 - MinusLogProbMetric: 17.1503 - val_loss: 17.2456 - val_MinusLogProbMetric: 17.2456 - lr: 5.5556e-05 - 59s/epoch - 299ms/step
Epoch 875/1000
2023-10-24 04:59:51.728 
Epoch 875/1000 
	 loss: 17.1235, MinusLogProbMetric: 17.1235, val_loss: 17.1637, val_MinusLogProbMetric: 17.1637

Epoch 875: val_loss did not improve from 17.10287
196/196 - 65s - loss: 17.1235 - MinusLogProbMetric: 17.1235 - val_loss: 17.1637 - val_MinusLogProbMetric: 17.1637 - lr: 5.5556e-05 - 65s/epoch - 331ms/step
Epoch 876/1000
2023-10-24 05:00:52.183 
Epoch 876/1000 
	 loss: 17.1437, MinusLogProbMetric: 17.1437, val_loss: 17.2192, val_MinusLogProbMetric: 17.2192

Epoch 876: val_loss did not improve from 17.10287
196/196 - 60s - loss: 17.1437 - MinusLogProbMetric: 17.1437 - val_loss: 17.2192 - val_MinusLogProbMetric: 17.2192 - lr: 5.5556e-05 - 60s/epoch - 308ms/step
Epoch 877/1000
2023-10-24 05:01:52.668 
Epoch 877/1000 
	 loss: 17.1099, MinusLogProbMetric: 17.1099, val_loss: 17.1750, val_MinusLogProbMetric: 17.1750

Epoch 877: val_loss did not improve from 17.10287
196/196 - 60s - loss: 17.1099 - MinusLogProbMetric: 17.1099 - val_loss: 17.1750 - val_MinusLogProbMetric: 17.1750 - lr: 5.5556e-05 - 60s/epoch - 309ms/step
Epoch 878/1000
2023-10-24 05:02:52.577 
Epoch 878/1000 
	 loss: 17.1334, MinusLogProbMetric: 17.1334, val_loss: 17.2624, val_MinusLogProbMetric: 17.2624

Epoch 878: val_loss did not improve from 17.10287
196/196 - 60s - loss: 17.1334 - MinusLogProbMetric: 17.1334 - val_loss: 17.2624 - val_MinusLogProbMetric: 17.2624 - lr: 5.5556e-05 - 60s/epoch - 306ms/step
Epoch 879/1000
2023-10-24 05:03:55.730 
Epoch 879/1000 
	 loss: 17.1509, MinusLogProbMetric: 17.1509, val_loss: 17.1613, val_MinusLogProbMetric: 17.1613

Epoch 879: val_loss did not improve from 17.10287
196/196 - 63s - loss: 17.1509 - MinusLogProbMetric: 17.1509 - val_loss: 17.1613 - val_MinusLogProbMetric: 17.1613 - lr: 5.5556e-05 - 63s/epoch - 322ms/step
Epoch 880/1000
2023-10-24 05:04:56.656 
Epoch 880/1000 
	 loss: 17.1412, MinusLogProbMetric: 17.1412, val_loss: 17.2980, val_MinusLogProbMetric: 17.2980

Epoch 880: val_loss did not improve from 17.10287
196/196 - 61s - loss: 17.1412 - MinusLogProbMetric: 17.1412 - val_loss: 17.2980 - val_MinusLogProbMetric: 17.2980 - lr: 5.5556e-05 - 61s/epoch - 311ms/step
Epoch 881/1000
2023-10-24 05:06:00.057 
Epoch 881/1000 
	 loss: 17.1482, MinusLogProbMetric: 17.1482, val_loss: 17.3376, val_MinusLogProbMetric: 17.3376

Epoch 881: val_loss did not improve from 17.10287
196/196 - 63s - loss: 17.1482 - MinusLogProbMetric: 17.1482 - val_loss: 17.3376 - val_MinusLogProbMetric: 17.3376 - lr: 5.5556e-05 - 63s/epoch - 323ms/step
Epoch 882/1000
2023-10-24 05:06:58.059 
Epoch 882/1000 
	 loss: 17.1322, MinusLogProbMetric: 17.1322, val_loss: 17.1918, val_MinusLogProbMetric: 17.1918

Epoch 882: val_loss did not improve from 17.10287
196/196 - 58s - loss: 17.1322 - MinusLogProbMetric: 17.1322 - val_loss: 17.1918 - val_MinusLogProbMetric: 17.1918 - lr: 5.5556e-05 - 58s/epoch - 296ms/step
Epoch 883/1000
2023-10-24 05:07:57.697 
Epoch 883/1000 
	 loss: 17.1314, MinusLogProbMetric: 17.1314, val_loss: 17.2574, val_MinusLogProbMetric: 17.2574

Epoch 883: val_loss did not improve from 17.10287
196/196 - 60s - loss: 17.1314 - MinusLogProbMetric: 17.1314 - val_loss: 17.2574 - val_MinusLogProbMetric: 17.2574 - lr: 5.5556e-05 - 60s/epoch - 304ms/step
Epoch 884/1000
2023-10-24 05:09:03.331 
Epoch 884/1000 
	 loss: 17.1402, MinusLogProbMetric: 17.1402, val_loss: 17.2276, val_MinusLogProbMetric: 17.2276

Epoch 884: val_loss did not improve from 17.10287
196/196 - 66s - loss: 17.1402 - MinusLogProbMetric: 17.1402 - val_loss: 17.2276 - val_MinusLogProbMetric: 17.2276 - lr: 5.5556e-05 - 66s/epoch - 335ms/step
Epoch 885/1000
2023-10-24 05:10:03.382 
Epoch 885/1000 
	 loss: 17.1289, MinusLogProbMetric: 17.1289, val_loss: 17.1905, val_MinusLogProbMetric: 17.1905

Epoch 885: val_loss did not improve from 17.10287
196/196 - 60s - loss: 17.1289 - MinusLogProbMetric: 17.1289 - val_loss: 17.1905 - val_MinusLogProbMetric: 17.1905 - lr: 5.5556e-05 - 60s/epoch - 306ms/step
Epoch 886/1000
2023-10-24 05:11:07.225 
Epoch 886/1000 
	 loss: 17.1529, MinusLogProbMetric: 17.1529, val_loss: 17.2258, val_MinusLogProbMetric: 17.2258

Epoch 886: val_loss did not improve from 17.10287
196/196 - 64s - loss: 17.1529 - MinusLogProbMetric: 17.1529 - val_loss: 17.2258 - val_MinusLogProbMetric: 17.2258 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 887/1000
2023-10-24 05:12:16.125 
Epoch 887/1000 
	 loss: 17.1464, MinusLogProbMetric: 17.1464, val_loss: 17.2769, val_MinusLogProbMetric: 17.2769

Epoch 887: val_loss did not improve from 17.10287
196/196 - 69s - loss: 17.1464 - MinusLogProbMetric: 17.1464 - val_loss: 17.2769 - val_MinusLogProbMetric: 17.2769 - lr: 5.5556e-05 - 69s/epoch - 352ms/step
Epoch 888/1000
2023-10-24 05:13:17.144 
Epoch 888/1000 
	 loss: 17.1289, MinusLogProbMetric: 17.1289, val_loss: 17.2185, val_MinusLogProbMetric: 17.2185

Epoch 888: val_loss did not improve from 17.10287
196/196 - 61s - loss: 17.1289 - MinusLogProbMetric: 17.1289 - val_loss: 17.2185 - val_MinusLogProbMetric: 17.2185 - lr: 5.5556e-05 - 61s/epoch - 311ms/step
Epoch 889/1000
2023-10-24 05:14:16.924 
Epoch 889/1000 
	 loss: 17.1203, MinusLogProbMetric: 17.1203, val_loss: 17.1633, val_MinusLogProbMetric: 17.1633

Epoch 889: val_loss did not improve from 17.10287
196/196 - 60s - loss: 17.1203 - MinusLogProbMetric: 17.1203 - val_loss: 17.1633 - val_MinusLogProbMetric: 17.1633 - lr: 5.5556e-05 - 60s/epoch - 305ms/step
Epoch 890/1000
2023-10-24 05:15:20.678 
Epoch 890/1000 
	 loss: 17.1268, MinusLogProbMetric: 17.1268, val_loss: 17.2332, val_MinusLogProbMetric: 17.2332

Epoch 890: val_loss did not improve from 17.10287
196/196 - 64s - loss: 17.1268 - MinusLogProbMetric: 17.1268 - val_loss: 17.2332 - val_MinusLogProbMetric: 17.2332 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 891/1000
2023-10-24 05:16:21.342 
Epoch 891/1000 
	 loss: 17.1325, MinusLogProbMetric: 17.1325, val_loss: 17.1903, val_MinusLogProbMetric: 17.1903

Epoch 891: val_loss did not improve from 17.10287
196/196 - 61s - loss: 17.1325 - MinusLogProbMetric: 17.1325 - val_loss: 17.1903 - val_MinusLogProbMetric: 17.1903 - lr: 5.5556e-05 - 61s/epoch - 309ms/step
Epoch 892/1000
2023-10-24 05:17:21.990 
Epoch 892/1000 
	 loss: 17.1292, MinusLogProbMetric: 17.1292, val_loss: 17.1455, val_MinusLogProbMetric: 17.1455

Epoch 892: val_loss did not improve from 17.10287
196/196 - 61s - loss: 17.1292 - MinusLogProbMetric: 17.1292 - val_loss: 17.1455 - val_MinusLogProbMetric: 17.1455 - lr: 5.5556e-05 - 61s/epoch - 309ms/step
Epoch 893/1000
2023-10-24 05:18:20.598 
Epoch 893/1000 
	 loss: 17.1543, MinusLogProbMetric: 17.1543, val_loss: 17.1759, val_MinusLogProbMetric: 17.1759

Epoch 893: val_loss did not improve from 17.10287
196/196 - 59s - loss: 17.1543 - MinusLogProbMetric: 17.1543 - val_loss: 17.1759 - val_MinusLogProbMetric: 17.1759 - lr: 5.5556e-05 - 59s/epoch - 299ms/step
Epoch 894/1000
2023-10-24 05:19:20.387 
Epoch 894/1000 
	 loss: 17.1376, MinusLogProbMetric: 17.1376, val_loss: 17.2924, val_MinusLogProbMetric: 17.2924

Epoch 894: val_loss did not improve from 17.10287
196/196 - 60s - loss: 17.1376 - MinusLogProbMetric: 17.1376 - val_loss: 17.2924 - val_MinusLogProbMetric: 17.2924 - lr: 5.5556e-05 - 60s/epoch - 305ms/step
Epoch 895/1000
2023-10-24 05:20:18.275 
Epoch 895/1000 
	 loss: 17.1348, MinusLogProbMetric: 17.1348, val_loss: 17.1329, val_MinusLogProbMetric: 17.1329

Epoch 895: val_loss did not improve from 17.10287
196/196 - 58s - loss: 17.1348 - MinusLogProbMetric: 17.1348 - val_loss: 17.1329 - val_MinusLogProbMetric: 17.1329 - lr: 5.5556e-05 - 58s/epoch - 295ms/step
Epoch 896/1000
2023-10-24 05:21:17.201 
Epoch 896/1000 
	 loss: 17.0368, MinusLogProbMetric: 17.0368, val_loss: 17.0786, val_MinusLogProbMetric: 17.0786

Epoch 896: val_loss improved from 17.10287 to 17.07856, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 60s - loss: 17.0368 - MinusLogProbMetric: 17.0368 - val_loss: 17.0786 - val_MinusLogProbMetric: 17.0786 - lr: 2.7778e-05 - 60s/epoch - 306ms/step
Epoch 897/1000
2023-10-24 05:22:17.855 
Epoch 897/1000 
	 loss: 17.0334, MinusLogProbMetric: 17.0334, val_loss: 17.0735, val_MinusLogProbMetric: 17.0735

Epoch 897: val_loss improved from 17.07856 to 17.07348, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 61s - loss: 17.0334 - MinusLogProbMetric: 17.0334 - val_loss: 17.0735 - val_MinusLogProbMetric: 17.0735 - lr: 2.7778e-05 - 61s/epoch - 310ms/step
Epoch 898/1000
2023-10-24 05:23:23.371 
Epoch 898/1000 
	 loss: 17.0400, MinusLogProbMetric: 17.0400, val_loss: 17.0936, val_MinusLogProbMetric: 17.0936

Epoch 898: val_loss did not improve from 17.07348
196/196 - 64s - loss: 17.0400 - MinusLogProbMetric: 17.0400 - val_loss: 17.0936 - val_MinusLogProbMetric: 17.0936 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 899/1000
2023-10-24 05:24:22.919 
Epoch 899/1000 
	 loss: 17.0421, MinusLogProbMetric: 17.0421, val_loss: 17.1046, val_MinusLogProbMetric: 17.1046

Epoch 899: val_loss did not improve from 17.07348
196/196 - 60s - loss: 17.0421 - MinusLogProbMetric: 17.0421 - val_loss: 17.1046 - val_MinusLogProbMetric: 17.1046 - lr: 2.7778e-05 - 60s/epoch - 304ms/step
Epoch 900/1000
2023-10-24 05:25:22.727 
Epoch 900/1000 
	 loss: 17.0298, MinusLogProbMetric: 17.0298, val_loss: 17.0742, val_MinusLogProbMetric: 17.0742

Epoch 900: val_loss did not improve from 17.07348
196/196 - 60s - loss: 17.0298 - MinusLogProbMetric: 17.0298 - val_loss: 17.0742 - val_MinusLogProbMetric: 17.0742 - lr: 2.7778e-05 - 60s/epoch - 305ms/step
Epoch 901/1000
2023-10-24 05:26:27.016 
Epoch 901/1000 
	 loss: 17.0240, MinusLogProbMetric: 17.0240, val_loss: 17.0954, val_MinusLogProbMetric: 17.0954

Epoch 901: val_loss did not improve from 17.07348
196/196 - 64s - loss: 17.0240 - MinusLogProbMetric: 17.0240 - val_loss: 17.0954 - val_MinusLogProbMetric: 17.0954 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 902/1000
2023-10-24 05:27:26.758 
Epoch 902/1000 
	 loss: 17.0325, MinusLogProbMetric: 17.0325, val_loss: 17.0707, val_MinusLogProbMetric: 17.0707

Epoch 902: val_loss improved from 17.07348 to 17.07070, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 61s - loss: 17.0325 - MinusLogProbMetric: 17.0325 - val_loss: 17.0707 - val_MinusLogProbMetric: 17.0707 - lr: 2.7778e-05 - 61s/epoch - 311ms/step
Epoch 903/1000
2023-10-24 05:28:26.587 
Epoch 903/1000 
	 loss: 17.0356, MinusLogProbMetric: 17.0356, val_loss: 17.0640, val_MinusLogProbMetric: 17.0640

Epoch 903: val_loss improved from 17.07070 to 17.06404, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 60s - loss: 17.0356 - MinusLogProbMetric: 17.0356 - val_loss: 17.0640 - val_MinusLogProbMetric: 17.0640 - lr: 2.7778e-05 - 60s/epoch - 305ms/step
Epoch 904/1000
2023-10-24 05:29:29.682 
Epoch 904/1000 
	 loss: 17.0364, MinusLogProbMetric: 17.0364, val_loss: 17.0722, val_MinusLogProbMetric: 17.0722

Epoch 904: val_loss did not improve from 17.06404
196/196 - 62s - loss: 17.0364 - MinusLogProbMetric: 17.0364 - val_loss: 17.0722 - val_MinusLogProbMetric: 17.0722 - lr: 2.7778e-05 - 62s/epoch - 316ms/step
Epoch 905/1000
2023-10-24 05:30:30.730 
Epoch 905/1000 
	 loss: 17.0378, MinusLogProbMetric: 17.0378, val_loss: 17.0573, val_MinusLogProbMetric: 17.0573

Epoch 905: val_loss improved from 17.06404 to 17.05735, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 62s - loss: 17.0378 - MinusLogProbMetric: 17.0378 - val_loss: 17.0573 - val_MinusLogProbMetric: 17.0573 - lr: 2.7778e-05 - 62s/epoch - 318ms/step
Epoch 906/1000
2023-10-24 05:31:31.906 
Epoch 906/1000 
	 loss: 17.0278, MinusLogProbMetric: 17.0278, val_loss: 17.0605, val_MinusLogProbMetric: 17.0605

Epoch 906: val_loss did not improve from 17.05735
196/196 - 60s - loss: 17.0278 - MinusLogProbMetric: 17.0278 - val_loss: 17.0605 - val_MinusLogProbMetric: 17.0605 - lr: 2.7778e-05 - 60s/epoch - 305ms/step
Epoch 907/1000
2023-10-24 05:32:31.481 
Epoch 907/1000 
	 loss: 17.0332, MinusLogProbMetric: 17.0332, val_loss: 17.0851, val_MinusLogProbMetric: 17.0851

Epoch 907: val_loss did not improve from 17.05735
196/196 - 60s - loss: 17.0332 - MinusLogProbMetric: 17.0332 - val_loss: 17.0851 - val_MinusLogProbMetric: 17.0851 - lr: 2.7778e-05 - 60s/epoch - 304ms/step
Epoch 908/1000
2023-10-24 05:33:31.580 
Epoch 908/1000 
	 loss: 17.0336, MinusLogProbMetric: 17.0336, val_loss: 17.0691, val_MinusLogProbMetric: 17.0691

Epoch 908: val_loss did not improve from 17.05735
196/196 - 60s - loss: 17.0336 - MinusLogProbMetric: 17.0336 - val_loss: 17.0691 - val_MinusLogProbMetric: 17.0691 - lr: 2.7778e-05 - 60s/epoch - 307ms/step
Epoch 909/1000
2023-10-24 05:34:38.586 
Epoch 909/1000 
	 loss: 17.0262, MinusLogProbMetric: 17.0262, val_loss: 17.0924, val_MinusLogProbMetric: 17.0924

Epoch 909: val_loss did not improve from 17.05735
196/196 - 67s - loss: 17.0262 - MinusLogProbMetric: 17.0262 - val_loss: 17.0924 - val_MinusLogProbMetric: 17.0924 - lr: 2.7778e-05 - 67s/epoch - 342ms/step
Epoch 910/1000
2023-10-24 05:35:39.134 
Epoch 910/1000 
	 loss: 17.0368, MinusLogProbMetric: 17.0368, val_loss: 17.0896, val_MinusLogProbMetric: 17.0896

Epoch 910: val_loss did not improve from 17.05735
196/196 - 61s - loss: 17.0368 - MinusLogProbMetric: 17.0368 - val_loss: 17.0896 - val_MinusLogProbMetric: 17.0896 - lr: 2.7778e-05 - 61s/epoch - 309ms/step
Epoch 911/1000
2023-10-24 05:36:41.655 
Epoch 911/1000 
	 loss: 17.0372, MinusLogProbMetric: 17.0372, val_loss: 17.0998, val_MinusLogProbMetric: 17.0998

Epoch 911: val_loss did not improve from 17.05735
196/196 - 63s - loss: 17.0372 - MinusLogProbMetric: 17.0372 - val_loss: 17.0998 - val_MinusLogProbMetric: 17.0998 - lr: 2.7778e-05 - 63s/epoch - 319ms/step
Epoch 912/1000
2023-10-24 05:37:40.892 
Epoch 912/1000 
	 loss: 17.0444, MinusLogProbMetric: 17.0444, val_loss: 17.1569, val_MinusLogProbMetric: 17.1569

Epoch 912: val_loss did not improve from 17.05735
196/196 - 59s - loss: 17.0444 - MinusLogProbMetric: 17.0444 - val_loss: 17.1569 - val_MinusLogProbMetric: 17.1569 - lr: 2.7778e-05 - 59s/epoch - 302ms/step
Epoch 913/1000
2023-10-24 05:38:40.653 
Epoch 913/1000 
	 loss: 17.0314, MinusLogProbMetric: 17.0314, val_loss: 17.0867, val_MinusLogProbMetric: 17.0867

Epoch 913: val_loss did not improve from 17.05735
196/196 - 60s - loss: 17.0314 - MinusLogProbMetric: 17.0314 - val_loss: 17.0867 - val_MinusLogProbMetric: 17.0867 - lr: 2.7778e-05 - 60s/epoch - 305ms/step
Epoch 914/1000
2023-10-24 05:39:45.086 
Epoch 914/1000 
	 loss: 17.0299, MinusLogProbMetric: 17.0299, val_loss: 17.0762, val_MinusLogProbMetric: 17.0762

Epoch 914: val_loss did not improve from 17.05735
196/196 - 64s - loss: 17.0299 - MinusLogProbMetric: 17.0299 - val_loss: 17.0762 - val_MinusLogProbMetric: 17.0762 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 915/1000
2023-10-24 05:40:49.049 
Epoch 915/1000 
	 loss: 17.0400, MinusLogProbMetric: 17.0400, val_loss: 17.0696, val_MinusLogProbMetric: 17.0696

Epoch 915: val_loss did not improve from 17.05735
196/196 - 64s - loss: 17.0400 - MinusLogProbMetric: 17.0400 - val_loss: 17.0696 - val_MinusLogProbMetric: 17.0696 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 916/1000
2023-10-24 05:41:48.855 
Epoch 916/1000 
	 loss: 17.0338, MinusLogProbMetric: 17.0338, val_loss: 17.0636, val_MinusLogProbMetric: 17.0636

Epoch 916: val_loss did not improve from 17.05735
196/196 - 60s - loss: 17.0338 - MinusLogProbMetric: 17.0338 - val_loss: 17.0636 - val_MinusLogProbMetric: 17.0636 - lr: 2.7778e-05 - 60s/epoch - 305ms/step
Epoch 917/1000
2023-10-24 05:42:49.354 
Epoch 917/1000 
	 loss: 17.0260, MinusLogProbMetric: 17.0260, val_loss: 17.0551, val_MinusLogProbMetric: 17.0551

Epoch 917: val_loss improved from 17.05735 to 17.05514, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 62s - loss: 17.0260 - MinusLogProbMetric: 17.0260 - val_loss: 17.0551 - val_MinusLogProbMetric: 17.0551 - lr: 2.7778e-05 - 62s/epoch - 315ms/step
Epoch 918/1000
2023-10-24 05:43:47.894 
Epoch 918/1000 
	 loss: 17.0292, MinusLogProbMetric: 17.0292, val_loss: 17.0779, val_MinusLogProbMetric: 17.0779

Epoch 918: val_loss did not improve from 17.05514
196/196 - 57s - loss: 17.0292 - MinusLogProbMetric: 17.0292 - val_loss: 17.0779 - val_MinusLogProbMetric: 17.0779 - lr: 2.7778e-05 - 57s/epoch - 293ms/step
Epoch 919/1000
2023-10-24 05:44:49.250 
Epoch 919/1000 
	 loss: 17.0356, MinusLogProbMetric: 17.0356, val_loss: 17.1022, val_MinusLogProbMetric: 17.1022

Epoch 919: val_loss did not improve from 17.05514
196/196 - 61s - loss: 17.0356 - MinusLogProbMetric: 17.0356 - val_loss: 17.1022 - val_MinusLogProbMetric: 17.1022 - lr: 2.7778e-05 - 61s/epoch - 313ms/step
Epoch 920/1000
2023-10-24 05:45:47.561 
Epoch 920/1000 
	 loss: 17.0333, MinusLogProbMetric: 17.0333, val_loss: 17.0739, val_MinusLogProbMetric: 17.0739

Epoch 920: val_loss did not improve from 17.05514
196/196 - 58s - loss: 17.0333 - MinusLogProbMetric: 17.0333 - val_loss: 17.0739 - val_MinusLogProbMetric: 17.0739 - lr: 2.7778e-05 - 58s/epoch - 297ms/step
Epoch 921/1000
2023-10-24 05:46:47.853 
Epoch 921/1000 
	 loss: 17.0280, MinusLogProbMetric: 17.0280, val_loss: 17.0890, val_MinusLogProbMetric: 17.0890

Epoch 921: val_loss did not improve from 17.05514
196/196 - 60s - loss: 17.0280 - MinusLogProbMetric: 17.0280 - val_loss: 17.0890 - val_MinusLogProbMetric: 17.0890 - lr: 2.7778e-05 - 60s/epoch - 308ms/step
Epoch 922/1000
2023-10-24 05:47:48.683 
Epoch 922/1000 
	 loss: 17.0231, MinusLogProbMetric: 17.0231, val_loss: 17.0692, val_MinusLogProbMetric: 17.0692

Epoch 922: val_loss did not improve from 17.05514
196/196 - 61s - loss: 17.0231 - MinusLogProbMetric: 17.0231 - val_loss: 17.0692 - val_MinusLogProbMetric: 17.0692 - lr: 2.7778e-05 - 61s/epoch - 310ms/step
Epoch 923/1000
2023-10-24 05:48:48.031 
Epoch 923/1000 
	 loss: 17.0387, MinusLogProbMetric: 17.0387, val_loss: 17.1568, val_MinusLogProbMetric: 17.1568

Epoch 923: val_loss did not improve from 17.05514
196/196 - 59s - loss: 17.0387 - MinusLogProbMetric: 17.0387 - val_loss: 17.1568 - val_MinusLogProbMetric: 17.1568 - lr: 2.7778e-05 - 59s/epoch - 303ms/step
Epoch 924/1000
2023-10-24 05:49:46.921 
Epoch 924/1000 
	 loss: 17.0249, MinusLogProbMetric: 17.0249, val_loss: 17.0961, val_MinusLogProbMetric: 17.0961

Epoch 924: val_loss did not improve from 17.05514
196/196 - 59s - loss: 17.0249 - MinusLogProbMetric: 17.0249 - val_loss: 17.0961 - val_MinusLogProbMetric: 17.0961 - lr: 2.7778e-05 - 59s/epoch - 300ms/step
Epoch 925/1000
2023-10-24 05:50:49.497 
Epoch 925/1000 
	 loss: 17.0488, MinusLogProbMetric: 17.0488, val_loss: 17.0976, val_MinusLogProbMetric: 17.0976

Epoch 925: val_loss did not improve from 17.05514
196/196 - 63s - loss: 17.0488 - MinusLogProbMetric: 17.0488 - val_loss: 17.0976 - val_MinusLogProbMetric: 17.0976 - lr: 2.7778e-05 - 63s/epoch - 319ms/step
Epoch 926/1000
2023-10-24 05:51:47.584 
Epoch 926/1000 
	 loss: 17.0313, MinusLogProbMetric: 17.0313, val_loss: 17.0785, val_MinusLogProbMetric: 17.0785

Epoch 926: val_loss did not improve from 17.05514
196/196 - 58s - loss: 17.0313 - MinusLogProbMetric: 17.0313 - val_loss: 17.0785 - val_MinusLogProbMetric: 17.0785 - lr: 2.7778e-05 - 58s/epoch - 296ms/step
Epoch 927/1000
2023-10-24 05:52:51.578 
Epoch 927/1000 
	 loss: 17.0321, MinusLogProbMetric: 17.0321, val_loss: 17.1399, val_MinusLogProbMetric: 17.1399

Epoch 927: val_loss did not improve from 17.05514
196/196 - 64s - loss: 17.0321 - MinusLogProbMetric: 17.0321 - val_loss: 17.1399 - val_MinusLogProbMetric: 17.1399 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 928/1000
2023-10-24 05:53:50.683 
Epoch 928/1000 
	 loss: 17.0249, MinusLogProbMetric: 17.0249, val_loss: 17.0835, val_MinusLogProbMetric: 17.0835

Epoch 928: val_loss did not improve from 17.05514
196/196 - 59s - loss: 17.0249 - MinusLogProbMetric: 17.0249 - val_loss: 17.0835 - val_MinusLogProbMetric: 17.0835 - lr: 2.7778e-05 - 59s/epoch - 302ms/step
Epoch 929/1000
2023-10-24 05:54:50.012 
Epoch 929/1000 
	 loss: 17.0252, MinusLogProbMetric: 17.0252, val_loss: 17.0873, val_MinusLogProbMetric: 17.0873

Epoch 929: val_loss did not improve from 17.05514
196/196 - 59s - loss: 17.0252 - MinusLogProbMetric: 17.0252 - val_loss: 17.0873 - val_MinusLogProbMetric: 17.0873 - lr: 2.7778e-05 - 59s/epoch - 303ms/step
Epoch 930/1000
2023-10-24 05:55:49.377 
Epoch 930/1000 
	 loss: 17.0257, MinusLogProbMetric: 17.0257, val_loss: 17.0754, val_MinusLogProbMetric: 17.0754

Epoch 930: val_loss did not improve from 17.05514
196/196 - 59s - loss: 17.0257 - MinusLogProbMetric: 17.0257 - val_loss: 17.0754 - val_MinusLogProbMetric: 17.0754 - lr: 2.7778e-05 - 59s/epoch - 303ms/step
Epoch 931/1000
2023-10-24 05:56:56.092 
Epoch 931/1000 
	 loss: 17.0231, MinusLogProbMetric: 17.0231, val_loss: 17.0849, val_MinusLogProbMetric: 17.0849

Epoch 931: val_loss did not improve from 17.05514
196/196 - 67s - loss: 17.0231 - MinusLogProbMetric: 17.0231 - val_loss: 17.0849 - val_MinusLogProbMetric: 17.0849 - lr: 2.7778e-05 - 67s/epoch - 340ms/step
Epoch 932/1000
2023-10-24 05:57:57.407 
Epoch 932/1000 
	 loss: 17.0363, MinusLogProbMetric: 17.0363, val_loss: 17.0372, val_MinusLogProbMetric: 17.0372

Epoch 932: val_loss improved from 17.05514 to 17.03716, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 62s - loss: 17.0363 - MinusLogProbMetric: 17.0363 - val_loss: 17.0372 - val_MinusLogProbMetric: 17.0372 - lr: 2.7778e-05 - 62s/epoch - 319ms/step
Epoch 933/1000
2023-10-24 05:59:00.076 
Epoch 933/1000 
	 loss: 17.0266, MinusLogProbMetric: 17.0266, val_loss: 17.1440, val_MinusLogProbMetric: 17.1440

Epoch 933: val_loss did not improve from 17.03716
196/196 - 62s - loss: 17.0266 - MinusLogProbMetric: 17.0266 - val_loss: 17.1440 - val_MinusLogProbMetric: 17.1440 - lr: 2.7778e-05 - 62s/epoch - 314ms/step
Epoch 934/1000
2023-10-24 06:00:02.197 
Epoch 934/1000 
	 loss: 17.0311, MinusLogProbMetric: 17.0311, val_loss: 17.1241, val_MinusLogProbMetric: 17.1241

Epoch 934: val_loss did not improve from 17.03716
196/196 - 62s - loss: 17.0311 - MinusLogProbMetric: 17.0311 - val_loss: 17.1241 - val_MinusLogProbMetric: 17.1241 - lr: 2.7778e-05 - 62s/epoch - 317ms/step
Epoch 935/1000
2023-10-24 06:01:01.109 
Epoch 935/1000 
	 loss: 17.0264, MinusLogProbMetric: 17.0264, val_loss: 17.0814, val_MinusLogProbMetric: 17.0814

Epoch 935: val_loss did not improve from 17.03716
196/196 - 59s - loss: 17.0264 - MinusLogProbMetric: 17.0264 - val_loss: 17.0814 - val_MinusLogProbMetric: 17.0814 - lr: 2.7778e-05 - 59s/epoch - 301ms/step
Epoch 936/1000
2023-10-24 06:02:00.481 
Epoch 936/1000 
	 loss: 17.0347, MinusLogProbMetric: 17.0347, val_loss: 17.1590, val_MinusLogProbMetric: 17.1590

Epoch 936: val_loss did not improve from 17.03716
196/196 - 59s - loss: 17.0347 - MinusLogProbMetric: 17.0347 - val_loss: 17.1590 - val_MinusLogProbMetric: 17.1590 - lr: 2.7778e-05 - 59s/epoch - 303ms/step
Epoch 937/1000
2023-10-24 06:03:04.835 
Epoch 937/1000 
	 loss: 17.0446, MinusLogProbMetric: 17.0446, val_loss: 17.1125, val_MinusLogProbMetric: 17.1125

Epoch 937: val_loss did not improve from 17.03716
196/196 - 64s - loss: 17.0446 - MinusLogProbMetric: 17.0446 - val_loss: 17.1125 - val_MinusLogProbMetric: 17.1125 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 938/1000
2023-10-24 06:04:12.066 
Epoch 938/1000 
	 loss: 17.0313, MinusLogProbMetric: 17.0313, val_loss: 17.1066, val_MinusLogProbMetric: 17.1066

Epoch 938: val_loss did not improve from 17.03716
196/196 - 67s - loss: 17.0313 - MinusLogProbMetric: 17.0313 - val_loss: 17.1066 - val_MinusLogProbMetric: 17.1066 - lr: 2.7778e-05 - 67s/epoch - 343ms/step
Epoch 939/1000
2023-10-24 06:05:11.845 
Epoch 939/1000 
	 loss: 17.0329, MinusLogProbMetric: 17.0329, val_loss: 17.1707, val_MinusLogProbMetric: 17.1707

Epoch 939: val_loss did not improve from 17.03716
196/196 - 60s - loss: 17.0329 - MinusLogProbMetric: 17.0329 - val_loss: 17.1707 - val_MinusLogProbMetric: 17.1707 - lr: 2.7778e-05 - 60s/epoch - 305ms/step
Epoch 940/1000
2023-10-24 06:06:13.277 
Epoch 940/1000 
	 loss: 17.0464, MinusLogProbMetric: 17.0464, val_loss: 17.1082, val_MinusLogProbMetric: 17.1082

Epoch 940: val_loss did not improve from 17.03716
196/196 - 61s - loss: 17.0464 - MinusLogProbMetric: 17.0464 - val_loss: 17.1082 - val_MinusLogProbMetric: 17.1082 - lr: 2.7778e-05 - 61s/epoch - 313ms/step
Epoch 941/1000
2023-10-24 06:07:15.069 
Epoch 941/1000 
	 loss: 17.0225, MinusLogProbMetric: 17.0225, val_loss: 17.1033, val_MinusLogProbMetric: 17.1033

Epoch 941: val_loss did not improve from 17.03716
196/196 - 62s - loss: 17.0225 - MinusLogProbMetric: 17.0225 - val_loss: 17.1033 - val_MinusLogProbMetric: 17.1033 - lr: 2.7778e-05 - 62s/epoch - 315ms/step
Epoch 942/1000
2023-10-24 06:08:16.179 
Epoch 942/1000 
	 loss: 17.0387, MinusLogProbMetric: 17.0387, val_loss: 17.0524, val_MinusLogProbMetric: 17.0524

Epoch 942: val_loss did not improve from 17.03716
196/196 - 61s - loss: 17.0387 - MinusLogProbMetric: 17.0387 - val_loss: 17.0524 - val_MinusLogProbMetric: 17.0524 - lr: 2.7778e-05 - 61s/epoch - 312ms/step
Epoch 943/1000
2023-10-24 06:09:17.964 
Epoch 943/1000 
	 loss: 17.0261, MinusLogProbMetric: 17.0261, val_loss: 17.0917, val_MinusLogProbMetric: 17.0917

Epoch 943: val_loss did not improve from 17.03716
196/196 - 62s - loss: 17.0261 - MinusLogProbMetric: 17.0261 - val_loss: 17.0917 - val_MinusLogProbMetric: 17.0917 - lr: 2.7778e-05 - 62s/epoch - 315ms/step
Epoch 944/1000
2023-10-24 06:10:17.699 
Epoch 944/1000 
	 loss: 17.0258, MinusLogProbMetric: 17.0258, val_loss: 17.0789, val_MinusLogProbMetric: 17.0789

Epoch 944: val_loss did not improve from 17.03716
196/196 - 60s - loss: 17.0258 - MinusLogProbMetric: 17.0258 - val_loss: 17.0789 - val_MinusLogProbMetric: 17.0789 - lr: 2.7778e-05 - 60s/epoch - 305ms/step
Epoch 945/1000
2023-10-24 06:11:18.260 
Epoch 945/1000 
	 loss: 17.0260, MinusLogProbMetric: 17.0260, val_loss: 17.0795, val_MinusLogProbMetric: 17.0795

Epoch 945: val_loss did not improve from 17.03716
196/196 - 61s - loss: 17.0260 - MinusLogProbMetric: 17.0260 - val_loss: 17.0795 - val_MinusLogProbMetric: 17.0795 - lr: 2.7778e-05 - 61s/epoch - 309ms/step
Epoch 946/1000
2023-10-24 06:12:19.937 
Epoch 946/1000 
	 loss: 17.0242, MinusLogProbMetric: 17.0242, val_loss: 17.0927, val_MinusLogProbMetric: 17.0927

Epoch 946: val_loss did not improve from 17.03716
196/196 - 62s - loss: 17.0242 - MinusLogProbMetric: 17.0242 - val_loss: 17.0927 - val_MinusLogProbMetric: 17.0927 - lr: 2.7778e-05 - 62s/epoch - 315ms/step
Epoch 947/1000
2023-10-24 06:13:24.979 
Epoch 947/1000 
	 loss: 17.0352, MinusLogProbMetric: 17.0352, val_loss: 17.1285, val_MinusLogProbMetric: 17.1285

Epoch 947: val_loss did not improve from 17.03716
196/196 - 65s - loss: 17.0352 - MinusLogProbMetric: 17.0352 - val_loss: 17.1285 - val_MinusLogProbMetric: 17.1285 - lr: 2.7778e-05 - 65s/epoch - 332ms/step
Epoch 948/1000
2023-10-24 06:14:23.904 
Epoch 948/1000 
	 loss: 17.0317, MinusLogProbMetric: 17.0317, val_loss: 17.0609, val_MinusLogProbMetric: 17.0609

Epoch 948: val_loss did not improve from 17.03716
196/196 - 59s - loss: 17.0317 - MinusLogProbMetric: 17.0317 - val_loss: 17.0609 - val_MinusLogProbMetric: 17.0609 - lr: 2.7778e-05 - 59s/epoch - 301ms/step
Epoch 949/1000
2023-10-24 06:15:26.919 
Epoch 949/1000 
	 loss: 17.0303, MinusLogProbMetric: 17.0303, val_loss: 17.0908, val_MinusLogProbMetric: 17.0908

Epoch 949: val_loss did not improve from 17.03716
196/196 - 63s - loss: 17.0303 - MinusLogProbMetric: 17.0303 - val_loss: 17.0908 - val_MinusLogProbMetric: 17.0908 - lr: 2.7778e-05 - 63s/epoch - 321ms/step
Epoch 950/1000
2023-10-24 06:16:26.622 
Epoch 950/1000 
	 loss: 17.0245, MinusLogProbMetric: 17.0245, val_loss: 17.2060, val_MinusLogProbMetric: 17.2060

Epoch 950: val_loss did not improve from 17.03716
196/196 - 60s - loss: 17.0245 - MinusLogProbMetric: 17.0245 - val_loss: 17.2060 - val_MinusLogProbMetric: 17.2060 - lr: 2.7778e-05 - 60s/epoch - 305ms/step
Epoch 951/1000
2023-10-24 06:17:28.497 
Epoch 951/1000 
	 loss: 17.0247, MinusLogProbMetric: 17.0247, val_loss: 17.0898, val_MinusLogProbMetric: 17.0898

Epoch 951: val_loss did not improve from 17.03716
196/196 - 62s - loss: 17.0247 - MinusLogProbMetric: 17.0247 - val_loss: 17.0898 - val_MinusLogProbMetric: 17.0898 - lr: 2.7778e-05 - 62s/epoch - 316ms/step
Epoch 952/1000
2023-10-24 06:18:43.040 
Epoch 952/1000 
	 loss: 17.0231, MinusLogProbMetric: 17.0231, val_loss: 17.0972, val_MinusLogProbMetric: 17.0972

Epoch 952: val_loss did not improve from 17.03716
196/196 - 75s - loss: 17.0231 - MinusLogProbMetric: 17.0231 - val_loss: 17.0972 - val_MinusLogProbMetric: 17.0972 - lr: 2.7778e-05 - 75s/epoch - 380ms/step
Epoch 953/1000
2023-10-24 06:19:56.016 
Epoch 953/1000 
	 loss: 17.0223, MinusLogProbMetric: 17.0223, val_loss: 17.0856, val_MinusLogProbMetric: 17.0856

Epoch 953: val_loss did not improve from 17.03716
196/196 - 73s - loss: 17.0223 - MinusLogProbMetric: 17.0223 - val_loss: 17.0856 - val_MinusLogProbMetric: 17.0856 - lr: 2.7778e-05 - 73s/epoch - 372ms/step
Epoch 954/1000
2023-10-24 06:21:09.911 
Epoch 954/1000 
	 loss: 17.0345, MinusLogProbMetric: 17.0345, val_loss: 17.0924, val_MinusLogProbMetric: 17.0924

Epoch 954: val_loss did not improve from 17.03716
196/196 - 74s - loss: 17.0345 - MinusLogProbMetric: 17.0345 - val_loss: 17.0924 - val_MinusLogProbMetric: 17.0924 - lr: 2.7778e-05 - 74s/epoch - 377ms/step
Epoch 955/1000
2023-10-24 06:22:25.431 
Epoch 955/1000 
	 loss: 17.0241, MinusLogProbMetric: 17.0241, val_loss: 17.0780, val_MinusLogProbMetric: 17.0780

Epoch 955: val_loss did not improve from 17.03716
196/196 - 76s - loss: 17.0241 - MinusLogProbMetric: 17.0241 - val_loss: 17.0780 - val_MinusLogProbMetric: 17.0780 - lr: 2.7778e-05 - 76s/epoch - 385ms/step
Epoch 956/1000
2023-10-24 06:23:37.703 
Epoch 956/1000 
	 loss: 17.0216, MinusLogProbMetric: 17.0216, val_loss: 17.0632, val_MinusLogProbMetric: 17.0632

Epoch 956: val_loss did not improve from 17.03716
196/196 - 72s - loss: 17.0216 - MinusLogProbMetric: 17.0216 - val_loss: 17.0632 - val_MinusLogProbMetric: 17.0632 - lr: 2.7778e-05 - 72s/epoch - 369ms/step
Epoch 957/1000
2023-10-24 06:24:49.791 
Epoch 957/1000 
	 loss: 17.0192, MinusLogProbMetric: 17.0192, val_loss: 17.0703, val_MinusLogProbMetric: 17.0703

Epoch 957: val_loss did not improve from 17.03716
196/196 - 72s - loss: 17.0192 - MinusLogProbMetric: 17.0192 - val_loss: 17.0703 - val_MinusLogProbMetric: 17.0703 - lr: 2.7778e-05 - 72s/epoch - 368ms/step
Epoch 958/1000
2023-10-24 06:26:03.765 
Epoch 958/1000 
	 loss: 17.0248, MinusLogProbMetric: 17.0248, val_loss: 17.0841, val_MinusLogProbMetric: 17.0841

Epoch 958: val_loss did not improve from 17.03716
196/196 - 74s - loss: 17.0248 - MinusLogProbMetric: 17.0248 - val_loss: 17.0841 - val_MinusLogProbMetric: 17.0841 - lr: 2.7778e-05 - 74s/epoch - 377ms/step
Epoch 959/1000
2023-10-24 06:27:17.463 
Epoch 959/1000 
	 loss: 17.0223, MinusLogProbMetric: 17.0223, val_loss: 17.0950, val_MinusLogProbMetric: 17.0950

Epoch 959: val_loss did not improve from 17.03716
196/196 - 74s - loss: 17.0223 - MinusLogProbMetric: 17.0223 - val_loss: 17.0950 - val_MinusLogProbMetric: 17.0950 - lr: 2.7778e-05 - 74s/epoch - 376ms/step
Epoch 960/1000
2023-10-24 06:28:33.254 
Epoch 960/1000 
	 loss: 17.0207, MinusLogProbMetric: 17.0207, val_loss: 17.0841, val_MinusLogProbMetric: 17.0841

Epoch 960: val_loss did not improve from 17.03716
196/196 - 76s - loss: 17.0207 - MinusLogProbMetric: 17.0207 - val_loss: 17.0841 - val_MinusLogProbMetric: 17.0841 - lr: 2.7778e-05 - 76s/epoch - 387ms/step
Epoch 961/1000
2023-10-24 06:29:44.952 
Epoch 961/1000 
	 loss: 17.0265, MinusLogProbMetric: 17.0265, val_loss: 17.0699, val_MinusLogProbMetric: 17.0699

Epoch 961: val_loss did not improve from 17.03716
196/196 - 72s - loss: 17.0265 - MinusLogProbMetric: 17.0265 - val_loss: 17.0699 - val_MinusLogProbMetric: 17.0699 - lr: 2.7778e-05 - 72s/epoch - 366ms/step
Epoch 962/1000
2023-10-24 06:30:55.660 
Epoch 962/1000 
	 loss: 17.0229, MinusLogProbMetric: 17.0229, val_loss: 17.0963, val_MinusLogProbMetric: 17.0963

Epoch 962: val_loss did not improve from 17.03716
196/196 - 71s - loss: 17.0229 - MinusLogProbMetric: 17.0229 - val_loss: 17.0963 - val_MinusLogProbMetric: 17.0963 - lr: 2.7778e-05 - 71s/epoch - 361ms/step
Epoch 963/1000
2023-10-24 06:32:06.413 
Epoch 963/1000 
	 loss: 17.0357, MinusLogProbMetric: 17.0357, val_loss: 17.0693, val_MinusLogProbMetric: 17.0693

Epoch 963: val_loss did not improve from 17.03716
196/196 - 71s - loss: 17.0357 - MinusLogProbMetric: 17.0357 - val_loss: 17.0693 - val_MinusLogProbMetric: 17.0693 - lr: 2.7778e-05 - 71s/epoch - 361ms/step
Epoch 964/1000
2023-10-24 06:33:23.689 
Epoch 964/1000 
	 loss: 17.0335, MinusLogProbMetric: 17.0335, val_loss: 17.1049, val_MinusLogProbMetric: 17.1049

Epoch 964: val_loss did not improve from 17.03716
196/196 - 77s - loss: 17.0335 - MinusLogProbMetric: 17.0335 - val_loss: 17.1049 - val_MinusLogProbMetric: 17.1049 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 965/1000
2023-10-24 06:34:37.375 
Epoch 965/1000 
	 loss: 17.0216, MinusLogProbMetric: 17.0216, val_loss: 17.0693, val_MinusLogProbMetric: 17.0693

Epoch 965: val_loss did not improve from 17.03716
196/196 - 74s - loss: 17.0216 - MinusLogProbMetric: 17.0216 - val_loss: 17.0693 - val_MinusLogProbMetric: 17.0693 - lr: 2.7778e-05 - 74s/epoch - 376ms/step
Epoch 966/1000
2023-10-24 06:35:52.429 
Epoch 966/1000 
	 loss: 17.0255, MinusLogProbMetric: 17.0255, val_loss: 17.0622, val_MinusLogProbMetric: 17.0622

Epoch 966: val_loss did not improve from 17.03716
196/196 - 75s - loss: 17.0255 - MinusLogProbMetric: 17.0255 - val_loss: 17.0622 - val_MinusLogProbMetric: 17.0622 - lr: 2.7778e-05 - 75s/epoch - 383ms/step
Epoch 967/1000
2023-10-24 06:37:09.972 
Epoch 967/1000 
	 loss: 17.0178, MinusLogProbMetric: 17.0178, val_loss: 17.0899, val_MinusLogProbMetric: 17.0899

Epoch 967: val_loss did not improve from 17.03716
196/196 - 78s - loss: 17.0178 - MinusLogProbMetric: 17.0178 - val_loss: 17.0899 - val_MinusLogProbMetric: 17.0899 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 968/1000
2023-10-24 06:38:27.503 
Epoch 968/1000 
	 loss: 17.0166, MinusLogProbMetric: 17.0166, val_loss: 17.1996, val_MinusLogProbMetric: 17.1996

Epoch 968: val_loss did not improve from 17.03716
196/196 - 78s - loss: 17.0166 - MinusLogProbMetric: 17.0166 - val_loss: 17.1996 - val_MinusLogProbMetric: 17.1996 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 969/1000
2023-10-24 06:39:43.834 
Epoch 969/1000 
	 loss: 17.0249, MinusLogProbMetric: 17.0249, val_loss: 17.0775, val_MinusLogProbMetric: 17.0775

Epoch 969: val_loss did not improve from 17.03716
196/196 - 76s - loss: 17.0249 - MinusLogProbMetric: 17.0249 - val_loss: 17.0775 - val_MinusLogProbMetric: 17.0775 - lr: 2.7778e-05 - 76s/epoch - 389ms/step
Epoch 970/1000
2023-10-24 06:41:01.856 
Epoch 970/1000 
	 loss: 17.0407, MinusLogProbMetric: 17.0407, val_loss: 17.1162, val_MinusLogProbMetric: 17.1162

Epoch 970: val_loss did not improve from 17.03716
196/196 - 78s - loss: 17.0407 - MinusLogProbMetric: 17.0407 - val_loss: 17.1162 - val_MinusLogProbMetric: 17.1162 - lr: 2.7778e-05 - 78s/epoch - 398ms/step
Epoch 971/1000
2023-10-24 06:42:17.608 
Epoch 971/1000 
	 loss: 17.0283, MinusLogProbMetric: 17.0283, val_loss: 17.0756, val_MinusLogProbMetric: 17.0756

Epoch 971: val_loss did not improve from 17.03716
196/196 - 76s - loss: 17.0283 - MinusLogProbMetric: 17.0283 - val_loss: 17.0756 - val_MinusLogProbMetric: 17.0756 - lr: 2.7778e-05 - 76s/epoch - 386ms/step
Epoch 972/1000
2023-10-24 06:43:35.223 
Epoch 972/1000 
	 loss: 17.0183, MinusLogProbMetric: 17.0183, val_loss: 17.1278, val_MinusLogProbMetric: 17.1278

Epoch 972: val_loss did not improve from 17.03716
196/196 - 78s - loss: 17.0183 - MinusLogProbMetric: 17.0183 - val_loss: 17.1278 - val_MinusLogProbMetric: 17.1278 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 973/1000
2023-10-24 06:44:51.786 
Epoch 973/1000 
	 loss: 17.0285, MinusLogProbMetric: 17.0285, val_loss: 17.1266, val_MinusLogProbMetric: 17.1266

Epoch 973: val_loss did not improve from 17.03716
196/196 - 77s - loss: 17.0285 - MinusLogProbMetric: 17.0285 - val_loss: 17.1266 - val_MinusLogProbMetric: 17.1266 - lr: 2.7778e-05 - 77s/epoch - 391ms/step
Epoch 974/1000
2023-10-24 06:46:08.865 
Epoch 974/1000 
	 loss: 17.0180, MinusLogProbMetric: 17.0180, val_loss: 17.0825, val_MinusLogProbMetric: 17.0825

Epoch 974: val_loss did not improve from 17.03716
196/196 - 77s - loss: 17.0180 - MinusLogProbMetric: 17.0180 - val_loss: 17.0825 - val_MinusLogProbMetric: 17.0825 - lr: 2.7778e-05 - 77s/epoch - 393ms/step
Epoch 975/1000
2023-10-24 06:47:27.008 
Epoch 975/1000 
	 loss: 17.0278, MinusLogProbMetric: 17.0278, val_loss: 17.0850, val_MinusLogProbMetric: 17.0850

Epoch 975: val_loss did not improve from 17.03716
196/196 - 78s - loss: 17.0278 - MinusLogProbMetric: 17.0278 - val_loss: 17.0850 - val_MinusLogProbMetric: 17.0850 - lr: 2.7778e-05 - 78s/epoch - 399ms/step
Epoch 976/1000
2023-10-24 06:48:44.703 
Epoch 976/1000 
	 loss: 17.0167, MinusLogProbMetric: 17.0167, val_loss: 17.0644, val_MinusLogProbMetric: 17.0644

Epoch 976: val_loss did not improve from 17.03716
196/196 - 78s - loss: 17.0167 - MinusLogProbMetric: 17.0167 - val_loss: 17.0644 - val_MinusLogProbMetric: 17.0644 - lr: 2.7778e-05 - 78s/epoch - 396ms/step
Epoch 977/1000
2023-10-24 06:49:59.945 
Epoch 977/1000 
	 loss: 17.0249, MinusLogProbMetric: 17.0249, val_loss: 17.1086, val_MinusLogProbMetric: 17.1086

Epoch 977: val_loss did not improve from 17.03716
196/196 - 75s - loss: 17.0249 - MinusLogProbMetric: 17.0249 - val_loss: 17.1086 - val_MinusLogProbMetric: 17.1086 - lr: 2.7778e-05 - 75s/epoch - 384ms/step
Epoch 978/1000
2023-10-24 06:51:17.408 
Epoch 978/1000 
	 loss: 17.0181, MinusLogProbMetric: 17.0181, val_loss: 17.0416, val_MinusLogProbMetric: 17.0416

Epoch 978: val_loss did not improve from 17.03716
196/196 - 77s - loss: 17.0181 - MinusLogProbMetric: 17.0181 - val_loss: 17.0416 - val_MinusLogProbMetric: 17.0416 - lr: 2.7778e-05 - 77s/epoch - 395ms/step
Epoch 979/1000
2023-10-24 06:52:30.867 
Epoch 979/1000 
	 loss: 17.0244, MinusLogProbMetric: 17.0244, val_loss: 17.0952, val_MinusLogProbMetric: 17.0952

Epoch 979: val_loss did not improve from 17.03716
196/196 - 73s - loss: 17.0244 - MinusLogProbMetric: 17.0244 - val_loss: 17.0952 - val_MinusLogProbMetric: 17.0952 - lr: 2.7778e-05 - 73s/epoch - 375ms/step
Epoch 980/1000
2023-10-24 06:53:48.088 
Epoch 980/1000 
	 loss: 17.0202, MinusLogProbMetric: 17.0202, val_loss: 17.0563, val_MinusLogProbMetric: 17.0563

Epoch 980: val_loss did not improve from 17.03716
196/196 - 77s - loss: 17.0202 - MinusLogProbMetric: 17.0202 - val_loss: 17.0563 - val_MinusLogProbMetric: 17.0563 - lr: 2.7778e-05 - 77s/epoch - 394ms/step
Epoch 981/1000
2023-10-24 06:55:04.352 
Epoch 981/1000 
	 loss: 17.0139, MinusLogProbMetric: 17.0139, val_loss: 17.1035, val_MinusLogProbMetric: 17.1035

Epoch 981: val_loss did not improve from 17.03716
196/196 - 76s - loss: 17.0139 - MinusLogProbMetric: 17.0139 - val_loss: 17.1035 - val_MinusLogProbMetric: 17.1035 - lr: 2.7778e-05 - 76s/epoch - 389ms/step
Epoch 982/1000
2023-10-24 06:56:21.402 
Epoch 982/1000 
	 loss: 17.0250, MinusLogProbMetric: 17.0250, val_loss: 17.1304, val_MinusLogProbMetric: 17.1304

Epoch 982: val_loss did not improve from 17.03716
196/196 - 77s - loss: 17.0250 - MinusLogProbMetric: 17.0250 - val_loss: 17.1304 - val_MinusLogProbMetric: 17.1304 - lr: 2.7778e-05 - 77s/epoch - 393ms/step
Epoch 983/1000
2023-10-24 06:57:38.844 
Epoch 983/1000 
	 loss: 16.9868, MinusLogProbMetric: 16.9868, val_loss: 17.0622, val_MinusLogProbMetric: 17.0622

Epoch 983: val_loss did not improve from 17.03716
196/196 - 77s - loss: 16.9868 - MinusLogProbMetric: 16.9868 - val_loss: 17.0622 - val_MinusLogProbMetric: 17.0622 - lr: 1.3889e-05 - 77s/epoch - 395ms/step
Epoch 984/1000
2023-10-24 06:58:56.930 
Epoch 984/1000 
	 loss: 16.9805, MinusLogProbMetric: 16.9805, val_loss: 17.0267, val_MinusLogProbMetric: 17.0267

Epoch 984: val_loss improved from 17.03716 to 17.02673, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 79s - loss: 16.9805 - MinusLogProbMetric: 16.9805 - val_loss: 17.0267 - val_MinusLogProbMetric: 17.0267 - lr: 1.3889e-05 - 79s/epoch - 405ms/step
Epoch 985/1000
2023-10-24 07:00:15.624 
Epoch 985/1000 
	 loss: 16.9820, MinusLogProbMetric: 16.9820, val_loss: 17.0267, val_MinusLogProbMetric: 17.0267

Epoch 985: val_loss improved from 17.02673 to 17.02672, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 78s - loss: 16.9820 - MinusLogProbMetric: 16.9820 - val_loss: 17.0267 - val_MinusLogProbMetric: 17.0267 - lr: 1.3889e-05 - 78s/epoch - 400ms/step
Epoch 986/1000
2023-10-24 07:01:34.448 
Epoch 986/1000 
	 loss: 16.9842, MinusLogProbMetric: 16.9842, val_loss: 17.0453, val_MinusLogProbMetric: 17.0453

Epoch 986: val_loss did not improve from 17.02672
196/196 - 78s - loss: 16.9842 - MinusLogProbMetric: 16.9842 - val_loss: 17.0453 - val_MinusLogProbMetric: 17.0453 - lr: 1.3889e-05 - 78s/epoch - 397ms/step
Epoch 987/1000
2023-10-24 07:02:51.537 
Epoch 987/1000 
	 loss: 16.9823, MinusLogProbMetric: 16.9823, val_loss: 17.0548, val_MinusLogProbMetric: 17.0548

Epoch 987: val_loss did not improve from 17.02672
196/196 - 77s - loss: 16.9823 - MinusLogProbMetric: 16.9823 - val_loss: 17.0548 - val_MinusLogProbMetric: 17.0548 - lr: 1.3889e-05 - 77s/epoch - 393ms/step
Epoch 988/1000
2023-10-24 07:04:06.691 
Epoch 988/1000 
	 loss: 16.9801, MinusLogProbMetric: 16.9801, val_loss: 17.0293, val_MinusLogProbMetric: 17.0293

Epoch 988: val_loss did not improve from 17.02672
196/196 - 75s - loss: 16.9801 - MinusLogProbMetric: 16.9801 - val_loss: 17.0293 - val_MinusLogProbMetric: 17.0293 - lr: 1.3889e-05 - 75s/epoch - 383ms/step
Epoch 989/1000
2023-10-24 07:05:21.700 
Epoch 989/1000 
	 loss: 16.9792, MinusLogProbMetric: 16.9792, val_loss: 17.0275, val_MinusLogProbMetric: 17.0275

Epoch 989: val_loss did not improve from 17.02672
196/196 - 75s - loss: 16.9792 - MinusLogProbMetric: 16.9792 - val_loss: 17.0275 - val_MinusLogProbMetric: 17.0275 - lr: 1.3889e-05 - 75s/epoch - 383ms/step
Epoch 990/1000
2023-10-24 07:06:36.640 
Epoch 990/1000 
	 loss: 16.9765, MinusLogProbMetric: 16.9765, val_loss: 17.0420, val_MinusLogProbMetric: 17.0420

Epoch 990: val_loss did not improve from 17.02672
196/196 - 75s - loss: 16.9765 - MinusLogProbMetric: 16.9765 - val_loss: 17.0420 - val_MinusLogProbMetric: 17.0420 - lr: 1.3889e-05 - 75s/epoch - 382ms/step
Epoch 991/1000
2023-10-24 07:07:53.854 
Epoch 991/1000 
	 loss: 16.9856, MinusLogProbMetric: 16.9856, val_loss: 17.0490, val_MinusLogProbMetric: 17.0490

Epoch 991: val_loss did not improve from 17.02672
196/196 - 77s - loss: 16.9856 - MinusLogProbMetric: 16.9856 - val_loss: 17.0490 - val_MinusLogProbMetric: 17.0490 - lr: 1.3889e-05 - 77s/epoch - 394ms/step
Epoch 992/1000
2023-10-24 07:09:10.876 
Epoch 992/1000 
	 loss: 16.9844, MinusLogProbMetric: 16.9844, val_loss: 17.0497, val_MinusLogProbMetric: 17.0497

Epoch 992: val_loss did not improve from 17.02672
196/196 - 77s - loss: 16.9844 - MinusLogProbMetric: 16.9844 - val_loss: 17.0497 - val_MinusLogProbMetric: 17.0497 - lr: 1.3889e-05 - 77s/epoch - 393ms/step
Epoch 993/1000
2023-10-24 07:10:29.313 
Epoch 993/1000 
	 loss: 16.9880, MinusLogProbMetric: 16.9880, val_loss: 17.0300, val_MinusLogProbMetric: 17.0300

Epoch 993: val_loss did not improve from 17.02672
196/196 - 78s - loss: 16.9880 - MinusLogProbMetric: 16.9880 - val_loss: 17.0300 - val_MinusLogProbMetric: 17.0300 - lr: 1.3889e-05 - 78s/epoch - 400ms/step
Epoch 994/1000
2023-10-24 07:11:46.556 
Epoch 994/1000 
	 loss: 16.9785, MinusLogProbMetric: 16.9785, val_loss: 17.0594, val_MinusLogProbMetric: 17.0594

Epoch 994: val_loss did not improve from 17.02672
196/196 - 77s - loss: 16.9785 - MinusLogProbMetric: 16.9785 - val_loss: 17.0594 - val_MinusLogProbMetric: 17.0594 - lr: 1.3889e-05 - 77s/epoch - 394ms/step
Epoch 995/1000
2023-10-24 07:13:03.116 
Epoch 995/1000 
	 loss: 16.9820, MinusLogProbMetric: 16.9820, val_loss: 17.0472, val_MinusLogProbMetric: 17.0472

Epoch 995: val_loss did not improve from 17.02672
196/196 - 77s - loss: 16.9820 - MinusLogProbMetric: 16.9820 - val_loss: 17.0472 - val_MinusLogProbMetric: 17.0472 - lr: 1.3889e-05 - 77s/epoch - 391ms/step
Epoch 996/1000
2023-10-24 07:14:20.061 
Epoch 996/1000 
	 loss: 16.9791, MinusLogProbMetric: 16.9791, val_loss: 17.0443, val_MinusLogProbMetric: 17.0443

Epoch 996: val_loss did not improve from 17.02672
196/196 - 77s - loss: 16.9791 - MinusLogProbMetric: 16.9791 - val_loss: 17.0443 - val_MinusLogProbMetric: 17.0443 - lr: 1.3889e-05 - 77s/epoch - 393ms/step
Epoch 997/1000
2023-10-24 07:15:37.851 
Epoch 997/1000 
	 loss: 16.9800, MinusLogProbMetric: 16.9800, val_loss: 17.0349, val_MinusLogProbMetric: 17.0349

Epoch 997: val_loss did not improve from 17.02672
196/196 - 78s - loss: 16.9800 - MinusLogProbMetric: 16.9800 - val_loss: 17.0349 - val_MinusLogProbMetric: 17.0349 - lr: 1.3889e-05 - 78s/epoch - 397ms/step
Epoch 998/1000
2023-10-24 07:16:53.713 
Epoch 998/1000 
	 loss: 16.9805, MinusLogProbMetric: 16.9805, val_loss: 17.0315, val_MinusLogProbMetric: 17.0315

Epoch 998: val_loss did not improve from 17.02672
196/196 - 76s - loss: 16.9805 - MinusLogProbMetric: 16.9805 - val_loss: 17.0315 - val_MinusLogProbMetric: 17.0315 - lr: 1.3889e-05 - 76s/epoch - 387ms/step
Epoch 999/1000
2023-10-24 07:18:09.914 
Epoch 999/1000 
	 loss: 16.9800, MinusLogProbMetric: 16.9800, val_loss: 17.0201, val_MinusLogProbMetric: 17.0201

Epoch 999: val_loss improved from 17.02672 to 17.02013, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 77s - loss: 16.9800 - MinusLogProbMetric: 16.9800 - val_loss: 17.0201 - val_MinusLogProbMetric: 17.0201 - lr: 1.3889e-05 - 77s/epoch - 394ms/step
Epoch 1000/1000
2023-10-24 07:19:25.021 
Epoch 1000/1000 
	 loss: 16.9774, MinusLogProbMetric: 16.9774, val_loss: 17.0641, val_MinusLogProbMetric: 17.0641

Epoch 1000: val_loss did not improve from 17.02013
196/196 - 74s - loss: 16.9774 - MinusLogProbMetric: 16.9774 - val_loss: 17.0641 - val_MinusLogProbMetric: 17.0641 - lr: 1.3889e-05 - 74s/epoch - 378ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 377.
Model trained in 67086.67 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.27 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.67 s.
===========
Run 263/720 done in 67550.23 s.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_319/ already exists.
Skipping it.
===========
Run 319/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_321/ already exists.
Skipping it.
===========
Run 321/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_322/ already exists.
Skipping it.
===========
Run 322/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_323/ already exists.
Skipping it.
===========
Run 323/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_324/ already exists.
Skipping it.
===========
Run 324/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_325/ already exists.
Skipping it.
===========
Run 325/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_326/ already exists.
Skipping it.
===========
Run 326/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_327/ already exists.
Skipping it.
===========
Run 327/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_328/ already exists.
Skipping it.
===========
Run 328/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_329/ already exists.
Skipping it.
===========
Run 329/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_330/ already exists.
Skipping it.
===========
Run 330/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_331/ already exists.
Skipping it.
===========
Run 331/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_332/ already exists.
Skipping it.
===========
Run 332/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_333/ already exists.
Skipping it.
===========
Run 333/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_334/ already exists.
Skipping it.
===========
Run 334/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_335/ already exists.
Skipping it.
===========
Run 335/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_336/ already exists.
Skipping it.
===========
Run 336/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_337/ already exists.
Skipping it.
===========
Run 337/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_338/ already exists.
Skipping it.
===========
Run 338/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_339/ already exists.
Skipping it.
===========
Run 339/720 already exists. Skipping it.
===========

===========
Generating train data for run 340.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_340/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_340/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_340/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_340
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_38"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_39 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  2139360   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,139,360
Trainable params: 2,139,360
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7fe7c06d3220>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe791d8f670>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe791d8f670>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe7913c83a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe7921584f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe792158a60>, <keras.callbacks.ModelCheckpoint object at 0x7fe792158b20>, <keras.callbacks.EarlyStopping object at 0x7fe792158d90>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe792158dc0>, <keras.callbacks.TerminateOnNaN object at 0x7fe792158a00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_340/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 340/720 with hyperparameters:
timestamp = 2023-10-24 07:19:31.291495
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2139360
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
2023-10-24 07:21:25.484 
Epoch 1/1000 
	 loss: 975.5152, MinusLogProbMetric: 975.5152, val_loss: 284.7541, val_MinusLogProbMetric: 284.7541

Epoch 1: val_loss improved from inf to 284.75409, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 115s - loss: 975.5152 - MinusLogProbMetric: 975.5152 - val_loss: 284.7541 - val_MinusLogProbMetric: 284.7541 - lr: 0.0010 - 115s/epoch - 586ms/step
Epoch 2/1000
2023-10-24 07:22:06.603 
Epoch 2/1000 
	 loss: 192.0965, MinusLogProbMetric: 192.0965, val_loss: 186.4296, val_MinusLogProbMetric: 186.4296

Epoch 2: val_loss improved from 284.75409 to 186.42963, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 41s - loss: 192.0965 - MinusLogProbMetric: 192.0965 - val_loss: 186.4296 - val_MinusLogProbMetric: 186.4296 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 3/1000
2023-10-24 07:22:47.762 
Epoch 3/1000 
	 loss: 120.2215, MinusLogProbMetric: 120.2215, val_loss: 96.3744, val_MinusLogProbMetric: 96.3744

Epoch 3: val_loss improved from 186.42963 to 96.37435, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 41s - loss: 120.2215 - MinusLogProbMetric: 120.2215 - val_loss: 96.3744 - val_MinusLogProbMetric: 96.3744 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 4/1000
2023-10-24 07:23:30.947 
Epoch 4/1000 
	 loss: 87.3161, MinusLogProbMetric: 87.3161, val_loss: 76.6115, val_MinusLogProbMetric: 76.6115

Epoch 4: val_loss improved from 96.37435 to 76.61153, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 43s - loss: 87.3161 - MinusLogProbMetric: 87.3161 - val_loss: 76.6115 - val_MinusLogProbMetric: 76.6115 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 5/1000
2023-10-24 07:24:13.339 
Epoch 5/1000 
	 loss: 95.7548, MinusLogProbMetric: 95.7548, val_loss: 83.1934, val_MinusLogProbMetric: 83.1934

Epoch 5: val_loss did not improve from 76.61153
196/196 - 42s - loss: 95.7548 - MinusLogProbMetric: 95.7548 - val_loss: 83.1934 - val_MinusLogProbMetric: 83.1934 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 6/1000
2023-10-24 07:24:55.389 
Epoch 6/1000 
	 loss: 70.3797, MinusLogProbMetric: 70.3797, val_loss: 65.1986, val_MinusLogProbMetric: 65.1986

Epoch 6: val_loss improved from 76.61153 to 65.19862, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 43s - loss: 70.3797 - MinusLogProbMetric: 70.3797 - val_loss: 65.1986 - val_MinusLogProbMetric: 65.1986 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 7/1000
2023-10-24 07:25:38.011 
Epoch 7/1000 
	 loss: 60.3126, MinusLogProbMetric: 60.3126, val_loss: 56.7380, val_MinusLogProbMetric: 56.7380

Epoch 7: val_loss improved from 65.19862 to 56.73804, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 43s - loss: 60.3126 - MinusLogProbMetric: 60.3126 - val_loss: 56.7380 - val_MinusLogProbMetric: 56.7380 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 8/1000
2023-10-24 07:26:20.979 
Epoch 8/1000 
	 loss: 56.2198, MinusLogProbMetric: 56.2198, val_loss: 53.4571, val_MinusLogProbMetric: 53.4571

Epoch 8: val_loss improved from 56.73804 to 53.45709, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 43s - loss: 56.2198 - MinusLogProbMetric: 56.2198 - val_loss: 53.4571 - val_MinusLogProbMetric: 53.4571 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 9/1000
2023-10-24 07:27:03.157 
Epoch 9/1000 
	 loss: 52.0599, MinusLogProbMetric: 52.0599, val_loss: 51.3156, val_MinusLogProbMetric: 51.3156

Epoch 9: val_loss improved from 53.45709 to 51.31564, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 42s - loss: 52.0599 - MinusLogProbMetric: 52.0599 - val_loss: 51.3156 - val_MinusLogProbMetric: 51.3156 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 10/1000
2023-10-24 07:27:45.134 
Epoch 10/1000 
	 loss: 53.6766, MinusLogProbMetric: 53.6766, val_loss: 50.1369, val_MinusLogProbMetric: 50.1369

Epoch 10: val_loss improved from 51.31564 to 50.13687, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 42s - loss: 53.6766 - MinusLogProbMetric: 53.6766 - val_loss: 50.1369 - val_MinusLogProbMetric: 50.1369 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 11/1000
2023-10-24 07:28:26.577 
Epoch 11/1000 
	 loss: 48.7870, MinusLogProbMetric: 48.7870, val_loss: 50.1491, val_MinusLogProbMetric: 50.1491

Epoch 11: val_loss did not improve from 50.13687
196/196 - 41s - loss: 48.7870 - MinusLogProbMetric: 48.7870 - val_loss: 50.1491 - val_MinusLogProbMetric: 50.1491 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 12/1000
2023-10-24 07:29:07.922 
Epoch 12/1000 
	 loss: 47.3795, MinusLogProbMetric: 47.3795, val_loss: 44.6317, val_MinusLogProbMetric: 44.6317

Epoch 12: val_loss improved from 50.13687 to 44.63168, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 42s - loss: 47.3795 - MinusLogProbMetric: 47.3795 - val_loss: 44.6317 - val_MinusLogProbMetric: 44.6317 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 13/1000
2023-10-24 07:29:47.388 
Epoch 13/1000 
	 loss: 47.0611, MinusLogProbMetric: 47.0611, val_loss: 45.1621, val_MinusLogProbMetric: 45.1621

Epoch 13: val_loss did not improve from 44.63168
196/196 - 39s - loss: 47.0611 - MinusLogProbMetric: 47.0611 - val_loss: 45.1621 - val_MinusLogProbMetric: 45.1621 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 14/1000
2023-10-24 07:30:27.077 
Epoch 14/1000 
	 loss: 44.7164, MinusLogProbMetric: 44.7164, val_loss: 43.4098, val_MinusLogProbMetric: 43.4098

Epoch 14: val_loss improved from 44.63168 to 43.40981, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 40s - loss: 44.7164 - MinusLogProbMetric: 44.7164 - val_loss: 43.4098 - val_MinusLogProbMetric: 43.4098 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 15/1000
2023-10-24 07:31:09.893 
Epoch 15/1000 
	 loss: 44.7531, MinusLogProbMetric: 44.7531, val_loss: 42.8579, val_MinusLogProbMetric: 42.8579

Epoch 15: val_loss improved from 43.40981 to 42.85787, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 43s - loss: 44.7531 - MinusLogProbMetric: 44.7531 - val_loss: 42.8579 - val_MinusLogProbMetric: 42.8579 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 16/1000
2023-10-24 07:31:51.502 
Epoch 16/1000 
	 loss: 42.7317, MinusLogProbMetric: 42.7317, val_loss: 42.0005, val_MinusLogProbMetric: 42.0005

Epoch 16: val_loss improved from 42.85787 to 42.00051, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 42s - loss: 42.7317 - MinusLogProbMetric: 42.7317 - val_loss: 42.0005 - val_MinusLogProbMetric: 42.0005 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 17/1000
2023-10-24 07:32:32.889 
Epoch 17/1000 
	 loss: 41.8827, MinusLogProbMetric: 41.8827, val_loss: 41.0279, val_MinusLogProbMetric: 41.0279

Epoch 17: val_loss improved from 42.00051 to 41.02785, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 41s - loss: 41.8827 - MinusLogProbMetric: 41.8827 - val_loss: 41.0279 - val_MinusLogProbMetric: 41.0279 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 18/1000
2023-10-24 07:33:14.607 
Epoch 18/1000 
	 loss: 41.0577, MinusLogProbMetric: 41.0577, val_loss: 40.6430, val_MinusLogProbMetric: 40.6430

Epoch 18: val_loss improved from 41.02785 to 40.64302, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 42s - loss: 41.0577 - MinusLogProbMetric: 41.0577 - val_loss: 40.6430 - val_MinusLogProbMetric: 40.6430 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 19/1000
2023-10-24 07:33:56.173 
Epoch 19/1000 
	 loss: 40.6700, MinusLogProbMetric: 40.6700, val_loss: 39.5872, val_MinusLogProbMetric: 39.5872

Epoch 19: val_loss improved from 40.64302 to 39.58723, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 42s - loss: 40.6700 - MinusLogProbMetric: 40.6700 - val_loss: 39.5872 - val_MinusLogProbMetric: 39.5872 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 20/1000
2023-10-24 07:34:35.405 
Epoch 20/1000 
	 loss: 40.6353, MinusLogProbMetric: 40.6353, val_loss: 39.4388, val_MinusLogProbMetric: 39.4388

Epoch 20: val_loss improved from 39.58723 to 39.43882, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 39s - loss: 40.6353 - MinusLogProbMetric: 40.6353 - val_loss: 39.4388 - val_MinusLogProbMetric: 39.4388 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 21/1000
2023-10-24 07:35:15.717 
Epoch 21/1000 
	 loss: 41.9885, MinusLogProbMetric: 41.9885, val_loss: 39.8261, val_MinusLogProbMetric: 39.8261

Epoch 21: val_loss did not improve from 39.43882
196/196 - 40s - loss: 41.9885 - MinusLogProbMetric: 41.9885 - val_loss: 39.8261 - val_MinusLogProbMetric: 39.8261 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 22/1000
2023-10-24 07:35:56.971 
Epoch 22/1000 
	 loss: 39.0650, MinusLogProbMetric: 39.0650, val_loss: 38.7757, val_MinusLogProbMetric: 38.7757

Epoch 22: val_loss improved from 39.43882 to 38.77565, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 42s - loss: 39.0650 - MinusLogProbMetric: 39.0650 - val_loss: 38.7757 - val_MinusLogProbMetric: 38.7757 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 23/1000
2023-10-24 07:36:38.880 
Epoch 23/1000 
	 loss: 39.1975, MinusLogProbMetric: 39.1975, val_loss: 38.6178, val_MinusLogProbMetric: 38.6178

Epoch 23: val_loss improved from 38.77565 to 38.61785, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 42s - loss: 39.1975 - MinusLogProbMetric: 39.1975 - val_loss: 38.6178 - val_MinusLogProbMetric: 38.6178 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 24/1000
2023-10-24 07:37:19.706 
Epoch 24/1000 
	 loss: 38.0284, MinusLogProbMetric: 38.0284, val_loss: 36.6887, val_MinusLogProbMetric: 36.6887

Epoch 24: val_loss improved from 38.61785 to 36.68869, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 41s - loss: 38.0284 - MinusLogProbMetric: 38.0284 - val_loss: 36.6887 - val_MinusLogProbMetric: 36.6887 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 25/1000
2023-10-24 07:38:00.910 
Epoch 25/1000 
	 loss: 39.0858, MinusLogProbMetric: 39.0858, val_loss: 38.4840, val_MinusLogProbMetric: 38.4840

Epoch 25: val_loss did not improve from 36.68869
196/196 - 41s - loss: 39.0858 - MinusLogProbMetric: 39.0858 - val_loss: 38.4840 - val_MinusLogProbMetric: 38.4840 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 26/1000
2023-10-24 07:38:43.018 
Epoch 26/1000 
	 loss: 38.3868, MinusLogProbMetric: 38.3868, val_loss: 39.9570, val_MinusLogProbMetric: 39.9570

Epoch 26: val_loss did not improve from 36.68869
196/196 - 42s - loss: 38.3868 - MinusLogProbMetric: 38.3868 - val_loss: 39.9570 - val_MinusLogProbMetric: 39.9570 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 27/1000
2023-10-24 07:39:24.236 
Epoch 27/1000 
	 loss: 37.0356, MinusLogProbMetric: 37.0356, val_loss: 36.5557, val_MinusLogProbMetric: 36.5557

Epoch 27: val_loss improved from 36.68869 to 36.55569, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 42s - loss: 37.0356 - MinusLogProbMetric: 37.0356 - val_loss: 36.5557 - val_MinusLogProbMetric: 36.5557 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 28/1000
2023-10-24 07:40:06.544 
Epoch 28/1000 
	 loss: 36.8836, MinusLogProbMetric: 36.8836, val_loss: 36.3460, val_MinusLogProbMetric: 36.3460

Epoch 28: val_loss improved from 36.55569 to 36.34595, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 42s - loss: 36.8836 - MinusLogProbMetric: 36.8836 - val_loss: 36.3460 - val_MinusLogProbMetric: 36.3460 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 29/1000
2023-10-24 07:40:48.781 
Epoch 29/1000 
	 loss: 37.4783, MinusLogProbMetric: 37.4783, val_loss: 35.2152, val_MinusLogProbMetric: 35.2152

Epoch 29: val_loss improved from 36.34595 to 35.21523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 42s - loss: 37.4783 - MinusLogProbMetric: 37.4783 - val_loss: 35.2152 - val_MinusLogProbMetric: 35.2152 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 30/1000
2023-10-24 07:41:29.895 
Epoch 30/1000 
	 loss: 36.9504, MinusLogProbMetric: 36.9504, val_loss: 36.8627, val_MinusLogProbMetric: 36.8627

Epoch 30: val_loss did not improve from 35.21523
196/196 - 40s - loss: 36.9504 - MinusLogProbMetric: 36.9504 - val_loss: 36.8627 - val_MinusLogProbMetric: 36.8627 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 31/1000
2023-10-24 07:42:10.840 
Epoch 31/1000 
	 loss: 36.6925, MinusLogProbMetric: 36.6925, val_loss: 46.5830, val_MinusLogProbMetric: 46.5830

Epoch 31: val_loss did not improve from 35.21523
196/196 - 41s - loss: 36.6925 - MinusLogProbMetric: 36.6925 - val_loss: 46.5830 - val_MinusLogProbMetric: 46.5830 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 32/1000
2023-10-24 07:42:51.527 
Epoch 32/1000 
	 loss: 36.4914, MinusLogProbMetric: 36.4914, val_loss: 35.0133, val_MinusLogProbMetric: 35.0133

Epoch 32: val_loss improved from 35.21523 to 35.01328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 41s - loss: 36.4914 - MinusLogProbMetric: 36.4914 - val_loss: 35.0133 - val_MinusLogProbMetric: 35.0133 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 33/1000
2023-10-24 07:43:34.412 
Epoch 33/1000 
	 loss: 36.0337, MinusLogProbMetric: 36.0337, val_loss: 35.2955, val_MinusLogProbMetric: 35.2955

Epoch 33: val_loss did not improve from 35.01328
196/196 - 42s - loss: 36.0337 - MinusLogProbMetric: 36.0337 - val_loss: 35.2955 - val_MinusLogProbMetric: 35.2955 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 34/1000
2023-10-24 07:44:16.416 
Epoch 34/1000 
	 loss: 35.7484, MinusLogProbMetric: 35.7484, val_loss: 35.8702, val_MinusLogProbMetric: 35.8702

Epoch 34: val_loss did not improve from 35.01328
196/196 - 42s - loss: 35.7484 - MinusLogProbMetric: 35.7484 - val_loss: 35.8702 - val_MinusLogProbMetric: 35.8702 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 35/1000
2023-10-24 07:44:57.385 
Epoch 35/1000 
	 loss: 35.5534, MinusLogProbMetric: 35.5534, val_loss: 35.6595, val_MinusLogProbMetric: 35.6595

Epoch 35: val_loss did not improve from 35.01328
196/196 - 41s - loss: 35.5534 - MinusLogProbMetric: 35.5534 - val_loss: 35.6595 - val_MinusLogProbMetric: 35.6595 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 36/1000
2023-10-24 07:45:38.419 
Epoch 36/1000 
	 loss: 35.1431, MinusLogProbMetric: 35.1431, val_loss: 34.4617, val_MinusLogProbMetric: 34.4617

Epoch 36: val_loss improved from 35.01328 to 34.46172, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 42s - loss: 35.1431 - MinusLogProbMetric: 35.1431 - val_loss: 34.4617 - val_MinusLogProbMetric: 34.4617 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 37/1000
2023-10-24 07:46:19.790 
Epoch 37/1000 
	 loss: 36.6978, MinusLogProbMetric: 36.6978, val_loss: 36.4127, val_MinusLogProbMetric: 36.4127

Epoch 37: val_loss did not improve from 34.46172
196/196 - 41s - loss: 36.6978 - MinusLogProbMetric: 36.6978 - val_loss: 36.4127 - val_MinusLogProbMetric: 36.4127 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 38/1000
2023-10-24 07:47:01.516 
Epoch 38/1000 
	 loss: 34.7906, MinusLogProbMetric: 34.7906, val_loss: 33.9195, val_MinusLogProbMetric: 33.9195

Epoch 38: val_loss improved from 34.46172 to 33.91950, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 42s - loss: 34.7906 - MinusLogProbMetric: 34.7906 - val_loss: 33.9195 - val_MinusLogProbMetric: 33.9195 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 39/1000
2023-10-24 07:47:44.528 
Epoch 39/1000 
	 loss: 34.5124, MinusLogProbMetric: 34.5124, val_loss: 34.5666, val_MinusLogProbMetric: 34.5666

Epoch 39: val_loss did not improve from 33.91950
196/196 - 42s - loss: 34.5124 - MinusLogProbMetric: 34.5124 - val_loss: 34.5666 - val_MinusLogProbMetric: 34.5666 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 40/1000
2023-10-24 07:48:26.197 
Epoch 40/1000 
	 loss: 34.8081, MinusLogProbMetric: 34.8081, val_loss: 33.8297, val_MinusLogProbMetric: 33.8297

Epoch 40: val_loss improved from 33.91950 to 33.82969, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 42s - loss: 34.8081 - MinusLogProbMetric: 34.8081 - val_loss: 33.8297 - val_MinusLogProbMetric: 33.8297 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 41/1000
2023-10-24 07:49:06.575 
Epoch 41/1000 
	 loss: 34.1267, MinusLogProbMetric: 34.1267, val_loss: 33.6104, val_MinusLogProbMetric: 33.6104

Epoch 41: val_loss improved from 33.82969 to 33.61040, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 41s - loss: 34.1267 - MinusLogProbMetric: 34.1267 - val_loss: 33.6104 - val_MinusLogProbMetric: 33.6104 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 42/1000
2023-10-24 07:49:48.721 
Epoch 42/1000 
	 loss: 34.3600, MinusLogProbMetric: 34.3600, val_loss: 38.2960, val_MinusLogProbMetric: 38.2960

Epoch 42: val_loss did not improve from 33.61040
196/196 - 41s - loss: 34.3600 - MinusLogProbMetric: 34.3600 - val_loss: 38.2960 - val_MinusLogProbMetric: 38.2960 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 43/1000
2023-10-24 07:50:29.153 
Epoch 43/1000 
	 loss: 33.8685, MinusLogProbMetric: 33.8685, val_loss: 34.3529, val_MinusLogProbMetric: 34.3529

Epoch 43: val_loss did not improve from 33.61040
196/196 - 40s - loss: 33.8685 - MinusLogProbMetric: 33.8685 - val_loss: 34.3529 - val_MinusLogProbMetric: 34.3529 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 44/1000
2023-10-24 07:51:10.084 
Epoch 44/1000 
	 loss: 34.0571, MinusLogProbMetric: 34.0571, val_loss: 33.9201, val_MinusLogProbMetric: 33.9201

Epoch 44: val_loss did not improve from 33.61040
196/196 - 41s - loss: 34.0571 - MinusLogProbMetric: 34.0571 - val_loss: 33.9201 - val_MinusLogProbMetric: 33.9201 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 45/1000
2023-10-24 07:51:51.804 
Epoch 45/1000 
	 loss: 33.8540, MinusLogProbMetric: 33.8540, val_loss: 35.2582, val_MinusLogProbMetric: 35.2582

Epoch 45: val_loss did not improve from 33.61040
196/196 - 42s - loss: 33.8540 - MinusLogProbMetric: 33.8540 - val_loss: 35.2582 - val_MinusLogProbMetric: 35.2582 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 46/1000
2023-10-24 07:52:31.429 
Epoch 46/1000 
	 loss: 33.4828, MinusLogProbMetric: 33.4828, val_loss: 33.0699, val_MinusLogProbMetric: 33.0699

Epoch 46: val_loss improved from 33.61040 to 33.06988, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 40s - loss: 33.4828 - MinusLogProbMetric: 33.4828 - val_loss: 33.0699 - val_MinusLogProbMetric: 33.0699 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 47/1000
2023-10-24 07:53:13.718 
Epoch 47/1000 
	 loss: 33.6219, MinusLogProbMetric: 33.6219, val_loss: 34.1811, val_MinusLogProbMetric: 34.1811

Epoch 47: val_loss did not improve from 33.06988
196/196 - 42s - loss: 33.6219 - MinusLogProbMetric: 33.6219 - val_loss: 34.1811 - val_MinusLogProbMetric: 34.1811 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 48/1000
2023-10-24 07:53:53.926 
Epoch 48/1000 
	 loss: 33.4867, MinusLogProbMetric: 33.4867, val_loss: 33.0915, val_MinusLogProbMetric: 33.0915

Epoch 48: val_loss did not improve from 33.06988
196/196 - 40s - loss: 33.4867 - MinusLogProbMetric: 33.4867 - val_loss: 33.0915 - val_MinusLogProbMetric: 33.0915 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 49/1000
2023-10-24 07:54:35.154 
Epoch 49/1000 
	 loss: 33.3750, MinusLogProbMetric: 33.3750, val_loss: 34.8297, val_MinusLogProbMetric: 34.8297

Epoch 49: val_loss did not improve from 33.06988
196/196 - 41s - loss: 33.3750 - MinusLogProbMetric: 33.3750 - val_loss: 34.8297 - val_MinusLogProbMetric: 34.8297 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 50/1000
2023-10-24 07:55:16.070 
Epoch 50/1000 
	 loss: 34.1162, MinusLogProbMetric: 34.1162, val_loss: 35.5052, val_MinusLogProbMetric: 35.5052

Epoch 50: val_loss did not improve from 33.06988
196/196 - 41s - loss: 34.1162 - MinusLogProbMetric: 34.1162 - val_loss: 35.5052 - val_MinusLogProbMetric: 35.5052 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 51/1000
2023-10-24 07:55:55.614 
Epoch 51/1000 
	 loss: 33.4480, MinusLogProbMetric: 33.4480, val_loss: 33.5906, val_MinusLogProbMetric: 33.5906

Epoch 51: val_loss did not improve from 33.06988
196/196 - 40s - loss: 33.4480 - MinusLogProbMetric: 33.4480 - val_loss: 33.5906 - val_MinusLogProbMetric: 33.5906 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 52/1000
2023-10-24 07:56:36.328 
Epoch 52/1000 
	 loss: 33.3855, MinusLogProbMetric: 33.3855, val_loss: 33.6774, val_MinusLogProbMetric: 33.6774

Epoch 52: val_loss did not improve from 33.06988
196/196 - 41s - loss: 33.3855 - MinusLogProbMetric: 33.3855 - val_loss: 33.6774 - val_MinusLogProbMetric: 33.6774 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 53/1000
2023-10-24 07:57:16.265 
Epoch 53/1000 
	 loss: 33.3894, MinusLogProbMetric: 33.3894, val_loss: 33.4202, val_MinusLogProbMetric: 33.4202

Epoch 53: val_loss did not improve from 33.06988
196/196 - 40s - loss: 33.3894 - MinusLogProbMetric: 33.3894 - val_loss: 33.4202 - val_MinusLogProbMetric: 33.4202 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 54/1000
2023-10-24 07:57:57.543 
Epoch 54/1000 
	 loss: 33.1297, MinusLogProbMetric: 33.1297, val_loss: 33.1497, val_MinusLogProbMetric: 33.1497

Epoch 54: val_loss did not improve from 33.06988
196/196 - 41s - loss: 33.1297 - MinusLogProbMetric: 33.1297 - val_loss: 33.1497 - val_MinusLogProbMetric: 33.1497 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 55/1000
2023-10-24 07:58:36.933 
Epoch 55/1000 
	 loss: 33.0238, MinusLogProbMetric: 33.0238, val_loss: 33.8934, val_MinusLogProbMetric: 33.8934

Epoch 55: val_loss did not improve from 33.06988
196/196 - 39s - loss: 33.0238 - MinusLogProbMetric: 33.0238 - val_loss: 33.8934 - val_MinusLogProbMetric: 33.8934 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 56/1000
2023-10-24 07:59:15.845 
Epoch 56/1000 
	 loss: 32.6380, MinusLogProbMetric: 32.6380, val_loss: 33.9791, val_MinusLogProbMetric: 33.9791

Epoch 56: val_loss did not improve from 33.06988
196/196 - 39s - loss: 32.6380 - MinusLogProbMetric: 32.6380 - val_loss: 33.9791 - val_MinusLogProbMetric: 33.9791 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 57/1000
2023-10-24 07:59:55.649 
Epoch 57/1000 
	 loss: 32.6650, MinusLogProbMetric: 32.6650, val_loss: 32.7559, val_MinusLogProbMetric: 32.7559

Epoch 57: val_loss improved from 33.06988 to 32.75592, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 40s - loss: 32.6650 - MinusLogProbMetric: 32.6650 - val_loss: 32.7559 - val_MinusLogProbMetric: 32.7559 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 58/1000
2023-10-24 08:00:34.650 
Epoch 58/1000 
	 loss: 32.6803, MinusLogProbMetric: 32.6803, val_loss: 32.9407, val_MinusLogProbMetric: 32.9407

Epoch 58: val_loss did not improve from 32.75592
196/196 - 38s - loss: 32.6803 - MinusLogProbMetric: 32.6803 - val_loss: 32.9407 - val_MinusLogProbMetric: 32.9407 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 59/1000
2023-10-24 08:01:13.953 
Epoch 59/1000 
	 loss: 32.7118, MinusLogProbMetric: 32.7118, val_loss: 33.2590, val_MinusLogProbMetric: 33.2590

Epoch 59: val_loss did not improve from 32.75592
196/196 - 39s - loss: 32.7118 - MinusLogProbMetric: 32.7118 - val_loss: 33.2590 - val_MinusLogProbMetric: 33.2590 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 60/1000
2023-10-24 08:01:54.097 
Epoch 60/1000 
	 loss: 32.5283, MinusLogProbMetric: 32.5283, val_loss: 33.6112, val_MinusLogProbMetric: 33.6112

Epoch 60: val_loss did not improve from 32.75592
196/196 - 40s - loss: 32.5283 - MinusLogProbMetric: 32.5283 - val_loss: 33.6112 - val_MinusLogProbMetric: 33.6112 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 61/1000
2023-10-24 08:02:33.461 
Epoch 61/1000 
	 loss: 32.7149, MinusLogProbMetric: 32.7149, val_loss: 32.9326, val_MinusLogProbMetric: 32.9326

Epoch 61: val_loss did not improve from 32.75592
196/196 - 39s - loss: 32.7149 - MinusLogProbMetric: 32.7149 - val_loss: 32.9326 - val_MinusLogProbMetric: 32.9326 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 62/1000
2023-10-24 08:03:10.942 
Epoch 62/1000 
	 loss: 32.4526, MinusLogProbMetric: 32.4526, val_loss: 33.2180, val_MinusLogProbMetric: 33.2180

Epoch 62: val_loss did not improve from 32.75592
196/196 - 37s - loss: 32.4526 - MinusLogProbMetric: 32.4526 - val_loss: 33.2180 - val_MinusLogProbMetric: 33.2180 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 63/1000
2023-10-24 08:03:49.470 
Epoch 63/1000 
	 loss: 32.5124, MinusLogProbMetric: 32.5124, val_loss: 33.7442, val_MinusLogProbMetric: 33.7442

Epoch 63: val_loss did not improve from 32.75592
196/196 - 39s - loss: 32.5124 - MinusLogProbMetric: 32.5124 - val_loss: 33.7442 - val_MinusLogProbMetric: 33.7442 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 64/1000
2023-10-24 08:04:29.048 
Epoch 64/1000 
	 loss: 32.3929, MinusLogProbMetric: 32.3929, val_loss: 32.6124, val_MinusLogProbMetric: 32.6124

Epoch 64: val_loss improved from 32.75592 to 32.61235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 40s - loss: 32.3929 - MinusLogProbMetric: 32.3929 - val_loss: 32.6124 - val_MinusLogProbMetric: 32.6124 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 65/1000
2023-10-24 08:05:03.689 
Epoch 65/1000 
	 loss: 32.4903, MinusLogProbMetric: 32.4903, val_loss: 32.8394, val_MinusLogProbMetric: 32.8394

Epoch 65: val_loss did not improve from 32.61235
196/196 - 34s - loss: 32.4903 - MinusLogProbMetric: 32.4903 - val_loss: 32.8394 - val_MinusLogProbMetric: 32.8394 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 66/1000
2023-10-24 08:05:34.533 
Epoch 66/1000 
	 loss: 32.3444, MinusLogProbMetric: 32.3444, val_loss: 32.5256, val_MinusLogProbMetric: 32.5256

Epoch 66: val_loss improved from 32.61235 to 32.52555, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 31s - loss: 32.3444 - MinusLogProbMetric: 32.3444 - val_loss: 32.5256 - val_MinusLogProbMetric: 32.5256 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 67/1000
2023-10-24 08:06:07.447 
Epoch 67/1000 
	 loss: 32.1133, MinusLogProbMetric: 32.1133, val_loss: 33.4612, val_MinusLogProbMetric: 33.4612

Epoch 67: val_loss did not improve from 32.52555
196/196 - 32s - loss: 32.1133 - MinusLogProbMetric: 32.1133 - val_loss: 33.4612 - val_MinusLogProbMetric: 33.4612 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 68/1000
2023-10-24 08:06:40.597 
Epoch 68/1000 
	 loss: 32.4805, MinusLogProbMetric: 32.4805, val_loss: 32.2111, val_MinusLogProbMetric: 32.2111

Epoch 68: val_loss improved from 32.52555 to 32.21106, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 34s - loss: 32.4805 - MinusLogProbMetric: 32.4805 - val_loss: 32.2111 - val_MinusLogProbMetric: 32.2111 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 69/1000
2023-10-24 08:07:15.266 
Epoch 69/1000 
	 loss: 32.3975, MinusLogProbMetric: 32.3975, val_loss: 34.4942, val_MinusLogProbMetric: 34.4942

Epoch 69: val_loss did not improve from 32.21106
196/196 - 34s - loss: 32.3975 - MinusLogProbMetric: 32.3975 - val_loss: 34.4942 - val_MinusLogProbMetric: 34.4942 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 70/1000
2023-10-24 08:07:47.163 
Epoch 70/1000 
	 loss: 32.6778, MinusLogProbMetric: 32.6778, val_loss: 32.7280, val_MinusLogProbMetric: 32.7280

Epoch 70: val_loss did not improve from 32.21106
196/196 - 32s - loss: 32.6778 - MinusLogProbMetric: 32.6778 - val_loss: 32.7280 - val_MinusLogProbMetric: 32.7280 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 71/1000
2023-10-24 08:08:18.866 
Epoch 71/1000 
	 loss: 31.9619, MinusLogProbMetric: 31.9619, val_loss: 34.0793, val_MinusLogProbMetric: 34.0793

Epoch 71: val_loss did not improve from 32.21106
196/196 - 32s - loss: 31.9619 - MinusLogProbMetric: 31.9619 - val_loss: 34.0793 - val_MinusLogProbMetric: 34.0793 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 72/1000
2023-10-24 08:08:51.191 
Epoch 72/1000 
	 loss: 32.2132, MinusLogProbMetric: 32.2132, val_loss: 32.9279, val_MinusLogProbMetric: 32.9279

Epoch 72: val_loss did not improve from 32.21106
196/196 - 32s - loss: 32.2132 - MinusLogProbMetric: 32.2132 - val_loss: 32.9279 - val_MinusLogProbMetric: 32.9279 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 73/1000
2023-10-24 08:09:26.085 
Epoch 73/1000 
	 loss: 32.1278, MinusLogProbMetric: 32.1278, val_loss: 31.7383, val_MinusLogProbMetric: 31.7383

Epoch 73: val_loss improved from 32.21106 to 31.73825, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 36s - loss: 32.1278 - MinusLogProbMetric: 32.1278 - val_loss: 31.7383 - val_MinusLogProbMetric: 31.7383 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 74/1000
2023-10-24 08:10:00.179 
Epoch 74/1000 
	 loss: 31.9619, MinusLogProbMetric: 31.9619, val_loss: 31.5067, val_MinusLogProbMetric: 31.5067

Epoch 74: val_loss improved from 31.73825 to 31.50666, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 34s - loss: 31.9619 - MinusLogProbMetric: 31.9619 - val_loss: 31.5067 - val_MinusLogProbMetric: 31.5067 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 75/1000
2023-10-24 08:10:34.416 
Epoch 75/1000 
	 loss: 31.9368, MinusLogProbMetric: 31.9368, val_loss: 32.1749, val_MinusLogProbMetric: 32.1749

Epoch 75: val_loss did not improve from 31.50666
196/196 - 34s - loss: 31.9368 - MinusLogProbMetric: 31.9368 - val_loss: 32.1749 - val_MinusLogProbMetric: 32.1749 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 76/1000
2023-10-24 08:11:07.864 
Epoch 76/1000 
	 loss: 32.0568, MinusLogProbMetric: 32.0568, val_loss: 33.8377, val_MinusLogProbMetric: 33.8377

Epoch 76: val_loss did not improve from 31.50666
196/196 - 33s - loss: 32.0568 - MinusLogProbMetric: 32.0568 - val_loss: 33.8377 - val_MinusLogProbMetric: 33.8377 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 77/1000
2023-10-24 08:11:41.309 
Epoch 77/1000 
	 loss: 31.8840, MinusLogProbMetric: 31.8840, val_loss: 32.3261, val_MinusLogProbMetric: 32.3261

Epoch 77: val_loss did not improve from 31.50666
196/196 - 33s - loss: 31.8840 - MinusLogProbMetric: 31.8840 - val_loss: 32.3261 - val_MinusLogProbMetric: 32.3261 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 78/1000
2023-10-24 08:12:13.503 
Epoch 78/1000 
	 loss: 32.5495, MinusLogProbMetric: 32.5495, val_loss: 31.5559, val_MinusLogProbMetric: 31.5559

Epoch 78: val_loss did not improve from 31.50666
196/196 - 32s - loss: 32.5495 - MinusLogProbMetric: 32.5495 - val_loss: 31.5559 - val_MinusLogProbMetric: 31.5559 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 79/1000
2023-10-24 08:12:45.444 
Epoch 79/1000 
	 loss: 31.7513, MinusLogProbMetric: 31.7513, val_loss: 33.1359, val_MinusLogProbMetric: 33.1359

Epoch 79: val_loss did not improve from 31.50666
196/196 - 32s - loss: 31.7513 - MinusLogProbMetric: 31.7513 - val_loss: 33.1359 - val_MinusLogProbMetric: 33.1359 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 80/1000
2023-10-24 08:13:17.899 
Epoch 80/1000 
	 loss: 32.0150, MinusLogProbMetric: 32.0150, val_loss: 31.8380, val_MinusLogProbMetric: 31.8380

Epoch 80: val_loss did not improve from 31.50666
196/196 - 32s - loss: 32.0150 - MinusLogProbMetric: 32.0150 - val_loss: 31.8380 - val_MinusLogProbMetric: 31.8380 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 81/1000
2023-10-24 08:13:53.842 
Epoch 81/1000 
	 loss: 31.5589, MinusLogProbMetric: 31.5589, val_loss: 31.5286, val_MinusLogProbMetric: 31.5286

Epoch 81: val_loss did not improve from 31.50666
196/196 - 36s - loss: 31.5589 - MinusLogProbMetric: 31.5589 - val_loss: 31.5286 - val_MinusLogProbMetric: 31.5286 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 82/1000
2023-10-24 08:14:30.719 
Epoch 82/1000 
	 loss: 31.7067, MinusLogProbMetric: 31.7067, val_loss: 32.2521, val_MinusLogProbMetric: 32.2521

Epoch 82: val_loss did not improve from 31.50666
196/196 - 37s - loss: 31.7067 - MinusLogProbMetric: 31.7067 - val_loss: 32.2521 - val_MinusLogProbMetric: 32.2521 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 83/1000
2023-10-24 08:15:06.703 
Epoch 83/1000 
	 loss: 31.5721, MinusLogProbMetric: 31.5721, val_loss: 32.1020, val_MinusLogProbMetric: 32.1020

Epoch 83: val_loss did not improve from 31.50666
196/196 - 36s - loss: 31.5721 - MinusLogProbMetric: 31.5721 - val_loss: 32.1020 - val_MinusLogProbMetric: 32.1020 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 84/1000
2023-10-24 08:15:39.294 
Epoch 84/1000 
	 loss: 31.6607, MinusLogProbMetric: 31.6607, val_loss: 32.5188, val_MinusLogProbMetric: 32.5188

Epoch 84: val_loss did not improve from 31.50666
196/196 - 33s - loss: 31.6607 - MinusLogProbMetric: 31.6607 - val_loss: 32.5188 - val_MinusLogProbMetric: 32.5188 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 85/1000
2023-10-24 08:16:10.724 
Epoch 85/1000 
	 loss: 31.4146, MinusLogProbMetric: 31.4146, val_loss: 32.5433, val_MinusLogProbMetric: 32.5433

Epoch 85: val_loss did not improve from 31.50666
196/196 - 31s - loss: 31.4146 - MinusLogProbMetric: 31.4146 - val_loss: 32.5433 - val_MinusLogProbMetric: 32.5433 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 86/1000
2023-10-24 08:16:43.605 
Epoch 86/1000 
	 loss: 32.0163, MinusLogProbMetric: 32.0163, val_loss: 31.2091, val_MinusLogProbMetric: 31.2091

Epoch 86: val_loss improved from 31.50666 to 31.20909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 33s - loss: 32.0163 - MinusLogProbMetric: 32.0163 - val_loss: 31.2091 - val_MinusLogProbMetric: 31.2091 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 87/1000
2023-10-24 08:17:19.666 
Epoch 87/1000 
	 loss: 31.6058, MinusLogProbMetric: 31.6058, val_loss: 31.4831, val_MinusLogProbMetric: 31.4831

Epoch 87: val_loss did not improve from 31.20909
196/196 - 35s - loss: 31.6058 - MinusLogProbMetric: 31.6058 - val_loss: 31.4831 - val_MinusLogProbMetric: 31.4831 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 88/1000
2023-10-24 08:17:51.206 
Epoch 88/1000 
	 loss: 32.3124, MinusLogProbMetric: 32.3124, val_loss: 32.0692, val_MinusLogProbMetric: 32.0692

Epoch 88: val_loss did not improve from 31.20909
196/196 - 32s - loss: 32.3124 - MinusLogProbMetric: 32.3124 - val_loss: 32.0692 - val_MinusLogProbMetric: 32.0692 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 89/1000
2023-10-24 08:18:23.986 
Epoch 89/1000 
	 loss: 32.1565, MinusLogProbMetric: 32.1565, val_loss: 33.2573, val_MinusLogProbMetric: 33.2573

Epoch 89: val_loss did not improve from 31.20909
196/196 - 33s - loss: 32.1565 - MinusLogProbMetric: 32.1565 - val_loss: 33.2573 - val_MinusLogProbMetric: 33.2573 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 90/1000
2023-10-24 08:18:59.657 
Epoch 90/1000 
	 loss: 31.6370, MinusLogProbMetric: 31.6370, val_loss: 32.7356, val_MinusLogProbMetric: 32.7356

Epoch 90: val_loss did not improve from 31.20909
196/196 - 36s - loss: 31.6370 - MinusLogProbMetric: 31.6370 - val_loss: 32.7356 - val_MinusLogProbMetric: 32.7356 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 91/1000
2023-10-24 08:19:37.379 
Epoch 91/1000 
	 loss: 31.6163, MinusLogProbMetric: 31.6163, val_loss: 33.6810, val_MinusLogProbMetric: 33.6810

Epoch 91: val_loss did not improve from 31.20909
196/196 - 38s - loss: 31.6163 - MinusLogProbMetric: 31.6163 - val_loss: 33.6810 - val_MinusLogProbMetric: 33.6810 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 92/1000
2023-10-24 08:20:10.244 
Epoch 92/1000 
	 loss: 31.6966, MinusLogProbMetric: 31.6966, val_loss: 32.5378, val_MinusLogProbMetric: 32.5378

Epoch 92: val_loss did not improve from 31.20909
196/196 - 33s - loss: 31.6966 - MinusLogProbMetric: 31.6966 - val_loss: 32.5378 - val_MinusLogProbMetric: 32.5378 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 93/1000
2023-10-24 08:20:41.573 
Epoch 93/1000 
	 loss: 31.2729, MinusLogProbMetric: 31.2729, val_loss: 31.5351, val_MinusLogProbMetric: 31.5351

Epoch 93: val_loss did not improve from 31.20909
196/196 - 31s - loss: 31.2729 - MinusLogProbMetric: 31.2729 - val_loss: 31.5351 - val_MinusLogProbMetric: 31.5351 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 94/1000
2023-10-24 08:21:12.764 
Epoch 94/1000 
	 loss: 31.4792, MinusLogProbMetric: 31.4792, val_loss: 31.0620, val_MinusLogProbMetric: 31.0620

Epoch 94: val_loss improved from 31.20909 to 31.06195, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 32s - loss: 31.4792 - MinusLogProbMetric: 31.4792 - val_loss: 31.0620 - val_MinusLogProbMetric: 31.0620 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 95/1000
2023-10-24 08:21:48.416 
Epoch 95/1000 
	 loss: 31.3825, MinusLogProbMetric: 31.3825, val_loss: 33.1870, val_MinusLogProbMetric: 33.1870

Epoch 95: val_loss did not improve from 31.06195
196/196 - 35s - loss: 31.3825 - MinusLogProbMetric: 31.3825 - val_loss: 33.1870 - val_MinusLogProbMetric: 33.1870 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 96/1000
2023-10-24 08:22:23.504 
Epoch 96/1000 
	 loss: 31.2828, MinusLogProbMetric: 31.2828, val_loss: 31.1753, val_MinusLogProbMetric: 31.1753

Epoch 96: val_loss did not improve from 31.06195
196/196 - 35s - loss: 31.2828 - MinusLogProbMetric: 31.2828 - val_loss: 31.1753 - val_MinusLogProbMetric: 31.1753 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 97/1000
2023-10-24 08:22:55.242 
Epoch 97/1000 
	 loss: 31.2316, MinusLogProbMetric: 31.2316, val_loss: 31.3314, val_MinusLogProbMetric: 31.3314

Epoch 97: val_loss did not improve from 31.06195
196/196 - 32s - loss: 31.2316 - MinusLogProbMetric: 31.2316 - val_loss: 31.3314 - val_MinusLogProbMetric: 31.3314 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 98/1000
2023-10-24 08:23:28.514 
Epoch 98/1000 
	 loss: 31.5475, MinusLogProbMetric: 31.5475, val_loss: 31.9630, val_MinusLogProbMetric: 31.9630

Epoch 98: val_loss did not improve from 31.06195
196/196 - 33s - loss: 31.5475 - MinusLogProbMetric: 31.5475 - val_loss: 31.9630 - val_MinusLogProbMetric: 31.9630 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 99/1000
2023-10-24 08:24:00.778 
Epoch 99/1000 
	 loss: 31.0426, MinusLogProbMetric: 31.0426, val_loss: 31.4961, val_MinusLogProbMetric: 31.4961

Epoch 99: val_loss did not improve from 31.06195
196/196 - 32s - loss: 31.0426 - MinusLogProbMetric: 31.0426 - val_loss: 31.4961 - val_MinusLogProbMetric: 31.4961 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 100/1000
2023-10-24 08:24:35.266 
Epoch 100/1000 
	 loss: 31.1730, MinusLogProbMetric: 31.1730, val_loss: 30.9616, val_MinusLogProbMetric: 30.9616

Epoch 100: val_loss improved from 31.06195 to 30.96162, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 35s - loss: 31.1730 - MinusLogProbMetric: 31.1730 - val_loss: 30.9616 - val_MinusLogProbMetric: 30.9616 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 101/1000
2023-10-24 08:25:07.920 
Epoch 101/1000 
	 loss: 31.2801, MinusLogProbMetric: 31.2801, val_loss: 31.8097, val_MinusLogProbMetric: 31.8097

Epoch 101: val_loss did not improve from 30.96162
196/196 - 32s - loss: 31.2801 - MinusLogProbMetric: 31.2801 - val_loss: 31.8097 - val_MinusLogProbMetric: 31.8097 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 102/1000
2023-10-24 08:25:39.786 
Epoch 102/1000 
	 loss: 31.5375, MinusLogProbMetric: 31.5375, val_loss: 31.8390, val_MinusLogProbMetric: 31.8390

Epoch 102: val_loss did not improve from 30.96162
196/196 - 32s - loss: 31.5375 - MinusLogProbMetric: 31.5375 - val_loss: 31.8390 - val_MinusLogProbMetric: 31.8390 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 103/1000
2023-10-24 08:26:11.389 
Epoch 103/1000 
	 loss: 31.2162, MinusLogProbMetric: 31.2162, val_loss: 34.4225, val_MinusLogProbMetric: 34.4225

Epoch 103: val_loss did not improve from 30.96162
196/196 - 32s - loss: 31.2162 - MinusLogProbMetric: 31.2162 - val_loss: 34.4225 - val_MinusLogProbMetric: 34.4225 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 104/1000
2023-10-24 08:26:46.862 
Epoch 104/1000 
	 loss: 31.1330, MinusLogProbMetric: 31.1330, val_loss: 31.4773, val_MinusLogProbMetric: 31.4773

Epoch 104: val_loss did not improve from 30.96162
196/196 - 35s - loss: 31.1330 - MinusLogProbMetric: 31.1330 - val_loss: 31.4773 - val_MinusLogProbMetric: 31.4773 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 105/1000
2023-10-24 08:27:21.635 
Epoch 105/1000 
	 loss: 31.1680, MinusLogProbMetric: 31.1680, val_loss: 32.3937, val_MinusLogProbMetric: 32.3937

Epoch 105: val_loss did not improve from 30.96162
196/196 - 35s - loss: 31.1680 - MinusLogProbMetric: 31.1680 - val_loss: 32.3937 - val_MinusLogProbMetric: 32.3937 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 106/1000
2023-10-24 08:27:55.063 
Epoch 106/1000 
	 loss: 31.0607, MinusLogProbMetric: 31.0607, val_loss: 31.4836, val_MinusLogProbMetric: 31.4836

Epoch 106: val_loss did not improve from 30.96162
196/196 - 33s - loss: 31.0607 - MinusLogProbMetric: 31.0607 - val_loss: 31.4836 - val_MinusLogProbMetric: 31.4836 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 107/1000
2023-10-24 08:28:29.830 
Epoch 107/1000 
	 loss: 30.9610, MinusLogProbMetric: 30.9610, val_loss: 32.5542, val_MinusLogProbMetric: 32.5542

Epoch 107: val_loss did not improve from 30.96162
196/196 - 35s - loss: 30.9610 - MinusLogProbMetric: 30.9610 - val_loss: 32.5542 - val_MinusLogProbMetric: 32.5542 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 108/1000
2023-10-24 08:29:03.102 
Epoch 108/1000 
	 loss: 31.1085, MinusLogProbMetric: 31.1085, val_loss: 31.1066, val_MinusLogProbMetric: 31.1066

Epoch 108: val_loss did not improve from 30.96162
196/196 - 33s - loss: 31.1085 - MinusLogProbMetric: 31.1085 - val_loss: 31.1066 - val_MinusLogProbMetric: 31.1066 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 109/1000
2023-10-24 08:29:37.841 
Epoch 109/1000 
	 loss: 30.9773, MinusLogProbMetric: 30.9773, val_loss: 31.8870, val_MinusLogProbMetric: 31.8870

Epoch 109: val_loss did not improve from 30.96162
196/196 - 35s - loss: 30.9773 - MinusLogProbMetric: 30.9773 - val_loss: 31.8870 - val_MinusLogProbMetric: 31.8870 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 110/1000
2023-10-24 08:30:13.153 
Epoch 110/1000 
	 loss: 30.8782, MinusLogProbMetric: 30.8782, val_loss: 31.9862, val_MinusLogProbMetric: 31.9862

Epoch 110: val_loss did not improve from 30.96162
196/196 - 35s - loss: 30.8782 - MinusLogProbMetric: 30.8782 - val_loss: 31.9862 - val_MinusLogProbMetric: 31.9862 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 111/1000
2023-10-24 08:30:44.572 
Epoch 111/1000 
	 loss: 31.0279, MinusLogProbMetric: 31.0279, val_loss: 32.9385, val_MinusLogProbMetric: 32.9385

Epoch 111: val_loss did not improve from 30.96162
196/196 - 31s - loss: 31.0279 - MinusLogProbMetric: 31.0279 - val_loss: 32.9385 - val_MinusLogProbMetric: 32.9385 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 112/1000
2023-10-24 08:31:16.296 
Epoch 112/1000 
	 loss: 31.0720, MinusLogProbMetric: 31.0720, val_loss: 31.0998, val_MinusLogProbMetric: 31.0998

Epoch 112: val_loss did not improve from 30.96162
196/196 - 32s - loss: 31.0720 - MinusLogProbMetric: 31.0720 - val_loss: 31.0998 - val_MinusLogProbMetric: 31.0998 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 113/1000
2023-10-24 08:31:49.915 
Epoch 113/1000 
	 loss: 31.1467, MinusLogProbMetric: 31.1467, val_loss: 31.5686, val_MinusLogProbMetric: 31.5686

Epoch 113: val_loss did not improve from 30.96162
196/196 - 34s - loss: 31.1467 - MinusLogProbMetric: 31.1467 - val_loss: 31.5686 - val_MinusLogProbMetric: 31.5686 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 114/1000
2023-10-24 08:32:25.367 
Epoch 114/1000 
	 loss: 30.8522, MinusLogProbMetric: 30.8522, val_loss: 30.9959, val_MinusLogProbMetric: 30.9959

Epoch 114: val_loss did not improve from 30.96162
196/196 - 35s - loss: 30.8522 - MinusLogProbMetric: 30.8522 - val_loss: 30.9959 - val_MinusLogProbMetric: 30.9959 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 115/1000
2023-10-24 08:33:00.885 
Epoch 115/1000 
	 loss: 30.8746, MinusLogProbMetric: 30.8746, val_loss: 31.1812, val_MinusLogProbMetric: 31.1812

Epoch 115: val_loss did not improve from 30.96162
196/196 - 36s - loss: 30.8746 - MinusLogProbMetric: 30.8746 - val_loss: 31.1812 - val_MinusLogProbMetric: 31.1812 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 116/1000
2023-10-24 08:33:33.439 
Epoch 116/1000 
	 loss: 30.9123, MinusLogProbMetric: 30.9123, val_loss: 31.3904, val_MinusLogProbMetric: 31.3904

Epoch 116: val_loss did not improve from 30.96162
196/196 - 33s - loss: 30.9123 - MinusLogProbMetric: 30.9123 - val_loss: 31.3904 - val_MinusLogProbMetric: 31.3904 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 117/1000
2023-10-24 08:34:05.453 
Epoch 117/1000 
	 loss: 30.8254, MinusLogProbMetric: 30.8254, val_loss: 31.4346, val_MinusLogProbMetric: 31.4346

Epoch 117: val_loss did not improve from 30.96162
196/196 - 32s - loss: 30.8254 - MinusLogProbMetric: 30.8254 - val_loss: 31.4346 - val_MinusLogProbMetric: 31.4346 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 118/1000
2023-10-24 08:34:40.869 
Epoch 118/1000 
	 loss: 30.7514, MinusLogProbMetric: 30.7514, val_loss: 32.8995, val_MinusLogProbMetric: 32.8995

Epoch 118: val_loss did not improve from 30.96162
196/196 - 35s - loss: 30.7514 - MinusLogProbMetric: 30.7514 - val_loss: 32.8995 - val_MinusLogProbMetric: 32.8995 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 119/1000
2023-10-24 08:35:14.582 
Epoch 119/1000 
	 loss: 31.0141, MinusLogProbMetric: 31.0141, val_loss: 31.5053, val_MinusLogProbMetric: 31.5053

Epoch 119: val_loss did not improve from 30.96162
196/196 - 34s - loss: 31.0141 - MinusLogProbMetric: 31.0141 - val_loss: 31.5053 - val_MinusLogProbMetric: 31.5053 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 120/1000
2023-10-24 08:35:45.646 
Epoch 120/1000 
	 loss: 30.7899, MinusLogProbMetric: 30.7899, val_loss: 31.9259, val_MinusLogProbMetric: 31.9259

Epoch 120: val_loss did not improve from 30.96162
196/196 - 31s - loss: 30.7899 - MinusLogProbMetric: 30.7899 - val_loss: 31.9259 - val_MinusLogProbMetric: 31.9259 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 121/1000
2023-10-24 08:36:16.622 
Epoch 121/1000 
	 loss: 30.7457, MinusLogProbMetric: 30.7457, val_loss: 31.9543, val_MinusLogProbMetric: 31.9543

Epoch 121: val_loss did not improve from 30.96162
196/196 - 31s - loss: 30.7457 - MinusLogProbMetric: 30.7457 - val_loss: 31.9543 - val_MinusLogProbMetric: 31.9543 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 122/1000
2023-10-24 08:36:47.293 
Epoch 122/1000 
	 loss: 30.9036, MinusLogProbMetric: 30.9036, val_loss: 30.7242, val_MinusLogProbMetric: 30.7242

Epoch 122: val_loss improved from 30.96162 to 30.72416, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 31s - loss: 30.9036 - MinusLogProbMetric: 30.9036 - val_loss: 30.7242 - val_MinusLogProbMetric: 30.7242 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 123/1000
2023-10-24 08:37:22.070 
Epoch 123/1000 
	 loss: 30.6810, MinusLogProbMetric: 30.6810, val_loss: 30.9917, val_MinusLogProbMetric: 30.9917

Epoch 123: val_loss did not improve from 30.72416
196/196 - 34s - loss: 30.6810 - MinusLogProbMetric: 30.6810 - val_loss: 30.9917 - val_MinusLogProbMetric: 30.9917 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 124/1000
2023-10-24 08:37:57.029 
Epoch 124/1000 
	 loss: 30.8196, MinusLogProbMetric: 30.8196, val_loss: 30.6321, val_MinusLogProbMetric: 30.6321

Epoch 124: val_loss improved from 30.72416 to 30.63214, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 36s - loss: 30.8196 - MinusLogProbMetric: 30.8196 - val_loss: 30.6321 - val_MinusLogProbMetric: 30.6321 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 125/1000
2023-10-24 08:38:31.853 
Epoch 125/1000 
	 loss: 30.7855, MinusLogProbMetric: 30.7855, val_loss: 30.6735, val_MinusLogProbMetric: 30.6735

Epoch 125: val_loss did not improve from 30.63214
196/196 - 34s - loss: 30.7855 - MinusLogProbMetric: 30.7855 - val_loss: 30.6735 - val_MinusLogProbMetric: 30.6735 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 126/1000
2023-10-24 08:39:03.018 
Epoch 126/1000 
	 loss: 30.6627, MinusLogProbMetric: 30.6627, val_loss: 31.3703, val_MinusLogProbMetric: 31.3703

Epoch 126: val_loss did not improve from 30.63214
196/196 - 31s - loss: 30.6627 - MinusLogProbMetric: 30.6627 - val_loss: 31.3703 - val_MinusLogProbMetric: 31.3703 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 127/1000
2023-10-24 08:39:33.875 
Epoch 127/1000 
	 loss: 30.7644, MinusLogProbMetric: 30.7644, val_loss: 31.1959, val_MinusLogProbMetric: 31.1959

Epoch 127: val_loss did not improve from 30.63214
196/196 - 31s - loss: 30.7644 - MinusLogProbMetric: 30.7644 - val_loss: 31.1959 - val_MinusLogProbMetric: 31.1959 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 128/1000
2023-10-24 08:40:08.732 
Epoch 128/1000 
	 loss: 30.7707, MinusLogProbMetric: 30.7707, val_loss: 30.9012, val_MinusLogProbMetric: 30.9012

Epoch 128: val_loss did not improve from 30.63214
196/196 - 35s - loss: 30.7707 - MinusLogProbMetric: 30.7707 - val_loss: 30.9012 - val_MinusLogProbMetric: 30.9012 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 129/1000
2023-10-24 08:40:44.344 
Epoch 129/1000 
	 loss: 30.6207, MinusLogProbMetric: 30.6207, val_loss: 31.4977, val_MinusLogProbMetric: 31.4977

Epoch 129: val_loss did not improve from 30.63214
196/196 - 36s - loss: 30.6207 - MinusLogProbMetric: 30.6207 - val_loss: 31.4977 - val_MinusLogProbMetric: 31.4977 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 130/1000
2023-10-24 08:41:23.317 
Epoch 130/1000 
	 loss: 30.7676, MinusLogProbMetric: 30.7676, val_loss: 30.9859, val_MinusLogProbMetric: 30.9859

Epoch 130: val_loss did not improve from 30.63214
196/196 - 39s - loss: 30.7676 - MinusLogProbMetric: 30.7676 - val_loss: 30.9859 - val_MinusLogProbMetric: 30.9859 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 131/1000
2023-10-24 08:42:00.543 
Epoch 131/1000 
	 loss: 30.6480, MinusLogProbMetric: 30.6480, val_loss: 31.8737, val_MinusLogProbMetric: 31.8737

Epoch 131: val_loss did not improve from 30.63214
196/196 - 37s - loss: 30.6480 - MinusLogProbMetric: 30.6480 - val_loss: 31.8737 - val_MinusLogProbMetric: 31.8737 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 132/1000
2023-10-24 08:42:35.951 
Epoch 132/1000 
	 loss: 30.4937, MinusLogProbMetric: 30.4937, val_loss: 30.9560, val_MinusLogProbMetric: 30.9560

Epoch 132: val_loss did not improve from 30.63214
196/196 - 35s - loss: 30.4937 - MinusLogProbMetric: 30.4937 - val_loss: 30.9560 - val_MinusLogProbMetric: 30.9560 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 133/1000
2023-10-24 08:43:07.285 
Epoch 133/1000 
	 loss: 30.5477, MinusLogProbMetric: 30.5477, val_loss: 31.1000, val_MinusLogProbMetric: 31.1000

Epoch 133: val_loss did not improve from 30.63214
196/196 - 31s - loss: 30.5477 - MinusLogProbMetric: 30.5477 - val_loss: 31.1000 - val_MinusLogProbMetric: 31.1000 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 134/1000
2023-10-24 08:43:39.359 
Epoch 134/1000 
	 loss: 30.5116, MinusLogProbMetric: 30.5116, val_loss: 31.3135, val_MinusLogProbMetric: 31.3135

Epoch 134: val_loss did not improve from 30.63214
196/196 - 32s - loss: 30.5116 - MinusLogProbMetric: 30.5116 - val_loss: 31.3135 - val_MinusLogProbMetric: 31.3135 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 135/1000
2023-10-24 08:44:11.483 
Epoch 135/1000 
	 loss: 30.7249, MinusLogProbMetric: 30.7249, val_loss: 31.1726, val_MinusLogProbMetric: 31.1726

Epoch 135: val_loss did not improve from 30.63214
196/196 - 32s - loss: 30.7249 - MinusLogProbMetric: 30.7249 - val_loss: 31.1726 - val_MinusLogProbMetric: 31.1726 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 136/1000
2023-10-24 08:44:45.559 
Epoch 136/1000 
	 loss: 30.5677, MinusLogProbMetric: 30.5677, val_loss: 30.2679, val_MinusLogProbMetric: 30.2679

Epoch 136: val_loss improved from 30.63214 to 30.26788, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 35s - loss: 30.5677 - MinusLogProbMetric: 30.5677 - val_loss: 30.2679 - val_MinusLogProbMetric: 30.2679 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 137/1000
2023-10-24 08:45:22.705 
Epoch 137/1000 
	 loss: 30.4319, MinusLogProbMetric: 30.4319, val_loss: 30.8901, val_MinusLogProbMetric: 30.8901

Epoch 137: val_loss did not improve from 30.26788
196/196 - 36s - loss: 30.4319 - MinusLogProbMetric: 30.4319 - val_loss: 30.8901 - val_MinusLogProbMetric: 30.8901 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 138/1000
2023-10-24 08:45:58.043 
Epoch 138/1000 
	 loss: 30.6143, MinusLogProbMetric: 30.6143, val_loss: 30.8783, val_MinusLogProbMetric: 30.8783

Epoch 138: val_loss did not improve from 30.26788
196/196 - 35s - loss: 30.6143 - MinusLogProbMetric: 30.6143 - val_loss: 30.8783 - val_MinusLogProbMetric: 30.8783 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 139/1000
2023-10-24 08:46:34.354 
Epoch 139/1000 
	 loss: 30.3969, MinusLogProbMetric: 30.3969, val_loss: 30.7763, val_MinusLogProbMetric: 30.7763

Epoch 139: val_loss did not improve from 30.26788
196/196 - 36s - loss: 30.3969 - MinusLogProbMetric: 30.3969 - val_loss: 30.7763 - val_MinusLogProbMetric: 30.7763 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 140/1000
2023-10-24 08:47:09.283 
Epoch 140/1000 
	 loss: 30.5154, MinusLogProbMetric: 30.5154, val_loss: 30.5981, val_MinusLogProbMetric: 30.5981

Epoch 140: val_loss did not improve from 30.26788
196/196 - 35s - loss: 30.5154 - MinusLogProbMetric: 30.5154 - val_loss: 30.5981 - val_MinusLogProbMetric: 30.5981 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 141/1000
2023-10-24 08:47:44.241 
Epoch 141/1000 
	 loss: 30.4574, MinusLogProbMetric: 30.4574, val_loss: 31.2545, val_MinusLogProbMetric: 31.2545

Epoch 141: val_loss did not improve from 30.26788
196/196 - 35s - loss: 30.4574 - MinusLogProbMetric: 30.4574 - val_loss: 31.2545 - val_MinusLogProbMetric: 31.2545 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 142/1000
2023-10-24 08:48:17.002 
Epoch 142/1000 
	 loss: 30.3613, MinusLogProbMetric: 30.3613, val_loss: 30.6292, val_MinusLogProbMetric: 30.6292

Epoch 142: val_loss did not improve from 30.26788
196/196 - 33s - loss: 30.3613 - MinusLogProbMetric: 30.3613 - val_loss: 30.6292 - val_MinusLogProbMetric: 30.6292 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 143/1000
2023-10-24 08:48:49.705 
Epoch 143/1000 
	 loss: 30.5914, MinusLogProbMetric: 30.5914, val_loss: 30.5488, val_MinusLogProbMetric: 30.5488

Epoch 143: val_loss did not improve from 30.26788
196/196 - 33s - loss: 30.5914 - MinusLogProbMetric: 30.5914 - val_loss: 30.5488 - val_MinusLogProbMetric: 30.5488 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 144/1000
2023-10-24 08:49:25.612 
Epoch 144/1000 
	 loss: 30.5291, MinusLogProbMetric: 30.5291, val_loss: 30.5495, val_MinusLogProbMetric: 30.5495

Epoch 144: val_loss did not improve from 30.26788
196/196 - 36s - loss: 30.5291 - MinusLogProbMetric: 30.5291 - val_loss: 30.5495 - val_MinusLogProbMetric: 30.5495 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 145/1000
2023-10-24 08:50:03.957 
Epoch 145/1000 
	 loss: 30.4367, MinusLogProbMetric: 30.4367, val_loss: 30.9171, val_MinusLogProbMetric: 30.9171

Epoch 145: val_loss did not improve from 30.26788
196/196 - 38s - loss: 30.4367 - MinusLogProbMetric: 30.4367 - val_loss: 30.9171 - val_MinusLogProbMetric: 30.9171 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 146/1000
2023-10-24 08:50:41.193 
Epoch 146/1000 
	 loss: 30.3247, MinusLogProbMetric: 30.3247, val_loss: 32.0185, val_MinusLogProbMetric: 32.0185

Epoch 146: val_loss did not improve from 30.26788
196/196 - 37s - loss: 30.3247 - MinusLogProbMetric: 30.3247 - val_loss: 32.0185 - val_MinusLogProbMetric: 32.0185 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 147/1000
2023-10-24 08:51:16.763 
Epoch 147/1000 
	 loss: 30.5435, MinusLogProbMetric: 30.5435, val_loss: 31.1093, val_MinusLogProbMetric: 31.1093

Epoch 147: val_loss did not improve from 30.26788
196/196 - 36s - loss: 30.5435 - MinusLogProbMetric: 30.5435 - val_loss: 31.1093 - val_MinusLogProbMetric: 31.1093 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 148/1000
2023-10-24 08:51:52.998 
Epoch 148/1000 
	 loss: 30.4742, MinusLogProbMetric: 30.4742, val_loss: 31.3398, val_MinusLogProbMetric: 31.3398

Epoch 148: val_loss did not improve from 30.26788
196/196 - 36s - loss: 30.4742 - MinusLogProbMetric: 30.4742 - val_loss: 31.3398 - val_MinusLogProbMetric: 31.3398 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 149/1000
2023-10-24 08:52:29.362 
Epoch 149/1000 
	 loss: 30.4135, MinusLogProbMetric: 30.4135, val_loss: 30.5010, val_MinusLogProbMetric: 30.5010

Epoch 149: val_loss did not improve from 30.26788
196/196 - 36s - loss: 30.4135 - MinusLogProbMetric: 30.4135 - val_loss: 30.5010 - val_MinusLogProbMetric: 30.5010 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 150/1000
2023-10-24 08:53:06.286 
Epoch 150/1000 
	 loss: 30.5212, MinusLogProbMetric: 30.5212, val_loss: 30.4230, val_MinusLogProbMetric: 30.4230

Epoch 150: val_loss did not improve from 30.26788
196/196 - 37s - loss: 30.5212 - MinusLogProbMetric: 30.5212 - val_loss: 30.4230 - val_MinusLogProbMetric: 30.4230 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 151/1000
2023-10-24 08:53:42.096 
Epoch 151/1000 
	 loss: 30.4080, MinusLogProbMetric: 30.4080, val_loss: 30.7614, val_MinusLogProbMetric: 30.7614

Epoch 151: val_loss did not improve from 30.26788
196/196 - 36s - loss: 30.4080 - MinusLogProbMetric: 30.4080 - val_loss: 30.7614 - val_MinusLogProbMetric: 30.7614 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 152/1000
2023-10-24 08:54:17.830 
Epoch 152/1000 
	 loss: 30.2112, MinusLogProbMetric: 30.2112, val_loss: 30.7405, val_MinusLogProbMetric: 30.7405

Epoch 152: val_loss did not improve from 30.26788
196/196 - 36s - loss: 30.2112 - MinusLogProbMetric: 30.2112 - val_loss: 30.7405 - val_MinusLogProbMetric: 30.7405 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 153/1000
2023-10-24 08:54:54.523 
Epoch 153/1000 
	 loss: 30.3132, MinusLogProbMetric: 30.3132, val_loss: 30.8212, val_MinusLogProbMetric: 30.8212

Epoch 153: val_loss did not improve from 30.26788
196/196 - 37s - loss: 30.3132 - MinusLogProbMetric: 30.3132 - val_loss: 30.8212 - val_MinusLogProbMetric: 30.8212 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 154/1000
2023-10-24 08:55:30.531 
Epoch 154/1000 
	 loss: 30.4125, MinusLogProbMetric: 30.4125, val_loss: 30.3157, val_MinusLogProbMetric: 30.3157

Epoch 154: val_loss did not improve from 30.26788
196/196 - 36s - loss: 30.4125 - MinusLogProbMetric: 30.4125 - val_loss: 30.3157 - val_MinusLogProbMetric: 30.3157 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 155/1000
2023-10-24 08:56:06.950 
Epoch 155/1000 
	 loss: 30.3039, MinusLogProbMetric: 30.3039, val_loss: 30.5848, val_MinusLogProbMetric: 30.5848

Epoch 155: val_loss did not improve from 30.26788
196/196 - 36s - loss: 30.3039 - MinusLogProbMetric: 30.3039 - val_loss: 30.5848 - val_MinusLogProbMetric: 30.5848 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 156/1000
2023-10-24 08:56:42.883 
Epoch 156/1000 
	 loss: 30.2926, MinusLogProbMetric: 30.2926, val_loss: 30.7903, val_MinusLogProbMetric: 30.7903

Epoch 156: val_loss did not improve from 30.26788
196/196 - 36s - loss: 30.2926 - MinusLogProbMetric: 30.2926 - val_loss: 30.7903 - val_MinusLogProbMetric: 30.7903 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 157/1000
2023-10-24 08:57:18.782 
Epoch 157/1000 
	 loss: 30.3562, MinusLogProbMetric: 30.3562, val_loss: 31.4171, val_MinusLogProbMetric: 31.4171

Epoch 157: val_loss did not improve from 30.26788
196/196 - 36s - loss: 30.3562 - MinusLogProbMetric: 30.3562 - val_loss: 31.4171 - val_MinusLogProbMetric: 31.4171 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 158/1000
2023-10-24 08:57:55.693 
Epoch 158/1000 
	 loss: 30.3026, MinusLogProbMetric: 30.3026, val_loss: 30.5067, val_MinusLogProbMetric: 30.5067

Epoch 158: val_loss did not improve from 30.26788
196/196 - 37s - loss: 30.3026 - MinusLogProbMetric: 30.3026 - val_loss: 30.5067 - val_MinusLogProbMetric: 30.5067 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 159/1000
2023-10-24 08:58:31.820 
Epoch 159/1000 
	 loss: 30.2929, MinusLogProbMetric: 30.2929, val_loss: 30.4729, val_MinusLogProbMetric: 30.4729

Epoch 159: val_loss did not improve from 30.26788
196/196 - 36s - loss: 30.2929 - MinusLogProbMetric: 30.2929 - val_loss: 30.4729 - val_MinusLogProbMetric: 30.4729 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 160/1000
2023-10-24 08:59:07.780 
Epoch 160/1000 
	 loss: 30.3404, MinusLogProbMetric: 30.3404, val_loss: 32.5943, val_MinusLogProbMetric: 32.5943

Epoch 160: val_loss did not improve from 30.26788
196/196 - 36s - loss: 30.3404 - MinusLogProbMetric: 30.3404 - val_loss: 32.5943 - val_MinusLogProbMetric: 32.5943 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 161/1000
2023-10-24 08:59:45.990 
Epoch 161/1000 
	 loss: 30.2912, MinusLogProbMetric: 30.2912, val_loss: 32.3921, val_MinusLogProbMetric: 32.3921

Epoch 161: val_loss did not improve from 30.26788
196/196 - 38s - loss: 30.2912 - MinusLogProbMetric: 30.2912 - val_loss: 32.3921 - val_MinusLogProbMetric: 32.3921 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 162/1000
2023-10-24 09:00:24.668 
Epoch 162/1000 
	 loss: 30.3141, MinusLogProbMetric: 30.3141, val_loss: 30.5898, val_MinusLogProbMetric: 30.5898

Epoch 162: val_loss did not improve from 30.26788
196/196 - 39s - loss: 30.3141 - MinusLogProbMetric: 30.3141 - val_loss: 30.5898 - val_MinusLogProbMetric: 30.5898 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 163/1000
2023-10-24 09:01:02.380 
Epoch 163/1000 
	 loss: 30.1982, MinusLogProbMetric: 30.1982, val_loss: 30.6826, val_MinusLogProbMetric: 30.6826

Epoch 163: val_loss did not improve from 30.26788
196/196 - 38s - loss: 30.1982 - MinusLogProbMetric: 30.1982 - val_loss: 30.6826 - val_MinusLogProbMetric: 30.6826 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 164/1000
2023-10-24 09:01:41.264 
Epoch 164/1000 
	 loss: 30.2191, MinusLogProbMetric: 30.2191, val_loss: 30.5169, val_MinusLogProbMetric: 30.5169

Epoch 164: val_loss did not improve from 30.26788
196/196 - 39s - loss: 30.2191 - MinusLogProbMetric: 30.2191 - val_loss: 30.5169 - val_MinusLogProbMetric: 30.5169 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 165/1000
2023-10-24 09:02:19.299 
Epoch 165/1000 
	 loss: 30.0558, MinusLogProbMetric: 30.0558, val_loss: 30.5412, val_MinusLogProbMetric: 30.5412

Epoch 165: val_loss did not improve from 30.26788
196/196 - 38s - loss: 30.0558 - MinusLogProbMetric: 30.0558 - val_loss: 30.5412 - val_MinusLogProbMetric: 30.5412 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 166/1000
2023-10-24 09:02:56.284 
Epoch 166/1000 
	 loss: 30.2483, MinusLogProbMetric: 30.2483, val_loss: 30.6832, val_MinusLogProbMetric: 30.6832

Epoch 166: val_loss did not improve from 30.26788
196/196 - 37s - loss: 30.2483 - MinusLogProbMetric: 30.2483 - val_loss: 30.6832 - val_MinusLogProbMetric: 30.6832 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 167/1000
2023-10-24 09:03:34.560 
Epoch 167/1000 
	 loss: 30.1953, MinusLogProbMetric: 30.1953, val_loss: 30.2949, val_MinusLogProbMetric: 30.2949

Epoch 167: val_loss did not improve from 30.26788
196/196 - 38s - loss: 30.1953 - MinusLogProbMetric: 30.1953 - val_loss: 30.2949 - val_MinusLogProbMetric: 30.2949 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 168/1000
2023-10-24 09:04:12.077 
Epoch 168/1000 
	 loss: 30.2573, MinusLogProbMetric: 30.2573, val_loss: 30.9192, val_MinusLogProbMetric: 30.9192

Epoch 168: val_loss did not improve from 30.26788
196/196 - 38s - loss: 30.2573 - MinusLogProbMetric: 30.2573 - val_loss: 30.9192 - val_MinusLogProbMetric: 30.9192 - lr: 0.0010 - 38s/epoch - 191ms/step
Epoch 169/1000
2023-10-24 09:04:50.027 
Epoch 169/1000 
	 loss: 30.2523, MinusLogProbMetric: 30.2523, val_loss: 30.5992, val_MinusLogProbMetric: 30.5992

Epoch 169: val_loss did not improve from 30.26788
196/196 - 38s - loss: 30.2523 - MinusLogProbMetric: 30.2523 - val_loss: 30.5992 - val_MinusLogProbMetric: 30.5992 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 170/1000
2023-10-24 09:05:28.831 
Epoch 170/1000 
	 loss: 30.1459, MinusLogProbMetric: 30.1459, val_loss: 30.6686, val_MinusLogProbMetric: 30.6686

Epoch 170: val_loss did not improve from 30.26788
196/196 - 39s - loss: 30.1459 - MinusLogProbMetric: 30.1459 - val_loss: 30.6686 - val_MinusLogProbMetric: 30.6686 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 171/1000
2023-10-24 09:06:06.525 
Epoch 171/1000 
	 loss: 30.2525, MinusLogProbMetric: 30.2525, val_loss: 31.1943, val_MinusLogProbMetric: 31.1943

Epoch 171: val_loss did not improve from 30.26788
196/196 - 38s - loss: 30.2525 - MinusLogProbMetric: 30.2525 - val_loss: 31.1943 - val_MinusLogProbMetric: 31.1943 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 172/1000
2023-10-24 09:06:44.884 
Epoch 172/1000 
	 loss: 29.9914, MinusLogProbMetric: 29.9914, val_loss: 30.6225, val_MinusLogProbMetric: 30.6225

Epoch 172: val_loss did not improve from 30.26788
196/196 - 38s - loss: 29.9914 - MinusLogProbMetric: 29.9914 - val_loss: 30.6225 - val_MinusLogProbMetric: 30.6225 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 173/1000
2023-10-24 09:07:23.517 
Epoch 173/1000 
	 loss: 30.2335, MinusLogProbMetric: 30.2335, val_loss: 31.0106, val_MinusLogProbMetric: 31.0106

Epoch 173: val_loss did not improve from 30.26788
196/196 - 39s - loss: 30.2335 - MinusLogProbMetric: 30.2335 - val_loss: 31.0106 - val_MinusLogProbMetric: 31.0106 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 174/1000
2023-10-24 09:08:02.520 
Epoch 174/1000 
	 loss: 30.3533, MinusLogProbMetric: 30.3533, val_loss: 31.3686, val_MinusLogProbMetric: 31.3686

Epoch 174: val_loss did not improve from 30.26788
196/196 - 39s - loss: 30.3533 - MinusLogProbMetric: 30.3533 - val_loss: 31.3686 - val_MinusLogProbMetric: 31.3686 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 175/1000
2023-10-24 09:08:41.310 
Epoch 175/1000 
	 loss: 30.0655, MinusLogProbMetric: 30.0655, val_loss: 30.5504, val_MinusLogProbMetric: 30.5504

Epoch 175: val_loss did not improve from 30.26788
196/196 - 39s - loss: 30.0655 - MinusLogProbMetric: 30.0655 - val_loss: 30.5504 - val_MinusLogProbMetric: 30.5504 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 176/1000
2023-10-24 09:09:19.804 
Epoch 176/1000 
	 loss: 30.0343, MinusLogProbMetric: 30.0343, val_loss: 30.0935, val_MinusLogProbMetric: 30.0935

Epoch 176: val_loss improved from 30.26788 to 30.09352, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 39s - loss: 30.0343 - MinusLogProbMetric: 30.0343 - val_loss: 30.0935 - val_MinusLogProbMetric: 30.0935 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 177/1000
2023-10-24 09:09:58.919 
Epoch 177/1000 
	 loss: 30.4540, MinusLogProbMetric: 30.4540, val_loss: 31.7436, val_MinusLogProbMetric: 31.7436

Epoch 177: val_loss did not improve from 30.09352
196/196 - 38s - loss: 30.4540 - MinusLogProbMetric: 30.4540 - val_loss: 31.7436 - val_MinusLogProbMetric: 31.7436 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 178/1000
2023-10-24 09:10:37.677 
Epoch 178/1000 
	 loss: 30.0512, MinusLogProbMetric: 30.0512, val_loss: 30.9856, val_MinusLogProbMetric: 30.9856

Epoch 178: val_loss did not improve from 30.09352
196/196 - 39s - loss: 30.0512 - MinusLogProbMetric: 30.0512 - val_loss: 30.9856 - val_MinusLogProbMetric: 30.9856 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 179/1000
2023-10-24 09:11:16.332 
Epoch 179/1000 
	 loss: 30.1898, MinusLogProbMetric: 30.1898, val_loss: 33.7738, val_MinusLogProbMetric: 33.7738

Epoch 179: val_loss did not improve from 30.09352
196/196 - 39s - loss: 30.1898 - MinusLogProbMetric: 30.1898 - val_loss: 33.7738 - val_MinusLogProbMetric: 33.7738 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 180/1000
2023-10-24 09:11:53.102 
Epoch 180/1000 
	 loss: 36.3111, MinusLogProbMetric: 36.3111, val_loss: 35.0870, val_MinusLogProbMetric: 35.0870

Epoch 180: val_loss did not improve from 30.09352
196/196 - 37s - loss: 36.3111 - MinusLogProbMetric: 36.3111 - val_loss: 35.0870 - val_MinusLogProbMetric: 35.0870 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 181/1000
2023-10-24 09:12:31.162 
Epoch 181/1000 
	 loss: 32.2298, MinusLogProbMetric: 32.2298, val_loss: 31.5698, val_MinusLogProbMetric: 31.5698

Epoch 181: val_loss did not improve from 30.09352
196/196 - 38s - loss: 32.2298 - MinusLogProbMetric: 32.2298 - val_loss: 31.5698 - val_MinusLogProbMetric: 31.5698 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 182/1000
2023-10-24 09:13:09.441 
Epoch 182/1000 
	 loss: 31.2512, MinusLogProbMetric: 31.2512, val_loss: 32.1472, val_MinusLogProbMetric: 32.1472

Epoch 182: val_loss did not improve from 30.09352
196/196 - 38s - loss: 31.2512 - MinusLogProbMetric: 31.2512 - val_loss: 32.1472 - val_MinusLogProbMetric: 32.1472 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 183/1000
2023-10-24 09:13:47.392 
Epoch 183/1000 
	 loss: 31.3019, MinusLogProbMetric: 31.3019, val_loss: 31.4393, val_MinusLogProbMetric: 31.4393

Epoch 183: val_loss did not improve from 30.09352
196/196 - 38s - loss: 31.3019 - MinusLogProbMetric: 31.3019 - val_loss: 31.4393 - val_MinusLogProbMetric: 31.4393 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 184/1000
2023-10-24 09:14:24.027 
Epoch 184/1000 
	 loss: 30.7445, MinusLogProbMetric: 30.7445, val_loss: 31.2576, val_MinusLogProbMetric: 31.2576

Epoch 184: val_loss did not improve from 30.09352
196/196 - 37s - loss: 30.7445 - MinusLogProbMetric: 30.7445 - val_loss: 31.2576 - val_MinusLogProbMetric: 31.2576 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 185/1000
2023-10-24 09:15:00.556 
Epoch 185/1000 
	 loss: 30.7783, MinusLogProbMetric: 30.7783, val_loss: 31.5191, val_MinusLogProbMetric: 31.5191

Epoch 185: val_loss did not improve from 30.09352
196/196 - 37s - loss: 30.7783 - MinusLogProbMetric: 30.7783 - val_loss: 31.5191 - val_MinusLogProbMetric: 31.5191 - lr: 0.0010 - 37s/epoch - 186ms/step
Epoch 186/1000
2023-10-24 09:15:37.945 
Epoch 186/1000 
	 loss: 30.6907, MinusLogProbMetric: 30.6907, val_loss: 31.8433, val_MinusLogProbMetric: 31.8433

Epoch 186: val_loss did not improve from 30.09352
196/196 - 37s - loss: 30.6907 - MinusLogProbMetric: 30.6907 - val_loss: 31.8433 - val_MinusLogProbMetric: 31.8433 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 187/1000
2023-10-24 09:16:13.319 
Epoch 187/1000 
	 loss: 30.6007, MinusLogProbMetric: 30.6007, val_loss: 32.5143, val_MinusLogProbMetric: 32.5143

Epoch 187: val_loss did not improve from 30.09352
196/196 - 35s - loss: 30.6007 - MinusLogProbMetric: 30.6007 - val_loss: 32.5143 - val_MinusLogProbMetric: 32.5143 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 188/1000
2023-10-24 09:16:50.966 
Epoch 188/1000 
	 loss: 30.6207, MinusLogProbMetric: 30.6207, val_loss: 32.4366, val_MinusLogProbMetric: 32.4366

Epoch 188: val_loss did not improve from 30.09352
196/196 - 38s - loss: 30.6207 - MinusLogProbMetric: 30.6207 - val_loss: 32.4366 - val_MinusLogProbMetric: 32.4366 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 189/1000
2023-10-24 09:17:27.926 
Epoch 189/1000 
	 loss: 30.6813, MinusLogProbMetric: 30.6813, val_loss: 32.9186, val_MinusLogProbMetric: 32.9186

Epoch 189: val_loss did not improve from 30.09352
196/196 - 37s - loss: 30.6813 - MinusLogProbMetric: 30.6813 - val_loss: 32.9186 - val_MinusLogProbMetric: 32.9186 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 190/1000
2023-10-24 09:18:06.654 
Epoch 190/1000 
	 loss: 30.3348, MinusLogProbMetric: 30.3348, val_loss: 30.4665, val_MinusLogProbMetric: 30.4665

Epoch 190: val_loss did not improve from 30.09352
196/196 - 39s - loss: 30.3348 - MinusLogProbMetric: 30.3348 - val_loss: 30.4665 - val_MinusLogProbMetric: 30.4665 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 191/1000
2023-10-24 09:18:45.208 
Epoch 191/1000 
	 loss: 30.4538, MinusLogProbMetric: 30.4538, val_loss: 32.1381, val_MinusLogProbMetric: 32.1381

Epoch 191: val_loss did not improve from 30.09352
196/196 - 39s - loss: 30.4538 - MinusLogProbMetric: 30.4538 - val_loss: 32.1381 - val_MinusLogProbMetric: 32.1381 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 192/1000
2023-10-24 09:19:22.215 
Epoch 192/1000 
	 loss: 30.4623, MinusLogProbMetric: 30.4623, val_loss: 30.4877, val_MinusLogProbMetric: 30.4877

Epoch 192: val_loss did not improve from 30.09352
196/196 - 37s - loss: 30.4623 - MinusLogProbMetric: 30.4623 - val_loss: 30.4877 - val_MinusLogProbMetric: 30.4877 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 193/1000
2023-10-24 09:19:57.778 
Epoch 193/1000 
	 loss: 30.3015, MinusLogProbMetric: 30.3015, val_loss: 31.6366, val_MinusLogProbMetric: 31.6366

Epoch 193: val_loss did not improve from 30.09352
196/196 - 36s - loss: 30.3015 - MinusLogProbMetric: 30.3015 - val_loss: 31.6366 - val_MinusLogProbMetric: 31.6366 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 194/1000
2023-10-24 09:20:35.541 
Epoch 194/1000 
	 loss: 30.2498, MinusLogProbMetric: 30.2498, val_loss: 30.2482, val_MinusLogProbMetric: 30.2482

Epoch 194: val_loss did not improve from 30.09352
196/196 - 38s - loss: 30.2498 - MinusLogProbMetric: 30.2498 - val_loss: 30.2482 - val_MinusLogProbMetric: 30.2482 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 195/1000
2023-10-24 09:21:12.854 
Epoch 195/1000 
	 loss: 30.2723, MinusLogProbMetric: 30.2723, val_loss: 30.4171, val_MinusLogProbMetric: 30.4171

Epoch 195: val_loss did not improve from 30.09352
196/196 - 37s - loss: 30.2723 - MinusLogProbMetric: 30.2723 - val_loss: 30.4171 - val_MinusLogProbMetric: 30.4171 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 196/1000
2023-10-24 09:21:48.219 
Epoch 196/1000 
	 loss: 30.2535, MinusLogProbMetric: 30.2535, val_loss: 30.7868, val_MinusLogProbMetric: 30.7868

Epoch 196: val_loss did not improve from 30.09352
196/196 - 35s - loss: 30.2535 - MinusLogProbMetric: 30.2535 - val_loss: 30.7868 - val_MinusLogProbMetric: 30.7868 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 197/1000
2023-10-24 09:22:26.648 
Epoch 197/1000 
	 loss: 30.1405, MinusLogProbMetric: 30.1405, val_loss: 30.5878, val_MinusLogProbMetric: 30.5878

Epoch 197: val_loss did not improve from 30.09352
196/196 - 38s - loss: 30.1405 - MinusLogProbMetric: 30.1405 - val_loss: 30.5878 - val_MinusLogProbMetric: 30.5878 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 198/1000
2023-10-24 09:23:04.259 
Epoch 198/1000 
	 loss: 30.2547, MinusLogProbMetric: 30.2547, val_loss: 34.2362, val_MinusLogProbMetric: 34.2362

Epoch 198: val_loss did not improve from 30.09352
196/196 - 38s - loss: 30.2547 - MinusLogProbMetric: 30.2547 - val_loss: 34.2362 - val_MinusLogProbMetric: 34.2362 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 199/1000
2023-10-24 09:23:40.720 
Epoch 199/1000 
	 loss: 30.4703, MinusLogProbMetric: 30.4703, val_loss: 30.0209, val_MinusLogProbMetric: 30.0209

Epoch 199: val_loss improved from 30.09352 to 30.02094, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 37s - loss: 30.4703 - MinusLogProbMetric: 30.4703 - val_loss: 30.0209 - val_MinusLogProbMetric: 30.0209 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 200/1000
2023-10-24 09:24:17.150 
Epoch 200/1000 
	 loss: 29.9397, MinusLogProbMetric: 29.9397, val_loss: 31.0881, val_MinusLogProbMetric: 31.0881

Epoch 200: val_loss did not improve from 30.02094
196/196 - 36s - loss: 29.9397 - MinusLogProbMetric: 29.9397 - val_loss: 31.0881 - val_MinusLogProbMetric: 31.0881 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 201/1000
2023-10-24 09:24:54.912 
Epoch 201/1000 
	 loss: 30.2516, MinusLogProbMetric: 30.2516, val_loss: 31.3638, val_MinusLogProbMetric: 31.3638

Epoch 201: val_loss did not improve from 30.02094
196/196 - 38s - loss: 30.2516 - MinusLogProbMetric: 30.2516 - val_loss: 31.3638 - val_MinusLogProbMetric: 31.3638 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 202/1000
2023-10-24 09:25:30.241 
Epoch 202/1000 
	 loss: 30.1019, MinusLogProbMetric: 30.1019, val_loss: 30.8640, val_MinusLogProbMetric: 30.8640

Epoch 202: val_loss did not improve from 30.02094
196/196 - 35s - loss: 30.1019 - MinusLogProbMetric: 30.1019 - val_loss: 30.8640 - val_MinusLogProbMetric: 30.8640 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 203/1000
2023-10-24 09:26:09.032 
Epoch 203/1000 
	 loss: 30.2361, MinusLogProbMetric: 30.2361, val_loss: 30.3356, val_MinusLogProbMetric: 30.3356

Epoch 203: val_loss did not improve from 30.02094
196/196 - 39s - loss: 30.2361 - MinusLogProbMetric: 30.2361 - val_loss: 30.3356 - val_MinusLogProbMetric: 30.3356 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 204/1000
2023-10-24 09:26:47.564 
Epoch 204/1000 
	 loss: 30.1258, MinusLogProbMetric: 30.1258, val_loss: 30.9417, val_MinusLogProbMetric: 30.9417

Epoch 204: val_loss did not improve from 30.02094
196/196 - 39s - loss: 30.1258 - MinusLogProbMetric: 30.1258 - val_loss: 30.9417 - val_MinusLogProbMetric: 30.9417 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 205/1000
2023-10-24 09:27:26.043 
Epoch 205/1000 
	 loss: 30.0690, MinusLogProbMetric: 30.0690, val_loss: 32.0674, val_MinusLogProbMetric: 32.0674

Epoch 205: val_loss did not improve from 30.02094
196/196 - 38s - loss: 30.0690 - MinusLogProbMetric: 30.0690 - val_loss: 32.0674 - val_MinusLogProbMetric: 32.0674 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 206/1000
2023-10-24 09:28:04.428 
Epoch 206/1000 
	 loss: 30.0715, MinusLogProbMetric: 30.0715, val_loss: 30.7554, val_MinusLogProbMetric: 30.7554

Epoch 206: val_loss did not improve from 30.02094
196/196 - 38s - loss: 30.0715 - MinusLogProbMetric: 30.0715 - val_loss: 30.7554 - val_MinusLogProbMetric: 30.7554 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 207/1000
2023-10-24 09:28:41.226 
Epoch 207/1000 
	 loss: 30.2353, MinusLogProbMetric: 30.2353, val_loss: 31.7597, val_MinusLogProbMetric: 31.7597

Epoch 207: val_loss did not improve from 30.02094
196/196 - 37s - loss: 30.2353 - MinusLogProbMetric: 30.2353 - val_loss: 31.7597 - val_MinusLogProbMetric: 31.7597 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 208/1000
2023-10-24 09:29:16.770 
Epoch 208/1000 
	 loss: 30.0941, MinusLogProbMetric: 30.0941, val_loss: 30.0839, val_MinusLogProbMetric: 30.0839

Epoch 208: val_loss did not improve from 30.02094
196/196 - 36s - loss: 30.0941 - MinusLogProbMetric: 30.0941 - val_loss: 30.0839 - val_MinusLogProbMetric: 30.0839 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 209/1000
2023-10-24 09:29:52.918 
Epoch 209/1000 
	 loss: 29.9993, MinusLogProbMetric: 29.9993, val_loss: 31.0993, val_MinusLogProbMetric: 31.0993

Epoch 209: val_loss did not improve from 30.02094
196/196 - 36s - loss: 29.9993 - MinusLogProbMetric: 29.9993 - val_loss: 31.0993 - val_MinusLogProbMetric: 31.0993 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 210/1000
2023-10-24 09:30:27.746 
Epoch 210/1000 
	 loss: 29.9403, MinusLogProbMetric: 29.9403, val_loss: 30.4971, val_MinusLogProbMetric: 30.4971

Epoch 210: val_loss did not improve from 30.02094
196/196 - 35s - loss: 29.9403 - MinusLogProbMetric: 29.9403 - val_loss: 30.4971 - val_MinusLogProbMetric: 30.4971 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 211/1000
2023-10-24 09:31:03.900 
Epoch 211/1000 
	 loss: 29.8249, MinusLogProbMetric: 29.8249, val_loss: 30.3929, val_MinusLogProbMetric: 30.3929

Epoch 211: val_loss did not improve from 30.02094
196/196 - 36s - loss: 29.8249 - MinusLogProbMetric: 29.8249 - val_loss: 30.3929 - val_MinusLogProbMetric: 30.3929 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 212/1000
2023-10-24 09:31:42.413 
Epoch 212/1000 
	 loss: 30.0510, MinusLogProbMetric: 30.0510, val_loss: 31.0670, val_MinusLogProbMetric: 31.0670

Epoch 212: val_loss did not improve from 30.02094
196/196 - 39s - loss: 30.0510 - MinusLogProbMetric: 30.0510 - val_loss: 31.0670 - val_MinusLogProbMetric: 31.0670 - lr: 0.0010 - 39s/epoch - 196ms/step
Epoch 213/1000
2023-10-24 09:32:18.357 
Epoch 213/1000 
	 loss: 29.8476, MinusLogProbMetric: 29.8476, val_loss: 30.2930, val_MinusLogProbMetric: 30.2930

Epoch 213: val_loss did not improve from 30.02094
196/196 - 36s - loss: 29.8476 - MinusLogProbMetric: 29.8476 - val_loss: 30.2930 - val_MinusLogProbMetric: 30.2930 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 214/1000
2023-10-24 09:32:53.942 
Epoch 214/1000 
	 loss: 29.7581, MinusLogProbMetric: 29.7581, val_loss: 30.1924, val_MinusLogProbMetric: 30.1924

Epoch 214: val_loss did not improve from 30.02094
196/196 - 36s - loss: 29.7581 - MinusLogProbMetric: 29.7581 - val_loss: 30.1924 - val_MinusLogProbMetric: 30.1924 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 215/1000
2023-10-24 09:33:29.294 
Epoch 215/1000 
	 loss: 29.9572, MinusLogProbMetric: 29.9572, val_loss: 30.3892, val_MinusLogProbMetric: 30.3892

Epoch 215: val_loss did not improve from 30.02094
196/196 - 35s - loss: 29.9572 - MinusLogProbMetric: 29.9572 - val_loss: 30.3892 - val_MinusLogProbMetric: 30.3892 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 216/1000
2023-10-24 09:34:07.543 
Epoch 216/1000 
	 loss: 30.2435, MinusLogProbMetric: 30.2435, val_loss: 30.0766, val_MinusLogProbMetric: 30.0766

Epoch 216: val_loss did not improve from 30.02094
196/196 - 38s - loss: 30.2435 - MinusLogProbMetric: 30.2435 - val_loss: 30.0766 - val_MinusLogProbMetric: 30.0766 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 217/1000
2023-10-24 09:34:44.457 
Epoch 217/1000 
	 loss: 29.7477, MinusLogProbMetric: 29.7477, val_loss: 29.9943, val_MinusLogProbMetric: 29.9943

Epoch 217: val_loss improved from 30.02094 to 29.99433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 38s - loss: 29.7477 - MinusLogProbMetric: 29.7477 - val_loss: 29.9943 - val_MinusLogProbMetric: 29.9943 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 218/1000
2023-10-24 09:35:23.380 
Epoch 218/1000 
	 loss: 29.8633, MinusLogProbMetric: 29.8633, val_loss: 31.0032, val_MinusLogProbMetric: 31.0032

Epoch 218: val_loss did not improve from 29.99433
196/196 - 38s - loss: 29.8633 - MinusLogProbMetric: 29.8633 - val_loss: 31.0032 - val_MinusLogProbMetric: 31.0032 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 219/1000
2023-10-24 09:36:01.968 
Epoch 219/1000 
	 loss: 29.9974, MinusLogProbMetric: 29.9974, val_loss: 30.0163, val_MinusLogProbMetric: 30.0163

Epoch 219: val_loss did not improve from 29.99433
196/196 - 39s - loss: 29.9974 - MinusLogProbMetric: 29.9974 - val_loss: 30.0163 - val_MinusLogProbMetric: 30.0163 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 220/1000
2023-10-24 09:36:40.912 
Epoch 220/1000 
	 loss: 29.9517, MinusLogProbMetric: 29.9517, val_loss: 32.2315, val_MinusLogProbMetric: 32.2315

Epoch 220: val_loss did not improve from 29.99433
196/196 - 39s - loss: 29.9517 - MinusLogProbMetric: 29.9517 - val_loss: 32.2315 - val_MinusLogProbMetric: 32.2315 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 221/1000
2023-10-24 09:37:18.656 
Epoch 221/1000 
	 loss: 29.9625, MinusLogProbMetric: 29.9625, val_loss: 30.4251, val_MinusLogProbMetric: 30.4251

Epoch 221: val_loss did not improve from 29.99433
196/196 - 38s - loss: 29.9625 - MinusLogProbMetric: 29.9625 - val_loss: 30.4251 - val_MinusLogProbMetric: 30.4251 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 222/1000
2023-10-24 09:37:56.219 
Epoch 222/1000 
	 loss: 30.0268, MinusLogProbMetric: 30.0268, val_loss: 30.5328, val_MinusLogProbMetric: 30.5328

Epoch 222: val_loss did not improve from 29.99433
196/196 - 38s - loss: 30.0268 - MinusLogProbMetric: 30.0268 - val_loss: 30.5328 - val_MinusLogProbMetric: 30.5328 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 223/1000
2023-10-24 09:38:34.370 
Epoch 223/1000 
	 loss: 29.8002, MinusLogProbMetric: 29.8002, val_loss: 32.3656, val_MinusLogProbMetric: 32.3656

Epoch 223: val_loss did not improve from 29.99433
196/196 - 38s - loss: 29.8002 - MinusLogProbMetric: 29.8002 - val_loss: 32.3656 - val_MinusLogProbMetric: 32.3656 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 224/1000
2023-10-24 09:39:12.959 
Epoch 224/1000 
	 loss: 29.6882, MinusLogProbMetric: 29.6882, val_loss: 32.3598, val_MinusLogProbMetric: 32.3598

Epoch 224: val_loss did not improve from 29.99433
196/196 - 39s - loss: 29.6882 - MinusLogProbMetric: 29.6882 - val_loss: 32.3598 - val_MinusLogProbMetric: 32.3598 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 225/1000
2023-10-24 09:39:51.538 
Epoch 225/1000 
	 loss: 29.8631, MinusLogProbMetric: 29.8631, val_loss: 30.9220, val_MinusLogProbMetric: 30.9220

Epoch 225: val_loss did not improve from 29.99433
196/196 - 39s - loss: 29.8631 - MinusLogProbMetric: 29.8631 - val_loss: 30.9220 - val_MinusLogProbMetric: 30.9220 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 226/1000
2023-10-24 09:40:30.480 
Epoch 226/1000 
	 loss: 30.0966, MinusLogProbMetric: 30.0966, val_loss: 30.1520, val_MinusLogProbMetric: 30.1520

Epoch 226: val_loss did not improve from 29.99433
196/196 - 39s - loss: 30.0966 - MinusLogProbMetric: 30.0966 - val_loss: 30.1520 - val_MinusLogProbMetric: 30.1520 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 227/1000
2023-10-24 09:41:08.911 
Epoch 227/1000 
	 loss: 29.6003, MinusLogProbMetric: 29.6003, val_loss: 30.5917, val_MinusLogProbMetric: 30.5917

Epoch 227: val_loss did not improve from 29.99433
196/196 - 38s - loss: 29.6003 - MinusLogProbMetric: 29.6003 - val_loss: 30.5917 - val_MinusLogProbMetric: 30.5917 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 228/1000
2023-10-24 09:41:47.422 
Epoch 228/1000 
	 loss: 30.1616, MinusLogProbMetric: 30.1616, val_loss: 30.3935, val_MinusLogProbMetric: 30.3935

Epoch 228: val_loss did not improve from 29.99433
196/196 - 39s - loss: 30.1616 - MinusLogProbMetric: 30.1616 - val_loss: 30.3935 - val_MinusLogProbMetric: 30.3935 - lr: 0.0010 - 39s/epoch - 196ms/step
Epoch 229/1000
2023-10-24 09:42:26.479 
Epoch 229/1000 
	 loss: 29.7837, MinusLogProbMetric: 29.7837, val_loss: 30.7079, val_MinusLogProbMetric: 30.7079

Epoch 229: val_loss did not improve from 29.99433
196/196 - 39s - loss: 29.7837 - MinusLogProbMetric: 29.7837 - val_loss: 30.7079 - val_MinusLogProbMetric: 30.7079 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 230/1000
2023-10-24 09:43:04.588 
Epoch 230/1000 
	 loss: 29.8722, MinusLogProbMetric: 29.8722, val_loss: 31.6746, val_MinusLogProbMetric: 31.6746

Epoch 230: val_loss did not improve from 29.99433
196/196 - 38s - loss: 29.8722 - MinusLogProbMetric: 29.8722 - val_loss: 31.6746 - val_MinusLogProbMetric: 31.6746 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 231/1000
2023-10-24 09:43:41.415 
Epoch 231/1000 
	 loss: 29.7394, MinusLogProbMetric: 29.7394, val_loss: 30.0695, val_MinusLogProbMetric: 30.0695

Epoch 231: val_loss did not improve from 29.99433
196/196 - 37s - loss: 29.7394 - MinusLogProbMetric: 29.7394 - val_loss: 30.0695 - val_MinusLogProbMetric: 30.0695 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 232/1000
2023-10-24 09:44:20.360 
Epoch 232/1000 
	 loss: 29.7116, MinusLogProbMetric: 29.7116, val_loss: 30.5478, val_MinusLogProbMetric: 30.5478

Epoch 232: val_loss did not improve from 29.99433
196/196 - 39s - loss: 29.7116 - MinusLogProbMetric: 29.7116 - val_loss: 30.5478 - val_MinusLogProbMetric: 30.5478 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 233/1000
2023-10-24 09:44:59.153 
Epoch 233/1000 
	 loss: 29.7617, MinusLogProbMetric: 29.7617, val_loss: 30.4569, val_MinusLogProbMetric: 30.4569

Epoch 233: val_loss did not improve from 29.99433
196/196 - 39s - loss: 29.7617 - MinusLogProbMetric: 29.7617 - val_loss: 30.4569 - val_MinusLogProbMetric: 30.4569 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 234/1000
2023-10-24 09:45:37.925 
Epoch 234/1000 
	 loss: 29.6378, MinusLogProbMetric: 29.6378, val_loss: 30.2222, val_MinusLogProbMetric: 30.2222

Epoch 234: val_loss did not improve from 29.99433
196/196 - 39s - loss: 29.6378 - MinusLogProbMetric: 29.6378 - val_loss: 30.2222 - val_MinusLogProbMetric: 30.2222 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 235/1000
2023-10-24 09:46:16.824 
Epoch 235/1000 
	 loss: 29.6852, MinusLogProbMetric: 29.6852, val_loss: 30.9962, val_MinusLogProbMetric: 30.9962

Epoch 235: val_loss did not improve from 29.99433
196/196 - 39s - loss: 29.6852 - MinusLogProbMetric: 29.6852 - val_loss: 30.9962 - val_MinusLogProbMetric: 30.9962 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 236/1000
2023-10-24 09:46:55.428 
Epoch 236/1000 
	 loss: 29.9308, MinusLogProbMetric: 29.9308, val_loss: 30.3735, val_MinusLogProbMetric: 30.3735

Epoch 236: val_loss did not improve from 29.99433
196/196 - 39s - loss: 29.9308 - MinusLogProbMetric: 29.9308 - val_loss: 30.3735 - val_MinusLogProbMetric: 30.3735 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 237/1000
2023-10-24 09:47:34.349 
Epoch 237/1000 
	 loss: 29.6388, MinusLogProbMetric: 29.6388, val_loss: 29.9948, val_MinusLogProbMetric: 29.9948

Epoch 237: val_loss did not improve from 29.99433
196/196 - 39s - loss: 29.6388 - MinusLogProbMetric: 29.6388 - val_loss: 29.9948 - val_MinusLogProbMetric: 29.9948 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 238/1000
2023-10-24 09:48:12.589 
Epoch 238/1000 
	 loss: 29.7808, MinusLogProbMetric: 29.7808, val_loss: 29.8908, val_MinusLogProbMetric: 29.8908

Epoch 238: val_loss improved from 29.99433 to 29.89075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 39s - loss: 29.7808 - MinusLogProbMetric: 29.7808 - val_loss: 29.8908 - val_MinusLogProbMetric: 29.8908 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 239/1000
2023-10-24 09:48:51.080 
Epoch 239/1000 
	 loss: 29.7546, MinusLogProbMetric: 29.7546, val_loss: 29.9013, val_MinusLogProbMetric: 29.9013

Epoch 239: val_loss did not improve from 29.89075
196/196 - 38s - loss: 29.7546 - MinusLogProbMetric: 29.7546 - val_loss: 29.9013 - val_MinusLogProbMetric: 29.9013 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 240/1000
2023-10-24 09:49:30.028 
Epoch 240/1000 
	 loss: 29.8286, MinusLogProbMetric: 29.8286, val_loss: 30.2055, val_MinusLogProbMetric: 30.2055

Epoch 240: val_loss did not improve from 29.89075
196/196 - 39s - loss: 29.8286 - MinusLogProbMetric: 29.8286 - val_loss: 30.2055 - val_MinusLogProbMetric: 30.2055 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 241/1000
2023-10-24 09:50:08.414 
Epoch 241/1000 
	 loss: 29.6550, MinusLogProbMetric: 29.6550, val_loss: 31.0935, val_MinusLogProbMetric: 31.0935

Epoch 241: val_loss did not improve from 29.89075
196/196 - 38s - loss: 29.6550 - MinusLogProbMetric: 29.6550 - val_loss: 31.0935 - val_MinusLogProbMetric: 31.0935 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 242/1000
2023-10-24 09:50:47.264 
Epoch 242/1000 
	 loss: 29.6860, MinusLogProbMetric: 29.6860, val_loss: 29.9067, val_MinusLogProbMetric: 29.9067

Epoch 242: val_loss did not improve from 29.89075
196/196 - 39s - loss: 29.6860 - MinusLogProbMetric: 29.6860 - val_loss: 29.9067 - val_MinusLogProbMetric: 29.9067 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 243/1000
2023-10-24 09:51:25.701 
Epoch 243/1000 
	 loss: 29.7600, MinusLogProbMetric: 29.7600, val_loss: 30.9972, val_MinusLogProbMetric: 30.9972

Epoch 243: val_loss did not improve from 29.89075
196/196 - 38s - loss: 29.7600 - MinusLogProbMetric: 29.7600 - val_loss: 30.9972 - val_MinusLogProbMetric: 30.9972 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 244/1000
2023-10-24 09:52:04.445 
Epoch 244/1000 
	 loss: 29.5361, MinusLogProbMetric: 29.5361, val_loss: 30.7263, val_MinusLogProbMetric: 30.7263

Epoch 244: val_loss did not improve from 29.89075
196/196 - 39s - loss: 29.5361 - MinusLogProbMetric: 29.5361 - val_loss: 30.7263 - val_MinusLogProbMetric: 30.7263 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 245/1000
2023-10-24 09:52:41.833 
Epoch 245/1000 
	 loss: 29.6625, MinusLogProbMetric: 29.6625, val_loss: 29.7275, val_MinusLogProbMetric: 29.7275

Epoch 245: val_loss improved from 29.89075 to 29.72747, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 38s - loss: 29.6625 - MinusLogProbMetric: 29.6625 - val_loss: 29.7275 - val_MinusLogProbMetric: 29.7275 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 246/1000
2023-10-24 09:53:21.429 
Epoch 246/1000 
	 loss: 29.5366, MinusLogProbMetric: 29.5366, val_loss: 30.3616, val_MinusLogProbMetric: 30.3616

Epoch 246: val_loss did not improve from 29.72747
196/196 - 39s - loss: 29.5366 - MinusLogProbMetric: 29.5366 - val_loss: 30.3616 - val_MinusLogProbMetric: 30.3616 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 247/1000
2023-10-24 09:53:59.280 
Epoch 247/1000 
	 loss: 29.7409, MinusLogProbMetric: 29.7409, val_loss: 30.4055, val_MinusLogProbMetric: 30.4055

Epoch 247: val_loss did not improve from 29.72747
196/196 - 38s - loss: 29.7409 - MinusLogProbMetric: 29.7409 - val_loss: 30.4055 - val_MinusLogProbMetric: 30.4055 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 248/1000
2023-10-24 09:54:37.670 
Epoch 248/1000 
	 loss: 29.6499, MinusLogProbMetric: 29.6499, val_loss: 30.6670, val_MinusLogProbMetric: 30.6670

Epoch 248: val_loss did not improve from 29.72747
196/196 - 38s - loss: 29.6499 - MinusLogProbMetric: 29.6499 - val_loss: 30.6670 - val_MinusLogProbMetric: 30.6670 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 249/1000
2023-10-24 09:55:16.640 
Epoch 249/1000 
	 loss: 29.7051, MinusLogProbMetric: 29.7051, val_loss: 30.6810, val_MinusLogProbMetric: 30.6810

Epoch 249: val_loss did not improve from 29.72747
196/196 - 39s - loss: 29.7051 - MinusLogProbMetric: 29.7051 - val_loss: 30.6810 - val_MinusLogProbMetric: 30.6810 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 250/1000
2023-10-24 09:55:54.694 
Epoch 250/1000 
	 loss: 29.5387, MinusLogProbMetric: 29.5387, val_loss: 30.5615, val_MinusLogProbMetric: 30.5615

Epoch 250: val_loss did not improve from 29.72747
196/196 - 38s - loss: 29.5387 - MinusLogProbMetric: 29.5387 - val_loss: 30.5615 - val_MinusLogProbMetric: 30.5615 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 251/1000
2023-10-24 09:56:33.659 
Epoch 251/1000 
	 loss: 29.6040, MinusLogProbMetric: 29.6040, val_loss: 30.4864, val_MinusLogProbMetric: 30.4864

Epoch 251: val_loss did not improve from 29.72747
196/196 - 39s - loss: 29.6040 - MinusLogProbMetric: 29.6040 - val_loss: 30.4864 - val_MinusLogProbMetric: 30.4864 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 252/1000
2023-10-24 09:57:12.715 
Epoch 252/1000 
	 loss: 29.5159, MinusLogProbMetric: 29.5159, val_loss: 30.3172, val_MinusLogProbMetric: 30.3172

Epoch 252: val_loss did not improve from 29.72747
196/196 - 39s - loss: 29.5159 - MinusLogProbMetric: 29.5159 - val_loss: 30.3172 - val_MinusLogProbMetric: 30.3172 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 253/1000
2023-10-24 09:57:51.136 
Epoch 253/1000 
	 loss: 29.7140, MinusLogProbMetric: 29.7140, val_loss: 30.0855, val_MinusLogProbMetric: 30.0855

Epoch 253: val_loss did not improve from 29.72747
196/196 - 38s - loss: 29.7140 - MinusLogProbMetric: 29.7140 - val_loss: 30.0855 - val_MinusLogProbMetric: 30.0855 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 254/1000
2023-10-24 09:58:26.995 
Epoch 254/1000 
	 loss: 29.5961, MinusLogProbMetric: 29.5961, val_loss: 30.3542, val_MinusLogProbMetric: 30.3542

Epoch 254: val_loss did not improve from 29.72747
196/196 - 36s - loss: 29.5961 - MinusLogProbMetric: 29.5961 - val_loss: 30.3542 - val_MinusLogProbMetric: 30.3542 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 255/1000
2023-10-24 09:59:02.840 
Epoch 255/1000 
	 loss: 29.5016, MinusLogProbMetric: 29.5016, val_loss: 30.2838, val_MinusLogProbMetric: 30.2838

Epoch 255: val_loss did not improve from 29.72747
196/196 - 36s - loss: 29.5016 - MinusLogProbMetric: 29.5016 - val_loss: 30.2838 - val_MinusLogProbMetric: 30.2838 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 256/1000
2023-10-24 09:59:38.505 
Epoch 256/1000 
	 loss: 29.5154, MinusLogProbMetric: 29.5154, val_loss: 30.1080, val_MinusLogProbMetric: 30.1080

Epoch 256: val_loss did not improve from 29.72747
196/196 - 36s - loss: 29.5154 - MinusLogProbMetric: 29.5154 - val_loss: 30.1080 - val_MinusLogProbMetric: 30.1080 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 257/1000
2023-10-24 10:00:15.665 
Epoch 257/1000 
	 loss: 29.5388, MinusLogProbMetric: 29.5388, val_loss: 30.0672, val_MinusLogProbMetric: 30.0672

Epoch 257: val_loss did not improve from 29.72747
196/196 - 37s - loss: 29.5388 - MinusLogProbMetric: 29.5388 - val_loss: 30.0672 - val_MinusLogProbMetric: 30.0672 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 258/1000
2023-10-24 10:00:51.293 
Epoch 258/1000 
	 loss: 29.5782, MinusLogProbMetric: 29.5782, val_loss: 30.7041, val_MinusLogProbMetric: 30.7041

Epoch 258: val_loss did not improve from 29.72747
196/196 - 36s - loss: 29.5782 - MinusLogProbMetric: 29.5782 - val_loss: 30.7041 - val_MinusLogProbMetric: 30.7041 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 259/1000
2023-10-24 10:01:29.767 
Epoch 259/1000 
	 loss: 29.5241, MinusLogProbMetric: 29.5241, val_loss: 30.3489, val_MinusLogProbMetric: 30.3489

Epoch 259: val_loss did not improve from 29.72747
196/196 - 38s - loss: 29.5241 - MinusLogProbMetric: 29.5241 - val_loss: 30.3489 - val_MinusLogProbMetric: 30.3489 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 260/1000
2023-10-24 10:02:08.552 
Epoch 260/1000 
	 loss: 29.5702, MinusLogProbMetric: 29.5702, val_loss: 29.7452, val_MinusLogProbMetric: 29.7452

Epoch 260: val_loss did not improve from 29.72747
196/196 - 39s - loss: 29.5702 - MinusLogProbMetric: 29.5702 - val_loss: 29.7452 - val_MinusLogProbMetric: 29.7452 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 261/1000
2023-10-24 10:02:45.757 
Epoch 261/1000 
	 loss: 29.4804, MinusLogProbMetric: 29.4804, val_loss: 30.5351, val_MinusLogProbMetric: 30.5351

Epoch 261: val_loss did not improve from 29.72747
196/196 - 37s - loss: 29.4804 - MinusLogProbMetric: 29.4804 - val_loss: 30.5351 - val_MinusLogProbMetric: 30.5351 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 262/1000
2023-10-24 10:03:24.608 
Epoch 262/1000 
	 loss: 29.5391, MinusLogProbMetric: 29.5391, val_loss: 29.8507, val_MinusLogProbMetric: 29.8507

Epoch 262: val_loss did not improve from 29.72747
196/196 - 39s - loss: 29.5391 - MinusLogProbMetric: 29.5391 - val_loss: 29.8507 - val_MinusLogProbMetric: 29.8507 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 263/1000
2023-10-24 10:04:03.561 
Epoch 263/1000 
	 loss: 29.4501, MinusLogProbMetric: 29.4501, val_loss: 29.8860, val_MinusLogProbMetric: 29.8860

Epoch 263: val_loss did not improve from 29.72747
196/196 - 39s - loss: 29.4501 - MinusLogProbMetric: 29.4501 - val_loss: 29.8860 - val_MinusLogProbMetric: 29.8860 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 264/1000
2023-10-24 10:04:42.352 
Epoch 264/1000 
	 loss: 29.4113, MinusLogProbMetric: 29.4113, val_loss: 29.9357, val_MinusLogProbMetric: 29.9357

Epoch 264: val_loss did not improve from 29.72747
196/196 - 39s - loss: 29.4113 - MinusLogProbMetric: 29.4113 - val_loss: 29.9357 - val_MinusLogProbMetric: 29.9357 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 265/1000
2023-10-24 10:05:21.044 
Epoch 265/1000 
	 loss: 29.7027, MinusLogProbMetric: 29.7027, val_loss: 30.0434, val_MinusLogProbMetric: 30.0434

Epoch 265: val_loss did not improve from 29.72747
196/196 - 39s - loss: 29.7027 - MinusLogProbMetric: 29.7027 - val_loss: 30.0434 - val_MinusLogProbMetric: 30.0434 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 266/1000
2023-10-24 10:06:00.514 
Epoch 266/1000 
	 loss: 29.6392, MinusLogProbMetric: 29.6392, val_loss: 31.1585, val_MinusLogProbMetric: 31.1585

Epoch 266: val_loss did not improve from 29.72747
196/196 - 39s - loss: 29.6392 - MinusLogProbMetric: 29.6392 - val_loss: 31.1585 - val_MinusLogProbMetric: 31.1585 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 267/1000
2023-10-24 10:06:39.842 
Epoch 267/1000 
	 loss: 29.5203, MinusLogProbMetric: 29.5203, val_loss: 30.1024, val_MinusLogProbMetric: 30.1024

Epoch 267: val_loss did not improve from 29.72747
196/196 - 39s - loss: 29.5203 - MinusLogProbMetric: 29.5203 - val_loss: 30.1024 - val_MinusLogProbMetric: 30.1024 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 268/1000
2023-10-24 10:07:18.619 
Epoch 268/1000 
	 loss: 29.6617, MinusLogProbMetric: 29.6617, val_loss: 31.8138, val_MinusLogProbMetric: 31.8138

Epoch 268: val_loss did not improve from 29.72747
196/196 - 39s - loss: 29.6617 - MinusLogProbMetric: 29.6617 - val_loss: 31.8138 - val_MinusLogProbMetric: 31.8138 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 269/1000
2023-10-24 10:07:57.443 
Epoch 269/1000 
	 loss: 29.5385, MinusLogProbMetric: 29.5385, val_loss: 30.5615, val_MinusLogProbMetric: 30.5615

Epoch 269: val_loss did not improve from 29.72747
196/196 - 39s - loss: 29.5385 - MinusLogProbMetric: 29.5385 - val_loss: 30.5615 - val_MinusLogProbMetric: 30.5615 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 270/1000
2023-10-24 10:08:36.400 
Epoch 270/1000 
	 loss: 29.7821, MinusLogProbMetric: 29.7821, val_loss: 31.4865, val_MinusLogProbMetric: 31.4865

Epoch 270: val_loss did not improve from 29.72747
196/196 - 39s - loss: 29.7821 - MinusLogProbMetric: 29.7821 - val_loss: 31.4865 - val_MinusLogProbMetric: 31.4865 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 271/1000
2023-10-24 10:09:15.601 
Epoch 271/1000 
	 loss: 29.4060, MinusLogProbMetric: 29.4060, val_loss: 29.9119, val_MinusLogProbMetric: 29.9119

Epoch 271: val_loss did not improve from 29.72747
196/196 - 39s - loss: 29.4060 - MinusLogProbMetric: 29.4060 - val_loss: 29.9119 - val_MinusLogProbMetric: 29.9119 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 272/1000
2023-10-24 10:09:53.926 
Epoch 272/1000 
	 loss: 29.4804, MinusLogProbMetric: 29.4804, val_loss: 29.8603, val_MinusLogProbMetric: 29.8603

Epoch 272: val_loss did not improve from 29.72747
196/196 - 38s - loss: 29.4804 - MinusLogProbMetric: 29.4804 - val_loss: 29.8603 - val_MinusLogProbMetric: 29.8603 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 273/1000
2023-10-24 10:10:32.405 
Epoch 273/1000 
	 loss: 29.6724, MinusLogProbMetric: 29.6724, val_loss: 29.8877, val_MinusLogProbMetric: 29.8877

Epoch 273: val_loss did not improve from 29.72747
196/196 - 38s - loss: 29.6724 - MinusLogProbMetric: 29.6724 - val_loss: 29.8877 - val_MinusLogProbMetric: 29.8877 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 274/1000
2023-10-24 10:11:10.128 
Epoch 274/1000 
	 loss: 29.3040, MinusLogProbMetric: 29.3040, val_loss: 29.6475, val_MinusLogProbMetric: 29.6475

Epoch 274: val_loss improved from 29.72747 to 29.64746, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 38s - loss: 29.3040 - MinusLogProbMetric: 29.3040 - val_loss: 29.6475 - val_MinusLogProbMetric: 29.6475 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 275/1000
2023-10-24 10:11:49.408 
Epoch 275/1000 
	 loss: 29.5674, MinusLogProbMetric: 29.5674, val_loss: 31.5235, val_MinusLogProbMetric: 31.5235

Epoch 275: val_loss did not improve from 29.64746
196/196 - 39s - loss: 29.5674 - MinusLogProbMetric: 29.5674 - val_loss: 31.5235 - val_MinusLogProbMetric: 31.5235 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 276/1000
2023-10-24 10:12:28.161 
Epoch 276/1000 
	 loss: 29.4042, MinusLogProbMetric: 29.4042, val_loss: 30.3066, val_MinusLogProbMetric: 30.3066

Epoch 276: val_loss did not improve from 29.64746
196/196 - 39s - loss: 29.4042 - MinusLogProbMetric: 29.4042 - val_loss: 30.3066 - val_MinusLogProbMetric: 30.3066 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 277/1000
2023-10-24 10:13:04.255 
Epoch 277/1000 
	 loss: 29.3690, MinusLogProbMetric: 29.3690, val_loss: 30.4048, val_MinusLogProbMetric: 30.4048

Epoch 277: val_loss did not improve from 29.64746
196/196 - 36s - loss: 29.3690 - MinusLogProbMetric: 29.3690 - val_loss: 30.4048 - val_MinusLogProbMetric: 30.4048 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 278/1000
2023-10-24 10:13:41.329 
Epoch 278/1000 
	 loss: 29.4918, MinusLogProbMetric: 29.4918, val_loss: 35.5938, val_MinusLogProbMetric: 35.5938

Epoch 278: val_loss did not improve from 29.64746
196/196 - 37s - loss: 29.4918 - MinusLogProbMetric: 29.4918 - val_loss: 35.5938 - val_MinusLogProbMetric: 35.5938 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 279/1000
2023-10-24 10:14:17.551 
Epoch 279/1000 
	 loss: 29.6676, MinusLogProbMetric: 29.6676, val_loss: 29.7095, val_MinusLogProbMetric: 29.7095

Epoch 279: val_loss did not improve from 29.64746
196/196 - 36s - loss: 29.6676 - MinusLogProbMetric: 29.6676 - val_loss: 29.7095 - val_MinusLogProbMetric: 29.7095 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 280/1000
2023-10-24 10:14:53.601 
Epoch 280/1000 
	 loss: 29.4953, MinusLogProbMetric: 29.4953, val_loss: 30.1114, val_MinusLogProbMetric: 30.1114

Epoch 280: val_loss did not improve from 29.64746
196/196 - 36s - loss: 29.4953 - MinusLogProbMetric: 29.4953 - val_loss: 30.1114 - val_MinusLogProbMetric: 30.1114 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 281/1000
2023-10-24 10:15:31.906 
Epoch 281/1000 
	 loss: 29.6107, MinusLogProbMetric: 29.6107, val_loss: 29.7351, val_MinusLogProbMetric: 29.7351

Epoch 281: val_loss did not improve from 29.64746
196/196 - 38s - loss: 29.6107 - MinusLogProbMetric: 29.6107 - val_loss: 29.7351 - val_MinusLogProbMetric: 29.7351 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 282/1000
2023-10-24 10:16:09.235 
Epoch 282/1000 
	 loss: 29.3082, MinusLogProbMetric: 29.3082, val_loss: 30.3262, val_MinusLogProbMetric: 30.3262

Epoch 282: val_loss did not improve from 29.64746
196/196 - 37s - loss: 29.3082 - MinusLogProbMetric: 29.3082 - val_loss: 30.3262 - val_MinusLogProbMetric: 30.3262 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 283/1000
2023-10-24 10:16:45.111 
Epoch 283/1000 
	 loss: 29.5958, MinusLogProbMetric: 29.5958, val_loss: 29.9828, val_MinusLogProbMetric: 29.9828

Epoch 283: val_loss did not improve from 29.64746
196/196 - 36s - loss: 29.5958 - MinusLogProbMetric: 29.5958 - val_loss: 29.9828 - val_MinusLogProbMetric: 29.9828 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 284/1000
2023-10-24 10:17:23.025 
Epoch 284/1000 
	 loss: 29.3632, MinusLogProbMetric: 29.3632, val_loss: 30.0930, val_MinusLogProbMetric: 30.0930

Epoch 284: val_loss did not improve from 29.64746
196/196 - 38s - loss: 29.3632 - MinusLogProbMetric: 29.3632 - val_loss: 30.0930 - val_MinusLogProbMetric: 30.0930 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 285/1000
2023-10-24 10:17:59.949 
Epoch 285/1000 
	 loss: 29.5963, MinusLogProbMetric: 29.5963, val_loss: 31.1865, val_MinusLogProbMetric: 31.1865

Epoch 285: val_loss did not improve from 29.64746
196/196 - 37s - loss: 29.5963 - MinusLogProbMetric: 29.5963 - val_loss: 31.1865 - val_MinusLogProbMetric: 31.1865 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 286/1000
2023-10-24 10:18:36.364 
Epoch 286/1000 
	 loss: 29.5245, MinusLogProbMetric: 29.5245, val_loss: 30.0133, val_MinusLogProbMetric: 30.0133

Epoch 286: val_loss did not improve from 29.64746
196/196 - 36s - loss: 29.5245 - MinusLogProbMetric: 29.5245 - val_loss: 30.0133 - val_MinusLogProbMetric: 30.0133 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 287/1000
2023-10-24 10:19:11.740 
Epoch 287/1000 
	 loss: 29.2763, MinusLogProbMetric: 29.2763, val_loss: 29.9127, val_MinusLogProbMetric: 29.9127

Epoch 287: val_loss did not improve from 29.64746
196/196 - 35s - loss: 29.2763 - MinusLogProbMetric: 29.2763 - val_loss: 29.9127 - val_MinusLogProbMetric: 29.9127 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 288/1000
2023-10-24 10:19:49.201 
Epoch 288/1000 
	 loss: 29.4918, MinusLogProbMetric: 29.4918, val_loss: 32.3118, val_MinusLogProbMetric: 32.3118

Epoch 288: val_loss did not improve from 29.64746
196/196 - 37s - loss: 29.4918 - MinusLogProbMetric: 29.4918 - val_loss: 32.3118 - val_MinusLogProbMetric: 32.3118 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 289/1000
2023-10-24 10:20:25.951 
Epoch 289/1000 
	 loss: 29.3223, MinusLogProbMetric: 29.3223, val_loss: 29.6755, val_MinusLogProbMetric: 29.6755

Epoch 289: val_loss did not improve from 29.64746
196/196 - 37s - loss: 29.3223 - MinusLogProbMetric: 29.3223 - val_loss: 29.6755 - val_MinusLogProbMetric: 29.6755 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 290/1000
2023-10-24 10:21:02.346 
Epoch 290/1000 
	 loss: 29.4708, MinusLogProbMetric: 29.4708, val_loss: 30.7343, val_MinusLogProbMetric: 30.7343

Epoch 290: val_loss did not improve from 29.64746
196/196 - 36s - loss: 29.4708 - MinusLogProbMetric: 29.4708 - val_loss: 30.7343 - val_MinusLogProbMetric: 30.7343 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 291/1000
2023-10-24 10:21:39.832 
Epoch 291/1000 
	 loss: 29.3699, MinusLogProbMetric: 29.3699, val_loss: 29.8273, val_MinusLogProbMetric: 29.8273

Epoch 291: val_loss did not improve from 29.64746
196/196 - 37s - loss: 29.3699 - MinusLogProbMetric: 29.3699 - val_loss: 29.8273 - val_MinusLogProbMetric: 29.8273 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 292/1000
2023-10-24 10:22:16.259 
Epoch 292/1000 
	 loss: 29.4270, MinusLogProbMetric: 29.4270, val_loss: 30.7232, val_MinusLogProbMetric: 30.7232

Epoch 292: val_loss did not improve from 29.64746
196/196 - 36s - loss: 29.4270 - MinusLogProbMetric: 29.4270 - val_loss: 30.7232 - val_MinusLogProbMetric: 30.7232 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 293/1000
2023-10-24 10:22:51.917 
Epoch 293/1000 
	 loss: 29.4689, MinusLogProbMetric: 29.4689, val_loss: 30.0284, val_MinusLogProbMetric: 30.0284

Epoch 293: val_loss did not improve from 29.64746
196/196 - 36s - loss: 29.4689 - MinusLogProbMetric: 29.4689 - val_loss: 30.0284 - val_MinusLogProbMetric: 30.0284 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 294/1000
2023-10-24 10:23:29.645 
Epoch 294/1000 
	 loss: 29.3539, MinusLogProbMetric: 29.3539, val_loss: 29.8879, val_MinusLogProbMetric: 29.8879

Epoch 294: val_loss did not improve from 29.64746
196/196 - 38s - loss: 29.3539 - MinusLogProbMetric: 29.3539 - val_loss: 29.8879 - val_MinusLogProbMetric: 29.8879 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 295/1000
2023-10-24 10:24:06.237 
Epoch 295/1000 
	 loss: 29.6075, MinusLogProbMetric: 29.6075, val_loss: 29.9880, val_MinusLogProbMetric: 29.9880

Epoch 295: val_loss did not improve from 29.64746
196/196 - 37s - loss: 29.6075 - MinusLogProbMetric: 29.6075 - val_loss: 29.9880 - val_MinusLogProbMetric: 29.9880 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 296/1000
2023-10-24 10:24:43.103 
Epoch 296/1000 
	 loss: 29.2905, MinusLogProbMetric: 29.2905, val_loss: 29.7848, val_MinusLogProbMetric: 29.7848

Epoch 296: val_loss did not improve from 29.64746
196/196 - 37s - loss: 29.2905 - MinusLogProbMetric: 29.2905 - val_loss: 29.7848 - val_MinusLogProbMetric: 29.7848 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 297/1000
2023-10-24 10:25:21.271 
Epoch 297/1000 
	 loss: 29.3398, MinusLogProbMetric: 29.3398, val_loss: 30.3571, val_MinusLogProbMetric: 30.3571

Epoch 297: val_loss did not improve from 29.64746
196/196 - 38s - loss: 29.3398 - MinusLogProbMetric: 29.3398 - val_loss: 30.3571 - val_MinusLogProbMetric: 30.3571 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 298/1000
2023-10-24 10:25:58.057 
Epoch 298/1000 
	 loss: 29.5021, MinusLogProbMetric: 29.5021, val_loss: 29.9512, val_MinusLogProbMetric: 29.9512

Epoch 298: val_loss did not improve from 29.64746
196/196 - 37s - loss: 29.5021 - MinusLogProbMetric: 29.5021 - val_loss: 29.9512 - val_MinusLogProbMetric: 29.9512 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 299/1000
2023-10-24 10:26:36.794 
Epoch 299/1000 
	 loss: 29.3119, MinusLogProbMetric: 29.3119, val_loss: 31.2399, val_MinusLogProbMetric: 31.2399

Epoch 299: val_loss did not improve from 29.64746
196/196 - 39s - loss: 29.3119 - MinusLogProbMetric: 29.3119 - val_loss: 31.2399 - val_MinusLogProbMetric: 31.2399 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 300/1000
2023-10-24 10:27:12.892 
Epoch 300/1000 
	 loss: 29.3383, MinusLogProbMetric: 29.3383, val_loss: 30.0255, val_MinusLogProbMetric: 30.0255

Epoch 300: val_loss did not improve from 29.64746
196/196 - 36s - loss: 29.3383 - MinusLogProbMetric: 29.3383 - val_loss: 30.0255 - val_MinusLogProbMetric: 30.0255 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 301/1000
2023-10-24 10:27:49.428 
Epoch 301/1000 
	 loss: 29.2365, MinusLogProbMetric: 29.2365, val_loss: 29.8814, val_MinusLogProbMetric: 29.8814

Epoch 301: val_loss did not improve from 29.64746
196/196 - 37s - loss: 29.2365 - MinusLogProbMetric: 29.2365 - val_loss: 29.8814 - val_MinusLogProbMetric: 29.8814 - lr: 0.0010 - 37s/epoch - 186ms/step
Epoch 302/1000
2023-10-24 10:28:27.092 
Epoch 302/1000 
	 loss: 29.3644, MinusLogProbMetric: 29.3644, val_loss: 30.1930, val_MinusLogProbMetric: 30.1930

Epoch 302: val_loss did not improve from 29.64746
196/196 - 38s - loss: 29.3644 - MinusLogProbMetric: 29.3644 - val_loss: 30.1930 - val_MinusLogProbMetric: 30.1930 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 303/1000
2023-10-24 10:29:03.148 
Epoch 303/1000 
	 loss: 29.3354, MinusLogProbMetric: 29.3354, val_loss: 29.3609, val_MinusLogProbMetric: 29.3609

Epoch 303: val_loss improved from 29.64746 to 29.36090, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 37s - loss: 29.3354 - MinusLogProbMetric: 29.3354 - val_loss: 29.3609 - val_MinusLogProbMetric: 29.3609 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 304/1000
2023-10-24 10:29:40.155 
Epoch 304/1000 
	 loss: 29.3377, MinusLogProbMetric: 29.3377, val_loss: 30.0985, val_MinusLogProbMetric: 30.0985

Epoch 304: val_loss did not improve from 29.36090
196/196 - 36s - loss: 29.3377 - MinusLogProbMetric: 29.3377 - val_loss: 30.0985 - val_MinusLogProbMetric: 30.0985 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 305/1000
2023-10-24 10:30:17.577 
Epoch 305/1000 
	 loss: 29.2024, MinusLogProbMetric: 29.2024, val_loss: 30.3448, val_MinusLogProbMetric: 30.3448

Epoch 305: val_loss did not improve from 29.36090
196/196 - 37s - loss: 29.2024 - MinusLogProbMetric: 29.2024 - val_loss: 30.3448 - val_MinusLogProbMetric: 30.3448 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 306/1000
2023-10-24 10:30:54.099 
Epoch 306/1000 
	 loss: 29.2705, MinusLogProbMetric: 29.2705, val_loss: 29.7865, val_MinusLogProbMetric: 29.7865

Epoch 306: val_loss did not improve from 29.36090
196/196 - 37s - loss: 29.2705 - MinusLogProbMetric: 29.2705 - val_loss: 29.7865 - val_MinusLogProbMetric: 29.7865 - lr: 0.0010 - 37s/epoch - 186ms/step
Epoch 307/1000
2023-10-24 10:31:30.490 
Epoch 307/1000 
	 loss: 29.4175, MinusLogProbMetric: 29.4175, val_loss: 29.5933, val_MinusLogProbMetric: 29.5933

Epoch 307: val_loss did not improve from 29.36090
196/196 - 36s - loss: 29.4175 - MinusLogProbMetric: 29.4175 - val_loss: 29.5933 - val_MinusLogProbMetric: 29.5933 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 308/1000
2023-10-24 10:32:07.536 
Epoch 308/1000 
	 loss: 29.5743, MinusLogProbMetric: 29.5743, val_loss: 30.9286, val_MinusLogProbMetric: 30.9286

Epoch 308: val_loss did not improve from 29.36090
196/196 - 37s - loss: 29.5743 - MinusLogProbMetric: 29.5743 - val_loss: 30.9286 - val_MinusLogProbMetric: 30.9286 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 309/1000
2023-10-24 10:32:43.821 
Epoch 309/1000 
	 loss: 29.2384, MinusLogProbMetric: 29.2384, val_loss: 29.6209, val_MinusLogProbMetric: 29.6209

Epoch 309: val_loss did not improve from 29.36090
196/196 - 36s - loss: 29.2384 - MinusLogProbMetric: 29.2384 - val_loss: 29.6209 - val_MinusLogProbMetric: 29.6209 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 310/1000
2023-10-24 10:33:19.839 
Epoch 310/1000 
	 loss: 29.3754, MinusLogProbMetric: 29.3754, val_loss: 29.6376, val_MinusLogProbMetric: 29.6376

Epoch 310: val_loss did not improve from 29.36090
196/196 - 36s - loss: 29.3754 - MinusLogProbMetric: 29.3754 - val_loss: 29.6376 - val_MinusLogProbMetric: 29.6376 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 311/1000
2023-10-24 10:33:55.110 
Epoch 311/1000 
	 loss: 29.2926, MinusLogProbMetric: 29.2926, val_loss: 29.7357, val_MinusLogProbMetric: 29.7357

Epoch 311: val_loss did not improve from 29.36090
196/196 - 35s - loss: 29.2926 - MinusLogProbMetric: 29.2926 - val_loss: 29.7357 - val_MinusLogProbMetric: 29.7357 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 312/1000
2023-10-24 10:34:31.118 
Epoch 312/1000 
	 loss: 29.2193, MinusLogProbMetric: 29.2193, val_loss: 29.8013, val_MinusLogProbMetric: 29.8013

Epoch 312: val_loss did not improve from 29.36090
196/196 - 36s - loss: 29.2193 - MinusLogProbMetric: 29.2193 - val_loss: 29.8013 - val_MinusLogProbMetric: 29.8013 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 313/1000
2023-10-24 10:35:08.628 
Epoch 313/1000 
	 loss: 29.4586, MinusLogProbMetric: 29.4586, val_loss: 29.8893, val_MinusLogProbMetric: 29.8893

Epoch 313: val_loss did not improve from 29.36090
196/196 - 38s - loss: 29.4586 - MinusLogProbMetric: 29.4586 - val_loss: 29.8893 - val_MinusLogProbMetric: 29.8893 - lr: 0.0010 - 38s/epoch - 191ms/step
Epoch 314/1000
2023-10-24 10:35:46.713 
Epoch 314/1000 
	 loss: 29.3969, MinusLogProbMetric: 29.3969, val_loss: 29.6808, val_MinusLogProbMetric: 29.6808

Epoch 314: val_loss did not improve from 29.36090
196/196 - 38s - loss: 29.3969 - MinusLogProbMetric: 29.3969 - val_loss: 29.6808 - val_MinusLogProbMetric: 29.6808 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 315/1000
2023-10-24 10:36:24.765 
Epoch 315/1000 
	 loss: 29.2605, MinusLogProbMetric: 29.2605, val_loss: 29.7379, val_MinusLogProbMetric: 29.7379

Epoch 315: val_loss did not improve from 29.36090
196/196 - 38s - loss: 29.2605 - MinusLogProbMetric: 29.2605 - val_loss: 29.7379 - val_MinusLogProbMetric: 29.7379 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 316/1000
2023-10-24 10:37:02.972 
Epoch 316/1000 
	 loss: 29.3889, MinusLogProbMetric: 29.3889, val_loss: 29.7386, val_MinusLogProbMetric: 29.7386

Epoch 316: val_loss did not improve from 29.36090
196/196 - 38s - loss: 29.3889 - MinusLogProbMetric: 29.3889 - val_loss: 29.7386 - val_MinusLogProbMetric: 29.7386 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 317/1000
2023-10-24 10:37:39.751 
Epoch 317/1000 
	 loss: 29.2271, MinusLogProbMetric: 29.2271, val_loss: 29.5727, val_MinusLogProbMetric: 29.5727

Epoch 317: val_loss did not improve from 29.36090
196/196 - 37s - loss: 29.2271 - MinusLogProbMetric: 29.2271 - val_loss: 29.5727 - val_MinusLogProbMetric: 29.5727 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 318/1000
2023-10-24 10:38:15.493 
Epoch 318/1000 
	 loss: 29.1768, MinusLogProbMetric: 29.1768, val_loss: 30.5846, val_MinusLogProbMetric: 30.5846

Epoch 318: val_loss did not improve from 29.36090
196/196 - 36s - loss: 29.1768 - MinusLogProbMetric: 29.1768 - val_loss: 30.5846 - val_MinusLogProbMetric: 30.5846 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 319/1000
2023-10-24 10:38:51.442 
Epoch 319/1000 
	 loss: 29.2997, MinusLogProbMetric: 29.2997, val_loss: 29.6533, val_MinusLogProbMetric: 29.6533

Epoch 319: val_loss did not improve from 29.36090
196/196 - 36s - loss: 29.2997 - MinusLogProbMetric: 29.2997 - val_loss: 29.6533 - val_MinusLogProbMetric: 29.6533 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 320/1000
2023-10-24 10:39:28.942 
Epoch 320/1000 
	 loss: 29.2475, MinusLogProbMetric: 29.2475, val_loss: 29.8035, val_MinusLogProbMetric: 29.8035

Epoch 320: val_loss did not improve from 29.36090
196/196 - 37s - loss: 29.2475 - MinusLogProbMetric: 29.2475 - val_loss: 29.8035 - val_MinusLogProbMetric: 29.8035 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 321/1000
2023-10-24 10:40:04.731 
Epoch 321/1000 
	 loss: 29.4159, MinusLogProbMetric: 29.4159, val_loss: 30.5142, val_MinusLogProbMetric: 30.5142

Epoch 321: val_loss did not improve from 29.36090
196/196 - 36s - loss: 29.4159 - MinusLogProbMetric: 29.4159 - val_loss: 30.5142 - val_MinusLogProbMetric: 30.5142 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 322/1000
2023-10-24 10:40:42.008 
Epoch 322/1000 
	 loss: 29.1752, MinusLogProbMetric: 29.1752, val_loss: 30.0392, val_MinusLogProbMetric: 30.0392

Epoch 322: val_loss did not improve from 29.36090
196/196 - 37s - loss: 29.1752 - MinusLogProbMetric: 29.1752 - val_loss: 30.0392 - val_MinusLogProbMetric: 30.0392 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 323/1000
2023-10-24 10:41:20.672 
Epoch 323/1000 
	 loss: 29.2427, MinusLogProbMetric: 29.2427, val_loss: 29.7677, val_MinusLogProbMetric: 29.7677

Epoch 323: val_loss did not improve from 29.36090
196/196 - 39s - loss: 29.2427 - MinusLogProbMetric: 29.2427 - val_loss: 29.7677 - val_MinusLogProbMetric: 29.7677 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 324/1000
2023-10-24 10:41:58.398 
Epoch 324/1000 
	 loss: 29.1924, MinusLogProbMetric: 29.1924, val_loss: 30.5969, val_MinusLogProbMetric: 30.5969

Epoch 324: val_loss did not improve from 29.36090
196/196 - 38s - loss: 29.1924 - MinusLogProbMetric: 29.1924 - val_loss: 30.5969 - val_MinusLogProbMetric: 30.5969 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 325/1000
2023-10-24 10:42:34.353 
Epoch 325/1000 
	 loss: 29.1334, MinusLogProbMetric: 29.1334, val_loss: 30.2459, val_MinusLogProbMetric: 30.2459

Epoch 325: val_loss did not improve from 29.36090
196/196 - 36s - loss: 29.1334 - MinusLogProbMetric: 29.1334 - val_loss: 30.2459 - val_MinusLogProbMetric: 30.2459 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 326/1000
2023-10-24 10:43:10.109 
Epoch 326/1000 
	 loss: 29.2293, MinusLogProbMetric: 29.2293, val_loss: 30.1507, val_MinusLogProbMetric: 30.1507

Epoch 326: val_loss did not improve from 29.36090
196/196 - 36s - loss: 29.2293 - MinusLogProbMetric: 29.2293 - val_loss: 30.1507 - val_MinusLogProbMetric: 30.1507 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 327/1000
2023-10-24 10:43:45.712 
Epoch 327/1000 
	 loss: 29.2329, MinusLogProbMetric: 29.2329, val_loss: 30.2671, val_MinusLogProbMetric: 30.2671

Epoch 327: val_loss did not improve from 29.36090
196/196 - 36s - loss: 29.2329 - MinusLogProbMetric: 29.2329 - val_loss: 30.2671 - val_MinusLogProbMetric: 30.2671 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 328/1000
2023-10-24 10:44:23.060 
Epoch 328/1000 
	 loss: 29.1045, MinusLogProbMetric: 29.1045, val_loss: 30.0084, val_MinusLogProbMetric: 30.0084

Epoch 328: val_loss did not improve from 29.36090
196/196 - 37s - loss: 29.1045 - MinusLogProbMetric: 29.1045 - val_loss: 30.0084 - val_MinusLogProbMetric: 30.0084 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 329/1000
2023-10-24 10:45:00.406 
Epoch 329/1000 
	 loss: 29.6562, MinusLogProbMetric: 29.6562, val_loss: 29.7870, val_MinusLogProbMetric: 29.7870

Epoch 329: val_loss did not improve from 29.36090
196/196 - 37s - loss: 29.6562 - MinusLogProbMetric: 29.6562 - val_loss: 29.7870 - val_MinusLogProbMetric: 29.7870 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 330/1000
2023-10-24 10:45:39.257 
Epoch 330/1000 
	 loss: 29.0724, MinusLogProbMetric: 29.0724, val_loss: 29.8626, val_MinusLogProbMetric: 29.8626

Epoch 330: val_loss did not improve from 29.36090
196/196 - 39s - loss: 29.0724 - MinusLogProbMetric: 29.0724 - val_loss: 29.8626 - val_MinusLogProbMetric: 29.8626 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 331/1000
2023-10-24 10:46:16.824 
Epoch 331/1000 
	 loss: 29.2212, MinusLogProbMetric: 29.2212, val_loss: 29.5936, val_MinusLogProbMetric: 29.5936

Epoch 331: val_loss did not improve from 29.36090
196/196 - 38s - loss: 29.2212 - MinusLogProbMetric: 29.2212 - val_loss: 29.5936 - val_MinusLogProbMetric: 29.5936 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 332/1000
2023-10-24 10:46:54.174 
Epoch 332/1000 
	 loss: 29.2479, MinusLogProbMetric: 29.2479, val_loss: 30.5747, val_MinusLogProbMetric: 30.5747

Epoch 332: val_loss did not improve from 29.36090
196/196 - 37s - loss: 29.2479 - MinusLogProbMetric: 29.2479 - val_loss: 30.5747 - val_MinusLogProbMetric: 30.5747 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 333/1000
2023-10-24 10:47:29.939 
Epoch 333/1000 
	 loss: 29.3054, MinusLogProbMetric: 29.3054, val_loss: 32.6462, val_MinusLogProbMetric: 32.6462

Epoch 333: val_loss did not improve from 29.36090
196/196 - 36s - loss: 29.3054 - MinusLogProbMetric: 29.3054 - val_loss: 32.6462 - val_MinusLogProbMetric: 32.6462 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 334/1000
2023-10-24 10:48:06.133 
Epoch 334/1000 
	 loss: 29.4246, MinusLogProbMetric: 29.4246, val_loss: 30.6197, val_MinusLogProbMetric: 30.6197

Epoch 334: val_loss did not improve from 29.36090
196/196 - 36s - loss: 29.4246 - MinusLogProbMetric: 29.4246 - val_loss: 30.6197 - val_MinusLogProbMetric: 30.6197 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 335/1000
2023-10-24 10:48:43.290 
Epoch 335/1000 
	 loss: 29.3702, MinusLogProbMetric: 29.3702, val_loss: 31.1187, val_MinusLogProbMetric: 31.1187

Epoch 335: val_loss did not improve from 29.36090
196/196 - 37s - loss: 29.3702 - MinusLogProbMetric: 29.3702 - val_loss: 31.1187 - val_MinusLogProbMetric: 31.1187 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 336/1000
2023-10-24 10:49:20.685 
Epoch 336/1000 
	 loss: 29.1296, MinusLogProbMetric: 29.1296, val_loss: 30.7076, val_MinusLogProbMetric: 30.7076

Epoch 336: val_loss did not improve from 29.36090
196/196 - 37s - loss: 29.1296 - MinusLogProbMetric: 29.1296 - val_loss: 30.7076 - val_MinusLogProbMetric: 30.7076 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 337/1000
2023-10-24 10:49:58.951 
Epoch 337/1000 
	 loss: 29.1385, MinusLogProbMetric: 29.1385, val_loss: 29.8951, val_MinusLogProbMetric: 29.8951

Epoch 337: val_loss did not improve from 29.36090
196/196 - 38s - loss: 29.1385 - MinusLogProbMetric: 29.1385 - val_loss: 29.8951 - val_MinusLogProbMetric: 29.8951 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 338/1000
2023-10-24 10:50:35.855 
Epoch 338/1000 
	 loss: 29.1283, MinusLogProbMetric: 29.1283, val_loss: 29.9041, val_MinusLogProbMetric: 29.9041

Epoch 338: val_loss did not improve from 29.36090
196/196 - 37s - loss: 29.1283 - MinusLogProbMetric: 29.1283 - val_loss: 29.9041 - val_MinusLogProbMetric: 29.9041 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 339/1000
2023-10-24 10:51:13.499 
Epoch 339/1000 
	 loss: 29.3626, MinusLogProbMetric: 29.3626, val_loss: 30.1331, val_MinusLogProbMetric: 30.1331

Epoch 339: val_loss did not improve from 29.36090
196/196 - 38s - loss: 29.3626 - MinusLogProbMetric: 29.3626 - val_loss: 30.1331 - val_MinusLogProbMetric: 30.1331 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 340/1000
2023-10-24 10:51:50.813 
Epoch 340/1000 
	 loss: 29.1607, MinusLogProbMetric: 29.1607, val_loss: 29.7510, val_MinusLogProbMetric: 29.7510

Epoch 340: val_loss did not improve from 29.36090
196/196 - 37s - loss: 29.1607 - MinusLogProbMetric: 29.1607 - val_loss: 29.7510 - val_MinusLogProbMetric: 29.7510 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 341/1000
2023-10-24 10:52:29.062 
Epoch 341/1000 
	 loss: 29.0584, MinusLogProbMetric: 29.0584, val_loss: 31.0637, val_MinusLogProbMetric: 31.0637

Epoch 341: val_loss did not improve from 29.36090
196/196 - 38s - loss: 29.0584 - MinusLogProbMetric: 29.0584 - val_loss: 31.0637 - val_MinusLogProbMetric: 31.0637 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 342/1000
2023-10-24 10:53:07.899 
Epoch 342/1000 
	 loss: 29.4136, MinusLogProbMetric: 29.4136, val_loss: 29.9499, val_MinusLogProbMetric: 29.9499

Epoch 342: val_loss did not improve from 29.36090
196/196 - 39s - loss: 29.4136 - MinusLogProbMetric: 29.4136 - val_loss: 29.9499 - val_MinusLogProbMetric: 29.9499 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 343/1000
2023-10-24 10:53:46.218 
Epoch 343/1000 
	 loss: 29.2295, MinusLogProbMetric: 29.2295, val_loss: 30.4239, val_MinusLogProbMetric: 30.4239

Epoch 343: val_loss did not improve from 29.36090
196/196 - 38s - loss: 29.2295 - MinusLogProbMetric: 29.2295 - val_loss: 30.4239 - val_MinusLogProbMetric: 30.4239 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 344/1000
2023-10-24 10:54:25.330 
Epoch 344/1000 
	 loss: 29.3567, MinusLogProbMetric: 29.3567, val_loss: 30.3877, val_MinusLogProbMetric: 30.3877

Epoch 344: val_loss did not improve from 29.36090
196/196 - 39s - loss: 29.3567 - MinusLogProbMetric: 29.3567 - val_loss: 30.3877 - val_MinusLogProbMetric: 30.3877 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 345/1000
2023-10-24 10:55:01.821 
Epoch 345/1000 
	 loss: 28.9502, MinusLogProbMetric: 28.9502, val_loss: 29.8138, val_MinusLogProbMetric: 29.8138

Epoch 345: val_loss did not improve from 29.36090
196/196 - 36s - loss: 28.9502 - MinusLogProbMetric: 28.9502 - val_loss: 29.8138 - val_MinusLogProbMetric: 29.8138 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 346/1000
2023-10-24 10:55:40.584 
Epoch 346/1000 
	 loss: 29.1112, MinusLogProbMetric: 29.1112, val_loss: 30.4076, val_MinusLogProbMetric: 30.4076

Epoch 346: val_loss did not improve from 29.36090
196/196 - 39s - loss: 29.1112 - MinusLogProbMetric: 29.1112 - val_loss: 30.4076 - val_MinusLogProbMetric: 30.4076 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 347/1000
2023-10-24 10:56:18.498 
Epoch 347/1000 
	 loss: 29.1354, MinusLogProbMetric: 29.1354, val_loss: 32.7823, val_MinusLogProbMetric: 32.7823

Epoch 347: val_loss did not improve from 29.36090
196/196 - 38s - loss: 29.1354 - MinusLogProbMetric: 29.1354 - val_loss: 32.7823 - val_MinusLogProbMetric: 32.7823 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 348/1000
2023-10-24 10:56:55.802 
Epoch 348/1000 
	 loss: 29.3103, MinusLogProbMetric: 29.3103, val_loss: 29.8305, val_MinusLogProbMetric: 29.8305

Epoch 348: val_loss did not improve from 29.36090
196/196 - 37s - loss: 29.3103 - MinusLogProbMetric: 29.3103 - val_loss: 29.8305 - val_MinusLogProbMetric: 29.8305 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 349/1000
2023-10-24 10:57:33.195 
Epoch 349/1000 
	 loss: 29.2761, MinusLogProbMetric: 29.2761, val_loss: 29.9274, val_MinusLogProbMetric: 29.9274

Epoch 349: val_loss did not improve from 29.36090
196/196 - 37s - loss: 29.2761 - MinusLogProbMetric: 29.2761 - val_loss: 29.9274 - val_MinusLogProbMetric: 29.9274 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 350/1000
2023-10-24 10:58:10.240 
Epoch 350/1000 
	 loss: 29.1480, MinusLogProbMetric: 29.1480, val_loss: 29.9666, val_MinusLogProbMetric: 29.9666

Epoch 350: val_loss did not improve from 29.36090
196/196 - 37s - loss: 29.1480 - MinusLogProbMetric: 29.1480 - val_loss: 29.9666 - val_MinusLogProbMetric: 29.9666 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 351/1000
2023-10-24 10:58:48.266 
Epoch 351/1000 
	 loss: 29.2769, MinusLogProbMetric: 29.2769, val_loss: 29.6162, val_MinusLogProbMetric: 29.6162

Epoch 351: val_loss did not improve from 29.36090
196/196 - 38s - loss: 29.2769 - MinusLogProbMetric: 29.2769 - val_loss: 29.6162 - val_MinusLogProbMetric: 29.6162 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 352/1000
2023-10-24 10:59:25.994 
Epoch 352/1000 
	 loss: 29.2929, MinusLogProbMetric: 29.2929, val_loss: 30.8844, val_MinusLogProbMetric: 30.8844

Epoch 352: val_loss did not improve from 29.36090
196/196 - 38s - loss: 29.2929 - MinusLogProbMetric: 29.2929 - val_loss: 30.8844 - val_MinusLogProbMetric: 30.8844 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 353/1000
2023-10-24 11:00:03.903 
Epoch 353/1000 
	 loss: 29.0102, MinusLogProbMetric: 29.0102, val_loss: 29.7490, val_MinusLogProbMetric: 29.7490

Epoch 353: val_loss did not improve from 29.36090
196/196 - 38s - loss: 29.0102 - MinusLogProbMetric: 29.0102 - val_loss: 29.7490 - val_MinusLogProbMetric: 29.7490 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 354/1000
2023-10-24 11:00:41.630 
Epoch 354/1000 
	 loss: 28.4286, MinusLogProbMetric: 28.4286, val_loss: 29.0775, val_MinusLogProbMetric: 29.0775

Epoch 354: val_loss improved from 29.36090 to 29.07754, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 38s - loss: 28.4286 - MinusLogProbMetric: 28.4286 - val_loss: 29.0775 - val_MinusLogProbMetric: 29.0775 - lr: 5.0000e-04 - 38s/epoch - 196ms/step
Epoch 355/1000
2023-10-24 11:01:19.616 
Epoch 355/1000 
	 loss: 28.3684, MinusLogProbMetric: 28.3684, val_loss: 29.1580, val_MinusLogProbMetric: 29.1580

Epoch 355: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.3684 - MinusLogProbMetric: 28.3684 - val_loss: 29.1580 - val_MinusLogProbMetric: 29.1580 - lr: 5.0000e-04 - 37s/epoch - 190ms/step
Epoch 356/1000
2023-10-24 11:01:57.514 
Epoch 356/1000 
	 loss: 28.4003, MinusLogProbMetric: 28.4003, val_loss: 29.1235, val_MinusLogProbMetric: 29.1235

Epoch 356: val_loss did not improve from 29.07754
196/196 - 38s - loss: 28.4003 - MinusLogProbMetric: 28.4003 - val_loss: 29.1235 - val_MinusLogProbMetric: 29.1235 - lr: 5.0000e-04 - 38s/epoch - 193ms/step
Epoch 357/1000
2023-10-24 11:02:35.008 
Epoch 357/1000 
	 loss: 28.3988, MinusLogProbMetric: 28.3988, val_loss: 29.1704, val_MinusLogProbMetric: 29.1704

Epoch 357: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.3988 - MinusLogProbMetric: 28.3988 - val_loss: 29.1704 - val_MinusLogProbMetric: 29.1704 - lr: 5.0000e-04 - 37s/epoch - 191ms/step
Epoch 358/1000
2023-10-24 11:03:13.250 
Epoch 358/1000 
	 loss: 28.4037, MinusLogProbMetric: 28.4037, val_loss: 29.2817, val_MinusLogProbMetric: 29.2817

Epoch 358: val_loss did not improve from 29.07754
196/196 - 38s - loss: 28.4037 - MinusLogProbMetric: 28.4037 - val_loss: 29.2817 - val_MinusLogProbMetric: 29.2817 - lr: 5.0000e-04 - 38s/epoch - 195ms/step
Epoch 359/1000
2023-10-24 11:03:52.268 
Epoch 359/1000 
	 loss: 28.4238, MinusLogProbMetric: 28.4238, val_loss: 29.1991, val_MinusLogProbMetric: 29.1991

Epoch 359: val_loss did not improve from 29.07754
196/196 - 39s - loss: 28.4238 - MinusLogProbMetric: 28.4238 - val_loss: 29.1991 - val_MinusLogProbMetric: 29.1991 - lr: 5.0000e-04 - 39s/epoch - 199ms/step
Epoch 360/1000
2023-10-24 11:04:30.368 
Epoch 360/1000 
	 loss: 28.4128, MinusLogProbMetric: 28.4128, val_loss: 29.1627, val_MinusLogProbMetric: 29.1627

Epoch 360: val_loss did not improve from 29.07754
196/196 - 38s - loss: 28.4128 - MinusLogProbMetric: 28.4128 - val_loss: 29.1627 - val_MinusLogProbMetric: 29.1627 - lr: 5.0000e-04 - 38s/epoch - 194ms/step
Epoch 361/1000
2023-10-24 11:05:07.647 
Epoch 361/1000 
	 loss: 28.3696, MinusLogProbMetric: 28.3696, val_loss: 29.2454, val_MinusLogProbMetric: 29.2454

Epoch 361: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.3696 - MinusLogProbMetric: 28.3696 - val_loss: 29.2454 - val_MinusLogProbMetric: 29.2454 - lr: 5.0000e-04 - 37s/epoch - 190ms/step
Epoch 362/1000
2023-10-24 11:05:46.172 
Epoch 362/1000 
	 loss: 28.3677, MinusLogProbMetric: 28.3677, val_loss: 29.5323, val_MinusLogProbMetric: 29.5323

Epoch 362: val_loss did not improve from 29.07754
196/196 - 39s - loss: 28.3677 - MinusLogProbMetric: 28.3677 - val_loss: 29.5323 - val_MinusLogProbMetric: 29.5323 - lr: 5.0000e-04 - 39s/epoch - 197ms/step
Epoch 363/1000
2023-10-24 11:06:23.168 
Epoch 363/1000 
	 loss: 28.3842, MinusLogProbMetric: 28.3842, val_loss: 29.2710, val_MinusLogProbMetric: 29.2710

Epoch 363: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.3842 - MinusLogProbMetric: 28.3842 - val_loss: 29.2710 - val_MinusLogProbMetric: 29.2710 - lr: 5.0000e-04 - 37s/epoch - 189ms/step
Epoch 364/1000
2023-10-24 11:07:00.918 
Epoch 364/1000 
	 loss: 28.3945, MinusLogProbMetric: 28.3945, val_loss: 29.5155, val_MinusLogProbMetric: 29.5155

Epoch 364: val_loss did not improve from 29.07754
196/196 - 38s - loss: 28.3945 - MinusLogProbMetric: 28.3945 - val_loss: 29.5155 - val_MinusLogProbMetric: 29.5155 - lr: 5.0000e-04 - 38s/epoch - 193ms/step
Epoch 365/1000
2023-10-24 11:07:39.118 
Epoch 365/1000 
	 loss: 28.3993, MinusLogProbMetric: 28.3993, val_loss: 29.6687, val_MinusLogProbMetric: 29.6687

Epoch 365: val_loss did not improve from 29.07754
196/196 - 38s - loss: 28.3993 - MinusLogProbMetric: 28.3993 - val_loss: 29.6687 - val_MinusLogProbMetric: 29.6687 - lr: 5.0000e-04 - 38s/epoch - 195ms/step
Epoch 366/1000
2023-10-24 11:08:16.225 
Epoch 366/1000 
	 loss: 28.5620, MinusLogProbMetric: 28.5620, val_loss: 29.3455, val_MinusLogProbMetric: 29.3455

Epoch 366: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.5620 - MinusLogProbMetric: 28.5620 - val_loss: 29.3455 - val_MinusLogProbMetric: 29.3455 - lr: 5.0000e-04 - 37s/epoch - 189ms/step
Epoch 367/1000
2023-10-24 11:08:54.438 
Epoch 367/1000 
	 loss: 28.3603, MinusLogProbMetric: 28.3603, val_loss: 29.2122, val_MinusLogProbMetric: 29.2122

Epoch 367: val_loss did not improve from 29.07754
196/196 - 38s - loss: 28.3603 - MinusLogProbMetric: 28.3603 - val_loss: 29.2122 - val_MinusLogProbMetric: 29.2122 - lr: 5.0000e-04 - 38s/epoch - 195ms/step
Epoch 368/1000
2023-10-24 11:09:31.419 
Epoch 368/1000 
	 loss: 28.4269, MinusLogProbMetric: 28.4269, val_loss: 29.2760, val_MinusLogProbMetric: 29.2760

Epoch 368: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.4269 - MinusLogProbMetric: 28.4269 - val_loss: 29.2760 - val_MinusLogProbMetric: 29.2760 - lr: 5.0000e-04 - 37s/epoch - 189ms/step
Epoch 369/1000
2023-10-24 11:10:06.140 
Epoch 369/1000 
	 loss: 28.4377, MinusLogProbMetric: 28.4377, val_loss: 29.6233, val_MinusLogProbMetric: 29.6233

Epoch 369: val_loss did not improve from 29.07754
196/196 - 35s - loss: 28.4377 - MinusLogProbMetric: 28.4377 - val_loss: 29.6233 - val_MinusLogProbMetric: 29.6233 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 370/1000
2023-10-24 11:10:41.938 
Epoch 370/1000 
	 loss: 28.3801, MinusLogProbMetric: 28.3801, val_loss: 29.1136, val_MinusLogProbMetric: 29.1136

Epoch 370: val_loss did not improve from 29.07754
196/196 - 36s - loss: 28.3801 - MinusLogProbMetric: 28.3801 - val_loss: 29.1136 - val_MinusLogProbMetric: 29.1136 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 371/1000
2023-10-24 11:11:18.962 
Epoch 371/1000 
	 loss: 28.4462, MinusLogProbMetric: 28.4462, val_loss: 29.3887, val_MinusLogProbMetric: 29.3887

Epoch 371: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.4462 - MinusLogProbMetric: 28.4462 - val_loss: 29.3887 - val_MinusLogProbMetric: 29.3887 - lr: 5.0000e-04 - 37s/epoch - 189ms/step
Epoch 372/1000
2023-10-24 11:11:54.776 
Epoch 372/1000 
	 loss: 28.3647, MinusLogProbMetric: 28.3647, val_loss: 29.1202, val_MinusLogProbMetric: 29.1202

Epoch 372: val_loss did not improve from 29.07754
196/196 - 36s - loss: 28.3647 - MinusLogProbMetric: 28.3647 - val_loss: 29.1202 - val_MinusLogProbMetric: 29.1202 - lr: 5.0000e-04 - 36s/epoch - 183ms/step
Epoch 373/1000
2023-10-24 11:12:32.023 
Epoch 373/1000 
	 loss: 28.4104, MinusLogProbMetric: 28.4104, val_loss: 29.1529, val_MinusLogProbMetric: 29.1529

Epoch 373: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.4104 - MinusLogProbMetric: 28.4104 - val_loss: 29.1529 - val_MinusLogProbMetric: 29.1529 - lr: 5.0000e-04 - 37s/epoch - 190ms/step
Epoch 374/1000
2023-10-24 11:13:09.329 
Epoch 374/1000 
	 loss: 28.4073, MinusLogProbMetric: 28.4073, val_loss: 29.2612, val_MinusLogProbMetric: 29.2612

Epoch 374: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.4073 - MinusLogProbMetric: 28.4073 - val_loss: 29.2612 - val_MinusLogProbMetric: 29.2612 - lr: 5.0000e-04 - 37s/epoch - 190ms/step
Epoch 375/1000
2023-10-24 11:13:46.439 
Epoch 375/1000 
	 loss: 28.3585, MinusLogProbMetric: 28.3585, val_loss: 29.1981, val_MinusLogProbMetric: 29.1981

Epoch 375: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.3585 - MinusLogProbMetric: 28.3585 - val_loss: 29.1981 - val_MinusLogProbMetric: 29.1981 - lr: 5.0000e-04 - 37s/epoch - 189ms/step
Epoch 376/1000
2023-10-24 11:14:23.379 
Epoch 376/1000 
	 loss: 28.4066, MinusLogProbMetric: 28.4066, val_loss: 29.3378, val_MinusLogProbMetric: 29.3378

Epoch 376: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.4066 - MinusLogProbMetric: 28.4066 - val_loss: 29.3378 - val_MinusLogProbMetric: 29.3378 - lr: 5.0000e-04 - 37s/epoch - 188ms/step
Epoch 377/1000
2023-10-24 11:15:01.533 
Epoch 377/1000 
	 loss: 28.4743, MinusLogProbMetric: 28.4743, val_loss: 30.3990, val_MinusLogProbMetric: 30.3990

Epoch 377: val_loss did not improve from 29.07754
196/196 - 38s - loss: 28.4743 - MinusLogProbMetric: 28.4743 - val_loss: 30.3990 - val_MinusLogProbMetric: 30.3990 - lr: 5.0000e-04 - 38s/epoch - 195ms/step
Epoch 378/1000
2023-10-24 11:15:40.308 
Epoch 378/1000 
	 loss: 28.3749, MinusLogProbMetric: 28.3749, val_loss: 29.3012, val_MinusLogProbMetric: 29.3012

Epoch 378: val_loss did not improve from 29.07754
196/196 - 39s - loss: 28.3749 - MinusLogProbMetric: 28.3749 - val_loss: 29.3012 - val_MinusLogProbMetric: 29.3012 - lr: 5.0000e-04 - 39s/epoch - 198ms/step
Epoch 379/1000
2023-10-24 11:16:17.503 
Epoch 379/1000 
	 loss: 28.3887, MinusLogProbMetric: 28.3887, val_loss: 29.1816, val_MinusLogProbMetric: 29.1816

Epoch 379: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.3887 - MinusLogProbMetric: 28.3887 - val_loss: 29.1816 - val_MinusLogProbMetric: 29.1816 - lr: 5.0000e-04 - 37s/epoch - 190ms/step
Epoch 380/1000
2023-10-24 11:16:54.115 
Epoch 380/1000 
	 loss: 28.3751, MinusLogProbMetric: 28.3751, val_loss: 29.1183, val_MinusLogProbMetric: 29.1183

Epoch 380: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.3751 - MinusLogProbMetric: 28.3751 - val_loss: 29.1183 - val_MinusLogProbMetric: 29.1183 - lr: 5.0000e-04 - 37s/epoch - 187ms/step
Epoch 381/1000
2023-10-24 11:17:31.329 
Epoch 381/1000 
	 loss: 28.5698, MinusLogProbMetric: 28.5698, val_loss: 29.1258, val_MinusLogProbMetric: 29.1258

Epoch 381: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.5698 - MinusLogProbMetric: 28.5698 - val_loss: 29.1258 - val_MinusLogProbMetric: 29.1258 - lr: 5.0000e-04 - 37s/epoch - 190ms/step
Epoch 382/1000
2023-10-24 11:18:08.043 
Epoch 382/1000 
	 loss: 28.3399, MinusLogProbMetric: 28.3399, val_loss: 29.2439, val_MinusLogProbMetric: 29.2439

Epoch 382: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.3399 - MinusLogProbMetric: 28.3399 - val_loss: 29.2439 - val_MinusLogProbMetric: 29.2439 - lr: 5.0000e-04 - 37s/epoch - 187ms/step
Epoch 383/1000
2023-10-24 11:18:44.192 
Epoch 383/1000 
	 loss: 28.4322, MinusLogProbMetric: 28.4322, val_loss: 29.1220, val_MinusLogProbMetric: 29.1220

Epoch 383: val_loss did not improve from 29.07754
196/196 - 36s - loss: 28.4322 - MinusLogProbMetric: 28.4322 - val_loss: 29.1220 - val_MinusLogProbMetric: 29.1220 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 384/1000
2023-10-24 11:19:20.775 
Epoch 384/1000 
	 loss: 28.3731, MinusLogProbMetric: 28.3731, val_loss: 29.1885, val_MinusLogProbMetric: 29.1885

Epoch 384: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.3731 - MinusLogProbMetric: 28.3731 - val_loss: 29.1885 - val_MinusLogProbMetric: 29.1885 - lr: 5.0000e-04 - 37s/epoch - 187ms/step
Epoch 385/1000
2023-10-24 11:19:57.631 
Epoch 385/1000 
	 loss: 28.3668, MinusLogProbMetric: 28.3668, val_loss: 29.2477, val_MinusLogProbMetric: 29.2477

Epoch 385: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.3668 - MinusLogProbMetric: 28.3668 - val_loss: 29.2477 - val_MinusLogProbMetric: 29.2477 - lr: 5.0000e-04 - 37s/epoch - 188ms/step
Epoch 386/1000
2023-10-24 11:20:35.922 
Epoch 386/1000 
	 loss: 28.3308, MinusLogProbMetric: 28.3308, val_loss: 29.2017, val_MinusLogProbMetric: 29.2017

Epoch 386: val_loss did not improve from 29.07754
196/196 - 38s - loss: 28.3308 - MinusLogProbMetric: 28.3308 - val_loss: 29.2017 - val_MinusLogProbMetric: 29.2017 - lr: 5.0000e-04 - 38s/epoch - 195ms/step
Epoch 387/1000
2023-10-24 11:21:14.561 
Epoch 387/1000 
	 loss: 28.3882, MinusLogProbMetric: 28.3882, val_loss: 29.3925, val_MinusLogProbMetric: 29.3925

Epoch 387: val_loss did not improve from 29.07754
196/196 - 39s - loss: 28.3882 - MinusLogProbMetric: 28.3882 - val_loss: 29.3925 - val_MinusLogProbMetric: 29.3925 - lr: 5.0000e-04 - 39s/epoch - 197ms/step
Epoch 388/1000
2023-10-24 11:21:51.244 
Epoch 388/1000 
	 loss: 28.4010, MinusLogProbMetric: 28.4010, val_loss: 29.4341, val_MinusLogProbMetric: 29.4341

Epoch 388: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.4010 - MinusLogProbMetric: 28.4010 - val_loss: 29.4341 - val_MinusLogProbMetric: 29.4341 - lr: 5.0000e-04 - 37s/epoch - 187ms/step
Epoch 389/1000
2023-10-24 11:22:26.887 
Epoch 389/1000 
	 loss: 28.4191, MinusLogProbMetric: 28.4191, val_loss: 29.0793, val_MinusLogProbMetric: 29.0793

Epoch 389: val_loss did not improve from 29.07754
196/196 - 36s - loss: 28.4191 - MinusLogProbMetric: 28.4191 - val_loss: 29.0793 - val_MinusLogProbMetric: 29.0793 - lr: 5.0000e-04 - 36s/epoch - 182ms/step
Epoch 390/1000
2023-10-24 11:23:04.984 
Epoch 390/1000 
	 loss: 28.3803, MinusLogProbMetric: 28.3803, val_loss: 29.1298, val_MinusLogProbMetric: 29.1298

Epoch 390: val_loss did not improve from 29.07754
196/196 - 38s - loss: 28.3803 - MinusLogProbMetric: 28.3803 - val_loss: 29.1298 - val_MinusLogProbMetric: 29.1298 - lr: 5.0000e-04 - 38s/epoch - 194ms/step
Epoch 391/1000
2023-10-24 11:23:42.664 
Epoch 391/1000 
	 loss: 28.3204, MinusLogProbMetric: 28.3204, val_loss: 29.1284, val_MinusLogProbMetric: 29.1284

Epoch 391: val_loss did not improve from 29.07754
196/196 - 38s - loss: 28.3204 - MinusLogProbMetric: 28.3204 - val_loss: 29.1284 - val_MinusLogProbMetric: 29.1284 - lr: 5.0000e-04 - 38s/epoch - 192ms/step
Epoch 392/1000
2023-10-24 11:24:20.273 
Epoch 392/1000 
	 loss: 28.4311, MinusLogProbMetric: 28.4311, val_loss: 29.2362, val_MinusLogProbMetric: 29.2362

Epoch 392: val_loss did not improve from 29.07754
196/196 - 38s - loss: 28.4311 - MinusLogProbMetric: 28.4311 - val_loss: 29.2362 - val_MinusLogProbMetric: 29.2362 - lr: 5.0000e-04 - 38s/epoch - 192ms/step
Epoch 393/1000
2023-10-24 11:24:58.596 
Epoch 393/1000 
	 loss: 28.3549, MinusLogProbMetric: 28.3549, val_loss: 29.4662, val_MinusLogProbMetric: 29.4662

Epoch 393: val_loss did not improve from 29.07754
196/196 - 38s - loss: 28.3549 - MinusLogProbMetric: 28.3549 - val_loss: 29.4662 - val_MinusLogProbMetric: 29.4662 - lr: 5.0000e-04 - 38s/epoch - 196ms/step
Epoch 394/1000
2023-10-24 11:25:36.562 
Epoch 394/1000 
	 loss: 28.4075, MinusLogProbMetric: 28.4075, val_loss: 29.1952, val_MinusLogProbMetric: 29.1952

Epoch 394: val_loss did not improve from 29.07754
196/196 - 38s - loss: 28.4075 - MinusLogProbMetric: 28.4075 - val_loss: 29.1952 - val_MinusLogProbMetric: 29.1952 - lr: 5.0000e-04 - 38s/epoch - 194ms/step
Epoch 395/1000
2023-10-24 11:26:13.344 
Epoch 395/1000 
	 loss: 28.4383, MinusLogProbMetric: 28.4383, val_loss: 29.1111, val_MinusLogProbMetric: 29.1111

Epoch 395: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.4383 - MinusLogProbMetric: 28.4383 - val_loss: 29.1111 - val_MinusLogProbMetric: 29.1111 - lr: 5.0000e-04 - 37s/epoch - 188ms/step
Epoch 396/1000
2023-10-24 11:26:50.773 
Epoch 396/1000 
	 loss: 28.4422, MinusLogProbMetric: 28.4422, val_loss: 29.3057, val_MinusLogProbMetric: 29.3057

Epoch 396: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.4422 - MinusLogProbMetric: 28.4422 - val_loss: 29.3057 - val_MinusLogProbMetric: 29.3057 - lr: 5.0000e-04 - 37s/epoch - 191ms/step
Epoch 397/1000
2023-10-24 11:27:27.471 
Epoch 397/1000 
	 loss: 28.3737, MinusLogProbMetric: 28.3737, val_loss: 30.0526, val_MinusLogProbMetric: 30.0526

Epoch 397: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.3737 - MinusLogProbMetric: 28.3737 - val_loss: 30.0526 - val_MinusLogProbMetric: 30.0526 - lr: 5.0000e-04 - 37s/epoch - 187ms/step
Epoch 398/1000
2023-10-24 11:28:03.925 
Epoch 398/1000 
	 loss: 28.4166, MinusLogProbMetric: 28.4166, val_loss: 29.1631, val_MinusLogProbMetric: 29.1631

Epoch 398: val_loss did not improve from 29.07754
196/196 - 36s - loss: 28.4166 - MinusLogProbMetric: 28.4166 - val_loss: 29.1631 - val_MinusLogProbMetric: 29.1631 - lr: 5.0000e-04 - 36s/epoch - 186ms/step
Epoch 399/1000
2023-10-24 11:28:40.687 
Epoch 399/1000 
	 loss: 28.3339, MinusLogProbMetric: 28.3339, val_loss: 29.1905, val_MinusLogProbMetric: 29.1905

Epoch 399: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.3339 - MinusLogProbMetric: 28.3339 - val_loss: 29.1905 - val_MinusLogProbMetric: 29.1905 - lr: 5.0000e-04 - 37s/epoch - 188ms/step
Epoch 400/1000
2023-10-24 11:29:17.464 
Epoch 400/1000 
	 loss: 28.4295, MinusLogProbMetric: 28.4295, val_loss: 29.3179, val_MinusLogProbMetric: 29.3179

Epoch 400: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.4295 - MinusLogProbMetric: 28.4295 - val_loss: 29.3179 - val_MinusLogProbMetric: 29.3179 - lr: 5.0000e-04 - 37s/epoch - 188ms/step
Epoch 401/1000
2023-10-24 11:29:55.937 
Epoch 401/1000 
	 loss: 28.4257, MinusLogProbMetric: 28.4257, val_loss: 29.2719, val_MinusLogProbMetric: 29.2719

Epoch 401: val_loss did not improve from 29.07754
196/196 - 38s - loss: 28.4257 - MinusLogProbMetric: 28.4257 - val_loss: 29.2719 - val_MinusLogProbMetric: 29.2719 - lr: 5.0000e-04 - 38s/epoch - 196ms/step
Epoch 402/1000
2023-10-24 11:30:34.716 
Epoch 402/1000 
	 loss: 28.3369, MinusLogProbMetric: 28.3369, val_loss: 29.1043, val_MinusLogProbMetric: 29.1043

Epoch 402: val_loss did not improve from 29.07754
196/196 - 39s - loss: 28.3369 - MinusLogProbMetric: 28.3369 - val_loss: 29.1043 - val_MinusLogProbMetric: 29.1043 - lr: 5.0000e-04 - 39s/epoch - 198ms/step
Epoch 403/1000
2023-10-24 11:31:11.370 
Epoch 403/1000 
	 loss: 28.3042, MinusLogProbMetric: 28.3042, val_loss: 29.2002, val_MinusLogProbMetric: 29.2002

Epoch 403: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.3042 - MinusLogProbMetric: 28.3042 - val_loss: 29.2002 - val_MinusLogProbMetric: 29.2002 - lr: 5.0000e-04 - 37s/epoch - 187ms/step
Epoch 404/1000
2023-10-24 11:31:48.299 
Epoch 404/1000 
	 loss: 28.3111, MinusLogProbMetric: 28.3111, val_loss: 29.1945, val_MinusLogProbMetric: 29.1945

Epoch 404: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.3111 - MinusLogProbMetric: 28.3111 - val_loss: 29.1945 - val_MinusLogProbMetric: 29.1945 - lr: 5.0000e-04 - 37s/epoch - 188ms/step
Epoch 405/1000
2023-10-24 11:32:25.248 
Epoch 405/1000 
	 loss: 28.0779, MinusLogProbMetric: 28.0779, val_loss: 29.1475, val_MinusLogProbMetric: 29.1475

Epoch 405: val_loss did not improve from 29.07754
196/196 - 37s - loss: 28.0779 - MinusLogProbMetric: 28.0779 - val_loss: 29.1475 - val_MinusLogProbMetric: 29.1475 - lr: 2.5000e-04 - 37s/epoch - 188ms/step
Epoch 406/1000
2023-10-24 11:33:02.648 
Epoch 406/1000 
	 loss: 28.0741, MinusLogProbMetric: 28.0741, val_loss: 29.0244, val_MinusLogProbMetric: 29.0244

Epoch 406: val_loss improved from 29.07754 to 29.02440, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 38s - loss: 28.0741 - MinusLogProbMetric: 28.0741 - val_loss: 29.0244 - val_MinusLogProbMetric: 29.0244 - lr: 2.5000e-04 - 38s/epoch - 194ms/step
Epoch 407/1000
2023-10-24 11:33:41.530 
Epoch 407/1000 
	 loss: 28.0554, MinusLogProbMetric: 28.0554, val_loss: 29.1145, val_MinusLogProbMetric: 29.1145

Epoch 407: val_loss did not improve from 29.02440
196/196 - 38s - loss: 28.0554 - MinusLogProbMetric: 28.0554 - val_loss: 29.1145 - val_MinusLogProbMetric: 29.1145 - lr: 2.5000e-04 - 38s/epoch - 195ms/step
Epoch 408/1000
2023-10-24 11:34:17.472 
Epoch 408/1000 
	 loss: 28.0462, MinusLogProbMetric: 28.0462, val_loss: 28.9645, val_MinusLogProbMetric: 28.9645

Epoch 408: val_loss improved from 29.02440 to 28.96451, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 37s - loss: 28.0462 - MinusLogProbMetric: 28.0462 - val_loss: 28.9645 - val_MinusLogProbMetric: 28.9645 - lr: 2.5000e-04 - 37s/epoch - 187ms/step
Epoch 409/1000
2023-10-24 11:34:54.684 
Epoch 409/1000 
	 loss: 28.0842, MinusLogProbMetric: 28.0842, val_loss: 29.0119, val_MinusLogProbMetric: 29.0119

Epoch 409: val_loss did not improve from 28.96451
196/196 - 37s - loss: 28.0842 - MinusLogProbMetric: 28.0842 - val_loss: 29.0119 - val_MinusLogProbMetric: 29.0119 - lr: 2.5000e-04 - 37s/epoch - 186ms/step
Epoch 410/1000
2023-10-24 11:35:31.239 
Epoch 410/1000 
	 loss: 28.0575, MinusLogProbMetric: 28.0575, val_loss: 29.0002, val_MinusLogProbMetric: 29.0002

Epoch 410: val_loss did not improve from 28.96451
196/196 - 37s - loss: 28.0575 - MinusLogProbMetric: 28.0575 - val_loss: 29.0002 - val_MinusLogProbMetric: 29.0002 - lr: 2.5000e-04 - 37s/epoch - 186ms/step
Epoch 411/1000
2023-10-24 11:36:07.708 
Epoch 411/1000 
	 loss: 28.0784, MinusLogProbMetric: 28.0784, val_loss: 28.9089, val_MinusLogProbMetric: 28.9089

Epoch 411: val_loss improved from 28.96451 to 28.90887, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 37s - loss: 28.0784 - MinusLogProbMetric: 28.0784 - val_loss: 28.9089 - val_MinusLogProbMetric: 28.9089 - lr: 2.5000e-04 - 37s/epoch - 190ms/step
Epoch 412/1000
2023-10-24 11:36:45.834 
Epoch 412/1000 
	 loss: 28.0481, MinusLogProbMetric: 28.0481, val_loss: 28.9745, val_MinusLogProbMetric: 28.9745

Epoch 412: val_loss did not improve from 28.90887
196/196 - 37s - loss: 28.0481 - MinusLogProbMetric: 28.0481 - val_loss: 28.9745 - val_MinusLogProbMetric: 28.9745 - lr: 2.5000e-04 - 37s/epoch - 191ms/step
Epoch 413/1000
2023-10-24 11:37:22.596 
Epoch 413/1000 
	 loss: 28.0359, MinusLogProbMetric: 28.0359, val_loss: 28.9534, val_MinusLogProbMetric: 28.9534

Epoch 413: val_loss did not improve from 28.90887
196/196 - 37s - loss: 28.0359 - MinusLogProbMetric: 28.0359 - val_loss: 28.9534 - val_MinusLogProbMetric: 28.9534 - lr: 2.5000e-04 - 37s/epoch - 188ms/step
Epoch 414/1000
2023-10-24 11:38:01.314 
Epoch 414/1000 
	 loss: 28.0411, MinusLogProbMetric: 28.0411, val_loss: 28.9676, val_MinusLogProbMetric: 28.9676

Epoch 414: val_loss did not improve from 28.90887
196/196 - 39s - loss: 28.0411 - MinusLogProbMetric: 28.0411 - val_loss: 28.9676 - val_MinusLogProbMetric: 28.9676 - lr: 2.5000e-04 - 39s/epoch - 198ms/step
Epoch 415/1000
2023-10-24 11:38:38.680 
Epoch 415/1000 
	 loss: 28.0381, MinusLogProbMetric: 28.0381, val_loss: 28.9452, val_MinusLogProbMetric: 28.9452

Epoch 415: val_loss did not improve from 28.90887
196/196 - 37s - loss: 28.0381 - MinusLogProbMetric: 28.0381 - val_loss: 28.9452 - val_MinusLogProbMetric: 28.9452 - lr: 2.5000e-04 - 37s/epoch - 191ms/step
Epoch 416/1000
2023-10-24 11:39:16.435 
Epoch 416/1000 
	 loss: 28.0370, MinusLogProbMetric: 28.0370, val_loss: 29.0031, val_MinusLogProbMetric: 29.0031

Epoch 416: val_loss did not improve from 28.90887
196/196 - 38s - loss: 28.0370 - MinusLogProbMetric: 28.0370 - val_loss: 29.0031 - val_MinusLogProbMetric: 29.0031 - lr: 2.5000e-04 - 38s/epoch - 193ms/step
Epoch 417/1000
2023-10-24 11:39:53.050 
Epoch 417/1000 
	 loss: 28.0469, MinusLogProbMetric: 28.0469, val_loss: 28.9194, val_MinusLogProbMetric: 28.9194

Epoch 417: val_loss did not improve from 28.90887
196/196 - 37s - loss: 28.0469 - MinusLogProbMetric: 28.0469 - val_loss: 28.9194 - val_MinusLogProbMetric: 28.9194 - lr: 2.5000e-04 - 37s/epoch - 187ms/step
Epoch 418/1000
2023-10-24 11:40:29.730 
Epoch 418/1000 
	 loss: 28.0297, MinusLogProbMetric: 28.0297, val_loss: 28.9027, val_MinusLogProbMetric: 28.9027

Epoch 418: val_loss improved from 28.90887 to 28.90267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 37s - loss: 28.0297 - MinusLogProbMetric: 28.0297 - val_loss: 28.9027 - val_MinusLogProbMetric: 28.9027 - lr: 2.5000e-04 - 37s/epoch - 190ms/step
Epoch 419/1000
2023-10-24 11:41:07.348 
Epoch 419/1000 
	 loss: 28.0669, MinusLogProbMetric: 28.0669, val_loss: 29.0394, val_MinusLogProbMetric: 29.0394

Epoch 419: val_loss did not improve from 28.90267
196/196 - 37s - loss: 28.0669 - MinusLogProbMetric: 28.0669 - val_loss: 29.0394 - val_MinusLogProbMetric: 29.0394 - lr: 2.5000e-04 - 37s/epoch - 189ms/step
Epoch 420/1000
2023-10-24 11:41:44.005 
Epoch 420/1000 
	 loss: 28.1087, MinusLogProbMetric: 28.1087, val_loss: 28.9421, val_MinusLogProbMetric: 28.9421

Epoch 420: val_loss did not improve from 28.90267
196/196 - 37s - loss: 28.1087 - MinusLogProbMetric: 28.1087 - val_loss: 28.9421 - val_MinusLogProbMetric: 28.9421 - lr: 2.5000e-04 - 37s/epoch - 187ms/step
Epoch 421/1000
2023-10-24 11:42:20.686 
Epoch 421/1000 
	 loss: 28.0422, MinusLogProbMetric: 28.0422, val_loss: 29.2539, val_MinusLogProbMetric: 29.2539

Epoch 421: val_loss did not improve from 28.90267
196/196 - 37s - loss: 28.0422 - MinusLogProbMetric: 28.0422 - val_loss: 29.2539 - val_MinusLogProbMetric: 29.2539 - lr: 2.5000e-04 - 37s/epoch - 187ms/step
Epoch 422/1000
2023-10-24 11:42:56.051 
Epoch 422/1000 
	 loss: 28.0578, MinusLogProbMetric: 28.0578, val_loss: 28.9423, val_MinusLogProbMetric: 28.9423

Epoch 422: val_loss did not improve from 28.90267
196/196 - 35s - loss: 28.0578 - MinusLogProbMetric: 28.0578 - val_loss: 28.9423 - val_MinusLogProbMetric: 28.9423 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 423/1000
2023-10-24 11:43:31.973 
Epoch 423/1000 
	 loss: 28.0813, MinusLogProbMetric: 28.0813, val_loss: 28.9909, val_MinusLogProbMetric: 28.9909

Epoch 423: val_loss did not improve from 28.90267
196/196 - 36s - loss: 28.0813 - MinusLogProbMetric: 28.0813 - val_loss: 28.9909 - val_MinusLogProbMetric: 28.9909 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 424/1000
2023-10-24 11:44:10.597 
Epoch 424/1000 
	 loss: 28.0301, MinusLogProbMetric: 28.0301, val_loss: 28.9357, val_MinusLogProbMetric: 28.9357

Epoch 424: val_loss did not improve from 28.90267
196/196 - 39s - loss: 28.0301 - MinusLogProbMetric: 28.0301 - val_loss: 28.9357 - val_MinusLogProbMetric: 28.9357 - lr: 2.5000e-04 - 39s/epoch - 197ms/step
Epoch 425/1000
2023-10-24 11:44:47.440 
Epoch 425/1000 
	 loss: 28.0460, MinusLogProbMetric: 28.0460, val_loss: 28.9847, val_MinusLogProbMetric: 28.9847

Epoch 425: val_loss did not improve from 28.90267
196/196 - 37s - loss: 28.0460 - MinusLogProbMetric: 28.0460 - val_loss: 28.9847 - val_MinusLogProbMetric: 28.9847 - lr: 2.5000e-04 - 37s/epoch - 188ms/step
Epoch 426/1000
2023-10-24 11:45:24.522 
Epoch 426/1000 
	 loss: 28.0352, MinusLogProbMetric: 28.0352, val_loss: 28.9748, val_MinusLogProbMetric: 28.9748

Epoch 426: val_loss did not improve from 28.90267
196/196 - 37s - loss: 28.0352 - MinusLogProbMetric: 28.0352 - val_loss: 28.9748 - val_MinusLogProbMetric: 28.9748 - lr: 2.5000e-04 - 37s/epoch - 189ms/step
Epoch 427/1000
2023-10-24 11:46:01.613 
Epoch 427/1000 
	 loss: 28.0401, MinusLogProbMetric: 28.0401, val_loss: 28.9408, val_MinusLogProbMetric: 28.9408

Epoch 427: val_loss did not improve from 28.90267
196/196 - 37s - loss: 28.0401 - MinusLogProbMetric: 28.0401 - val_loss: 28.9408 - val_MinusLogProbMetric: 28.9408 - lr: 2.5000e-04 - 37s/epoch - 189ms/step
Epoch 428/1000
2023-10-24 11:46:37.439 
Epoch 428/1000 
	 loss: 28.0333, MinusLogProbMetric: 28.0333, val_loss: 28.9341, val_MinusLogProbMetric: 28.9341

Epoch 428: val_loss did not improve from 28.90267
196/196 - 36s - loss: 28.0333 - MinusLogProbMetric: 28.0333 - val_loss: 28.9341 - val_MinusLogProbMetric: 28.9341 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 429/1000
2023-10-24 11:47:14.520 
Epoch 429/1000 
	 loss: 28.0557, MinusLogProbMetric: 28.0557, val_loss: 28.9687, val_MinusLogProbMetric: 28.9687

Epoch 429: val_loss did not improve from 28.90267
196/196 - 37s - loss: 28.0557 - MinusLogProbMetric: 28.0557 - val_loss: 28.9687 - val_MinusLogProbMetric: 28.9687 - lr: 2.5000e-04 - 37s/epoch - 189ms/step
Epoch 430/1000
2023-10-24 11:47:51.233 
Epoch 430/1000 
	 loss: 28.0804, MinusLogProbMetric: 28.0804, val_loss: 28.9890, val_MinusLogProbMetric: 28.9890

Epoch 430: val_loss did not improve from 28.90267
196/196 - 37s - loss: 28.0804 - MinusLogProbMetric: 28.0804 - val_loss: 28.9890 - val_MinusLogProbMetric: 28.9890 - lr: 2.5000e-04 - 37s/epoch - 187ms/step
Epoch 431/1000
2023-10-24 11:48:27.032 
Epoch 431/1000 
	 loss: 28.0394, MinusLogProbMetric: 28.0394, val_loss: 29.2197, val_MinusLogProbMetric: 29.2197

Epoch 431: val_loss did not improve from 28.90267
196/196 - 36s - loss: 28.0394 - MinusLogProbMetric: 28.0394 - val_loss: 29.2197 - val_MinusLogProbMetric: 29.2197 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 432/1000
2023-10-24 11:49:03.892 
Epoch 432/1000 
	 loss: 28.0615, MinusLogProbMetric: 28.0615, val_loss: 29.1732, val_MinusLogProbMetric: 29.1732

Epoch 432: val_loss did not improve from 28.90267
196/196 - 37s - loss: 28.0615 - MinusLogProbMetric: 28.0615 - val_loss: 29.1732 - val_MinusLogProbMetric: 29.1732 - lr: 2.5000e-04 - 37s/epoch - 188ms/step
Epoch 433/1000
2023-10-24 11:49:39.056 
Epoch 433/1000 
	 loss: 28.0532, MinusLogProbMetric: 28.0532, val_loss: 28.9673, val_MinusLogProbMetric: 28.9673

Epoch 433: val_loss did not improve from 28.90267
196/196 - 35s - loss: 28.0532 - MinusLogProbMetric: 28.0532 - val_loss: 28.9673 - val_MinusLogProbMetric: 28.9673 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 434/1000
2023-10-24 11:50:15.438 
Epoch 434/1000 
	 loss: 28.0796, MinusLogProbMetric: 28.0796, val_loss: 29.0727, val_MinusLogProbMetric: 29.0727

Epoch 434: val_loss did not improve from 28.90267
196/196 - 36s - loss: 28.0796 - MinusLogProbMetric: 28.0796 - val_loss: 29.0727 - val_MinusLogProbMetric: 29.0727 - lr: 2.5000e-04 - 36s/epoch - 186ms/step
Epoch 435/1000
2023-10-24 11:50:51.416 
Epoch 435/1000 
	 loss: 28.0379, MinusLogProbMetric: 28.0379, val_loss: 29.0435, val_MinusLogProbMetric: 29.0435

Epoch 435: val_loss did not improve from 28.90267
196/196 - 36s - loss: 28.0379 - MinusLogProbMetric: 28.0379 - val_loss: 29.0435 - val_MinusLogProbMetric: 29.0435 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 436/1000
2023-10-24 11:51:30.252 
Epoch 436/1000 
	 loss: 28.0289, MinusLogProbMetric: 28.0289, val_loss: 29.3439, val_MinusLogProbMetric: 29.3439

Epoch 436: val_loss did not improve from 28.90267
196/196 - 39s - loss: 28.0289 - MinusLogProbMetric: 28.0289 - val_loss: 29.3439 - val_MinusLogProbMetric: 29.3439 - lr: 2.5000e-04 - 39s/epoch - 198ms/step
Epoch 437/1000
2023-10-24 11:52:08.343 
Epoch 437/1000 
	 loss: 28.0176, MinusLogProbMetric: 28.0176, val_loss: 29.0181, val_MinusLogProbMetric: 29.0181

Epoch 437: val_loss did not improve from 28.90267
196/196 - 38s - loss: 28.0176 - MinusLogProbMetric: 28.0176 - val_loss: 29.0181 - val_MinusLogProbMetric: 29.0181 - lr: 2.5000e-04 - 38s/epoch - 194ms/step
Epoch 438/1000
2023-10-24 11:52:45.329 
Epoch 438/1000 
	 loss: 28.0526, MinusLogProbMetric: 28.0526, val_loss: 28.9440, val_MinusLogProbMetric: 28.9440

Epoch 438: val_loss did not improve from 28.90267
196/196 - 37s - loss: 28.0526 - MinusLogProbMetric: 28.0526 - val_loss: 28.9440 - val_MinusLogProbMetric: 28.9440 - lr: 2.5000e-04 - 37s/epoch - 189ms/step
Epoch 439/1000
2023-10-24 11:53:22.298 
Epoch 439/1000 
	 loss: 28.0187, MinusLogProbMetric: 28.0187, val_loss: 29.0369, val_MinusLogProbMetric: 29.0369

Epoch 439: val_loss did not improve from 28.90267
196/196 - 37s - loss: 28.0187 - MinusLogProbMetric: 28.0187 - val_loss: 29.0369 - val_MinusLogProbMetric: 29.0369 - lr: 2.5000e-04 - 37s/epoch - 189ms/step
Epoch 440/1000
2023-10-24 11:53:58.884 
Epoch 440/1000 
	 loss: 28.0258, MinusLogProbMetric: 28.0258, val_loss: 28.9418, val_MinusLogProbMetric: 28.9418

Epoch 440: val_loss did not improve from 28.90267
196/196 - 37s - loss: 28.0258 - MinusLogProbMetric: 28.0258 - val_loss: 28.9418 - val_MinusLogProbMetric: 28.9418 - lr: 2.5000e-04 - 37s/epoch - 187ms/step
Epoch 441/1000
2023-10-24 11:54:35.394 
Epoch 441/1000 
	 loss: 28.0285, MinusLogProbMetric: 28.0285, val_loss: 28.9775, val_MinusLogProbMetric: 28.9775

Epoch 441: val_loss did not improve from 28.90267
196/196 - 37s - loss: 28.0285 - MinusLogProbMetric: 28.0285 - val_loss: 28.9775 - val_MinusLogProbMetric: 28.9775 - lr: 2.5000e-04 - 37s/epoch - 186ms/step
Epoch 442/1000
2023-10-24 11:55:12.241 
Epoch 442/1000 
	 loss: 28.0580, MinusLogProbMetric: 28.0580, val_loss: 29.1491, val_MinusLogProbMetric: 29.1491

Epoch 442: val_loss did not improve from 28.90267
196/196 - 37s - loss: 28.0580 - MinusLogProbMetric: 28.0580 - val_loss: 29.1491 - val_MinusLogProbMetric: 29.1491 - lr: 2.5000e-04 - 37s/epoch - 188ms/step
Epoch 443/1000
2023-10-24 11:55:50.582 
Epoch 443/1000 
	 loss: 28.0175, MinusLogProbMetric: 28.0175, val_loss: 28.9866, val_MinusLogProbMetric: 28.9866

Epoch 443: val_loss did not improve from 28.90267
196/196 - 38s - loss: 28.0175 - MinusLogProbMetric: 28.0175 - val_loss: 28.9866 - val_MinusLogProbMetric: 28.9866 - lr: 2.5000e-04 - 38s/epoch - 196ms/step
Epoch 444/1000
2023-10-24 11:56:27.945 
Epoch 444/1000 
	 loss: 28.0451, MinusLogProbMetric: 28.0451, val_loss: 29.3053, val_MinusLogProbMetric: 29.3053

Epoch 444: val_loss did not improve from 28.90267
196/196 - 37s - loss: 28.0451 - MinusLogProbMetric: 28.0451 - val_loss: 29.3053 - val_MinusLogProbMetric: 29.3053 - lr: 2.5000e-04 - 37s/epoch - 191ms/step
Epoch 445/1000
2023-10-24 11:57:06.340 
Epoch 445/1000 
	 loss: 28.0472, MinusLogProbMetric: 28.0472, val_loss: 28.9246, val_MinusLogProbMetric: 28.9246

Epoch 445: val_loss did not improve from 28.90267
196/196 - 38s - loss: 28.0472 - MinusLogProbMetric: 28.0472 - val_loss: 28.9246 - val_MinusLogProbMetric: 28.9246 - lr: 2.5000e-04 - 38s/epoch - 196ms/step
Epoch 446/1000
2023-10-24 11:57:42.204 
Epoch 446/1000 
	 loss: 28.0391, MinusLogProbMetric: 28.0391, val_loss: 29.0726, val_MinusLogProbMetric: 29.0726

Epoch 446: val_loss did not improve from 28.90267
196/196 - 36s - loss: 28.0391 - MinusLogProbMetric: 28.0391 - val_loss: 29.0726 - val_MinusLogProbMetric: 29.0726 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 447/1000
2023-10-24 11:58:17.730 
Epoch 447/1000 
	 loss: 28.0376, MinusLogProbMetric: 28.0376, val_loss: 29.4068, val_MinusLogProbMetric: 29.4068

Epoch 447: val_loss did not improve from 28.90267
196/196 - 36s - loss: 28.0376 - MinusLogProbMetric: 28.0376 - val_loss: 29.4068 - val_MinusLogProbMetric: 29.4068 - lr: 2.5000e-04 - 36s/epoch - 181ms/step
Epoch 448/1000
2023-10-24 11:58:55.833 
Epoch 448/1000 
	 loss: 28.0114, MinusLogProbMetric: 28.0114, val_loss: 28.9151, val_MinusLogProbMetric: 28.9151

Epoch 448: val_loss did not improve from 28.90267
196/196 - 38s - loss: 28.0114 - MinusLogProbMetric: 28.0114 - val_loss: 28.9151 - val_MinusLogProbMetric: 28.9151 - lr: 2.5000e-04 - 38s/epoch - 194ms/step
Epoch 449/1000
2023-10-24 11:59:34.173 
Epoch 449/1000 
	 loss: 28.0154, MinusLogProbMetric: 28.0154, val_loss: 28.9154, val_MinusLogProbMetric: 28.9154

Epoch 449: val_loss did not improve from 28.90267
196/196 - 38s - loss: 28.0154 - MinusLogProbMetric: 28.0154 - val_loss: 28.9154 - val_MinusLogProbMetric: 28.9154 - lr: 2.5000e-04 - 38s/epoch - 196ms/step
Epoch 450/1000
2023-10-24 12:00:12.643 
Epoch 450/1000 
	 loss: 28.0569, MinusLogProbMetric: 28.0569, val_loss: 28.9344, val_MinusLogProbMetric: 28.9344

Epoch 450: val_loss did not improve from 28.90267
196/196 - 38s - loss: 28.0569 - MinusLogProbMetric: 28.0569 - val_loss: 28.9344 - val_MinusLogProbMetric: 28.9344 - lr: 2.5000e-04 - 38s/epoch - 196ms/step
Epoch 451/1000
2023-10-24 12:00:50.519 
Epoch 451/1000 
	 loss: 28.0227, MinusLogProbMetric: 28.0227, val_loss: 29.1328, val_MinusLogProbMetric: 29.1328

Epoch 451: val_loss did not improve from 28.90267
196/196 - 38s - loss: 28.0227 - MinusLogProbMetric: 28.0227 - val_loss: 29.1328 - val_MinusLogProbMetric: 29.1328 - lr: 2.5000e-04 - 38s/epoch - 193ms/step
Epoch 452/1000
2023-10-24 12:01:28.520 
Epoch 452/1000 
	 loss: 28.0232, MinusLogProbMetric: 28.0232, val_loss: 28.9758, val_MinusLogProbMetric: 28.9758

Epoch 452: val_loss did not improve from 28.90267
196/196 - 38s - loss: 28.0232 - MinusLogProbMetric: 28.0232 - val_loss: 28.9758 - val_MinusLogProbMetric: 28.9758 - lr: 2.5000e-04 - 38s/epoch - 194ms/step
Epoch 453/1000
2023-10-24 12:02:06.385 
Epoch 453/1000 
	 loss: 28.0358, MinusLogProbMetric: 28.0358, val_loss: 28.9827, val_MinusLogProbMetric: 28.9827

Epoch 453: val_loss did not improve from 28.90267
196/196 - 38s - loss: 28.0358 - MinusLogProbMetric: 28.0358 - val_loss: 28.9827 - val_MinusLogProbMetric: 28.9827 - lr: 2.5000e-04 - 38s/epoch - 193ms/step
Epoch 454/1000
2023-10-24 12:02:45.011 
Epoch 454/1000 
	 loss: 28.0075, MinusLogProbMetric: 28.0075, val_loss: 28.9363, val_MinusLogProbMetric: 28.9363

Epoch 454: val_loss did not improve from 28.90267
196/196 - 39s - loss: 28.0075 - MinusLogProbMetric: 28.0075 - val_loss: 28.9363 - val_MinusLogProbMetric: 28.9363 - lr: 2.5000e-04 - 39s/epoch - 197ms/step
Epoch 455/1000
2023-10-24 12:03:23.564 
Epoch 455/1000 
	 loss: 28.0390, MinusLogProbMetric: 28.0390, val_loss: 29.0475, val_MinusLogProbMetric: 29.0475

Epoch 455: val_loss did not improve from 28.90267
196/196 - 39s - loss: 28.0390 - MinusLogProbMetric: 28.0390 - val_loss: 29.0475 - val_MinusLogProbMetric: 29.0475 - lr: 2.5000e-04 - 39s/epoch - 197ms/step
Epoch 456/1000
2023-10-24 12:04:02.458 
Epoch 456/1000 
	 loss: 28.0073, MinusLogProbMetric: 28.0073, val_loss: 29.1389, val_MinusLogProbMetric: 29.1389

Epoch 456: val_loss did not improve from 28.90267
196/196 - 39s - loss: 28.0073 - MinusLogProbMetric: 28.0073 - val_loss: 29.1389 - val_MinusLogProbMetric: 29.1389 - lr: 2.5000e-04 - 39s/epoch - 198ms/step
Epoch 457/1000
2023-10-24 12:04:38.916 
Epoch 457/1000 
	 loss: 28.0112, MinusLogProbMetric: 28.0112, val_loss: 29.0548, val_MinusLogProbMetric: 29.0548

Epoch 457: val_loss did not improve from 28.90267
196/196 - 36s - loss: 28.0112 - MinusLogProbMetric: 28.0112 - val_loss: 29.0548 - val_MinusLogProbMetric: 29.0548 - lr: 2.5000e-04 - 36s/epoch - 186ms/step
Epoch 458/1000
2023-10-24 12:05:15.968 
Epoch 458/1000 
	 loss: 28.0166, MinusLogProbMetric: 28.0166, val_loss: 28.9224, val_MinusLogProbMetric: 28.9224

Epoch 458: val_loss did not improve from 28.90267
196/196 - 37s - loss: 28.0166 - MinusLogProbMetric: 28.0166 - val_loss: 28.9224 - val_MinusLogProbMetric: 28.9224 - lr: 2.5000e-04 - 37s/epoch - 189ms/step
Epoch 459/1000
2023-10-24 12:05:54.122 
Epoch 459/1000 
	 loss: 28.0389, MinusLogProbMetric: 28.0389, val_loss: 28.9932, val_MinusLogProbMetric: 28.9932

Epoch 459: val_loss did not improve from 28.90267
196/196 - 38s - loss: 28.0389 - MinusLogProbMetric: 28.0389 - val_loss: 28.9932 - val_MinusLogProbMetric: 28.9932 - lr: 2.5000e-04 - 38s/epoch - 195ms/step
Epoch 460/1000
2023-10-24 12:06:30.071 
Epoch 460/1000 
	 loss: 28.0241, MinusLogProbMetric: 28.0241, val_loss: 28.9138, val_MinusLogProbMetric: 28.9138

Epoch 460: val_loss did not improve from 28.90267
196/196 - 36s - loss: 28.0241 - MinusLogProbMetric: 28.0241 - val_loss: 28.9138 - val_MinusLogProbMetric: 28.9138 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 461/1000
2023-10-24 12:07:08.198 
Epoch 461/1000 
	 loss: 28.0051, MinusLogProbMetric: 28.0051, val_loss: 29.0086, val_MinusLogProbMetric: 29.0086

Epoch 461: val_loss did not improve from 28.90267
196/196 - 38s - loss: 28.0051 - MinusLogProbMetric: 28.0051 - val_loss: 29.0086 - val_MinusLogProbMetric: 29.0086 - lr: 2.5000e-04 - 38s/epoch - 195ms/step
Epoch 462/1000
2023-10-24 12:07:47.125 
Epoch 462/1000 
	 loss: 28.0021, MinusLogProbMetric: 28.0021, val_loss: 29.0096, val_MinusLogProbMetric: 29.0096

Epoch 462: val_loss did not improve from 28.90267
196/196 - 39s - loss: 28.0021 - MinusLogProbMetric: 28.0021 - val_loss: 29.0096 - val_MinusLogProbMetric: 29.0096 - lr: 2.5000e-04 - 39s/epoch - 199ms/step
Epoch 463/1000
2023-10-24 12:08:24.145 
Epoch 463/1000 
	 loss: 27.9922, MinusLogProbMetric: 27.9922, val_loss: 28.9595, val_MinusLogProbMetric: 28.9595

Epoch 463: val_loss did not improve from 28.90267
196/196 - 37s - loss: 27.9922 - MinusLogProbMetric: 27.9922 - val_loss: 28.9595 - val_MinusLogProbMetric: 28.9595 - lr: 2.5000e-04 - 37s/epoch - 189ms/step
Epoch 464/1000
2023-10-24 12:09:03.401 
Epoch 464/1000 
	 loss: 27.9988, MinusLogProbMetric: 27.9988, val_loss: 28.9450, val_MinusLogProbMetric: 28.9450

Epoch 464: val_loss did not improve from 28.90267
196/196 - 39s - loss: 27.9988 - MinusLogProbMetric: 27.9988 - val_loss: 28.9450 - val_MinusLogProbMetric: 28.9450 - lr: 2.5000e-04 - 39s/epoch - 200ms/step
Epoch 465/1000
2023-10-24 12:09:40.463 
Epoch 465/1000 
	 loss: 28.0604, MinusLogProbMetric: 28.0604, val_loss: 29.2606, val_MinusLogProbMetric: 29.2606

Epoch 465: val_loss did not improve from 28.90267
196/196 - 37s - loss: 28.0604 - MinusLogProbMetric: 28.0604 - val_loss: 29.2606 - val_MinusLogProbMetric: 29.2606 - lr: 2.5000e-04 - 37s/epoch - 189ms/step
Epoch 466/1000
2023-10-24 12:10:19.292 
Epoch 466/1000 
	 loss: 27.9933, MinusLogProbMetric: 27.9933, val_loss: 28.9310, val_MinusLogProbMetric: 28.9310

Epoch 466: val_loss did not improve from 28.90267
196/196 - 39s - loss: 27.9933 - MinusLogProbMetric: 27.9933 - val_loss: 28.9310 - val_MinusLogProbMetric: 28.9310 - lr: 2.5000e-04 - 39s/epoch - 198ms/step
Epoch 467/1000
2023-10-24 12:10:55.697 
Epoch 467/1000 
	 loss: 27.9898, MinusLogProbMetric: 27.9898, val_loss: 28.9595, val_MinusLogProbMetric: 28.9595

Epoch 467: val_loss did not improve from 28.90267
196/196 - 36s - loss: 27.9898 - MinusLogProbMetric: 27.9898 - val_loss: 28.9595 - val_MinusLogProbMetric: 28.9595 - lr: 2.5000e-04 - 36s/epoch - 186ms/step
Epoch 468/1000
2023-10-24 12:11:33.335 
Epoch 468/1000 
	 loss: 28.0433, MinusLogProbMetric: 28.0433, val_loss: 29.0267, val_MinusLogProbMetric: 29.0267

Epoch 468: val_loss did not improve from 28.90267
196/196 - 38s - loss: 28.0433 - MinusLogProbMetric: 28.0433 - val_loss: 29.0267 - val_MinusLogProbMetric: 29.0267 - lr: 2.5000e-04 - 38s/epoch - 192ms/step
Epoch 469/1000
2023-10-24 12:12:09.380 
Epoch 469/1000 
	 loss: 27.8991, MinusLogProbMetric: 27.8991, val_loss: 28.8671, val_MinusLogProbMetric: 28.8671

Epoch 469: val_loss improved from 28.90267 to 28.86707, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 37s - loss: 27.8991 - MinusLogProbMetric: 27.8991 - val_loss: 28.8671 - val_MinusLogProbMetric: 28.8671 - lr: 1.2500e-04 - 37s/epoch - 187ms/step
Epoch 470/1000
2023-10-24 12:12:48.304 
Epoch 470/1000 
	 loss: 27.8916, MinusLogProbMetric: 27.8916, val_loss: 28.9088, val_MinusLogProbMetric: 28.9088

Epoch 470: val_loss did not improve from 28.86707
196/196 - 38s - loss: 27.8916 - MinusLogProbMetric: 27.8916 - val_loss: 28.9088 - val_MinusLogProbMetric: 28.9088 - lr: 1.2500e-04 - 38s/epoch - 196ms/step
Epoch 471/1000
2023-10-24 12:13:24.536 
Epoch 471/1000 
	 loss: 27.8837, MinusLogProbMetric: 27.8837, val_loss: 28.8619, val_MinusLogProbMetric: 28.8619

Epoch 471: val_loss improved from 28.86707 to 28.86190, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 37s - loss: 27.8837 - MinusLogProbMetric: 27.8837 - val_loss: 28.8619 - val_MinusLogProbMetric: 28.8619 - lr: 1.2500e-04 - 37s/epoch - 188ms/step
Epoch 472/1000
2023-10-24 12:14:00.494 
Epoch 472/1000 
	 loss: 27.8752, MinusLogProbMetric: 27.8752, val_loss: 28.8726, val_MinusLogProbMetric: 28.8726

Epoch 472: val_loss did not improve from 28.86190
196/196 - 35s - loss: 27.8752 - MinusLogProbMetric: 27.8752 - val_loss: 28.8726 - val_MinusLogProbMetric: 28.8726 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 473/1000
2023-10-24 12:14:36.511 
Epoch 473/1000 
	 loss: 27.8819, MinusLogProbMetric: 27.8819, val_loss: 28.8616, val_MinusLogProbMetric: 28.8616

Epoch 473: val_loss improved from 28.86190 to 28.86165, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 37s - loss: 27.8819 - MinusLogProbMetric: 27.8819 - val_loss: 28.8616 - val_MinusLogProbMetric: 28.8616 - lr: 1.2500e-04 - 37s/epoch - 187ms/step
Epoch 474/1000
2023-10-24 12:15:15.732 
Epoch 474/1000 
	 loss: 27.8766, MinusLogProbMetric: 27.8766, val_loss: 28.9308, val_MinusLogProbMetric: 28.9308

Epoch 474: val_loss did not improve from 28.86165
196/196 - 39s - loss: 27.8766 - MinusLogProbMetric: 27.8766 - val_loss: 28.9308 - val_MinusLogProbMetric: 28.9308 - lr: 1.2500e-04 - 39s/epoch - 197ms/step
Epoch 475/1000
2023-10-24 12:15:52.475 
Epoch 475/1000 
	 loss: 27.8973, MinusLogProbMetric: 27.8973, val_loss: 28.9292, val_MinusLogProbMetric: 28.9292

Epoch 475: val_loss did not improve from 28.86165
196/196 - 37s - loss: 27.8973 - MinusLogProbMetric: 27.8973 - val_loss: 28.9292 - val_MinusLogProbMetric: 28.9292 - lr: 1.2500e-04 - 37s/epoch - 187ms/step
Epoch 476/1000
2023-10-24 12:16:30.706 
Epoch 476/1000 
	 loss: 27.8761, MinusLogProbMetric: 27.8761, val_loss: 28.8593, val_MinusLogProbMetric: 28.8593

Epoch 476: val_loss improved from 28.86165 to 28.85934, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 39s - loss: 27.8761 - MinusLogProbMetric: 27.8761 - val_loss: 28.8593 - val_MinusLogProbMetric: 28.8593 - lr: 1.2500e-04 - 39s/epoch - 198ms/step
Epoch 477/1000
2023-10-24 12:17:09.216 
Epoch 477/1000 
	 loss: 27.8722, MinusLogProbMetric: 27.8722, val_loss: 28.8909, val_MinusLogProbMetric: 28.8909

Epoch 477: val_loss did not improve from 28.85934
196/196 - 38s - loss: 27.8722 - MinusLogProbMetric: 27.8722 - val_loss: 28.8909 - val_MinusLogProbMetric: 28.8909 - lr: 1.2500e-04 - 38s/epoch - 193ms/step
Epoch 478/1000
2023-10-24 12:17:45.443 
Epoch 478/1000 
	 loss: 27.8766, MinusLogProbMetric: 27.8766, val_loss: 28.8529, val_MinusLogProbMetric: 28.8529

Epoch 478: val_loss improved from 28.85934 to 28.85290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 37s - loss: 27.8766 - MinusLogProbMetric: 27.8766 - val_loss: 28.8529 - val_MinusLogProbMetric: 28.8529 - lr: 1.2500e-04 - 37s/epoch - 188ms/step
Epoch 479/1000
2023-10-24 12:18:22.695 
Epoch 479/1000 
	 loss: 27.8724, MinusLogProbMetric: 27.8724, val_loss: 28.8393, val_MinusLogProbMetric: 28.8393

Epoch 479: val_loss improved from 28.85290 to 28.83927, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 37s - loss: 27.8724 - MinusLogProbMetric: 27.8724 - val_loss: 28.8393 - val_MinusLogProbMetric: 28.8393 - lr: 1.2500e-04 - 37s/epoch - 190ms/step
Epoch 480/1000
2023-10-24 12:19:00.644 
Epoch 480/1000 
	 loss: 27.8896, MinusLogProbMetric: 27.8896, val_loss: 28.8650, val_MinusLogProbMetric: 28.8650

Epoch 480: val_loss did not improve from 28.83927
196/196 - 37s - loss: 27.8896 - MinusLogProbMetric: 27.8896 - val_loss: 28.8650 - val_MinusLogProbMetric: 28.8650 - lr: 1.2500e-04 - 37s/epoch - 190ms/step
Epoch 481/1000
2023-10-24 12:19:36.375 
Epoch 481/1000 
	 loss: 27.8768, MinusLogProbMetric: 27.8768, val_loss: 28.8760, val_MinusLogProbMetric: 28.8760

Epoch 481: val_loss did not improve from 28.83927
196/196 - 36s - loss: 27.8768 - MinusLogProbMetric: 27.8768 - val_loss: 28.8760 - val_MinusLogProbMetric: 28.8760 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 482/1000
2023-10-24 12:20:13.051 
Epoch 482/1000 
	 loss: 27.8698, MinusLogProbMetric: 27.8698, val_loss: 28.9202, val_MinusLogProbMetric: 28.9202

Epoch 482: val_loss did not improve from 28.83927
196/196 - 37s - loss: 27.8698 - MinusLogProbMetric: 27.8698 - val_loss: 28.9202 - val_MinusLogProbMetric: 28.9202 - lr: 1.2500e-04 - 37s/epoch - 187ms/step
Epoch 483/1000
2023-10-24 12:20:49.355 
Epoch 483/1000 
	 loss: 27.8707, MinusLogProbMetric: 27.8707, val_loss: 28.9390, val_MinusLogProbMetric: 28.9390

Epoch 483: val_loss did not improve from 28.83927
196/196 - 36s - loss: 27.8707 - MinusLogProbMetric: 27.8707 - val_loss: 28.9390 - val_MinusLogProbMetric: 28.9390 - lr: 1.2500e-04 - 36s/epoch - 185ms/step
Epoch 484/1000
2023-10-24 12:21:25.139 
Epoch 484/1000 
	 loss: 27.9152, MinusLogProbMetric: 27.9152, val_loss: 28.9562, val_MinusLogProbMetric: 28.9562

Epoch 484: val_loss did not improve from 28.83927
196/196 - 36s - loss: 27.9152 - MinusLogProbMetric: 27.9152 - val_loss: 28.9562 - val_MinusLogProbMetric: 28.9562 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 485/1000
2023-10-24 12:22:03.219 
Epoch 485/1000 
	 loss: 27.8834, MinusLogProbMetric: 27.8834, val_loss: 28.9638, val_MinusLogProbMetric: 28.9638

Epoch 485: val_loss did not improve from 28.83927
196/196 - 38s - loss: 27.8834 - MinusLogProbMetric: 27.8834 - val_loss: 28.9638 - val_MinusLogProbMetric: 28.9638 - lr: 1.2500e-04 - 38s/epoch - 194ms/step
Epoch 486/1000
2023-10-24 12:22:42.296 
Epoch 486/1000 
	 loss: 27.8770, MinusLogProbMetric: 27.8770, val_loss: 28.9263, val_MinusLogProbMetric: 28.9263

Epoch 486: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8770 - MinusLogProbMetric: 27.8770 - val_loss: 28.9263 - val_MinusLogProbMetric: 28.9263 - lr: 1.2500e-04 - 39s/epoch - 199ms/step
Epoch 487/1000
2023-10-24 12:23:19.164 
Epoch 487/1000 
	 loss: 27.8790, MinusLogProbMetric: 27.8790, val_loss: 28.9239, val_MinusLogProbMetric: 28.9239

Epoch 487: val_loss did not improve from 28.83927
196/196 - 37s - loss: 27.8790 - MinusLogProbMetric: 27.8790 - val_loss: 28.9239 - val_MinusLogProbMetric: 28.9239 - lr: 1.2500e-04 - 37s/epoch - 188ms/step
Epoch 488/1000
2023-10-24 12:23:57.151 
Epoch 488/1000 
	 loss: 27.8835, MinusLogProbMetric: 27.8835, val_loss: 28.8814, val_MinusLogProbMetric: 28.8814

Epoch 488: val_loss did not improve from 28.83927
196/196 - 38s - loss: 27.8835 - MinusLogProbMetric: 27.8835 - val_loss: 28.8814 - val_MinusLogProbMetric: 28.8814 - lr: 1.2500e-04 - 38s/epoch - 194ms/step
Epoch 489/1000
2023-10-24 12:24:33.616 
Epoch 489/1000 
	 loss: 27.8734, MinusLogProbMetric: 27.8734, val_loss: 28.8468, val_MinusLogProbMetric: 28.8468

Epoch 489: val_loss did not improve from 28.83927
196/196 - 36s - loss: 27.8734 - MinusLogProbMetric: 27.8734 - val_loss: 28.8468 - val_MinusLogProbMetric: 28.8468 - lr: 1.2500e-04 - 36s/epoch - 186ms/step
Epoch 490/1000
2023-10-24 12:25:10.630 
Epoch 490/1000 
	 loss: 27.8965, MinusLogProbMetric: 27.8965, val_loss: 28.9628, val_MinusLogProbMetric: 28.9628

Epoch 490: val_loss did not improve from 28.83927
196/196 - 37s - loss: 27.8965 - MinusLogProbMetric: 27.8965 - val_loss: 28.9628 - val_MinusLogProbMetric: 28.9628 - lr: 1.2500e-04 - 37s/epoch - 189ms/step
Epoch 491/1000
2023-10-24 12:25:46.045 
Epoch 491/1000 
	 loss: 27.8572, MinusLogProbMetric: 27.8572, val_loss: 28.8771, val_MinusLogProbMetric: 28.8771

Epoch 491: val_loss did not improve from 28.83927
196/196 - 35s - loss: 27.8572 - MinusLogProbMetric: 27.8572 - val_loss: 28.8771 - val_MinusLogProbMetric: 28.8771 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 492/1000
2023-10-24 12:26:23.134 
Epoch 492/1000 
	 loss: 27.8588, MinusLogProbMetric: 27.8588, val_loss: 28.8713, val_MinusLogProbMetric: 28.8713

Epoch 492: val_loss did not improve from 28.83927
196/196 - 37s - loss: 27.8588 - MinusLogProbMetric: 27.8588 - val_loss: 28.8713 - val_MinusLogProbMetric: 28.8713 - lr: 1.2500e-04 - 37s/epoch - 189ms/step
Epoch 493/1000
2023-10-24 12:27:02.108 
Epoch 493/1000 
	 loss: 27.8701, MinusLogProbMetric: 27.8701, val_loss: 28.8552, val_MinusLogProbMetric: 28.8552

Epoch 493: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8701 - MinusLogProbMetric: 27.8701 - val_loss: 28.8552 - val_MinusLogProbMetric: 28.8552 - lr: 1.2500e-04 - 39s/epoch - 199ms/step
Epoch 494/1000
2023-10-24 12:27:40.668 
Epoch 494/1000 
	 loss: 27.8939, MinusLogProbMetric: 27.8939, val_loss: 28.9830, val_MinusLogProbMetric: 28.9830

Epoch 494: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8939 - MinusLogProbMetric: 27.8939 - val_loss: 28.9830 - val_MinusLogProbMetric: 28.9830 - lr: 1.2500e-04 - 39s/epoch - 197ms/step
Epoch 495/1000
2023-10-24 12:28:19.168 
Epoch 495/1000 
	 loss: 27.8663, MinusLogProbMetric: 27.8663, val_loss: 28.9858, val_MinusLogProbMetric: 28.9858

Epoch 495: val_loss did not improve from 28.83927
196/196 - 38s - loss: 27.8663 - MinusLogProbMetric: 27.8663 - val_loss: 28.9858 - val_MinusLogProbMetric: 28.9858 - lr: 1.2500e-04 - 38s/epoch - 196ms/step
Epoch 496/1000
2023-10-24 12:28:56.704 
Epoch 496/1000 
	 loss: 27.8499, MinusLogProbMetric: 27.8499, val_loss: 28.9101, val_MinusLogProbMetric: 28.9101

Epoch 496: val_loss did not improve from 28.83927
196/196 - 38s - loss: 27.8499 - MinusLogProbMetric: 27.8499 - val_loss: 28.9101 - val_MinusLogProbMetric: 28.9101 - lr: 1.2500e-04 - 38s/epoch - 191ms/step
Epoch 497/1000
2023-10-24 12:29:34.742 
Epoch 497/1000 
	 loss: 27.8779, MinusLogProbMetric: 27.8779, val_loss: 29.1178, val_MinusLogProbMetric: 29.1178

Epoch 497: val_loss did not improve from 28.83927
196/196 - 38s - loss: 27.8779 - MinusLogProbMetric: 27.8779 - val_loss: 29.1178 - val_MinusLogProbMetric: 29.1178 - lr: 1.2500e-04 - 38s/epoch - 194ms/step
Epoch 498/1000
2023-10-24 12:30:13.672 
Epoch 498/1000 
	 loss: 27.8713, MinusLogProbMetric: 27.8713, val_loss: 29.0238, val_MinusLogProbMetric: 29.0238

Epoch 498: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8713 - MinusLogProbMetric: 27.8713 - val_loss: 29.0238 - val_MinusLogProbMetric: 29.0238 - lr: 1.2500e-04 - 39s/epoch - 199ms/step
Epoch 499/1000
2023-10-24 12:30:52.913 
Epoch 499/1000 
	 loss: 27.8764, MinusLogProbMetric: 27.8764, val_loss: 28.8528, val_MinusLogProbMetric: 28.8528

Epoch 499: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8764 - MinusLogProbMetric: 27.8764 - val_loss: 28.8528 - val_MinusLogProbMetric: 28.8528 - lr: 1.2500e-04 - 39s/epoch - 200ms/step
Epoch 500/1000
2023-10-24 12:31:31.874 
Epoch 500/1000 
	 loss: 27.8552, MinusLogProbMetric: 27.8552, val_loss: 28.8513, val_MinusLogProbMetric: 28.8513

Epoch 500: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8552 - MinusLogProbMetric: 27.8552 - val_loss: 28.8513 - val_MinusLogProbMetric: 28.8513 - lr: 1.2500e-04 - 39s/epoch - 199ms/step
Epoch 501/1000
2023-10-24 12:32:10.759 
Epoch 501/1000 
	 loss: 27.8579, MinusLogProbMetric: 27.8579, val_loss: 28.8996, val_MinusLogProbMetric: 28.8996

Epoch 501: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8579 - MinusLogProbMetric: 27.8579 - val_loss: 28.8996 - val_MinusLogProbMetric: 28.8996 - lr: 1.2500e-04 - 39s/epoch - 198ms/step
Epoch 502/1000
2023-10-24 12:32:49.689 
Epoch 502/1000 
	 loss: 27.8628, MinusLogProbMetric: 27.8628, val_loss: 28.8866, val_MinusLogProbMetric: 28.8866

Epoch 502: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8628 - MinusLogProbMetric: 27.8628 - val_loss: 28.8866 - val_MinusLogProbMetric: 28.8866 - lr: 1.2500e-04 - 39s/epoch - 199ms/step
Epoch 503/1000
2023-10-24 12:33:28.313 
Epoch 503/1000 
	 loss: 27.8627, MinusLogProbMetric: 27.8627, val_loss: 28.8491, val_MinusLogProbMetric: 28.8491

Epoch 503: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8627 - MinusLogProbMetric: 27.8627 - val_loss: 28.8491 - val_MinusLogProbMetric: 28.8491 - lr: 1.2500e-04 - 39s/epoch - 197ms/step
Epoch 504/1000
2023-10-24 12:34:07.430 
Epoch 504/1000 
	 loss: 27.8630, MinusLogProbMetric: 27.8630, val_loss: 28.9476, val_MinusLogProbMetric: 28.9476

Epoch 504: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8630 - MinusLogProbMetric: 27.8630 - val_loss: 28.9476 - val_MinusLogProbMetric: 28.9476 - lr: 1.2500e-04 - 39s/epoch - 200ms/step
Epoch 505/1000
2023-10-24 12:34:46.276 
Epoch 505/1000 
	 loss: 27.8683, MinusLogProbMetric: 27.8683, val_loss: 28.8836, val_MinusLogProbMetric: 28.8836

Epoch 505: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8683 - MinusLogProbMetric: 27.8683 - val_loss: 28.8836 - val_MinusLogProbMetric: 28.8836 - lr: 1.2500e-04 - 39s/epoch - 198ms/step
Epoch 506/1000
2023-10-24 12:35:24.133 
Epoch 506/1000 
	 loss: 27.8682, MinusLogProbMetric: 27.8682, val_loss: 28.8526, val_MinusLogProbMetric: 28.8526

Epoch 506: val_loss did not improve from 28.83927
196/196 - 38s - loss: 27.8682 - MinusLogProbMetric: 27.8682 - val_loss: 28.8526 - val_MinusLogProbMetric: 28.8526 - lr: 1.2500e-04 - 38s/epoch - 193ms/step
Epoch 507/1000
2023-10-24 12:36:00.970 
Epoch 507/1000 
	 loss: 27.8639, MinusLogProbMetric: 27.8639, val_loss: 28.8911, val_MinusLogProbMetric: 28.8911

Epoch 507: val_loss did not improve from 28.83927
196/196 - 37s - loss: 27.8639 - MinusLogProbMetric: 27.8639 - val_loss: 28.8911 - val_MinusLogProbMetric: 28.8911 - lr: 1.2500e-04 - 37s/epoch - 188ms/step
Epoch 508/1000
2023-10-24 12:36:40.663 
Epoch 508/1000 
	 loss: 27.8572, MinusLogProbMetric: 27.8572, val_loss: 29.0587, val_MinusLogProbMetric: 29.0587

Epoch 508: val_loss did not improve from 28.83927
196/196 - 40s - loss: 27.8572 - MinusLogProbMetric: 27.8572 - val_loss: 29.0587 - val_MinusLogProbMetric: 29.0587 - lr: 1.2500e-04 - 40s/epoch - 202ms/step
Epoch 509/1000
2023-10-24 12:37:19.386 
Epoch 509/1000 
	 loss: 27.8656, MinusLogProbMetric: 27.8656, val_loss: 29.0669, val_MinusLogProbMetric: 29.0669

Epoch 509: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8656 - MinusLogProbMetric: 27.8656 - val_loss: 29.0669 - val_MinusLogProbMetric: 29.0669 - lr: 1.2500e-04 - 39s/epoch - 198ms/step
Epoch 510/1000
2023-10-24 12:37:58.570 
Epoch 510/1000 
	 loss: 27.8605, MinusLogProbMetric: 27.8605, val_loss: 28.8696, val_MinusLogProbMetric: 28.8696

Epoch 510: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8605 - MinusLogProbMetric: 27.8605 - val_loss: 28.8696 - val_MinusLogProbMetric: 28.8696 - lr: 1.2500e-04 - 39s/epoch - 200ms/step
Epoch 511/1000
2023-10-24 12:38:36.883 
Epoch 511/1000 
	 loss: 27.8585, MinusLogProbMetric: 27.8585, val_loss: 29.0724, val_MinusLogProbMetric: 29.0724

Epoch 511: val_loss did not improve from 28.83927
196/196 - 38s - loss: 27.8585 - MinusLogProbMetric: 27.8585 - val_loss: 29.0724 - val_MinusLogProbMetric: 29.0724 - lr: 1.2500e-04 - 38s/epoch - 195ms/step
Epoch 512/1000
2023-10-24 12:39:15.370 
Epoch 512/1000 
	 loss: 27.8669, MinusLogProbMetric: 27.8669, val_loss: 28.9388, val_MinusLogProbMetric: 28.9388

Epoch 512: val_loss did not improve from 28.83927
196/196 - 38s - loss: 27.8669 - MinusLogProbMetric: 27.8669 - val_loss: 28.9388 - val_MinusLogProbMetric: 28.9388 - lr: 1.2500e-04 - 38s/epoch - 196ms/step
Epoch 513/1000
2023-10-24 12:39:54.416 
Epoch 513/1000 
	 loss: 27.8569, MinusLogProbMetric: 27.8569, val_loss: 28.9146, val_MinusLogProbMetric: 28.9146

Epoch 513: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8569 - MinusLogProbMetric: 27.8569 - val_loss: 28.9146 - val_MinusLogProbMetric: 28.9146 - lr: 1.2500e-04 - 39s/epoch - 199ms/step
Epoch 514/1000
2023-10-24 12:40:33.225 
Epoch 514/1000 
	 loss: 27.8767, MinusLogProbMetric: 27.8767, val_loss: 28.9497, val_MinusLogProbMetric: 28.9497

Epoch 514: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8767 - MinusLogProbMetric: 27.8767 - val_loss: 28.9497 - val_MinusLogProbMetric: 28.9497 - lr: 1.2500e-04 - 39s/epoch - 198ms/step
Epoch 515/1000
2023-10-24 12:41:11.614 
Epoch 515/1000 
	 loss: 27.8535, MinusLogProbMetric: 27.8535, val_loss: 28.9313, val_MinusLogProbMetric: 28.9313

Epoch 515: val_loss did not improve from 28.83927
196/196 - 38s - loss: 27.8535 - MinusLogProbMetric: 27.8535 - val_loss: 28.9313 - val_MinusLogProbMetric: 28.9313 - lr: 1.2500e-04 - 38s/epoch - 196ms/step
Epoch 516/1000
2023-10-24 12:41:48.734 
Epoch 516/1000 
	 loss: 27.8667, MinusLogProbMetric: 27.8667, val_loss: 29.1655, val_MinusLogProbMetric: 29.1655

Epoch 516: val_loss did not improve from 28.83927
196/196 - 37s - loss: 27.8667 - MinusLogProbMetric: 27.8667 - val_loss: 29.1655 - val_MinusLogProbMetric: 29.1655 - lr: 1.2500e-04 - 37s/epoch - 189ms/step
Epoch 517/1000
2023-10-24 12:42:27.163 
Epoch 517/1000 
	 loss: 27.8773, MinusLogProbMetric: 27.8773, val_loss: 28.9457, val_MinusLogProbMetric: 28.9457

Epoch 517: val_loss did not improve from 28.83927
196/196 - 38s - loss: 27.8773 - MinusLogProbMetric: 27.8773 - val_loss: 28.9457 - val_MinusLogProbMetric: 28.9457 - lr: 1.2500e-04 - 38s/epoch - 196ms/step
Epoch 518/1000
2023-10-24 12:43:06.096 
Epoch 518/1000 
	 loss: 27.8607, MinusLogProbMetric: 27.8607, val_loss: 28.8828, val_MinusLogProbMetric: 28.8828

Epoch 518: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8607 - MinusLogProbMetric: 27.8607 - val_loss: 28.8828 - val_MinusLogProbMetric: 28.8828 - lr: 1.2500e-04 - 39s/epoch - 199ms/step
Epoch 519/1000
2023-10-24 12:43:44.501 
Epoch 519/1000 
	 loss: 27.8917, MinusLogProbMetric: 27.8917, val_loss: 28.9062, val_MinusLogProbMetric: 28.9062

Epoch 519: val_loss did not improve from 28.83927
196/196 - 38s - loss: 27.8917 - MinusLogProbMetric: 27.8917 - val_loss: 28.9062 - val_MinusLogProbMetric: 28.9062 - lr: 1.2500e-04 - 38s/epoch - 196ms/step
Epoch 520/1000
2023-10-24 12:44:23.409 
Epoch 520/1000 
	 loss: 27.8537, MinusLogProbMetric: 27.8537, val_loss: 28.8889, val_MinusLogProbMetric: 28.8889

Epoch 520: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8537 - MinusLogProbMetric: 27.8537 - val_loss: 28.8889 - val_MinusLogProbMetric: 28.8889 - lr: 1.2500e-04 - 39s/epoch - 198ms/step
Epoch 521/1000
2023-10-24 12:45:00.700 
Epoch 521/1000 
	 loss: 27.8688, MinusLogProbMetric: 27.8688, val_loss: 29.0265, val_MinusLogProbMetric: 29.0265

Epoch 521: val_loss did not improve from 28.83927
196/196 - 37s - loss: 27.8688 - MinusLogProbMetric: 27.8688 - val_loss: 29.0265 - val_MinusLogProbMetric: 29.0265 - lr: 1.2500e-04 - 37s/epoch - 190ms/step
Epoch 522/1000
2023-10-24 12:45:39.606 
Epoch 522/1000 
	 loss: 27.8676, MinusLogProbMetric: 27.8676, val_loss: 28.8987, val_MinusLogProbMetric: 28.8987

Epoch 522: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8676 - MinusLogProbMetric: 27.8676 - val_loss: 28.8987 - val_MinusLogProbMetric: 28.8987 - lr: 1.2500e-04 - 39s/epoch - 198ms/step
Epoch 523/1000
2023-10-24 12:46:18.637 
Epoch 523/1000 
	 loss: 27.8543, MinusLogProbMetric: 27.8543, val_loss: 28.9420, val_MinusLogProbMetric: 28.9420

Epoch 523: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8543 - MinusLogProbMetric: 27.8543 - val_loss: 28.9420 - val_MinusLogProbMetric: 28.9420 - lr: 1.2500e-04 - 39s/epoch - 199ms/step
Epoch 524/1000
2023-10-24 12:46:57.246 
Epoch 524/1000 
	 loss: 27.8670, MinusLogProbMetric: 27.8670, val_loss: 29.0930, val_MinusLogProbMetric: 29.0930

Epoch 524: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8670 - MinusLogProbMetric: 27.8670 - val_loss: 29.0930 - val_MinusLogProbMetric: 29.0930 - lr: 1.2500e-04 - 39s/epoch - 197ms/step
Epoch 525/1000
2023-10-24 12:47:36.230 
Epoch 525/1000 
	 loss: 27.8600, MinusLogProbMetric: 27.8600, val_loss: 28.9288, val_MinusLogProbMetric: 28.9288

Epoch 525: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8600 - MinusLogProbMetric: 27.8600 - val_loss: 28.9288 - val_MinusLogProbMetric: 28.9288 - lr: 1.2500e-04 - 39s/epoch - 199ms/step
Epoch 526/1000
2023-10-24 12:48:15.092 
Epoch 526/1000 
	 loss: 27.8669, MinusLogProbMetric: 27.8669, val_loss: 28.9081, val_MinusLogProbMetric: 28.9081

Epoch 526: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8669 - MinusLogProbMetric: 27.8669 - val_loss: 28.9081 - val_MinusLogProbMetric: 28.9081 - lr: 1.2500e-04 - 39s/epoch - 198ms/step
Epoch 527/1000
2023-10-24 12:48:54.125 
Epoch 527/1000 
	 loss: 27.8761, MinusLogProbMetric: 27.8761, val_loss: 28.9017, val_MinusLogProbMetric: 28.9017

Epoch 527: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8761 - MinusLogProbMetric: 27.8761 - val_loss: 28.9017 - val_MinusLogProbMetric: 28.9017 - lr: 1.2500e-04 - 39s/epoch - 199ms/step
Epoch 528/1000
2023-10-24 12:49:33.167 
Epoch 528/1000 
	 loss: 27.8659, MinusLogProbMetric: 27.8659, val_loss: 29.1281, val_MinusLogProbMetric: 29.1281

Epoch 528: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.8659 - MinusLogProbMetric: 27.8659 - val_loss: 29.1281 - val_MinusLogProbMetric: 29.1281 - lr: 1.2500e-04 - 39s/epoch - 199ms/step
Epoch 529/1000
2023-10-24 12:50:09.488 
Epoch 529/1000 
	 loss: 27.8706, MinusLogProbMetric: 27.8706, val_loss: 28.8536, val_MinusLogProbMetric: 28.8536

Epoch 529: val_loss did not improve from 28.83927
196/196 - 36s - loss: 27.8706 - MinusLogProbMetric: 27.8706 - val_loss: 28.8536 - val_MinusLogProbMetric: 28.8536 - lr: 1.2500e-04 - 36s/epoch - 185ms/step
Epoch 530/1000
2023-10-24 12:50:48.924 
Epoch 530/1000 
	 loss: 27.7979, MinusLogProbMetric: 27.7979, val_loss: 28.8514, val_MinusLogProbMetric: 28.8514

Epoch 530: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.7979 - MinusLogProbMetric: 27.7979 - val_loss: 28.8514 - val_MinusLogProbMetric: 28.8514 - lr: 6.2500e-05 - 39s/epoch - 201ms/step
Epoch 531/1000
2023-10-24 12:51:27.680 
Epoch 531/1000 
	 loss: 27.7974, MinusLogProbMetric: 27.7974, val_loss: 28.8522, val_MinusLogProbMetric: 28.8522

Epoch 531: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.7974 - MinusLogProbMetric: 27.7974 - val_loss: 28.8522 - val_MinusLogProbMetric: 28.8522 - lr: 6.2500e-05 - 39s/epoch - 198ms/step
Epoch 532/1000
2023-10-24 12:52:06.526 
Epoch 532/1000 
	 loss: 27.7925, MinusLogProbMetric: 27.7925, val_loss: 28.8859, val_MinusLogProbMetric: 28.8859

Epoch 532: val_loss did not improve from 28.83927
196/196 - 39s - loss: 27.7925 - MinusLogProbMetric: 27.7925 - val_loss: 28.8859 - val_MinusLogProbMetric: 28.8859 - lr: 6.2500e-05 - 39s/epoch - 198ms/step
Epoch 533/1000
2023-10-24 12:52:45.493 
Epoch 533/1000 
	 loss: 27.7925, MinusLogProbMetric: 27.7925, val_loss: 28.8361, val_MinusLogProbMetric: 28.8361

Epoch 533: val_loss improved from 28.83927 to 28.83605, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 40s - loss: 27.7925 - MinusLogProbMetric: 27.7925 - val_loss: 28.8361 - val_MinusLogProbMetric: 28.8361 - lr: 6.2500e-05 - 40s/epoch - 202ms/step
Epoch 534/1000
2023-10-24 12:53:23.082 
Epoch 534/1000 
	 loss: 27.7967, MinusLogProbMetric: 27.7967, val_loss: 28.8708, val_MinusLogProbMetric: 28.8708

Epoch 534: val_loss did not improve from 28.83605
196/196 - 37s - loss: 27.7967 - MinusLogProbMetric: 27.7967 - val_loss: 28.8708 - val_MinusLogProbMetric: 28.8708 - lr: 6.2500e-05 - 37s/epoch - 188ms/step
Epoch 535/1000
2023-10-24 12:54:01.610 
Epoch 535/1000 
	 loss: 27.8034, MinusLogProbMetric: 27.8034, val_loss: 28.9130, val_MinusLogProbMetric: 28.9130

Epoch 535: val_loss did not improve from 28.83605
196/196 - 39s - loss: 27.8034 - MinusLogProbMetric: 27.8034 - val_loss: 28.9130 - val_MinusLogProbMetric: 28.9130 - lr: 6.2500e-05 - 39s/epoch - 197ms/step
Epoch 536/1000
2023-10-24 12:54:40.528 
Epoch 536/1000 
	 loss: 27.7940, MinusLogProbMetric: 27.7940, val_loss: 28.8404, val_MinusLogProbMetric: 28.8404

Epoch 536: val_loss did not improve from 28.83605
196/196 - 39s - loss: 27.7940 - MinusLogProbMetric: 27.7940 - val_loss: 28.8404 - val_MinusLogProbMetric: 28.8404 - lr: 6.2500e-05 - 39s/epoch - 199ms/step
Epoch 537/1000
2023-10-24 12:55:19.305 
Epoch 537/1000 
	 loss: 27.7930, MinusLogProbMetric: 27.7930, val_loss: 28.8884, val_MinusLogProbMetric: 28.8884

Epoch 537: val_loss did not improve from 28.83605
196/196 - 39s - loss: 27.7930 - MinusLogProbMetric: 27.7930 - val_loss: 28.8884 - val_MinusLogProbMetric: 28.8884 - lr: 6.2500e-05 - 39s/epoch - 198ms/step
Epoch 538/1000
2023-10-24 12:55:57.875 
Epoch 538/1000 
	 loss: 27.8028, MinusLogProbMetric: 27.8028, val_loss: 28.8438, val_MinusLogProbMetric: 28.8438

Epoch 538: val_loss did not improve from 28.83605
196/196 - 39s - loss: 27.8028 - MinusLogProbMetric: 27.8028 - val_loss: 28.8438 - val_MinusLogProbMetric: 28.8438 - lr: 6.2500e-05 - 39s/epoch - 197ms/step
Epoch 539/1000
2023-10-24 12:56:35.559 
Epoch 539/1000 
	 loss: 27.7937, MinusLogProbMetric: 27.7937, val_loss: 28.8368, val_MinusLogProbMetric: 28.8368

Epoch 539: val_loss did not improve from 28.83605
196/196 - 38s - loss: 27.7937 - MinusLogProbMetric: 27.7937 - val_loss: 28.8368 - val_MinusLogProbMetric: 28.8368 - lr: 6.2500e-05 - 38s/epoch - 192ms/step
Epoch 540/1000
2023-10-24 12:57:13.647 
Epoch 540/1000 
	 loss: 27.7906, MinusLogProbMetric: 27.7906, val_loss: 28.8534, val_MinusLogProbMetric: 28.8534

Epoch 540: val_loss did not improve from 28.83605
196/196 - 38s - loss: 27.7906 - MinusLogProbMetric: 27.7906 - val_loss: 28.8534 - val_MinusLogProbMetric: 28.8534 - lr: 6.2500e-05 - 38s/epoch - 194ms/step
Epoch 541/1000
2023-10-24 12:57:52.277 
Epoch 541/1000 
	 loss: 27.7923, MinusLogProbMetric: 27.7923, val_loss: 28.8545, val_MinusLogProbMetric: 28.8545

Epoch 541: val_loss did not improve from 28.83605
196/196 - 39s - loss: 27.7923 - MinusLogProbMetric: 27.7923 - val_loss: 28.8545 - val_MinusLogProbMetric: 28.8545 - lr: 6.2500e-05 - 39s/epoch - 197ms/step
Epoch 542/1000
2023-10-24 12:58:30.830 
Epoch 542/1000 
	 loss: 27.7913, MinusLogProbMetric: 27.7913, val_loss: 28.8599, val_MinusLogProbMetric: 28.8599

Epoch 542: val_loss did not improve from 28.83605
196/196 - 39s - loss: 27.7913 - MinusLogProbMetric: 27.7913 - val_loss: 28.8599 - val_MinusLogProbMetric: 28.8599 - lr: 6.2500e-05 - 39s/epoch - 197ms/step
Epoch 543/1000
2023-10-24 12:59:09.991 
Epoch 543/1000 
	 loss: 27.7936, MinusLogProbMetric: 27.7936, val_loss: 28.8983, val_MinusLogProbMetric: 28.8983

Epoch 543: val_loss did not improve from 28.83605
196/196 - 39s - loss: 27.7936 - MinusLogProbMetric: 27.7936 - val_loss: 28.8983 - val_MinusLogProbMetric: 28.8983 - lr: 6.2500e-05 - 39s/epoch - 200ms/step
Epoch 544/1000
2023-10-24 12:59:49.035 
Epoch 544/1000 
	 loss: 27.7986, MinusLogProbMetric: 27.7986, val_loss: 28.8399, val_MinusLogProbMetric: 28.8399

Epoch 544: val_loss did not improve from 28.83605
196/196 - 39s - loss: 27.7986 - MinusLogProbMetric: 27.7986 - val_loss: 28.8399 - val_MinusLogProbMetric: 28.8399 - lr: 6.2500e-05 - 39s/epoch - 199ms/step
Epoch 545/1000
2023-10-24 13:00:27.729 
Epoch 545/1000 
	 loss: 27.7985, MinusLogProbMetric: 27.7985, val_loss: 28.8759, val_MinusLogProbMetric: 28.8759

Epoch 545: val_loss did not improve from 28.83605
196/196 - 39s - loss: 27.7985 - MinusLogProbMetric: 27.7985 - val_loss: 28.8759 - val_MinusLogProbMetric: 28.8759 - lr: 6.2500e-05 - 39s/epoch - 197ms/step
Epoch 546/1000
2023-10-24 13:01:06.187 
Epoch 546/1000 
	 loss: 27.7972, MinusLogProbMetric: 27.7972, val_loss: 28.8635, val_MinusLogProbMetric: 28.8635

Epoch 546: val_loss did not improve from 28.83605
196/196 - 38s - loss: 27.7972 - MinusLogProbMetric: 27.7972 - val_loss: 28.8635 - val_MinusLogProbMetric: 28.8635 - lr: 6.2500e-05 - 38s/epoch - 196ms/step
Epoch 547/1000
2023-10-24 13:01:44.864 
Epoch 547/1000 
	 loss: 27.8021, MinusLogProbMetric: 27.8021, val_loss: 28.8965, val_MinusLogProbMetric: 28.8965

Epoch 547: val_loss did not improve from 28.83605
196/196 - 39s - loss: 27.8021 - MinusLogProbMetric: 27.8021 - val_loss: 28.8965 - val_MinusLogProbMetric: 28.8965 - lr: 6.2500e-05 - 39s/epoch - 197ms/step
Epoch 548/1000
2023-10-24 13:02:23.413 
Epoch 548/1000 
	 loss: 27.7919, MinusLogProbMetric: 27.7919, val_loss: 28.8181, val_MinusLogProbMetric: 28.8181

Epoch 548: val_loss improved from 28.83605 to 28.81814, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 39s - loss: 27.7919 - MinusLogProbMetric: 27.7919 - val_loss: 28.8181 - val_MinusLogProbMetric: 28.8181 - lr: 6.2500e-05 - 39s/epoch - 200ms/step
Epoch 549/1000
2023-10-24 13:03:01.680 
Epoch 549/1000 
	 loss: 27.8016, MinusLogProbMetric: 27.8016, val_loss: 28.8575, val_MinusLogProbMetric: 28.8575

Epoch 549: val_loss did not improve from 28.81814
196/196 - 38s - loss: 27.8016 - MinusLogProbMetric: 27.8016 - val_loss: 28.8575 - val_MinusLogProbMetric: 28.8575 - lr: 6.2500e-05 - 38s/epoch - 192ms/step
Epoch 550/1000
2023-10-24 13:03:40.974 
Epoch 550/1000 
	 loss: 27.7976, MinusLogProbMetric: 27.7976, val_loss: 28.8611, val_MinusLogProbMetric: 28.8611

Epoch 550: val_loss did not improve from 28.81814
196/196 - 39s - loss: 27.7976 - MinusLogProbMetric: 27.7976 - val_loss: 28.8611 - val_MinusLogProbMetric: 28.8611 - lr: 6.2500e-05 - 39s/epoch - 200ms/step
Epoch 551/1000
2023-10-24 13:04:19.797 
Epoch 551/1000 
	 loss: 27.8080, MinusLogProbMetric: 27.8080, val_loss: 28.8408, val_MinusLogProbMetric: 28.8408

Epoch 551: val_loss did not improve from 28.81814
196/196 - 39s - loss: 27.8080 - MinusLogProbMetric: 27.8080 - val_loss: 28.8408 - val_MinusLogProbMetric: 28.8408 - lr: 6.2500e-05 - 39s/epoch - 198ms/step
Epoch 552/1000
2023-10-24 13:04:58.704 
Epoch 552/1000 
	 loss: 27.7905, MinusLogProbMetric: 27.7905, val_loss: 28.8625, val_MinusLogProbMetric: 28.8625

Epoch 552: val_loss did not improve from 28.81814
196/196 - 39s - loss: 27.7905 - MinusLogProbMetric: 27.7905 - val_loss: 28.8625 - val_MinusLogProbMetric: 28.8625 - lr: 6.2500e-05 - 39s/epoch - 198ms/step
Epoch 553/1000
2023-10-24 13:05:37.365 
Epoch 553/1000 
	 loss: 27.7814, MinusLogProbMetric: 27.7814, val_loss: 28.8930, val_MinusLogProbMetric: 28.8930

Epoch 553: val_loss did not improve from 28.81814
196/196 - 39s - loss: 27.7814 - MinusLogProbMetric: 27.7814 - val_loss: 28.8930 - val_MinusLogProbMetric: 28.8930 - lr: 6.2500e-05 - 39s/epoch - 197ms/step
Epoch 554/1000
2023-10-24 13:06:15.886 
Epoch 554/1000 
	 loss: 27.7873, MinusLogProbMetric: 27.7873, val_loss: 28.8583, val_MinusLogProbMetric: 28.8583

Epoch 554: val_loss did not improve from 28.81814
196/196 - 39s - loss: 27.7873 - MinusLogProbMetric: 27.7873 - val_loss: 28.8583 - val_MinusLogProbMetric: 28.8583 - lr: 6.2500e-05 - 39s/epoch - 197ms/step
Epoch 555/1000
2023-10-24 13:06:54.734 
Epoch 555/1000 
	 loss: 27.7914, MinusLogProbMetric: 27.7914, val_loss: 28.8801, val_MinusLogProbMetric: 28.8801

Epoch 555: val_loss did not improve from 28.81814
196/196 - 39s - loss: 27.7914 - MinusLogProbMetric: 27.7914 - val_loss: 28.8801 - val_MinusLogProbMetric: 28.8801 - lr: 6.2500e-05 - 39s/epoch - 198ms/step
Epoch 556/1000
2023-10-24 13:07:33.806 
Epoch 556/1000 
	 loss: 27.7894, MinusLogProbMetric: 27.7894, val_loss: 28.9868, val_MinusLogProbMetric: 28.9868

Epoch 556: val_loss did not improve from 28.81814
196/196 - 39s - loss: 27.7894 - MinusLogProbMetric: 27.7894 - val_loss: 28.9868 - val_MinusLogProbMetric: 28.9868 - lr: 6.2500e-05 - 39s/epoch - 199ms/step
Epoch 557/1000
2023-10-24 13:08:11.262 
Epoch 557/1000 
	 loss: 27.8151, MinusLogProbMetric: 27.8151, val_loss: 28.8168, val_MinusLogProbMetric: 28.8168

Epoch 557: val_loss improved from 28.81814 to 28.81683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_340/weights/best_weights.h5
196/196 - 38s - loss: 27.8151 - MinusLogProbMetric: 27.8151 - val_loss: 28.8168 - val_MinusLogProbMetric: 28.8168 - lr: 6.2500e-05 - 38s/epoch - 194ms/step
Epoch 558/1000
2023-10-24 13:08:50.780 
Epoch 558/1000 
	 loss: 27.7859, MinusLogProbMetric: 27.7859, val_loss: 28.8532, val_MinusLogProbMetric: 28.8532

Epoch 558: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7859 - MinusLogProbMetric: 27.7859 - val_loss: 28.8532 - val_MinusLogProbMetric: 28.8532 - lr: 6.2500e-05 - 39s/epoch - 199ms/step
Epoch 559/1000
2023-10-24 13:09:29.571 
Epoch 559/1000 
	 loss: 27.7853, MinusLogProbMetric: 27.7853, val_loss: 28.8331, val_MinusLogProbMetric: 28.8331

Epoch 559: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7853 - MinusLogProbMetric: 27.7853 - val_loss: 28.8331 - val_MinusLogProbMetric: 28.8331 - lr: 6.2500e-05 - 39s/epoch - 198ms/step
Epoch 560/1000
2023-10-24 13:10:07.258 
Epoch 560/1000 
	 loss: 27.8050, MinusLogProbMetric: 27.8050, val_loss: 28.8627, val_MinusLogProbMetric: 28.8627

Epoch 560: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.8050 - MinusLogProbMetric: 27.8050 - val_loss: 28.8627 - val_MinusLogProbMetric: 28.8627 - lr: 6.2500e-05 - 38s/epoch - 192ms/step
Epoch 561/1000
2023-10-24 13:10:45.795 
Epoch 561/1000 
	 loss: 27.7827, MinusLogProbMetric: 27.7827, val_loss: 28.8552, val_MinusLogProbMetric: 28.8552

Epoch 561: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7827 - MinusLogProbMetric: 27.7827 - val_loss: 28.8552 - val_MinusLogProbMetric: 28.8552 - lr: 6.2500e-05 - 39s/epoch - 197ms/step
Epoch 562/1000
2023-10-24 13:11:24.215 
Epoch 562/1000 
	 loss: 27.7805, MinusLogProbMetric: 27.7805, val_loss: 28.8312, val_MinusLogProbMetric: 28.8312

Epoch 562: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7805 - MinusLogProbMetric: 27.7805 - val_loss: 28.8312 - val_MinusLogProbMetric: 28.8312 - lr: 6.2500e-05 - 38s/epoch - 196ms/step
Epoch 563/1000
2023-10-24 13:12:02.741 
Epoch 563/1000 
	 loss: 27.7891, MinusLogProbMetric: 27.7891, val_loss: 29.0089, val_MinusLogProbMetric: 29.0089

Epoch 563: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7891 - MinusLogProbMetric: 27.7891 - val_loss: 29.0089 - val_MinusLogProbMetric: 29.0089 - lr: 6.2500e-05 - 39s/epoch - 197ms/step
Epoch 564/1000
2023-10-24 13:12:38.480 
Epoch 564/1000 
	 loss: 27.7923, MinusLogProbMetric: 27.7923, val_loss: 28.8739, val_MinusLogProbMetric: 28.8739

Epoch 564: val_loss did not improve from 28.81683
196/196 - 36s - loss: 27.7923 - MinusLogProbMetric: 27.7923 - val_loss: 28.8739 - val_MinusLogProbMetric: 28.8739 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 565/1000
2023-10-24 13:13:16.199 
Epoch 565/1000 
	 loss: 27.7829, MinusLogProbMetric: 27.7829, val_loss: 28.8426, val_MinusLogProbMetric: 28.8426

Epoch 565: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7829 - MinusLogProbMetric: 27.7829 - val_loss: 28.8426 - val_MinusLogProbMetric: 28.8426 - lr: 6.2500e-05 - 38s/epoch - 192ms/step
Epoch 566/1000
2023-10-24 13:13:54.639 
Epoch 566/1000 
	 loss: 27.7918, MinusLogProbMetric: 27.7918, val_loss: 28.8262, val_MinusLogProbMetric: 28.8262

Epoch 566: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7918 - MinusLogProbMetric: 27.7918 - val_loss: 28.8262 - val_MinusLogProbMetric: 28.8262 - lr: 6.2500e-05 - 38s/epoch - 196ms/step
Epoch 567/1000
2023-10-24 13:14:33.301 
Epoch 567/1000 
	 loss: 27.7835, MinusLogProbMetric: 27.7835, val_loss: 28.8727, val_MinusLogProbMetric: 28.8727

Epoch 567: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7835 - MinusLogProbMetric: 27.7835 - val_loss: 28.8727 - val_MinusLogProbMetric: 28.8727 - lr: 6.2500e-05 - 39s/epoch - 197ms/step
Epoch 568/1000
2023-10-24 13:15:08.464 
Epoch 568/1000 
	 loss: 27.7861, MinusLogProbMetric: 27.7861, val_loss: 28.8250, val_MinusLogProbMetric: 28.8250

Epoch 568: val_loss did not improve from 28.81683
196/196 - 35s - loss: 27.7861 - MinusLogProbMetric: 27.7861 - val_loss: 28.8250 - val_MinusLogProbMetric: 28.8250 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 569/1000
2023-10-24 13:15:44.752 
Epoch 569/1000 
	 loss: 27.7820, MinusLogProbMetric: 27.7820, val_loss: 28.8536, val_MinusLogProbMetric: 28.8536

Epoch 569: val_loss did not improve from 28.81683
196/196 - 36s - loss: 27.7820 - MinusLogProbMetric: 27.7820 - val_loss: 28.8536 - val_MinusLogProbMetric: 28.8536 - lr: 6.2500e-05 - 36s/epoch - 185ms/step
Epoch 570/1000
2023-10-24 13:16:19.503 
Epoch 570/1000 
	 loss: 27.7802, MinusLogProbMetric: 27.7802, val_loss: 28.8635, val_MinusLogProbMetric: 28.8635

Epoch 570: val_loss did not improve from 28.81683
196/196 - 35s - loss: 27.7802 - MinusLogProbMetric: 27.7802 - val_loss: 28.8635 - val_MinusLogProbMetric: 28.8635 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 571/1000
2023-10-24 13:16:54.237 
Epoch 571/1000 
	 loss: 27.7785, MinusLogProbMetric: 27.7785, val_loss: 28.8714, val_MinusLogProbMetric: 28.8714

Epoch 571: val_loss did not improve from 28.81683
196/196 - 35s - loss: 27.7785 - MinusLogProbMetric: 27.7785 - val_loss: 28.8714 - val_MinusLogProbMetric: 28.8714 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 572/1000
2023-10-24 13:17:31.240 
Epoch 572/1000 
	 loss: 27.7785, MinusLogProbMetric: 27.7785, val_loss: 28.8747, val_MinusLogProbMetric: 28.8747

Epoch 572: val_loss did not improve from 28.81683
196/196 - 37s - loss: 27.7785 - MinusLogProbMetric: 27.7785 - val_loss: 28.8747 - val_MinusLogProbMetric: 28.8747 - lr: 6.2500e-05 - 37s/epoch - 189ms/step
Epoch 573/1000
2023-10-24 13:18:07.162 
Epoch 573/1000 
	 loss: 27.7872, MinusLogProbMetric: 27.7872, val_loss: 28.8796, val_MinusLogProbMetric: 28.8796

Epoch 573: val_loss did not improve from 28.81683
196/196 - 36s - loss: 27.7872 - MinusLogProbMetric: 27.7872 - val_loss: 28.8796 - val_MinusLogProbMetric: 28.8796 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 574/1000
2023-10-24 13:18:43.901 
Epoch 574/1000 
	 loss: 27.7792, MinusLogProbMetric: 27.7792, val_loss: 28.8363, val_MinusLogProbMetric: 28.8363

Epoch 574: val_loss did not improve from 28.81683
196/196 - 37s - loss: 27.7792 - MinusLogProbMetric: 27.7792 - val_loss: 28.8363 - val_MinusLogProbMetric: 28.8363 - lr: 6.2500e-05 - 37s/epoch - 187ms/step
Epoch 575/1000
2023-10-24 13:19:20.613 
Epoch 575/1000 
	 loss: 27.7779, MinusLogProbMetric: 27.7779, val_loss: 28.8514, val_MinusLogProbMetric: 28.8514

Epoch 575: val_loss did not improve from 28.81683
196/196 - 37s - loss: 27.7779 - MinusLogProbMetric: 27.7779 - val_loss: 28.8514 - val_MinusLogProbMetric: 28.8514 - lr: 6.2500e-05 - 37s/epoch - 187ms/step
Epoch 576/1000
2023-10-24 13:19:57.384 
Epoch 576/1000 
	 loss: 27.7850, MinusLogProbMetric: 27.7850, val_loss: 28.8606, val_MinusLogProbMetric: 28.8606

Epoch 576: val_loss did not improve from 28.81683
196/196 - 37s - loss: 27.7850 - MinusLogProbMetric: 27.7850 - val_loss: 28.8606 - val_MinusLogProbMetric: 28.8606 - lr: 6.2500e-05 - 37s/epoch - 188ms/step
Epoch 577/1000
2023-10-24 13:20:33.637 
Epoch 577/1000 
	 loss: 27.7766, MinusLogProbMetric: 27.7766, val_loss: 28.8376, val_MinusLogProbMetric: 28.8376

Epoch 577: val_loss did not improve from 28.81683
196/196 - 36s - loss: 27.7766 - MinusLogProbMetric: 27.7766 - val_loss: 28.8376 - val_MinusLogProbMetric: 28.8376 - lr: 6.2500e-05 - 36s/epoch - 185ms/step
Epoch 578/1000
2023-10-24 13:21:10.166 
Epoch 578/1000 
	 loss: 27.7892, MinusLogProbMetric: 27.7892, val_loss: 28.9039, val_MinusLogProbMetric: 28.9039

Epoch 578: val_loss did not improve from 28.81683
196/196 - 37s - loss: 27.7892 - MinusLogProbMetric: 27.7892 - val_loss: 28.9039 - val_MinusLogProbMetric: 28.9039 - lr: 6.2500e-05 - 37s/epoch - 186ms/step
Epoch 579/1000
2023-10-24 13:21:49.042 
Epoch 579/1000 
	 loss: 27.7798, MinusLogProbMetric: 27.7798, val_loss: 28.8856, val_MinusLogProbMetric: 28.8856

Epoch 579: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7798 - MinusLogProbMetric: 27.7798 - val_loss: 28.8856 - val_MinusLogProbMetric: 28.8856 - lr: 6.2500e-05 - 39s/epoch - 198ms/step
Epoch 580/1000
2023-10-24 13:22:27.213 
Epoch 580/1000 
	 loss: 27.7867, MinusLogProbMetric: 27.7867, val_loss: 28.8514, val_MinusLogProbMetric: 28.8514

Epoch 580: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7867 - MinusLogProbMetric: 27.7867 - val_loss: 28.8514 - val_MinusLogProbMetric: 28.8514 - lr: 6.2500e-05 - 38s/epoch - 195ms/step
Epoch 581/1000
2023-10-24 13:23:06.238 
Epoch 581/1000 
	 loss: 27.7854, MinusLogProbMetric: 27.7854, val_loss: 28.8341, val_MinusLogProbMetric: 28.8341

Epoch 581: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7854 - MinusLogProbMetric: 27.7854 - val_loss: 28.8341 - val_MinusLogProbMetric: 28.8341 - lr: 6.2500e-05 - 39s/epoch - 199ms/step
Epoch 582/1000
2023-10-24 13:23:45.043 
Epoch 582/1000 
	 loss: 27.7856, MinusLogProbMetric: 27.7856, val_loss: 28.8366, val_MinusLogProbMetric: 28.8366

Epoch 582: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7856 - MinusLogProbMetric: 27.7856 - val_loss: 28.8366 - val_MinusLogProbMetric: 28.8366 - lr: 6.2500e-05 - 39s/epoch - 198ms/step
Epoch 583/1000
2023-10-24 13:24:21.888 
Epoch 583/1000 
	 loss: 27.7803, MinusLogProbMetric: 27.7803, val_loss: 28.8443, val_MinusLogProbMetric: 28.8443

Epoch 583: val_loss did not improve from 28.81683
196/196 - 37s - loss: 27.7803 - MinusLogProbMetric: 27.7803 - val_loss: 28.8443 - val_MinusLogProbMetric: 28.8443 - lr: 6.2500e-05 - 37s/epoch - 188ms/step
Epoch 584/1000
2023-10-24 13:24:58.544 
Epoch 584/1000 
	 loss: 27.7784, MinusLogProbMetric: 27.7784, val_loss: 28.8726, val_MinusLogProbMetric: 28.8726

Epoch 584: val_loss did not improve from 28.81683
196/196 - 37s - loss: 27.7784 - MinusLogProbMetric: 27.7784 - val_loss: 28.8726 - val_MinusLogProbMetric: 28.8726 - lr: 6.2500e-05 - 37s/epoch - 187ms/step
Epoch 585/1000
2023-10-24 13:25:34.341 
Epoch 585/1000 
	 loss: 27.7852, MinusLogProbMetric: 27.7852, val_loss: 28.8496, val_MinusLogProbMetric: 28.8496

Epoch 585: val_loss did not improve from 28.81683
196/196 - 36s - loss: 27.7852 - MinusLogProbMetric: 27.7852 - val_loss: 28.8496 - val_MinusLogProbMetric: 28.8496 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 586/1000
2023-10-24 13:26:10.552 
Epoch 586/1000 
	 loss: 27.7877, MinusLogProbMetric: 27.7877, val_loss: 28.8990, val_MinusLogProbMetric: 28.8990

Epoch 586: val_loss did not improve from 28.81683
196/196 - 36s - loss: 27.7877 - MinusLogProbMetric: 27.7877 - val_loss: 28.8990 - val_MinusLogProbMetric: 28.8990 - lr: 6.2500e-05 - 36s/epoch - 185ms/step
Epoch 587/1000
2023-10-24 13:26:47.060 
Epoch 587/1000 
	 loss: 27.7772, MinusLogProbMetric: 27.7772, val_loss: 28.8851, val_MinusLogProbMetric: 28.8851

Epoch 587: val_loss did not improve from 28.81683
196/196 - 37s - loss: 27.7772 - MinusLogProbMetric: 27.7772 - val_loss: 28.8851 - val_MinusLogProbMetric: 28.8851 - lr: 6.2500e-05 - 37s/epoch - 186ms/step
Epoch 588/1000
2023-10-24 13:27:24.310 
Epoch 588/1000 
	 loss: 27.7817, MinusLogProbMetric: 27.7817, val_loss: 28.9352, val_MinusLogProbMetric: 28.9352

Epoch 588: val_loss did not improve from 28.81683
196/196 - 37s - loss: 27.7817 - MinusLogProbMetric: 27.7817 - val_loss: 28.9352 - val_MinusLogProbMetric: 28.9352 - lr: 6.2500e-05 - 37s/epoch - 190ms/step
Epoch 589/1000
2023-10-24 13:28:00.703 
Epoch 589/1000 
	 loss: 27.7800, MinusLogProbMetric: 27.7800, val_loss: 28.8675, val_MinusLogProbMetric: 28.8675

Epoch 589: val_loss did not improve from 28.81683
196/196 - 36s - loss: 27.7800 - MinusLogProbMetric: 27.7800 - val_loss: 28.8675 - val_MinusLogProbMetric: 28.8675 - lr: 6.2500e-05 - 36s/epoch - 186ms/step
Epoch 590/1000
2023-10-24 13:28:37.617 
Epoch 590/1000 
	 loss: 27.7772, MinusLogProbMetric: 27.7772, val_loss: 28.8336, val_MinusLogProbMetric: 28.8336

Epoch 590: val_loss did not improve from 28.81683
196/196 - 37s - loss: 27.7772 - MinusLogProbMetric: 27.7772 - val_loss: 28.8336 - val_MinusLogProbMetric: 28.8336 - lr: 6.2500e-05 - 37s/epoch - 188ms/step
Epoch 591/1000
2023-10-24 13:29:14.402 
Epoch 591/1000 
	 loss: 27.7779, MinusLogProbMetric: 27.7779, val_loss: 28.9018, val_MinusLogProbMetric: 28.9018

Epoch 591: val_loss did not improve from 28.81683
196/196 - 37s - loss: 27.7779 - MinusLogProbMetric: 27.7779 - val_loss: 28.9018 - val_MinusLogProbMetric: 28.9018 - lr: 6.2500e-05 - 37s/epoch - 188ms/step
Epoch 592/1000
2023-10-24 13:29:51.182 
Epoch 592/1000 
	 loss: 27.7858, MinusLogProbMetric: 27.7858, val_loss: 28.8732, val_MinusLogProbMetric: 28.8732

Epoch 592: val_loss did not improve from 28.81683
196/196 - 37s - loss: 27.7858 - MinusLogProbMetric: 27.7858 - val_loss: 28.8732 - val_MinusLogProbMetric: 28.8732 - lr: 6.2500e-05 - 37s/epoch - 188ms/step
Epoch 593/1000
2023-10-24 13:30:28.505 
Epoch 593/1000 
	 loss: 27.7759, MinusLogProbMetric: 27.7759, val_loss: 28.8729, val_MinusLogProbMetric: 28.8729

Epoch 593: val_loss did not improve from 28.81683
196/196 - 37s - loss: 27.7759 - MinusLogProbMetric: 27.7759 - val_loss: 28.8729 - val_MinusLogProbMetric: 28.8729 - lr: 6.2500e-05 - 37s/epoch - 190ms/step
Epoch 594/1000
2023-10-24 13:31:04.306 
Epoch 594/1000 
	 loss: 27.7817, MinusLogProbMetric: 27.7817, val_loss: 28.8658, val_MinusLogProbMetric: 28.8658

Epoch 594: val_loss did not improve from 28.81683
196/196 - 36s - loss: 27.7817 - MinusLogProbMetric: 27.7817 - val_loss: 28.8658 - val_MinusLogProbMetric: 28.8658 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 595/1000
2023-10-24 13:31:40.585 
Epoch 595/1000 
	 loss: 27.7754, MinusLogProbMetric: 27.7754, val_loss: 28.8707, val_MinusLogProbMetric: 28.8707

Epoch 595: val_loss did not improve from 28.81683
196/196 - 36s - loss: 27.7754 - MinusLogProbMetric: 27.7754 - val_loss: 28.8707 - val_MinusLogProbMetric: 28.8707 - lr: 6.2500e-05 - 36s/epoch - 185ms/step
Epoch 596/1000
2023-10-24 13:32:16.992 
Epoch 596/1000 
	 loss: 27.7745, MinusLogProbMetric: 27.7745, val_loss: 28.8601, val_MinusLogProbMetric: 28.8601

Epoch 596: val_loss did not improve from 28.81683
196/196 - 36s - loss: 27.7745 - MinusLogProbMetric: 27.7745 - val_loss: 28.8601 - val_MinusLogProbMetric: 28.8601 - lr: 6.2500e-05 - 36s/epoch - 186ms/step
Epoch 597/1000
2023-10-24 13:32:55.472 
Epoch 597/1000 
	 loss: 27.7826, MinusLogProbMetric: 27.7826, val_loss: 28.8712, val_MinusLogProbMetric: 28.8712

Epoch 597: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7826 - MinusLogProbMetric: 27.7826 - val_loss: 28.8712 - val_MinusLogProbMetric: 28.8712 - lr: 6.2500e-05 - 38s/epoch - 196ms/step
Epoch 598/1000
2023-10-24 13:33:33.325 
Epoch 598/1000 
	 loss: 27.7815, MinusLogProbMetric: 27.7815, val_loss: 28.8637, val_MinusLogProbMetric: 28.8637

Epoch 598: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7815 - MinusLogProbMetric: 27.7815 - val_loss: 28.8637 - val_MinusLogProbMetric: 28.8637 - lr: 6.2500e-05 - 38s/epoch - 193ms/step
Epoch 599/1000
2023-10-24 13:34:12.369 
Epoch 599/1000 
	 loss: 27.7713, MinusLogProbMetric: 27.7713, val_loss: 28.8417, val_MinusLogProbMetric: 28.8417

Epoch 599: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7713 - MinusLogProbMetric: 27.7713 - val_loss: 28.8417 - val_MinusLogProbMetric: 28.8417 - lr: 6.2500e-05 - 39s/epoch - 199ms/step
Epoch 600/1000
2023-10-24 13:34:50.274 
Epoch 600/1000 
	 loss: 27.7785, MinusLogProbMetric: 27.7785, val_loss: 28.8789, val_MinusLogProbMetric: 28.8789

Epoch 600: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7785 - MinusLogProbMetric: 27.7785 - val_loss: 28.8789 - val_MinusLogProbMetric: 28.8789 - lr: 6.2500e-05 - 38s/epoch - 193ms/step
Epoch 601/1000
2023-10-24 13:35:28.512 
Epoch 601/1000 
	 loss: 27.7743, MinusLogProbMetric: 27.7743, val_loss: 28.8498, val_MinusLogProbMetric: 28.8498

Epoch 601: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7743 - MinusLogProbMetric: 27.7743 - val_loss: 28.8498 - val_MinusLogProbMetric: 28.8498 - lr: 6.2500e-05 - 38s/epoch - 195ms/step
Epoch 602/1000
2023-10-24 13:36:06.980 
Epoch 602/1000 
	 loss: 27.7688, MinusLogProbMetric: 27.7688, val_loss: 28.8666, val_MinusLogProbMetric: 28.8666

Epoch 602: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7688 - MinusLogProbMetric: 27.7688 - val_loss: 28.8666 - val_MinusLogProbMetric: 28.8666 - lr: 6.2500e-05 - 38s/epoch - 196ms/step
Epoch 603/1000
2023-10-24 13:36:45.021 
Epoch 603/1000 
	 loss: 27.7759, MinusLogProbMetric: 27.7759, val_loss: 28.8629, val_MinusLogProbMetric: 28.8629

Epoch 603: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7759 - MinusLogProbMetric: 27.7759 - val_loss: 28.8629 - val_MinusLogProbMetric: 28.8629 - lr: 6.2500e-05 - 38s/epoch - 194ms/step
Epoch 604/1000
2023-10-24 13:37:23.452 
Epoch 604/1000 
	 loss: 27.7778, MinusLogProbMetric: 27.7778, val_loss: 28.8345, val_MinusLogProbMetric: 28.8345

Epoch 604: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7778 - MinusLogProbMetric: 27.7778 - val_loss: 28.8345 - val_MinusLogProbMetric: 28.8345 - lr: 6.2500e-05 - 38s/epoch - 196ms/step
Epoch 605/1000
2023-10-24 13:38:02.301 
Epoch 605/1000 
	 loss: 27.7698, MinusLogProbMetric: 27.7698, val_loss: 28.8247, val_MinusLogProbMetric: 28.8247

Epoch 605: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7698 - MinusLogProbMetric: 27.7698 - val_loss: 28.8247 - val_MinusLogProbMetric: 28.8247 - lr: 6.2500e-05 - 39s/epoch - 198ms/step
Epoch 606/1000
2023-10-24 13:38:40.194 
Epoch 606/1000 
	 loss: 27.7725, MinusLogProbMetric: 27.7725, val_loss: 28.8558, val_MinusLogProbMetric: 28.8558

Epoch 606: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7725 - MinusLogProbMetric: 27.7725 - val_loss: 28.8558 - val_MinusLogProbMetric: 28.8558 - lr: 6.2500e-05 - 38s/epoch - 193ms/step
Epoch 607/1000
2023-10-24 13:39:19.085 
Epoch 607/1000 
	 loss: 27.7709, MinusLogProbMetric: 27.7709, val_loss: 28.8629, val_MinusLogProbMetric: 28.8629

Epoch 607: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7709 - MinusLogProbMetric: 27.7709 - val_loss: 28.8629 - val_MinusLogProbMetric: 28.8629 - lr: 6.2500e-05 - 39s/epoch - 198ms/step
Epoch 608/1000
2023-10-24 13:39:57.525 
Epoch 608/1000 
	 loss: 27.7490, MinusLogProbMetric: 27.7490, val_loss: 28.8569, val_MinusLogProbMetric: 28.8569

Epoch 608: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7490 - MinusLogProbMetric: 27.7490 - val_loss: 28.8569 - val_MinusLogProbMetric: 28.8569 - lr: 3.1250e-05 - 38s/epoch - 196ms/step
Epoch 609/1000
2023-10-24 13:40:36.335 
Epoch 609/1000 
	 loss: 27.7449, MinusLogProbMetric: 27.7449, val_loss: 28.8618, val_MinusLogProbMetric: 28.8618

Epoch 609: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7449 - MinusLogProbMetric: 27.7449 - val_loss: 28.8618 - val_MinusLogProbMetric: 28.8618 - lr: 3.1250e-05 - 39s/epoch - 198ms/step
Epoch 610/1000
2023-10-24 13:41:14.102 
Epoch 610/1000 
	 loss: 27.7504, MinusLogProbMetric: 27.7504, val_loss: 28.8474, val_MinusLogProbMetric: 28.8474

Epoch 610: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7504 - MinusLogProbMetric: 27.7504 - val_loss: 28.8474 - val_MinusLogProbMetric: 28.8474 - lr: 3.1250e-05 - 38s/epoch - 193ms/step
Epoch 611/1000
2023-10-24 13:41:47.836 
Epoch 611/1000 
	 loss: 27.7446, MinusLogProbMetric: 27.7446, val_loss: 28.8290, val_MinusLogProbMetric: 28.8290

Epoch 611: val_loss did not improve from 28.81683
196/196 - 34s - loss: 27.7446 - MinusLogProbMetric: 27.7446 - val_loss: 28.8290 - val_MinusLogProbMetric: 28.8290 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 612/1000
2023-10-24 13:42:21.649 
Epoch 612/1000 
	 loss: 27.7462, MinusLogProbMetric: 27.7462, val_loss: 28.8682, val_MinusLogProbMetric: 28.8682

Epoch 612: val_loss did not improve from 28.81683
196/196 - 34s - loss: 27.7462 - MinusLogProbMetric: 27.7462 - val_loss: 28.8682 - val_MinusLogProbMetric: 28.8682 - lr: 3.1250e-05 - 34s/epoch - 172ms/step
Epoch 613/1000
2023-10-24 13:42:55.837 
Epoch 613/1000 
	 loss: 27.7492, MinusLogProbMetric: 27.7492, val_loss: 28.8210, val_MinusLogProbMetric: 28.8210

Epoch 613: val_loss did not improve from 28.81683
196/196 - 34s - loss: 27.7492 - MinusLogProbMetric: 27.7492 - val_loss: 28.8210 - val_MinusLogProbMetric: 28.8210 - lr: 3.1250e-05 - 34s/epoch - 174ms/step
Epoch 614/1000
2023-10-24 13:43:32.991 
Epoch 614/1000 
	 loss: 27.7449, MinusLogProbMetric: 27.7449, val_loss: 28.8348, val_MinusLogProbMetric: 28.8348

Epoch 614: val_loss did not improve from 28.81683
196/196 - 37s - loss: 27.7449 - MinusLogProbMetric: 27.7449 - val_loss: 28.8348 - val_MinusLogProbMetric: 28.8348 - lr: 3.1250e-05 - 37s/epoch - 190ms/step
Epoch 615/1000
2023-10-24 13:44:09.937 
Epoch 615/1000 
	 loss: 27.7504, MinusLogProbMetric: 27.7504, val_loss: 28.8457, val_MinusLogProbMetric: 28.8457

Epoch 615: val_loss did not improve from 28.81683
196/196 - 37s - loss: 27.7504 - MinusLogProbMetric: 27.7504 - val_loss: 28.8457 - val_MinusLogProbMetric: 28.8457 - lr: 3.1250e-05 - 37s/epoch - 188ms/step
Epoch 616/1000
2023-10-24 13:44:47.910 
Epoch 616/1000 
	 loss: 27.7486, MinusLogProbMetric: 27.7486, val_loss: 28.8379, val_MinusLogProbMetric: 28.8379

Epoch 616: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7486 - MinusLogProbMetric: 27.7486 - val_loss: 28.8379 - val_MinusLogProbMetric: 28.8379 - lr: 3.1250e-05 - 38s/epoch - 194ms/step
Epoch 617/1000
2023-10-24 13:45:26.801 
Epoch 617/1000 
	 loss: 27.7496, MinusLogProbMetric: 27.7496, val_loss: 28.8408, val_MinusLogProbMetric: 28.8408

Epoch 617: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7496 - MinusLogProbMetric: 27.7496 - val_loss: 28.8408 - val_MinusLogProbMetric: 28.8408 - lr: 3.1250e-05 - 39s/epoch - 198ms/step
Epoch 618/1000
2023-10-24 13:46:05.697 
Epoch 618/1000 
	 loss: 27.7464, MinusLogProbMetric: 27.7464, val_loss: 28.8335, val_MinusLogProbMetric: 28.8335

Epoch 618: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7464 - MinusLogProbMetric: 27.7464 - val_loss: 28.8335 - val_MinusLogProbMetric: 28.8335 - lr: 3.1250e-05 - 39s/epoch - 198ms/step
Epoch 619/1000
2023-10-24 13:46:44.218 
Epoch 619/1000 
	 loss: 27.7439, MinusLogProbMetric: 27.7439, val_loss: 28.8436, val_MinusLogProbMetric: 28.8436

Epoch 619: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7439 - MinusLogProbMetric: 27.7439 - val_loss: 28.8436 - val_MinusLogProbMetric: 28.8436 - lr: 3.1250e-05 - 39s/epoch - 197ms/step
Epoch 620/1000
2023-10-24 13:47:23.238 
Epoch 620/1000 
	 loss: 27.7449, MinusLogProbMetric: 27.7449, val_loss: 28.8472, val_MinusLogProbMetric: 28.8472

Epoch 620: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7449 - MinusLogProbMetric: 27.7449 - val_loss: 28.8472 - val_MinusLogProbMetric: 28.8472 - lr: 3.1250e-05 - 39s/epoch - 199ms/step
Epoch 621/1000
2023-10-24 13:48:01.359 
Epoch 621/1000 
	 loss: 27.7453, MinusLogProbMetric: 27.7453, val_loss: 28.8429, val_MinusLogProbMetric: 28.8429

Epoch 621: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7453 - MinusLogProbMetric: 27.7453 - val_loss: 28.8429 - val_MinusLogProbMetric: 28.8429 - lr: 3.1250e-05 - 38s/epoch - 194ms/step
Epoch 622/1000
2023-10-24 13:48:39.995 
Epoch 622/1000 
	 loss: 27.7441, MinusLogProbMetric: 27.7441, val_loss: 28.8541, val_MinusLogProbMetric: 28.8541

Epoch 622: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7441 - MinusLogProbMetric: 27.7441 - val_loss: 28.8541 - val_MinusLogProbMetric: 28.8541 - lr: 3.1250e-05 - 39s/epoch - 197ms/step
Epoch 623/1000
2023-10-24 13:49:18.964 
Epoch 623/1000 
	 loss: 27.7451, MinusLogProbMetric: 27.7451, val_loss: 28.8332, val_MinusLogProbMetric: 28.8332

Epoch 623: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7451 - MinusLogProbMetric: 27.7451 - val_loss: 28.8332 - val_MinusLogProbMetric: 28.8332 - lr: 3.1250e-05 - 39s/epoch - 199ms/step
Epoch 624/1000
2023-10-24 13:49:57.605 
Epoch 624/1000 
	 loss: 27.7422, MinusLogProbMetric: 27.7422, val_loss: 28.8315, val_MinusLogProbMetric: 28.8315

Epoch 624: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7422 - MinusLogProbMetric: 27.7422 - val_loss: 28.8315 - val_MinusLogProbMetric: 28.8315 - lr: 3.1250e-05 - 39s/epoch - 197ms/step
Epoch 625/1000
2023-10-24 13:50:36.217 
Epoch 625/1000 
	 loss: 27.7484, MinusLogProbMetric: 27.7484, val_loss: 28.8435, val_MinusLogProbMetric: 28.8435

Epoch 625: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7484 - MinusLogProbMetric: 27.7484 - val_loss: 28.8435 - val_MinusLogProbMetric: 28.8435 - lr: 3.1250e-05 - 39s/epoch - 197ms/step
Epoch 626/1000
2023-10-24 13:51:14.884 
Epoch 626/1000 
	 loss: 27.7454, MinusLogProbMetric: 27.7454, val_loss: 28.8352, val_MinusLogProbMetric: 28.8352

Epoch 626: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7454 - MinusLogProbMetric: 27.7454 - val_loss: 28.8352 - val_MinusLogProbMetric: 28.8352 - lr: 3.1250e-05 - 39s/epoch - 197ms/step
Epoch 627/1000
2023-10-24 13:51:53.483 
Epoch 627/1000 
	 loss: 27.7432, MinusLogProbMetric: 27.7432, val_loss: 28.8232, val_MinusLogProbMetric: 28.8232

Epoch 627: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7432 - MinusLogProbMetric: 27.7432 - val_loss: 28.8232 - val_MinusLogProbMetric: 28.8232 - lr: 3.1250e-05 - 39s/epoch - 197ms/step
Epoch 628/1000
2023-10-24 13:52:32.472 
Epoch 628/1000 
	 loss: 27.7528, MinusLogProbMetric: 27.7528, val_loss: 28.8595, val_MinusLogProbMetric: 28.8595

Epoch 628: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7528 - MinusLogProbMetric: 27.7528 - val_loss: 28.8595 - val_MinusLogProbMetric: 28.8595 - lr: 3.1250e-05 - 39s/epoch - 199ms/step
Epoch 629/1000
2023-10-24 13:53:11.225 
Epoch 629/1000 
	 loss: 27.7456, MinusLogProbMetric: 27.7456, val_loss: 28.8502, val_MinusLogProbMetric: 28.8502

Epoch 629: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7456 - MinusLogProbMetric: 27.7456 - val_loss: 28.8502 - val_MinusLogProbMetric: 28.8502 - lr: 3.1250e-05 - 39s/epoch - 198ms/step
Epoch 630/1000
2023-10-24 13:53:49.494 
Epoch 630/1000 
	 loss: 27.7421, MinusLogProbMetric: 27.7421, val_loss: 28.8486, val_MinusLogProbMetric: 28.8486

Epoch 630: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7421 - MinusLogProbMetric: 27.7421 - val_loss: 28.8486 - val_MinusLogProbMetric: 28.8486 - lr: 3.1250e-05 - 38s/epoch - 195ms/step
Epoch 631/1000
2023-10-24 13:54:28.099 
Epoch 631/1000 
	 loss: 27.7419, MinusLogProbMetric: 27.7419, val_loss: 28.8386, val_MinusLogProbMetric: 28.8386

Epoch 631: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7419 - MinusLogProbMetric: 27.7419 - val_loss: 28.8386 - val_MinusLogProbMetric: 28.8386 - lr: 3.1250e-05 - 39s/epoch - 197ms/step
Epoch 632/1000
2023-10-24 13:55:06.794 
Epoch 632/1000 
	 loss: 27.7396, MinusLogProbMetric: 27.7396, val_loss: 28.8331, val_MinusLogProbMetric: 28.8331

Epoch 632: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7396 - MinusLogProbMetric: 27.7396 - val_loss: 28.8331 - val_MinusLogProbMetric: 28.8331 - lr: 3.1250e-05 - 39s/epoch - 197ms/step
Epoch 633/1000
2023-10-24 13:55:45.236 
Epoch 633/1000 
	 loss: 27.7450, MinusLogProbMetric: 27.7450, val_loss: 28.8382, val_MinusLogProbMetric: 28.8382

Epoch 633: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7450 - MinusLogProbMetric: 27.7450 - val_loss: 28.8382 - val_MinusLogProbMetric: 28.8382 - lr: 3.1250e-05 - 38s/epoch - 196ms/step
Epoch 634/1000
2023-10-24 13:56:23.667 
Epoch 634/1000 
	 loss: 27.7412, MinusLogProbMetric: 27.7412, val_loss: 28.8537, val_MinusLogProbMetric: 28.8537

Epoch 634: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7412 - MinusLogProbMetric: 27.7412 - val_loss: 28.8537 - val_MinusLogProbMetric: 28.8537 - lr: 3.1250e-05 - 38s/epoch - 196ms/step
Epoch 635/1000
2023-10-24 13:57:02.537 
Epoch 635/1000 
	 loss: 27.7420, MinusLogProbMetric: 27.7420, val_loss: 28.8387, val_MinusLogProbMetric: 28.8387

Epoch 635: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7420 - MinusLogProbMetric: 27.7420 - val_loss: 28.8387 - val_MinusLogProbMetric: 28.8387 - lr: 3.1250e-05 - 39s/epoch - 198ms/step
Epoch 636/1000
2023-10-24 13:57:41.407 
Epoch 636/1000 
	 loss: 27.7409, MinusLogProbMetric: 27.7409, val_loss: 28.8457, val_MinusLogProbMetric: 28.8457

Epoch 636: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7409 - MinusLogProbMetric: 27.7409 - val_loss: 28.8457 - val_MinusLogProbMetric: 28.8457 - lr: 3.1250e-05 - 39s/epoch - 198ms/step
Epoch 637/1000
2023-10-24 13:58:19.813 
Epoch 637/1000 
	 loss: 27.7426, MinusLogProbMetric: 27.7426, val_loss: 28.8292, val_MinusLogProbMetric: 28.8292

Epoch 637: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7426 - MinusLogProbMetric: 27.7426 - val_loss: 28.8292 - val_MinusLogProbMetric: 28.8292 - lr: 3.1250e-05 - 38s/epoch - 196ms/step
Epoch 638/1000
2023-10-24 13:58:58.136 
Epoch 638/1000 
	 loss: 27.7408, MinusLogProbMetric: 27.7408, val_loss: 28.8291, val_MinusLogProbMetric: 28.8291

Epoch 638: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7408 - MinusLogProbMetric: 27.7408 - val_loss: 28.8291 - val_MinusLogProbMetric: 28.8291 - lr: 3.1250e-05 - 38s/epoch - 196ms/step
Epoch 639/1000
2023-10-24 13:59:37.085 
Epoch 639/1000 
	 loss: 27.7469, MinusLogProbMetric: 27.7469, val_loss: 28.8286, val_MinusLogProbMetric: 28.8286

Epoch 639: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7469 - MinusLogProbMetric: 27.7469 - val_loss: 28.8286 - val_MinusLogProbMetric: 28.8286 - lr: 3.1250e-05 - 39s/epoch - 199ms/step
Epoch 640/1000
2023-10-24 14:00:14.382 
Epoch 640/1000 
	 loss: 27.7414, MinusLogProbMetric: 27.7414, val_loss: 28.8606, val_MinusLogProbMetric: 28.8606

Epoch 640: val_loss did not improve from 28.81683
196/196 - 37s - loss: 27.7414 - MinusLogProbMetric: 27.7414 - val_loss: 28.8606 - val_MinusLogProbMetric: 28.8606 - lr: 3.1250e-05 - 37s/epoch - 190ms/step
Epoch 641/1000
2023-10-24 14:00:52.072 
Epoch 641/1000 
	 loss: 27.7469, MinusLogProbMetric: 27.7469, val_loss: 28.8431, val_MinusLogProbMetric: 28.8431

Epoch 641: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7469 - MinusLogProbMetric: 27.7469 - val_loss: 28.8431 - val_MinusLogProbMetric: 28.8431 - lr: 3.1250e-05 - 38s/epoch - 192ms/step
Epoch 642/1000
2023-10-24 14:01:28.217 
Epoch 642/1000 
	 loss: 27.7474, MinusLogProbMetric: 27.7474, val_loss: 28.8340, val_MinusLogProbMetric: 28.8340

Epoch 642: val_loss did not improve from 28.81683
196/196 - 36s - loss: 27.7474 - MinusLogProbMetric: 27.7474 - val_loss: 28.8340 - val_MinusLogProbMetric: 28.8340 - lr: 3.1250e-05 - 36s/epoch - 184ms/step
Epoch 643/1000
2023-10-24 14:02:04.384 
Epoch 643/1000 
	 loss: 27.7398, MinusLogProbMetric: 27.7398, val_loss: 28.8275, val_MinusLogProbMetric: 28.8275

Epoch 643: val_loss did not improve from 28.81683
196/196 - 36s - loss: 27.7398 - MinusLogProbMetric: 27.7398 - val_loss: 28.8275 - val_MinusLogProbMetric: 28.8275 - lr: 3.1250e-05 - 36s/epoch - 185ms/step
Epoch 644/1000
2023-10-24 14:02:42.306 
Epoch 644/1000 
	 loss: 27.7437, MinusLogProbMetric: 27.7437, val_loss: 28.8382, val_MinusLogProbMetric: 28.8382

Epoch 644: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7437 - MinusLogProbMetric: 27.7437 - val_loss: 28.8382 - val_MinusLogProbMetric: 28.8382 - lr: 3.1250e-05 - 38s/epoch - 193ms/step
Epoch 645/1000
2023-10-24 14:03:21.073 
Epoch 645/1000 
	 loss: 27.7401, MinusLogProbMetric: 27.7401, val_loss: 28.8359, val_MinusLogProbMetric: 28.8359

Epoch 645: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7401 - MinusLogProbMetric: 27.7401 - val_loss: 28.8359 - val_MinusLogProbMetric: 28.8359 - lr: 3.1250e-05 - 39s/epoch - 198ms/step
Epoch 646/1000
2023-10-24 14:03:59.748 
Epoch 646/1000 
	 loss: 27.7464, MinusLogProbMetric: 27.7464, val_loss: 28.8342, val_MinusLogProbMetric: 28.8342

Epoch 646: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7464 - MinusLogProbMetric: 27.7464 - val_loss: 28.8342 - val_MinusLogProbMetric: 28.8342 - lr: 3.1250e-05 - 39s/epoch - 197ms/step
Epoch 647/1000
2023-10-24 14:04:39.001 
Epoch 647/1000 
	 loss: 27.7414, MinusLogProbMetric: 27.7414, val_loss: 28.8369, val_MinusLogProbMetric: 28.8369

Epoch 647: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7414 - MinusLogProbMetric: 27.7414 - val_loss: 28.8369 - val_MinusLogProbMetric: 28.8369 - lr: 3.1250e-05 - 39s/epoch - 200ms/step
Epoch 648/1000
2023-10-24 14:05:17.534 
Epoch 648/1000 
	 loss: 27.7397, MinusLogProbMetric: 27.7397, val_loss: 28.8547, val_MinusLogProbMetric: 28.8547

Epoch 648: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7397 - MinusLogProbMetric: 27.7397 - val_loss: 28.8547 - val_MinusLogProbMetric: 28.8547 - lr: 3.1250e-05 - 39s/epoch - 197ms/step
Epoch 649/1000
2023-10-24 14:05:55.992 
Epoch 649/1000 
	 loss: 27.7429, MinusLogProbMetric: 27.7429, val_loss: 28.8325, val_MinusLogProbMetric: 28.8325

Epoch 649: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7429 - MinusLogProbMetric: 27.7429 - val_loss: 28.8325 - val_MinusLogProbMetric: 28.8325 - lr: 3.1250e-05 - 38s/epoch - 196ms/step
Epoch 650/1000
2023-10-24 14:06:34.133 
Epoch 650/1000 
	 loss: 27.7433, MinusLogProbMetric: 27.7433, val_loss: 28.8525, val_MinusLogProbMetric: 28.8525

Epoch 650: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7433 - MinusLogProbMetric: 27.7433 - val_loss: 28.8525 - val_MinusLogProbMetric: 28.8525 - lr: 3.1250e-05 - 38s/epoch - 195ms/step
Epoch 651/1000
2023-10-24 14:07:13.272 
Epoch 651/1000 
	 loss: 27.7431, MinusLogProbMetric: 27.7431, val_loss: 28.8418, val_MinusLogProbMetric: 28.8418

Epoch 651: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7431 - MinusLogProbMetric: 27.7431 - val_loss: 28.8418 - val_MinusLogProbMetric: 28.8418 - lr: 3.1250e-05 - 39s/epoch - 200ms/step
Epoch 652/1000
2023-10-24 14:07:52.173 
Epoch 652/1000 
	 loss: 27.7447, MinusLogProbMetric: 27.7447, val_loss: 28.8400, val_MinusLogProbMetric: 28.8400

Epoch 652: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7447 - MinusLogProbMetric: 27.7447 - val_loss: 28.8400 - val_MinusLogProbMetric: 28.8400 - lr: 3.1250e-05 - 39s/epoch - 198ms/step
Epoch 653/1000
2023-10-24 14:08:30.607 
Epoch 653/1000 
	 loss: 27.7444, MinusLogProbMetric: 27.7444, val_loss: 28.8484, val_MinusLogProbMetric: 28.8484

Epoch 653: val_loss did not improve from 28.81683
196/196 - 38s - loss: 27.7444 - MinusLogProbMetric: 27.7444 - val_loss: 28.8484 - val_MinusLogProbMetric: 28.8484 - lr: 3.1250e-05 - 38s/epoch - 196ms/step
Epoch 654/1000
2023-10-24 14:09:07.552 
Epoch 654/1000 
	 loss: 27.7419, MinusLogProbMetric: 27.7419, val_loss: 28.8329, val_MinusLogProbMetric: 28.8329

Epoch 654: val_loss did not improve from 28.81683
196/196 - 37s - loss: 27.7419 - MinusLogProbMetric: 27.7419 - val_loss: 28.8329 - val_MinusLogProbMetric: 28.8329 - lr: 3.1250e-05 - 37s/epoch - 188ms/step
Epoch 655/1000
2023-10-24 14:09:43.609 
Epoch 655/1000 
	 loss: 27.7449, MinusLogProbMetric: 27.7449, val_loss: 28.8378, val_MinusLogProbMetric: 28.8378

Epoch 655: val_loss did not improve from 28.81683
196/196 - 36s - loss: 27.7449 - MinusLogProbMetric: 27.7449 - val_loss: 28.8378 - val_MinusLogProbMetric: 28.8378 - lr: 3.1250e-05 - 36s/epoch - 184ms/step
Epoch 656/1000
2023-10-24 14:10:22.744 
Epoch 656/1000 
	 loss: 27.7425, MinusLogProbMetric: 27.7425, val_loss: 28.8531, val_MinusLogProbMetric: 28.8531

Epoch 656: val_loss did not improve from 28.81683
196/196 - 39s - loss: 27.7425 - MinusLogProbMetric: 27.7425 - val_loss: 28.8531 - val_MinusLogProbMetric: 28.8531 - lr: 3.1250e-05 - 39s/epoch - 200ms/step
Epoch 657/1000
2023-10-24 14:10:59.231 
Epoch 657/1000 
	 loss: 27.7412, MinusLogProbMetric: 27.7412, val_loss: 28.8710, val_MinusLogProbMetric: 28.8710

Epoch 657: val_loss did not improve from 28.81683
Restoring model weights from the end of the best epoch: 557.
196/196 - 37s - loss: 27.7412 - MinusLogProbMetric: 27.7412 - val_loss: 28.8710 - val_MinusLogProbMetric: 28.8710 - lr: 3.1250e-05 - 37s/epoch - 188ms/step
Epoch 657: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 377.
Model trained in 24688.34 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.85 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.16 s.
===========
Run 340/720 done in 24693.82 s.
===========

Directory ../../results/CsplineN_new/run_341/ already exists.
Skipping it.
===========
Run 341/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_342/ already exists.
Skipping it.
===========
Run 342/720 already exists. Skipping it.
===========

===========
Generating train data for run 343.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_49"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_50 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7fe78af73970>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe789bac040>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe789bac040>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe789b8bf10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe789a1da80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe789a1dff0>, <keras.callbacks.ModelCheckpoint object at 0x7fe789a1e0b0>, <keras.callbacks.EarlyStopping object at 0x7fe789a1e320>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe789a1e350>, <keras.callbacks.TerminateOnNaN object at 0x7fe789a1df90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-10-24 14:11:08.151150
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 14:13:37.787 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 150s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 150s/epoch - 763ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 343.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_60"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_61 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7fe878206230>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe7898e3e80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe7898e3e80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe7897b3730>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe7893c3340>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe7893c38b0>, <keras.callbacks.ModelCheckpoint object at 0x7fe7893c3970>, <keras.callbacks.EarlyStopping object at 0x7fe7893c3be0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe7893c3c10>, <keras.callbacks.TerminateOnNaN object at 0x7fe7893c3850>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-10-24 14:13:48.299972
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 14:16:12.032 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 144s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 144s/epoch - 733ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0001111111111111111.
===========
Generating train data for run 343.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_71"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_72 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7fe840141c90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe7e0243a90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe7e0243a90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe7e02168c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fee59f422c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fee59f42830>, <keras.callbacks.ModelCheckpoint object at 0x7fee59f428f0>, <keras.callbacks.EarlyStopping object at 0x7fee59f42b60>, <keras.callbacks.ReduceLROnPlateau object at 0x7fee59f42b90>, <keras.callbacks.TerminateOnNaN object at 0x7fee59f427d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-10-24 14:16:18.768306
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 14:18:47.080 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 148s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 148s/epoch - 757ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 3.703703703703703e-05.
===========
Generating train data for run 343.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_82"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_83 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7fe5d4b610f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe92eb196c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe92eb196c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe5fc4e77f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe5fc421f30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe5fc4224a0>, <keras.callbacks.ModelCheckpoint object at 0x7fe5fc422560>, <keras.callbacks.EarlyStopping object at 0x7fe5fc4227d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe5fc422800>, <keras.callbacks.TerminateOnNaN object at 0x7fe5fc422440>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-10-24 14:18:56.280519
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:5 out of the last 128776 calls to <function Model.make_train_function.<locals>.train_function at 0x7fe792b741f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 14:21:25.180 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 149s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 149s/epoch - 759ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.2345679012345677e-05.
===========
Generating train data for run 343.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_93"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_94 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7fe94c200c70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe5cd0ba7a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe5cd0ba7a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe8f0432ec0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fea206baa10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fea206baf80>, <keras.callbacks.ModelCheckpoint object at 0x7fea206bb040>, <keras.callbacks.EarlyStopping object at 0x7fea206bb2b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fea206bb2e0>, <keras.callbacks.TerminateOnNaN object at 0x7fea206baf20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-10-24 14:21:34.590593
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:6 out of the last 128777 calls to <function Model.make_train_function.<locals>.train_function at 0x7fe5b4dba5f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 14:24:03.540 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 149s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 149s/epoch - 759ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.115226337448558e-06.
===========
Generating train data for run 343.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_104"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_105 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7fe7007538e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fee59d9f670>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fee59d9f670>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe7910f4340>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe789d00f70>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe789d014e0>, <keras.callbacks.ModelCheckpoint object at 0x7fe789d015a0>, <keras.callbacks.EarlyStopping object at 0x7fe789d01810>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe789d01840>, <keras.callbacks.TerminateOnNaN object at 0x7fe789d01480>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-10-24 14:24:13.035859
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 14:26:38.364 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 145s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 145s/epoch - 741ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.3717421124828526e-06.
===========
Generating train data for run 343.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_115"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_116 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7fe9fc7195d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe9fc331f90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe9fc331f90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe8f048dc30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fea20377eb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe730308460>, <keras.callbacks.ModelCheckpoint object at 0x7fe730308520>, <keras.callbacks.EarlyStopping object at 0x7fe730308790>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe7303087c0>, <keras.callbacks.TerminateOnNaN object at 0x7fe730308400>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-10-24 14:26:47.620966
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 14:29:17.029 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 149s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 149s/epoch - 762ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.572473708276175e-07.
===========
Generating train data for run 343.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_126"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_127 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7fe5b53267a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe5cc9642e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe5cc9642e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe5d5e90d30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe5b4c18f40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe5b4c194b0>, <keras.callbacks.ModelCheckpoint object at 0x7fe5b4c19570>, <keras.callbacks.EarlyStopping object at 0x7fe5b4c197e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe5b4c19810>, <keras.callbacks.TerminateOnNaN object at 0x7fe5b4c19450>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-10-24 14:29:26.846454
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 14:31:54.140 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 147s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 147s/epoch - 751ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.524157902758725e-07.
===========
Generating train data for run 343.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_137"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_138 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7fe7783c5c00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe61f1838b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe61f1838b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe78056ada0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe7783719c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe778371f30>, <keras.callbacks.ModelCheckpoint object at 0x7fe778371ff0>, <keras.callbacks.EarlyStopping object at 0x7fe778372260>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe778372290>, <keras.callbacks.TerminateOnNaN object at 0x7fe778371ed0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-10-24 14:32:03.986244
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 14:34:34.276 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 150s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 150s/epoch - 766ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.0805263425290834e-08.
===========
Generating train data for run 343.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_148"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_149 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7fe5a47bf220>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe5d561e5c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe5d561e5c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe61f303130>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe5a47e26e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe5a47e2c50>, <keras.callbacks.ModelCheckpoint object at 0x7fe5a47e2d10>, <keras.callbacks.EarlyStopping object at 0x7fe5a47e2f80>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe5a47e2fb0>, <keras.callbacks.TerminateOnNaN object at 0x7fe5a47e2bf0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-10-24 14:34:42.174615
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 14:37:13.961 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 152s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 152s/epoch - 774ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.6935087808430278e-08.
===========
Generating train data for run 343.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_159"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_160 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7fe5ad24ea70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe77838e5f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe77838e5f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe5b4627340>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe5b46946a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe5b4694c10>, <keras.callbacks.ModelCheckpoint object at 0x7fe5b4694cd0>, <keras.callbacks.EarlyStopping object at 0x7fe5b4694f40>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe5b4694f70>, <keras.callbacks.TerminateOnNaN object at 0x7fe5b4694bb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-10-24 14:37:22.383190
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 14:39:53.649 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 151s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 151s/epoch - 772ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.645029269476759e-09.
===========
Run 343/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 344.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_344
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_170"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_171 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7fe5cc226380>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe5991b81f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe5991b81f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe5cd9f3c40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe5a4862980>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe5a4862ef0>, <keras.callbacks.ModelCheckpoint object at 0x7fe5a4862fb0>, <keras.callbacks.EarlyStopping object at 0x7fe5a4863220>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe5a4863250>, <keras.callbacks.TerminateOnNaN object at 0x7fe5a4862e90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_344/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 344/720 with hyperparameters:
timestamp = 2023-10-24 14:40:04.641466
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 10: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 14:42:35.906 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6196.6948, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 151s - loss: nan - MinusLogProbMetric: 6196.6948 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 151s/epoch - 771ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 344.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_344
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_181"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_182 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7fe783593250>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe50c381ba0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe50c381ba0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe7830cada0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe782f59ba0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe782f5a110>, <keras.callbacks.ModelCheckpoint object at 0x7fe782f5a1d0>, <keras.callbacks.EarlyStopping object at 0x7fe782f5a440>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe782f5a470>, <keras.callbacks.TerminateOnNaN object at 0x7fe782f5a0b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_344/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 344/720 with hyperparameters:
timestamp = 2023-10-24 14:42:44.236767
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 44: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 14:45:26.406 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5114.8276, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 162s - loss: nan - MinusLogProbMetric: 5114.8276 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 162s/epoch - 827ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0001111111111111111.
===========
Generating train data for run 344.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_344
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_192"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_193 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7fe7825eace0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe7824abc70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe7824abc70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe54ca49e70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe781e96320>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe781e96890>, <keras.callbacks.ModelCheckpoint object at 0x7fe781e96950>, <keras.callbacks.EarlyStopping object at 0x7fe781e96bc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe781e96bf0>, <keras.callbacks.TerminateOnNaN object at 0x7fe781e96830>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_344/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 344/720 with hyperparameters:
timestamp = 2023-10-24 14:45:36.697648
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
2023-10-24 14:49:12.223 
Epoch 1/1000 
	 loss: 3605.4502, MinusLogProbMetric: 3605.4502, val_loss: 1772.3497, val_MinusLogProbMetric: 1772.3497

Epoch 1: val_loss improved from inf to 1772.34973, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 217s - loss: 3605.4502 - MinusLogProbMetric: 3605.4502 - val_loss: 1772.3497 - val_MinusLogProbMetric: 1772.3497 - lr: 1.1111e-04 - 217s/epoch - 1s/step
Epoch 2/1000
2023-10-24 14:50:22.839 
Epoch 2/1000 
	 loss: 1298.1680, MinusLogProbMetric: 1298.1680, val_loss: 1071.9395, val_MinusLogProbMetric: 1071.9395

Epoch 2: val_loss improved from 1772.34973 to 1071.93945, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 1298.1680 - MinusLogProbMetric: 1298.1680 - val_loss: 1071.9395 - val_MinusLogProbMetric: 1071.9395 - lr: 1.1111e-04 - 70s/epoch - 357ms/step
Epoch 3/1000
2023-10-24 14:51:30.176 
Epoch 3/1000 
	 loss: 1124.8073, MinusLogProbMetric: 1124.8073, val_loss: 1065.0955, val_MinusLogProbMetric: 1065.0955

Epoch 3: val_loss improved from 1071.93945 to 1065.09546, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 67s - loss: 1124.8073 - MinusLogProbMetric: 1124.8073 - val_loss: 1065.0955 - val_MinusLogProbMetric: 1065.0955 - lr: 1.1111e-04 - 67s/epoch - 344ms/step
Epoch 4/1000
2023-10-24 14:52:38.975 
Epoch 4/1000 
	 loss: 895.2070, MinusLogProbMetric: 895.2070, val_loss: 790.0752, val_MinusLogProbMetric: 790.0752

Epoch 4: val_loss improved from 1065.09546 to 790.07520, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 895.2070 - MinusLogProbMetric: 895.2070 - val_loss: 790.0752 - val_MinusLogProbMetric: 790.0752 - lr: 1.1111e-04 - 69s/epoch - 352ms/step
Epoch 5/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 126: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 14:53:26.891 
Epoch 5/1000 
	 loss: nan, MinusLogProbMetric: 780.4032, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 5: val_loss did not improve from 790.07520
196/196 - 47s - loss: nan - MinusLogProbMetric: 780.4032 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 47s/epoch - 239ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 3.703703703703703e-05.
===========
Generating train data for run 344.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_344
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_203"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_204 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7fe58093f850>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe5809f8cd0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe5809f8cd0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe580c88a60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe58091e860>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe58091edd0>, <keras.callbacks.ModelCheckpoint object at 0x7fe58091ee90>, <keras.callbacks.EarlyStopping object at 0x7fe58091f100>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe58091f130>, <keras.callbacks.TerminateOnNaN object at 0x7fe58091ed70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 344/720 with hyperparameters:
timestamp = 2023-10-24 14:53:37.544975
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
2023-10-24 14:57:07.538 
Epoch 1/1000 
	 loss: 939.6808, MinusLogProbMetric: 939.6808, val_loss: 841.3206, val_MinusLogProbMetric: 841.3206

Epoch 1: val_loss improved from inf to 841.32062, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 211s - loss: 939.6808 - MinusLogProbMetric: 939.6808 - val_loss: 841.3206 - val_MinusLogProbMetric: 841.3206 - lr: 3.7037e-05 - 211s/epoch - 1s/step
Epoch 2/1000
2023-10-24 14:58:17.099 
Epoch 2/1000 
	 loss: 828.0804, MinusLogProbMetric: 828.0804, val_loss: 830.9725, val_MinusLogProbMetric: 830.9725

Epoch 2: val_loss improved from 841.32062 to 830.97253, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 828.0804 - MinusLogProbMetric: 828.0804 - val_loss: 830.9725 - val_MinusLogProbMetric: 830.9725 - lr: 3.7037e-05 - 69s/epoch - 354ms/step
Epoch 3/1000
2023-10-24 14:59:26.746 
Epoch 3/1000 
	 loss: 680.6164, MinusLogProbMetric: 680.6164, val_loss: 696.6257, val_MinusLogProbMetric: 696.6257

Epoch 3: val_loss improved from 830.97253 to 696.62567, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 680.6164 - MinusLogProbMetric: 680.6164 - val_loss: 696.6257 - val_MinusLogProbMetric: 696.6257 - lr: 3.7037e-05 - 69s/epoch - 354ms/step
Epoch 4/1000
2023-10-24 15:00:33.792 
Epoch 4/1000 
	 loss: 640.0810, MinusLogProbMetric: 640.0810, val_loss: 597.4720, val_MinusLogProbMetric: 597.4720

Epoch 4: val_loss improved from 696.62567 to 597.47198, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 67s - loss: 640.0810 - MinusLogProbMetric: 640.0810 - val_loss: 597.4720 - val_MinusLogProbMetric: 597.4720 - lr: 3.7037e-05 - 67s/epoch - 342ms/step
Epoch 5/1000
2023-10-24 15:01:40.178 
Epoch 5/1000 
	 loss: 592.5281, MinusLogProbMetric: 592.5281, val_loss: 556.9847, val_MinusLogProbMetric: 556.9847

Epoch 5: val_loss improved from 597.47198 to 556.98468, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 67s - loss: 592.5281 - MinusLogProbMetric: 592.5281 - val_loss: 556.9847 - val_MinusLogProbMetric: 556.9847 - lr: 3.7037e-05 - 67s/epoch - 340ms/step
Epoch 6/1000
2023-10-24 15:02:51.162 
Epoch 6/1000 
	 loss: 525.2245, MinusLogProbMetric: 525.2245, val_loss: 482.3519, val_MinusLogProbMetric: 482.3519

Epoch 6: val_loss improved from 556.98468 to 482.35190, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 525.2245 - MinusLogProbMetric: 525.2245 - val_loss: 482.3519 - val_MinusLogProbMetric: 482.3519 - lr: 3.7037e-05 - 71s/epoch - 362ms/step
Epoch 7/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 114: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 15:03:37.104 
Epoch 7/1000 
	 loss: nan, MinusLogProbMetric: 618.1214, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 7: val_loss did not improve from 482.35190
196/196 - 45s - loss: nan - MinusLogProbMetric: 618.1214 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 45s/epoch - 228ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.2345679012345677e-05.
===========
Generating train data for run 344.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_344
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_214"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_215 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7fe581cca350>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe4ef68d000>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe4ef68d000>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe58025ab60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe78152a560>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe78152aad0>, <keras.callbacks.ModelCheckpoint object at 0x7fe78152ab90>, <keras.callbacks.EarlyStopping object at 0x7fe78152ae00>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe78152ae30>, <keras.callbacks.TerminateOnNaN object at 0x7fe78152aa70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 344/720 with hyperparameters:
timestamp = 2023-10-24 15:03:46.584026
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
2023-10-24 15:07:22.381 
Epoch 1/1000 
	 loss: 514.4644, MinusLogProbMetric: 514.4644, val_loss: 447.3737, val_MinusLogProbMetric: 447.3737

Epoch 1: val_loss improved from inf to 447.37372, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 217s - loss: 514.4644 - MinusLogProbMetric: 514.4644 - val_loss: 447.3737 - val_MinusLogProbMetric: 447.3737 - lr: 1.2346e-05 - 217s/epoch - 1s/step
Epoch 2/1000
2023-10-24 15:08:33.263 
Epoch 2/1000 
	 loss: 409.1862, MinusLogProbMetric: 409.1862, val_loss: 406.4526, val_MinusLogProbMetric: 406.4526

Epoch 2: val_loss improved from 447.37372 to 406.45264, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 409.1862 - MinusLogProbMetric: 409.1862 - val_loss: 406.4526 - val_MinusLogProbMetric: 406.4526 - lr: 1.2346e-05 - 71s/epoch - 360ms/step
Epoch 3/1000
2023-10-24 15:09:42.418 
Epoch 3/1000 
	 loss: 400.7114, MinusLogProbMetric: 400.7114, val_loss: 371.1191, val_MinusLogProbMetric: 371.1191

Epoch 3: val_loss improved from 406.45264 to 371.11908, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 400.7114 - MinusLogProbMetric: 400.7114 - val_loss: 371.1191 - val_MinusLogProbMetric: 371.1191 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 4/1000
2023-10-24 15:10:51.389 
Epoch 4/1000 
	 loss: 424.2187, MinusLogProbMetric: 424.2187, val_loss: 367.7484, val_MinusLogProbMetric: 367.7484

Epoch 4: val_loss improved from 371.11908 to 367.74835, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 424.2187 - MinusLogProbMetric: 424.2187 - val_loss: 367.7484 - val_MinusLogProbMetric: 367.7484 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 5/1000
2023-10-24 15:12:00.324 
Epoch 5/1000 
	 loss: 345.0695, MinusLogProbMetric: 345.0695, val_loss: 343.9267, val_MinusLogProbMetric: 343.9267

Epoch 5: val_loss improved from 367.74835 to 343.92667, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 345.0695 - MinusLogProbMetric: 345.0695 - val_loss: 343.9267 - val_MinusLogProbMetric: 343.9267 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 6/1000
2023-10-24 15:13:09.168 
Epoch 6/1000 
	 loss: 343.9660, MinusLogProbMetric: 343.9660, val_loss: 346.2855, val_MinusLogProbMetric: 346.2855

Epoch 6: val_loss did not improve from 343.92667
196/196 - 68s - loss: 343.9660 - MinusLogProbMetric: 343.9660 - val_loss: 346.2855 - val_MinusLogProbMetric: 346.2855 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 7/1000
2023-10-24 15:14:17.438 
Epoch 7/1000 
	 loss: 315.3464, MinusLogProbMetric: 315.3464, val_loss: 296.3126, val_MinusLogProbMetric: 296.3126

Epoch 7: val_loss improved from 343.92667 to 296.31256, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 315.3464 - MinusLogProbMetric: 315.3464 - val_loss: 296.3126 - val_MinusLogProbMetric: 296.3126 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 8/1000
2023-10-24 15:15:26.167 
Epoch 8/1000 
	 loss: 288.4757, MinusLogProbMetric: 288.4757, val_loss: 277.5059, val_MinusLogProbMetric: 277.5059

Epoch 8: val_loss improved from 296.31256 to 277.50592, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 288.4757 - MinusLogProbMetric: 288.4757 - val_loss: 277.5059 - val_MinusLogProbMetric: 277.5059 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 9/1000
2023-10-24 15:16:36.224 
Epoch 9/1000 
	 loss: 272.6632, MinusLogProbMetric: 272.6632, val_loss: 285.7739, val_MinusLogProbMetric: 285.7739

Epoch 9: val_loss did not improve from 277.50592
196/196 - 69s - loss: 272.6632 - MinusLogProbMetric: 272.6632 - val_loss: 285.7739 - val_MinusLogProbMetric: 285.7739 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 10/1000
2023-10-24 15:17:42.776 
Epoch 10/1000 
	 loss: 267.0374, MinusLogProbMetric: 267.0374, val_loss: 259.8369, val_MinusLogProbMetric: 259.8369

Epoch 10: val_loss improved from 277.50592 to 259.83694, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 68s - loss: 267.0374 - MinusLogProbMetric: 267.0374 - val_loss: 259.8369 - val_MinusLogProbMetric: 259.8369 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 11/1000
2023-10-24 15:18:51.560 
Epoch 11/1000 
	 loss: 376.9129, MinusLogProbMetric: 376.9129, val_loss: 316.2042, val_MinusLogProbMetric: 316.2042

Epoch 11: val_loss did not improve from 259.83694
196/196 - 68s - loss: 376.9129 - MinusLogProbMetric: 376.9129 - val_loss: 316.2042 - val_MinusLogProbMetric: 316.2042 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 12/1000
2023-10-24 15:19:58.383 
Epoch 12/1000 
	 loss: 302.6491, MinusLogProbMetric: 302.6491, val_loss: 285.3486, val_MinusLogProbMetric: 285.3486

Epoch 12: val_loss did not improve from 259.83694
196/196 - 67s - loss: 302.6491 - MinusLogProbMetric: 302.6491 - val_loss: 285.3486 - val_MinusLogProbMetric: 285.3486 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 13/1000
2023-10-24 15:21:06.606 
Epoch 13/1000 
	 loss: 272.2867, MinusLogProbMetric: 272.2867, val_loss: 255.9995, val_MinusLogProbMetric: 255.9995

Epoch 13: val_loss improved from 259.83694 to 255.99947, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 272.2867 - MinusLogProbMetric: 272.2867 - val_loss: 255.9995 - val_MinusLogProbMetric: 255.9995 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 14/1000
2023-10-24 15:22:15.493 
Epoch 14/1000 
	 loss: 252.5173, MinusLogProbMetric: 252.5173, val_loss: 248.9197, val_MinusLogProbMetric: 248.9197

Epoch 14: val_loss improved from 255.99947 to 248.91972, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 252.5173 - MinusLogProbMetric: 252.5173 - val_loss: 248.9197 - val_MinusLogProbMetric: 248.9197 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 15/1000
2023-10-24 15:23:23.933 
Epoch 15/1000 
	 loss: 262.2612, MinusLogProbMetric: 262.2612, val_loss: 279.9177, val_MinusLogProbMetric: 279.9177

Epoch 15: val_loss did not improve from 248.91972
196/196 - 67s - loss: 262.2612 - MinusLogProbMetric: 262.2612 - val_loss: 279.9177 - val_MinusLogProbMetric: 279.9177 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 16/1000
2023-10-24 15:24:29.134 
Epoch 16/1000 
	 loss: 247.6165, MinusLogProbMetric: 247.6165, val_loss: 231.3365, val_MinusLogProbMetric: 231.3365

Epoch 16: val_loss improved from 248.91972 to 231.33653, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 66s - loss: 247.6165 - MinusLogProbMetric: 247.6165 - val_loss: 231.3365 - val_MinusLogProbMetric: 231.3365 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 17/1000
2023-10-24 15:25:37.030 
Epoch 17/1000 
	 loss: 224.5759, MinusLogProbMetric: 224.5759, val_loss: 217.7265, val_MinusLogProbMetric: 217.7265

Epoch 17: val_loss improved from 231.33653 to 217.72655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 68s - loss: 224.5759 - MinusLogProbMetric: 224.5759 - val_loss: 217.7265 - val_MinusLogProbMetric: 217.7265 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 18/1000
2023-10-24 15:26:48.104 
Epoch 18/1000 
	 loss: 213.8699, MinusLogProbMetric: 213.8699, val_loss: 210.2504, val_MinusLogProbMetric: 210.2504

Epoch 18: val_loss improved from 217.72655 to 210.25038, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 213.8699 - MinusLogProbMetric: 213.8699 - val_loss: 210.2504 - val_MinusLogProbMetric: 210.2504 - lr: 1.2346e-05 - 71s/epoch - 362ms/step
Epoch 19/1000
2023-10-24 15:27:58.039 
Epoch 19/1000 
	 loss: 221.8959, MinusLogProbMetric: 221.8959, val_loss: 214.1211, val_MinusLogProbMetric: 214.1211

Epoch 19: val_loss did not improve from 210.25038
196/196 - 69s - loss: 221.8959 - MinusLogProbMetric: 221.8959 - val_loss: 214.1211 - val_MinusLogProbMetric: 214.1211 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 20/1000
2023-10-24 15:29:04.804 
Epoch 20/1000 
	 loss: 232.5576, MinusLogProbMetric: 232.5576, val_loss: 209.0721, val_MinusLogProbMetric: 209.0721

Epoch 20: val_loss improved from 210.25038 to 209.07205, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 68s - loss: 232.5576 - MinusLogProbMetric: 232.5576 - val_loss: 209.0721 - val_MinusLogProbMetric: 209.0721 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 21/1000
2023-10-24 15:30:11.989 
Epoch 21/1000 
	 loss: 201.6256, MinusLogProbMetric: 201.6256, val_loss: 195.2683, val_MinusLogProbMetric: 195.2683

Epoch 21: val_loss improved from 209.07205 to 195.26830, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 67s - loss: 201.6256 - MinusLogProbMetric: 201.6256 - val_loss: 195.2683 - val_MinusLogProbMetric: 195.2683 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 22/1000
2023-10-24 15:31:18.175 
Epoch 22/1000 
	 loss: 196.7751, MinusLogProbMetric: 196.7751, val_loss: 190.9296, val_MinusLogProbMetric: 190.9296

Epoch 22: val_loss improved from 195.26830 to 190.92957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 66s - loss: 196.7751 - MinusLogProbMetric: 196.7751 - val_loss: 190.9296 - val_MinusLogProbMetric: 190.9296 - lr: 1.2346e-05 - 66s/epoch - 337ms/step
Epoch 23/1000
2023-10-24 15:32:24.179 
Epoch 23/1000 
	 loss: 189.1623, MinusLogProbMetric: 189.1623, val_loss: 184.5191, val_MinusLogProbMetric: 184.5191

Epoch 23: val_loss improved from 190.92957 to 184.51910, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 66s - loss: 189.1623 - MinusLogProbMetric: 189.1623 - val_loss: 184.5191 - val_MinusLogProbMetric: 184.5191 - lr: 1.2346e-05 - 66s/epoch - 337ms/step
Epoch 24/1000
2023-10-24 15:33:32.417 
Epoch 24/1000 
	 loss: 428.0455, MinusLogProbMetric: 428.0455, val_loss: 397.1902, val_MinusLogProbMetric: 397.1902

Epoch 24: val_loss did not improve from 184.51910
196/196 - 67s - loss: 428.0455 - MinusLogProbMetric: 428.0455 - val_loss: 397.1902 - val_MinusLogProbMetric: 397.1902 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 25/1000
2023-10-24 15:34:39.159 
Epoch 25/1000 
	 loss: 351.6341, MinusLogProbMetric: 351.6341, val_loss: 313.5726, val_MinusLogProbMetric: 313.5726

Epoch 25: val_loss did not improve from 184.51910
196/196 - 67s - loss: 351.6341 - MinusLogProbMetric: 351.6341 - val_loss: 313.5726 - val_MinusLogProbMetric: 313.5726 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 26/1000
2023-10-24 15:35:48.059 
Epoch 26/1000 
	 loss: 294.6577, MinusLogProbMetric: 294.6577, val_loss: 278.9883, val_MinusLogProbMetric: 278.9883

Epoch 26: val_loss did not improve from 184.51910
196/196 - 69s - loss: 294.6577 - MinusLogProbMetric: 294.6577 - val_loss: 278.9883 - val_MinusLogProbMetric: 278.9883 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 27/1000
2023-10-24 15:36:55.334 
Epoch 27/1000 
	 loss: 267.0570, MinusLogProbMetric: 267.0570, val_loss: 256.7292, val_MinusLogProbMetric: 256.7292

Epoch 27: val_loss did not improve from 184.51910
196/196 - 67s - loss: 267.0570 - MinusLogProbMetric: 267.0570 - val_loss: 256.7292 - val_MinusLogProbMetric: 256.7292 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 28/1000
2023-10-24 15:38:03.947 
Epoch 28/1000 
	 loss: 250.2841, MinusLogProbMetric: 250.2841, val_loss: 242.6456, val_MinusLogProbMetric: 242.6456

Epoch 28: val_loss did not improve from 184.51910
196/196 - 69s - loss: 250.2841 - MinusLogProbMetric: 250.2841 - val_loss: 242.6456 - val_MinusLogProbMetric: 242.6456 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 29/1000
2023-10-24 15:39:12.235 
Epoch 29/1000 
	 loss: 238.9114, MinusLogProbMetric: 238.9114, val_loss: 256.2186, val_MinusLogProbMetric: 256.2186

Epoch 29: val_loss did not improve from 184.51910
196/196 - 68s - loss: 238.9114 - MinusLogProbMetric: 238.9114 - val_loss: 256.2186 - val_MinusLogProbMetric: 256.2186 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 30/1000
2023-10-24 15:40:20.361 
Epoch 30/1000 
	 loss: 231.5811, MinusLogProbMetric: 231.5811, val_loss: 223.6662, val_MinusLogProbMetric: 223.6662

Epoch 30: val_loss did not improve from 184.51910
196/196 - 68s - loss: 231.5811 - MinusLogProbMetric: 231.5811 - val_loss: 223.6662 - val_MinusLogProbMetric: 223.6662 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 31/1000
2023-10-24 15:41:28.895 
Epoch 31/1000 
	 loss: 218.9174, MinusLogProbMetric: 218.9174, val_loss: 212.8321, val_MinusLogProbMetric: 212.8321

Epoch 31: val_loss did not improve from 184.51910
196/196 - 69s - loss: 218.9174 - MinusLogProbMetric: 218.9174 - val_loss: 212.8321 - val_MinusLogProbMetric: 212.8321 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 32/1000
2023-10-24 15:42:34.776 
Epoch 32/1000 
	 loss: 208.6725, MinusLogProbMetric: 208.6725, val_loss: 206.3636, val_MinusLogProbMetric: 206.3636

Epoch 32: val_loss did not improve from 184.51910
196/196 - 66s - loss: 208.6725 - MinusLogProbMetric: 208.6725 - val_loss: 206.3636 - val_MinusLogProbMetric: 206.3636 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 33/1000
2023-10-24 15:43:43.806 
Epoch 33/1000 
	 loss: 202.2003, MinusLogProbMetric: 202.2003, val_loss: 199.9843, val_MinusLogProbMetric: 199.9843

Epoch 33: val_loss did not improve from 184.51910
196/196 - 69s - loss: 202.2003 - MinusLogProbMetric: 202.2003 - val_loss: 199.9843 - val_MinusLogProbMetric: 199.9843 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 34/1000
2023-10-24 15:44:52.533 
Epoch 34/1000 
	 loss: 196.9868, MinusLogProbMetric: 196.9868, val_loss: 194.4172, val_MinusLogProbMetric: 194.4172

Epoch 34: val_loss did not improve from 184.51910
196/196 - 69s - loss: 196.9868 - MinusLogProbMetric: 196.9868 - val_loss: 194.4172 - val_MinusLogProbMetric: 194.4172 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 35/1000
2023-10-24 15:46:04.228 
Epoch 35/1000 
	 loss: 192.8034, MinusLogProbMetric: 192.8034, val_loss: 192.9374, val_MinusLogProbMetric: 192.9374

Epoch 35: val_loss did not improve from 184.51910
196/196 - 72s - loss: 192.8034 - MinusLogProbMetric: 192.8034 - val_loss: 192.9374 - val_MinusLogProbMetric: 192.9374 - lr: 1.2346e-05 - 72s/epoch - 366ms/step
Epoch 36/1000
2023-10-24 15:47:13.881 
Epoch 36/1000 
	 loss: 189.9143, MinusLogProbMetric: 189.9143, val_loss: 185.9949, val_MinusLogProbMetric: 185.9949

Epoch 36: val_loss did not improve from 184.51910
196/196 - 70s - loss: 189.9143 - MinusLogProbMetric: 189.9143 - val_loss: 185.9949 - val_MinusLogProbMetric: 185.9949 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 37/1000
2023-10-24 15:48:21.032 
Epoch 37/1000 
	 loss: 184.4954, MinusLogProbMetric: 184.4954, val_loss: 185.4642, val_MinusLogProbMetric: 185.4642

Epoch 37: val_loss did not improve from 184.51910
196/196 - 67s - loss: 184.4954 - MinusLogProbMetric: 184.4954 - val_loss: 185.4642 - val_MinusLogProbMetric: 185.4642 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 38/1000
2023-10-24 15:49:31.894 
Epoch 38/1000 
	 loss: 181.1067, MinusLogProbMetric: 181.1067, val_loss: 179.2854, val_MinusLogProbMetric: 179.2854

Epoch 38: val_loss improved from 184.51910 to 179.28535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 72s - loss: 181.1067 - MinusLogProbMetric: 181.1067 - val_loss: 179.2854 - val_MinusLogProbMetric: 179.2854 - lr: 1.2346e-05 - 72s/epoch - 367ms/step
Epoch 39/1000
2023-10-24 15:50:40.437 
Epoch 39/1000 
	 loss: 178.8188, MinusLogProbMetric: 178.8188, val_loss: 176.7195, val_MinusLogProbMetric: 176.7195

Epoch 39: val_loss improved from 179.28535 to 176.71948, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 178.8188 - MinusLogProbMetric: 178.8188 - val_loss: 176.7195 - val_MinusLogProbMetric: 176.7195 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 40/1000
2023-10-24 15:51:50.432 
Epoch 40/1000 
	 loss: 174.7311, MinusLogProbMetric: 174.7311, val_loss: 174.1419, val_MinusLogProbMetric: 174.1419

Epoch 40: val_loss improved from 176.71948 to 174.14188, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 174.7311 - MinusLogProbMetric: 174.7311 - val_loss: 174.1419 - val_MinusLogProbMetric: 174.1419 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 41/1000
2023-10-24 15:53:00.204 
Epoch 41/1000 
	 loss: 171.8023, MinusLogProbMetric: 171.8023, val_loss: 170.4869, val_MinusLogProbMetric: 170.4869

Epoch 41: val_loss improved from 174.14188 to 170.48694, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 171.8023 - MinusLogProbMetric: 171.8023 - val_loss: 170.4869 - val_MinusLogProbMetric: 170.4869 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 42/1000
2023-10-24 15:54:07.900 
Epoch 42/1000 
	 loss: 169.0448, MinusLogProbMetric: 169.0448, val_loss: 167.4891, val_MinusLogProbMetric: 167.4891

Epoch 42: val_loss improved from 170.48694 to 167.48909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 68s - loss: 169.0448 - MinusLogProbMetric: 169.0448 - val_loss: 167.4891 - val_MinusLogProbMetric: 167.4891 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 43/1000
2023-10-24 15:55:17.725 
Epoch 43/1000 
	 loss: 165.4482, MinusLogProbMetric: 165.4482, val_loss: 164.2298, val_MinusLogProbMetric: 164.2298

Epoch 43: val_loss improved from 167.48909 to 164.22975, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 165.4482 - MinusLogProbMetric: 165.4482 - val_loss: 164.2298 - val_MinusLogProbMetric: 164.2298 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 44/1000
2023-10-24 15:56:29.714 
Epoch 44/1000 
	 loss: 163.1523, MinusLogProbMetric: 163.1523, val_loss: 161.3720, val_MinusLogProbMetric: 161.3720

Epoch 44: val_loss improved from 164.22975 to 161.37196, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 72s - loss: 163.1523 - MinusLogProbMetric: 163.1523 - val_loss: 161.3720 - val_MinusLogProbMetric: 161.3720 - lr: 1.2346e-05 - 72s/epoch - 368ms/step
Epoch 45/1000
2023-10-24 15:57:40.125 
Epoch 45/1000 
	 loss: 162.4675, MinusLogProbMetric: 162.4675, val_loss: 162.9453, val_MinusLogProbMetric: 162.9453

Epoch 45: val_loss did not improve from 161.37196
196/196 - 69s - loss: 162.4675 - MinusLogProbMetric: 162.4675 - val_loss: 162.9453 - val_MinusLogProbMetric: 162.9453 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 46/1000
2023-10-24 15:58:49.434 
Epoch 46/1000 
	 loss: 204.1891, MinusLogProbMetric: 204.1891, val_loss: 216.9925, val_MinusLogProbMetric: 216.9925

Epoch 46: val_loss did not improve from 161.37196
196/196 - 69s - loss: 204.1891 - MinusLogProbMetric: 204.1891 - val_loss: 216.9925 - val_MinusLogProbMetric: 216.9925 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 47/1000
2023-10-24 15:59:59.440 
Epoch 47/1000 
	 loss: 258.2715, MinusLogProbMetric: 258.2715, val_loss: 211.7140, val_MinusLogProbMetric: 211.7140

Epoch 47: val_loss did not improve from 161.37196
196/196 - 70s - loss: 258.2715 - MinusLogProbMetric: 258.2715 - val_loss: 211.7140 - val_MinusLogProbMetric: 211.7140 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 48/1000
2023-10-24 16:01:06.729 
Epoch 48/1000 
	 loss: 209.6270, MinusLogProbMetric: 209.6270, val_loss: 198.1944, val_MinusLogProbMetric: 198.1944

Epoch 48: val_loss did not improve from 161.37196
196/196 - 67s - loss: 209.6270 - MinusLogProbMetric: 209.6270 - val_loss: 198.1944 - val_MinusLogProbMetric: 198.1944 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 49/1000
2023-10-24 16:02:13.960 
Epoch 49/1000 
	 loss: 190.4018, MinusLogProbMetric: 190.4018, val_loss: 184.6106, val_MinusLogProbMetric: 184.6106

Epoch 49: val_loss did not improve from 161.37196
196/196 - 67s - loss: 190.4018 - MinusLogProbMetric: 190.4018 - val_loss: 184.6106 - val_MinusLogProbMetric: 184.6106 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 50/1000
2023-10-24 16:03:22.510 
Epoch 50/1000 
	 loss: 181.5638, MinusLogProbMetric: 181.5638, val_loss: 178.0604, val_MinusLogProbMetric: 178.0604

Epoch 50: val_loss did not improve from 161.37196
196/196 - 69s - loss: 181.5638 - MinusLogProbMetric: 181.5638 - val_loss: 178.0604 - val_MinusLogProbMetric: 178.0604 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 51/1000
2023-10-24 16:04:31.067 
Epoch 51/1000 
	 loss: 175.6435, MinusLogProbMetric: 175.6435, val_loss: 173.4791, val_MinusLogProbMetric: 173.4791

Epoch 51: val_loss did not improve from 161.37196
196/196 - 69s - loss: 175.6435 - MinusLogProbMetric: 175.6435 - val_loss: 173.4791 - val_MinusLogProbMetric: 173.4791 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 52/1000
2023-10-24 16:05:44.079 
Epoch 52/1000 
	 loss: 171.1005, MinusLogProbMetric: 171.1005, val_loss: 174.7220, val_MinusLogProbMetric: 174.7220

Epoch 52: val_loss did not improve from 161.37196
196/196 - 73s - loss: 171.1005 - MinusLogProbMetric: 171.1005 - val_loss: 174.7220 - val_MinusLogProbMetric: 174.7220 - lr: 1.2346e-05 - 73s/epoch - 372ms/step
Epoch 53/1000
2023-10-24 16:06:54.445 
Epoch 53/1000 
	 loss: 168.0163, MinusLogProbMetric: 168.0163, val_loss: 166.2579, val_MinusLogProbMetric: 166.2579

Epoch 53: val_loss did not improve from 161.37196
196/196 - 70s - loss: 168.0163 - MinusLogProbMetric: 168.0163 - val_loss: 166.2579 - val_MinusLogProbMetric: 166.2579 - lr: 1.2346e-05 - 70s/epoch - 359ms/step
Epoch 54/1000
2023-10-24 16:08:04.107 
Epoch 54/1000 
	 loss: 164.2362, MinusLogProbMetric: 164.2362, val_loss: 162.8294, val_MinusLogProbMetric: 162.8294

Epoch 54: val_loss did not improve from 161.37196
196/196 - 70s - loss: 164.2362 - MinusLogProbMetric: 164.2362 - val_loss: 162.8294 - val_MinusLogProbMetric: 162.8294 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 55/1000
2023-10-24 16:09:12.547 
Epoch 55/1000 
	 loss: 161.6640, MinusLogProbMetric: 161.6640, val_loss: 159.8540, val_MinusLogProbMetric: 159.8540

Epoch 55: val_loss improved from 161.37196 to 159.85405, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 161.6640 - MinusLogProbMetric: 161.6640 - val_loss: 159.8540 - val_MinusLogProbMetric: 159.8540 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 56/1000
2023-10-24 16:10:21.519 
Epoch 56/1000 
	 loss: 158.8118, MinusLogProbMetric: 158.8118, val_loss: 157.6092, val_MinusLogProbMetric: 157.6092

Epoch 56: val_loss improved from 159.85405 to 157.60924, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 158.8118 - MinusLogProbMetric: 158.8118 - val_loss: 157.6092 - val_MinusLogProbMetric: 157.6092 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 57/1000
2023-10-24 16:11:29.883 
Epoch 57/1000 
	 loss: 156.1692, MinusLogProbMetric: 156.1692, val_loss: 154.3472, val_MinusLogProbMetric: 154.3472

Epoch 57: val_loss improved from 157.60924 to 154.34721, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 68s - loss: 156.1692 - MinusLogProbMetric: 156.1692 - val_loss: 154.3472 - val_MinusLogProbMetric: 154.3472 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 58/1000
2023-10-24 16:12:38.663 
Epoch 58/1000 
	 loss: 153.7742, MinusLogProbMetric: 153.7742, val_loss: 154.8199, val_MinusLogProbMetric: 154.8199

Epoch 58: val_loss did not improve from 154.34721
196/196 - 68s - loss: 153.7742 - MinusLogProbMetric: 153.7742 - val_loss: 154.8199 - val_MinusLogProbMetric: 154.8199 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 59/1000
2023-10-24 16:13:46.834 
Epoch 59/1000 
	 loss: 151.3255, MinusLogProbMetric: 151.3255, val_loss: 151.4188, val_MinusLogProbMetric: 151.4188

Epoch 59: val_loss improved from 154.34721 to 151.41882, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 151.3255 - MinusLogProbMetric: 151.3255 - val_loss: 151.4188 - val_MinusLogProbMetric: 151.4188 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 60/1000
2023-10-24 16:14:55.111 
Epoch 60/1000 
	 loss: 148.9108, MinusLogProbMetric: 148.9108, val_loss: 148.1644, val_MinusLogProbMetric: 148.1644

Epoch 60: val_loss improved from 151.41882 to 148.16437, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 68s - loss: 148.9108 - MinusLogProbMetric: 148.9108 - val_loss: 148.1644 - val_MinusLogProbMetric: 148.1644 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 61/1000
2023-10-24 16:16:06.009 
Epoch 61/1000 
	 loss: 148.5560, MinusLogProbMetric: 148.5560, val_loss: 146.6373, val_MinusLogProbMetric: 146.6373

Epoch 61: val_loss improved from 148.16437 to 146.63728, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 148.5560 - MinusLogProbMetric: 148.5560 - val_loss: 146.6373 - val_MinusLogProbMetric: 146.6373 - lr: 1.2346e-05 - 71s/epoch - 362ms/step
Epoch 62/1000
2023-10-24 16:17:14.272 
Epoch 62/1000 
	 loss: 145.1953, MinusLogProbMetric: 145.1953, val_loss: 144.2250, val_MinusLogProbMetric: 144.2250

Epoch 62: val_loss improved from 146.63728 to 144.22504, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 68s - loss: 145.1953 - MinusLogProbMetric: 145.1953 - val_loss: 144.2250 - val_MinusLogProbMetric: 144.2250 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 63/1000
2023-10-24 16:18:22.995 
Epoch 63/1000 
	 loss: 143.5123, MinusLogProbMetric: 143.5123, val_loss: 143.9368, val_MinusLogProbMetric: 143.9368

Epoch 63: val_loss improved from 144.22504 to 143.93683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 143.5123 - MinusLogProbMetric: 143.5123 - val_loss: 143.9368 - val_MinusLogProbMetric: 143.9368 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 64/1000
2023-10-24 16:19:34.123 
Epoch 64/1000 
	 loss: 142.0830, MinusLogProbMetric: 142.0830, val_loss: 141.2346, val_MinusLogProbMetric: 141.2346

Epoch 64: val_loss improved from 143.93683 to 141.23463, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 142.0830 - MinusLogProbMetric: 142.0830 - val_loss: 141.2346 - val_MinusLogProbMetric: 141.2346 - lr: 1.2346e-05 - 71s/epoch - 364ms/step
Epoch 65/1000
2023-10-24 16:20:41.806 
Epoch 65/1000 
	 loss: 140.2990, MinusLogProbMetric: 140.2990, val_loss: 139.8826, val_MinusLogProbMetric: 139.8826

Epoch 65: val_loss improved from 141.23463 to 139.88258, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 68s - loss: 140.2990 - MinusLogProbMetric: 140.2990 - val_loss: 139.8826 - val_MinusLogProbMetric: 139.8826 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 66/1000
2023-10-24 16:21:50.600 
Epoch 66/1000 
	 loss: 139.1272, MinusLogProbMetric: 139.1272, val_loss: 138.0004, val_MinusLogProbMetric: 138.0004

Epoch 66: val_loss improved from 139.88258 to 138.00043, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 139.1272 - MinusLogProbMetric: 139.1272 - val_loss: 138.0004 - val_MinusLogProbMetric: 138.0004 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 67/1000
2023-10-24 16:23:01.021 
Epoch 67/1000 
	 loss: 137.7658, MinusLogProbMetric: 137.7658, val_loss: 136.7536, val_MinusLogProbMetric: 136.7536

Epoch 67: val_loss improved from 138.00043 to 136.75357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 137.7658 - MinusLogProbMetric: 137.7658 - val_loss: 136.7536 - val_MinusLogProbMetric: 136.7536 - lr: 1.2346e-05 - 71s/epoch - 360ms/step
Epoch 68/1000
2023-10-24 16:24:13.295 
Epoch 68/1000 
	 loss: 136.4692, MinusLogProbMetric: 136.4692, val_loss: 135.5399, val_MinusLogProbMetric: 135.5399

Epoch 68: val_loss improved from 136.75357 to 135.53993, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 72s - loss: 136.4692 - MinusLogProbMetric: 136.4692 - val_loss: 135.5399 - val_MinusLogProbMetric: 135.5399 - lr: 1.2346e-05 - 72s/epoch - 368ms/step
Epoch 69/1000
2023-10-24 16:25:21.762 
Epoch 69/1000 
	 loss: 138.5890, MinusLogProbMetric: 138.5890, val_loss: 134.9026, val_MinusLogProbMetric: 134.9026

Epoch 69: val_loss improved from 135.53993 to 134.90256, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 138.5890 - MinusLogProbMetric: 138.5890 - val_loss: 134.9026 - val_MinusLogProbMetric: 134.9026 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 70/1000
2023-10-24 16:26:32.484 
Epoch 70/1000 
	 loss: 134.2485, MinusLogProbMetric: 134.2485, val_loss: 133.5907, val_MinusLogProbMetric: 133.5907

Epoch 70: val_loss improved from 134.90256 to 133.59070, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 134.2485 - MinusLogProbMetric: 134.2485 - val_loss: 133.5907 - val_MinusLogProbMetric: 133.5907 - lr: 1.2346e-05 - 71s/epoch - 361ms/step
Epoch 71/1000
2023-10-24 16:27:43.171 
Epoch 71/1000 
	 loss: 132.8260, MinusLogProbMetric: 132.8260, val_loss: 133.7671, val_MinusLogProbMetric: 133.7671

Epoch 71: val_loss did not improve from 133.59070
196/196 - 69s - loss: 132.8260 - MinusLogProbMetric: 132.8260 - val_loss: 133.7671 - val_MinusLogProbMetric: 133.7671 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 72/1000
2023-10-24 16:28:50.858 
Epoch 72/1000 
	 loss: 131.3841, MinusLogProbMetric: 131.3841, val_loss: 131.5157, val_MinusLogProbMetric: 131.5157

Epoch 72: val_loss improved from 133.59070 to 131.51572, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 131.3841 - MinusLogProbMetric: 131.3841 - val_loss: 131.5157 - val_MinusLogProbMetric: 131.5157 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 73/1000
2023-10-24 16:30:01.657 
Epoch 73/1000 
	 loss: 130.4599, MinusLogProbMetric: 130.4599, val_loss: 130.4633, val_MinusLogProbMetric: 130.4633

Epoch 73: val_loss improved from 131.51572 to 130.46335, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 130.4599 - MinusLogProbMetric: 130.4599 - val_loss: 130.4633 - val_MinusLogProbMetric: 130.4633 - lr: 1.2346e-05 - 71s/epoch - 361ms/step
Epoch 74/1000
2023-10-24 16:31:11.279 
Epoch 74/1000 
	 loss: 129.4452, MinusLogProbMetric: 129.4452, val_loss: 129.5791, val_MinusLogProbMetric: 129.5791

Epoch 74: val_loss improved from 130.46335 to 129.57906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 129.4452 - MinusLogProbMetric: 129.4452 - val_loss: 129.5791 - val_MinusLogProbMetric: 129.5791 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 75/1000
2023-10-24 16:32:19.640 
Epoch 75/1000 
	 loss: 129.3103, MinusLogProbMetric: 129.3103, val_loss: 128.1567, val_MinusLogProbMetric: 128.1567

Epoch 75: val_loss improved from 129.57906 to 128.15672, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 129.3103 - MinusLogProbMetric: 129.3103 - val_loss: 128.1567 - val_MinusLogProbMetric: 128.1567 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 76/1000
2023-10-24 16:33:27.954 
Epoch 76/1000 
	 loss: 133.4292, MinusLogProbMetric: 133.4292, val_loss: 128.1536, val_MinusLogProbMetric: 128.1536

Epoch 76: val_loss improved from 128.15672 to 128.15361, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 68s - loss: 133.4292 - MinusLogProbMetric: 133.4292 - val_loss: 128.1536 - val_MinusLogProbMetric: 128.1536 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 77/1000
2023-10-24 16:34:35.388 
Epoch 77/1000 
	 loss: 127.0866, MinusLogProbMetric: 127.0866, val_loss: 127.4972, val_MinusLogProbMetric: 127.4972

Epoch 77: val_loss improved from 128.15361 to 127.49718, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 67s - loss: 127.0866 - MinusLogProbMetric: 127.0866 - val_loss: 127.4972 - val_MinusLogProbMetric: 127.4972 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 78/1000
2023-10-24 16:35:45.770 
Epoch 78/1000 
	 loss: 125.7194, MinusLogProbMetric: 125.7194, val_loss: 124.9588, val_MinusLogProbMetric: 124.9588

Epoch 78: val_loss improved from 127.49718 to 124.95876, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 125.7194 - MinusLogProbMetric: 125.7194 - val_loss: 124.9588 - val_MinusLogProbMetric: 124.9588 - lr: 1.2346e-05 - 71s/epoch - 360ms/step
Epoch 79/1000
2023-10-24 16:36:55.595 
Epoch 79/1000 
	 loss: 124.3538, MinusLogProbMetric: 124.3538, val_loss: 123.9825, val_MinusLogProbMetric: 123.9825

Epoch 79: val_loss improved from 124.95876 to 123.98254, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 124.3538 - MinusLogProbMetric: 124.3538 - val_loss: 123.9825 - val_MinusLogProbMetric: 123.9825 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 80/1000
2023-10-24 16:38:04.576 
Epoch 80/1000 
	 loss: 123.4596, MinusLogProbMetric: 123.4596, val_loss: 123.4882, val_MinusLogProbMetric: 123.4882

Epoch 80: val_loss improved from 123.98254 to 123.48822, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 123.4596 - MinusLogProbMetric: 123.4596 - val_loss: 123.4882 - val_MinusLogProbMetric: 123.4882 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 81/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-24 16:39:11.341 
Epoch 81/1000 
	 loss: 122.3390, MinusLogProbMetric: 122.3390, val_loss: 127.7896, val_MinusLogProbMetric: 127.7897

Epoch 81: val_loss did not improve from 123.48822
196/196 - 66s - loss: 122.3390 - MinusLogProbMetric: 122.3390 - val_loss: 127.7896 - val_MinusLogProbMetric: 127.7897 - lr: 1.2346e-05 - 66s/epoch - 335ms/step
Epoch 82/1000
2023-10-24 16:40:17.448 
Epoch 82/1000 
	 loss: 122.1107, MinusLogProbMetric: 122.1107, val_loss: 121.2727, val_MinusLogProbMetric: 121.2727

Epoch 82: val_loss improved from 123.48822 to 121.27274, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 67s - loss: 122.1107 - MinusLogProbMetric: 122.1107 - val_loss: 121.2727 - val_MinusLogProbMetric: 121.2727 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 83/1000
2023-10-24 16:41:25.568 
Epoch 83/1000 
	 loss: 120.7597, MinusLogProbMetric: 120.7597, val_loss: 120.6174, val_MinusLogProbMetric: 120.6174

Epoch 83: val_loss improved from 121.27274 to 120.61738, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 68s - loss: 120.7597 - MinusLogProbMetric: 120.7597 - val_loss: 120.6174 - val_MinusLogProbMetric: 120.6174 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 84/1000
2023-10-24 16:42:33.424 
Epoch 84/1000 
	 loss: 119.7999, MinusLogProbMetric: 119.7999, val_loss: 121.3270, val_MinusLogProbMetric: 121.3270

Epoch 84: val_loss did not improve from 120.61738
196/196 - 67s - loss: 119.7999 - MinusLogProbMetric: 119.7999 - val_loss: 121.3270 - val_MinusLogProbMetric: 121.3270 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 85/1000
2023-10-24 16:43:41.763 
Epoch 85/1000 
	 loss: 119.2653, MinusLogProbMetric: 119.2653, val_loss: 120.8427, val_MinusLogProbMetric: 120.8427

Epoch 85: val_loss did not improve from 120.61738
196/196 - 68s - loss: 119.2653 - MinusLogProbMetric: 119.2653 - val_loss: 120.8427 - val_MinusLogProbMetric: 120.8427 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 86/1000
2023-10-24 16:44:52.863 
Epoch 86/1000 
	 loss: 118.5792, MinusLogProbMetric: 118.5792, val_loss: 119.3076, val_MinusLogProbMetric: 119.3076

Epoch 86: val_loss improved from 120.61738 to 119.30756, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 72s - loss: 118.5792 - MinusLogProbMetric: 118.5792 - val_loss: 119.3076 - val_MinusLogProbMetric: 119.3076 - lr: 1.2346e-05 - 72s/epoch - 368ms/step
Epoch 87/1000
2023-10-24 16:46:01.737 
Epoch 87/1000 
	 loss: 117.5538, MinusLogProbMetric: 117.5538, val_loss: 116.8862, val_MinusLogProbMetric: 116.8862

Epoch 87: val_loss improved from 119.30756 to 116.88620, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 117.5538 - MinusLogProbMetric: 117.5538 - val_loss: 116.8862 - val_MinusLogProbMetric: 116.8862 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 88/1000
2023-10-24 16:47:12.304 
Epoch 88/1000 
	 loss: 116.6428, MinusLogProbMetric: 116.6428, val_loss: 116.7597, val_MinusLogProbMetric: 116.7597

Epoch 88: val_loss improved from 116.88620 to 116.75970, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 116.6428 - MinusLogProbMetric: 116.6428 - val_loss: 116.7597 - val_MinusLogProbMetric: 116.7597 - lr: 1.2346e-05 - 70s/epoch - 360ms/step
Epoch 89/1000
2023-10-24 16:48:23.100 
Epoch 89/1000 
	 loss: 116.6382, MinusLogProbMetric: 116.6382, val_loss: 115.5591, val_MinusLogProbMetric: 115.5591

Epoch 89: val_loss improved from 116.75970 to 115.55906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 116.6382 - MinusLogProbMetric: 116.6382 - val_loss: 115.5591 - val_MinusLogProbMetric: 115.5591 - lr: 1.2346e-05 - 71s/epoch - 362ms/step
Epoch 90/1000
2023-10-24 16:49:32.230 
Epoch 90/1000 
	 loss: 115.3354, MinusLogProbMetric: 115.3354, val_loss: 114.8483, val_MinusLogProbMetric: 114.8483

Epoch 90: val_loss improved from 115.55906 to 114.84827, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 115.3354 - MinusLogProbMetric: 115.3354 - val_loss: 114.8483 - val_MinusLogProbMetric: 114.8483 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 91/1000
2023-10-24 16:50:41.305 
Epoch 91/1000 
	 loss: 114.8982, MinusLogProbMetric: 114.8982, val_loss: 114.1610, val_MinusLogProbMetric: 114.1610

Epoch 91: val_loss improved from 114.84827 to 114.16103, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 114.8982 - MinusLogProbMetric: 114.8982 - val_loss: 114.1610 - val_MinusLogProbMetric: 114.1610 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 92/1000
2023-10-24 16:51:50.531 
Epoch 92/1000 
	 loss: 117.1013, MinusLogProbMetric: 117.1013, val_loss: 118.9890, val_MinusLogProbMetric: 118.9890

Epoch 92: val_loss did not improve from 114.16103
196/196 - 68s - loss: 117.1013 - MinusLogProbMetric: 117.1013 - val_loss: 118.9890 - val_MinusLogProbMetric: 118.9890 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 93/1000
2023-10-24 16:52:59.108 
Epoch 93/1000 
	 loss: 115.1641, MinusLogProbMetric: 115.1641, val_loss: 113.5621, val_MinusLogProbMetric: 113.5621

Epoch 93: val_loss improved from 114.16103 to 113.56212, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 115.1641 - MinusLogProbMetric: 115.1641 - val_loss: 113.5621 - val_MinusLogProbMetric: 113.5621 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 94/1000
2023-10-24 16:54:09.062 
Epoch 94/1000 
	 loss: 113.3804, MinusLogProbMetric: 113.3804, val_loss: 113.1507, val_MinusLogProbMetric: 113.1507

Epoch 94: val_loss improved from 113.56212 to 113.15068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 113.3804 - MinusLogProbMetric: 113.3804 - val_loss: 113.1507 - val_MinusLogProbMetric: 113.1507 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 95/1000
2023-10-24 16:55:17.051 
Epoch 95/1000 
	 loss: 112.2673, MinusLogProbMetric: 112.2673, val_loss: 113.6680, val_MinusLogProbMetric: 113.6680

Epoch 95: val_loss did not improve from 113.15068
196/196 - 67s - loss: 112.2673 - MinusLogProbMetric: 112.2673 - val_loss: 113.6680 - val_MinusLogProbMetric: 113.6680 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 96/1000
2023-10-24 16:56:25.910 
Epoch 96/1000 
	 loss: 111.6804, MinusLogProbMetric: 111.6804, val_loss: 112.0662, val_MinusLogProbMetric: 112.0662

Epoch 96: val_loss improved from 113.15068 to 112.06623, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 111.6804 - MinusLogProbMetric: 111.6804 - val_loss: 112.0662 - val_MinusLogProbMetric: 112.0662 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 97/1000
2023-10-24 16:57:36.208 
Epoch 97/1000 
	 loss: 110.5859, MinusLogProbMetric: 110.5859, val_loss: 110.5641, val_MinusLogProbMetric: 110.5641

Epoch 97: val_loss improved from 112.06623 to 110.56406, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 110.5859 - MinusLogProbMetric: 110.5859 - val_loss: 110.5641 - val_MinusLogProbMetric: 110.5641 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 98/1000
2023-10-24 16:58:47.410 
Epoch 98/1000 
	 loss: 110.2372, MinusLogProbMetric: 110.2372, val_loss: 109.6518, val_MinusLogProbMetric: 109.6518

Epoch 98: val_loss improved from 110.56406 to 109.65180, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 110.2372 - MinusLogProbMetric: 110.2372 - val_loss: 109.6518 - val_MinusLogProbMetric: 109.6518 - lr: 1.2346e-05 - 71s/epoch - 364ms/step
Epoch 99/1000
2023-10-24 16:59:56.800 
Epoch 99/1000 
	 loss: 110.2533, MinusLogProbMetric: 110.2533, val_loss: 110.4357, val_MinusLogProbMetric: 110.4357

Epoch 99: val_loss did not improve from 109.65180
196/196 - 68s - loss: 110.2533 - MinusLogProbMetric: 110.2533 - val_loss: 110.4357 - val_MinusLogProbMetric: 110.4357 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 100/1000
2023-10-24 17:01:03.662 
Epoch 100/1000 
	 loss: 109.5278, MinusLogProbMetric: 109.5278, val_loss: 109.2057, val_MinusLogProbMetric: 109.2057

Epoch 100: val_loss improved from 109.65180 to 109.20575, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 68s - loss: 109.5278 - MinusLogProbMetric: 109.5278 - val_loss: 109.2057 - val_MinusLogProbMetric: 109.2057 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 101/1000
2023-10-24 17:02:14.499 
Epoch 101/1000 
	 loss: 109.3657, MinusLogProbMetric: 109.3657, val_loss: 108.5243, val_MinusLogProbMetric: 108.5243

Epoch 101: val_loss improved from 109.20575 to 108.52435, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 109.3657 - MinusLogProbMetric: 109.3657 - val_loss: 108.5243 - val_MinusLogProbMetric: 108.5243 - lr: 1.2346e-05 - 71s/epoch - 362ms/step
Epoch 102/1000
2023-10-24 17:03:28.027 
Epoch 102/1000 
	 loss: 108.0325, MinusLogProbMetric: 108.0325, val_loss: 107.8603, val_MinusLogProbMetric: 107.8603

Epoch 102: val_loss improved from 108.52435 to 107.86034, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 73s - loss: 108.0325 - MinusLogProbMetric: 108.0325 - val_loss: 107.8603 - val_MinusLogProbMetric: 107.8603 - lr: 1.2346e-05 - 73s/epoch - 374ms/step
Epoch 103/1000
2023-10-24 17:04:37.062 
Epoch 103/1000 
	 loss: 107.8595, MinusLogProbMetric: 107.8595, val_loss: 107.1294, val_MinusLogProbMetric: 107.1294

Epoch 103: val_loss improved from 107.86034 to 107.12941, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 107.8595 - MinusLogProbMetric: 107.8595 - val_loss: 107.1294 - val_MinusLogProbMetric: 107.1294 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 104/1000
2023-10-24 17:05:44.757 
Epoch 104/1000 
	 loss: 107.1363, MinusLogProbMetric: 107.1363, val_loss: 107.3359, val_MinusLogProbMetric: 107.3359

Epoch 104: val_loss did not improve from 107.12941
196/196 - 67s - loss: 107.1363 - MinusLogProbMetric: 107.1363 - val_loss: 107.3359 - val_MinusLogProbMetric: 107.3359 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 105/1000
2023-10-24 17:06:51.470 
Epoch 105/1000 
	 loss: 107.0210, MinusLogProbMetric: 107.0210, val_loss: 106.0900, val_MinusLogProbMetric: 106.0900

Epoch 105: val_loss improved from 107.12941 to 106.09000, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 68s - loss: 107.0210 - MinusLogProbMetric: 107.0210 - val_loss: 106.0900 - val_MinusLogProbMetric: 106.0900 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 106/1000
2023-10-24 17:07:58.574 
Epoch 106/1000 
	 loss: 105.7185, MinusLogProbMetric: 105.7185, val_loss: 105.5394, val_MinusLogProbMetric: 105.5394

Epoch 106: val_loss improved from 106.09000 to 105.53940, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 67s - loss: 105.7185 - MinusLogProbMetric: 105.7185 - val_loss: 105.5394 - val_MinusLogProbMetric: 105.5394 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 107/1000
2023-10-24 17:09:06.290 
Epoch 107/1000 
	 loss: 105.0162, MinusLogProbMetric: 105.0162, val_loss: 106.3771, val_MinusLogProbMetric: 106.3771

Epoch 107: val_loss did not improve from 105.53940
196/196 - 67s - loss: 105.0162 - MinusLogProbMetric: 105.0162 - val_loss: 106.3771 - val_MinusLogProbMetric: 106.3771 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 108/1000
2023-10-24 17:10:14.954 
Epoch 108/1000 
	 loss: 106.7806, MinusLogProbMetric: 106.7806, val_loss: 105.0882, val_MinusLogProbMetric: 105.0882

Epoch 108: val_loss improved from 105.53940 to 105.08819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 106.7806 - MinusLogProbMetric: 106.7806 - val_loss: 105.0882 - val_MinusLogProbMetric: 105.0882 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 109/1000
2023-10-24 17:11:25.971 
Epoch 109/1000 
	 loss: 103.9894, MinusLogProbMetric: 103.9894, val_loss: 103.3672, val_MinusLogProbMetric: 103.3672

Epoch 109: val_loss improved from 105.08819 to 103.36725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 103.9894 - MinusLogProbMetric: 103.9894 - val_loss: 103.3672 - val_MinusLogProbMetric: 103.3672 - lr: 1.2346e-05 - 71s/epoch - 362ms/step
Epoch 110/1000
2023-10-24 17:12:34.374 
Epoch 110/1000 
	 loss: 103.7542, MinusLogProbMetric: 103.7542, val_loss: 103.8132, val_MinusLogProbMetric: 103.8132

Epoch 110: val_loss did not improve from 103.36725
196/196 - 67s - loss: 103.7542 - MinusLogProbMetric: 103.7542 - val_loss: 103.8132 - val_MinusLogProbMetric: 103.8132 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 111/1000
2023-10-24 17:13:42.068 
Epoch 111/1000 
	 loss: 103.8710, MinusLogProbMetric: 103.8710, val_loss: 103.9677, val_MinusLogProbMetric: 103.9677

Epoch 111: val_loss did not improve from 103.36725
196/196 - 68s - loss: 103.8710 - MinusLogProbMetric: 103.8710 - val_loss: 103.9677 - val_MinusLogProbMetric: 103.9677 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 112/1000
2023-10-24 17:14:49.346 
Epoch 112/1000 
	 loss: 103.3551, MinusLogProbMetric: 103.3551, val_loss: 102.6767, val_MinusLogProbMetric: 102.6767

Epoch 112: val_loss improved from 103.36725 to 102.67672, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 68s - loss: 103.3551 - MinusLogProbMetric: 103.3551 - val_loss: 102.6767 - val_MinusLogProbMetric: 102.6767 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 113/1000
2023-10-24 17:15:56.645 
Epoch 113/1000 
	 loss: 102.2256, MinusLogProbMetric: 102.2256, val_loss: 101.3989, val_MinusLogProbMetric: 101.3989

Epoch 113: val_loss improved from 102.67672 to 101.39888, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 67s - loss: 102.2256 - MinusLogProbMetric: 102.2256 - val_loss: 101.3989 - val_MinusLogProbMetric: 101.3989 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 114/1000
2023-10-24 17:17:08.056 
Epoch 114/1000 
	 loss: 101.7046, MinusLogProbMetric: 101.7046, val_loss: 106.1397, val_MinusLogProbMetric: 106.1397

Epoch 114: val_loss did not improve from 101.39888
196/196 - 70s - loss: 101.7046 - MinusLogProbMetric: 101.7046 - val_loss: 106.1397 - val_MinusLogProbMetric: 106.1397 - lr: 1.2346e-05 - 70s/epoch - 359ms/step
Epoch 115/1000
2023-10-24 17:18:16.803 
Epoch 115/1000 
	 loss: 101.4621, MinusLogProbMetric: 101.4621, val_loss: 100.9406, val_MinusLogProbMetric: 100.9406

Epoch 115: val_loss improved from 101.39888 to 100.94057, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 101.4621 - MinusLogProbMetric: 101.4621 - val_loss: 100.9406 - val_MinusLogProbMetric: 100.9406 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 116/1000
2023-10-24 17:19:28.204 
Epoch 116/1000 
	 loss: 100.7384, MinusLogProbMetric: 100.7384, val_loss: 100.1256, val_MinusLogProbMetric: 100.1256

Epoch 116: val_loss improved from 100.94057 to 100.12563, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 100.7384 - MinusLogProbMetric: 100.7384 - val_loss: 100.1256 - val_MinusLogProbMetric: 100.1256 - lr: 1.2346e-05 - 71s/epoch - 364ms/step
Epoch 117/1000
2023-10-24 17:20:36.054 
Epoch 117/1000 
	 loss: 104.6300, MinusLogProbMetric: 104.6300, val_loss: 106.7980, val_MinusLogProbMetric: 106.7980

Epoch 117: val_loss did not improve from 100.12563
196/196 - 67s - loss: 104.6300 - MinusLogProbMetric: 104.6300 - val_loss: 106.7980 - val_MinusLogProbMetric: 106.7980 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 118/1000
2023-10-24 17:21:44.081 
Epoch 118/1000 
	 loss: 109.8906, MinusLogProbMetric: 109.8906, val_loss: 273.1491, val_MinusLogProbMetric: 273.1491

Epoch 118: val_loss did not improve from 100.12563
196/196 - 68s - loss: 109.8906 - MinusLogProbMetric: 109.8906 - val_loss: 273.1491 - val_MinusLogProbMetric: 273.1491 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 119/1000
2023-10-24 17:22:54.176 
Epoch 119/1000 
	 loss: 204.1945, MinusLogProbMetric: 204.1945, val_loss: 303.8895, val_MinusLogProbMetric: 303.8895

Epoch 119: val_loss did not improve from 100.12563
196/196 - 70s - loss: 204.1945 - MinusLogProbMetric: 204.1945 - val_loss: 303.8895 - val_MinusLogProbMetric: 303.8895 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 120/1000
2023-10-24 17:23:59.870 
Epoch 120/1000 
	 loss: 212.7078, MinusLogProbMetric: 212.7078, val_loss: 178.4460, val_MinusLogProbMetric: 178.4460

Epoch 120: val_loss did not improve from 100.12563
196/196 - 66s - loss: 212.7078 - MinusLogProbMetric: 212.7078 - val_loss: 178.4460 - val_MinusLogProbMetric: 178.4460 - lr: 1.2346e-05 - 66s/epoch - 335ms/step
Epoch 121/1000
2023-10-24 17:25:07.934 
Epoch 121/1000 
	 loss: 161.8905, MinusLogProbMetric: 161.8905, val_loss: 149.3901, val_MinusLogProbMetric: 149.3901

Epoch 121: val_loss did not improve from 100.12563
196/196 - 68s - loss: 161.8905 - MinusLogProbMetric: 161.8905 - val_loss: 149.3901 - val_MinusLogProbMetric: 149.3901 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 122/1000
2023-10-24 17:26:17.164 
Epoch 122/1000 
	 loss: 140.9203, MinusLogProbMetric: 140.9203, val_loss: 134.3540, val_MinusLogProbMetric: 134.3540

Epoch 122: val_loss did not improve from 100.12563
196/196 - 69s - loss: 140.9203 - MinusLogProbMetric: 140.9203 - val_loss: 134.3540 - val_MinusLogProbMetric: 134.3540 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 123/1000
2023-10-24 17:27:26.413 
Epoch 123/1000 
	 loss: 130.7588, MinusLogProbMetric: 130.7588, val_loss: 127.6232, val_MinusLogProbMetric: 127.6232

Epoch 123: val_loss did not improve from 100.12563
196/196 - 69s - loss: 130.7588 - MinusLogProbMetric: 130.7588 - val_loss: 127.6232 - val_MinusLogProbMetric: 127.6232 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 124/1000
2023-10-24 17:28:33.947 
Epoch 124/1000 
	 loss: 124.7544, MinusLogProbMetric: 124.7544, val_loss: 151.1799, val_MinusLogProbMetric: 151.1799

Epoch 124: val_loss did not improve from 100.12563
196/196 - 68s - loss: 124.7544 - MinusLogProbMetric: 124.7544 - val_loss: 151.1799 - val_MinusLogProbMetric: 151.1799 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 125/1000
2023-10-24 17:29:42.643 
Epoch 125/1000 
	 loss: 128.1667, MinusLogProbMetric: 128.1667, val_loss: 122.1853, val_MinusLogProbMetric: 122.1853

Epoch 125: val_loss did not improve from 100.12563
196/196 - 69s - loss: 128.1667 - MinusLogProbMetric: 128.1667 - val_loss: 122.1853 - val_MinusLogProbMetric: 122.1853 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 126/1000
2023-10-24 17:30:53.247 
Epoch 126/1000 
	 loss: 119.6778, MinusLogProbMetric: 119.6778, val_loss: 118.0075, val_MinusLogProbMetric: 118.0075

Epoch 126: val_loss did not improve from 100.12563
196/196 - 71s - loss: 119.6778 - MinusLogProbMetric: 119.6778 - val_loss: 118.0075 - val_MinusLogProbMetric: 118.0075 - lr: 1.2346e-05 - 71s/epoch - 360ms/step
Epoch 127/1000
2023-10-24 17:32:02.922 
Epoch 127/1000 
	 loss: 116.1170, MinusLogProbMetric: 116.1170, val_loss: 114.7502, val_MinusLogProbMetric: 114.7502

Epoch 127: val_loss did not improve from 100.12563
196/196 - 70s - loss: 116.1170 - MinusLogProbMetric: 116.1170 - val_loss: 114.7502 - val_MinusLogProbMetric: 114.7502 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 128/1000
2023-10-24 17:33:12.007 
Epoch 128/1000 
	 loss: 113.3837, MinusLogProbMetric: 113.3837, val_loss: 112.5469, val_MinusLogProbMetric: 112.5469

Epoch 128: val_loss did not improve from 100.12563
196/196 - 69s - loss: 113.3837 - MinusLogProbMetric: 113.3837 - val_loss: 112.5469 - val_MinusLogProbMetric: 112.5469 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 129/1000
2023-10-24 17:34:20.260 
Epoch 129/1000 
	 loss: 111.6188, MinusLogProbMetric: 111.6188, val_loss: 111.2669, val_MinusLogProbMetric: 111.2669

Epoch 129: val_loss did not improve from 100.12563
196/196 - 68s - loss: 111.6188 - MinusLogProbMetric: 111.6188 - val_loss: 111.2669 - val_MinusLogProbMetric: 111.2669 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 130/1000
2023-10-24 17:35:28.381 
Epoch 130/1000 
	 loss: 114.3836, MinusLogProbMetric: 114.3836, val_loss: 111.6111, val_MinusLogProbMetric: 111.6111

Epoch 130: val_loss did not improve from 100.12563
196/196 - 68s - loss: 114.3836 - MinusLogProbMetric: 114.3836 - val_loss: 111.6111 - val_MinusLogProbMetric: 111.6111 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 131/1000
2023-10-24 17:36:36.626 
Epoch 131/1000 
	 loss: 110.3068, MinusLogProbMetric: 110.3068, val_loss: 109.3633, val_MinusLogProbMetric: 109.3633

Epoch 131: val_loss did not improve from 100.12563
196/196 - 68s - loss: 110.3068 - MinusLogProbMetric: 110.3068 - val_loss: 109.3633 - val_MinusLogProbMetric: 109.3633 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 132/1000
2023-10-24 17:37:45.189 
Epoch 132/1000 
	 loss: 108.5513, MinusLogProbMetric: 108.5513, val_loss: 107.8533, val_MinusLogProbMetric: 107.8533

Epoch 132: val_loss did not improve from 100.12563
196/196 - 69s - loss: 108.5513 - MinusLogProbMetric: 108.5513 - val_loss: 107.8533 - val_MinusLogProbMetric: 107.8533 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 133/1000
2023-10-24 17:38:53.045 
Epoch 133/1000 
	 loss: 107.3315, MinusLogProbMetric: 107.3315, val_loss: 106.6142, val_MinusLogProbMetric: 106.6142

Epoch 133: val_loss did not improve from 100.12563
196/196 - 68s - loss: 107.3315 - MinusLogProbMetric: 107.3315 - val_loss: 106.6142 - val_MinusLogProbMetric: 106.6142 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 134/1000
2023-10-24 17:39:58.853 
Epoch 134/1000 
	 loss: 106.1883, MinusLogProbMetric: 106.1883, val_loss: 105.5368, val_MinusLogProbMetric: 105.5368

Epoch 134: val_loss did not improve from 100.12563
196/196 - 66s - loss: 106.1883 - MinusLogProbMetric: 106.1883 - val_loss: 105.5368 - val_MinusLogProbMetric: 105.5368 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 135/1000
2023-10-24 17:41:07.068 
Epoch 135/1000 
	 loss: 105.4158, MinusLogProbMetric: 105.4158, val_loss: 104.7746, val_MinusLogProbMetric: 104.7746

Epoch 135: val_loss did not improve from 100.12563
196/196 - 68s - loss: 105.4158 - MinusLogProbMetric: 105.4158 - val_loss: 104.7746 - val_MinusLogProbMetric: 104.7746 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 136/1000
2023-10-24 17:42:14.388 
Epoch 136/1000 
	 loss: 104.0396, MinusLogProbMetric: 104.0396, val_loss: 103.6653, val_MinusLogProbMetric: 103.6653

Epoch 136: val_loss did not improve from 100.12563
196/196 - 67s - loss: 104.0396 - MinusLogProbMetric: 104.0396 - val_loss: 103.6653 - val_MinusLogProbMetric: 103.6653 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 137/1000
2023-10-24 17:43:22.127 
Epoch 137/1000 
	 loss: 103.3236, MinusLogProbMetric: 103.3236, val_loss: 103.0729, val_MinusLogProbMetric: 103.0729

Epoch 137: val_loss did not improve from 100.12563
196/196 - 68s - loss: 103.3236 - MinusLogProbMetric: 103.3236 - val_loss: 103.0729 - val_MinusLogProbMetric: 103.0729 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 138/1000
2023-10-24 17:44:30.781 
Epoch 138/1000 
	 loss: 102.6872, MinusLogProbMetric: 102.6872, val_loss: 102.3137, val_MinusLogProbMetric: 102.3137

Epoch 138: val_loss did not improve from 100.12563
196/196 - 69s - loss: 102.6872 - MinusLogProbMetric: 102.6872 - val_loss: 102.3137 - val_MinusLogProbMetric: 102.3137 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 139/1000
2023-10-24 17:45:40.044 
Epoch 139/1000 
	 loss: 101.5865, MinusLogProbMetric: 101.5865, val_loss: 101.8000, val_MinusLogProbMetric: 101.8000

Epoch 139: val_loss did not improve from 100.12563
196/196 - 69s - loss: 101.5865 - MinusLogProbMetric: 101.5865 - val_loss: 101.8000 - val_MinusLogProbMetric: 101.8000 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 140/1000
2023-10-24 17:46:48.783 
Epoch 140/1000 
	 loss: 100.8190, MinusLogProbMetric: 100.8190, val_loss: 100.6129, val_MinusLogProbMetric: 100.6129

Epoch 140: val_loss did not improve from 100.12563
196/196 - 69s - loss: 100.8190 - MinusLogProbMetric: 100.8190 - val_loss: 100.6129 - val_MinusLogProbMetric: 100.6129 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 141/1000
2023-10-24 17:47:57.602 
Epoch 141/1000 
	 loss: 100.1499, MinusLogProbMetric: 100.1499, val_loss: 99.9598, val_MinusLogProbMetric: 99.9598

Epoch 141: val_loss improved from 100.12563 to 99.95982, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 100.1499 - MinusLogProbMetric: 100.1499 - val_loss: 99.9598 - val_MinusLogProbMetric: 99.9598 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 142/1000
2023-10-24 17:49:05.511 
Epoch 142/1000 
	 loss: 99.5100, MinusLogProbMetric: 99.5100, val_loss: 99.3344, val_MinusLogProbMetric: 99.3344

Epoch 142: val_loss improved from 99.95982 to 99.33438, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 68s - loss: 99.5100 - MinusLogProbMetric: 99.5100 - val_loss: 99.3344 - val_MinusLogProbMetric: 99.3344 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 143/1000
2023-10-24 17:50:15.778 
Epoch 143/1000 
	 loss: 98.7798, MinusLogProbMetric: 98.7798, val_loss: 98.4818, val_MinusLogProbMetric: 98.4818

Epoch 143: val_loss improved from 99.33438 to 98.48182, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 98.7798 - MinusLogProbMetric: 98.7798 - val_loss: 98.4818 - val_MinusLogProbMetric: 98.4818 - lr: 1.2346e-05 - 71s/epoch - 360ms/step
Epoch 144/1000
2023-10-24 17:51:26.205 
Epoch 144/1000 
	 loss: 98.0474, MinusLogProbMetric: 98.0474, val_loss: 97.8681, val_MinusLogProbMetric: 97.8681

Epoch 144: val_loss improved from 98.48182 to 97.86814, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 98.0474 - MinusLogProbMetric: 98.0474 - val_loss: 97.8681 - val_MinusLogProbMetric: 97.8681 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 145/1000
2023-10-24 17:52:35.380 
Epoch 145/1000 
	 loss: 97.8984, MinusLogProbMetric: 97.8984, val_loss: 98.5450, val_MinusLogProbMetric: 98.5450

Epoch 145: val_loss did not improve from 97.86814
196/196 - 68s - loss: 97.8984 - MinusLogProbMetric: 97.8984 - val_loss: 98.5450 - val_MinusLogProbMetric: 98.5450 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 146/1000
2023-10-24 17:53:43.251 
Epoch 146/1000 
	 loss: 97.3956, MinusLogProbMetric: 97.3956, val_loss: 97.0114, val_MinusLogProbMetric: 97.0114

Epoch 146: val_loss improved from 97.86814 to 97.01135, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 97.3956 - MinusLogProbMetric: 97.3956 - val_loss: 97.0114 - val_MinusLogProbMetric: 97.0114 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 147/1000
2023-10-24 17:54:51.950 
Epoch 147/1000 
	 loss: 96.5860, MinusLogProbMetric: 96.5860, val_loss: 96.4577, val_MinusLogProbMetric: 96.4577

Epoch 147: val_loss improved from 97.01135 to 96.45766, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 96.5860 - MinusLogProbMetric: 96.5860 - val_loss: 96.4577 - val_MinusLogProbMetric: 96.4577 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 148/1000
2023-10-24 17:55:58.923 
Epoch 148/1000 
	 loss: 95.7774, MinusLogProbMetric: 95.7774, val_loss: 95.9838, val_MinusLogProbMetric: 95.9838

Epoch 148: val_loss improved from 96.45766 to 95.98383, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 67s - loss: 95.7774 - MinusLogProbMetric: 95.7774 - val_loss: 95.9838 - val_MinusLogProbMetric: 95.9838 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 149/1000
2023-10-24 17:57:07.434 
Epoch 149/1000 
	 loss: 95.1627, MinusLogProbMetric: 95.1627, val_loss: 95.2007, val_MinusLogProbMetric: 95.2007

Epoch 149: val_loss improved from 95.98383 to 95.20069, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 95.1627 - MinusLogProbMetric: 95.1627 - val_loss: 95.2007 - val_MinusLogProbMetric: 95.2007 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 150/1000
2023-10-24 17:58:16.350 
Epoch 150/1000 
	 loss: 94.5855, MinusLogProbMetric: 94.5855, val_loss: 94.5155, val_MinusLogProbMetric: 94.5155

Epoch 150: val_loss improved from 95.20069 to 94.51550, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 94.5855 - MinusLogProbMetric: 94.5855 - val_loss: 94.5155 - val_MinusLogProbMetric: 94.5155 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 151/1000
2023-10-24 17:59:27.531 
Epoch 151/1000 
	 loss: 94.1239, MinusLogProbMetric: 94.1239, val_loss: 94.2517, val_MinusLogProbMetric: 94.2517

Epoch 151: val_loss improved from 94.51550 to 94.25166, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 94.1239 - MinusLogProbMetric: 94.1239 - val_loss: 94.2517 - val_MinusLogProbMetric: 94.2517 - lr: 1.2346e-05 - 71s/epoch - 364ms/step
Epoch 152/1000
2023-10-24 18:00:36.443 
Epoch 152/1000 
	 loss: 93.6450, MinusLogProbMetric: 93.6450, val_loss: 93.8230, val_MinusLogProbMetric: 93.8230

Epoch 152: val_loss improved from 94.25166 to 93.82296, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 93.6450 - MinusLogProbMetric: 93.6450 - val_loss: 93.8230 - val_MinusLogProbMetric: 93.8230 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 153/1000
2023-10-24 18:01:43.762 
Epoch 153/1000 
	 loss: 93.2184, MinusLogProbMetric: 93.2184, val_loss: 93.2593, val_MinusLogProbMetric: 93.2593

Epoch 153: val_loss improved from 93.82296 to 93.25930, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 67s - loss: 93.2184 - MinusLogProbMetric: 93.2184 - val_loss: 93.2593 - val_MinusLogProbMetric: 93.2593 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 154/1000
2023-10-24 18:02:51.130 
Epoch 154/1000 
	 loss: 92.8518, MinusLogProbMetric: 92.8518, val_loss: 93.2064, val_MinusLogProbMetric: 93.2064

Epoch 154: val_loss improved from 93.25930 to 93.20640, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 67s - loss: 92.8518 - MinusLogProbMetric: 92.8518 - val_loss: 93.2064 - val_MinusLogProbMetric: 93.2064 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 155/1000
2023-10-24 18:03:59.466 
Epoch 155/1000 
	 loss: 92.3174, MinusLogProbMetric: 92.3174, val_loss: 92.5792, val_MinusLogProbMetric: 92.5792

Epoch 155: val_loss improved from 93.20640 to 92.57923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 68s - loss: 92.3174 - MinusLogProbMetric: 92.3174 - val_loss: 92.5792 - val_MinusLogProbMetric: 92.5792 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 156/1000
2023-10-24 18:05:08.030 
Epoch 156/1000 
	 loss: 92.1056, MinusLogProbMetric: 92.1056, val_loss: 92.3931, val_MinusLogProbMetric: 92.3931

Epoch 156: val_loss improved from 92.57923 to 92.39313, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 68s - loss: 92.1056 - MinusLogProbMetric: 92.1056 - val_loss: 92.3931 - val_MinusLogProbMetric: 92.3931 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 157/1000
2023-10-24 18:06:16.379 
Epoch 157/1000 
	 loss: 91.7928, MinusLogProbMetric: 91.7928, val_loss: 91.3413, val_MinusLogProbMetric: 91.3413

Epoch 157: val_loss improved from 92.39313 to 91.34131, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 91.7928 - MinusLogProbMetric: 91.7928 - val_loss: 91.3413 - val_MinusLogProbMetric: 91.3413 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 158/1000
2023-10-24 18:07:27.619 
Epoch 158/1000 
	 loss: 91.5809, MinusLogProbMetric: 91.5809, val_loss: 91.7921, val_MinusLogProbMetric: 91.7921

Epoch 158: val_loss did not improve from 91.34131
196/196 - 70s - loss: 91.5809 - MinusLogProbMetric: 91.5809 - val_loss: 91.7921 - val_MinusLogProbMetric: 91.7921 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 159/1000
2023-10-24 18:08:36.647 
Epoch 159/1000 
	 loss: 91.0370, MinusLogProbMetric: 91.0370, val_loss: 90.8650, val_MinusLogProbMetric: 90.8650

Epoch 159: val_loss improved from 91.34131 to 90.86497, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 91.0370 - MinusLogProbMetric: 91.0370 - val_loss: 90.8650 - val_MinusLogProbMetric: 90.8650 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 160/1000
2023-10-24 18:09:44.182 
Epoch 160/1000 
	 loss: 90.5521, MinusLogProbMetric: 90.5521, val_loss: 90.8855, val_MinusLogProbMetric: 90.8855

Epoch 160: val_loss did not improve from 90.86497
196/196 - 66s - loss: 90.5521 - MinusLogProbMetric: 90.5521 - val_loss: 90.8855 - val_MinusLogProbMetric: 90.8855 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 161/1000
2023-10-24 18:10:52.615 
Epoch 161/1000 
	 loss: 90.4923, MinusLogProbMetric: 90.4923, val_loss: 90.0406, val_MinusLogProbMetric: 90.0406

Epoch 161: val_loss improved from 90.86497 to 90.04056, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 90.4923 - MinusLogProbMetric: 90.4923 - val_loss: 90.0406 - val_MinusLogProbMetric: 90.0406 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 162/1000
2023-10-24 18:12:01.301 
Epoch 162/1000 
	 loss: 89.7779, MinusLogProbMetric: 89.7779, val_loss: 90.9900, val_MinusLogProbMetric: 90.9900

Epoch 162: val_loss did not improve from 90.04056
196/196 - 68s - loss: 89.7779 - MinusLogProbMetric: 89.7779 - val_loss: 90.9900 - val_MinusLogProbMetric: 90.9900 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 163/1000
2023-10-24 18:13:07.872 
Epoch 163/1000 
	 loss: 89.9756, MinusLogProbMetric: 89.9756, val_loss: 90.0568, val_MinusLogProbMetric: 90.0568

Epoch 163: val_loss did not improve from 90.04056
196/196 - 67s - loss: 89.9756 - MinusLogProbMetric: 89.9756 - val_loss: 90.0568 - val_MinusLogProbMetric: 90.0568 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 164/1000
2023-10-24 18:14:16.876 
Epoch 164/1000 
	 loss: 89.2585, MinusLogProbMetric: 89.2585, val_loss: 90.7242, val_MinusLogProbMetric: 90.7242

Epoch 164: val_loss did not improve from 90.04056
196/196 - 69s - loss: 89.2585 - MinusLogProbMetric: 89.2585 - val_loss: 90.7242 - val_MinusLogProbMetric: 90.7242 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 165/1000
2023-10-24 18:15:25.308 
Epoch 165/1000 
	 loss: 89.0353, MinusLogProbMetric: 89.0353, val_loss: 88.9081, val_MinusLogProbMetric: 88.9081

Epoch 165: val_loss improved from 90.04056 to 88.90806, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 89.0353 - MinusLogProbMetric: 89.0353 - val_loss: 88.9081 - val_MinusLogProbMetric: 88.9081 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 166/1000
2023-10-24 18:16:33.318 
Epoch 166/1000 
	 loss: 88.6911, MinusLogProbMetric: 88.6911, val_loss: 89.1423, val_MinusLogProbMetric: 89.1423

Epoch 166: val_loss did not improve from 88.90806
196/196 - 67s - loss: 88.6911 - MinusLogProbMetric: 88.6911 - val_loss: 89.1423 - val_MinusLogProbMetric: 89.1423 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 167/1000
2023-10-24 18:17:42.221 
Epoch 167/1000 
	 loss: 98.6276, MinusLogProbMetric: 98.6276, val_loss: 90.0212, val_MinusLogProbMetric: 90.0212

Epoch 167: val_loss did not improve from 88.90806
196/196 - 69s - loss: 98.6276 - MinusLogProbMetric: 98.6276 - val_loss: 90.0212 - val_MinusLogProbMetric: 90.0212 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 168/1000
2023-10-24 18:18:52.016 
Epoch 168/1000 
	 loss: 88.7837, MinusLogProbMetric: 88.7837, val_loss: 88.5778, val_MinusLogProbMetric: 88.5778

Epoch 168: val_loss improved from 88.90806 to 88.57784, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 88.7837 - MinusLogProbMetric: 88.7837 - val_loss: 88.5778 - val_MinusLogProbMetric: 88.5778 - lr: 1.2346e-05 - 71s/epoch - 362ms/step
Epoch 169/1000
2023-10-24 18:20:03.262 
Epoch 169/1000 
	 loss: 88.0910, MinusLogProbMetric: 88.0910, val_loss: 87.7191, val_MinusLogProbMetric: 87.7191

Epoch 169: val_loss improved from 88.57784 to 87.71913, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 88.0910 - MinusLogProbMetric: 88.0910 - val_loss: 87.7191 - val_MinusLogProbMetric: 87.7191 - lr: 1.2346e-05 - 71s/epoch - 364ms/step
Epoch 170/1000
2023-10-24 18:21:15.080 
Epoch 170/1000 
	 loss: 87.4412, MinusLogProbMetric: 87.4412, val_loss: 87.4103, val_MinusLogProbMetric: 87.4103

Epoch 170: val_loss improved from 87.71913 to 87.41032, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 72s - loss: 87.4412 - MinusLogProbMetric: 87.4412 - val_loss: 87.4103 - val_MinusLogProbMetric: 87.4103 - lr: 1.2346e-05 - 72s/epoch - 366ms/step
Epoch 171/1000
2023-10-24 18:22:24.551 
Epoch 171/1000 
	 loss: 86.9682, MinusLogProbMetric: 86.9682, val_loss: 87.7768, val_MinusLogProbMetric: 87.7768

Epoch 171: val_loss did not improve from 87.41032
196/196 - 68s - loss: 86.9682 - MinusLogProbMetric: 86.9682 - val_loss: 87.7768 - val_MinusLogProbMetric: 87.7768 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 172/1000
2023-10-24 18:23:33.290 
Epoch 172/1000 
	 loss: 86.7073, MinusLogProbMetric: 86.7073, val_loss: 87.0500, val_MinusLogProbMetric: 87.0500

Epoch 172: val_loss improved from 87.41032 to 87.04996, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 86.7073 - MinusLogProbMetric: 86.7073 - val_loss: 87.0500 - val_MinusLogProbMetric: 87.0500 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 173/1000
2023-10-24 18:24:44.831 
Epoch 173/1000 
	 loss: 86.8189, MinusLogProbMetric: 86.8189, val_loss: 87.0769, val_MinusLogProbMetric: 87.0769

Epoch 173: val_loss did not improve from 87.04996
196/196 - 70s - loss: 86.8189 - MinusLogProbMetric: 86.8189 - val_loss: 87.0769 - val_MinusLogProbMetric: 87.0769 - lr: 1.2346e-05 - 70s/epoch - 359ms/step
Epoch 174/1000
2023-10-24 18:25:53.917 
Epoch 174/1000 
	 loss: 86.8464, MinusLogProbMetric: 86.8464, val_loss: 87.2633, val_MinusLogProbMetric: 87.2633

Epoch 174: val_loss did not improve from 87.04996
196/196 - 69s - loss: 86.8464 - MinusLogProbMetric: 86.8464 - val_loss: 87.2633 - val_MinusLogProbMetric: 87.2633 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 175/1000
2023-10-24 18:27:04.159 
Epoch 175/1000 
	 loss: 86.1948, MinusLogProbMetric: 86.1948, val_loss: 85.6804, val_MinusLogProbMetric: 85.6804

Epoch 175: val_loss improved from 87.04996 to 85.68044, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 86.1948 - MinusLogProbMetric: 86.1948 - val_loss: 85.6804 - val_MinusLogProbMetric: 85.6804 - lr: 1.2346e-05 - 71s/epoch - 364ms/step
Epoch 176/1000
2023-10-24 18:28:13.958 
Epoch 176/1000 
	 loss: 85.8021, MinusLogProbMetric: 85.8021, val_loss: 85.6698, val_MinusLogProbMetric: 85.6698

Epoch 176: val_loss improved from 85.68044 to 85.66984, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 85.8021 - MinusLogProbMetric: 85.8021 - val_loss: 85.6698 - val_MinusLogProbMetric: 85.6698 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 177/1000
2023-10-24 18:29:21.591 
Epoch 177/1000 
	 loss: 85.3398, MinusLogProbMetric: 85.3398, val_loss: 85.4183, val_MinusLogProbMetric: 85.4183

Epoch 177: val_loss improved from 85.66984 to 85.41829, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 68s - loss: 85.3398 - MinusLogProbMetric: 85.3398 - val_loss: 85.4183 - val_MinusLogProbMetric: 85.4183 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 178/1000
2023-10-24 18:30:30.521 
Epoch 178/1000 
	 loss: 85.1629, MinusLogProbMetric: 85.1629, val_loss: 85.2360, val_MinusLogProbMetric: 85.2360

Epoch 178: val_loss improved from 85.41829 to 85.23597, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 85.1629 - MinusLogProbMetric: 85.1629 - val_loss: 85.2360 - val_MinusLogProbMetric: 85.2360 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 179/1000
2023-10-24 18:31:41.813 
Epoch 179/1000 
	 loss: 84.8069, MinusLogProbMetric: 84.8069, val_loss: 85.0095, val_MinusLogProbMetric: 85.0095

Epoch 179: val_loss improved from 85.23597 to 85.00947, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 84.8069 - MinusLogProbMetric: 84.8069 - val_loss: 85.0095 - val_MinusLogProbMetric: 85.0095 - lr: 1.2346e-05 - 71s/epoch - 365ms/step
Epoch 180/1000
2023-10-24 18:32:52.476 
Epoch 180/1000 
	 loss: 84.6973, MinusLogProbMetric: 84.6973, val_loss: 84.8001, val_MinusLogProbMetric: 84.8001

Epoch 180: val_loss improved from 85.00947 to 84.80013, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 84.6973 - MinusLogProbMetric: 84.6973 - val_loss: 84.8001 - val_MinusLogProbMetric: 84.8001 - lr: 1.2346e-05 - 71s/epoch - 360ms/step
Epoch 181/1000
2023-10-24 18:34:03.208 
Epoch 181/1000 
	 loss: 84.9124, MinusLogProbMetric: 84.9124, val_loss: 84.4926, val_MinusLogProbMetric: 84.4926

Epoch 181: val_loss improved from 84.80013 to 84.49265, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 84.9124 - MinusLogProbMetric: 84.9124 - val_loss: 84.4926 - val_MinusLogProbMetric: 84.4926 - lr: 1.2346e-05 - 71s/epoch - 361ms/step
Epoch 182/1000
2023-10-24 18:35:13.068 
Epoch 182/1000 
	 loss: 85.5763, MinusLogProbMetric: 85.5763, val_loss: 84.5087, val_MinusLogProbMetric: 84.5087

Epoch 182: val_loss did not improve from 84.49265
196/196 - 69s - loss: 85.5763 - MinusLogProbMetric: 85.5763 - val_loss: 84.5087 - val_MinusLogProbMetric: 84.5087 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 183/1000
2023-10-24 18:36:22.461 
Epoch 183/1000 
	 loss: 84.1258, MinusLogProbMetric: 84.1258, val_loss: 83.8986, val_MinusLogProbMetric: 83.8986

Epoch 183: val_loss improved from 84.49265 to 83.89861, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 84.1258 - MinusLogProbMetric: 84.1258 - val_loss: 83.8986 - val_MinusLogProbMetric: 83.8986 - lr: 1.2346e-05 - 71s/epoch - 360ms/step
Epoch 184/1000
2023-10-24 18:37:33.269 
Epoch 184/1000 
	 loss: 84.0505, MinusLogProbMetric: 84.0505, val_loss: 83.9021, val_MinusLogProbMetric: 83.9021

Epoch 184: val_loss did not improve from 83.89861
196/196 - 70s - loss: 84.0505 - MinusLogProbMetric: 84.0505 - val_loss: 83.9021 - val_MinusLogProbMetric: 83.9021 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 185/1000
2023-10-24 18:38:42.115 
Epoch 185/1000 
	 loss: 83.6007, MinusLogProbMetric: 83.6007, val_loss: 83.8199, val_MinusLogProbMetric: 83.8199

Epoch 185: val_loss improved from 83.89861 to 83.81990, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 83.6007 - MinusLogProbMetric: 83.6007 - val_loss: 83.8199 - val_MinusLogProbMetric: 83.8199 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 186/1000
2023-10-24 18:39:53.789 
Epoch 186/1000 
	 loss: 83.1742, MinusLogProbMetric: 83.1742, val_loss: 83.2074, val_MinusLogProbMetric: 83.2074

Epoch 186: val_loss improved from 83.81990 to 83.20744, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 72s - loss: 83.1742 - MinusLogProbMetric: 83.1742 - val_loss: 83.2074 - val_MinusLogProbMetric: 83.2074 - lr: 1.2346e-05 - 72s/epoch - 366ms/step
Epoch 187/1000
2023-10-24 18:41:04.510 
Epoch 187/1000 
	 loss: 88.3072, MinusLogProbMetric: 88.3072, val_loss: 96.3818, val_MinusLogProbMetric: 96.3818

Epoch 187: val_loss did not improve from 83.20744
196/196 - 70s - loss: 88.3072 - MinusLogProbMetric: 88.3072 - val_loss: 96.3818 - val_MinusLogProbMetric: 96.3818 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 188/1000
2023-10-24 18:42:13.146 
Epoch 188/1000 
	 loss: 88.2493, MinusLogProbMetric: 88.2493, val_loss: 85.1543, val_MinusLogProbMetric: 85.1543

Epoch 188: val_loss did not improve from 83.20744
196/196 - 69s - loss: 88.2493 - MinusLogProbMetric: 88.2493 - val_loss: 85.1543 - val_MinusLogProbMetric: 85.1543 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 189/1000
2023-10-24 18:43:22.286 
Epoch 189/1000 
	 loss: 84.5600, MinusLogProbMetric: 84.5600, val_loss: 83.8543, val_MinusLogProbMetric: 83.8543

Epoch 189: val_loss did not improve from 83.20744
196/196 - 69s - loss: 84.5600 - MinusLogProbMetric: 84.5600 - val_loss: 83.8543 - val_MinusLogProbMetric: 83.8543 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 190/1000
2023-10-24 18:44:30.221 
Epoch 190/1000 
	 loss: 84.2505, MinusLogProbMetric: 84.2505, val_loss: 86.1764, val_MinusLogProbMetric: 86.1764

Epoch 190: val_loss did not improve from 83.20744
196/196 - 68s - loss: 84.2505 - MinusLogProbMetric: 84.2505 - val_loss: 86.1764 - val_MinusLogProbMetric: 86.1764 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 191/1000
2023-10-24 18:45:39.666 
Epoch 191/1000 
	 loss: 83.1326, MinusLogProbMetric: 83.1326, val_loss: 83.2057, val_MinusLogProbMetric: 83.2057

Epoch 191: val_loss improved from 83.20744 to 83.20571, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 71s - loss: 83.1326 - MinusLogProbMetric: 83.1326 - val_loss: 83.2057 - val_MinusLogProbMetric: 83.2057 - lr: 1.2346e-05 - 71s/epoch - 360ms/step
Epoch 192/1000
2023-10-24 18:46:49.688 
Epoch 192/1000 
	 loss: 83.2758, MinusLogProbMetric: 83.2758, val_loss: 82.8245, val_MinusLogProbMetric: 82.8245

Epoch 192: val_loss improved from 83.20571 to 82.82454, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 70s - loss: 83.2758 - MinusLogProbMetric: 83.2758 - val_loss: 82.8245 - val_MinusLogProbMetric: 82.8245 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 193/1000
2023-10-24 18:47:59.500 
Epoch 193/1000 
	 loss: 84.9243, MinusLogProbMetric: 84.9243, val_loss: 89.3193, val_MinusLogProbMetric: 89.3193

Epoch 193: val_loss did not improve from 82.82454
196/196 - 69s - loss: 84.9243 - MinusLogProbMetric: 84.9243 - val_loss: 89.3193 - val_MinusLogProbMetric: 89.3193 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 194/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 3: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 18:48:05.051 
Epoch 194/1000 
	 loss: nan, MinusLogProbMetric: 93.5538, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 194: val_loss did not improve from 82.82454
196/196 - 6s - loss: nan - MinusLogProbMetric: 93.5538 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 6s/epoch - 28ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.115226337448558e-06.
===========
Generating train data for run 344.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_344
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_225"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_226 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7fe5ac35b640>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe4efc0e110>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe4efc0e110>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe5058b90c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe54c57dc90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe54c57e200>, <keras.callbacks.ModelCheckpoint object at 0x7fe54c57e2c0>, <keras.callbacks.EarlyStopping object at 0x7fe54c57e530>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe54c57e560>, <keras.callbacks.TerminateOnNaN object at 0x7fe54c57e1a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 344/720 with hyperparameters:
timestamp = 2023-10-24 18:48:13.591407
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
2023-10-24 18:51:50.060 
Epoch 1/1000 
	 loss: 85.1060, MinusLogProbMetric: 85.1060, val_loss: 83.1403, val_MinusLogProbMetric: 83.1403

Epoch 1: val_loss improved from inf to 83.14034, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 217s - loss: 85.1060 - MinusLogProbMetric: 85.1060 - val_loss: 83.1403 - val_MinusLogProbMetric: 83.1403 - lr: 4.1152e-06 - 217s/epoch - 1s/step
Epoch 2/1000
2023-10-24 18:52:59.005 
Epoch 2/1000 
	 loss: 82.6766, MinusLogProbMetric: 82.6766, val_loss: 81.6387, val_MinusLogProbMetric: 81.6387

Epoch 2: val_loss improved from 83.14034 to 81.63869, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 69s - loss: 82.6766 - MinusLogProbMetric: 82.6766 - val_loss: 81.6387 - val_MinusLogProbMetric: 81.6387 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 3/1000
2023-10-24 18:54:11.873 
Epoch 3/1000 
	 loss: 82.6542, MinusLogProbMetric: 82.6542, val_loss: 80.9017, val_MinusLogProbMetric: 80.9017

Epoch 3: val_loss improved from 81.63869 to 80.90171, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 73s - loss: 82.6542 - MinusLogProbMetric: 82.6542 - val_loss: 80.9017 - val_MinusLogProbMetric: 80.9017 - lr: 4.1152e-06 - 73s/epoch - 373ms/step
Epoch 4/1000
2023-10-24 18:55:22.040 
Epoch 4/1000 
	 loss: 121.1565, MinusLogProbMetric: 121.1565, val_loss: 238.7431, val_MinusLogProbMetric: 238.7431

Epoch 4: val_loss did not improve from 80.90171
196/196 - 69s - loss: 121.1565 - MinusLogProbMetric: 121.1565 - val_loss: 238.7431 - val_MinusLogProbMetric: 238.7431 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 5/1000
2023-10-24 18:56:29.064 
Epoch 5/1000 
	 loss: 162.2017, MinusLogProbMetric: 162.2017, val_loss: 131.7363, val_MinusLogProbMetric: 131.7363

Epoch 5: val_loss did not improve from 80.90171
196/196 - 67s - loss: 162.2017 - MinusLogProbMetric: 162.2017 - val_loss: 131.7363 - val_MinusLogProbMetric: 131.7363 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 6/1000
2023-10-24 18:57:38.081 
Epoch 6/1000 
	 loss: 123.5215, MinusLogProbMetric: 123.5215, val_loss: 116.8472, val_MinusLogProbMetric: 116.8472

Epoch 6: val_loss did not improve from 80.90171
196/196 - 69s - loss: 123.5215 - MinusLogProbMetric: 123.5215 - val_loss: 116.8472 - val_MinusLogProbMetric: 116.8472 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 7/1000
2023-10-24 18:58:47.159 
Epoch 7/1000 
	 loss: 115.9924, MinusLogProbMetric: 115.9924, val_loss: 112.3754, val_MinusLogProbMetric: 112.3754

Epoch 7: val_loss did not improve from 80.90171
196/196 - 69s - loss: 115.9924 - MinusLogProbMetric: 115.9924 - val_loss: 112.3754 - val_MinusLogProbMetric: 112.3754 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 8/1000
2023-10-24 18:59:56.500 
Epoch 8/1000 
	 loss: 109.4316, MinusLogProbMetric: 109.4316, val_loss: 106.9260, val_MinusLogProbMetric: 106.9260

Epoch 8: val_loss did not improve from 80.90171
196/196 - 69s - loss: 109.4316 - MinusLogProbMetric: 109.4316 - val_loss: 106.9260 - val_MinusLogProbMetric: 106.9260 - lr: 4.1152e-06 - 69s/epoch - 354ms/step
Epoch 9/1000
2023-10-24 19:01:06.266 
Epoch 9/1000 
	 loss: 105.4086, MinusLogProbMetric: 105.4086, val_loss: 103.1476, val_MinusLogProbMetric: 103.1476

Epoch 9: val_loss did not improve from 80.90171
196/196 - 70s - loss: 105.4086 - MinusLogProbMetric: 105.4086 - val_loss: 103.1476 - val_MinusLogProbMetric: 103.1476 - lr: 4.1152e-06 - 70s/epoch - 356ms/step
Epoch 10/1000
2023-10-24 19:02:14.912 
Epoch 10/1000 
	 loss: 217.8217, MinusLogProbMetric: 217.8217, val_loss: 261.9453, val_MinusLogProbMetric: 261.9453

Epoch 10: val_loss did not improve from 80.90171
196/196 - 69s - loss: 217.8217 - MinusLogProbMetric: 217.8217 - val_loss: 261.9453 - val_MinusLogProbMetric: 261.9453 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 11/1000
2023-10-24 19:03:25.659 
Epoch 11/1000 
	 loss: 210.3054, MinusLogProbMetric: 210.3054, val_loss: 185.2367, val_MinusLogProbMetric: 185.2367

Epoch 11: val_loss did not improve from 80.90171
196/196 - 71s - loss: 210.3054 - MinusLogProbMetric: 210.3054 - val_loss: 185.2367 - val_MinusLogProbMetric: 185.2367 - lr: 4.1152e-06 - 71s/epoch - 361ms/step
Epoch 12/1000
2023-10-24 19:04:36.492 
Epoch 12/1000 
	 loss: 177.5716, MinusLogProbMetric: 177.5716, val_loss: 169.3925, val_MinusLogProbMetric: 169.3925

Epoch 12: val_loss did not improve from 80.90171
196/196 - 71s - loss: 177.5716 - MinusLogProbMetric: 177.5716 - val_loss: 169.3925 - val_MinusLogProbMetric: 169.3925 - lr: 4.1152e-06 - 71s/epoch - 361ms/step
Epoch 13/1000
2023-10-24 19:05:46.834 
Epoch 13/1000 
	 loss: 165.7873, MinusLogProbMetric: 165.7873, val_loss: 160.6817, val_MinusLogProbMetric: 160.6817

Epoch 13: val_loss did not improve from 80.90171
196/196 - 70s - loss: 165.7873 - MinusLogProbMetric: 165.7873 - val_loss: 160.6817 - val_MinusLogProbMetric: 160.6817 - lr: 4.1152e-06 - 70s/epoch - 359ms/step
Epoch 14/1000
2023-10-24 19:06:57.860 
Epoch 14/1000 
	 loss: 158.2849, MinusLogProbMetric: 158.2849, val_loss: 154.2866, val_MinusLogProbMetric: 154.2866

Epoch 14: val_loss did not improve from 80.90171
196/196 - 71s - loss: 158.2849 - MinusLogProbMetric: 158.2849 - val_loss: 154.2866 - val_MinusLogProbMetric: 154.2866 - lr: 4.1152e-06 - 71s/epoch - 362ms/step
Epoch 15/1000
2023-10-24 19:08:08.377 
Epoch 15/1000 
	 loss: 153.4139, MinusLogProbMetric: 153.4139, val_loss: 149.9028, val_MinusLogProbMetric: 149.9028

Epoch 15: val_loss did not improve from 80.90171
196/196 - 71s - loss: 153.4139 - MinusLogProbMetric: 153.4139 - val_loss: 149.9028 - val_MinusLogProbMetric: 149.9028 - lr: 4.1152e-06 - 71s/epoch - 360ms/step
Epoch 16/1000
2023-10-24 19:09:19.974 
Epoch 16/1000 
	 loss: 148.6942, MinusLogProbMetric: 148.6942, val_loss: 146.9541, val_MinusLogProbMetric: 146.9541

Epoch 16: val_loss did not improve from 80.90171
196/196 - 72s - loss: 148.6942 - MinusLogProbMetric: 148.6942 - val_loss: 146.9541 - val_MinusLogProbMetric: 146.9541 - lr: 4.1152e-06 - 72s/epoch - 365ms/step
Epoch 17/1000
2023-10-24 19:10:27.181 
Epoch 17/1000 
	 loss: 144.8775, MinusLogProbMetric: 144.8775, val_loss: 142.5169, val_MinusLogProbMetric: 142.5169

Epoch 17: val_loss did not improve from 80.90171
196/196 - 67s - loss: 144.8775 - MinusLogProbMetric: 144.8775 - val_loss: 142.5169 - val_MinusLogProbMetric: 142.5169 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 18/1000
2023-10-24 19:11:35.847 
Epoch 18/1000 
	 loss: 142.0380, MinusLogProbMetric: 142.0380, val_loss: 140.1502, val_MinusLogProbMetric: 140.1502

Epoch 18: val_loss did not improve from 80.90171
196/196 - 69s - loss: 142.0380 - MinusLogProbMetric: 142.0380 - val_loss: 140.1502 - val_MinusLogProbMetric: 140.1502 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 19/1000
2023-10-24 19:12:43.820 
Epoch 19/1000 
	 loss: 140.5998, MinusLogProbMetric: 140.5998, val_loss: 137.7363, val_MinusLogProbMetric: 137.7363

Epoch 19: val_loss did not improve from 80.90171
196/196 - 68s - loss: 140.5998 - MinusLogProbMetric: 140.5998 - val_loss: 137.7363 - val_MinusLogProbMetric: 137.7363 - lr: 4.1152e-06 - 68s/epoch - 347ms/step
Epoch 20/1000
2023-10-24 19:13:52.733 
Epoch 20/1000 
	 loss: 137.9306, MinusLogProbMetric: 137.9306, val_loss: 135.8272, val_MinusLogProbMetric: 135.8272

Epoch 20: val_loss did not improve from 80.90171
196/196 - 69s - loss: 137.9306 - MinusLogProbMetric: 137.9306 - val_loss: 135.8272 - val_MinusLogProbMetric: 135.8272 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 21/1000
2023-10-24 19:15:04.421 
Epoch 21/1000 
	 loss: 135.9634, MinusLogProbMetric: 135.9634, val_loss: 134.0128, val_MinusLogProbMetric: 134.0128

Epoch 21: val_loss did not improve from 80.90171
196/196 - 72s - loss: 135.9634 - MinusLogProbMetric: 135.9634 - val_loss: 134.0128 - val_MinusLogProbMetric: 134.0128 - lr: 4.1152e-06 - 72s/epoch - 366ms/step
Epoch 22/1000
2023-10-24 19:16:13.778 
Epoch 22/1000 
	 loss: 133.6577, MinusLogProbMetric: 133.6577, val_loss: 132.2208, val_MinusLogProbMetric: 132.2208

Epoch 22: val_loss did not improve from 80.90171
196/196 - 69s - loss: 133.6577 - MinusLogProbMetric: 133.6577 - val_loss: 132.2208 - val_MinusLogProbMetric: 132.2208 - lr: 4.1152e-06 - 69s/epoch - 354ms/step
Epoch 23/1000
2023-10-24 19:17:23.194 
Epoch 23/1000 
	 loss: 135.6840, MinusLogProbMetric: 135.6840, val_loss: 135.6659, val_MinusLogProbMetric: 135.6659

Epoch 23: val_loss did not improve from 80.90171
196/196 - 69s - loss: 135.6840 - MinusLogProbMetric: 135.6840 - val_loss: 135.6659 - val_MinusLogProbMetric: 135.6659 - lr: 4.1152e-06 - 69s/epoch - 354ms/step
Epoch 24/1000
2023-10-24 19:18:31.731 
Epoch 24/1000 
	 loss: 132.5511, MinusLogProbMetric: 132.5511, val_loss: 130.8108, val_MinusLogProbMetric: 130.8108

Epoch 24: val_loss did not improve from 80.90171
196/196 - 69s - loss: 132.5511 - MinusLogProbMetric: 132.5511 - val_loss: 130.8108 - val_MinusLogProbMetric: 130.8108 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 25/1000
2023-10-24 19:19:40.656 
Epoch 25/1000 
	 loss: 130.1537, MinusLogProbMetric: 130.1537, val_loss: 128.8069, val_MinusLogProbMetric: 128.8069

Epoch 25: val_loss did not improve from 80.90171
196/196 - 69s - loss: 130.1537 - MinusLogProbMetric: 130.1537 - val_loss: 128.8069 - val_MinusLogProbMetric: 128.8069 - lr: 4.1152e-06 - 69s/epoch - 352ms/step
Epoch 26/1000
2023-10-24 19:20:49.237 
Epoch 26/1000 
	 loss: 128.4293, MinusLogProbMetric: 128.4293, val_loss: 126.9565, val_MinusLogProbMetric: 126.9565

Epoch 26: val_loss did not improve from 80.90171
196/196 - 69s - loss: 128.4293 - MinusLogProbMetric: 128.4293 - val_loss: 126.9565 - val_MinusLogProbMetric: 126.9565 - lr: 4.1152e-06 - 69s/epoch - 350ms/step
Epoch 27/1000
2023-10-24 19:21:58.877 
Epoch 27/1000 
	 loss: 126.6605, MinusLogProbMetric: 126.6605, val_loss: 125.5500, val_MinusLogProbMetric: 125.5500

Epoch 27: val_loss did not improve from 80.90171
196/196 - 70s - loss: 126.6605 - MinusLogProbMetric: 126.6605 - val_loss: 125.5500 - val_MinusLogProbMetric: 125.5500 - lr: 4.1152e-06 - 70s/epoch - 355ms/step
Epoch 28/1000
2023-10-24 19:23:06.258 
Epoch 28/1000 
	 loss: 125.5633, MinusLogProbMetric: 125.5633, val_loss: 124.7353, val_MinusLogProbMetric: 124.7353

Epoch 28: val_loss did not improve from 80.90171
196/196 - 67s - loss: 125.5633 - MinusLogProbMetric: 125.5633 - val_loss: 124.7353 - val_MinusLogProbMetric: 124.7353 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 29/1000
2023-10-24 19:24:14.695 
Epoch 29/1000 
	 loss: 124.5850, MinusLogProbMetric: 124.5850, val_loss: 123.4931, val_MinusLogProbMetric: 123.4931

Epoch 29: val_loss did not improve from 80.90171
196/196 - 68s - loss: 124.5850 - MinusLogProbMetric: 124.5850 - val_loss: 123.4931 - val_MinusLogProbMetric: 123.4931 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 30/1000
2023-10-24 19:25:22.065 
Epoch 30/1000 
	 loss: 123.5377, MinusLogProbMetric: 123.5377, val_loss: 122.4294, val_MinusLogProbMetric: 122.4294

Epoch 30: val_loss did not improve from 80.90171
196/196 - 67s - loss: 123.5377 - MinusLogProbMetric: 123.5377 - val_loss: 122.4294 - val_MinusLogProbMetric: 122.4294 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 31/1000
2023-10-24 19:26:28.482 
Epoch 31/1000 
	 loss: 122.4119, MinusLogProbMetric: 122.4119, val_loss: 121.6950, val_MinusLogProbMetric: 121.6950

Epoch 31: val_loss did not improve from 80.90171
196/196 - 66s - loss: 122.4119 - MinusLogProbMetric: 122.4119 - val_loss: 121.6950 - val_MinusLogProbMetric: 121.6950 - lr: 4.1152e-06 - 66s/epoch - 339ms/step
Epoch 32/1000
2023-10-24 19:27:36.330 
Epoch 32/1000 
	 loss: 121.5357, MinusLogProbMetric: 121.5357, val_loss: 120.5358, val_MinusLogProbMetric: 120.5358

Epoch 32: val_loss did not improve from 80.90171
196/196 - 68s - loss: 121.5357 - MinusLogProbMetric: 121.5357 - val_loss: 120.5358 - val_MinusLogProbMetric: 120.5358 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 33/1000
2023-10-24 19:28:41.551 
Epoch 33/1000 
	 loss: 120.7680, MinusLogProbMetric: 120.7680, val_loss: 119.7693, val_MinusLogProbMetric: 119.7693

Epoch 33: val_loss did not improve from 80.90171
196/196 - 65s - loss: 120.7680 - MinusLogProbMetric: 120.7680 - val_loss: 119.7693 - val_MinusLogProbMetric: 119.7693 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 34/1000
2023-10-24 19:29:48.498 
Epoch 34/1000 
	 loss: 119.8396, MinusLogProbMetric: 119.8396, val_loss: 118.9743, val_MinusLogProbMetric: 118.9743

Epoch 34: val_loss did not improve from 80.90171
196/196 - 67s - loss: 119.8396 - MinusLogProbMetric: 119.8396 - val_loss: 118.9743 - val_MinusLogProbMetric: 118.9743 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 35/1000
2023-10-24 19:30:56.677 
Epoch 35/1000 
	 loss: 118.9747, MinusLogProbMetric: 118.9747, val_loss: 118.2374, val_MinusLogProbMetric: 118.2374

Epoch 35: val_loss did not improve from 80.90171
196/196 - 68s - loss: 118.9747 - MinusLogProbMetric: 118.9747 - val_loss: 118.2374 - val_MinusLogProbMetric: 118.2374 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 36/1000
2023-10-24 19:32:06.191 
Epoch 36/1000 
	 loss: 118.2895, MinusLogProbMetric: 118.2895, val_loss: 117.4354, val_MinusLogProbMetric: 117.4354

Epoch 36: val_loss did not improve from 80.90171
196/196 - 70s - loss: 118.2895 - MinusLogProbMetric: 118.2895 - val_loss: 117.4354 - val_MinusLogProbMetric: 117.4354 - lr: 4.1152e-06 - 70s/epoch - 355ms/step
Epoch 37/1000
2023-10-24 19:33:13.940 
Epoch 37/1000 
	 loss: 117.4870, MinusLogProbMetric: 117.4870, val_loss: 116.7865, val_MinusLogProbMetric: 116.7865

Epoch 37: val_loss did not improve from 80.90171
196/196 - 68s - loss: 117.4870 - MinusLogProbMetric: 117.4870 - val_loss: 116.7865 - val_MinusLogProbMetric: 116.7865 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 38/1000
2023-10-24 19:34:19.184 
Epoch 38/1000 
	 loss: 116.7048, MinusLogProbMetric: 116.7048, val_loss: 116.0237, val_MinusLogProbMetric: 116.0237

Epoch 38: val_loss did not improve from 80.90171
196/196 - 65s - loss: 116.7048 - MinusLogProbMetric: 116.7048 - val_loss: 116.0237 - val_MinusLogProbMetric: 116.0237 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 39/1000
2023-10-24 19:35:27.483 
Epoch 39/1000 
	 loss: 116.0581, MinusLogProbMetric: 116.0581, val_loss: 115.4407, val_MinusLogProbMetric: 115.4407

Epoch 39: val_loss did not improve from 80.90171
196/196 - 68s - loss: 116.0581 - MinusLogProbMetric: 116.0581 - val_loss: 115.4407 - val_MinusLogProbMetric: 115.4407 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 40/1000
2023-10-24 19:36:34.500 
Epoch 40/1000 
	 loss: 115.4357, MinusLogProbMetric: 115.4357, val_loss: 114.6890, val_MinusLogProbMetric: 114.6890

Epoch 40: val_loss did not improve from 80.90171
196/196 - 67s - loss: 115.4357 - MinusLogProbMetric: 115.4357 - val_loss: 114.6890 - val_MinusLogProbMetric: 114.6890 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 41/1000
2023-10-24 19:37:41.621 
Epoch 41/1000 
	 loss: 114.6513, MinusLogProbMetric: 114.6513, val_loss: 113.9391, val_MinusLogProbMetric: 113.9391

Epoch 41: val_loss did not improve from 80.90171
196/196 - 67s - loss: 114.6513 - MinusLogProbMetric: 114.6513 - val_loss: 113.9391 - val_MinusLogProbMetric: 113.9391 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 42/1000
2023-10-24 19:38:49.241 
Epoch 42/1000 
	 loss: 113.8889, MinusLogProbMetric: 113.8889, val_loss: 113.2910, val_MinusLogProbMetric: 113.2910

Epoch 42: val_loss did not improve from 80.90171
196/196 - 68s - loss: 113.8889 - MinusLogProbMetric: 113.8889 - val_loss: 113.2910 - val_MinusLogProbMetric: 113.2910 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 43/1000
2023-10-24 19:39:56.208 
Epoch 43/1000 
	 loss: 113.2018, MinusLogProbMetric: 113.2018, val_loss: 112.3061, val_MinusLogProbMetric: 112.3061

Epoch 43: val_loss did not improve from 80.90171
196/196 - 67s - loss: 113.2018 - MinusLogProbMetric: 113.2018 - val_loss: 112.3061 - val_MinusLogProbMetric: 112.3061 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 44/1000
2023-10-24 19:41:03.564 
Epoch 44/1000 
	 loss: 112.2941, MinusLogProbMetric: 112.2941, val_loss: 111.3766, val_MinusLogProbMetric: 111.3766

Epoch 44: val_loss did not improve from 80.90171
196/196 - 67s - loss: 112.2941 - MinusLogProbMetric: 112.2941 - val_loss: 111.3766 - val_MinusLogProbMetric: 111.3766 - lr: 4.1152e-06 - 67s/epoch - 344ms/step
Epoch 45/1000
2023-10-24 19:42:10.350 
Epoch 45/1000 
	 loss: 112.3248, MinusLogProbMetric: 112.3248, val_loss: 121.6977, val_MinusLogProbMetric: 121.6977

Epoch 45: val_loss did not improve from 80.90171
196/196 - 67s - loss: 112.3248 - MinusLogProbMetric: 112.3248 - val_loss: 121.6977 - val_MinusLogProbMetric: 121.6977 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 46/1000
2023-10-24 19:43:18.732 
Epoch 46/1000 
	 loss: 114.3718, MinusLogProbMetric: 114.3718, val_loss: 111.9423, val_MinusLogProbMetric: 111.9423

Epoch 46: val_loss did not improve from 80.90171
196/196 - 68s - loss: 114.3718 - MinusLogProbMetric: 114.3718 - val_loss: 111.9423 - val_MinusLogProbMetric: 111.9423 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 47/1000
2023-10-24 19:44:26.352 
Epoch 47/1000 
	 loss: 110.8151, MinusLogProbMetric: 110.8151, val_loss: 109.9874, val_MinusLogProbMetric: 109.9874

Epoch 47: val_loss did not improve from 80.90171
196/196 - 68s - loss: 110.8151 - MinusLogProbMetric: 110.8151 - val_loss: 109.9874 - val_MinusLogProbMetric: 109.9874 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 48/1000
2023-10-24 19:45:35.971 
Epoch 48/1000 
	 loss: 119.5145, MinusLogProbMetric: 119.5145, val_loss: 115.2110, val_MinusLogProbMetric: 115.2110

Epoch 48: val_loss did not improve from 80.90171
196/196 - 70s - loss: 119.5145 - MinusLogProbMetric: 119.5145 - val_loss: 115.2110 - val_MinusLogProbMetric: 115.2110 - lr: 4.1152e-06 - 70s/epoch - 355ms/step
Epoch 49/1000
2023-10-24 19:46:42.636 
Epoch 49/1000 
	 loss: 113.3240, MinusLogProbMetric: 113.3240, val_loss: 111.5695, val_MinusLogProbMetric: 111.5695

Epoch 49: val_loss did not improve from 80.90171
196/196 - 67s - loss: 113.3240 - MinusLogProbMetric: 113.3240 - val_loss: 111.5695 - val_MinusLogProbMetric: 111.5695 - lr: 4.1152e-06 - 67s/epoch - 340ms/step
Epoch 50/1000
2023-10-24 19:47:50.917 
Epoch 50/1000 
	 loss: 111.1197, MinusLogProbMetric: 111.1197, val_loss: 110.0787, val_MinusLogProbMetric: 110.0787

Epoch 50: val_loss did not improve from 80.90171
196/196 - 68s - loss: 111.1197 - MinusLogProbMetric: 111.1197 - val_loss: 110.0787 - val_MinusLogProbMetric: 110.0787 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 51/1000
2023-10-24 19:48:57.870 
Epoch 51/1000 
	 loss: 109.9503, MinusLogProbMetric: 109.9503, val_loss: 108.9899, val_MinusLogProbMetric: 108.9899

Epoch 51: val_loss did not improve from 80.90171
196/196 - 67s - loss: 109.9503 - MinusLogProbMetric: 109.9503 - val_loss: 108.9899 - val_MinusLogProbMetric: 108.9899 - lr: 4.1152e-06 - 67s/epoch - 342ms/step
Epoch 52/1000
2023-10-24 19:50:07.291 
Epoch 52/1000 
	 loss: 119.9271, MinusLogProbMetric: 119.9271, val_loss: 115.3328, val_MinusLogProbMetric: 115.3328

Epoch 52: val_loss did not improve from 80.90171
196/196 - 69s - loss: 119.9271 - MinusLogProbMetric: 119.9271 - val_loss: 115.3328 - val_MinusLogProbMetric: 115.3328 - lr: 4.1152e-06 - 69s/epoch - 354ms/step
Epoch 53/1000
2023-10-24 19:51:15.461 
Epoch 53/1000 
	 loss: 109.6140, MinusLogProbMetric: 109.6140, val_loss: 106.0457, val_MinusLogProbMetric: 106.0457

Epoch 53: val_loss did not improve from 80.90171
196/196 - 68s - loss: 109.6140 - MinusLogProbMetric: 109.6140 - val_loss: 106.0457 - val_MinusLogProbMetric: 106.0457 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 54/1000
2023-10-24 19:52:21.595 
Epoch 54/1000 
	 loss: 105.1720, MinusLogProbMetric: 105.1720, val_loss: 104.4729, val_MinusLogProbMetric: 104.4729

Epoch 54: val_loss did not improve from 80.90171
196/196 - 66s - loss: 105.1720 - MinusLogProbMetric: 105.1720 - val_loss: 104.4729 - val_MinusLogProbMetric: 104.4729 - lr: 2.0576e-06 - 66s/epoch - 337ms/step
Epoch 55/1000
2023-10-24 19:53:30.504 
Epoch 55/1000 
	 loss: 103.6430, MinusLogProbMetric: 103.6430, val_loss: 102.4677, val_MinusLogProbMetric: 102.4677

Epoch 55: val_loss did not improve from 80.90171
196/196 - 69s - loss: 103.6430 - MinusLogProbMetric: 103.6430 - val_loss: 102.4677 - val_MinusLogProbMetric: 102.4677 - lr: 2.0576e-06 - 69s/epoch - 352ms/step
Epoch 56/1000
2023-10-24 19:54:38.241 
Epoch 56/1000 
	 loss: 102.2503, MinusLogProbMetric: 102.2503, val_loss: 101.6156, val_MinusLogProbMetric: 101.6156

Epoch 56: val_loss did not improve from 80.90171
196/196 - 68s - loss: 102.2503 - MinusLogProbMetric: 102.2503 - val_loss: 101.6156 - val_MinusLogProbMetric: 101.6156 - lr: 2.0576e-06 - 68s/epoch - 346ms/step
Epoch 57/1000
2023-10-24 19:55:46.846 
Epoch 57/1000 
	 loss: 130.2958, MinusLogProbMetric: 130.2958, val_loss: 166.5449, val_MinusLogProbMetric: 166.5449

Epoch 57: val_loss did not improve from 80.90171
196/196 - 69s - loss: 130.2958 - MinusLogProbMetric: 130.2958 - val_loss: 166.5449 - val_MinusLogProbMetric: 166.5449 - lr: 2.0576e-06 - 69s/epoch - 350ms/step
Epoch 58/1000
2023-10-24 19:56:51.595 
Epoch 58/1000 
	 loss: 140.0259, MinusLogProbMetric: 140.0259, val_loss: 127.3291, val_MinusLogProbMetric: 127.3291

Epoch 58: val_loss did not improve from 80.90171
196/196 - 65s - loss: 140.0259 - MinusLogProbMetric: 140.0259 - val_loss: 127.3291 - val_MinusLogProbMetric: 127.3291 - lr: 2.0576e-06 - 65s/epoch - 330ms/step
Epoch 59/1000
2023-10-24 19:57:59.987 
Epoch 59/1000 
	 loss: 123.3112, MinusLogProbMetric: 123.3112, val_loss: 119.7229, val_MinusLogProbMetric: 119.7229

Epoch 59: val_loss did not improve from 80.90171
196/196 - 68s - loss: 123.3112 - MinusLogProbMetric: 123.3112 - val_loss: 119.7229 - val_MinusLogProbMetric: 119.7229 - lr: 2.0576e-06 - 68s/epoch - 349ms/step
Epoch 60/1000
2023-10-24 19:59:07.307 
Epoch 60/1000 
	 loss: 118.1292, MinusLogProbMetric: 118.1292, val_loss: 115.8624, val_MinusLogProbMetric: 115.8624

Epoch 60: val_loss did not improve from 80.90171
196/196 - 67s - loss: 118.1292 - MinusLogProbMetric: 118.1292 - val_loss: 115.8624 - val_MinusLogProbMetric: 115.8624 - lr: 2.0576e-06 - 67s/epoch - 343ms/step
Epoch 61/1000
2023-10-24 20:00:16.586 
Epoch 61/1000 
	 loss: 114.8965, MinusLogProbMetric: 114.8965, val_loss: 112.8050, val_MinusLogProbMetric: 112.8050

Epoch 61: val_loss did not improve from 80.90171
196/196 - 69s - loss: 114.8965 - MinusLogProbMetric: 114.8965 - val_loss: 112.8050 - val_MinusLogProbMetric: 112.8050 - lr: 2.0576e-06 - 69s/epoch - 353ms/step
Epoch 62/1000
2023-10-24 20:01:25.111 
Epoch 62/1000 
	 loss: 112.3050, MinusLogProbMetric: 112.3050, val_loss: 111.0357, val_MinusLogProbMetric: 111.0357

Epoch 62: val_loss did not improve from 80.90171
196/196 - 69s - loss: 112.3050 - MinusLogProbMetric: 112.3050 - val_loss: 111.0357 - val_MinusLogProbMetric: 111.0357 - lr: 2.0576e-06 - 69s/epoch - 350ms/step
Epoch 63/1000
2023-10-24 20:02:31.278 
Epoch 63/1000 
	 loss: 110.0595, MinusLogProbMetric: 110.0595, val_loss: 108.8516, val_MinusLogProbMetric: 108.8516

Epoch 63: val_loss did not improve from 80.90171
196/196 - 66s - loss: 110.0595 - MinusLogProbMetric: 110.0595 - val_loss: 108.8516 - val_MinusLogProbMetric: 108.8516 - lr: 2.0576e-06 - 66s/epoch - 338ms/step
Epoch 64/1000
2023-10-24 20:03:37.508 
Epoch 64/1000 
	 loss: 108.3329, MinusLogProbMetric: 108.3329, val_loss: 107.2886, val_MinusLogProbMetric: 107.2886

Epoch 64: val_loss did not improve from 80.90171
196/196 - 66s - loss: 108.3329 - MinusLogProbMetric: 108.3329 - val_loss: 107.2886 - val_MinusLogProbMetric: 107.2886 - lr: 2.0576e-06 - 66s/epoch - 338ms/step
Epoch 65/1000
2023-10-24 20:04:42.901 
Epoch 65/1000 
	 loss: 107.0057, MinusLogProbMetric: 107.0057, val_loss: 106.0771, val_MinusLogProbMetric: 106.0771

Epoch 65: val_loss did not improve from 80.90171
196/196 - 65s - loss: 107.0057 - MinusLogProbMetric: 107.0057 - val_loss: 106.0771 - val_MinusLogProbMetric: 106.0771 - lr: 2.0576e-06 - 65s/epoch - 334ms/step
Epoch 66/1000
2023-10-24 20:05:53.136 
Epoch 66/1000 
	 loss: 105.9547, MinusLogProbMetric: 105.9547, val_loss: 105.1428, val_MinusLogProbMetric: 105.1428

Epoch 66: val_loss did not improve from 80.90171
196/196 - 70s - loss: 105.9547 - MinusLogProbMetric: 105.9547 - val_loss: 105.1428 - val_MinusLogProbMetric: 105.1428 - lr: 2.0576e-06 - 70s/epoch - 358ms/step
Epoch 67/1000
2023-10-24 20:07:02.614 
Epoch 67/1000 
	 loss: 105.1342, MinusLogProbMetric: 105.1342, val_loss: 104.4690, val_MinusLogProbMetric: 104.4690

Epoch 67: val_loss did not improve from 80.90171
196/196 - 69s - loss: 105.1342 - MinusLogProbMetric: 105.1342 - val_loss: 104.4690 - val_MinusLogProbMetric: 104.4690 - lr: 2.0576e-06 - 69s/epoch - 354ms/step
Epoch 68/1000
2023-10-24 20:08:10.675 
Epoch 68/1000 
	 loss: 104.4981, MinusLogProbMetric: 104.4981, val_loss: 103.7640, val_MinusLogProbMetric: 103.7640

Epoch 68: val_loss did not improve from 80.90171
196/196 - 68s - loss: 104.4981 - MinusLogProbMetric: 104.4981 - val_loss: 103.7640 - val_MinusLogProbMetric: 103.7640 - lr: 2.0576e-06 - 68s/epoch - 347ms/step
Epoch 69/1000
2023-10-24 20:09:17.476 
Epoch 69/1000 
	 loss: 103.7141, MinusLogProbMetric: 103.7141, val_loss: 103.1100, val_MinusLogProbMetric: 103.1100

Epoch 69: val_loss did not improve from 80.90171
196/196 - 67s - loss: 103.7141 - MinusLogProbMetric: 103.7141 - val_loss: 103.1100 - val_MinusLogProbMetric: 103.1100 - lr: 2.0576e-06 - 67s/epoch - 341ms/step
Epoch 70/1000
2023-10-24 20:10:26.141 
Epoch 70/1000 
	 loss: 103.1871, MinusLogProbMetric: 103.1871, val_loss: 102.4963, val_MinusLogProbMetric: 102.4963

Epoch 70: val_loss did not improve from 80.90171
196/196 - 69s - loss: 103.1871 - MinusLogProbMetric: 103.1871 - val_loss: 102.4963 - val_MinusLogProbMetric: 102.4963 - lr: 2.0576e-06 - 69s/epoch - 350ms/step
Epoch 71/1000
2023-10-24 20:11:35.280 
Epoch 71/1000 
	 loss: 102.4727, MinusLogProbMetric: 102.4727, val_loss: 101.9663, val_MinusLogProbMetric: 101.9663

Epoch 71: val_loss did not improve from 80.90171
196/196 - 69s - loss: 102.4727 - MinusLogProbMetric: 102.4727 - val_loss: 101.9663 - val_MinusLogProbMetric: 101.9663 - lr: 2.0576e-06 - 69s/epoch - 353ms/step
Epoch 72/1000
2023-10-24 20:12:44.187 
Epoch 72/1000 
	 loss: 101.9706, MinusLogProbMetric: 101.9706, val_loss: 101.4898, val_MinusLogProbMetric: 101.4898

Epoch 72: val_loss did not improve from 80.90171
196/196 - 69s - loss: 101.9706 - MinusLogProbMetric: 101.9706 - val_loss: 101.4898 - val_MinusLogProbMetric: 101.4898 - lr: 2.0576e-06 - 69s/epoch - 352ms/step
Epoch 73/1000
2023-10-24 20:13:53.334 
Epoch 73/1000 
	 loss: 101.5668, MinusLogProbMetric: 101.5668, val_loss: 101.0857, val_MinusLogProbMetric: 101.0857

Epoch 73: val_loss did not improve from 80.90171
196/196 - 69s - loss: 101.5668 - MinusLogProbMetric: 101.5668 - val_loss: 101.0857 - val_MinusLogProbMetric: 101.0857 - lr: 2.0576e-06 - 69s/epoch - 353ms/step
Epoch 74/1000
2023-10-24 20:15:00.435 
Epoch 74/1000 
	 loss: 101.2987, MinusLogProbMetric: 101.2987, val_loss: 100.7525, val_MinusLogProbMetric: 100.7525

Epoch 74: val_loss did not improve from 80.90171
196/196 - 67s - loss: 101.2987 - MinusLogProbMetric: 101.2987 - val_loss: 100.7525 - val_MinusLogProbMetric: 100.7525 - lr: 2.0576e-06 - 67s/epoch - 342ms/step
Epoch 75/1000
2023-10-24 20:16:09.201 
Epoch 75/1000 
	 loss: 100.8461, MinusLogProbMetric: 100.8461, val_loss: 100.7003, val_MinusLogProbMetric: 100.7003

Epoch 75: val_loss did not improve from 80.90171
196/196 - 69s - loss: 100.8461 - MinusLogProbMetric: 100.8461 - val_loss: 100.7003 - val_MinusLogProbMetric: 100.7003 - lr: 2.0576e-06 - 69s/epoch - 351ms/step
Epoch 76/1000
2023-10-24 20:17:17.835 
Epoch 76/1000 
	 loss: 100.5176, MinusLogProbMetric: 100.5176, val_loss: 100.0950, val_MinusLogProbMetric: 100.0950

Epoch 76: val_loss did not improve from 80.90171
196/196 - 69s - loss: 100.5176 - MinusLogProbMetric: 100.5176 - val_loss: 100.0950 - val_MinusLogProbMetric: 100.0950 - lr: 2.0576e-06 - 69s/epoch - 350ms/step
Epoch 77/1000
2023-10-24 20:18:25.965 
Epoch 77/1000 
	 loss: 100.1245, MinusLogProbMetric: 100.1245, val_loss: 99.7713, val_MinusLogProbMetric: 99.7713

Epoch 77: val_loss did not improve from 80.90171
196/196 - 68s - loss: 100.1245 - MinusLogProbMetric: 100.1245 - val_loss: 99.7713 - val_MinusLogProbMetric: 99.7713 - lr: 2.0576e-06 - 68s/epoch - 348ms/step
Epoch 78/1000
2023-10-24 20:19:33.423 
Epoch 78/1000 
	 loss: 99.8228, MinusLogProbMetric: 99.8228, val_loss: 99.4459, val_MinusLogProbMetric: 99.4459

Epoch 78: val_loss did not improve from 80.90171
196/196 - 67s - loss: 99.8228 - MinusLogProbMetric: 99.8228 - val_loss: 99.4459 - val_MinusLogProbMetric: 99.4459 - lr: 2.0576e-06 - 67s/epoch - 344ms/step
Epoch 79/1000
2023-10-24 20:20:40.927 
Epoch 79/1000 
	 loss: 99.5597, MinusLogProbMetric: 99.5597, val_loss: 99.1449, val_MinusLogProbMetric: 99.1449

Epoch 79: val_loss did not improve from 80.90171
196/196 - 68s - loss: 99.5597 - MinusLogProbMetric: 99.5597 - val_loss: 99.1449 - val_MinusLogProbMetric: 99.1449 - lr: 2.0576e-06 - 68s/epoch - 344ms/step
Epoch 80/1000
2023-10-24 20:21:50.724 
Epoch 80/1000 
	 loss: 99.3042, MinusLogProbMetric: 99.3042, val_loss: 98.9687, val_MinusLogProbMetric: 98.9687

Epoch 80: val_loss did not improve from 80.90171
196/196 - 70s - loss: 99.3042 - MinusLogProbMetric: 99.3042 - val_loss: 98.9687 - val_MinusLogProbMetric: 98.9687 - lr: 2.0576e-06 - 70s/epoch - 356ms/step
Epoch 81/1000
2023-10-24 20:22:59.984 
Epoch 81/1000 
	 loss: 99.0010, MinusLogProbMetric: 99.0010, val_loss: 98.7005, val_MinusLogProbMetric: 98.7005

Epoch 81: val_loss did not improve from 80.90171
196/196 - 69s - loss: 99.0010 - MinusLogProbMetric: 99.0010 - val_loss: 98.7005 - val_MinusLogProbMetric: 98.7005 - lr: 2.0576e-06 - 69s/epoch - 353ms/step
Epoch 82/1000
2023-10-24 20:24:08.505 
Epoch 82/1000 
	 loss: 98.7498, MinusLogProbMetric: 98.7498, val_loss: 98.4093, val_MinusLogProbMetric: 98.4093

Epoch 82: val_loss did not improve from 80.90171
196/196 - 69s - loss: 98.7498 - MinusLogProbMetric: 98.7498 - val_loss: 98.4093 - val_MinusLogProbMetric: 98.4093 - lr: 2.0576e-06 - 69s/epoch - 350ms/step
Epoch 83/1000
2023-10-24 20:25:15.976 
Epoch 83/1000 
	 loss: 98.5213, MinusLogProbMetric: 98.5213, val_loss: 98.1937, val_MinusLogProbMetric: 98.1937

Epoch 83: val_loss did not improve from 80.90171
196/196 - 67s - loss: 98.5213 - MinusLogProbMetric: 98.5213 - val_loss: 98.1937 - val_MinusLogProbMetric: 98.1937 - lr: 2.0576e-06 - 67s/epoch - 344ms/step
Epoch 84/1000
2023-10-24 20:26:24.241 
Epoch 84/1000 
	 loss: 98.3002, MinusLogProbMetric: 98.3002, val_loss: 97.9629, val_MinusLogProbMetric: 97.9629

Epoch 84: val_loss did not improve from 80.90171
196/196 - 68s - loss: 98.3002 - MinusLogProbMetric: 98.3002 - val_loss: 97.9629 - val_MinusLogProbMetric: 97.9629 - lr: 2.0576e-06 - 68s/epoch - 348ms/step
Epoch 85/1000
2023-10-24 20:27:34.026 
Epoch 85/1000 
	 loss: 98.0708, MinusLogProbMetric: 98.0708, val_loss: 97.7246, val_MinusLogProbMetric: 97.7246

Epoch 85: val_loss did not improve from 80.90171
196/196 - 70s - loss: 98.0708 - MinusLogProbMetric: 98.0708 - val_loss: 97.7246 - val_MinusLogProbMetric: 97.7246 - lr: 2.0576e-06 - 70s/epoch - 356ms/step
Epoch 86/1000
2023-10-24 20:28:46.107 
Epoch 86/1000 
	 loss: 97.9029, MinusLogProbMetric: 97.9029, val_loss: 97.6213, val_MinusLogProbMetric: 97.6213

Epoch 86: val_loss did not improve from 80.90171
196/196 - 72s - loss: 97.9029 - MinusLogProbMetric: 97.9029 - val_loss: 97.6213 - val_MinusLogProbMetric: 97.6213 - lr: 2.0576e-06 - 72s/epoch - 368ms/step
Epoch 87/1000
2023-10-24 20:29:54.518 
Epoch 87/1000 
	 loss: 97.6734, MinusLogProbMetric: 97.6734, val_loss: 97.3251, val_MinusLogProbMetric: 97.3251

Epoch 87: val_loss did not improve from 80.90171
196/196 - 68s - loss: 97.6734 - MinusLogProbMetric: 97.6734 - val_loss: 97.3251 - val_MinusLogProbMetric: 97.3251 - lr: 2.0576e-06 - 68s/epoch - 349ms/step
Epoch 88/1000
2023-10-24 20:31:04.911 
Epoch 88/1000 
	 loss: 97.4933, MinusLogProbMetric: 97.4933, val_loss: 97.1445, val_MinusLogProbMetric: 97.1445

Epoch 88: val_loss did not improve from 80.90171
196/196 - 70s - loss: 97.4933 - MinusLogProbMetric: 97.4933 - val_loss: 97.1445 - val_MinusLogProbMetric: 97.1445 - lr: 2.0576e-06 - 70s/epoch - 359ms/step
Epoch 89/1000
2023-10-24 20:32:10.215 
Epoch 89/1000 
	 loss: 97.2920, MinusLogProbMetric: 97.2920, val_loss: 97.0459, val_MinusLogProbMetric: 97.0459

Epoch 89: val_loss did not improve from 80.90171
196/196 - 65s - loss: 97.2920 - MinusLogProbMetric: 97.2920 - val_loss: 97.0459 - val_MinusLogProbMetric: 97.0459 - lr: 2.0576e-06 - 65s/epoch - 333ms/step
Epoch 90/1000
2023-10-24 20:33:16.547 
Epoch 90/1000 
	 loss: 97.0918, MinusLogProbMetric: 97.0918, val_loss: 96.8154, val_MinusLogProbMetric: 96.8154

Epoch 90: val_loss did not improve from 80.90171
196/196 - 66s - loss: 97.0918 - MinusLogProbMetric: 97.0918 - val_loss: 96.8154 - val_MinusLogProbMetric: 96.8154 - lr: 2.0576e-06 - 66s/epoch - 338ms/step
Epoch 91/1000
2023-10-24 20:34:24.465 
Epoch 91/1000 
	 loss: 97.0968, MinusLogProbMetric: 97.0968, val_loss: 97.1807, val_MinusLogProbMetric: 97.1807

Epoch 91: val_loss did not improve from 80.90171
196/196 - 68s - loss: 97.0968 - MinusLogProbMetric: 97.0968 - val_loss: 97.1807 - val_MinusLogProbMetric: 97.1807 - lr: 2.0576e-06 - 68s/epoch - 347ms/step
Epoch 92/1000
2023-10-24 20:35:31.654 
Epoch 92/1000 
	 loss: 96.8383, MinusLogProbMetric: 96.8383, val_loss: 96.3976, val_MinusLogProbMetric: 96.3976

Epoch 92: val_loss did not improve from 80.90171
196/196 - 67s - loss: 96.8383 - MinusLogProbMetric: 96.8383 - val_loss: 96.3976 - val_MinusLogProbMetric: 96.3976 - lr: 2.0576e-06 - 67s/epoch - 343ms/step
Epoch 93/1000
2023-10-24 20:36:41.805 
Epoch 93/1000 
	 loss: 96.5347, MinusLogProbMetric: 96.5347, val_loss: 96.2742, val_MinusLogProbMetric: 96.2742

Epoch 93: val_loss did not improve from 80.90171
196/196 - 70s - loss: 96.5347 - MinusLogProbMetric: 96.5347 - val_loss: 96.2742 - val_MinusLogProbMetric: 96.2742 - lr: 2.0576e-06 - 70s/epoch - 358ms/step
Epoch 94/1000
2023-10-24 20:37:50.093 
Epoch 94/1000 
	 loss: 96.3559, MinusLogProbMetric: 96.3559, val_loss: 96.0856, val_MinusLogProbMetric: 96.0856

Epoch 94: val_loss did not improve from 80.90171
196/196 - 68s - loss: 96.3559 - MinusLogProbMetric: 96.3559 - val_loss: 96.0856 - val_MinusLogProbMetric: 96.0856 - lr: 2.0576e-06 - 68s/epoch - 348ms/step
Epoch 95/1000
2023-10-24 20:38:59.654 
Epoch 95/1000 
	 loss: 96.2569, MinusLogProbMetric: 96.2569, val_loss: 95.9197, val_MinusLogProbMetric: 95.9197

Epoch 95: val_loss did not improve from 80.90171
196/196 - 70s - loss: 96.2569 - MinusLogProbMetric: 96.2569 - val_loss: 95.9197 - val_MinusLogProbMetric: 95.9197 - lr: 2.0576e-06 - 70s/epoch - 355ms/step
Epoch 96/1000
2023-10-24 20:40:07.778 
Epoch 96/1000 
	 loss: 96.0489, MinusLogProbMetric: 96.0489, val_loss: 95.7792, val_MinusLogProbMetric: 95.7792

Epoch 96: val_loss did not improve from 80.90171
196/196 - 68s - loss: 96.0489 - MinusLogProbMetric: 96.0489 - val_loss: 95.7792 - val_MinusLogProbMetric: 95.7792 - lr: 2.0576e-06 - 68s/epoch - 348ms/step
Epoch 97/1000
2023-10-24 20:41:15.395 
Epoch 97/1000 
	 loss: 95.9136, MinusLogProbMetric: 95.9136, val_loss: 95.7071, val_MinusLogProbMetric: 95.7071

Epoch 97: val_loss did not improve from 80.90171
196/196 - 68s - loss: 95.9136 - MinusLogProbMetric: 95.9136 - val_loss: 95.7071 - val_MinusLogProbMetric: 95.7071 - lr: 2.0576e-06 - 68s/epoch - 345ms/step
Epoch 98/1000
2023-10-24 20:42:27.379 
Epoch 98/1000 
	 loss: 95.7366, MinusLogProbMetric: 95.7366, val_loss: 95.5062, val_MinusLogProbMetric: 95.5062

Epoch 98: val_loss did not improve from 80.90171
196/196 - 72s - loss: 95.7366 - MinusLogProbMetric: 95.7366 - val_loss: 95.5062 - val_MinusLogProbMetric: 95.5062 - lr: 2.0576e-06 - 72s/epoch - 367ms/step
Epoch 99/1000
2023-10-24 20:43:35.036 
Epoch 99/1000 
	 loss: 95.5906, MinusLogProbMetric: 95.5906, val_loss: 95.3404, val_MinusLogProbMetric: 95.3404

Epoch 99: val_loss did not improve from 80.90171
196/196 - 68s - loss: 95.5906 - MinusLogProbMetric: 95.5906 - val_loss: 95.3404 - val_MinusLogProbMetric: 95.3404 - lr: 2.0576e-06 - 68s/epoch - 345ms/step
Epoch 100/1000
2023-10-24 20:44:44.730 
Epoch 100/1000 
	 loss: 95.4483, MinusLogProbMetric: 95.4483, val_loss: 95.2759, val_MinusLogProbMetric: 95.2759

Epoch 100: val_loss did not improve from 80.90171
196/196 - 70s - loss: 95.4483 - MinusLogProbMetric: 95.4483 - val_loss: 95.2759 - val_MinusLogProbMetric: 95.2759 - lr: 2.0576e-06 - 70s/epoch - 356ms/step
Epoch 101/1000
2023-10-24 20:45:55.134 
Epoch 101/1000 
	 loss: 95.2953, MinusLogProbMetric: 95.2953, val_loss: 95.0863, val_MinusLogProbMetric: 95.0863

Epoch 101: val_loss did not improve from 80.90171
196/196 - 70s - loss: 95.2953 - MinusLogProbMetric: 95.2953 - val_loss: 95.0863 - val_MinusLogProbMetric: 95.0863 - lr: 2.0576e-06 - 70s/epoch - 359ms/step
Epoch 102/1000
2023-10-24 20:47:02.604 
Epoch 102/1000 
	 loss: 95.1962, MinusLogProbMetric: 95.1962, val_loss: 95.1223, val_MinusLogProbMetric: 95.1223

Epoch 102: val_loss did not improve from 80.90171
196/196 - 67s - loss: 95.1962 - MinusLogProbMetric: 95.1962 - val_loss: 95.1223 - val_MinusLogProbMetric: 95.1223 - lr: 2.0576e-06 - 67s/epoch - 344ms/step
Epoch 103/1000
2023-10-24 20:48:10.861 
Epoch 103/1000 
	 loss: 95.0189, MinusLogProbMetric: 95.0189, val_loss: 94.8109, val_MinusLogProbMetric: 94.8109

Epoch 103: val_loss did not improve from 80.90171
Restoring model weights from the end of the best epoch: 3.
196/196 - 69s - loss: 95.0189 - MinusLogProbMetric: 95.0189 - val_loss: 94.8109 - val_MinusLogProbMetric: 94.8109 - lr: 2.0576e-06 - 69s/epoch - 351ms/step
Epoch 103: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 377.
Model trained in 7197.88 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.22 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.42 s.
===========
Run 344/720 done in 22098.66 s.
===========

Directory ../../results/CsplineN_new/run_345/ already exists.
Skipping it.
===========
Run 345/720 already exists. Skipping it.
===========

===========
Generating train data for run 346.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_346/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_346/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_346/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_346
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_231"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_232 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  1645920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,645,920
Trainable params: 1,645,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7fe581391de0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe5051394e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe5051394e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe55449ecb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe4ef5d7610>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe4ef5d7b80>, <keras.callbacks.ModelCheckpoint object at 0x7fe4ef5d7c40>, <keras.callbacks.EarlyStopping object at 0x7fe4ef5d7eb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe4ef5d7ee0>, <keras.callbacks.TerminateOnNaN object at 0x7fe4ef5d7b20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_346/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 346/720 with hyperparameters:
timestamp = 2023-10-24 20:48:16.578425
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1645920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-10-24 20:49:44.736 
Epoch 1/1000 
	 loss: 495.5482, MinusLogProbMetric: 495.5482, val_loss: 164.4377, val_MinusLogProbMetric: 164.4377

Epoch 1: val_loss improved from inf to 164.43774, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 88s - loss: 495.5482 - MinusLogProbMetric: 495.5482 - val_loss: 164.4377 - val_MinusLogProbMetric: 164.4377 - lr: 0.0010 - 88s/epoch - 451ms/step
Epoch 2/1000
2023-10-24 20:50:16.238 
Epoch 2/1000 
	 loss: 102.6332, MinusLogProbMetric: 102.6332, val_loss: 78.6491, val_MinusLogProbMetric: 78.6491

Epoch 2: val_loss improved from 164.43774 to 78.64911, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 102.6332 - MinusLogProbMetric: 102.6332 - val_loss: 78.6491 - val_MinusLogProbMetric: 78.6491 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 3/1000
2023-10-24 20:50:48.806 
Epoch 3/1000 
	 loss: 70.0492, MinusLogProbMetric: 70.0492, val_loss: 60.2635, val_MinusLogProbMetric: 60.2635

Epoch 3: val_loss improved from 78.64911 to 60.26345, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 70.0492 - MinusLogProbMetric: 70.0492 - val_loss: 60.2635 - val_MinusLogProbMetric: 60.2635 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 4/1000
2023-10-24 20:51:20.125 
Epoch 4/1000 
	 loss: 57.3288, MinusLogProbMetric: 57.3288, val_loss: 53.7317, val_MinusLogProbMetric: 53.7317

Epoch 4: val_loss improved from 60.26345 to 53.73172, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 31s - loss: 57.3288 - MinusLogProbMetric: 57.3288 - val_loss: 53.7317 - val_MinusLogProbMetric: 53.7317 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 5/1000
2023-10-24 20:51:51.343 
Epoch 5/1000 
	 loss: 51.0619, MinusLogProbMetric: 51.0619, val_loss: 50.2159, val_MinusLogProbMetric: 50.2159

Epoch 5: val_loss improved from 53.73172 to 50.21592, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 31s - loss: 51.0619 - MinusLogProbMetric: 51.0619 - val_loss: 50.2159 - val_MinusLogProbMetric: 50.2159 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 6/1000
2023-10-24 20:52:24.232 
Epoch 6/1000 
	 loss: 46.8940, MinusLogProbMetric: 46.8940, val_loss: 45.9650, val_MinusLogProbMetric: 45.9650

Epoch 6: val_loss improved from 50.21592 to 45.96501, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 46.8940 - MinusLogProbMetric: 46.8940 - val_loss: 45.9650 - val_MinusLogProbMetric: 45.9650 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 7/1000
2023-10-24 20:52:55.786 
Epoch 7/1000 
	 loss: 44.4005, MinusLogProbMetric: 44.4005, val_loss: 42.8774, val_MinusLogProbMetric: 42.8774

Epoch 7: val_loss improved from 45.96501 to 42.87742, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 44.4005 - MinusLogProbMetric: 44.4005 - val_loss: 42.8774 - val_MinusLogProbMetric: 42.8774 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 8/1000
2023-10-24 20:53:28.509 
Epoch 8/1000 
	 loss: 45.1693, MinusLogProbMetric: 45.1693, val_loss: 44.1820, val_MinusLogProbMetric: 44.1820

Epoch 8: val_loss did not improve from 42.87742
196/196 - 32s - loss: 45.1693 - MinusLogProbMetric: 45.1693 - val_loss: 44.1820 - val_MinusLogProbMetric: 44.1820 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 9/1000
2023-10-24 20:54:00.954 
Epoch 9/1000 
	 loss: 42.2064, MinusLogProbMetric: 42.2064, val_loss: 42.7534, val_MinusLogProbMetric: 42.7534

Epoch 9: val_loss improved from 42.87742 to 42.75342, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 42.2064 - MinusLogProbMetric: 42.2064 - val_loss: 42.7534 - val_MinusLogProbMetric: 42.7534 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 10/1000
2023-10-24 20:54:32.861 
Epoch 10/1000 
	 loss: 40.4478, MinusLogProbMetric: 40.4478, val_loss: 40.2179, val_MinusLogProbMetric: 40.2179

Epoch 10: val_loss improved from 42.75342 to 40.21788, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 40.4478 - MinusLogProbMetric: 40.4478 - val_loss: 40.2179 - val_MinusLogProbMetric: 40.2179 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 11/1000
2023-10-24 20:55:05.302 
Epoch 11/1000 
	 loss: 39.7510, MinusLogProbMetric: 39.7510, val_loss: 41.0563, val_MinusLogProbMetric: 41.0563

Epoch 11: val_loss did not improve from 40.21788
196/196 - 32s - loss: 39.7510 - MinusLogProbMetric: 39.7510 - val_loss: 41.0563 - val_MinusLogProbMetric: 41.0563 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 12/1000
2023-10-24 20:55:36.764 
Epoch 12/1000 
	 loss: 39.5265, MinusLogProbMetric: 39.5265, val_loss: 38.7151, val_MinusLogProbMetric: 38.7151

Epoch 12: val_loss improved from 40.21788 to 38.71511, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 39.5265 - MinusLogProbMetric: 39.5265 - val_loss: 38.7151 - val_MinusLogProbMetric: 38.7151 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 13/1000
2023-10-24 20:56:07.229 
Epoch 13/1000 
	 loss: 38.1617, MinusLogProbMetric: 38.1617, val_loss: 39.1016, val_MinusLogProbMetric: 39.1016

Epoch 13: val_loss did not improve from 38.71511
196/196 - 30s - loss: 38.1617 - MinusLogProbMetric: 38.1617 - val_loss: 39.1016 - val_MinusLogProbMetric: 39.1016 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 14/1000
2023-10-24 20:56:38.704 
Epoch 14/1000 
	 loss: 37.1067, MinusLogProbMetric: 37.1067, val_loss: 36.2035, val_MinusLogProbMetric: 36.2035

Epoch 14: val_loss improved from 38.71511 to 36.20348, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 37.1067 - MinusLogProbMetric: 37.1067 - val_loss: 36.2035 - val_MinusLogProbMetric: 36.2035 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 15/1000
2023-10-24 20:57:10.194 
Epoch 15/1000 
	 loss: 37.6220, MinusLogProbMetric: 37.6220, val_loss: 39.3662, val_MinusLogProbMetric: 39.3662

Epoch 15: val_loss did not improve from 36.20348
196/196 - 31s - loss: 37.6220 - MinusLogProbMetric: 37.6220 - val_loss: 39.3662 - val_MinusLogProbMetric: 39.3662 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 16/1000
2023-10-24 20:57:41.405 
Epoch 16/1000 
	 loss: 36.3193, MinusLogProbMetric: 36.3193, val_loss: 36.4319, val_MinusLogProbMetric: 36.4319

Epoch 16: val_loss did not improve from 36.20348
196/196 - 31s - loss: 36.3193 - MinusLogProbMetric: 36.3193 - val_loss: 36.4319 - val_MinusLogProbMetric: 36.4319 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 17/1000
2023-10-24 20:58:13.094 
Epoch 17/1000 
	 loss: 36.0975, MinusLogProbMetric: 36.0975, val_loss: 36.1072, val_MinusLogProbMetric: 36.1072

Epoch 17: val_loss improved from 36.20348 to 36.10721, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 36.0975 - MinusLogProbMetric: 36.0975 - val_loss: 36.1072 - val_MinusLogProbMetric: 36.1072 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 18/1000
2023-10-24 20:58:44.713 
Epoch 18/1000 
	 loss: 36.1650, MinusLogProbMetric: 36.1650, val_loss: 36.1680, val_MinusLogProbMetric: 36.1680

Epoch 18: val_loss did not improve from 36.10721
196/196 - 31s - loss: 36.1650 - MinusLogProbMetric: 36.1650 - val_loss: 36.1680 - val_MinusLogProbMetric: 36.1680 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 19/1000
2023-10-24 20:59:17.217 
Epoch 19/1000 
	 loss: 35.5354, MinusLogProbMetric: 35.5354, val_loss: 37.2388, val_MinusLogProbMetric: 37.2388

Epoch 19: val_loss did not improve from 36.10721
196/196 - 32s - loss: 35.5354 - MinusLogProbMetric: 35.5354 - val_loss: 37.2388 - val_MinusLogProbMetric: 37.2388 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 20/1000
2023-10-24 20:59:49.370 
Epoch 20/1000 
	 loss: 35.3243, MinusLogProbMetric: 35.3243, val_loss: 35.1533, val_MinusLogProbMetric: 35.1533

Epoch 20: val_loss improved from 36.10721 to 35.15326, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 35.3243 - MinusLogProbMetric: 35.3243 - val_loss: 35.1533 - val_MinusLogProbMetric: 35.1533 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 21/1000
2023-10-24 21:00:22.195 
Epoch 21/1000 
	 loss: 34.8108, MinusLogProbMetric: 34.8108, val_loss: 34.7741, val_MinusLogProbMetric: 34.7741

Epoch 21: val_loss improved from 35.15326 to 34.77414, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 34.8108 - MinusLogProbMetric: 34.8108 - val_loss: 34.7741 - val_MinusLogProbMetric: 34.7741 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 22/1000
2023-10-24 21:00:55.267 
Epoch 22/1000 
	 loss: 34.6873, MinusLogProbMetric: 34.6873, val_loss: 34.6365, val_MinusLogProbMetric: 34.6365

Epoch 22: val_loss improved from 34.77414 to 34.63652, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 34.6873 - MinusLogProbMetric: 34.6873 - val_loss: 34.6365 - val_MinusLogProbMetric: 34.6365 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 23/1000
2023-10-24 21:01:27.093 
Epoch 23/1000 
	 loss: 34.6029, MinusLogProbMetric: 34.6029, val_loss: 34.3295, val_MinusLogProbMetric: 34.3295

Epoch 23: val_loss improved from 34.63652 to 34.32950, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 34.6029 - MinusLogProbMetric: 34.6029 - val_loss: 34.3295 - val_MinusLogProbMetric: 34.3295 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 24/1000
2023-10-24 21:01:59.760 
Epoch 24/1000 
	 loss: 34.2086, MinusLogProbMetric: 34.2086, val_loss: 34.6995, val_MinusLogProbMetric: 34.6995

Epoch 24: val_loss did not improve from 34.32950
196/196 - 32s - loss: 34.2086 - MinusLogProbMetric: 34.2086 - val_loss: 34.6995 - val_MinusLogProbMetric: 34.6995 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 25/1000
2023-10-24 21:02:32.291 
Epoch 25/1000 
	 loss: 34.4831, MinusLogProbMetric: 34.4831, val_loss: 33.1969, val_MinusLogProbMetric: 33.1969

Epoch 25: val_loss improved from 34.32950 to 33.19693, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 34.4831 - MinusLogProbMetric: 34.4831 - val_loss: 33.1969 - val_MinusLogProbMetric: 33.1969 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 26/1000
2023-10-24 21:03:02.966 
Epoch 26/1000 
	 loss: 33.5430, MinusLogProbMetric: 33.5430, val_loss: 35.3294, val_MinusLogProbMetric: 35.3294

Epoch 26: val_loss did not improve from 33.19693
196/196 - 30s - loss: 33.5430 - MinusLogProbMetric: 33.5430 - val_loss: 35.3294 - val_MinusLogProbMetric: 35.3294 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 27/1000
2023-10-24 21:03:35.645 
Epoch 27/1000 
	 loss: 33.8034, MinusLogProbMetric: 33.8034, val_loss: 33.0966, val_MinusLogProbMetric: 33.0966

Epoch 27: val_loss improved from 33.19693 to 33.09657, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 33.8034 - MinusLogProbMetric: 33.8034 - val_loss: 33.0966 - val_MinusLogProbMetric: 33.0966 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 28/1000
2023-10-24 21:04:07.504 
Epoch 28/1000 
	 loss: 33.5069, MinusLogProbMetric: 33.5069, val_loss: 33.4481, val_MinusLogProbMetric: 33.4481

Epoch 28: val_loss did not improve from 33.09657
196/196 - 31s - loss: 33.5069 - MinusLogProbMetric: 33.5069 - val_loss: 33.4481 - val_MinusLogProbMetric: 33.4481 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 29/1000
2023-10-24 21:04:39.719 
Epoch 29/1000 
	 loss: 33.3305, MinusLogProbMetric: 33.3305, val_loss: 35.0514, val_MinusLogProbMetric: 35.0514

Epoch 29: val_loss did not improve from 33.09657
196/196 - 32s - loss: 33.3305 - MinusLogProbMetric: 33.3305 - val_loss: 35.0514 - val_MinusLogProbMetric: 35.0514 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 30/1000
2023-10-24 21:05:11.123 
Epoch 30/1000 
	 loss: 33.2242, MinusLogProbMetric: 33.2242, val_loss: 34.2599, val_MinusLogProbMetric: 34.2599

Epoch 30: val_loss did not improve from 33.09657
196/196 - 31s - loss: 33.2242 - MinusLogProbMetric: 33.2242 - val_loss: 34.2599 - val_MinusLogProbMetric: 34.2599 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 31/1000
2023-10-24 21:05:42.965 
Epoch 31/1000 
	 loss: 33.0894, MinusLogProbMetric: 33.0894, val_loss: 34.0877, val_MinusLogProbMetric: 34.0877

Epoch 31: val_loss did not improve from 33.09657
196/196 - 32s - loss: 33.0894 - MinusLogProbMetric: 33.0894 - val_loss: 34.0877 - val_MinusLogProbMetric: 34.0877 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 32/1000
2023-10-24 21:06:13.151 
Epoch 32/1000 
	 loss: 32.9200, MinusLogProbMetric: 32.9200, val_loss: 33.4122, val_MinusLogProbMetric: 33.4122

Epoch 32: val_loss did not improve from 33.09657
196/196 - 30s - loss: 32.9200 - MinusLogProbMetric: 32.9200 - val_loss: 33.4122 - val_MinusLogProbMetric: 33.4122 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 33/1000
2023-10-24 21:06:43.313 
Epoch 33/1000 
	 loss: 32.8623, MinusLogProbMetric: 32.8623, val_loss: 33.3068, val_MinusLogProbMetric: 33.3068

Epoch 33: val_loss did not improve from 33.09657
196/196 - 30s - loss: 32.8623 - MinusLogProbMetric: 32.8623 - val_loss: 33.3068 - val_MinusLogProbMetric: 33.3068 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 34/1000
2023-10-24 21:07:14.880 
Epoch 34/1000 
	 loss: 32.8260, MinusLogProbMetric: 32.8260, val_loss: 32.7860, val_MinusLogProbMetric: 32.7860

Epoch 34: val_loss improved from 33.09657 to 32.78596, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 32.8260 - MinusLogProbMetric: 32.8260 - val_loss: 32.7860 - val_MinusLogProbMetric: 32.7860 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 35/1000
2023-10-24 21:07:47.139 
Epoch 35/1000 
	 loss: 32.5264, MinusLogProbMetric: 32.5264, val_loss: 32.7485, val_MinusLogProbMetric: 32.7485

Epoch 35: val_loss improved from 32.78596 to 32.74852, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 32.5264 - MinusLogProbMetric: 32.5264 - val_loss: 32.7485 - val_MinusLogProbMetric: 32.7485 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 36/1000
2023-10-24 21:08:18.237 
Epoch 36/1000 
	 loss: 32.6756, MinusLogProbMetric: 32.6756, val_loss: 33.5611, val_MinusLogProbMetric: 33.5611

Epoch 36: val_loss did not improve from 32.74852
196/196 - 31s - loss: 32.6756 - MinusLogProbMetric: 32.6756 - val_loss: 33.5611 - val_MinusLogProbMetric: 33.5611 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 37/1000
2023-10-24 21:08:50.364 
Epoch 37/1000 
	 loss: 32.6343, MinusLogProbMetric: 32.6343, val_loss: 33.9746, val_MinusLogProbMetric: 33.9746

Epoch 37: val_loss did not improve from 32.74852
196/196 - 32s - loss: 32.6343 - MinusLogProbMetric: 32.6343 - val_loss: 33.9746 - val_MinusLogProbMetric: 33.9746 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 38/1000
2023-10-24 21:09:23.055 
Epoch 38/1000 
	 loss: 32.3748, MinusLogProbMetric: 32.3748, val_loss: 32.4307, val_MinusLogProbMetric: 32.4307

Epoch 38: val_loss improved from 32.74852 to 32.43074, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 32.3748 - MinusLogProbMetric: 32.3748 - val_loss: 32.4307 - val_MinusLogProbMetric: 32.4307 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 39/1000
2023-10-24 21:09:55.751 
Epoch 39/1000 
	 loss: 32.4575, MinusLogProbMetric: 32.4575, val_loss: 32.4035, val_MinusLogProbMetric: 32.4035

Epoch 39: val_loss improved from 32.43074 to 32.40353, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 32.4575 - MinusLogProbMetric: 32.4575 - val_loss: 32.4035 - val_MinusLogProbMetric: 32.4035 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 40/1000
2023-10-24 21:10:26.906 
Epoch 40/1000 
	 loss: 32.2444, MinusLogProbMetric: 32.2444, val_loss: 32.9304, val_MinusLogProbMetric: 32.9304

Epoch 40: val_loss did not improve from 32.40353
196/196 - 31s - loss: 32.2444 - MinusLogProbMetric: 32.2444 - val_loss: 32.9304 - val_MinusLogProbMetric: 32.9304 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 41/1000
2023-10-24 21:10:56.795 
Epoch 41/1000 
	 loss: 32.3454, MinusLogProbMetric: 32.3454, val_loss: 34.2156, val_MinusLogProbMetric: 34.2156

Epoch 41: val_loss did not improve from 32.40353
196/196 - 30s - loss: 32.3454 - MinusLogProbMetric: 32.3454 - val_loss: 34.2156 - val_MinusLogProbMetric: 34.2156 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 42/1000
2023-10-24 21:11:29.409 
Epoch 42/1000 
	 loss: 32.1590, MinusLogProbMetric: 32.1590, val_loss: 34.1793, val_MinusLogProbMetric: 34.1793

Epoch 42: val_loss did not improve from 32.40353
196/196 - 33s - loss: 32.1590 - MinusLogProbMetric: 32.1590 - val_loss: 34.1793 - val_MinusLogProbMetric: 34.1793 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 43/1000
2023-10-24 21:12:01.732 
Epoch 43/1000 
	 loss: 31.9702, MinusLogProbMetric: 31.9702, val_loss: 32.3232, val_MinusLogProbMetric: 32.3232

Epoch 43: val_loss improved from 32.40353 to 32.32322, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 31.9702 - MinusLogProbMetric: 31.9702 - val_loss: 32.3232 - val_MinusLogProbMetric: 32.3232 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 44/1000
2023-10-24 21:12:34.083 
Epoch 44/1000 
	 loss: 32.0224, MinusLogProbMetric: 32.0224, val_loss: 32.9524, val_MinusLogProbMetric: 32.9524

Epoch 44: val_loss did not improve from 32.32322
196/196 - 32s - loss: 32.0224 - MinusLogProbMetric: 32.0224 - val_loss: 32.9524 - val_MinusLogProbMetric: 32.9524 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 45/1000
2023-10-24 21:13:04.145 
Epoch 45/1000 
	 loss: 31.9566, MinusLogProbMetric: 31.9566, val_loss: 31.5973, val_MinusLogProbMetric: 31.5973

Epoch 45: val_loss improved from 32.32322 to 31.59734, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 31s - loss: 31.9566 - MinusLogProbMetric: 31.9566 - val_loss: 31.5973 - val_MinusLogProbMetric: 31.5973 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 46/1000
2023-10-24 21:13:34.691 
Epoch 46/1000 
	 loss: 31.7387, MinusLogProbMetric: 31.7387, val_loss: 33.0016, val_MinusLogProbMetric: 33.0016

Epoch 46: val_loss did not improve from 31.59734
196/196 - 30s - loss: 31.7387 - MinusLogProbMetric: 31.7387 - val_loss: 33.0016 - val_MinusLogProbMetric: 33.0016 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 47/1000
2023-10-24 21:14:06.684 
Epoch 47/1000 
	 loss: 31.6373, MinusLogProbMetric: 31.6373, val_loss: 31.9530, val_MinusLogProbMetric: 31.9530

Epoch 47: val_loss did not improve from 31.59734
196/196 - 32s - loss: 31.6373 - MinusLogProbMetric: 31.6373 - val_loss: 31.9530 - val_MinusLogProbMetric: 31.9530 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 48/1000
2023-10-24 21:14:39.164 
Epoch 48/1000 
	 loss: 31.6679, MinusLogProbMetric: 31.6679, val_loss: 31.7555, val_MinusLogProbMetric: 31.7555

Epoch 48: val_loss did not improve from 31.59734
196/196 - 32s - loss: 31.6679 - MinusLogProbMetric: 31.6679 - val_loss: 31.7555 - val_MinusLogProbMetric: 31.7555 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 49/1000
2023-10-24 21:15:11.782 
Epoch 49/1000 
	 loss: 31.7714, MinusLogProbMetric: 31.7714, val_loss: 34.0903, val_MinusLogProbMetric: 34.0903

Epoch 49: val_loss did not improve from 31.59734
196/196 - 33s - loss: 31.7714 - MinusLogProbMetric: 31.7714 - val_loss: 34.0903 - val_MinusLogProbMetric: 34.0903 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 50/1000
2023-10-24 21:15:43.941 
Epoch 50/1000 
	 loss: 31.7138, MinusLogProbMetric: 31.7138, val_loss: 33.0085, val_MinusLogProbMetric: 33.0085

Epoch 50: val_loss did not improve from 31.59734
196/196 - 32s - loss: 31.7138 - MinusLogProbMetric: 31.7138 - val_loss: 33.0085 - val_MinusLogProbMetric: 33.0085 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 51/1000
2023-10-24 21:16:15.489 
Epoch 51/1000 
	 loss: 31.7328, MinusLogProbMetric: 31.7328, val_loss: 32.0600, val_MinusLogProbMetric: 32.0600

Epoch 51: val_loss did not improve from 31.59734
196/196 - 32s - loss: 31.7328 - MinusLogProbMetric: 31.7328 - val_loss: 32.0600 - val_MinusLogProbMetric: 32.0600 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 52/1000
2023-10-24 21:16:47.893 
Epoch 52/1000 
	 loss: 31.7380, MinusLogProbMetric: 31.7380, val_loss: 31.1923, val_MinusLogProbMetric: 31.1923

Epoch 52: val_loss improved from 31.59734 to 31.19230, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 31.7380 - MinusLogProbMetric: 31.7380 - val_loss: 31.1923 - val_MinusLogProbMetric: 31.1923 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 53/1000
2023-10-24 21:17:18.627 
Epoch 53/1000 
	 loss: 31.3740, MinusLogProbMetric: 31.3740, val_loss: 33.0529, val_MinusLogProbMetric: 33.0529

Epoch 53: val_loss did not improve from 31.19230
196/196 - 30s - loss: 31.3740 - MinusLogProbMetric: 31.3740 - val_loss: 33.0529 - val_MinusLogProbMetric: 33.0529 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 54/1000
2023-10-24 21:17:48.629 
Epoch 54/1000 
	 loss: 31.4348, MinusLogProbMetric: 31.4348, val_loss: 32.0722, val_MinusLogProbMetric: 32.0722

Epoch 54: val_loss did not improve from 31.19230
196/196 - 30s - loss: 31.4348 - MinusLogProbMetric: 31.4348 - val_loss: 32.0722 - val_MinusLogProbMetric: 32.0722 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 55/1000
2023-10-24 21:18:19.562 
Epoch 55/1000 
	 loss: 31.3733, MinusLogProbMetric: 31.3733, val_loss: 31.9205, val_MinusLogProbMetric: 31.9205

Epoch 55: val_loss did not improve from 31.19230
196/196 - 31s - loss: 31.3733 - MinusLogProbMetric: 31.3733 - val_loss: 31.9205 - val_MinusLogProbMetric: 31.9205 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 56/1000
2023-10-24 21:18:51.368 
Epoch 56/1000 
	 loss: 31.4351, MinusLogProbMetric: 31.4351, val_loss: 30.5182, val_MinusLogProbMetric: 30.5182

Epoch 56: val_loss improved from 31.19230 to 30.51818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 31.4351 - MinusLogProbMetric: 31.4351 - val_loss: 30.5182 - val_MinusLogProbMetric: 30.5182 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 57/1000
2023-10-24 21:19:24.100 
Epoch 57/1000 
	 loss: 31.2248, MinusLogProbMetric: 31.2248, val_loss: 31.1869, val_MinusLogProbMetric: 31.1869

Epoch 57: val_loss did not improve from 30.51818
196/196 - 32s - loss: 31.2248 - MinusLogProbMetric: 31.2248 - val_loss: 31.1869 - val_MinusLogProbMetric: 31.1869 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 58/1000
2023-10-24 21:19:54.378 
Epoch 58/1000 
	 loss: 31.3137, MinusLogProbMetric: 31.3137, val_loss: 32.0492, val_MinusLogProbMetric: 32.0492

Epoch 58: val_loss did not improve from 30.51818
196/196 - 30s - loss: 31.3137 - MinusLogProbMetric: 31.3137 - val_loss: 32.0492 - val_MinusLogProbMetric: 32.0492 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 59/1000
2023-10-24 21:20:25.986 
Epoch 59/1000 
	 loss: 31.2905, MinusLogProbMetric: 31.2905, val_loss: 33.0975, val_MinusLogProbMetric: 33.0975

Epoch 59: val_loss did not improve from 30.51818
196/196 - 32s - loss: 31.2905 - MinusLogProbMetric: 31.2905 - val_loss: 33.0975 - val_MinusLogProbMetric: 33.0975 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 60/1000
2023-10-24 21:20:58.533 
Epoch 60/1000 
	 loss: 31.3827, MinusLogProbMetric: 31.3827, val_loss: 31.2444, val_MinusLogProbMetric: 31.2444

Epoch 60: val_loss did not improve from 30.51818
196/196 - 33s - loss: 31.3827 - MinusLogProbMetric: 31.3827 - val_loss: 31.2444 - val_MinusLogProbMetric: 31.2444 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 61/1000
2023-10-24 21:21:31.336 
Epoch 61/1000 
	 loss: 31.1810, MinusLogProbMetric: 31.1810, val_loss: 31.5441, val_MinusLogProbMetric: 31.5441

Epoch 61: val_loss did not improve from 30.51818
196/196 - 33s - loss: 31.1810 - MinusLogProbMetric: 31.1810 - val_loss: 31.5441 - val_MinusLogProbMetric: 31.5441 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 62/1000
2023-10-24 21:22:02.105 
Epoch 62/1000 
	 loss: 31.1976, MinusLogProbMetric: 31.1976, val_loss: 31.9098, val_MinusLogProbMetric: 31.9098

Epoch 62: val_loss did not improve from 30.51818
196/196 - 31s - loss: 31.1976 - MinusLogProbMetric: 31.1976 - val_loss: 31.9098 - val_MinusLogProbMetric: 31.9098 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 63/1000
2023-10-24 21:22:33.238 
Epoch 63/1000 
	 loss: 31.1960, MinusLogProbMetric: 31.1960, val_loss: 31.6423, val_MinusLogProbMetric: 31.6423

Epoch 63: val_loss did not improve from 30.51818
196/196 - 31s - loss: 31.1960 - MinusLogProbMetric: 31.1960 - val_loss: 31.6423 - val_MinusLogProbMetric: 31.6423 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 64/1000
2023-10-24 21:23:04.500 
Epoch 64/1000 
	 loss: 31.0205, MinusLogProbMetric: 31.0205, val_loss: 32.7224, val_MinusLogProbMetric: 32.7224

Epoch 64: val_loss did not improve from 30.51818
196/196 - 31s - loss: 31.0205 - MinusLogProbMetric: 31.0205 - val_loss: 32.7224 - val_MinusLogProbMetric: 32.7224 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 65/1000
2023-10-24 21:23:36.243 
Epoch 65/1000 
	 loss: 31.0155, MinusLogProbMetric: 31.0155, val_loss: 30.9206, val_MinusLogProbMetric: 30.9206

Epoch 65: val_loss did not improve from 30.51818
196/196 - 32s - loss: 31.0155 - MinusLogProbMetric: 31.0155 - val_loss: 30.9206 - val_MinusLogProbMetric: 30.9206 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 66/1000
2023-10-24 21:24:06.263 
Epoch 66/1000 
	 loss: 31.2177, MinusLogProbMetric: 31.2177, val_loss: 31.9204, val_MinusLogProbMetric: 31.9204

Epoch 66: val_loss did not improve from 30.51818
196/196 - 30s - loss: 31.2177 - MinusLogProbMetric: 31.2177 - val_loss: 31.9204 - val_MinusLogProbMetric: 31.9204 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 67/1000
2023-10-24 21:24:37.643 
Epoch 67/1000 
	 loss: 30.8123, MinusLogProbMetric: 30.8123, val_loss: 30.8765, val_MinusLogProbMetric: 30.8765

Epoch 67: val_loss did not improve from 30.51818
196/196 - 31s - loss: 30.8123 - MinusLogProbMetric: 30.8123 - val_loss: 30.8765 - val_MinusLogProbMetric: 30.8765 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 68/1000
2023-10-24 21:25:08.434 
Epoch 68/1000 
	 loss: 30.8312, MinusLogProbMetric: 30.8312, val_loss: 30.6891, val_MinusLogProbMetric: 30.6891

Epoch 68: val_loss did not improve from 30.51818
196/196 - 31s - loss: 30.8312 - MinusLogProbMetric: 30.8312 - val_loss: 30.6891 - val_MinusLogProbMetric: 30.6891 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 69/1000
2023-10-24 21:25:39.731 
Epoch 69/1000 
	 loss: 30.9328, MinusLogProbMetric: 30.9328, val_loss: 30.8649, val_MinusLogProbMetric: 30.8649

Epoch 69: val_loss did not improve from 30.51818
196/196 - 31s - loss: 30.9328 - MinusLogProbMetric: 30.9328 - val_loss: 30.8649 - val_MinusLogProbMetric: 30.8649 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 70/1000
2023-10-24 21:26:11.820 
Epoch 70/1000 
	 loss: 30.8052, MinusLogProbMetric: 30.8052, val_loss: 31.3717, val_MinusLogProbMetric: 31.3717

Epoch 70: val_loss did not improve from 30.51818
196/196 - 32s - loss: 30.8052 - MinusLogProbMetric: 30.8052 - val_loss: 31.3717 - val_MinusLogProbMetric: 31.3717 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 71/1000
2023-10-24 21:26:43.106 
Epoch 71/1000 
	 loss: 31.0015, MinusLogProbMetric: 31.0015, val_loss: 32.1591, val_MinusLogProbMetric: 32.1591

Epoch 71: val_loss did not improve from 30.51818
196/196 - 31s - loss: 31.0015 - MinusLogProbMetric: 31.0015 - val_loss: 32.1591 - val_MinusLogProbMetric: 32.1591 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 72/1000
2023-10-24 21:27:14.069 
Epoch 72/1000 
	 loss: 30.8248, MinusLogProbMetric: 30.8248, val_loss: 31.8476, val_MinusLogProbMetric: 31.8476

Epoch 72: val_loss did not improve from 30.51818
196/196 - 31s - loss: 30.8248 - MinusLogProbMetric: 30.8248 - val_loss: 31.8476 - val_MinusLogProbMetric: 31.8476 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 73/1000
2023-10-24 21:27:44.233 
Epoch 73/1000 
	 loss: 30.9188, MinusLogProbMetric: 30.9188, val_loss: 31.5109, val_MinusLogProbMetric: 31.5109

Epoch 73: val_loss did not improve from 30.51818
196/196 - 30s - loss: 30.9188 - MinusLogProbMetric: 30.9188 - val_loss: 31.5109 - val_MinusLogProbMetric: 31.5109 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 74/1000
2023-10-24 21:28:15.544 
Epoch 74/1000 
	 loss: 30.6656, MinusLogProbMetric: 30.6656, val_loss: 30.2090, val_MinusLogProbMetric: 30.2090

Epoch 74: val_loss improved from 30.51818 to 30.20900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 30.6656 - MinusLogProbMetric: 30.6656 - val_loss: 30.2090 - val_MinusLogProbMetric: 30.2090 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 75/1000
2023-10-24 21:28:46.086 
Epoch 75/1000 
	 loss: 30.6504, MinusLogProbMetric: 30.6504, val_loss: 33.0521, val_MinusLogProbMetric: 33.0521

Epoch 75: val_loss did not improve from 30.20900
196/196 - 30s - loss: 30.6504 - MinusLogProbMetric: 30.6504 - val_loss: 33.0521 - val_MinusLogProbMetric: 33.0521 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 76/1000
2023-10-24 21:29:18.393 
Epoch 76/1000 
	 loss: 30.9831, MinusLogProbMetric: 30.9831, val_loss: 30.7256, val_MinusLogProbMetric: 30.7256

Epoch 76: val_loss did not improve from 30.20900
196/196 - 32s - loss: 30.9831 - MinusLogProbMetric: 30.9831 - val_loss: 30.7256 - val_MinusLogProbMetric: 30.7256 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 77/1000
2023-10-24 21:29:50.974 
Epoch 77/1000 
	 loss: 30.5508, MinusLogProbMetric: 30.5508, val_loss: 32.8059, val_MinusLogProbMetric: 32.8059

Epoch 77: val_loss did not improve from 30.20900
196/196 - 33s - loss: 30.5508 - MinusLogProbMetric: 30.5508 - val_loss: 32.8059 - val_MinusLogProbMetric: 32.8059 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 78/1000
2023-10-24 21:30:23.758 
Epoch 78/1000 
	 loss: 30.6607, MinusLogProbMetric: 30.6607, val_loss: 30.6473, val_MinusLogProbMetric: 30.6473

Epoch 78: val_loss did not improve from 30.20900
196/196 - 33s - loss: 30.6607 - MinusLogProbMetric: 30.6607 - val_loss: 30.6473 - val_MinusLogProbMetric: 30.6473 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 79/1000
2023-10-24 21:30:54.894 
Epoch 79/1000 
	 loss: 30.6473, MinusLogProbMetric: 30.6473, val_loss: 31.6894, val_MinusLogProbMetric: 31.6894

Epoch 79: val_loss did not improve from 30.20900
196/196 - 31s - loss: 30.6473 - MinusLogProbMetric: 30.6473 - val_loss: 31.6894 - val_MinusLogProbMetric: 31.6894 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 80/1000
2023-10-24 21:31:25.276 
Epoch 80/1000 
	 loss: 30.5549, MinusLogProbMetric: 30.5549, val_loss: 30.8637, val_MinusLogProbMetric: 30.8637

Epoch 80: val_loss did not improve from 30.20900
196/196 - 30s - loss: 30.5549 - MinusLogProbMetric: 30.5549 - val_loss: 30.8637 - val_MinusLogProbMetric: 30.8637 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 81/1000
2023-10-24 21:31:56.810 
Epoch 81/1000 
	 loss: 30.4775, MinusLogProbMetric: 30.4775, val_loss: 33.2677, val_MinusLogProbMetric: 33.2677

Epoch 81: val_loss did not improve from 30.20900
196/196 - 32s - loss: 30.4775 - MinusLogProbMetric: 30.4775 - val_loss: 33.2677 - val_MinusLogProbMetric: 33.2677 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 82/1000
2023-10-24 21:32:26.692 
Epoch 82/1000 
	 loss: 30.5952, MinusLogProbMetric: 30.5952, val_loss: 30.7869, val_MinusLogProbMetric: 30.7869

Epoch 82: val_loss did not improve from 30.20900
196/196 - 30s - loss: 30.5952 - MinusLogProbMetric: 30.5952 - val_loss: 30.7869 - val_MinusLogProbMetric: 30.7869 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 83/1000
2023-10-24 21:32:57.596 
Epoch 83/1000 
	 loss: 30.5493, MinusLogProbMetric: 30.5493, val_loss: 30.4502, val_MinusLogProbMetric: 30.4502

Epoch 83: val_loss did not improve from 30.20900
196/196 - 31s - loss: 30.5493 - MinusLogProbMetric: 30.5493 - val_loss: 30.4502 - val_MinusLogProbMetric: 30.4502 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 84/1000
2023-10-24 21:33:28.240 
Epoch 84/1000 
	 loss: 30.7914, MinusLogProbMetric: 30.7914, val_loss: 30.5814, val_MinusLogProbMetric: 30.5814

Epoch 84: val_loss did not improve from 30.20900
196/196 - 31s - loss: 30.7914 - MinusLogProbMetric: 30.7914 - val_loss: 30.5814 - val_MinusLogProbMetric: 30.5814 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 85/1000
2023-10-24 21:33:59.641 
Epoch 85/1000 
	 loss: 30.4156, MinusLogProbMetric: 30.4156, val_loss: 30.8125, val_MinusLogProbMetric: 30.8125

Epoch 85: val_loss did not improve from 30.20900
196/196 - 31s - loss: 30.4156 - MinusLogProbMetric: 30.4156 - val_loss: 30.8125 - val_MinusLogProbMetric: 30.8125 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 86/1000
2023-10-24 21:34:31.745 
Epoch 86/1000 
	 loss: 30.5299, MinusLogProbMetric: 30.5299, val_loss: 31.4911, val_MinusLogProbMetric: 31.4911

Epoch 86: val_loss did not improve from 30.20900
196/196 - 32s - loss: 30.5299 - MinusLogProbMetric: 30.5299 - val_loss: 31.4911 - val_MinusLogProbMetric: 31.4911 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 87/1000
2023-10-24 21:35:01.620 
Epoch 87/1000 
	 loss: 30.5201, MinusLogProbMetric: 30.5201, val_loss: 30.3694, val_MinusLogProbMetric: 30.3694

Epoch 87: val_loss did not improve from 30.20900
196/196 - 30s - loss: 30.5201 - MinusLogProbMetric: 30.5201 - val_loss: 30.3694 - val_MinusLogProbMetric: 30.3694 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 88/1000
2023-10-24 21:35:33.366 
Epoch 88/1000 
	 loss: 30.4245, MinusLogProbMetric: 30.4245, val_loss: 29.9068, val_MinusLogProbMetric: 29.9068

Epoch 88: val_loss improved from 30.20900 to 29.90683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 30.4245 - MinusLogProbMetric: 30.4245 - val_loss: 29.9068 - val_MinusLogProbMetric: 29.9068 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 89/1000
2023-10-24 21:36:06.235 
Epoch 89/1000 
	 loss: 30.4163, MinusLogProbMetric: 30.4163, val_loss: 30.4699, val_MinusLogProbMetric: 30.4699

Epoch 89: val_loss did not improve from 29.90683
196/196 - 32s - loss: 30.4163 - MinusLogProbMetric: 30.4163 - val_loss: 30.4699 - val_MinusLogProbMetric: 30.4699 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 90/1000
2023-10-24 21:36:38.621 
Epoch 90/1000 
	 loss: 30.5136, MinusLogProbMetric: 30.5136, val_loss: 31.1370, val_MinusLogProbMetric: 31.1370

Epoch 90: val_loss did not improve from 29.90683
196/196 - 32s - loss: 30.5136 - MinusLogProbMetric: 30.5136 - val_loss: 31.1370 - val_MinusLogProbMetric: 31.1370 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 91/1000
2023-10-24 21:37:10.339 
Epoch 91/1000 
	 loss: 30.2256, MinusLogProbMetric: 30.2256, val_loss: 30.1138, val_MinusLogProbMetric: 30.1138

Epoch 91: val_loss did not improve from 29.90683
196/196 - 32s - loss: 30.2256 - MinusLogProbMetric: 30.2256 - val_loss: 30.1138 - val_MinusLogProbMetric: 30.1138 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 92/1000
2023-10-24 21:37:42.680 
Epoch 92/1000 
	 loss: 30.1961, MinusLogProbMetric: 30.1961, val_loss: 30.8300, val_MinusLogProbMetric: 30.8300

Epoch 92: val_loss did not improve from 29.90683
196/196 - 32s - loss: 30.1961 - MinusLogProbMetric: 30.1961 - val_loss: 30.8300 - val_MinusLogProbMetric: 30.8300 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 93/1000
2023-10-24 21:38:14.153 
Epoch 93/1000 
	 loss: 30.5357, MinusLogProbMetric: 30.5357, val_loss: 31.3772, val_MinusLogProbMetric: 31.3772

Epoch 93: val_loss did not improve from 29.90683
196/196 - 31s - loss: 30.5357 - MinusLogProbMetric: 30.5357 - val_loss: 31.3772 - val_MinusLogProbMetric: 31.3772 - lr: 0.0010 - 31s/epoch - 161ms/step
Epoch 94/1000
2023-10-24 21:38:44.334 
Epoch 94/1000 
	 loss: 30.2935, MinusLogProbMetric: 30.2935, val_loss: 30.4503, val_MinusLogProbMetric: 30.4503

Epoch 94: val_loss did not improve from 29.90683
196/196 - 30s - loss: 30.2935 - MinusLogProbMetric: 30.2935 - val_loss: 30.4503 - val_MinusLogProbMetric: 30.4503 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 95/1000
2023-10-24 21:39:16.546 
Epoch 95/1000 
	 loss: 30.1641, MinusLogProbMetric: 30.1641, val_loss: 30.2056, val_MinusLogProbMetric: 30.2056

Epoch 95: val_loss did not improve from 29.90683
196/196 - 32s - loss: 30.1641 - MinusLogProbMetric: 30.1641 - val_loss: 30.2056 - val_MinusLogProbMetric: 30.2056 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 96/1000
2023-10-24 21:39:48.699 
Epoch 96/1000 
	 loss: 30.1084, MinusLogProbMetric: 30.1084, val_loss: 30.2467, val_MinusLogProbMetric: 30.2467

Epoch 96: val_loss did not improve from 29.90683
196/196 - 32s - loss: 30.1084 - MinusLogProbMetric: 30.1084 - val_loss: 30.2467 - val_MinusLogProbMetric: 30.2467 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 97/1000
2023-10-24 21:40:21.369 
Epoch 97/1000 
	 loss: 30.3974, MinusLogProbMetric: 30.3974, val_loss: 30.4907, val_MinusLogProbMetric: 30.4907

Epoch 97: val_loss did not improve from 29.90683
196/196 - 33s - loss: 30.3974 - MinusLogProbMetric: 30.3974 - val_loss: 30.4907 - val_MinusLogProbMetric: 30.4907 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 98/1000
2023-10-24 21:40:53.786 
Epoch 98/1000 
	 loss: 30.2390, MinusLogProbMetric: 30.2390, val_loss: 32.8072, val_MinusLogProbMetric: 32.8072

Epoch 98: val_loss did not improve from 29.90683
196/196 - 32s - loss: 30.2390 - MinusLogProbMetric: 30.2390 - val_loss: 32.8072 - val_MinusLogProbMetric: 32.8072 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 99/1000
2023-10-24 21:41:26.012 
Epoch 99/1000 
	 loss: 30.2490, MinusLogProbMetric: 30.2490, val_loss: 29.7608, val_MinusLogProbMetric: 29.7608

Epoch 99: val_loss improved from 29.90683 to 29.76082, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 30.2490 - MinusLogProbMetric: 30.2490 - val_loss: 29.7608 - val_MinusLogProbMetric: 29.7608 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 100/1000
2023-10-24 21:41:56.530 
Epoch 100/1000 
	 loss: 30.2214, MinusLogProbMetric: 30.2214, val_loss: 31.1200, val_MinusLogProbMetric: 31.1200

Epoch 100: val_loss did not improve from 29.76082
196/196 - 30s - loss: 30.2214 - MinusLogProbMetric: 30.2214 - val_loss: 31.1200 - val_MinusLogProbMetric: 31.1200 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 101/1000
2023-10-24 21:42:27.663 
Epoch 101/1000 
	 loss: 30.2058, MinusLogProbMetric: 30.2058, val_loss: 30.4930, val_MinusLogProbMetric: 30.4930

Epoch 101: val_loss did not improve from 29.76082
196/196 - 31s - loss: 30.2058 - MinusLogProbMetric: 30.2058 - val_loss: 30.4930 - val_MinusLogProbMetric: 30.4930 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 102/1000
2023-10-24 21:42:58.647 
Epoch 102/1000 
	 loss: 30.1480, MinusLogProbMetric: 30.1480, val_loss: 31.3091, val_MinusLogProbMetric: 31.3091

Epoch 102: val_loss did not improve from 29.76082
196/196 - 31s - loss: 30.1480 - MinusLogProbMetric: 30.1480 - val_loss: 31.3091 - val_MinusLogProbMetric: 31.3091 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 103/1000
2023-10-24 21:43:29.913 
Epoch 103/1000 
	 loss: 30.1428, MinusLogProbMetric: 30.1428, val_loss: 29.9315, val_MinusLogProbMetric: 29.9315

Epoch 103: val_loss did not improve from 29.76082
196/196 - 31s - loss: 30.1428 - MinusLogProbMetric: 30.1428 - val_loss: 29.9315 - val_MinusLogProbMetric: 29.9315 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 104/1000
2023-10-24 21:44:02.030 
Epoch 104/1000 
	 loss: 30.2374, MinusLogProbMetric: 30.2374, val_loss: 30.4444, val_MinusLogProbMetric: 30.4444

Epoch 104: val_loss did not improve from 29.76082
196/196 - 32s - loss: 30.2374 - MinusLogProbMetric: 30.2374 - val_loss: 30.4444 - val_MinusLogProbMetric: 30.4444 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 105/1000
2023-10-24 21:44:33.504 
Epoch 105/1000 
	 loss: 29.9816, MinusLogProbMetric: 29.9816, val_loss: 30.2976, val_MinusLogProbMetric: 30.2976

Epoch 105: val_loss did not improve from 29.76082
196/196 - 31s - loss: 29.9816 - MinusLogProbMetric: 29.9816 - val_loss: 30.2976 - val_MinusLogProbMetric: 30.2976 - lr: 0.0010 - 31s/epoch - 161ms/step
Epoch 106/1000
2023-10-24 21:45:04.682 
Epoch 106/1000 
	 loss: 30.2632, MinusLogProbMetric: 30.2632, val_loss: 29.9272, val_MinusLogProbMetric: 29.9272

Epoch 106: val_loss did not improve from 29.76082
196/196 - 31s - loss: 30.2632 - MinusLogProbMetric: 30.2632 - val_loss: 29.9272 - val_MinusLogProbMetric: 29.9272 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 107/1000
2023-10-24 21:45:35.788 
Epoch 107/1000 
	 loss: 29.9014, MinusLogProbMetric: 29.9014, val_loss: 30.9081, val_MinusLogProbMetric: 30.9081

Epoch 107: val_loss did not improve from 29.76082
196/196 - 31s - loss: 29.9014 - MinusLogProbMetric: 29.9014 - val_loss: 30.9081 - val_MinusLogProbMetric: 30.9081 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 108/1000
2023-10-24 21:46:07.411 
Epoch 108/1000 
	 loss: 30.0722, MinusLogProbMetric: 30.0722, val_loss: 30.1986, val_MinusLogProbMetric: 30.1986

Epoch 108: val_loss did not improve from 29.76082
196/196 - 32s - loss: 30.0722 - MinusLogProbMetric: 30.0722 - val_loss: 30.1986 - val_MinusLogProbMetric: 30.1986 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 109/1000
2023-10-24 21:46:38.848 
Epoch 109/1000 
	 loss: 30.0543, MinusLogProbMetric: 30.0543, val_loss: 30.3797, val_MinusLogProbMetric: 30.3797

Epoch 109: val_loss did not improve from 29.76082
196/196 - 31s - loss: 30.0543 - MinusLogProbMetric: 30.0543 - val_loss: 30.3797 - val_MinusLogProbMetric: 30.3797 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 110/1000
2023-10-24 21:47:11.449 
Epoch 110/1000 
	 loss: 29.9887, MinusLogProbMetric: 29.9887, val_loss: 30.6647, val_MinusLogProbMetric: 30.6647

Epoch 110: val_loss did not improve from 29.76082
196/196 - 33s - loss: 29.9887 - MinusLogProbMetric: 29.9887 - val_loss: 30.6647 - val_MinusLogProbMetric: 30.6647 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 111/1000
2023-10-24 21:47:42.401 
Epoch 111/1000 
	 loss: 29.9542, MinusLogProbMetric: 29.9542, val_loss: 30.0260, val_MinusLogProbMetric: 30.0260

Epoch 111: val_loss did not improve from 29.76082
196/196 - 31s - loss: 29.9542 - MinusLogProbMetric: 29.9542 - val_loss: 30.0260 - val_MinusLogProbMetric: 30.0260 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 112/1000
2023-10-24 21:48:14.874 
Epoch 112/1000 
	 loss: 29.9794, MinusLogProbMetric: 29.9794, val_loss: 30.5301, val_MinusLogProbMetric: 30.5301

Epoch 112: val_loss did not improve from 29.76082
196/196 - 32s - loss: 29.9794 - MinusLogProbMetric: 29.9794 - val_loss: 30.5301 - val_MinusLogProbMetric: 30.5301 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 113/1000
2023-10-24 21:48:47.441 
Epoch 113/1000 
	 loss: 29.9406, MinusLogProbMetric: 29.9406, val_loss: 31.7295, val_MinusLogProbMetric: 31.7295

Epoch 113: val_loss did not improve from 29.76082
196/196 - 33s - loss: 29.9406 - MinusLogProbMetric: 29.9406 - val_loss: 31.7295 - val_MinusLogProbMetric: 31.7295 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 114/1000
2023-10-24 21:49:18.614 
Epoch 114/1000 
	 loss: 29.9653, MinusLogProbMetric: 29.9653, val_loss: 30.6058, val_MinusLogProbMetric: 30.6058

Epoch 114: val_loss did not improve from 29.76082
196/196 - 31s - loss: 29.9653 - MinusLogProbMetric: 29.9653 - val_loss: 30.6058 - val_MinusLogProbMetric: 30.6058 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 115/1000
2023-10-24 21:49:48.627 
Epoch 115/1000 
	 loss: 29.8923, MinusLogProbMetric: 29.8923, val_loss: 31.2062, val_MinusLogProbMetric: 31.2062

Epoch 115: val_loss did not improve from 29.76082
196/196 - 30s - loss: 29.8923 - MinusLogProbMetric: 29.8923 - val_loss: 31.2062 - val_MinusLogProbMetric: 31.2062 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 116/1000
2023-10-24 21:50:18.391 
Epoch 116/1000 
	 loss: 29.9792, MinusLogProbMetric: 29.9792, val_loss: 30.2206, val_MinusLogProbMetric: 30.2206

Epoch 116: val_loss did not improve from 29.76082
196/196 - 30s - loss: 29.9792 - MinusLogProbMetric: 29.9792 - val_loss: 30.2206 - val_MinusLogProbMetric: 30.2206 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 117/1000
2023-10-24 21:50:50.933 
Epoch 117/1000 
	 loss: 29.8401, MinusLogProbMetric: 29.8401, val_loss: 30.5953, val_MinusLogProbMetric: 30.5953

Epoch 117: val_loss did not improve from 29.76082
196/196 - 33s - loss: 29.8401 - MinusLogProbMetric: 29.8401 - val_loss: 30.5953 - val_MinusLogProbMetric: 30.5953 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 118/1000
2023-10-24 21:51:22.707 
Epoch 118/1000 
	 loss: 29.8795, MinusLogProbMetric: 29.8795, val_loss: 30.1441, val_MinusLogProbMetric: 30.1441

Epoch 118: val_loss did not improve from 29.76082
196/196 - 32s - loss: 29.8795 - MinusLogProbMetric: 29.8795 - val_loss: 30.1441 - val_MinusLogProbMetric: 30.1441 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 119/1000
2023-10-24 21:51:55.181 
Epoch 119/1000 
	 loss: 29.8810, MinusLogProbMetric: 29.8810, val_loss: 29.6363, val_MinusLogProbMetric: 29.6363

Epoch 119: val_loss improved from 29.76082 to 29.63632, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 29.8810 - MinusLogProbMetric: 29.8810 - val_loss: 29.6363 - val_MinusLogProbMetric: 29.6363 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 120/1000
2023-10-24 21:52:27.576 
Epoch 120/1000 
	 loss: 29.8501, MinusLogProbMetric: 29.8501, val_loss: 30.2060, val_MinusLogProbMetric: 30.2060

Epoch 120: val_loss did not improve from 29.63632
196/196 - 32s - loss: 29.8501 - MinusLogProbMetric: 29.8501 - val_loss: 30.2060 - val_MinusLogProbMetric: 30.2060 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 121/1000
2023-10-24 21:53:00.570 
Epoch 121/1000 
	 loss: 29.8954, MinusLogProbMetric: 29.8954, val_loss: 29.9121, val_MinusLogProbMetric: 29.9121

Epoch 121: val_loss did not improve from 29.63632
196/196 - 33s - loss: 29.8954 - MinusLogProbMetric: 29.8954 - val_loss: 29.9121 - val_MinusLogProbMetric: 29.9121 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 122/1000
2023-10-24 21:53:31.315 
Epoch 122/1000 
	 loss: 29.8184, MinusLogProbMetric: 29.8184, val_loss: 30.5490, val_MinusLogProbMetric: 30.5490

Epoch 122: val_loss did not improve from 29.63632
196/196 - 31s - loss: 29.8184 - MinusLogProbMetric: 29.8184 - val_loss: 30.5490 - val_MinusLogProbMetric: 30.5490 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 123/1000
2023-10-24 21:54:02.189 
Epoch 123/1000 
	 loss: 29.9993, MinusLogProbMetric: 29.9993, val_loss: 30.7319, val_MinusLogProbMetric: 30.7319

Epoch 123: val_loss did not improve from 29.63632
196/196 - 31s - loss: 29.9993 - MinusLogProbMetric: 29.9993 - val_loss: 30.7319 - val_MinusLogProbMetric: 30.7319 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 124/1000
2023-10-24 21:54:32.877 
Epoch 124/1000 
	 loss: 29.8340, MinusLogProbMetric: 29.8340, val_loss: 29.9847, val_MinusLogProbMetric: 29.9847

Epoch 124: val_loss did not improve from 29.63632
196/196 - 31s - loss: 29.8340 - MinusLogProbMetric: 29.8340 - val_loss: 29.9847 - val_MinusLogProbMetric: 29.9847 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 125/1000
2023-10-24 21:55:03.980 
Epoch 125/1000 
	 loss: 29.7723, MinusLogProbMetric: 29.7723, val_loss: 29.6683, val_MinusLogProbMetric: 29.6683

Epoch 125: val_loss did not improve from 29.63632
196/196 - 31s - loss: 29.7723 - MinusLogProbMetric: 29.7723 - val_loss: 29.6683 - val_MinusLogProbMetric: 29.6683 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 126/1000
2023-10-24 21:55:34.295 
Epoch 126/1000 
	 loss: 29.7516, MinusLogProbMetric: 29.7516, val_loss: 31.2326, val_MinusLogProbMetric: 31.2326

Epoch 126: val_loss did not improve from 29.63632
196/196 - 30s - loss: 29.7516 - MinusLogProbMetric: 29.7516 - val_loss: 31.2326 - val_MinusLogProbMetric: 31.2326 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 127/1000
2023-10-24 21:56:05.860 
Epoch 127/1000 
	 loss: 29.8616, MinusLogProbMetric: 29.8616, val_loss: 30.0371, val_MinusLogProbMetric: 30.0371

Epoch 127: val_loss did not improve from 29.63632
196/196 - 32s - loss: 29.8616 - MinusLogProbMetric: 29.8616 - val_loss: 30.0371 - val_MinusLogProbMetric: 30.0371 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 128/1000
2023-10-24 21:56:37.624 
Epoch 128/1000 
	 loss: 29.6414, MinusLogProbMetric: 29.6414, val_loss: 31.3999, val_MinusLogProbMetric: 31.3999

Epoch 128: val_loss did not improve from 29.63632
196/196 - 32s - loss: 29.6414 - MinusLogProbMetric: 29.6414 - val_loss: 31.3999 - val_MinusLogProbMetric: 31.3999 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 129/1000
2023-10-24 21:57:10.255 
Epoch 129/1000 
	 loss: 29.8491, MinusLogProbMetric: 29.8491, val_loss: 29.5849, val_MinusLogProbMetric: 29.5849

Epoch 129: val_loss improved from 29.63632 to 29.58491, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 29.8491 - MinusLogProbMetric: 29.8491 - val_loss: 29.5849 - val_MinusLogProbMetric: 29.5849 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 130/1000
2023-10-24 21:57:41.183 
Epoch 130/1000 
	 loss: 29.6794, MinusLogProbMetric: 29.6794, val_loss: 30.1725, val_MinusLogProbMetric: 30.1725

Epoch 130: val_loss did not improve from 29.58491
196/196 - 30s - loss: 29.6794 - MinusLogProbMetric: 29.6794 - val_loss: 30.1725 - val_MinusLogProbMetric: 30.1725 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 131/1000
2023-10-24 21:58:12.428 
Epoch 131/1000 
	 loss: 29.8884, MinusLogProbMetric: 29.8884, val_loss: 29.8166, val_MinusLogProbMetric: 29.8166

Epoch 131: val_loss did not improve from 29.58491
196/196 - 31s - loss: 29.8884 - MinusLogProbMetric: 29.8884 - val_loss: 29.8166 - val_MinusLogProbMetric: 29.8166 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 132/1000
2023-10-24 21:58:44.461 
Epoch 132/1000 
	 loss: 29.5909, MinusLogProbMetric: 29.5909, val_loss: 29.8651, val_MinusLogProbMetric: 29.8651

Epoch 132: val_loss did not improve from 29.58491
196/196 - 32s - loss: 29.5909 - MinusLogProbMetric: 29.5909 - val_loss: 29.8651 - val_MinusLogProbMetric: 29.8651 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 133/1000
2023-10-24 21:59:14.372 
Epoch 133/1000 
	 loss: 29.6616, MinusLogProbMetric: 29.6616, val_loss: 29.9256, val_MinusLogProbMetric: 29.9256

Epoch 133: val_loss did not improve from 29.58491
196/196 - 30s - loss: 29.6616 - MinusLogProbMetric: 29.6616 - val_loss: 29.9256 - val_MinusLogProbMetric: 29.9256 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 134/1000
2023-10-24 21:59:46.340 
Epoch 134/1000 
	 loss: 29.7298, MinusLogProbMetric: 29.7298, val_loss: 30.4447, val_MinusLogProbMetric: 30.4447

Epoch 134: val_loss did not improve from 29.58491
196/196 - 32s - loss: 29.7298 - MinusLogProbMetric: 29.7298 - val_loss: 30.4447 - val_MinusLogProbMetric: 30.4447 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 135/1000
2023-10-24 22:00:18.284 
Epoch 135/1000 
	 loss: 29.6876, MinusLogProbMetric: 29.6876, val_loss: 30.9389, val_MinusLogProbMetric: 30.9389

Epoch 135: val_loss did not improve from 29.58491
196/196 - 32s - loss: 29.6876 - MinusLogProbMetric: 29.6876 - val_loss: 30.9389 - val_MinusLogProbMetric: 30.9389 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 136/1000
2023-10-24 22:00:49.935 
Epoch 136/1000 
	 loss: 29.5619, MinusLogProbMetric: 29.5619, val_loss: 30.2784, val_MinusLogProbMetric: 30.2784

Epoch 136: val_loss did not improve from 29.58491
196/196 - 32s - loss: 29.5619 - MinusLogProbMetric: 29.5619 - val_loss: 30.2784 - val_MinusLogProbMetric: 30.2784 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 137/1000
2023-10-24 22:01:21.596 
Epoch 137/1000 
	 loss: 29.7547, MinusLogProbMetric: 29.7547, val_loss: 31.0454, val_MinusLogProbMetric: 31.0454

Epoch 137: val_loss did not improve from 29.58491
196/196 - 32s - loss: 29.7547 - MinusLogProbMetric: 29.7547 - val_loss: 31.0454 - val_MinusLogProbMetric: 31.0454 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 138/1000
2023-10-24 22:01:52.642 
Epoch 138/1000 
	 loss: 29.5626, MinusLogProbMetric: 29.5626, val_loss: 30.6115, val_MinusLogProbMetric: 30.6115

Epoch 138: val_loss did not improve from 29.58491
196/196 - 31s - loss: 29.5626 - MinusLogProbMetric: 29.5626 - val_loss: 30.6115 - val_MinusLogProbMetric: 30.6115 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 139/1000
2023-10-24 22:02:24.365 
Epoch 139/1000 
	 loss: 29.5588, MinusLogProbMetric: 29.5588, val_loss: 30.1374, val_MinusLogProbMetric: 30.1374

Epoch 139: val_loss did not improve from 29.58491
196/196 - 32s - loss: 29.5588 - MinusLogProbMetric: 29.5588 - val_loss: 30.1374 - val_MinusLogProbMetric: 30.1374 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 140/1000
2023-10-24 22:02:55.410 
Epoch 140/1000 
	 loss: 29.7604, MinusLogProbMetric: 29.7604, val_loss: 30.9820, val_MinusLogProbMetric: 30.9820

Epoch 140: val_loss did not improve from 29.58491
196/196 - 31s - loss: 29.7604 - MinusLogProbMetric: 29.7604 - val_loss: 30.9820 - val_MinusLogProbMetric: 30.9820 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 141/1000
2023-10-24 22:03:24.879 
Epoch 141/1000 
	 loss: 29.7055, MinusLogProbMetric: 29.7055, val_loss: 30.7263, val_MinusLogProbMetric: 30.7263

Epoch 141: val_loss did not improve from 29.58491
196/196 - 29s - loss: 29.7055 - MinusLogProbMetric: 29.7055 - val_loss: 30.7263 - val_MinusLogProbMetric: 30.7263 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 142/1000
2023-10-24 22:03:55.986 
Epoch 142/1000 
	 loss: 29.7067, MinusLogProbMetric: 29.7067, val_loss: 30.6955, val_MinusLogProbMetric: 30.6955

Epoch 142: val_loss did not improve from 29.58491
196/196 - 31s - loss: 29.7067 - MinusLogProbMetric: 29.7067 - val_loss: 30.6955 - val_MinusLogProbMetric: 30.6955 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 143/1000
2023-10-24 22:04:27.014 
Epoch 143/1000 
	 loss: 29.6545, MinusLogProbMetric: 29.6545, val_loss: 29.3838, val_MinusLogProbMetric: 29.3838

Epoch 143: val_loss improved from 29.58491 to 29.38377, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 31s - loss: 29.6545 - MinusLogProbMetric: 29.6545 - val_loss: 29.3838 - val_MinusLogProbMetric: 29.3838 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 144/1000
2023-10-24 22:04:56.769 
Epoch 144/1000 
	 loss: 29.4289, MinusLogProbMetric: 29.4289, val_loss: 29.6181, val_MinusLogProbMetric: 29.6181

Epoch 144: val_loss did not improve from 29.38377
196/196 - 29s - loss: 29.4289 - MinusLogProbMetric: 29.4289 - val_loss: 29.6181 - val_MinusLogProbMetric: 29.6181 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 145/1000
2023-10-24 22:05:26.816 
Epoch 145/1000 
	 loss: 29.3726, MinusLogProbMetric: 29.3726, val_loss: 30.0031, val_MinusLogProbMetric: 30.0031

Epoch 145: val_loss did not improve from 29.38377
196/196 - 30s - loss: 29.3726 - MinusLogProbMetric: 29.3726 - val_loss: 30.0031 - val_MinusLogProbMetric: 30.0031 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 146/1000
2023-10-24 22:05:58.544 
Epoch 146/1000 
	 loss: 29.4585, MinusLogProbMetric: 29.4585, val_loss: 29.9153, val_MinusLogProbMetric: 29.9153

Epoch 146: val_loss did not improve from 29.38377
196/196 - 32s - loss: 29.4585 - MinusLogProbMetric: 29.4585 - val_loss: 29.9153 - val_MinusLogProbMetric: 29.9153 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 147/1000
2023-10-24 22:06:30.910 
Epoch 147/1000 
	 loss: 29.4435, MinusLogProbMetric: 29.4435, val_loss: 30.0959, val_MinusLogProbMetric: 30.0959

Epoch 147: val_loss did not improve from 29.38377
196/196 - 32s - loss: 29.4435 - MinusLogProbMetric: 29.4435 - val_loss: 30.0959 - val_MinusLogProbMetric: 30.0959 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 148/1000
2023-10-24 22:07:03.681 
Epoch 148/1000 
	 loss: 29.3825, MinusLogProbMetric: 29.3825, val_loss: 30.0524, val_MinusLogProbMetric: 30.0524

Epoch 148: val_loss did not improve from 29.38377
196/196 - 33s - loss: 29.3825 - MinusLogProbMetric: 29.3825 - val_loss: 30.0524 - val_MinusLogProbMetric: 30.0524 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 149/1000
2023-10-24 22:07:35.841 
Epoch 149/1000 
	 loss: 29.4041, MinusLogProbMetric: 29.4041, val_loss: 29.9998, val_MinusLogProbMetric: 29.9998

Epoch 149: val_loss did not improve from 29.38377
196/196 - 32s - loss: 29.4041 - MinusLogProbMetric: 29.4041 - val_loss: 29.9998 - val_MinusLogProbMetric: 29.9998 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 150/1000
2023-10-24 22:08:08.614 
Epoch 150/1000 
	 loss: 29.5440, MinusLogProbMetric: 29.5440, val_loss: 30.1190, val_MinusLogProbMetric: 30.1190

Epoch 150: val_loss did not improve from 29.38377
196/196 - 33s - loss: 29.5440 - MinusLogProbMetric: 29.5440 - val_loss: 30.1190 - val_MinusLogProbMetric: 30.1190 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 151/1000
2023-10-24 22:08:39.880 
Epoch 151/1000 
	 loss: 29.4415, MinusLogProbMetric: 29.4415, val_loss: 29.3305, val_MinusLogProbMetric: 29.3305

Epoch 151: val_loss improved from 29.38377 to 29.33051, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 29.4415 - MinusLogProbMetric: 29.4415 - val_loss: 29.3305 - val_MinusLogProbMetric: 29.3305 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 152/1000
2023-10-24 22:09:11.610 
Epoch 152/1000 
	 loss: 29.4351, MinusLogProbMetric: 29.4351, val_loss: 29.5877, val_MinusLogProbMetric: 29.5877

Epoch 152: val_loss did not improve from 29.33051
196/196 - 31s - loss: 29.4351 - MinusLogProbMetric: 29.4351 - val_loss: 29.5877 - val_MinusLogProbMetric: 29.5877 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 153/1000
2023-10-24 22:09:43.386 
Epoch 153/1000 
	 loss: 29.5112, MinusLogProbMetric: 29.5112, val_loss: 30.2373, val_MinusLogProbMetric: 30.2373

Epoch 153: val_loss did not improve from 29.33051
196/196 - 32s - loss: 29.5112 - MinusLogProbMetric: 29.5112 - val_loss: 30.2373 - val_MinusLogProbMetric: 30.2373 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 154/1000
2023-10-24 22:10:14.937 
Epoch 154/1000 
	 loss: 29.3480, MinusLogProbMetric: 29.3480, val_loss: 29.2357, val_MinusLogProbMetric: 29.2357

Epoch 154: val_loss improved from 29.33051 to 29.23572, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 29.3480 - MinusLogProbMetric: 29.3480 - val_loss: 29.2357 - val_MinusLogProbMetric: 29.2357 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 155/1000
2023-10-24 22:10:46.471 
Epoch 155/1000 
	 loss: 29.2871, MinusLogProbMetric: 29.2871, val_loss: 30.7538, val_MinusLogProbMetric: 30.7538

Epoch 155: val_loss did not improve from 29.23572
196/196 - 31s - loss: 29.2871 - MinusLogProbMetric: 29.2871 - val_loss: 30.7538 - val_MinusLogProbMetric: 30.7538 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 156/1000
2023-10-24 22:11:18.174 
Epoch 156/1000 
	 loss: 29.2821, MinusLogProbMetric: 29.2821, val_loss: 31.3304, val_MinusLogProbMetric: 31.3304

Epoch 156: val_loss did not improve from 29.23572
196/196 - 32s - loss: 29.2821 - MinusLogProbMetric: 29.2821 - val_loss: 31.3304 - val_MinusLogProbMetric: 31.3304 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 157/1000
2023-10-24 22:11:50.244 
Epoch 157/1000 
	 loss: 29.5590, MinusLogProbMetric: 29.5590, val_loss: 30.0677, val_MinusLogProbMetric: 30.0677

Epoch 157: val_loss did not improve from 29.23572
196/196 - 32s - loss: 29.5590 - MinusLogProbMetric: 29.5590 - val_loss: 30.0677 - val_MinusLogProbMetric: 30.0677 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 158/1000
2023-10-24 22:12:22.994 
Epoch 158/1000 
	 loss: 29.2489, MinusLogProbMetric: 29.2489, val_loss: 29.7893, val_MinusLogProbMetric: 29.7893

Epoch 158: val_loss did not improve from 29.23572
196/196 - 33s - loss: 29.2489 - MinusLogProbMetric: 29.2489 - val_loss: 29.7893 - val_MinusLogProbMetric: 29.7893 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 159/1000
2023-10-24 22:12:55.142 
Epoch 159/1000 
	 loss: 29.3074, MinusLogProbMetric: 29.3074, val_loss: 29.2584, val_MinusLogProbMetric: 29.2584

Epoch 159: val_loss did not improve from 29.23572
196/196 - 32s - loss: 29.3074 - MinusLogProbMetric: 29.3074 - val_loss: 29.2584 - val_MinusLogProbMetric: 29.2584 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 160/1000
2023-10-24 22:13:27.320 
Epoch 160/1000 
	 loss: 29.2980, MinusLogProbMetric: 29.2980, val_loss: 29.2128, val_MinusLogProbMetric: 29.2128

Epoch 160: val_loss improved from 29.23572 to 29.21277, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 29.2980 - MinusLogProbMetric: 29.2980 - val_loss: 29.2128 - val_MinusLogProbMetric: 29.2128 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 161/1000
2023-10-24 22:14:00.367 
Epoch 161/1000 
	 loss: 29.2676, MinusLogProbMetric: 29.2676, val_loss: 29.2760, val_MinusLogProbMetric: 29.2760

Epoch 161: val_loss did not improve from 29.21277
196/196 - 33s - loss: 29.2676 - MinusLogProbMetric: 29.2676 - val_loss: 29.2760 - val_MinusLogProbMetric: 29.2760 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 162/1000
2023-10-24 22:14:32.056 
Epoch 162/1000 
	 loss: 29.3616, MinusLogProbMetric: 29.3616, val_loss: 29.7256, val_MinusLogProbMetric: 29.7256

Epoch 162: val_loss did not improve from 29.21277
196/196 - 32s - loss: 29.3616 - MinusLogProbMetric: 29.3616 - val_loss: 29.7256 - val_MinusLogProbMetric: 29.7256 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 163/1000
2023-10-24 22:15:04.247 
Epoch 163/1000 
	 loss: 29.2695, MinusLogProbMetric: 29.2695, val_loss: 29.3660, val_MinusLogProbMetric: 29.3660

Epoch 163: val_loss did not improve from 29.21277
196/196 - 32s - loss: 29.2695 - MinusLogProbMetric: 29.2695 - val_loss: 29.3660 - val_MinusLogProbMetric: 29.3660 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 164/1000
2023-10-24 22:15:35.779 
Epoch 164/1000 
	 loss: 29.2026, MinusLogProbMetric: 29.2026, val_loss: 29.2798, val_MinusLogProbMetric: 29.2798

Epoch 164: val_loss did not improve from 29.21277
196/196 - 32s - loss: 29.2026 - MinusLogProbMetric: 29.2026 - val_loss: 29.2798 - val_MinusLogProbMetric: 29.2798 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 165/1000
2023-10-24 22:16:07.035 
Epoch 165/1000 
	 loss: 29.3702, MinusLogProbMetric: 29.3702, val_loss: 29.4615, val_MinusLogProbMetric: 29.4615

Epoch 165: val_loss did not improve from 29.21277
196/196 - 31s - loss: 29.3702 - MinusLogProbMetric: 29.3702 - val_loss: 29.4615 - val_MinusLogProbMetric: 29.4615 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 166/1000
2023-10-24 22:16:39.177 
Epoch 166/1000 
	 loss: 29.0613, MinusLogProbMetric: 29.0613, val_loss: 29.4593, val_MinusLogProbMetric: 29.4593

Epoch 166: val_loss did not improve from 29.21277
196/196 - 32s - loss: 29.0613 - MinusLogProbMetric: 29.0613 - val_loss: 29.4593 - val_MinusLogProbMetric: 29.4593 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 167/1000
2023-10-24 22:17:11.906 
Epoch 167/1000 
	 loss: 29.1162, MinusLogProbMetric: 29.1162, val_loss: 29.3427, val_MinusLogProbMetric: 29.3427

Epoch 167: val_loss did not improve from 29.21277
196/196 - 33s - loss: 29.1162 - MinusLogProbMetric: 29.1162 - val_loss: 29.3427 - val_MinusLogProbMetric: 29.3427 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 168/1000
2023-10-24 22:17:42.945 
Epoch 168/1000 
	 loss: 29.5312, MinusLogProbMetric: 29.5312, val_loss: 29.9818, val_MinusLogProbMetric: 29.9818

Epoch 168: val_loss did not improve from 29.21277
196/196 - 31s - loss: 29.5312 - MinusLogProbMetric: 29.5312 - val_loss: 29.9818 - val_MinusLogProbMetric: 29.9818 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 169/1000
2023-10-24 22:18:14.446 
Epoch 169/1000 
	 loss: 29.0783, MinusLogProbMetric: 29.0783, val_loss: 30.3387, val_MinusLogProbMetric: 30.3387

Epoch 169: val_loss did not improve from 29.21277
196/196 - 31s - loss: 29.0783 - MinusLogProbMetric: 29.0783 - val_loss: 30.3387 - val_MinusLogProbMetric: 30.3387 - lr: 0.0010 - 31s/epoch - 161ms/step
Epoch 170/1000
2023-10-24 22:18:46.708 
Epoch 170/1000 
	 loss: 29.2827, MinusLogProbMetric: 29.2827, val_loss: 29.7157, val_MinusLogProbMetric: 29.7157

Epoch 170: val_loss did not improve from 29.21277
196/196 - 32s - loss: 29.2827 - MinusLogProbMetric: 29.2827 - val_loss: 29.7157 - val_MinusLogProbMetric: 29.7157 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 171/1000
2023-10-24 22:19:18.729 
Epoch 171/1000 
	 loss: 29.0807, MinusLogProbMetric: 29.0807, val_loss: 29.3981, val_MinusLogProbMetric: 29.3981

Epoch 171: val_loss did not improve from 29.21277
196/196 - 32s - loss: 29.0807 - MinusLogProbMetric: 29.0807 - val_loss: 29.3981 - val_MinusLogProbMetric: 29.3981 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 172/1000
2023-10-24 22:19:49.706 
Epoch 172/1000 
	 loss: 29.2755, MinusLogProbMetric: 29.2755, val_loss: 30.8301, val_MinusLogProbMetric: 30.8301

Epoch 172: val_loss did not improve from 29.21277
196/196 - 31s - loss: 29.2755 - MinusLogProbMetric: 29.2755 - val_loss: 30.8301 - val_MinusLogProbMetric: 30.8301 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 173/1000
2023-10-24 22:20:21.755 
Epoch 173/1000 
	 loss: 29.1213, MinusLogProbMetric: 29.1213, val_loss: 29.6846, val_MinusLogProbMetric: 29.6846

Epoch 173: val_loss did not improve from 29.21277
196/196 - 32s - loss: 29.1213 - MinusLogProbMetric: 29.1213 - val_loss: 29.6846 - val_MinusLogProbMetric: 29.6846 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 174/1000
2023-10-24 22:20:53.933 
Epoch 174/1000 
	 loss: 29.1591, MinusLogProbMetric: 29.1591, val_loss: 29.7181, val_MinusLogProbMetric: 29.7181

Epoch 174: val_loss did not improve from 29.21277
196/196 - 32s - loss: 29.1591 - MinusLogProbMetric: 29.1591 - val_loss: 29.7181 - val_MinusLogProbMetric: 29.7181 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 175/1000
2023-10-24 22:21:26.184 
Epoch 175/1000 
	 loss: 29.1779, MinusLogProbMetric: 29.1779, val_loss: 29.6199, val_MinusLogProbMetric: 29.6199

Epoch 175: val_loss did not improve from 29.21277
196/196 - 32s - loss: 29.1779 - MinusLogProbMetric: 29.1779 - val_loss: 29.6199 - val_MinusLogProbMetric: 29.6199 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 176/1000
2023-10-24 22:21:56.658 
Epoch 176/1000 
	 loss: 29.0824, MinusLogProbMetric: 29.0824, val_loss: 29.6916, val_MinusLogProbMetric: 29.6916

Epoch 176: val_loss did not improve from 29.21277
196/196 - 30s - loss: 29.0824 - MinusLogProbMetric: 29.0824 - val_loss: 29.6916 - val_MinusLogProbMetric: 29.6916 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 177/1000
2023-10-24 22:22:28.854 
Epoch 177/1000 
	 loss: 29.2052, MinusLogProbMetric: 29.2052, val_loss: 29.8703, val_MinusLogProbMetric: 29.8703

Epoch 177: val_loss did not improve from 29.21277
196/196 - 32s - loss: 29.2052 - MinusLogProbMetric: 29.2052 - val_loss: 29.8703 - val_MinusLogProbMetric: 29.8703 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 178/1000
2023-10-24 22:22:58.966 
Epoch 178/1000 
	 loss: 29.1110, MinusLogProbMetric: 29.1110, val_loss: 29.2401, val_MinusLogProbMetric: 29.2401

Epoch 178: val_loss did not improve from 29.21277
196/196 - 30s - loss: 29.1110 - MinusLogProbMetric: 29.1110 - val_loss: 29.2401 - val_MinusLogProbMetric: 29.2401 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 179/1000
2023-10-24 22:23:30.653 
Epoch 179/1000 
	 loss: 29.0703, MinusLogProbMetric: 29.0703, val_loss: 29.5101, val_MinusLogProbMetric: 29.5101

Epoch 179: val_loss did not improve from 29.21277
196/196 - 32s - loss: 29.0703 - MinusLogProbMetric: 29.0703 - val_loss: 29.5101 - val_MinusLogProbMetric: 29.5101 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 180/1000
2023-10-24 22:24:00.877 
Epoch 180/1000 
	 loss: 29.3267, MinusLogProbMetric: 29.3267, val_loss: 29.9301, val_MinusLogProbMetric: 29.9301

Epoch 180: val_loss did not improve from 29.21277
196/196 - 30s - loss: 29.3267 - MinusLogProbMetric: 29.3267 - val_loss: 29.9301 - val_MinusLogProbMetric: 29.9301 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 181/1000
2023-10-24 22:24:31.525 
Epoch 181/1000 
	 loss: 29.0152, MinusLogProbMetric: 29.0152, val_loss: 29.3939, val_MinusLogProbMetric: 29.3939

Epoch 181: val_loss did not improve from 29.21277
196/196 - 31s - loss: 29.0152 - MinusLogProbMetric: 29.0152 - val_loss: 29.3939 - val_MinusLogProbMetric: 29.3939 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 182/1000
2023-10-24 22:25:01.449 
Epoch 182/1000 
	 loss: 29.1576, MinusLogProbMetric: 29.1576, val_loss: 29.1697, val_MinusLogProbMetric: 29.1697

Epoch 182: val_loss improved from 29.21277 to 29.16966, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 30s - loss: 29.1576 - MinusLogProbMetric: 29.1576 - val_loss: 29.1697 - val_MinusLogProbMetric: 29.1697 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 183/1000
2023-10-24 22:25:31.175 
Epoch 183/1000 
	 loss: 28.9763, MinusLogProbMetric: 28.9763, val_loss: 29.3689, val_MinusLogProbMetric: 29.3689

Epoch 183: val_loss did not improve from 29.16966
196/196 - 29s - loss: 28.9763 - MinusLogProbMetric: 28.9763 - val_loss: 29.3689 - val_MinusLogProbMetric: 29.3689 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 184/1000
2023-10-24 22:26:01.219 
Epoch 184/1000 
	 loss: 29.0469, MinusLogProbMetric: 29.0469, val_loss: 29.4208, val_MinusLogProbMetric: 29.4208

Epoch 184: val_loss did not improve from 29.16966
196/196 - 30s - loss: 29.0469 - MinusLogProbMetric: 29.0469 - val_loss: 29.4208 - val_MinusLogProbMetric: 29.4208 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 185/1000
2023-10-24 22:26:31.688 
Epoch 185/1000 
	 loss: 29.0987, MinusLogProbMetric: 29.0987, val_loss: 29.6706, val_MinusLogProbMetric: 29.6706

Epoch 185: val_loss did not improve from 29.16966
196/196 - 30s - loss: 29.0987 - MinusLogProbMetric: 29.0987 - val_loss: 29.6706 - val_MinusLogProbMetric: 29.6706 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 186/1000
2023-10-24 22:27:01.547 
Epoch 186/1000 
	 loss: 29.2932, MinusLogProbMetric: 29.2932, val_loss: 29.0373, val_MinusLogProbMetric: 29.0373

Epoch 186: val_loss improved from 29.16966 to 29.03733, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 30s - loss: 29.2932 - MinusLogProbMetric: 29.2932 - val_loss: 29.0373 - val_MinusLogProbMetric: 29.0373 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 187/1000
2023-10-24 22:27:32.220 
Epoch 187/1000 
	 loss: 28.9703, MinusLogProbMetric: 28.9703, val_loss: 29.5281, val_MinusLogProbMetric: 29.5281

Epoch 187: val_loss did not improve from 29.03733
196/196 - 30s - loss: 28.9703 - MinusLogProbMetric: 28.9703 - val_loss: 29.5281 - val_MinusLogProbMetric: 29.5281 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 188/1000
2023-10-24 22:28:04.915 
Epoch 188/1000 
	 loss: 29.1198, MinusLogProbMetric: 29.1198, val_loss: 29.4569, val_MinusLogProbMetric: 29.4569

Epoch 188: val_loss did not improve from 29.03733
196/196 - 33s - loss: 29.1198 - MinusLogProbMetric: 29.1198 - val_loss: 29.4569 - val_MinusLogProbMetric: 29.4569 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 189/1000
2023-10-24 22:28:36.297 
Epoch 189/1000 
	 loss: 29.0804, MinusLogProbMetric: 29.0804, val_loss: 29.6834, val_MinusLogProbMetric: 29.6834

Epoch 189: val_loss did not improve from 29.03733
196/196 - 31s - loss: 29.0804 - MinusLogProbMetric: 29.0804 - val_loss: 29.6834 - val_MinusLogProbMetric: 29.6834 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 190/1000
2023-10-24 22:29:08.137 
Epoch 190/1000 
	 loss: 29.0286, MinusLogProbMetric: 29.0286, val_loss: 29.3765, val_MinusLogProbMetric: 29.3765

Epoch 190: val_loss did not improve from 29.03733
196/196 - 32s - loss: 29.0286 - MinusLogProbMetric: 29.0286 - val_loss: 29.3765 - val_MinusLogProbMetric: 29.3765 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 191/1000
2023-10-24 22:29:39.581 
Epoch 191/1000 
	 loss: 28.8841, MinusLogProbMetric: 28.8841, val_loss: 29.6913, val_MinusLogProbMetric: 29.6913

Epoch 191: val_loss did not improve from 29.03733
196/196 - 31s - loss: 28.8841 - MinusLogProbMetric: 28.8841 - val_loss: 29.6913 - val_MinusLogProbMetric: 29.6913 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 192/1000
2023-10-24 22:30:11.058 
Epoch 192/1000 
	 loss: 29.2340, MinusLogProbMetric: 29.2340, val_loss: 29.1770, val_MinusLogProbMetric: 29.1770

Epoch 192: val_loss did not improve from 29.03733
196/196 - 31s - loss: 29.2340 - MinusLogProbMetric: 29.2340 - val_loss: 29.1770 - val_MinusLogProbMetric: 29.1770 - lr: 0.0010 - 31s/epoch - 161ms/step
Epoch 193/1000
2023-10-24 22:30:42.883 
Epoch 193/1000 
	 loss: 28.8907, MinusLogProbMetric: 28.8907, val_loss: 29.5106, val_MinusLogProbMetric: 29.5106

Epoch 193: val_loss did not improve from 29.03733
196/196 - 32s - loss: 28.8907 - MinusLogProbMetric: 28.8907 - val_loss: 29.5106 - val_MinusLogProbMetric: 29.5106 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 194/1000
2023-10-24 22:31:14.639 
Epoch 194/1000 
	 loss: 28.9864, MinusLogProbMetric: 28.9864, val_loss: 29.3283, val_MinusLogProbMetric: 29.3283

Epoch 194: val_loss did not improve from 29.03733
196/196 - 32s - loss: 28.9864 - MinusLogProbMetric: 28.9864 - val_loss: 29.3283 - val_MinusLogProbMetric: 29.3283 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 195/1000
2023-10-24 22:31:46.489 
Epoch 195/1000 
	 loss: 29.0663, MinusLogProbMetric: 29.0663, val_loss: 29.3406, val_MinusLogProbMetric: 29.3406

Epoch 195: val_loss did not improve from 29.03733
196/196 - 32s - loss: 29.0663 - MinusLogProbMetric: 29.0663 - val_loss: 29.3406 - val_MinusLogProbMetric: 29.3406 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 196/1000
2023-10-24 22:32:18.835 
Epoch 196/1000 
	 loss: 28.9528, MinusLogProbMetric: 28.9528, val_loss: 29.7613, val_MinusLogProbMetric: 29.7613

Epoch 196: val_loss did not improve from 29.03733
196/196 - 32s - loss: 28.9528 - MinusLogProbMetric: 28.9528 - val_loss: 29.7613 - val_MinusLogProbMetric: 29.7613 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 197/1000
2023-10-24 22:32:48.643 
Epoch 197/1000 
	 loss: 29.1925, MinusLogProbMetric: 29.1925, val_loss: 29.1722, val_MinusLogProbMetric: 29.1722

Epoch 197: val_loss did not improve from 29.03733
196/196 - 30s - loss: 29.1925 - MinusLogProbMetric: 29.1925 - val_loss: 29.1722 - val_MinusLogProbMetric: 29.1722 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 198/1000
2023-10-24 22:33:18.276 
Epoch 198/1000 
	 loss: 29.0148, MinusLogProbMetric: 29.0148, val_loss: 31.0299, val_MinusLogProbMetric: 31.0299

Epoch 198: val_loss did not improve from 29.03733
196/196 - 30s - loss: 29.0148 - MinusLogProbMetric: 29.0148 - val_loss: 31.0299 - val_MinusLogProbMetric: 31.0299 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 199/1000
2023-10-24 22:33:48.441 
Epoch 199/1000 
	 loss: 28.9278, MinusLogProbMetric: 28.9278, val_loss: 28.9600, val_MinusLogProbMetric: 28.9600

Epoch 199: val_loss improved from 29.03733 to 28.95996, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 31s - loss: 28.9278 - MinusLogProbMetric: 28.9278 - val_loss: 28.9600 - val_MinusLogProbMetric: 28.9600 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 200/1000
2023-10-24 22:34:19.918 
Epoch 200/1000 
	 loss: 29.0166, MinusLogProbMetric: 29.0166, val_loss: 29.5669, val_MinusLogProbMetric: 29.5669

Epoch 200: val_loss did not improve from 28.95996
196/196 - 31s - loss: 29.0166 - MinusLogProbMetric: 29.0166 - val_loss: 29.5669 - val_MinusLogProbMetric: 29.5669 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 201/1000
2023-10-24 22:34:52.447 
Epoch 201/1000 
	 loss: 28.8993, MinusLogProbMetric: 28.8993, val_loss: 29.3908, val_MinusLogProbMetric: 29.3908

Epoch 201: val_loss did not improve from 28.95996
196/196 - 33s - loss: 28.8993 - MinusLogProbMetric: 28.8993 - val_loss: 29.3908 - val_MinusLogProbMetric: 29.3908 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 202/1000
2023-10-24 22:35:24.412 
Epoch 202/1000 
	 loss: 29.1179, MinusLogProbMetric: 29.1179, val_loss: 29.3854, val_MinusLogProbMetric: 29.3854

Epoch 202: val_loss did not improve from 28.95996
196/196 - 32s - loss: 29.1179 - MinusLogProbMetric: 29.1179 - val_loss: 29.3854 - val_MinusLogProbMetric: 29.3854 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 203/1000
2023-10-24 22:35:55.630 
Epoch 203/1000 
	 loss: 29.0360, MinusLogProbMetric: 29.0360, val_loss: 29.7818, val_MinusLogProbMetric: 29.7818

Epoch 203: val_loss did not improve from 28.95996
196/196 - 31s - loss: 29.0360 - MinusLogProbMetric: 29.0360 - val_loss: 29.7818 - val_MinusLogProbMetric: 29.7818 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 204/1000
2023-10-24 22:36:27.145 
Epoch 204/1000 
	 loss: 29.0906, MinusLogProbMetric: 29.0906, val_loss: 29.8984, val_MinusLogProbMetric: 29.8984

Epoch 204: val_loss did not improve from 28.95996
196/196 - 32s - loss: 29.0906 - MinusLogProbMetric: 29.0906 - val_loss: 29.8984 - val_MinusLogProbMetric: 29.8984 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 205/1000
2023-10-24 22:36:58.701 
Epoch 205/1000 
	 loss: 28.8704, MinusLogProbMetric: 28.8704, val_loss: 29.0180, val_MinusLogProbMetric: 29.0180

Epoch 205: val_loss did not improve from 28.95996
196/196 - 32s - loss: 28.8704 - MinusLogProbMetric: 28.8704 - val_loss: 29.0180 - val_MinusLogProbMetric: 29.0180 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 206/1000
2023-10-24 22:37:30.155 
Epoch 206/1000 
	 loss: 28.9895, MinusLogProbMetric: 28.9895, val_loss: 30.1773, val_MinusLogProbMetric: 30.1773

Epoch 206: val_loss did not improve from 28.95996
196/196 - 31s - loss: 28.9895 - MinusLogProbMetric: 28.9895 - val_loss: 30.1773 - val_MinusLogProbMetric: 30.1773 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 207/1000
2023-10-24 22:38:02.065 
Epoch 207/1000 
	 loss: 28.9399, MinusLogProbMetric: 28.9399, val_loss: 29.5079, val_MinusLogProbMetric: 29.5079

Epoch 207: val_loss did not improve from 28.95996
196/196 - 32s - loss: 28.9399 - MinusLogProbMetric: 28.9399 - val_loss: 29.5079 - val_MinusLogProbMetric: 29.5079 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 208/1000
2023-10-24 22:38:31.722 
Epoch 208/1000 
	 loss: 28.9944, MinusLogProbMetric: 28.9944, val_loss: 29.1756, val_MinusLogProbMetric: 29.1756

Epoch 208: val_loss did not improve from 28.95996
196/196 - 30s - loss: 28.9944 - MinusLogProbMetric: 28.9944 - val_loss: 29.1756 - val_MinusLogProbMetric: 29.1756 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 209/1000
2023-10-24 22:39:01.786 
Epoch 209/1000 
	 loss: 28.8254, MinusLogProbMetric: 28.8254, val_loss: 29.2834, val_MinusLogProbMetric: 29.2834

Epoch 209: val_loss did not improve from 28.95996
196/196 - 30s - loss: 28.8254 - MinusLogProbMetric: 28.8254 - val_loss: 29.2834 - val_MinusLogProbMetric: 29.2834 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 210/1000
2023-10-24 22:39:31.170 
Epoch 210/1000 
	 loss: 28.8989, MinusLogProbMetric: 28.8989, val_loss: 29.2392, val_MinusLogProbMetric: 29.2392

Epoch 210: val_loss did not improve from 28.95996
196/196 - 29s - loss: 28.8989 - MinusLogProbMetric: 28.8989 - val_loss: 29.2392 - val_MinusLogProbMetric: 29.2392 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 211/1000
2023-10-24 22:40:00.698 
Epoch 211/1000 
	 loss: 29.1200, MinusLogProbMetric: 29.1200, val_loss: 29.0620, val_MinusLogProbMetric: 29.0620

Epoch 211: val_loss did not improve from 28.95996
196/196 - 30s - loss: 29.1200 - MinusLogProbMetric: 29.1200 - val_loss: 29.0620 - val_MinusLogProbMetric: 29.0620 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 212/1000
2023-10-24 22:40:30.886 
Epoch 212/1000 
	 loss: 28.9468, MinusLogProbMetric: 28.9468, val_loss: 28.9060, val_MinusLogProbMetric: 28.9060

Epoch 212: val_loss improved from 28.95996 to 28.90604, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 31s - loss: 28.9468 - MinusLogProbMetric: 28.9468 - val_loss: 28.9060 - val_MinusLogProbMetric: 28.9060 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 213/1000
2023-10-24 22:41:02.294 
Epoch 213/1000 
	 loss: 28.8651, MinusLogProbMetric: 28.8651, val_loss: 29.8613, val_MinusLogProbMetric: 29.8613

Epoch 213: val_loss did not improve from 28.90604
196/196 - 31s - loss: 28.8651 - MinusLogProbMetric: 28.8651 - val_loss: 29.8613 - val_MinusLogProbMetric: 29.8613 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 214/1000
2023-10-24 22:41:34.809 
Epoch 214/1000 
	 loss: 28.7373, MinusLogProbMetric: 28.7373, val_loss: 29.1757, val_MinusLogProbMetric: 29.1757

Epoch 214: val_loss did not improve from 28.90604
196/196 - 33s - loss: 28.7373 - MinusLogProbMetric: 28.7373 - val_loss: 29.1757 - val_MinusLogProbMetric: 29.1757 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 215/1000
2023-10-24 22:42:06.768 
Epoch 215/1000 
	 loss: 28.8620, MinusLogProbMetric: 28.8620, val_loss: 29.1398, val_MinusLogProbMetric: 29.1398

Epoch 215: val_loss did not improve from 28.90604
196/196 - 32s - loss: 28.8620 - MinusLogProbMetric: 28.8620 - val_loss: 29.1398 - val_MinusLogProbMetric: 29.1398 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 216/1000
2023-10-24 22:42:38.294 
Epoch 216/1000 
	 loss: 28.7799, MinusLogProbMetric: 28.7799, val_loss: 29.1886, val_MinusLogProbMetric: 29.1886

Epoch 216: val_loss did not improve from 28.90604
196/196 - 32s - loss: 28.7799 - MinusLogProbMetric: 28.7799 - val_loss: 29.1886 - val_MinusLogProbMetric: 29.1886 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 217/1000
2023-10-24 22:43:08.524 
Epoch 217/1000 
	 loss: 29.1009, MinusLogProbMetric: 29.1009, val_loss: 29.9614, val_MinusLogProbMetric: 29.9614

Epoch 217: val_loss did not improve from 28.90604
196/196 - 30s - loss: 29.1009 - MinusLogProbMetric: 29.1009 - val_loss: 29.9614 - val_MinusLogProbMetric: 29.9614 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 218/1000
2023-10-24 22:43:41.064 
Epoch 218/1000 
	 loss: 28.7626, MinusLogProbMetric: 28.7626, val_loss: 29.8820, val_MinusLogProbMetric: 29.8820

Epoch 218: val_loss did not improve from 28.90604
196/196 - 33s - loss: 28.7626 - MinusLogProbMetric: 28.7626 - val_loss: 29.8820 - val_MinusLogProbMetric: 29.8820 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 219/1000
2023-10-24 22:44:13.676 
Epoch 219/1000 
	 loss: 28.7839, MinusLogProbMetric: 28.7839, val_loss: 29.4425, val_MinusLogProbMetric: 29.4425

Epoch 219: val_loss did not improve from 28.90604
196/196 - 33s - loss: 28.7839 - MinusLogProbMetric: 28.7839 - val_loss: 29.4425 - val_MinusLogProbMetric: 29.4425 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 220/1000
2023-10-24 22:44:45.668 
Epoch 220/1000 
	 loss: 28.9282, MinusLogProbMetric: 28.9282, val_loss: 30.1234, val_MinusLogProbMetric: 30.1234

Epoch 220: val_loss did not improve from 28.90604
196/196 - 32s - loss: 28.9282 - MinusLogProbMetric: 28.9282 - val_loss: 30.1234 - val_MinusLogProbMetric: 30.1234 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 221/1000
2023-10-24 22:45:18.076 
Epoch 221/1000 
	 loss: 28.8102, MinusLogProbMetric: 28.8102, val_loss: 29.2601, val_MinusLogProbMetric: 29.2601

Epoch 221: val_loss did not improve from 28.90604
196/196 - 32s - loss: 28.8102 - MinusLogProbMetric: 28.8102 - val_loss: 29.2601 - val_MinusLogProbMetric: 29.2601 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 222/1000
2023-10-24 22:45:50.266 
Epoch 222/1000 
	 loss: 28.7925, MinusLogProbMetric: 28.7925, val_loss: 29.5264, val_MinusLogProbMetric: 29.5264

Epoch 222: val_loss did not improve from 28.90604
196/196 - 32s - loss: 28.7925 - MinusLogProbMetric: 28.7925 - val_loss: 29.5264 - val_MinusLogProbMetric: 29.5264 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 223/1000
2023-10-24 22:46:22.848 
Epoch 223/1000 
	 loss: 28.7445, MinusLogProbMetric: 28.7445, val_loss: 29.0816, val_MinusLogProbMetric: 29.0816

Epoch 223: val_loss did not improve from 28.90604
196/196 - 33s - loss: 28.7445 - MinusLogProbMetric: 28.7445 - val_loss: 29.0816 - val_MinusLogProbMetric: 29.0816 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 224/1000
2023-10-24 22:46:55.460 
Epoch 224/1000 
	 loss: 28.7464, MinusLogProbMetric: 28.7464, val_loss: 29.2896, val_MinusLogProbMetric: 29.2896

Epoch 224: val_loss did not improve from 28.90604
196/196 - 33s - loss: 28.7464 - MinusLogProbMetric: 28.7464 - val_loss: 29.2896 - val_MinusLogProbMetric: 29.2896 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 225/1000
2023-10-24 22:47:28.105 
Epoch 225/1000 
	 loss: 28.7827, MinusLogProbMetric: 28.7827, val_loss: 29.4963, val_MinusLogProbMetric: 29.4963

Epoch 225: val_loss did not improve from 28.90604
196/196 - 33s - loss: 28.7827 - MinusLogProbMetric: 28.7827 - val_loss: 29.4963 - val_MinusLogProbMetric: 29.4963 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 226/1000
2023-10-24 22:48:00.407 
Epoch 226/1000 
	 loss: 28.8131, MinusLogProbMetric: 28.8131, val_loss: 30.8937, val_MinusLogProbMetric: 30.8937

Epoch 226: val_loss did not improve from 28.90604
196/196 - 32s - loss: 28.8131 - MinusLogProbMetric: 28.8131 - val_loss: 30.8937 - val_MinusLogProbMetric: 30.8937 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 227/1000
2023-10-24 22:48:33.129 
Epoch 227/1000 
	 loss: 28.8042, MinusLogProbMetric: 28.8042, val_loss: 30.1542, val_MinusLogProbMetric: 30.1542

Epoch 227: val_loss did not improve from 28.90604
196/196 - 33s - loss: 28.8042 - MinusLogProbMetric: 28.8042 - val_loss: 30.1542 - val_MinusLogProbMetric: 30.1542 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 228/1000
2023-10-24 22:49:05.535 
Epoch 228/1000 
	 loss: 29.0368, MinusLogProbMetric: 29.0368, val_loss: 29.3322, val_MinusLogProbMetric: 29.3322

Epoch 228: val_loss did not improve from 28.90604
196/196 - 32s - loss: 29.0368 - MinusLogProbMetric: 29.0368 - val_loss: 29.3322 - val_MinusLogProbMetric: 29.3322 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 229/1000
2023-10-24 22:49:37.645 
Epoch 229/1000 
	 loss: 28.7827, MinusLogProbMetric: 28.7827, val_loss: 28.8134, val_MinusLogProbMetric: 28.8134

Epoch 229: val_loss improved from 28.90604 to 28.81338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 28.7827 - MinusLogProbMetric: 28.7827 - val_loss: 28.8134 - val_MinusLogProbMetric: 28.8134 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 230/1000
2023-10-24 22:50:09.311 
Epoch 230/1000 
	 loss: 28.8201, MinusLogProbMetric: 28.8201, val_loss: 29.0182, val_MinusLogProbMetric: 29.0182

Epoch 230: val_loss did not improve from 28.81338
196/196 - 31s - loss: 28.8201 - MinusLogProbMetric: 28.8201 - val_loss: 29.0182 - val_MinusLogProbMetric: 29.0182 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 231/1000
2023-10-24 22:50:41.637 
Epoch 231/1000 
	 loss: 28.8705, MinusLogProbMetric: 28.8705, val_loss: 29.4213, val_MinusLogProbMetric: 29.4213

Epoch 231: val_loss did not improve from 28.81338
196/196 - 32s - loss: 28.8705 - MinusLogProbMetric: 28.8705 - val_loss: 29.4213 - val_MinusLogProbMetric: 29.4213 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 232/1000
2023-10-24 22:51:14.167 
Epoch 232/1000 
	 loss: 28.6842, MinusLogProbMetric: 28.6842, val_loss: 28.9669, val_MinusLogProbMetric: 28.9669

Epoch 232: val_loss did not improve from 28.81338
196/196 - 33s - loss: 28.6842 - MinusLogProbMetric: 28.6842 - val_loss: 28.9669 - val_MinusLogProbMetric: 28.9669 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 233/1000
2023-10-24 22:51:45.543 
Epoch 233/1000 
	 loss: 28.7587, MinusLogProbMetric: 28.7587, val_loss: 28.9884, val_MinusLogProbMetric: 28.9884

Epoch 233: val_loss did not improve from 28.81338
196/196 - 31s - loss: 28.7587 - MinusLogProbMetric: 28.7587 - val_loss: 28.9884 - val_MinusLogProbMetric: 28.9884 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 234/1000
2023-10-24 22:52:17.937 
Epoch 234/1000 
	 loss: 28.6390, MinusLogProbMetric: 28.6390, val_loss: 29.2303, val_MinusLogProbMetric: 29.2303

Epoch 234: val_loss did not improve from 28.81338
196/196 - 32s - loss: 28.6390 - MinusLogProbMetric: 28.6390 - val_loss: 29.2303 - val_MinusLogProbMetric: 29.2303 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 235/1000
2023-10-24 22:52:50.585 
Epoch 235/1000 
	 loss: 28.8219, MinusLogProbMetric: 28.8219, val_loss: 29.2891, val_MinusLogProbMetric: 29.2891

Epoch 235: val_loss did not improve from 28.81338
196/196 - 33s - loss: 28.8219 - MinusLogProbMetric: 28.8219 - val_loss: 29.2891 - val_MinusLogProbMetric: 29.2891 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 236/1000
2023-10-24 22:53:21.484 
Epoch 236/1000 
	 loss: 28.8644, MinusLogProbMetric: 28.8644, val_loss: 29.2687, val_MinusLogProbMetric: 29.2687

Epoch 236: val_loss did not improve from 28.81338
196/196 - 31s - loss: 28.8644 - MinusLogProbMetric: 28.8644 - val_loss: 29.2687 - val_MinusLogProbMetric: 29.2687 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 237/1000
2023-10-24 22:53:53.814 
Epoch 237/1000 
	 loss: 28.5833, MinusLogProbMetric: 28.5833, val_loss: 29.0859, val_MinusLogProbMetric: 29.0859

Epoch 237: val_loss did not improve from 28.81338
196/196 - 32s - loss: 28.5833 - MinusLogProbMetric: 28.5833 - val_loss: 29.0859 - val_MinusLogProbMetric: 29.0859 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 238/1000
2023-10-24 22:54:25.855 
Epoch 238/1000 
	 loss: 28.9034, MinusLogProbMetric: 28.9034, val_loss: 29.4493, val_MinusLogProbMetric: 29.4493

Epoch 238: val_loss did not improve from 28.81338
196/196 - 32s - loss: 28.9034 - MinusLogProbMetric: 28.9034 - val_loss: 29.4493 - val_MinusLogProbMetric: 29.4493 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 239/1000
2023-10-24 22:54:58.732 
Epoch 239/1000 
	 loss: 28.6741, MinusLogProbMetric: 28.6741, val_loss: 29.2106, val_MinusLogProbMetric: 29.2106

Epoch 239: val_loss did not improve from 28.81338
196/196 - 33s - loss: 28.6741 - MinusLogProbMetric: 28.6741 - val_loss: 29.2106 - val_MinusLogProbMetric: 29.2106 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 240/1000
2023-10-24 22:55:31.178 
Epoch 240/1000 
	 loss: 28.7082, MinusLogProbMetric: 28.7082, val_loss: 29.1246, val_MinusLogProbMetric: 29.1246

Epoch 240: val_loss did not improve from 28.81338
196/196 - 32s - loss: 28.7082 - MinusLogProbMetric: 28.7082 - val_loss: 29.1246 - val_MinusLogProbMetric: 29.1246 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 241/1000
2023-10-24 22:56:03.763 
Epoch 241/1000 
	 loss: 28.8204, MinusLogProbMetric: 28.8204, val_loss: 28.8891, val_MinusLogProbMetric: 28.8891

Epoch 241: val_loss did not improve from 28.81338
196/196 - 33s - loss: 28.8204 - MinusLogProbMetric: 28.8204 - val_loss: 28.8891 - val_MinusLogProbMetric: 28.8891 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 242/1000
2023-10-24 22:56:36.101 
Epoch 242/1000 
	 loss: 28.6481, MinusLogProbMetric: 28.6481, val_loss: 29.2580, val_MinusLogProbMetric: 29.2580

Epoch 242: val_loss did not improve from 28.81338
196/196 - 32s - loss: 28.6481 - MinusLogProbMetric: 28.6481 - val_loss: 29.2580 - val_MinusLogProbMetric: 29.2580 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 243/1000
2023-10-24 22:57:08.489 
Epoch 243/1000 
	 loss: 28.7184, MinusLogProbMetric: 28.7184, val_loss: 29.8478, val_MinusLogProbMetric: 29.8478

Epoch 243: val_loss did not improve from 28.81338
196/196 - 32s - loss: 28.7184 - MinusLogProbMetric: 28.7184 - val_loss: 29.8478 - val_MinusLogProbMetric: 29.8478 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 244/1000
2023-10-24 22:57:40.799 
Epoch 244/1000 
	 loss: 28.7797, MinusLogProbMetric: 28.7797, val_loss: 30.1433, val_MinusLogProbMetric: 30.1433

Epoch 244: val_loss did not improve from 28.81338
196/196 - 32s - loss: 28.7797 - MinusLogProbMetric: 28.7797 - val_loss: 30.1433 - val_MinusLogProbMetric: 30.1433 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 245/1000
2023-10-24 22:58:13.313 
Epoch 245/1000 
	 loss: 28.6686, MinusLogProbMetric: 28.6686, val_loss: 28.9827, val_MinusLogProbMetric: 28.9827

Epoch 245: val_loss did not improve from 28.81338
196/196 - 33s - loss: 28.6686 - MinusLogProbMetric: 28.6686 - val_loss: 28.9827 - val_MinusLogProbMetric: 28.9827 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 246/1000
2023-10-24 22:58:45.555 
Epoch 246/1000 
	 loss: 28.7349, MinusLogProbMetric: 28.7349, val_loss: 29.3664, val_MinusLogProbMetric: 29.3664

Epoch 246: val_loss did not improve from 28.81338
196/196 - 32s - loss: 28.7349 - MinusLogProbMetric: 28.7349 - val_loss: 29.3664 - val_MinusLogProbMetric: 29.3664 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 247/1000
2023-10-24 22:59:18.001 
Epoch 247/1000 
	 loss: 28.7733, MinusLogProbMetric: 28.7733, val_loss: 30.7065, val_MinusLogProbMetric: 30.7065

Epoch 247: val_loss did not improve from 28.81338
196/196 - 32s - loss: 28.7733 - MinusLogProbMetric: 28.7733 - val_loss: 30.7065 - val_MinusLogProbMetric: 30.7065 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 248/1000
2023-10-24 22:59:50.492 
Epoch 248/1000 
	 loss: 28.7887, MinusLogProbMetric: 28.7887, val_loss: 28.9305, val_MinusLogProbMetric: 28.9305

Epoch 248: val_loss did not improve from 28.81338
196/196 - 32s - loss: 28.7887 - MinusLogProbMetric: 28.7887 - val_loss: 28.9305 - val_MinusLogProbMetric: 28.9305 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 249/1000
2023-10-24 23:00:22.862 
Epoch 249/1000 
	 loss: 28.7661, MinusLogProbMetric: 28.7661, val_loss: 28.9942, val_MinusLogProbMetric: 28.9942

Epoch 249: val_loss did not improve from 28.81338
196/196 - 32s - loss: 28.7661 - MinusLogProbMetric: 28.7661 - val_loss: 28.9942 - val_MinusLogProbMetric: 28.9942 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 250/1000
2023-10-24 23:00:55.279 
Epoch 250/1000 
	 loss: 28.6777, MinusLogProbMetric: 28.6777, val_loss: 28.9595, val_MinusLogProbMetric: 28.9595

Epoch 250: val_loss did not improve from 28.81338
196/196 - 32s - loss: 28.6777 - MinusLogProbMetric: 28.6777 - val_loss: 28.9595 - val_MinusLogProbMetric: 28.9595 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 251/1000
2023-10-24 23:01:27.802 
Epoch 251/1000 
	 loss: 28.7376, MinusLogProbMetric: 28.7376, val_loss: 29.7529, val_MinusLogProbMetric: 29.7529

Epoch 251: val_loss did not improve from 28.81338
196/196 - 33s - loss: 28.7376 - MinusLogProbMetric: 28.7376 - val_loss: 29.7529 - val_MinusLogProbMetric: 29.7529 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 252/1000
2023-10-24 23:02:00.231 
Epoch 252/1000 
	 loss: 28.7888, MinusLogProbMetric: 28.7888, val_loss: 29.8736, val_MinusLogProbMetric: 29.8736

Epoch 252: val_loss did not improve from 28.81338
196/196 - 32s - loss: 28.7888 - MinusLogProbMetric: 28.7888 - val_loss: 29.8736 - val_MinusLogProbMetric: 29.8736 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 253/1000
2023-10-24 23:02:31.730 
Epoch 253/1000 
	 loss: 28.7450, MinusLogProbMetric: 28.7450, val_loss: 28.7683, val_MinusLogProbMetric: 28.7683

Epoch 253: val_loss improved from 28.81338 to 28.76829, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 28.7450 - MinusLogProbMetric: 28.7450 - val_loss: 28.7683 - val_MinusLogProbMetric: 28.7683 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 254/1000
2023-10-24 23:03:04.741 
Epoch 254/1000 
	 loss: 28.4686, MinusLogProbMetric: 28.4686, val_loss: 29.0062, val_MinusLogProbMetric: 29.0062

Epoch 254: val_loss did not improve from 28.76829
196/196 - 32s - loss: 28.4686 - MinusLogProbMetric: 28.4686 - val_loss: 29.0062 - val_MinusLogProbMetric: 29.0062 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 255/1000
2023-10-24 23:03:37.554 
Epoch 255/1000 
	 loss: 28.7080, MinusLogProbMetric: 28.7080, val_loss: 28.8903, val_MinusLogProbMetric: 28.8903

Epoch 255: val_loss did not improve from 28.76829
196/196 - 33s - loss: 28.7080 - MinusLogProbMetric: 28.7080 - val_loss: 28.8903 - val_MinusLogProbMetric: 28.8903 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 256/1000
2023-10-24 23:04:10.000 
Epoch 256/1000 
	 loss: 28.8188, MinusLogProbMetric: 28.8188, val_loss: 29.3048, val_MinusLogProbMetric: 29.3048

Epoch 256: val_loss did not improve from 28.76829
196/196 - 32s - loss: 28.8188 - MinusLogProbMetric: 28.8188 - val_loss: 29.3048 - val_MinusLogProbMetric: 29.3048 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 257/1000
2023-10-24 23:04:42.835 
Epoch 257/1000 
	 loss: 28.5723, MinusLogProbMetric: 28.5723, val_loss: 28.8992, val_MinusLogProbMetric: 28.8992

Epoch 257: val_loss did not improve from 28.76829
196/196 - 33s - loss: 28.5723 - MinusLogProbMetric: 28.5723 - val_loss: 28.8992 - val_MinusLogProbMetric: 28.8992 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 258/1000
2023-10-24 23:05:14.952 
Epoch 258/1000 
	 loss: 28.8121, MinusLogProbMetric: 28.8121, val_loss: 29.0019, val_MinusLogProbMetric: 29.0019

Epoch 258: val_loss did not improve from 28.76829
196/196 - 32s - loss: 28.8121 - MinusLogProbMetric: 28.8121 - val_loss: 29.0019 - val_MinusLogProbMetric: 29.0019 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 259/1000
2023-10-24 23:05:47.200 
Epoch 259/1000 
	 loss: 28.7416, MinusLogProbMetric: 28.7416, val_loss: 29.4297, val_MinusLogProbMetric: 29.4297

Epoch 259: val_loss did not improve from 28.76829
196/196 - 32s - loss: 28.7416 - MinusLogProbMetric: 28.7416 - val_loss: 29.4297 - val_MinusLogProbMetric: 29.4297 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 260/1000
2023-10-24 23:06:19.703 
Epoch 260/1000 
	 loss: 28.5801, MinusLogProbMetric: 28.5801, val_loss: 28.5863, val_MinusLogProbMetric: 28.5863

Epoch 260: val_loss improved from 28.76829 to 28.58633, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 28.5801 - MinusLogProbMetric: 28.5801 - val_loss: 28.5863 - val_MinusLogProbMetric: 28.5863 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 261/1000
2023-10-24 23:06:52.465 
Epoch 261/1000 
	 loss: 28.6854, MinusLogProbMetric: 28.6854, val_loss: 29.1072, val_MinusLogProbMetric: 29.1072

Epoch 261: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.6854 - MinusLogProbMetric: 28.6854 - val_loss: 29.1072 - val_MinusLogProbMetric: 29.1072 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 262/1000
2023-10-24 23:07:24.623 
Epoch 262/1000 
	 loss: 28.4572, MinusLogProbMetric: 28.4572, val_loss: 29.2683, val_MinusLogProbMetric: 29.2683

Epoch 262: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.4572 - MinusLogProbMetric: 28.4572 - val_loss: 29.2683 - val_MinusLogProbMetric: 29.2683 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 263/1000
2023-10-24 23:07:56.435 
Epoch 263/1000 
	 loss: 28.6991, MinusLogProbMetric: 28.6991, val_loss: 28.7066, val_MinusLogProbMetric: 28.7066

Epoch 263: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.6991 - MinusLogProbMetric: 28.6991 - val_loss: 28.7066 - val_MinusLogProbMetric: 28.7066 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 264/1000
2023-10-24 23:08:29.088 
Epoch 264/1000 
	 loss: 28.6662, MinusLogProbMetric: 28.6662, val_loss: 29.1492, val_MinusLogProbMetric: 29.1492

Epoch 264: val_loss did not improve from 28.58633
196/196 - 33s - loss: 28.6662 - MinusLogProbMetric: 28.6662 - val_loss: 29.1492 - val_MinusLogProbMetric: 29.1492 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 265/1000
2023-10-24 23:09:00.484 
Epoch 265/1000 
	 loss: 28.7527, MinusLogProbMetric: 28.7527, val_loss: 30.0295, val_MinusLogProbMetric: 30.0295

Epoch 265: val_loss did not improve from 28.58633
196/196 - 31s - loss: 28.7527 - MinusLogProbMetric: 28.7527 - val_loss: 30.0295 - val_MinusLogProbMetric: 30.0295 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 266/1000
2023-10-24 23:09:32.933 
Epoch 266/1000 
	 loss: 28.4935, MinusLogProbMetric: 28.4935, val_loss: 29.0389, val_MinusLogProbMetric: 29.0389

Epoch 266: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.4935 - MinusLogProbMetric: 28.4935 - val_loss: 29.0389 - val_MinusLogProbMetric: 29.0389 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 267/1000
2023-10-24 23:10:05.270 
Epoch 267/1000 
	 loss: 28.7229, MinusLogProbMetric: 28.7229, val_loss: 28.8621, val_MinusLogProbMetric: 28.8621

Epoch 267: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.7229 - MinusLogProbMetric: 28.7229 - val_loss: 28.8621 - val_MinusLogProbMetric: 28.8621 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 268/1000
2023-10-24 23:10:36.942 
Epoch 268/1000 
	 loss: 28.8042, MinusLogProbMetric: 28.8042, val_loss: 29.1031, val_MinusLogProbMetric: 29.1031

Epoch 268: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.8042 - MinusLogProbMetric: 28.8042 - val_loss: 29.1031 - val_MinusLogProbMetric: 29.1031 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 269/1000
2023-10-24 23:11:07.205 
Epoch 269/1000 
	 loss: 28.6123, MinusLogProbMetric: 28.6123, val_loss: 29.2433, val_MinusLogProbMetric: 29.2433

Epoch 269: val_loss did not improve from 28.58633
196/196 - 30s - loss: 28.6123 - MinusLogProbMetric: 28.6123 - val_loss: 29.2433 - val_MinusLogProbMetric: 29.2433 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 270/1000
2023-10-24 23:11:36.608 
Epoch 270/1000 
	 loss: 28.6374, MinusLogProbMetric: 28.6374, val_loss: 29.7045, val_MinusLogProbMetric: 29.7045

Epoch 270: val_loss did not improve from 28.58633
196/196 - 29s - loss: 28.6374 - MinusLogProbMetric: 28.6374 - val_loss: 29.7045 - val_MinusLogProbMetric: 29.7045 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 271/1000
2023-10-24 23:12:06.806 
Epoch 271/1000 
	 loss: 28.6403, MinusLogProbMetric: 28.6403, val_loss: 29.5801, val_MinusLogProbMetric: 29.5801

Epoch 271: val_loss did not improve from 28.58633
196/196 - 30s - loss: 28.6403 - MinusLogProbMetric: 28.6403 - val_loss: 29.5801 - val_MinusLogProbMetric: 29.5801 - lr: 0.0010 - 30s/epoch - 154ms/step
Epoch 272/1000
2023-10-24 23:12:39.364 
Epoch 272/1000 
	 loss: 28.6925, MinusLogProbMetric: 28.6925, val_loss: 29.5968, val_MinusLogProbMetric: 29.5968

Epoch 272: val_loss did not improve from 28.58633
196/196 - 33s - loss: 28.6925 - MinusLogProbMetric: 28.6925 - val_loss: 29.5968 - val_MinusLogProbMetric: 29.5968 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 273/1000
2023-10-24 23:13:11.333 
Epoch 273/1000 
	 loss: 28.4250, MinusLogProbMetric: 28.4250, val_loss: 28.8930, val_MinusLogProbMetric: 28.8930

Epoch 273: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.4250 - MinusLogProbMetric: 28.4250 - val_loss: 28.8930 - val_MinusLogProbMetric: 28.8930 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 274/1000
2023-10-24 23:13:43.404 
Epoch 274/1000 
	 loss: 28.5934, MinusLogProbMetric: 28.5934, val_loss: 29.2311, val_MinusLogProbMetric: 29.2311

Epoch 274: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.5934 - MinusLogProbMetric: 28.5934 - val_loss: 29.2311 - val_MinusLogProbMetric: 29.2311 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 275/1000
2023-10-24 23:14:14.652 
Epoch 275/1000 
	 loss: 28.4590, MinusLogProbMetric: 28.4590, val_loss: 29.0598, val_MinusLogProbMetric: 29.0598

Epoch 275: val_loss did not improve from 28.58633
196/196 - 31s - loss: 28.4590 - MinusLogProbMetric: 28.4590 - val_loss: 29.0598 - val_MinusLogProbMetric: 29.0598 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 276/1000
2023-10-24 23:14:46.087 
Epoch 276/1000 
	 loss: 28.4508, MinusLogProbMetric: 28.4508, val_loss: 29.3527, val_MinusLogProbMetric: 29.3527

Epoch 276: val_loss did not improve from 28.58633
196/196 - 31s - loss: 28.4508 - MinusLogProbMetric: 28.4508 - val_loss: 29.3527 - val_MinusLogProbMetric: 29.3527 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 277/1000
2023-10-24 23:15:15.831 
Epoch 277/1000 
	 loss: 28.5117, MinusLogProbMetric: 28.5117, val_loss: 29.0028, val_MinusLogProbMetric: 29.0028

Epoch 277: val_loss did not improve from 28.58633
196/196 - 30s - loss: 28.5117 - MinusLogProbMetric: 28.5117 - val_loss: 29.0028 - val_MinusLogProbMetric: 29.0028 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 278/1000
2023-10-24 23:15:48.186 
Epoch 278/1000 
	 loss: 28.7368, MinusLogProbMetric: 28.7368, val_loss: 29.4928, val_MinusLogProbMetric: 29.4928

Epoch 278: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.7368 - MinusLogProbMetric: 28.7368 - val_loss: 29.4928 - val_MinusLogProbMetric: 29.4928 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 279/1000
2023-10-24 23:16:19.334 
Epoch 279/1000 
	 loss: 28.5335, MinusLogProbMetric: 28.5335, val_loss: 28.7340, val_MinusLogProbMetric: 28.7340

Epoch 279: val_loss did not improve from 28.58633
196/196 - 31s - loss: 28.5335 - MinusLogProbMetric: 28.5335 - val_loss: 28.7340 - val_MinusLogProbMetric: 28.7340 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 280/1000
2023-10-24 23:16:49.734 
Epoch 280/1000 
	 loss: 28.4484, MinusLogProbMetric: 28.4484, val_loss: 29.3139, val_MinusLogProbMetric: 29.3139

Epoch 280: val_loss did not improve from 28.58633
196/196 - 30s - loss: 28.4484 - MinusLogProbMetric: 28.4484 - val_loss: 29.3139 - val_MinusLogProbMetric: 29.3139 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 281/1000
2023-10-24 23:17:21.250 
Epoch 281/1000 
	 loss: 28.5658, MinusLogProbMetric: 28.5658, val_loss: 29.4883, val_MinusLogProbMetric: 29.4883

Epoch 281: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.5658 - MinusLogProbMetric: 28.5658 - val_loss: 29.4883 - val_MinusLogProbMetric: 29.4883 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 282/1000
2023-10-24 23:17:53.079 
Epoch 282/1000 
	 loss: 28.7253, MinusLogProbMetric: 28.7253, val_loss: 29.1886, val_MinusLogProbMetric: 29.1886

Epoch 282: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.7253 - MinusLogProbMetric: 28.7253 - val_loss: 29.1886 - val_MinusLogProbMetric: 29.1886 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 283/1000
2023-10-24 23:18:25.894 
Epoch 283/1000 
	 loss: 28.4488, MinusLogProbMetric: 28.4488, val_loss: 28.8959, val_MinusLogProbMetric: 28.8959

Epoch 283: val_loss did not improve from 28.58633
196/196 - 33s - loss: 28.4488 - MinusLogProbMetric: 28.4488 - val_loss: 28.8959 - val_MinusLogProbMetric: 28.8959 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 284/1000
2023-10-24 23:18:57.381 
Epoch 284/1000 
	 loss: 28.3638, MinusLogProbMetric: 28.3638, val_loss: 29.1878, val_MinusLogProbMetric: 29.1878

Epoch 284: val_loss did not improve from 28.58633
196/196 - 31s - loss: 28.3638 - MinusLogProbMetric: 28.3638 - val_loss: 29.1878 - val_MinusLogProbMetric: 29.1878 - lr: 0.0010 - 31s/epoch - 161ms/step
Epoch 285/1000
2023-10-24 23:19:28.637 
Epoch 285/1000 
	 loss: 28.5272, MinusLogProbMetric: 28.5272, val_loss: 28.9311, val_MinusLogProbMetric: 28.9311

Epoch 285: val_loss did not improve from 28.58633
196/196 - 31s - loss: 28.5272 - MinusLogProbMetric: 28.5272 - val_loss: 28.9311 - val_MinusLogProbMetric: 28.9311 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 286/1000
2023-10-24 23:20:01.488 
Epoch 286/1000 
	 loss: 28.5979, MinusLogProbMetric: 28.5979, val_loss: 29.4511, val_MinusLogProbMetric: 29.4511

Epoch 286: val_loss did not improve from 28.58633
196/196 - 33s - loss: 28.5979 - MinusLogProbMetric: 28.5979 - val_loss: 29.4511 - val_MinusLogProbMetric: 29.4511 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 287/1000
2023-10-24 23:20:33.018 
Epoch 287/1000 
	 loss: 28.7092, MinusLogProbMetric: 28.7092, val_loss: 29.1984, val_MinusLogProbMetric: 29.1984

Epoch 287: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.7092 - MinusLogProbMetric: 28.7092 - val_loss: 29.1984 - val_MinusLogProbMetric: 29.1984 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 288/1000
2023-10-24 23:21:03.532 
Epoch 288/1000 
	 loss: 28.5719, MinusLogProbMetric: 28.5719, val_loss: 29.0770, val_MinusLogProbMetric: 29.0770

Epoch 288: val_loss did not improve from 28.58633
196/196 - 31s - loss: 28.5719 - MinusLogProbMetric: 28.5719 - val_loss: 29.0770 - val_MinusLogProbMetric: 29.0770 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 289/1000
2023-10-24 23:21:35.108 
Epoch 289/1000 
	 loss: 28.6073, MinusLogProbMetric: 28.6073, val_loss: 28.8609, val_MinusLogProbMetric: 28.8609

Epoch 289: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.6073 - MinusLogProbMetric: 28.6073 - val_loss: 28.8609 - val_MinusLogProbMetric: 28.8609 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 290/1000
2023-10-24 23:22:07.471 
Epoch 290/1000 
	 loss: 28.5096, MinusLogProbMetric: 28.5096, val_loss: 28.6206, val_MinusLogProbMetric: 28.6206

Epoch 290: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.5096 - MinusLogProbMetric: 28.5096 - val_loss: 28.6206 - val_MinusLogProbMetric: 28.6206 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 291/1000
2023-10-24 23:22:38.921 
Epoch 291/1000 
	 loss: 28.7291, MinusLogProbMetric: 28.7291, val_loss: 28.8147, val_MinusLogProbMetric: 28.8147

Epoch 291: val_loss did not improve from 28.58633
196/196 - 31s - loss: 28.7291 - MinusLogProbMetric: 28.7291 - val_loss: 28.8147 - val_MinusLogProbMetric: 28.8147 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 292/1000
2023-10-24 23:23:10.947 
Epoch 292/1000 
	 loss: 28.3894, MinusLogProbMetric: 28.3894, val_loss: 29.2457, val_MinusLogProbMetric: 29.2457

Epoch 292: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.3894 - MinusLogProbMetric: 28.3894 - val_loss: 29.2457 - val_MinusLogProbMetric: 29.2457 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 293/1000
2023-10-24 23:23:42.187 
Epoch 293/1000 
	 loss: 28.5837, MinusLogProbMetric: 28.5837, val_loss: 28.8454, val_MinusLogProbMetric: 28.8454

Epoch 293: val_loss did not improve from 28.58633
196/196 - 31s - loss: 28.5837 - MinusLogProbMetric: 28.5837 - val_loss: 28.8454 - val_MinusLogProbMetric: 28.8454 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 294/1000
2023-10-24 23:24:14.105 
Epoch 294/1000 
	 loss: 28.3711, MinusLogProbMetric: 28.3711, val_loss: 29.5944, val_MinusLogProbMetric: 29.5944

Epoch 294: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.3711 - MinusLogProbMetric: 28.3711 - val_loss: 29.5944 - val_MinusLogProbMetric: 29.5944 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 295/1000
2023-10-24 23:24:46.727 
Epoch 295/1000 
	 loss: 28.5012, MinusLogProbMetric: 28.5012, val_loss: 29.0507, val_MinusLogProbMetric: 29.0507

Epoch 295: val_loss did not improve from 28.58633
196/196 - 33s - loss: 28.5012 - MinusLogProbMetric: 28.5012 - val_loss: 29.0507 - val_MinusLogProbMetric: 29.0507 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 296/1000
2023-10-24 23:25:18.797 
Epoch 296/1000 
	 loss: 28.4484, MinusLogProbMetric: 28.4484, val_loss: 29.6112, val_MinusLogProbMetric: 29.6112

Epoch 296: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.4484 - MinusLogProbMetric: 28.4484 - val_loss: 29.6112 - val_MinusLogProbMetric: 29.6112 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 297/1000
2023-10-24 23:25:51.577 
Epoch 297/1000 
	 loss: 28.5970, MinusLogProbMetric: 28.5970, val_loss: 28.9483, val_MinusLogProbMetric: 28.9483

Epoch 297: val_loss did not improve from 28.58633
196/196 - 33s - loss: 28.5970 - MinusLogProbMetric: 28.5970 - val_loss: 28.9483 - val_MinusLogProbMetric: 28.9483 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 298/1000
2023-10-24 23:26:23.233 
Epoch 298/1000 
	 loss: 28.4446, MinusLogProbMetric: 28.4446, val_loss: 28.7908, val_MinusLogProbMetric: 28.7908

Epoch 298: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.4446 - MinusLogProbMetric: 28.4446 - val_loss: 28.7908 - val_MinusLogProbMetric: 28.7908 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 299/1000
2023-10-24 23:26:55.516 
Epoch 299/1000 
	 loss: 28.7501, MinusLogProbMetric: 28.7501, val_loss: 29.5900, val_MinusLogProbMetric: 29.5900

Epoch 299: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.7501 - MinusLogProbMetric: 28.7501 - val_loss: 29.5900 - val_MinusLogProbMetric: 29.5900 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 300/1000
2023-10-24 23:27:26.641 
Epoch 300/1000 
	 loss: 28.3287, MinusLogProbMetric: 28.3287, val_loss: 29.3697, val_MinusLogProbMetric: 29.3697

Epoch 300: val_loss did not improve from 28.58633
196/196 - 31s - loss: 28.3287 - MinusLogProbMetric: 28.3287 - val_loss: 29.3697 - val_MinusLogProbMetric: 29.3697 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 301/1000
2023-10-24 23:27:58.695 
Epoch 301/1000 
	 loss: 28.4016, MinusLogProbMetric: 28.4016, val_loss: 29.3680, val_MinusLogProbMetric: 29.3680

Epoch 301: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.4016 - MinusLogProbMetric: 28.4016 - val_loss: 29.3680 - val_MinusLogProbMetric: 29.3680 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 302/1000
2023-10-24 23:28:30.912 
Epoch 302/1000 
	 loss: 28.4656, MinusLogProbMetric: 28.4656, val_loss: 29.4906, val_MinusLogProbMetric: 29.4906

Epoch 302: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.4656 - MinusLogProbMetric: 28.4656 - val_loss: 29.4906 - val_MinusLogProbMetric: 29.4906 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 303/1000
2023-10-24 23:29:03.419 
Epoch 303/1000 
	 loss: 28.4594, MinusLogProbMetric: 28.4594, val_loss: 28.6488, val_MinusLogProbMetric: 28.6488

Epoch 303: val_loss did not improve from 28.58633
196/196 - 33s - loss: 28.4594 - MinusLogProbMetric: 28.4594 - val_loss: 28.6488 - val_MinusLogProbMetric: 28.6488 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 304/1000
2023-10-24 23:29:36.205 
Epoch 304/1000 
	 loss: 28.3299, MinusLogProbMetric: 28.3299, val_loss: 29.6170, val_MinusLogProbMetric: 29.6170

Epoch 304: val_loss did not improve from 28.58633
196/196 - 33s - loss: 28.3299 - MinusLogProbMetric: 28.3299 - val_loss: 29.6170 - val_MinusLogProbMetric: 29.6170 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 305/1000
2023-10-24 23:30:07.311 
Epoch 305/1000 
	 loss: 28.5425, MinusLogProbMetric: 28.5425, val_loss: 29.2572, val_MinusLogProbMetric: 29.2572

Epoch 305: val_loss did not improve from 28.58633
196/196 - 31s - loss: 28.5425 - MinusLogProbMetric: 28.5425 - val_loss: 29.2572 - val_MinusLogProbMetric: 29.2572 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 306/1000
2023-10-24 23:30:38.791 
Epoch 306/1000 
	 loss: 28.3904, MinusLogProbMetric: 28.3904, val_loss: 28.8496, val_MinusLogProbMetric: 28.8496

Epoch 306: val_loss did not improve from 28.58633
196/196 - 31s - loss: 28.3904 - MinusLogProbMetric: 28.3904 - val_loss: 28.8496 - val_MinusLogProbMetric: 28.8496 - lr: 0.0010 - 31s/epoch - 161ms/step
Epoch 307/1000
2023-10-24 23:31:09.831 
Epoch 307/1000 
	 loss: 28.2402, MinusLogProbMetric: 28.2402, val_loss: 29.4292, val_MinusLogProbMetric: 29.4292

Epoch 307: val_loss did not improve from 28.58633
196/196 - 31s - loss: 28.2402 - MinusLogProbMetric: 28.2402 - val_loss: 29.4292 - val_MinusLogProbMetric: 29.4292 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 308/1000
2023-10-24 23:31:41.645 
Epoch 308/1000 
	 loss: 28.3656, MinusLogProbMetric: 28.3656, val_loss: 29.1276, val_MinusLogProbMetric: 29.1276

Epoch 308: val_loss did not improve from 28.58633
196/196 - 32s - loss: 28.3656 - MinusLogProbMetric: 28.3656 - val_loss: 29.1276 - val_MinusLogProbMetric: 29.1276 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 309/1000
2023-10-24 23:32:12.460 
Epoch 309/1000 
	 loss: 28.3643, MinusLogProbMetric: 28.3643, val_loss: 29.1668, val_MinusLogProbMetric: 29.1668

Epoch 309: val_loss did not improve from 28.58633
196/196 - 31s - loss: 28.3643 - MinusLogProbMetric: 28.3643 - val_loss: 29.1668 - val_MinusLogProbMetric: 29.1668 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 310/1000
2023-10-24 23:32:43.767 
Epoch 310/1000 
	 loss: 28.3907, MinusLogProbMetric: 28.3907, val_loss: 29.1881, val_MinusLogProbMetric: 29.1881

Epoch 310: val_loss did not improve from 28.58633
196/196 - 31s - loss: 28.3907 - MinusLogProbMetric: 28.3907 - val_loss: 29.1881 - val_MinusLogProbMetric: 29.1881 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 311/1000
2023-10-24 23:33:16.067 
Epoch 311/1000 
	 loss: 27.7816, MinusLogProbMetric: 27.7816, val_loss: 28.3761, val_MinusLogProbMetric: 28.3761

Epoch 311: val_loss improved from 28.58633 to 28.37609, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 27.7816 - MinusLogProbMetric: 27.7816 - val_loss: 28.3761 - val_MinusLogProbMetric: 28.3761 - lr: 5.0000e-04 - 33s/epoch - 167ms/step
Epoch 312/1000
2023-10-24 23:33:47.965 
Epoch 312/1000 
	 loss: 27.7483, MinusLogProbMetric: 27.7483, val_loss: 28.3544, val_MinusLogProbMetric: 28.3544

Epoch 312: val_loss improved from 28.37609 to 28.35443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 27.7483 - MinusLogProbMetric: 27.7483 - val_loss: 28.3544 - val_MinusLogProbMetric: 28.3544 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 313/1000
2023-10-24 23:34:18.456 
Epoch 313/1000 
	 loss: 27.7588, MinusLogProbMetric: 27.7588, val_loss: 28.4126, val_MinusLogProbMetric: 28.4126

Epoch 313: val_loss did not improve from 28.35443
196/196 - 30s - loss: 27.7588 - MinusLogProbMetric: 27.7588 - val_loss: 28.4126 - val_MinusLogProbMetric: 28.4126 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 314/1000
2023-10-24 23:34:48.320 
Epoch 314/1000 
	 loss: 27.7473, MinusLogProbMetric: 27.7473, val_loss: 28.5585, val_MinusLogProbMetric: 28.5585

Epoch 314: val_loss did not improve from 28.35443
196/196 - 30s - loss: 27.7473 - MinusLogProbMetric: 27.7473 - val_loss: 28.5585 - val_MinusLogProbMetric: 28.5585 - lr: 5.0000e-04 - 30s/epoch - 152ms/step
Epoch 315/1000
2023-10-24 23:35:19.237 
Epoch 315/1000 
	 loss: 27.7340, MinusLogProbMetric: 27.7340, val_loss: 28.4105, val_MinusLogProbMetric: 28.4105

Epoch 315: val_loss did not improve from 28.35443
196/196 - 31s - loss: 27.7340 - MinusLogProbMetric: 27.7340 - val_loss: 28.4105 - val_MinusLogProbMetric: 28.4105 - lr: 5.0000e-04 - 31s/epoch - 158ms/step
Epoch 316/1000
2023-10-24 23:35:48.650 
Epoch 316/1000 
	 loss: 27.7209, MinusLogProbMetric: 27.7209, val_loss: 28.3722, val_MinusLogProbMetric: 28.3722

Epoch 316: val_loss did not improve from 28.35443
196/196 - 29s - loss: 27.7209 - MinusLogProbMetric: 27.7209 - val_loss: 28.3722 - val_MinusLogProbMetric: 28.3722 - lr: 5.0000e-04 - 29s/epoch - 150ms/step
Epoch 317/1000
2023-10-24 23:36:20.188 
Epoch 317/1000 
	 loss: 27.7172, MinusLogProbMetric: 27.7172, val_loss: 28.4243, val_MinusLogProbMetric: 28.4243

Epoch 317: val_loss did not improve from 28.35443
196/196 - 32s - loss: 27.7172 - MinusLogProbMetric: 27.7172 - val_loss: 28.4243 - val_MinusLogProbMetric: 28.4243 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 318/1000
2023-10-24 23:36:51.930 
Epoch 318/1000 
	 loss: 27.7383, MinusLogProbMetric: 27.7383, val_loss: 28.5004, val_MinusLogProbMetric: 28.5004

Epoch 318: val_loss did not improve from 28.35443
196/196 - 32s - loss: 27.7383 - MinusLogProbMetric: 27.7383 - val_loss: 28.5004 - val_MinusLogProbMetric: 28.5004 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 319/1000
2023-10-24 23:37:23.809 
Epoch 319/1000 
	 loss: 27.7181, MinusLogProbMetric: 27.7181, val_loss: 28.2765, val_MinusLogProbMetric: 28.2765

Epoch 319: val_loss improved from 28.35443 to 28.27655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 27.7181 - MinusLogProbMetric: 27.7181 - val_loss: 28.2765 - val_MinusLogProbMetric: 28.2765 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 320/1000
2023-10-24 23:37:56.159 
Epoch 320/1000 
	 loss: 27.7234, MinusLogProbMetric: 27.7234, val_loss: 28.3929, val_MinusLogProbMetric: 28.3929

Epoch 320: val_loss did not improve from 28.27655
196/196 - 32s - loss: 27.7234 - MinusLogProbMetric: 27.7234 - val_loss: 28.3929 - val_MinusLogProbMetric: 28.3929 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 321/1000
2023-10-24 23:38:27.602 
Epoch 321/1000 
	 loss: 27.7539, MinusLogProbMetric: 27.7539, val_loss: 28.4428, val_MinusLogProbMetric: 28.4428

Epoch 321: val_loss did not improve from 28.27655
196/196 - 31s - loss: 27.7539 - MinusLogProbMetric: 27.7539 - val_loss: 28.4428 - val_MinusLogProbMetric: 28.4428 - lr: 5.0000e-04 - 31s/epoch - 160ms/step
Epoch 322/1000
2023-10-24 23:38:58.591 
Epoch 322/1000 
	 loss: 27.8051, MinusLogProbMetric: 27.8051, val_loss: 28.8879, val_MinusLogProbMetric: 28.8879

Epoch 322: val_loss did not improve from 28.27655
196/196 - 31s - loss: 27.8051 - MinusLogProbMetric: 27.8051 - val_loss: 28.8879 - val_MinusLogProbMetric: 28.8879 - lr: 5.0000e-04 - 31s/epoch - 158ms/step
Epoch 323/1000
2023-10-24 23:39:30.685 
Epoch 323/1000 
	 loss: 27.7839, MinusLogProbMetric: 27.7839, val_loss: 28.3691, val_MinusLogProbMetric: 28.3691

Epoch 323: val_loss did not improve from 28.27655
196/196 - 32s - loss: 27.7839 - MinusLogProbMetric: 27.7839 - val_loss: 28.3691 - val_MinusLogProbMetric: 28.3691 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 324/1000
2023-10-24 23:40:03.176 
Epoch 324/1000 
	 loss: 27.7375, MinusLogProbMetric: 27.7375, val_loss: 28.4304, val_MinusLogProbMetric: 28.4304

Epoch 324: val_loss did not improve from 28.27655
196/196 - 32s - loss: 27.7375 - MinusLogProbMetric: 27.7375 - val_loss: 28.4304 - val_MinusLogProbMetric: 28.4304 - lr: 5.0000e-04 - 32s/epoch - 166ms/step
Epoch 325/1000
2023-10-24 23:40:33.236 
Epoch 325/1000 
	 loss: 27.7676, MinusLogProbMetric: 27.7676, val_loss: 28.6235, val_MinusLogProbMetric: 28.6235

Epoch 325: val_loss did not improve from 28.27655
196/196 - 30s - loss: 27.7676 - MinusLogProbMetric: 27.7676 - val_loss: 28.6235 - val_MinusLogProbMetric: 28.6235 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 326/1000
2023-10-24 23:41:05.635 
Epoch 326/1000 
	 loss: 27.7360, MinusLogProbMetric: 27.7360, val_loss: 28.4452, val_MinusLogProbMetric: 28.4452

Epoch 326: val_loss did not improve from 28.27655
196/196 - 32s - loss: 27.7360 - MinusLogProbMetric: 27.7360 - val_loss: 28.4452 - val_MinusLogProbMetric: 28.4452 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 327/1000
2023-10-24 23:41:36.007 
Epoch 327/1000 
	 loss: 27.7375, MinusLogProbMetric: 27.7375, val_loss: 28.8283, val_MinusLogProbMetric: 28.8283

Epoch 327: val_loss did not improve from 28.27655
196/196 - 30s - loss: 27.7375 - MinusLogProbMetric: 27.7375 - val_loss: 28.8283 - val_MinusLogProbMetric: 28.8283 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 328/1000
2023-10-24 23:42:06.832 
Epoch 328/1000 
	 loss: 27.8168, MinusLogProbMetric: 27.8168, val_loss: 28.4344, val_MinusLogProbMetric: 28.4344

Epoch 328: val_loss did not improve from 28.27655
196/196 - 31s - loss: 27.8168 - MinusLogProbMetric: 27.8168 - val_loss: 28.4344 - val_MinusLogProbMetric: 28.4344 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 329/1000
2023-10-24 23:42:36.403 
Epoch 329/1000 
	 loss: 27.8702, MinusLogProbMetric: 27.8702, val_loss: 28.4299, val_MinusLogProbMetric: 28.4299

Epoch 329: val_loss did not improve from 28.27655
196/196 - 30s - loss: 27.8702 - MinusLogProbMetric: 27.8702 - val_loss: 28.4299 - val_MinusLogProbMetric: 28.4299 - lr: 5.0000e-04 - 30s/epoch - 151ms/step
Epoch 330/1000
2023-10-24 23:43:07.864 
Epoch 330/1000 
	 loss: 27.7425, MinusLogProbMetric: 27.7425, val_loss: 28.2834, val_MinusLogProbMetric: 28.2834

Epoch 330: val_loss did not improve from 28.27655
196/196 - 31s - loss: 27.7425 - MinusLogProbMetric: 27.7425 - val_loss: 28.2834 - val_MinusLogProbMetric: 28.2834 - lr: 5.0000e-04 - 31s/epoch - 161ms/step
Epoch 331/1000
2023-10-24 23:43:38.078 
Epoch 331/1000 
	 loss: 27.7106, MinusLogProbMetric: 27.7106, val_loss: 28.2993, val_MinusLogProbMetric: 28.2993

Epoch 331: val_loss did not improve from 28.27655
196/196 - 30s - loss: 27.7106 - MinusLogProbMetric: 27.7106 - val_loss: 28.2993 - val_MinusLogProbMetric: 28.2993 - lr: 5.0000e-04 - 30s/epoch - 154ms/step
Epoch 332/1000
2023-10-24 23:44:10.601 
Epoch 332/1000 
	 loss: 27.6874, MinusLogProbMetric: 27.6874, val_loss: 28.3970, val_MinusLogProbMetric: 28.3970

Epoch 332: val_loss did not improve from 28.27655
196/196 - 33s - loss: 27.6874 - MinusLogProbMetric: 27.6874 - val_loss: 28.3970 - val_MinusLogProbMetric: 28.3970 - lr: 5.0000e-04 - 33s/epoch - 166ms/step
Epoch 333/1000
2023-10-24 23:44:41.087 
Epoch 333/1000 
	 loss: 27.8692, MinusLogProbMetric: 27.8692, val_loss: 28.3168, val_MinusLogProbMetric: 28.3168

Epoch 333: val_loss did not improve from 28.27655
196/196 - 30s - loss: 27.8692 - MinusLogProbMetric: 27.8692 - val_loss: 28.3168 - val_MinusLogProbMetric: 28.3168 - lr: 5.0000e-04 - 30s/epoch - 156ms/step
Epoch 334/1000
2023-10-24 23:45:11.914 
Epoch 334/1000 
	 loss: 27.7698, MinusLogProbMetric: 27.7698, val_loss: 28.3001, val_MinusLogProbMetric: 28.3001

Epoch 334: val_loss did not improve from 28.27655
196/196 - 31s - loss: 27.7698 - MinusLogProbMetric: 27.7698 - val_loss: 28.3001 - val_MinusLogProbMetric: 28.3001 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 335/1000
2023-10-24 23:45:43.098 
Epoch 335/1000 
	 loss: 27.6906, MinusLogProbMetric: 27.6906, val_loss: 28.3742, val_MinusLogProbMetric: 28.3742

Epoch 335: val_loss did not improve from 28.27655
196/196 - 31s - loss: 27.6906 - MinusLogProbMetric: 27.6906 - val_loss: 28.3742 - val_MinusLogProbMetric: 28.3742 - lr: 5.0000e-04 - 31s/epoch - 159ms/step
Epoch 336/1000
2023-10-24 23:46:15.229 
Epoch 336/1000 
	 loss: 27.7454, MinusLogProbMetric: 27.7454, val_loss: 28.9452, val_MinusLogProbMetric: 28.9452

Epoch 336: val_loss did not improve from 28.27655
196/196 - 32s - loss: 27.7454 - MinusLogProbMetric: 27.7454 - val_loss: 28.9452 - val_MinusLogProbMetric: 28.9452 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 337/1000
2023-10-24 23:46:47.051 
Epoch 337/1000 
	 loss: 27.8142, MinusLogProbMetric: 27.8142, val_loss: 28.4290, val_MinusLogProbMetric: 28.4290

Epoch 337: val_loss did not improve from 28.27655
196/196 - 32s - loss: 27.8142 - MinusLogProbMetric: 27.8142 - val_loss: 28.4290 - val_MinusLogProbMetric: 28.4290 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 338/1000
2023-10-24 23:47:17.166 
Epoch 338/1000 
	 loss: 27.7326, MinusLogProbMetric: 27.7326, val_loss: 28.4346, val_MinusLogProbMetric: 28.4346

Epoch 338: val_loss did not improve from 28.27655
196/196 - 30s - loss: 27.7326 - MinusLogProbMetric: 27.7326 - val_loss: 28.4346 - val_MinusLogProbMetric: 28.4346 - lr: 5.0000e-04 - 30s/epoch - 154ms/step
Epoch 339/1000
2023-10-24 23:47:48.188 
Epoch 339/1000 
	 loss: 27.7077, MinusLogProbMetric: 27.7077, val_loss: 28.3377, val_MinusLogProbMetric: 28.3377

Epoch 339: val_loss did not improve from 28.27655
196/196 - 31s - loss: 27.7077 - MinusLogProbMetric: 27.7077 - val_loss: 28.3377 - val_MinusLogProbMetric: 28.3377 - lr: 5.0000e-04 - 31s/epoch - 158ms/step
Epoch 340/1000
2023-10-24 23:48:19.353 
Epoch 340/1000 
	 loss: 27.7270, MinusLogProbMetric: 27.7270, val_loss: 28.5707, val_MinusLogProbMetric: 28.5707

Epoch 340: val_loss did not improve from 28.27655
196/196 - 31s - loss: 27.7270 - MinusLogProbMetric: 27.7270 - val_loss: 28.5707 - val_MinusLogProbMetric: 28.5707 - lr: 5.0000e-04 - 31s/epoch - 159ms/step
Epoch 341/1000
2023-10-24 23:48:51.353 
Epoch 341/1000 
	 loss: 27.7119, MinusLogProbMetric: 27.7119, val_loss: 28.6287, val_MinusLogProbMetric: 28.6287

Epoch 341: val_loss did not improve from 28.27655
196/196 - 32s - loss: 27.7119 - MinusLogProbMetric: 27.7119 - val_loss: 28.6287 - val_MinusLogProbMetric: 28.6287 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 342/1000
2023-10-24 23:49:20.386 
Epoch 342/1000 
	 loss: 27.7149, MinusLogProbMetric: 27.7149, val_loss: 28.3399, val_MinusLogProbMetric: 28.3399

Epoch 342: val_loss did not improve from 28.27655
196/196 - 29s - loss: 27.7149 - MinusLogProbMetric: 27.7149 - val_loss: 28.3399 - val_MinusLogProbMetric: 28.3399 - lr: 5.0000e-04 - 29s/epoch - 148ms/step
Epoch 343/1000
2023-10-24 23:49:52.656 
Epoch 343/1000 
	 loss: 27.7589, MinusLogProbMetric: 27.7589, val_loss: 28.3331, val_MinusLogProbMetric: 28.3331

Epoch 343: val_loss did not improve from 28.27655
196/196 - 32s - loss: 27.7589 - MinusLogProbMetric: 27.7589 - val_loss: 28.3331 - val_MinusLogProbMetric: 28.3331 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 344/1000
2023-10-24 23:50:23.858 
Epoch 344/1000 
	 loss: 27.7796, MinusLogProbMetric: 27.7796, val_loss: 28.3851, val_MinusLogProbMetric: 28.3851

Epoch 344: val_loss did not improve from 28.27655
196/196 - 31s - loss: 27.7796 - MinusLogProbMetric: 27.7796 - val_loss: 28.3851 - val_MinusLogProbMetric: 28.3851 - lr: 5.0000e-04 - 31s/epoch - 159ms/step
Epoch 345/1000
2023-10-24 23:50:56.592 
Epoch 345/1000 
	 loss: 27.6888, MinusLogProbMetric: 27.6888, val_loss: 28.4716, val_MinusLogProbMetric: 28.4716

Epoch 345: val_loss did not improve from 28.27655
196/196 - 33s - loss: 27.6888 - MinusLogProbMetric: 27.6888 - val_loss: 28.4716 - val_MinusLogProbMetric: 28.4716 - lr: 5.0000e-04 - 33s/epoch - 167ms/step
Epoch 346/1000
2023-10-24 23:51:28.456 
Epoch 346/1000 
	 loss: 27.8041, MinusLogProbMetric: 27.8041, val_loss: 28.3713, val_MinusLogProbMetric: 28.3713

Epoch 346: val_loss did not improve from 28.27655
196/196 - 32s - loss: 27.8041 - MinusLogProbMetric: 27.8041 - val_loss: 28.3713 - val_MinusLogProbMetric: 28.3713 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 347/1000
2023-10-24 23:52:00.417 
Epoch 347/1000 
	 loss: 27.6885, MinusLogProbMetric: 27.6885, val_loss: 28.4775, val_MinusLogProbMetric: 28.4775

Epoch 347: val_loss did not improve from 28.27655
196/196 - 32s - loss: 27.6885 - MinusLogProbMetric: 27.6885 - val_loss: 28.4775 - val_MinusLogProbMetric: 28.4775 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 348/1000
2023-10-24 23:52:30.521 
Epoch 348/1000 
	 loss: 27.7795, MinusLogProbMetric: 27.7795, val_loss: 28.5216, val_MinusLogProbMetric: 28.5216

Epoch 348: val_loss did not improve from 28.27655
196/196 - 30s - loss: 27.7795 - MinusLogProbMetric: 27.7795 - val_loss: 28.5216 - val_MinusLogProbMetric: 28.5216 - lr: 5.0000e-04 - 30s/epoch - 154ms/step
Epoch 349/1000
2023-10-24 23:53:02.744 
Epoch 349/1000 
	 loss: 27.7352, MinusLogProbMetric: 27.7352, val_loss: 28.8686, val_MinusLogProbMetric: 28.8686

Epoch 349: val_loss did not improve from 28.27655
196/196 - 32s - loss: 27.7352 - MinusLogProbMetric: 27.7352 - val_loss: 28.8686 - val_MinusLogProbMetric: 28.8686 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 350/1000
2023-10-24 23:53:33.803 
Epoch 350/1000 
	 loss: 27.8465, MinusLogProbMetric: 27.8465, val_loss: 28.3129, val_MinusLogProbMetric: 28.3129

Epoch 350: val_loss did not improve from 28.27655
196/196 - 31s - loss: 27.8465 - MinusLogProbMetric: 27.8465 - val_loss: 28.3129 - val_MinusLogProbMetric: 28.3129 - lr: 5.0000e-04 - 31s/epoch - 158ms/step
Epoch 351/1000
2023-10-24 23:54:05.161 
Epoch 351/1000 
	 loss: 27.7171, MinusLogProbMetric: 27.7171, val_loss: 28.9484, val_MinusLogProbMetric: 28.9484

Epoch 351: val_loss did not improve from 28.27655
196/196 - 31s - loss: 27.7171 - MinusLogProbMetric: 27.7171 - val_loss: 28.9484 - val_MinusLogProbMetric: 28.9484 - lr: 5.0000e-04 - 31s/epoch - 160ms/step
Epoch 352/1000
2023-10-24 23:54:36.633 
Epoch 352/1000 
	 loss: 27.6951, MinusLogProbMetric: 27.6951, val_loss: 28.2795, val_MinusLogProbMetric: 28.2795

Epoch 352: val_loss did not improve from 28.27655
196/196 - 31s - loss: 27.6951 - MinusLogProbMetric: 27.6951 - val_loss: 28.2795 - val_MinusLogProbMetric: 28.2795 - lr: 5.0000e-04 - 31s/epoch - 161ms/step
Epoch 353/1000
2023-10-24 23:55:06.090 
Epoch 353/1000 
	 loss: 27.7226, MinusLogProbMetric: 27.7226, val_loss: 28.3909, val_MinusLogProbMetric: 28.3909

Epoch 353: val_loss did not improve from 28.27655
196/196 - 29s - loss: 27.7226 - MinusLogProbMetric: 27.7226 - val_loss: 28.3909 - val_MinusLogProbMetric: 28.3909 - lr: 5.0000e-04 - 29s/epoch - 150ms/step
Epoch 354/1000
2023-10-24 23:55:38.568 
Epoch 354/1000 
	 loss: 27.7022, MinusLogProbMetric: 27.7022, val_loss: 28.5462, val_MinusLogProbMetric: 28.5462

Epoch 354: val_loss did not improve from 28.27655
196/196 - 32s - loss: 27.7022 - MinusLogProbMetric: 27.7022 - val_loss: 28.5462 - val_MinusLogProbMetric: 28.5462 - lr: 5.0000e-04 - 32s/epoch - 166ms/step
Epoch 355/1000
2023-10-24 23:56:11.245 
Epoch 355/1000 
	 loss: 27.7007, MinusLogProbMetric: 27.7007, val_loss: 28.4996, val_MinusLogProbMetric: 28.4996

Epoch 355: val_loss did not improve from 28.27655
196/196 - 33s - loss: 27.7007 - MinusLogProbMetric: 27.7007 - val_loss: 28.4996 - val_MinusLogProbMetric: 28.4996 - lr: 5.0000e-04 - 33s/epoch - 167ms/step
Epoch 356/1000
2023-10-24 23:56:42.853 
Epoch 356/1000 
	 loss: 27.6987, MinusLogProbMetric: 27.6987, val_loss: 28.3578, val_MinusLogProbMetric: 28.3578

Epoch 356: val_loss did not improve from 28.27655
196/196 - 32s - loss: 27.6987 - MinusLogProbMetric: 27.6987 - val_loss: 28.3578 - val_MinusLogProbMetric: 28.3578 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 357/1000
2023-10-24 23:57:13.671 
Epoch 357/1000 
	 loss: 27.7049, MinusLogProbMetric: 27.7049, val_loss: 28.3629, val_MinusLogProbMetric: 28.3629

Epoch 357: val_loss did not improve from 28.27655
196/196 - 31s - loss: 27.7049 - MinusLogProbMetric: 27.7049 - val_loss: 28.3629 - val_MinusLogProbMetric: 28.3629 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 358/1000
2023-10-24 23:57:46.225 
Epoch 358/1000 
	 loss: 27.6945, MinusLogProbMetric: 27.6945, val_loss: 28.4329, val_MinusLogProbMetric: 28.4329

Epoch 358: val_loss did not improve from 28.27655
196/196 - 33s - loss: 27.6945 - MinusLogProbMetric: 27.6945 - val_loss: 28.4329 - val_MinusLogProbMetric: 28.4329 - lr: 5.0000e-04 - 33s/epoch - 166ms/step
Epoch 359/1000
2023-10-24 23:58:17.388 
Epoch 359/1000 
	 loss: 27.8063, MinusLogProbMetric: 27.8063, val_loss: 28.5567, val_MinusLogProbMetric: 28.5567

Epoch 359: val_loss did not improve from 28.27655
196/196 - 31s - loss: 27.8063 - MinusLogProbMetric: 27.8063 - val_loss: 28.5567 - val_MinusLogProbMetric: 28.5567 - lr: 5.0000e-04 - 31s/epoch - 159ms/step
Epoch 360/1000
2023-10-24 23:58:48.826 
Epoch 360/1000 
	 loss: 27.7989, MinusLogProbMetric: 27.7989, val_loss: 28.3949, val_MinusLogProbMetric: 28.3949

Epoch 360: val_loss did not improve from 28.27655
196/196 - 31s - loss: 27.7989 - MinusLogProbMetric: 27.7989 - val_loss: 28.3949 - val_MinusLogProbMetric: 28.3949 - lr: 5.0000e-04 - 31s/epoch - 160ms/step
Epoch 361/1000
2023-10-24 23:59:20.960 
Epoch 361/1000 
	 loss: 27.6812, MinusLogProbMetric: 27.6812, val_loss: 28.3528, val_MinusLogProbMetric: 28.3528

Epoch 361: val_loss did not improve from 28.27655
196/196 - 32s - loss: 27.6812 - MinusLogProbMetric: 27.6812 - val_loss: 28.3528 - val_MinusLogProbMetric: 28.3528 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 362/1000
2023-10-24 23:59:53.341 
Epoch 362/1000 
	 loss: 27.6870, MinusLogProbMetric: 27.6870, val_loss: 28.4592, val_MinusLogProbMetric: 28.4592

Epoch 362: val_loss did not improve from 28.27655
196/196 - 32s - loss: 27.6870 - MinusLogProbMetric: 27.6870 - val_loss: 28.4592 - val_MinusLogProbMetric: 28.4592 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 363/1000
2023-10-25 00:00:24.986 
Epoch 363/1000 
	 loss: 27.6809, MinusLogProbMetric: 27.6809, val_loss: 28.3124, val_MinusLogProbMetric: 28.3124

Epoch 363: val_loss did not improve from 28.27655
196/196 - 32s - loss: 27.6809 - MinusLogProbMetric: 27.6809 - val_loss: 28.3124 - val_MinusLogProbMetric: 28.3124 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 364/1000
2023-10-25 00:00:55.229 
Epoch 364/1000 
	 loss: 27.7159, MinusLogProbMetric: 27.7159, val_loss: 28.4231, val_MinusLogProbMetric: 28.4231

Epoch 364: val_loss did not improve from 28.27655
196/196 - 30s - loss: 27.7159 - MinusLogProbMetric: 27.7159 - val_loss: 28.4231 - val_MinusLogProbMetric: 28.4231 - lr: 5.0000e-04 - 30s/epoch - 154ms/step
Epoch 365/1000
2023-10-25 00:01:27.189 
Epoch 365/1000 
	 loss: 27.7801, MinusLogProbMetric: 27.7801, val_loss: 28.4351, val_MinusLogProbMetric: 28.4351

Epoch 365: val_loss did not improve from 28.27655
196/196 - 32s - loss: 27.7801 - MinusLogProbMetric: 27.7801 - val_loss: 28.4351 - val_MinusLogProbMetric: 28.4351 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 366/1000
2023-10-25 00:01:58.840 
Epoch 366/1000 
	 loss: 27.6688, MinusLogProbMetric: 27.6688, val_loss: 28.4002, val_MinusLogProbMetric: 28.4002

Epoch 366: val_loss did not improve from 28.27655
196/196 - 32s - loss: 27.6688 - MinusLogProbMetric: 27.6688 - val_loss: 28.4002 - val_MinusLogProbMetric: 28.4002 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 367/1000
2023-10-25 00:02:30.419 
Epoch 367/1000 
	 loss: 27.6918, MinusLogProbMetric: 27.6918, val_loss: 28.3281, val_MinusLogProbMetric: 28.3281

Epoch 367: val_loss did not improve from 28.27655
196/196 - 32s - loss: 27.6918 - MinusLogProbMetric: 27.6918 - val_loss: 28.3281 - val_MinusLogProbMetric: 28.3281 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 368/1000
2023-10-25 00:03:01.384 
Epoch 368/1000 
	 loss: 27.7591, MinusLogProbMetric: 27.7591, val_loss: 28.5216, val_MinusLogProbMetric: 28.5216

Epoch 368: val_loss did not improve from 28.27655
196/196 - 31s - loss: 27.7591 - MinusLogProbMetric: 27.7591 - val_loss: 28.5216 - val_MinusLogProbMetric: 28.5216 - lr: 5.0000e-04 - 31s/epoch - 158ms/step
Epoch 369/1000
2023-10-25 00:03:31.215 
Epoch 369/1000 
	 loss: 27.7179, MinusLogProbMetric: 27.7179, val_loss: 28.3431, val_MinusLogProbMetric: 28.3431

Epoch 369: val_loss did not improve from 28.27655
196/196 - 30s - loss: 27.7179 - MinusLogProbMetric: 27.7179 - val_loss: 28.3431 - val_MinusLogProbMetric: 28.3431 - lr: 5.0000e-04 - 30s/epoch - 152ms/step
Epoch 370/1000
2023-10-25 00:04:03.012 
Epoch 370/1000 
	 loss: 27.5035, MinusLogProbMetric: 27.5035, val_loss: 28.2184, val_MinusLogProbMetric: 28.2184

Epoch 370: val_loss improved from 28.27655 to 28.21841, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 27.5035 - MinusLogProbMetric: 27.5035 - val_loss: 28.2184 - val_MinusLogProbMetric: 28.2184 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 371/1000
2023-10-25 00:04:35.978 
Epoch 371/1000 
	 loss: 27.5084, MinusLogProbMetric: 27.5084, val_loss: 28.2071, val_MinusLogProbMetric: 28.2071

Epoch 371: val_loss improved from 28.21841 to 28.20706, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 27.5084 - MinusLogProbMetric: 27.5084 - val_loss: 28.2071 - val_MinusLogProbMetric: 28.2071 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 372/1000
2023-10-25 00:05:07.616 
Epoch 372/1000 
	 loss: 27.5152, MinusLogProbMetric: 27.5152, val_loss: 28.2666, val_MinusLogProbMetric: 28.2666

Epoch 372: val_loss did not improve from 28.20706
196/196 - 31s - loss: 27.5152 - MinusLogProbMetric: 27.5152 - val_loss: 28.2666 - val_MinusLogProbMetric: 28.2666 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 373/1000
2023-10-25 00:05:39.580 
Epoch 373/1000 
	 loss: 27.5213, MinusLogProbMetric: 27.5213, val_loss: 28.2928, val_MinusLogProbMetric: 28.2928

Epoch 373: val_loss did not improve from 28.20706
196/196 - 32s - loss: 27.5213 - MinusLogProbMetric: 27.5213 - val_loss: 28.2928 - val_MinusLogProbMetric: 28.2928 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 374/1000
2023-10-25 00:06:10.775 
Epoch 374/1000 
	 loss: 27.5147, MinusLogProbMetric: 27.5147, val_loss: 28.2039, val_MinusLogProbMetric: 28.2039

Epoch 374: val_loss improved from 28.20706 to 28.20394, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 27.5147 - MinusLogProbMetric: 27.5147 - val_loss: 28.2039 - val_MinusLogProbMetric: 28.2039 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 375/1000
2023-10-25 00:06:42.254 
Epoch 375/1000 
	 loss: 27.5101, MinusLogProbMetric: 27.5101, val_loss: 28.2836, val_MinusLogProbMetric: 28.2836

Epoch 375: val_loss did not improve from 28.20394
196/196 - 31s - loss: 27.5101 - MinusLogProbMetric: 27.5101 - val_loss: 28.2836 - val_MinusLogProbMetric: 28.2836 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 376/1000
2023-10-25 00:07:13.539 
Epoch 376/1000 
	 loss: 27.5015, MinusLogProbMetric: 27.5015, val_loss: 28.2452, val_MinusLogProbMetric: 28.2452

Epoch 376: val_loss did not improve from 28.20394
196/196 - 31s - loss: 27.5015 - MinusLogProbMetric: 27.5015 - val_loss: 28.2452 - val_MinusLogProbMetric: 28.2452 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 377/1000
2023-10-25 00:07:45.838 
Epoch 377/1000 
	 loss: 27.5037, MinusLogProbMetric: 27.5037, val_loss: 28.2592, val_MinusLogProbMetric: 28.2592

Epoch 377: val_loss did not improve from 28.20394
196/196 - 32s - loss: 27.5037 - MinusLogProbMetric: 27.5037 - val_loss: 28.2592 - val_MinusLogProbMetric: 28.2592 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 378/1000
2023-10-25 00:08:16.951 
Epoch 378/1000 
	 loss: 27.4969, MinusLogProbMetric: 27.4969, val_loss: 28.3053, val_MinusLogProbMetric: 28.3053

Epoch 378: val_loss did not improve from 28.20394
196/196 - 31s - loss: 27.4969 - MinusLogProbMetric: 27.4969 - val_loss: 28.3053 - val_MinusLogProbMetric: 28.3053 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 379/1000
2023-10-25 00:08:47.363 
Epoch 379/1000 
	 loss: 27.5018, MinusLogProbMetric: 27.5018, val_loss: 28.1618, val_MinusLogProbMetric: 28.1618

Epoch 379: val_loss improved from 28.20394 to 28.16183, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 31s - loss: 27.5018 - MinusLogProbMetric: 27.5018 - val_loss: 28.1618 - val_MinusLogProbMetric: 28.1618 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 380/1000
2023-10-25 00:09:19.226 
Epoch 380/1000 
	 loss: 27.4974, MinusLogProbMetric: 27.4974, val_loss: 28.2164, val_MinusLogProbMetric: 28.2164

Epoch 380: val_loss did not improve from 28.16183
196/196 - 31s - loss: 27.4974 - MinusLogProbMetric: 27.4974 - val_loss: 28.2164 - val_MinusLogProbMetric: 28.2164 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 381/1000
2023-10-25 00:09:51.982 
Epoch 381/1000 
	 loss: 27.5019, MinusLogProbMetric: 27.5019, val_loss: 28.1695, val_MinusLogProbMetric: 28.1695

Epoch 381: val_loss did not improve from 28.16183
196/196 - 33s - loss: 27.5019 - MinusLogProbMetric: 27.5019 - val_loss: 28.1695 - val_MinusLogProbMetric: 28.1695 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 382/1000
2023-10-25 00:10:24.524 
Epoch 382/1000 
	 loss: 27.4974, MinusLogProbMetric: 27.4974, val_loss: 28.2210, val_MinusLogProbMetric: 28.2210

Epoch 382: val_loss did not improve from 28.16183
196/196 - 33s - loss: 27.4974 - MinusLogProbMetric: 27.4974 - val_loss: 28.2210 - val_MinusLogProbMetric: 28.2210 - lr: 2.5000e-04 - 33s/epoch - 166ms/step
Epoch 383/1000
2023-10-25 00:10:57.424 
Epoch 383/1000 
	 loss: 27.5153, MinusLogProbMetric: 27.5153, val_loss: 28.2279, val_MinusLogProbMetric: 28.2279

Epoch 383: val_loss did not improve from 28.16183
196/196 - 33s - loss: 27.5153 - MinusLogProbMetric: 27.5153 - val_loss: 28.2279 - val_MinusLogProbMetric: 28.2279 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 384/1000
2023-10-25 00:11:29.688 
Epoch 384/1000 
	 loss: 27.4832, MinusLogProbMetric: 27.4832, val_loss: 28.3071, val_MinusLogProbMetric: 28.3071

Epoch 384: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.4832 - MinusLogProbMetric: 27.4832 - val_loss: 28.3071 - val_MinusLogProbMetric: 28.3071 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 385/1000
2023-10-25 00:12:01.604 
Epoch 385/1000 
	 loss: 27.5126, MinusLogProbMetric: 27.5126, val_loss: 28.1949, val_MinusLogProbMetric: 28.1949

Epoch 385: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.5126 - MinusLogProbMetric: 27.5126 - val_loss: 28.1949 - val_MinusLogProbMetric: 28.1949 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 386/1000
2023-10-25 00:12:34.080 
Epoch 386/1000 
	 loss: 27.4953, MinusLogProbMetric: 27.4953, val_loss: 28.1993, val_MinusLogProbMetric: 28.1993

Epoch 386: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.4953 - MinusLogProbMetric: 27.4953 - val_loss: 28.1993 - val_MinusLogProbMetric: 28.1993 - lr: 2.5000e-04 - 32s/epoch - 166ms/step
Epoch 387/1000
2023-10-25 00:13:06.660 
Epoch 387/1000 
	 loss: 27.5128, MinusLogProbMetric: 27.5128, val_loss: 28.1818, val_MinusLogProbMetric: 28.1818

Epoch 387: val_loss did not improve from 28.16183
196/196 - 33s - loss: 27.5128 - MinusLogProbMetric: 27.5128 - val_loss: 28.1818 - val_MinusLogProbMetric: 28.1818 - lr: 2.5000e-04 - 33s/epoch - 166ms/step
Epoch 388/1000
2023-10-25 00:13:38.360 
Epoch 388/1000 
	 loss: 27.5066, MinusLogProbMetric: 27.5066, val_loss: 28.2319, val_MinusLogProbMetric: 28.2319

Epoch 388: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.5066 - MinusLogProbMetric: 27.5066 - val_loss: 28.2319 - val_MinusLogProbMetric: 28.2319 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 389/1000
2023-10-25 00:14:08.423 
Epoch 389/1000 
	 loss: 27.5061, MinusLogProbMetric: 27.5061, val_loss: 28.2166, val_MinusLogProbMetric: 28.2166

Epoch 389: val_loss did not improve from 28.16183
196/196 - 30s - loss: 27.5061 - MinusLogProbMetric: 27.5061 - val_loss: 28.2166 - val_MinusLogProbMetric: 28.2166 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 390/1000
2023-10-25 00:14:38.611 
Epoch 390/1000 
	 loss: 27.4879, MinusLogProbMetric: 27.4879, val_loss: 28.1913, val_MinusLogProbMetric: 28.1913

Epoch 390: val_loss did not improve from 28.16183
196/196 - 30s - loss: 27.4879 - MinusLogProbMetric: 27.4879 - val_loss: 28.1913 - val_MinusLogProbMetric: 28.1913 - lr: 2.5000e-04 - 30s/epoch - 154ms/step
Epoch 391/1000
2023-10-25 00:15:09.682 
Epoch 391/1000 
	 loss: 27.5022, MinusLogProbMetric: 27.5022, val_loss: 28.2260, val_MinusLogProbMetric: 28.2260

Epoch 391: val_loss did not improve from 28.16183
196/196 - 31s - loss: 27.5022 - MinusLogProbMetric: 27.5022 - val_loss: 28.2260 - val_MinusLogProbMetric: 28.2260 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 392/1000
2023-10-25 00:15:41.336 
Epoch 392/1000 
	 loss: 27.4900, MinusLogProbMetric: 27.4900, val_loss: 28.1925, val_MinusLogProbMetric: 28.1925

Epoch 392: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.4900 - MinusLogProbMetric: 27.4900 - val_loss: 28.1925 - val_MinusLogProbMetric: 28.1925 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 393/1000
2023-10-25 00:16:13.530 
Epoch 393/1000 
	 loss: 27.4907, MinusLogProbMetric: 27.4907, val_loss: 28.2751, val_MinusLogProbMetric: 28.2751

Epoch 393: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.4907 - MinusLogProbMetric: 27.4907 - val_loss: 28.2751 - val_MinusLogProbMetric: 28.2751 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 394/1000
2023-10-25 00:16:45.529 
Epoch 394/1000 
	 loss: 27.4892, MinusLogProbMetric: 27.4892, val_loss: 28.1799, val_MinusLogProbMetric: 28.1799

Epoch 394: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.4892 - MinusLogProbMetric: 27.4892 - val_loss: 28.1799 - val_MinusLogProbMetric: 28.1799 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 395/1000
2023-10-25 00:17:16.218 
Epoch 395/1000 
	 loss: 27.4826, MinusLogProbMetric: 27.4826, val_loss: 28.1728, val_MinusLogProbMetric: 28.1728

Epoch 395: val_loss did not improve from 28.16183
196/196 - 31s - loss: 27.4826 - MinusLogProbMetric: 27.4826 - val_loss: 28.1728 - val_MinusLogProbMetric: 28.1728 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 396/1000
2023-10-25 00:17:47.966 
Epoch 396/1000 
	 loss: 27.5021, MinusLogProbMetric: 27.5021, val_loss: 28.2063, val_MinusLogProbMetric: 28.2063

Epoch 396: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.5021 - MinusLogProbMetric: 27.5021 - val_loss: 28.2063 - val_MinusLogProbMetric: 28.2063 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 397/1000
2023-10-25 00:18:19.024 
Epoch 397/1000 
	 loss: 27.4929, MinusLogProbMetric: 27.4929, val_loss: 28.1899, val_MinusLogProbMetric: 28.1899

Epoch 397: val_loss did not improve from 28.16183
196/196 - 31s - loss: 27.4929 - MinusLogProbMetric: 27.4929 - val_loss: 28.1899 - val_MinusLogProbMetric: 28.1899 - lr: 2.5000e-04 - 31s/epoch - 158ms/step
Epoch 398/1000
2023-10-25 00:18:51.223 
Epoch 398/1000 
	 loss: 27.4923, MinusLogProbMetric: 27.4923, val_loss: 28.1905, val_MinusLogProbMetric: 28.1905

Epoch 398: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.4923 - MinusLogProbMetric: 27.4923 - val_loss: 28.1905 - val_MinusLogProbMetric: 28.1905 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 399/1000
2023-10-25 00:19:23.850 
Epoch 399/1000 
	 loss: 27.5109, MinusLogProbMetric: 27.5109, val_loss: 28.2888, val_MinusLogProbMetric: 28.2888

Epoch 399: val_loss did not improve from 28.16183
196/196 - 33s - loss: 27.5109 - MinusLogProbMetric: 27.5109 - val_loss: 28.2888 - val_MinusLogProbMetric: 28.2888 - lr: 2.5000e-04 - 33s/epoch - 166ms/step
Epoch 400/1000
2023-10-25 00:19:56.394 
Epoch 400/1000 
	 loss: 27.4931, MinusLogProbMetric: 27.4931, val_loss: 28.2164, val_MinusLogProbMetric: 28.2164

Epoch 400: val_loss did not improve from 28.16183
196/196 - 33s - loss: 27.4931 - MinusLogProbMetric: 27.4931 - val_loss: 28.2164 - val_MinusLogProbMetric: 28.2164 - lr: 2.5000e-04 - 33s/epoch - 166ms/step
Epoch 401/1000
2023-10-25 00:20:27.625 
Epoch 401/1000 
	 loss: 27.5022, MinusLogProbMetric: 27.5022, val_loss: 28.2481, val_MinusLogProbMetric: 28.2481

Epoch 401: val_loss did not improve from 28.16183
196/196 - 31s - loss: 27.5022 - MinusLogProbMetric: 27.5022 - val_loss: 28.2481 - val_MinusLogProbMetric: 28.2481 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 402/1000
2023-10-25 00:20:57.076 
Epoch 402/1000 
	 loss: 27.4931, MinusLogProbMetric: 27.4931, val_loss: 28.2751, val_MinusLogProbMetric: 28.2751

Epoch 402: val_loss did not improve from 28.16183
196/196 - 29s - loss: 27.4931 - MinusLogProbMetric: 27.4931 - val_loss: 28.2751 - val_MinusLogProbMetric: 28.2751 - lr: 2.5000e-04 - 29s/epoch - 150ms/step
Epoch 403/1000
2023-10-25 00:21:27.499 
Epoch 403/1000 
	 loss: 27.4930, MinusLogProbMetric: 27.4930, val_loss: 28.2017, val_MinusLogProbMetric: 28.2017

Epoch 403: val_loss did not improve from 28.16183
196/196 - 30s - loss: 27.4930 - MinusLogProbMetric: 27.4930 - val_loss: 28.2017 - val_MinusLogProbMetric: 28.2017 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 404/1000
2023-10-25 00:21:59.917 
Epoch 404/1000 
	 loss: 27.4850, MinusLogProbMetric: 27.4850, val_loss: 28.1741, val_MinusLogProbMetric: 28.1741

Epoch 404: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.4850 - MinusLogProbMetric: 27.4850 - val_loss: 28.1741 - val_MinusLogProbMetric: 28.1741 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 405/1000
2023-10-25 00:22:31.708 
Epoch 405/1000 
	 loss: 27.5006, MinusLogProbMetric: 27.5006, val_loss: 28.2202, val_MinusLogProbMetric: 28.2202

Epoch 405: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.5006 - MinusLogProbMetric: 27.5006 - val_loss: 28.2202 - val_MinusLogProbMetric: 28.2202 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 406/1000
2023-10-25 00:23:01.593 
Epoch 406/1000 
	 loss: 27.4887, MinusLogProbMetric: 27.4887, val_loss: 28.2040, val_MinusLogProbMetric: 28.2040

Epoch 406: val_loss did not improve from 28.16183
196/196 - 30s - loss: 27.4887 - MinusLogProbMetric: 27.4887 - val_loss: 28.2040 - val_MinusLogProbMetric: 28.2040 - lr: 2.5000e-04 - 30s/epoch - 152ms/step
Epoch 407/1000
2023-10-25 00:23:32.244 
Epoch 407/1000 
	 loss: 27.4903, MinusLogProbMetric: 27.4903, val_loss: 28.2267, val_MinusLogProbMetric: 28.2267

Epoch 407: val_loss did not improve from 28.16183
196/196 - 31s - loss: 27.4903 - MinusLogProbMetric: 27.4903 - val_loss: 28.2267 - val_MinusLogProbMetric: 28.2267 - lr: 2.5000e-04 - 31s/epoch - 156ms/step
Epoch 408/1000
2023-10-25 00:24:03.173 
Epoch 408/1000 
	 loss: 27.4835, MinusLogProbMetric: 27.4835, val_loss: 28.3417, val_MinusLogProbMetric: 28.3417

Epoch 408: val_loss did not improve from 28.16183
196/196 - 31s - loss: 27.4835 - MinusLogProbMetric: 27.4835 - val_loss: 28.3417 - val_MinusLogProbMetric: 28.3417 - lr: 2.5000e-04 - 31s/epoch - 158ms/step
Epoch 409/1000
2023-10-25 00:24:35.334 
Epoch 409/1000 
	 loss: 27.4823, MinusLogProbMetric: 27.4823, val_loss: 28.2155, val_MinusLogProbMetric: 28.2155

Epoch 409: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.4823 - MinusLogProbMetric: 27.4823 - val_loss: 28.2155 - val_MinusLogProbMetric: 28.2155 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 410/1000
2023-10-25 00:25:07.613 
Epoch 410/1000 
	 loss: 27.4925, MinusLogProbMetric: 27.4925, val_loss: 28.2051, val_MinusLogProbMetric: 28.2051

Epoch 410: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.4925 - MinusLogProbMetric: 27.4925 - val_loss: 28.2051 - val_MinusLogProbMetric: 28.2051 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 411/1000
2023-10-25 00:25:40.077 
Epoch 411/1000 
	 loss: 27.4940, MinusLogProbMetric: 27.4940, val_loss: 28.1968, val_MinusLogProbMetric: 28.1968

Epoch 411: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.4940 - MinusLogProbMetric: 27.4940 - val_loss: 28.1968 - val_MinusLogProbMetric: 28.1968 - lr: 2.5000e-04 - 32s/epoch - 166ms/step
Epoch 412/1000
2023-10-25 00:26:10.995 
Epoch 412/1000 
	 loss: 27.4918, MinusLogProbMetric: 27.4918, val_loss: 28.2517, val_MinusLogProbMetric: 28.2517

Epoch 412: val_loss did not improve from 28.16183
196/196 - 31s - loss: 27.4918 - MinusLogProbMetric: 27.4918 - val_loss: 28.2517 - val_MinusLogProbMetric: 28.2517 - lr: 2.5000e-04 - 31s/epoch - 158ms/step
Epoch 413/1000
2023-10-25 00:26:42.977 
Epoch 413/1000 
	 loss: 27.4973, MinusLogProbMetric: 27.4973, val_loss: 28.2018, val_MinusLogProbMetric: 28.2018

Epoch 413: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.4973 - MinusLogProbMetric: 27.4973 - val_loss: 28.2018 - val_MinusLogProbMetric: 28.2018 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 414/1000
2023-10-25 00:27:15.156 
Epoch 414/1000 
	 loss: 27.4959, MinusLogProbMetric: 27.4959, val_loss: 28.2360, val_MinusLogProbMetric: 28.2360

Epoch 414: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.4959 - MinusLogProbMetric: 27.4959 - val_loss: 28.2360 - val_MinusLogProbMetric: 28.2360 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 415/1000
2023-10-25 00:27:46.270 
Epoch 415/1000 
	 loss: 27.4994, MinusLogProbMetric: 27.4994, val_loss: 28.1696, val_MinusLogProbMetric: 28.1696

Epoch 415: val_loss did not improve from 28.16183
196/196 - 31s - loss: 27.4994 - MinusLogProbMetric: 27.4994 - val_loss: 28.1696 - val_MinusLogProbMetric: 28.1696 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 416/1000
2023-10-25 00:28:17.861 
Epoch 416/1000 
	 loss: 27.4818, MinusLogProbMetric: 27.4818, val_loss: 28.2169, val_MinusLogProbMetric: 28.2169

Epoch 416: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.4818 - MinusLogProbMetric: 27.4818 - val_loss: 28.2169 - val_MinusLogProbMetric: 28.2169 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 417/1000
2023-10-25 00:28:48.665 
Epoch 417/1000 
	 loss: 27.4726, MinusLogProbMetric: 27.4726, val_loss: 28.2225, val_MinusLogProbMetric: 28.2225

Epoch 417: val_loss did not improve from 28.16183
196/196 - 31s - loss: 27.4726 - MinusLogProbMetric: 27.4726 - val_loss: 28.2225 - val_MinusLogProbMetric: 28.2225 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 418/1000
2023-10-25 00:29:21.149 
Epoch 418/1000 
	 loss: 27.4786, MinusLogProbMetric: 27.4786, val_loss: 28.2191, val_MinusLogProbMetric: 28.2191

Epoch 418: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.4786 - MinusLogProbMetric: 27.4786 - val_loss: 28.2191 - val_MinusLogProbMetric: 28.2191 - lr: 2.5000e-04 - 32s/epoch - 166ms/step
Epoch 419/1000
2023-10-25 00:29:52.965 
Epoch 419/1000 
	 loss: 27.4686, MinusLogProbMetric: 27.4686, val_loss: 28.2319, val_MinusLogProbMetric: 28.2319

Epoch 419: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.4686 - MinusLogProbMetric: 27.4686 - val_loss: 28.2319 - val_MinusLogProbMetric: 28.2319 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 420/1000
2023-10-25 00:30:24.452 
Epoch 420/1000 
	 loss: 27.4862, MinusLogProbMetric: 27.4862, val_loss: 28.2129, val_MinusLogProbMetric: 28.2129

Epoch 420: val_loss did not improve from 28.16183
196/196 - 31s - loss: 27.4862 - MinusLogProbMetric: 27.4862 - val_loss: 28.2129 - val_MinusLogProbMetric: 28.2129 - lr: 2.5000e-04 - 31s/epoch - 161ms/step
Epoch 421/1000
2023-10-25 00:30:56.377 
Epoch 421/1000 
	 loss: 27.4714, MinusLogProbMetric: 27.4714, val_loss: 28.2638, val_MinusLogProbMetric: 28.2638

Epoch 421: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.4714 - MinusLogProbMetric: 27.4714 - val_loss: 28.2638 - val_MinusLogProbMetric: 28.2638 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 422/1000
2023-10-25 00:31:28.659 
Epoch 422/1000 
	 loss: 27.4784, MinusLogProbMetric: 27.4784, val_loss: 28.2675, val_MinusLogProbMetric: 28.2675

Epoch 422: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.4784 - MinusLogProbMetric: 27.4784 - val_loss: 28.2675 - val_MinusLogProbMetric: 28.2675 - lr: 2.5000e-04 - 32s/epoch - 165ms/step
Epoch 423/1000
2023-10-25 00:31:58.959 
Epoch 423/1000 
	 loss: 27.4891, MinusLogProbMetric: 27.4891, val_loss: 28.2285, val_MinusLogProbMetric: 28.2285

Epoch 423: val_loss did not improve from 28.16183
196/196 - 30s - loss: 27.4891 - MinusLogProbMetric: 27.4891 - val_loss: 28.2285 - val_MinusLogProbMetric: 28.2285 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 424/1000
2023-10-25 00:32:28.305 
Epoch 424/1000 
	 loss: 27.4743, MinusLogProbMetric: 27.4743, val_loss: 28.2697, val_MinusLogProbMetric: 28.2697

Epoch 424: val_loss did not improve from 28.16183
196/196 - 29s - loss: 27.4743 - MinusLogProbMetric: 27.4743 - val_loss: 28.2697 - val_MinusLogProbMetric: 28.2697 - lr: 2.5000e-04 - 29s/epoch - 150ms/step
Epoch 425/1000
2023-10-25 00:32:57.829 
Epoch 425/1000 
	 loss: 27.4787, MinusLogProbMetric: 27.4787, val_loss: 28.2690, val_MinusLogProbMetric: 28.2690

Epoch 425: val_loss did not improve from 28.16183
196/196 - 30s - loss: 27.4787 - MinusLogProbMetric: 27.4787 - val_loss: 28.2690 - val_MinusLogProbMetric: 28.2690 - lr: 2.5000e-04 - 30s/epoch - 151ms/step
Epoch 426/1000
2023-10-25 00:33:27.866 
Epoch 426/1000 
	 loss: 27.4798, MinusLogProbMetric: 27.4798, val_loss: 28.2397, val_MinusLogProbMetric: 28.2397

Epoch 426: val_loss did not improve from 28.16183
196/196 - 30s - loss: 27.4798 - MinusLogProbMetric: 27.4798 - val_loss: 28.2397 - val_MinusLogProbMetric: 28.2397 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 427/1000
2023-10-25 00:33:59.858 
Epoch 427/1000 
	 loss: 27.4814, MinusLogProbMetric: 27.4814, val_loss: 28.2170, val_MinusLogProbMetric: 28.2170

Epoch 427: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.4814 - MinusLogProbMetric: 27.4814 - val_loss: 28.2170 - val_MinusLogProbMetric: 28.2170 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 428/1000
2023-10-25 00:34:30.854 
Epoch 428/1000 
	 loss: 27.4984, MinusLogProbMetric: 27.4984, val_loss: 28.2309, val_MinusLogProbMetric: 28.2309

Epoch 428: val_loss did not improve from 28.16183
196/196 - 31s - loss: 27.4984 - MinusLogProbMetric: 27.4984 - val_loss: 28.2309 - val_MinusLogProbMetric: 28.2309 - lr: 2.5000e-04 - 31s/epoch - 158ms/step
Epoch 429/1000
2023-10-25 00:35:02.653 
Epoch 429/1000 
	 loss: 27.4656, MinusLogProbMetric: 27.4656, val_loss: 28.3045, val_MinusLogProbMetric: 28.3045

Epoch 429: val_loss did not improve from 28.16183
196/196 - 32s - loss: 27.4656 - MinusLogProbMetric: 27.4656 - val_loss: 28.3045 - val_MinusLogProbMetric: 28.3045 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 430/1000
2023-10-25 00:35:35.234 
Epoch 430/1000 
	 loss: 27.4171, MinusLogProbMetric: 27.4171, val_loss: 28.1399, val_MinusLogProbMetric: 28.1399

Epoch 430: val_loss improved from 28.16183 to 28.13987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 27.4171 - MinusLogProbMetric: 27.4171 - val_loss: 28.1399 - val_MinusLogProbMetric: 28.1399 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 431/1000
2023-10-25 00:36:07.469 
Epoch 431/1000 
	 loss: 27.4049, MinusLogProbMetric: 27.4049, val_loss: 28.1418, val_MinusLogProbMetric: 28.1418

Epoch 431: val_loss did not improve from 28.13987
196/196 - 32s - loss: 27.4049 - MinusLogProbMetric: 27.4049 - val_loss: 28.1418 - val_MinusLogProbMetric: 28.1418 - lr: 1.2500e-04 - 32s/epoch - 161ms/step
Epoch 432/1000
2023-10-25 00:36:39.167 
Epoch 432/1000 
	 loss: 27.4095, MinusLogProbMetric: 27.4095, val_loss: 28.1605, val_MinusLogProbMetric: 28.1605

Epoch 432: val_loss did not improve from 28.13987
196/196 - 32s - loss: 27.4095 - MinusLogProbMetric: 27.4095 - val_loss: 28.1605 - val_MinusLogProbMetric: 28.1605 - lr: 1.2500e-04 - 32s/epoch - 162ms/step
Epoch 433/1000
2023-10-25 00:37:11.300 
Epoch 433/1000 
	 loss: 27.4050, MinusLogProbMetric: 27.4050, val_loss: 28.1970, val_MinusLogProbMetric: 28.1970

Epoch 433: val_loss did not improve from 28.13987
196/196 - 32s - loss: 27.4050 - MinusLogProbMetric: 27.4050 - val_loss: 28.1970 - val_MinusLogProbMetric: 28.1970 - lr: 1.2500e-04 - 32s/epoch - 164ms/step
Epoch 434/1000
2023-10-25 00:37:43.117 
Epoch 434/1000 
	 loss: 27.4095, MinusLogProbMetric: 27.4095, val_loss: 28.1561, val_MinusLogProbMetric: 28.1561

Epoch 434: val_loss did not improve from 28.13987
196/196 - 32s - loss: 27.4095 - MinusLogProbMetric: 27.4095 - val_loss: 28.1561 - val_MinusLogProbMetric: 28.1561 - lr: 1.2500e-04 - 32s/epoch - 162ms/step
Epoch 435/1000
2023-10-25 00:38:15.240 
Epoch 435/1000 
	 loss: 27.4115, MinusLogProbMetric: 27.4115, val_loss: 28.1864, val_MinusLogProbMetric: 28.1864

Epoch 435: val_loss did not improve from 28.13987
196/196 - 32s - loss: 27.4115 - MinusLogProbMetric: 27.4115 - val_loss: 28.1864 - val_MinusLogProbMetric: 28.1864 - lr: 1.2500e-04 - 32s/epoch - 164ms/step
Epoch 436/1000
2023-10-25 00:38:46.933 
Epoch 436/1000 
	 loss: 27.4055, MinusLogProbMetric: 27.4055, val_loss: 28.1847, val_MinusLogProbMetric: 28.1847

Epoch 436: val_loss did not improve from 28.13987
196/196 - 32s - loss: 27.4055 - MinusLogProbMetric: 27.4055 - val_loss: 28.1847 - val_MinusLogProbMetric: 28.1847 - lr: 1.2500e-04 - 32s/epoch - 162ms/step
Epoch 437/1000
2023-10-25 00:39:19.135 
Epoch 437/1000 
	 loss: 27.4027, MinusLogProbMetric: 27.4027, val_loss: 28.1802, val_MinusLogProbMetric: 28.1802

Epoch 437: val_loss did not improve from 28.13987
196/196 - 32s - loss: 27.4027 - MinusLogProbMetric: 27.4027 - val_loss: 28.1802 - val_MinusLogProbMetric: 28.1802 - lr: 1.2500e-04 - 32s/epoch - 164ms/step
Epoch 438/1000
2023-10-25 00:39:49.592 
Epoch 438/1000 
	 loss: 27.4055, MinusLogProbMetric: 27.4055, val_loss: 28.1761, val_MinusLogProbMetric: 28.1761

Epoch 438: val_loss did not improve from 28.13987
196/196 - 30s - loss: 27.4055 - MinusLogProbMetric: 27.4055 - val_loss: 28.1761 - val_MinusLogProbMetric: 28.1761 - lr: 1.2500e-04 - 30s/epoch - 155ms/step
Epoch 439/1000
2023-10-25 00:40:22.310 
Epoch 439/1000 
	 loss: 27.4063, MinusLogProbMetric: 27.4063, val_loss: 28.1594, val_MinusLogProbMetric: 28.1594

Epoch 439: val_loss did not improve from 28.13987
196/196 - 33s - loss: 27.4063 - MinusLogProbMetric: 27.4063 - val_loss: 28.1594 - val_MinusLogProbMetric: 28.1594 - lr: 1.2500e-04 - 33s/epoch - 167ms/step
Epoch 440/1000
2023-10-25 00:40:54.073 
Epoch 440/1000 
	 loss: 27.3971, MinusLogProbMetric: 27.3971, val_loss: 28.1591, val_MinusLogProbMetric: 28.1591

Epoch 440: val_loss did not improve from 28.13987
196/196 - 32s - loss: 27.3971 - MinusLogProbMetric: 27.3971 - val_loss: 28.1591 - val_MinusLogProbMetric: 28.1591 - lr: 1.2500e-04 - 32s/epoch - 162ms/step
Epoch 441/1000
2023-10-25 00:41:24.856 
Epoch 441/1000 
	 loss: 27.4059, MinusLogProbMetric: 27.4059, val_loss: 28.1584, val_MinusLogProbMetric: 28.1584

Epoch 441: val_loss did not improve from 28.13987
196/196 - 31s - loss: 27.4059 - MinusLogProbMetric: 27.4059 - val_loss: 28.1584 - val_MinusLogProbMetric: 28.1584 - lr: 1.2500e-04 - 31s/epoch - 157ms/step
Epoch 442/1000
2023-10-25 00:41:55.222 
Epoch 442/1000 
	 loss: 27.4038, MinusLogProbMetric: 27.4038, val_loss: 28.1817, val_MinusLogProbMetric: 28.1817

Epoch 442: val_loss did not improve from 28.13987
196/196 - 30s - loss: 27.4038 - MinusLogProbMetric: 27.4038 - val_loss: 28.1817 - val_MinusLogProbMetric: 28.1817 - lr: 1.2500e-04 - 30s/epoch - 155ms/step
Epoch 443/1000
2023-10-25 00:42:28.347 
Epoch 443/1000 
	 loss: 27.3999, MinusLogProbMetric: 27.3999, val_loss: 28.1728, val_MinusLogProbMetric: 28.1728

Epoch 443: val_loss did not improve from 28.13987
196/196 - 33s - loss: 27.3999 - MinusLogProbMetric: 27.3999 - val_loss: 28.1728 - val_MinusLogProbMetric: 28.1728 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 444/1000
2023-10-25 00:42:58.975 
Epoch 444/1000 
	 loss: 27.4054, MinusLogProbMetric: 27.4054, val_loss: 28.1361, val_MinusLogProbMetric: 28.1361

Epoch 444: val_loss improved from 28.13987 to 28.13610, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 31s - loss: 27.4054 - MinusLogProbMetric: 27.4054 - val_loss: 28.1361 - val_MinusLogProbMetric: 28.1361 - lr: 1.2500e-04 - 31s/epoch - 159ms/step
Epoch 445/1000
2023-10-25 00:43:29.901 
Epoch 445/1000 
	 loss: 27.4024, MinusLogProbMetric: 27.4024, val_loss: 28.1588, val_MinusLogProbMetric: 28.1588

Epoch 445: val_loss did not improve from 28.13610
196/196 - 30s - loss: 27.4024 - MinusLogProbMetric: 27.4024 - val_loss: 28.1588 - val_MinusLogProbMetric: 28.1588 - lr: 1.2500e-04 - 30s/epoch - 155ms/step
Epoch 446/1000
2023-10-25 00:43:59.369 
Epoch 446/1000 
	 loss: 27.3999, MinusLogProbMetric: 27.3999, val_loss: 28.1854, val_MinusLogProbMetric: 28.1854

Epoch 446: val_loss did not improve from 28.13610
196/196 - 29s - loss: 27.3999 - MinusLogProbMetric: 27.3999 - val_loss: 28.1854 - val_MinusLogProbMetric: 28.1854 - lr: 1.2500e-04 - 29s/epoch - 150ms/step
Epoch 447/1000
2023-10-25 00:44:29.368 
Epoch 447/1000 
	 loss: 27.3964, MinusLogProbMetric: 27.3964, val_loss: 28.1765, val_MinusLogProbMetric: 28.1765

Epoch 447: val_loss did not improve from 28.13610
196/196 - 30s - loss: 27.3964 - MinusLogProbMetric: 27.3964 - val_loss: 28.1765 - val_MinusLogProbMetric: 28.1765 - lr: 1.2500e-04 - 30s/epoch - 153ms/step
Epoch 448/1000
2023-10-25 00:45:01.161 
Epoch 448/1000 
	 loss: 27.3981, MinusLogProbMetric: 27.3981, val_loss: 28.1584, val_MinusLogProbMetric: 28.1584

Epoch 448: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3981 - MinusLogProbMetric: 27.3981 - val_loss: 28.1584 - val_MinusLogProbMetric: 28.1584 - lr: 1.2500e-04 - 32s/epoch - 162ms/step
Epoch 449/1000
2023-10-25 00:45:32.678 
Epoch 449/1000 
	 loss: 27.4064, MinusLogProbMetric: 27.4064, val_loss: 28.1571, val_MinusLogProbMetric: 28.1571

Epoch 449: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.4064 - MinusLogProbMetric: 27.4064 - val_loss: 28.1571 - val_MinusLogProbMetric: 28.1571 - lr: 1.2500e-04 - 32s/epoch - 161ms/step
Epoch 450/1000
2023-10-25 00:46:04.989 
Epoch 450/1000 
	 loss: 27.4025, MinusLogProbMetric: 27.4025, val_loss: 28.1923, val_MinusLogProbMetric: 28.1923

Epoch 450: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.4025 - MinusLogProbMetric: 27.4025 - val_loss: 28.1923 - val_MinusLogProbMetric: 28.1923 - lr: 1.2500e-04 - 32s/epoch - 165ms/step
Epoch 451/1000
2023-10-25 00:46:35.657 
Epoch 451/1000 
	 loss: 27.4115, MinusLogProbMetric: 27.4115, val_loss: 28.1984, val_MinusLogProbMetric: 28.1984

Epoch 451: val_loss did not improve from 28.13610
196/196 - 31s - loss: 27.4115 - MinusLogProbMetric: 27.4115 - val_loss: 28.1984 - val_MinusLogProbMetric: 28.1984 - lr: 1.2500e-04 - 31s/epoch - 156ms/step
Epoch 452/1000
2023-10-25 00:47:07.074 
Epoch 452/1000 
	 loss: 27.4015, MinusLogProbMetric: 27.4015, val_loss: 28.1727, val_MinusLogProbMetric: 28.1727

Epoch 452: val_loss did not improve from 28.13610
196/196 - 31s - loss: 27.4015 - MinusLogProbMetric: 27.4015 - val_loss: 28.1727 - val_MinusLogProbMetric: 28.1727 - lr: 1.2500e-04 - 31s/epoch - 160ms/step
Epoch 453/1000
2023-10-25 00:47:38.996 
Epoch 453/1000 
	 loss: 27.3969, MinusLogProbMetric: 27.3969, val_loss: 28.1652, val_MinusLogProbMetric: 28.1652

Epoch 453: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3969 - MinusLogProbMetric: 27.3969 - val_loss: 28.1652 - val_MinusLogProbMetric: 28.1652 - lr: 1.2500e-04 - 32s/epoch - 163ms/step
Epoch 454/1000
2023-10-25 00:48:11.377 
Epoch 454/1000 
	 loss: 27.4015, MinusLogProbMetric: 27.4015, val_loss: 28.1735, val_MinusLogProbMetric: 28.1735

Epoch 454: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.4015 - MinusLogProbMetric: 27.4015 - val_loss: 28.1735 - val_MinusLogProbMetric: 28.1735 - lr: 1.2500e-04 - 32s/epoch - 165ms/step
Epoch 455/1000
2023-10-25 00:48:43.369 
Epoch 455/1000 
	 loss: 27.4005, MinusLogProbMetric: 27.4005, val_loss: 28.1631, val_MinusLogProbMetric: 28.1631

Epoch 455: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.4005 - MinusLogProbMetric: 27.4005 - val_loss: 28.1631 - val_MinusLogProbMetric: 28.1631 - lr: 1.2500e-04 - 32s/epoch - 163ms/step
Epoch 456/1000
2023-10-25 00:49:14.898 
Epoch 456/1000 
	 loss: 27.3978, MinusLogProbMetric: 27.3978, val_loss: 28.1474, val_MinusLogProbMetric: 28.1474

Epoch 456: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3978 - MinusLogProbMetric: 27.3978 - val_loss: 28.1474 - val_MinusLogProbMetric: 28.1474 - lr: 1.2500e-04 - 32s/epoch - 161ms/step
Epoch 457/1000
2023-10-25 00:49:47.001 
Epoch 457/1000 
	 loss: 27.3950, MinusLogProbMetric: 27.3950, val_loss: 28.1507, val_MinusLogProbMetric: 28.1507

Epoch 457: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3950 - MinusLogProbMetric: 27.3950 - val_loss: 28.1507 - val_MinusLogProbMetric: 28.1507 - lr: 1.2500e-04 - 32s/epoch - 164ms/step
Epoch 458/1000
2023-10-25 00:50:17.539 
Epoch 458/1000 
	 loss: 27.3974, MinusLogProbMetric: 27.3974, val_loss: 28.1750, val_MinusLogProbMetric: 28.1750

Epoch 458: val_loss did not improve from 28.13610
196/196 - 31s - loss: 27.3974 - MinusLogProbMetric: 27.3974 - val_loss: 28.1750 - val_MinusLogProbMetric: 28.1750 - lr: 1.2500e-04 - 31s/epoch - 156ms/step
Epoch 459/1000
2023-10-25 00:50:49.677 
Epoch 459/1000 
	 loss: 27.3932, MinusLogProbMetric: 27.3932, val_loss: 28.1892, val_MinusLogProbMetric: 28.1892

Epoch 459: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3932 - MinusLogProbMetric: 27.3932 - val_loss: 28.1892 - val_MinusLogProbMetric: 28.1892 - lr: 1.2500e-04 - 32s/epoch - 164ms/step
Epoch 460/1000
2023-10-25 00:51:21.925 
Epoch 460/1000 
	 loss: 27.3995, MinusLogProbMetric: 27.3995, val_loss: 28.1806, val_MinusLogProbMetric: 28.1806

Epoch 460: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3995 - MinusLogProbMetric: 27.3995 - val_loss: 28.1806 - val_MinusLogProbMetric: 28.1806 - lr: 1.2500e-04 - 32s/epoch - 165ms/step
Epoch 461/1000
2023-10-25 00:51:52.944 
Epoch 461/1000 
	 loss: 27.4029, MinusLogProbMetric: 27.4029, val_loss: 28.1590, val_MinusLogProbMetric: 28.1590

Epoch 461: val_loss did not improve from 28.13610
196/196 - 31s - loss: 27.4029 - MinusLogProbMetric: 27.4029 - val_loss: 28.1590 - val_MinusLogProbMetric: 28.1590 - lr: 1.2500e-04 - 31s/epoch - 158ms/step
Epoch 462/1000
2023-10-25 00:52:25.434 
Epoch 462/1000 
	 loss: 27.3939, MinusLogProbMetric: 27.3939, val_loss: 28.1836, val_MinusLogProbMetric: 28.1836

Epoch 462: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3939 - MinusLogProbMetric: 27.3939 - val_loss: 28.1836 - val_MinusLogProbMetric: 28.1836 - lr: 1.2500e-04 - 32s/epoch - 166ms/step
Epoch 463/1000
2023-10-25 00:52:58.065 
Epoch 463/1000 
	 loss: 27.4039, MinusLogProbMetric: 27.4039, val_loss: 28.1587, val_MinusLogProbMetric: 28.1587

Epoch 463: val_loss did not improve from 28.13610
196/196 - 33s - loss: 27.4039 - MinusLogProbMetric: 27.4039 - val_loss: 28.1587 - val_MinusLogProbMetric: 28.1587 - lr: 1.2500e-04 - 33s/epoch - 166ms/step
Epoch 464/1000
2023-10-25 00:53:29.853 
Epoch 464/1000 
	 loss: 27.4060, MinusLogProbMetric: 27.4060, val_loss: 28.2046, val_MinusLogProbMetric: 28.2046

Epoch 464: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.4060 - MinusLogProbMetric: 27.4060 - val_loss: 28.2046 - val_MinusLogProbMetric: 28.2046 - lr: 1.2500e-04 - 32s/epoch - 162ms/step
Epoch 465/1000
2023-10-25 00:54:01.767 
Epoch 465/1000 
	 loss: 27.3950, MinusLogProbMetric: 27.3950, val_loss: 28.1551, val_MinusLogProbMetric: 28.1551

Epoch 465: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3950 - MinusLogProbMetric: 27.3950 - val_loss: 28.1551 - val_MinusLogProbMetric: 28.1551 - lr: 1.2500e-04 - 32s/epoch - 163ms/step
Epoch 466/1000
2023-10-25 00:54:32.889 
Epoch 466/1000 
	 loss: 27.3949, MinusLogProbMetric: 27.3949, val_loss: 28.1767, val_MinusLogProbMetric: 28.1767

Epoch 466: val_loss did not improve from 28.13610
196/196 - 31s - loss: 27.3949 - MinusLogProbMetric: 27.3949 - val_loss: 28.1767 - val_MinusLogProbMetric: 28.1767 - lr: 1.2500e-04 - 31s/epoch - 159ms/step
Epoch 467/1000
2023-10-25 00:55:04.104 
Epoch 467/1000 
	 loss: 27.3981, MinusLogProbMetric: 27.3981, val_loss: 28.1682, val_MinusLogProbMetric: 28.1682

Epoch 467: val_loss did not improve from 28.13610
196/196 - 31s - loss: 27.3981 - MinusLogProbMetric: 27.3981 - val_loss: 28.1682 - val_MinusLogProbMetric: 28.1682 - lr: 1.2500e-04 - 31s/epoch - 159ms/step
Epoch 468/1000
2023-10-25 00:55:36.152 
Epoch 468/1000 
	 loss: 27.3993, MinusLogProbMetric: 27.3993, val_loss: 28.1870, val_MinusLogProbMetric: 28.1870

Epoch 468: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3993 - MinusLogProbMetric: 27.3993 - val_loss: 28.1870 - val_MinusLogProbMetric: 28.1870 - lr: 1.2500e-04 - 32s/epoch - 163ms/step
Epoch 469/1000
2023-10-25 00:56:07.513 
Epoch 469/1000 
	 loss: 27.3924, MinusLogProbMetric: 27.3924, val_loss: 28.1931, val_MinusLogProbMetric: 28.1931

Epoch 469: val_loss did not improve from 28.13610
196/196 - 31s - loss: 27.3924 - MinusLogProbMetric: 27.3924 - val_loss: 28.1931 - val_MinusLogProbMetric: 28.1931 - lr: 1.2500e-04 - 31s/epoch - 160ms/step
Epoch 470/1000
2023-10-25 00:56:39.480 
Epoch 470/1000 
	 loss: 27.4021, MinusLogProbMetric: 27.4021, val_loss: 28.1577, val_MinusLogProbMetric: 28.1577

Epoch 470: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.4021 - MinusLogProbMetric: 27.4021 - val_loss: 28.1577 - val_MinusLogProbMetric: 28.1577 - lr: 1.2500e-04 - 32s/epoch - 163ms/step
Epoch 471/1000
2023-10-25 00:57:10.918 
Epoch 471/1000 
	 loss: 27.3966, MinusLogProbMetric: 27.3966, val_loss: 28.1631, val_MinusLogProbMetric: 28.1631

Epoch 471: val_loss did not improve from 28.13610
196/196 - 31s - loss: 27.3966 - MinusLogProbMetric: 27.3966 - val_loss: 28.1631 - val_MinusLogProbMetric: 28.1631 - lr: 1.2500e-04 - 31s/epoch - 160ms/step
Epoch 472/1000
2023-10-25 00:57:41.963 
Epoch 472/1000 
	 loss: 27.4020, MinusLogProbMetric: 27.4020, val_loss: 28.1610, val_MinusLogProbMetric: 28.1610

Epoch 472: val_loss did not improve from 28.13610
196/196 - 31s - loss: 27.4020 - MinusLogProbMetric: 27.4020 - val_loss: 28.1610 - val_MinusLogProbMetric: 28.1610 - lr: 1.2500e-04 - 31s/epoch - 158ms/step
Epoch 473/1000
2023-10-25 00:58:13.130 
Epoch 473/1000 
	 loss: 27.3956, MinusLogProbMetric: 27.3956, val_loss: 28.1665, val_MinusLogProbMetric: 28.1665

Epoch 473: val_loss did not improve from 28.13610
196/196 - 31s - loss: 27.3956 - MinusLogProbMetric: 27.3956 - val_loss: 28.1665 - val_MinusLogProbMetric: 28.1665 - lr: 1.2500e-04 - 31s/epoch - 159ms/step
Epoch 474/1000
2023-10-25 00:58:45.643 
Epoch 474/1000 
	 loss: 27.3933, MinusLogProbMetric: 27.3933, val_loss: 28.1700, val_MinusLogProbMetric: 28.1700

Epoch 474: val_loss did not improve from 28.13610
196/196 - 33s - loss: 27.3933 - MinusLogProbMetric: 27.3933 - val_loss: 28.1700 - val_MinusLogProbMetric: 28.1700 - lr: 1.2500e-04 - 33s/epoch - 166ms/step
Epoch 475/1000
2023-10-25 00:59:17.389 
Epoch 475/1000 
	 loss: 27.3942, MinusLogProbMetric: 27.3942, val_loss: 28.1927, val_MinusLogProbMetric: 28.1927

Epoch 475: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3942 - MinusLogProbMetric: 27.3942 - val_loss: 28.1927 - val_MinusLogProbMetric: 28.1927 - lr: 1.2500e-04 - 32s/epoch - 162ms/step
Epoch 476/1000
2023-10-25 00:59:48.078 
Epoch 476/1000 
	 loss: 27.3960, MinusLogProbMetric: 27.3960, val_loss: 28.1700, val_MinusLogProbMetric: 28.1700

Epoch 476: val_loss did not improve from 28.13610
196/196 - 31s - loss: 27.3960 - MinusLogProbMetric: 27.3960 - val_loss: 28.1700 - val_MinusLogProbMetric: 28.1700 - lr: 1.2500e-04 - 31s/epoch - 157ms/step
Epoch 477/1000
2023-10-25 01:00:17.687 
Epoch 477/1000 
	 loss: 27.3937, MinusLogProbMetric: 27.3937, val_loss: 28.1506, val_MinusLogProbMetric: 28.1506

Epoch 477: val_loss did not improve from 28.13610
196/196 - 30s - loss: 27.3937 - MinusLogProbMetric: 27.3937 - val_loss: 28.1506 - val_MinusLogProbMetric: 28.1506 - lr: 1.2500e-04 - 30s/epoch - 151ms/step
Epoch 478/1000
2023-10-25 01:00:49.010 
Epoch 478/1000 
	 loss: 27.3995, MinusLogProbMetric: 27.3995, val_loss: 28.1524, val_MinusLogProbMetric: 28.1524

Epoch 478: val_loss did not improve from 28.13610
196/196 - 31s - loss: 27.3995 - MinusLogProbMetric: 27.3995 - val_loss: 28.1524 - val_MinusLogProbMetric: 28.1524 - lr: 1.2500e-04 - 31s/epoch - 160ms/step
Epoch 479/1000
2023-10-25 01:01:19.391 
Epoch 479/1000 
	 loss: 27.3949, MinusLogProbMetric: 27.3949, val_loss: 28.1770, val_MinusLogProbMetric: 28.1770

Epoch 479: val_loss did not improve from 28.13610
196/196 - 30s - loss: 27.3949 - MinusLogProbMetric: 27.3949 - val_loss: 28.1770 - val_MinusLogProbMetric: 28.1770 - lr: 1.2500e-04 - 30s/epoch - 155ms/step
Epoch 480/1000
2023-10-25 01:01:51.285 
Epoch 480/1000 
	 loss: 27.3932, MinusLogProbMetric: 27.3932, val_loss: 28.1417, val_MinusLogProbMetric: 28.1417

Epoch 480: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3932 - MinusLogProbMetric: 27.3932 - val_loss: 28.1417 - val_MinusLogProbMetric: 28.1417 - lr: 1.2500e-04 - 32s/epoch - 163ms/step
Epoch 481/1000
2023-10-25 01:02:22.595 
Epoch 481/1000 
	 loss: 27.3932, MinusLogProbMetric: 27.3932, val_loss: 28.1946, val_MinusLogProbMetric: 28.1946

Epoch 481: val_loss did not improve from 28.13610
196/196 - 31s - loss: 27.3932 - MinusLogProbMetric: 27.3932 - val_loss: 28.1946 - val_MinusLogProbMetric: 28.1946 - lr: 1.2500e-04 - 31s/epoch - 160ms/step
Epoch 482/1000
2023-10-25 01:02:55.656 
Epoch 482/1000 
	 loss: 27.3902, MinusLogProbMetric: 27.3902, val_loss: 28.1921, val_MinusLogProbMetric: 28.1921

Epoch 482: val_loss did not improve from 28.13610
196/196 - 33s - loss: 27.3902 - MinusLogProbMetric: 27.3902 - val_loss: 28.1921 - val_MinusLogProbMetric: 28.1921 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 483/1000
2023-10-25 01:03:28.613 
Epoch 483/1000 
	 loss: 27.3984, MinusLogProbMetric: 27.3984, val_loss: 28.2309, val_MinusLogProbMetric: 28.2309

Epoch 483: val_loss did not improve from 28.13610
196/196 - 33s - loss: 27.3984 - MinusLogProbMetric: 27.3984 - val_loss: 28.2309 - val_MinusLogProbMetric: 28.2309 - lr: 1.2500e-04 - 33s/epoch - 168ms/step
Epoch 484/1000
2023-10-25 01:04:00.831 
Epoch 484/1000 
	 loss: 27.3923, MinusLogProbMetric: 27.3923, val_loss: 28.1546, val_MinusLogProbMetric: 28.1546

Epoch 484: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3923 - MinusLogProbMetric: 27.3923 - val_loss: 28.1546 - val_MinusLogProbMetric: 28.1546 - lr: 1.2500e-04 - 32s/epoch - 164ms/step
Epoch 485/1000
2023-10-25 01:04:32.812 
Epoch 485/1000 
	 loss: 27.3901, MinusLogProbMetric: 27.3901, val_loss: 28.1639, val_MinusLogProbMetric: 28.1639

Epoch 485: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3901 - MinusLogProbMetric: 27.3901 - val_loss: 28.1639 - val_MinusLogProbMetric: 28.1639 - lr: 1.2500e-04 - 32s/epoch - 163ms/step
Epoch 486/1000
2023-10-25 01:05:04.919 
Epoch 486/1000 
	 loss: 27.3984, MinusLogProbMetric: 27.3984, val_loss: 28.1611, val_MinusLogProbMetric: 28.1611

Epoch 486: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3984 - MinusLogProbMetric: 27.3984 - val_loss: 28.1611 - val_MinusLogProbMetric: 28.1611 - lr: 1.2500e-04 - 32s/epoch - 164ms/step
Epoch 487/1000
2023-10-25 01:05:35.685 
Epoch 487/1000 
	 loss: 27.3915, MinusLogProbMetric: 27.3915, val_loss: 28.1769, val_MinusLogProbMetric: 28.1769

Epoch 487: val_loss did not improve from 28.13610
196/196 - 31s - loss: 27.3915 - MinusLogProbMetric: 27.3915 - val_loss: 28.1769 - val_MinusLogProbMetric: 28.1769 - lr: 1.2500e-04 - 31s/epoch - 157ms/step
Epoch 488/1000
2023-10-25 01:06:07.452 
Epoch 488/1000 
	 loss: 27.3920, MinusLogProbMetric: 27.3920, val_loss: 28.1781, val_MinusLogProbMetric: 28.1781

Epoch 488: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3920 - MinusLogProbMetric: 27.3920 - val_loss: 28.1781 - val_MinusLogProbMetric: 28.1781 - lr: 1.2500e-04 - 32s/epoch - 162ms/step
Epoch 489/1000
2023-10-25 01:06:39.286 
Epoch 489/1000 
	 loss: 27.3896, MinusLogProbMetric: 27.3896, val_loss: 28.1575, val_MinusLogProbMetric: 28.1575

Epoch 489: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3896 - MinusLogProbMetric: 27.3896 - val_loss: 28.1575 - val_MinusLogProbMetric: 28.1575 - lr: 1.2500e-04 - 32s/epoch - 162ms/step
Epoch 490/1000
2023-10-25 01:07:11.740 
Epoch 490/1000 
	 loss: 27.3902, MinusLogProbMetric: 27.3902, val_loss: 28.2068, val_MinusLogProbMetric: 28.2068

Epoch 490: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3902 - MinusLogProbMetric: 27.3902 - val_loss: 28.2068 - val_MinusLogProbMetric: 28.2068 - lr: 1.2500e-04 - 32s/epoch - 166ms/step
Epoch 491/1000
2023-10-25 01:07:44.188 
Epoch 491/1000 
	 loss: 27.3914, MinusLogProbMetric: 27.3914, val_loss: 28.1605, val_MinusLogProbMetric: 28.1605

Epoch 491: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3914 - MinusLogProbMetric: 27.3914 - val_loss: 28.1605 - val_MinusLogProbMetric: 28.1605 - lr: 1.2500e-04 - 32s/epoch - 166ms/step
Epoch 492/1000
2023-10-25 01:08:16.749 
Epoch 492/1000 
	 loss: 27.3954, MinusLogProbMetric: 27.3954, val_loss: 28.2031, val_MinusLogProbMetric: 28.2031

Epoch 492: val_loss did not improve from 28.13610
196/196 - 33s - loss: 27.3954 - MinusLogProbMetric: 27.3954 - val_loss: 28.2031 - val_MinusLogProbMetric: 28.2031 - lr: 1.2500e-04 - 33s/epoch - 166ms/step
Epoch 493/1000
2023-10-25 01:08:48.715 
Epoch 493/1000 
	 loss: 27.3927, MinusLogProbMetric: 27.3927, val_loss: 28.1740, val_MinusLogProbMetric: 28.1740

Epoch 493: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3927 - MinusLogProbMetric: 27.3927 - val_loss: 28.1740 - val_MinusLogProbMetric: 28.1740 - lr: 1.2500e-04 - 32s/epoch - 163ms/step
Epoch 494/1000
2023-10-25 01:09:20.990 
Epoch 494/1000 
	 loss: 27.3964, MinusLogProbMetric: 27.3964, val_loss: 28.1744, val_MinusLogProbMetric: 28.1744

Epoch 494: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3964 - MinusLogProbMetric: 27.3964 - val_loss: 28.1744 - val_MinusLogProbMetric: 28.1744 - lr: 1.2500e-04 - 32s/epoch - 165ms/step
Epoch 495/1000
2023-10-25 01:09:53.375 
Epoch 495/1000 
	 loss: 27.3575, MinusLogProbMetric: 27.3575, val_loss: 28.1547, val_MinusLogProbMetric: 28.1547

Epoch 495: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3575 - MinusLogProbMetric: 27.3575 - val_loss: 28.1547 - val_MinusLogProbMetric: 28.1547 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 496/1000
2023-10-25 01:10:25.631 
Epoch 496/1000 
	 loss: 27.3549, MinusLogProbMetric: 27.3549, val_loss: 28.1576, val_MinusLogProbMetric: 28.1576

Epoch 496: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3549 - MinusLogProbMetric: 27.3549 - val_loss: 28.1576 - val_MinusLogProbMetric: 28.1576 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 497/1000
2023-10-25 01:10:58.291 
Epoch 497/1000 
	 loss: 27.3559, MinusLogProbMetric: 27.3559, val_loss: 28.1449, val_MinusLogProbMetric: 28.1449

Epoch 497: val_loss did not improve from 28.13610
196/196 - 33s - loss: 27.3559 - MinusLogProbMetric: 27.3559 - val_loss: 28.1449 - val_MinusLogProbMetric: 28.1449 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 498/1000
2023-10-25 01:11:31.154 
Epoch 498/1000 
	 loss: 27.3551, MinusLogProbMetric: 27.3551, val_loss: 28.1454, val_MinusLogProbMetric: 28.1454

Epoch 498: val_loss did not improve from 28.13610
196/196 - 33s - loss: 27.3551 - MinusLogProbMetric: 27.3551 - val_loss: 28.1454 - val_MinusLogProbMetric: 28.1454 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 499/1000
2023-10-25 01:12:02.941 
Epoch 499/1000 
	 loss: 27.3567, MinusLogProbMetric: 27.3567, val_loss: 28.1382, val_MinusLogProbMetric: 28.1382

Epoch 499: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3567 - MinusLogProbMetric: 27.3567 - val_loss: 28.1382 - val_MinusLogProbMetric: 28.1382 - lr: 6.2500e-05 - 32s/epoch - 162ms/step
Epoch 500/1000
2023-10-25 01:12:35.199 
Epoch 500/1000 
	 loss: 27.3562, MinusLogProbMetric: 27.3562, val_loss: 28.1434, val_MinusLogProbMetric: 28.1434

Epoch 500: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3562 - MinusLogProbMetric: 27.3562 - val_loss: 28.1434 - val_MinusLogProbMetric: 28.1434 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 501/1000
2023-10-25 01:13:07.745 
Epoch 501/1000 
	 loss: 27.3531, MinusLogProbMetric: 27.3531, val_loss: 28.1550, val_MinusLogProbMetric: 28.1550

Epoch 501: val_loss did not improve from 28.13610
196/196 - 33s - loss: 27.3531 - MinusLogProbMetric: 27.3531 - val_loss: 28.1550 - val_MinusLogProbMetric: 28.1550 - lr: 6.2500e-05 - 33s/epoch - 166ms/step
Epoch 502/1000
2023-10-25 01:13:40.489 
Epoch 502/1000 
	 loss: 27.3554, MinusLogProbMetric: 27.3554, val_loss: 28.1616, val_MinusLogProbMetric: 28.1616

Epoch 502: val_loss did not improve from 28.13610
196/196 - 33s - loss: 27.3554 - MinusLogProbMetric: 27.3554 - val_loss: 28.1616 - val_MinusLogProbMetric: 28.1616 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 503/1000
2023-10-25 01:14:10.776 
Epoch 503/1000 
	 loss: 27.3565, MinusLogProbMetric: 27.3565, val_loss: 28.1604, val_MinusLogProbMetric: 28.1604

Epoch 503: val_loss did not improve from 28.13610
196/196 - 30s - loss: 27.3565 - MinusLogProbMetric: 27.3565 - val_loss: 28.1604 - val_MinusLogProbMetric: 28.1604 - lr: 6.2500e-05 - 30s/epoch - 155ms/step
Epoch 504/1000
2023-10-25 01:14:39.064 
Epoch 504/1000 
	 loss: 27.3529, MinusLogProbMetric: 27.3529, val_loss: 28.1511, val_MinusLogProbMetric: 28.1511

Epoch 504: val_loss did not improve from 28.13610
196/196 - 28s - loss: 27.3529 - MinusLogProbMetric: 27.3529 - val_loss: 28.1511 - val_MinusLogProbMetric: 28.1511 - lr: 6.2500e-05 - 28s/epoch - 144ms/step
Epoch 505/1000
2023-10-25 01:15:09.682 
Epoch 505/1000 
	 loss: 27.3543, MinusLogProbMetric: 27.3543, val_loss: 28.1538, val_MinusLogProbMetric: 28.1538

Epoch 505: val_loss did not improve from 28.13610
196/196 - 31s - loss: 27.3543 - MinusLogProbMetric: 27.3543 - val_loss: 28.1538 - val_MinusLogProbMetric: 28.1538 - lr: 6.2500e-05 - 31s/epoch - 156ms/step
Epoch 506/1000
2023-10-25 01:15:42.222 
Epoch 506/1000 
	 loss: 27.3554, MinusLogProbMetric: 27.3554, val_loss: 28.1714, val_MinusLogProbMetric: 28.1714

Epoch 506: val_loss did not improve from 28.13610
196/196 - 33s - loss: 27.3554 - MinusLogProbMetric: 27.3554 - val_loss: 28.1714 - val_MinusLogProbMetric: 28.1714 - lr: 6.2500e-05 - 33s/epoch - 166ms/step
Epoch 507/1000
2023-10-25 01:16:13.804 
Epoch 507/1000 
	 loss: 27.3536, MinusLogProbMetric: 27.3536, val_loss: 28.1529, val_MinusLogProbMetric: 28.1529

Epoch 507: val_loss did not improve from 28.13610
196/196 - 32s - loss: 27.3536 - MinusLogProbMetric: 27.3536 - val_loss: 28.1529 - val_MinusLogProbMetric: 28.1529 - lr: 6.2500e-05 - 32s/epoch - 161ms/step
Epoch 508/1000
2023-10-25 01:16:43.647 
Epoch 508/1000 
	 loss: 27.3535, MinusLogProbMetric: 27.3535, val_loss: 28.1421, val_MinusLogProbMetric: 28.1421

Epoch 508: val_loss did not improve from 28.13610
196/196 - 30s - loss: 27.3535 - MinusLogProbMetric: 27.3535 - val_loss: 28.1421 - val_MinusLogProbMetric: 28.1421 - lr: 6.2500e-05 - 30s/epoch - 152ms/step
Epoch 509/1000
2023-10-25 01:17:11.560 
Epoch 509/1000 
	 loss: 27.3537, MinusLogProbMetric: 27.3537, val_loss: 28.1333, val_MinusLogProbMetric: 28.1333

Epoch 509: val_loss improved from 28.13610 to 28.13330, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 28s - loss: 27.3537 - MinusLogProbMetric: 27.3537 - val_loss: 28.1333 - val_MinusLogProbMetric: 28.1333 - lr: 6.2500e-05 - 28s/epoch - 145ms/step
Epoch 510/1000
2023-10-25 01:17:43.238 
Epoch 510/1000 
	 loss: 27.3524, MinusLogProbMetric: 27.3524, val_loss: 28.1509, val_MinusLogProbMetric: 28.1509

Epoch 510: val_loss did not improve from 28.13330
196/196 - 31s - loss: 27.3524 - MinusLogProbMetric: 27.3524 - val_loss: 28.1509 - val_MinusLogProbMetric: 28.1509 - lr: 6.2500e-05 - 31s/epoch - 159ms/step
Epoch 511/1000
2023-10-25 01:18:15.291 
Epoch 511/1000 
	 loss: 27.3504, MinusLogProbMetric: 27.3504, val_loss: 28.1495, val_MinusLogProbMetric: 28.1495

Epoch 511: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3504 - MinusLogProbMetric: 27.3504 - val_loss: 28.1495 - val_MinusLogProbMetric: 28.1495 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 512/1000
2023-10-25 01:18:46.196 
Epoch 512/1000 
	 loss: 27.3552, MinusLogProbMetric: 27.3552, val_loss: 28.1454, val_MinusLogProbMetric: 28.1454

Epoch 512: val_loss did not improve from 28.13330
196/196 - 31s - loss: 27.3552 - MinusLogProbMetric: 27.3552 - val_loss: 28.1454 - val_MinusLogProbMetric: 28.1454 - lr: 6.2500e-05 - 31s/epoch - 158ms/step
Epoch 513/1000
2023-10-25 01:19:18.572 
Epoch 513/1000 
	 loss: 27.3528, MinusLogProbMetric: 27.3528, val_loss: 28.1634, val_MinusLogProbMetric: 28.1634

Epoch 513: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3528 - MinusLogProbMetric: 27.3528 - val_loss: 28.1634 - val_MinusLogProbMetric: 28.1634 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 514/1000
2023-10-25 01:19:49.617 
Epoch 514/1000 
	 loss: 27.3545, MinusLogProbMetric: 27.3545, val_loss: 28.1536, val_MinusLogProbMetric: 28.1536

Epoch 514: val_loss did not improve from 28.13330
196/196 - 31s - loss: 27.3545 - MinusLogProbMetric: 27.3545 - val_loss: 28.1536 - val_MinusLogProbMetric: 28.1536 - lr: 6.2500e-05 - 31s/epoch - 158ms/step
Epoch 515/1000
2023-10-25 01:20:21.321 
Epoch 515/1000 
	 loss: 27.3539, MinusLogProbMetric: 27.3539, val_loss: 28.1453, val_MinusLogProbMetric: 28.1453

Epoch 515: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3539 - MinusLogProbMetric: 27.3539 - val_loss: 28.1453 - val_MinusLogProbMetric: 28.1453 - lr: 6.2500e-05 - 32s/epoch - 162ms/step
Epoch 516/1000
2023-10-25 01:20:51.213 
Epoch 516/1000 
	 loss: 27.3510, MinusLogProbMetric: 27.3510, val_loss: 28.1454, val_MinusLogProbMetric: 28.1454

Epoch 516: val_loss did not improve from 28.13330
196/196 - 30s - loss: 27.3510 - MinusLogProbMetric: 27.3510 - val_loss: 28.1454 - val_MinusLogProbMetric: 28.1454 - lr: 6.2500e-05 - 30s/epoch - 152ms/step
Epoch 517/1000
2023-10-25 01:21:23.380 
Epoch 517/1000 
	 loss: 27.3514, MinusLogProbMetric: 27.3514, val_loss: 28.1513, val_MinusLogProbMetric: 28.1513

Epoch 517: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3514 - MinusLogProbMetric: 27.3514 - val_loss: 28.1513 - val_MinusLogProbMetric: 28.1513 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 518/1000
2023-10-25 01:21:56.166 
Epoch 518/1000 
	 loss: 27.3541, MinusLogProbMetric: 27.3541, val_loss: 28.1643, val_MinusLogProbMetric: 28.1643

Epoch 518: val_loss did not improve from 28.13330
196/196 - 33s - loss: 27.3541 - MinusLogProbMetric: 27.3541 - val_loss: 28.1643 - val_MinusLogProbMetric: 28.1643 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 519/1000
2023-10-25 01:22:27.168 
Epoch 519/1000 
	 loss: 27.3566, MinusLogProbMetric: 27.3566, val_loss: 28.1621, val_MinusLogProbMetric: 28.1621

Epoch 519: val_loss did not improve from 28.13330
196/196 - 31s - loss: 27.3566 - MinusLogProbMetric: 27.3566 - val_loss: 28.1621 - val_MinusLogProbMetric: 28.1621 - lr: 6.2500e-05 - 31s/epoch - 158ms/step
Epoch 520/1000
2023-10-25 01:22:59.780 
Epoch 520/1000 
	 loss: 27.3563, MinusLogProbMetric: 27.3563, val_loss: 28.1755, val_MinusLogProbMetric: 28.1755

Epoch 520: val_loss did not improve from 28.13330
196/196 - 33s - loss: 27.3563 - MinusLogProbMetric: 27.3563 - val_loss: 28.1755 - val_MinusLogProbMetric: 28.1755 - lr: 6.2500e-05 - 33s/epoch - 166ms/step
Epoch 521/1000
2023-10-25 01:23:30.725 
Epoch 521/1000 
	 loss: 27.3513, MinusLogProbMetric: 27.3513, val_loss: 28.1438, val_MinusLogProbMetric: 28.1438

Epoch 521: val_loss did not improve from 28.13330
196/196 - 31s - loss: 27.3513 - MinusLogProbMetric: 27.3513 - val_loss: 28.1438 - val_MinusLogProbMetric: 28.1438 - lr: 6.2500e-05 - 31s/epoch - 158ms/step
Epoch 522/1000
2023-10-25 01:24:00.369 
Epoch 522/1000 
	 loss: 27.3535, MinusLogProbMetric: 27.3535, val_loss: 28.1445, val_MinusLogProbMetric: 28.1445

Epoch 522: val_loss did not improve from 28.13330
196/196 - 30s - loss: 27.3535 - MinusLogProbMetric: 27.3535 - val_loss: 28.1445 - val_MinusLogProbMetric: 28.1445 - lr: 6.2500e-05 - 30s/epoch - 151ms/step
Epoch 523/1000
2023-10-25 01:24:30.768 
Epoch 523/1000 
	 loss: 27.3500, MinusLogProbMetric: 27.3500, val_loss: 28.1396, val_MinusLogProbMetric: 28.1396

Epoch 523: val_loss did not improve from 28.13330
196/196 - 30s - loss: 27.3500 - MinusLogProbMetric: 27.3500 - val_loss: 28.1396 - val_MinusLogProbMetric: 28.1396 - lr: 6.2500e-05 - 30s/epoch - 155ms/step
Epoch 524/1000
2023-10-25 01:25:01.392 
Epoch 524/1000 
	 loss: 27.3498, MinusLogProbMetric: 27.3498, val_loss: 28.1459, val_MinusLogProbMetric: 28.1459

Epoch 524: val_loss did not improve from 28.13330
196/196 - 31s - loss: 27.3498 - MinusLogProbMetric: 27.3498 - val_loss: 28.1459 - val_MinusLogProbMetric: 28.1459 - lr: 6.2500e-05 - 31s/epoch - 156ms/step
Epoch 525/1000
2023-10-25 01:25:32.471 
Epoch 525/1000 
	 loss: 27.3510, MinusLogProbMetric: 27.3510, val_loss: 28.1450, val_MinusLogProbMetric: 28.1450

Epoch 525: val_loss did not improve from 28.13330
196/196 - 31s - loss: 27.3510 - MinusLogProbMetric: 27.3510 - val_loss: 28.1450 - val_MinusLogProbMetric: 28.1450 - lr: 6.2500e-05 - 31s/epoch - 159ms/step
Epoch 526/1000
2023-10-25 01:26:04.755 
Epoch 526/1000 
	 loss: 27.3483, MinusLogProbMetric: 27.3483, val_loss: 28.1469, val_MinusLogProbMetric: 28.1469

Epoch 526: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3483 - MinusLogProbMetric: 27.3483 - val_loss: 28.1469 - val_MinusLogProbMetric: 28.1469 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 527/1000
2023-10-25 01:26:36.321 
Epoch 527/1000 
	 loss: 27.3522, MinusLogProbMetric: 27.3522, val_loss: 28.1399, val_MinusLogProbMetric: 28.1399

Epoch 527: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3522 - MinusLogProbMetric: 27.3522 - val_loss: 28.1399 - val_MinusLogProbMetric: 28.1399 - lr: 6.2500e-05 - 32s/epoch - 161ms/step
Epoch 528/1000
2023-10-25 01:27:06.343 
Epoch 528/1000 
	 loss: 27.3533, MinusLogProbMetric: 27.3533, val_loss: 28.1437, val_MinusLogProbMetric: 28.1437

Epoch 528: val_loss did not improve from 28.13330
196/196 - 30s - loss: 27.3533 - MinusLogProbMetric: 27.3533 - val_loss: 28.1437 - val_MinusLogProbMetric: 28.1437 - lr: 6.2500e-05 - 30s/epoch - 153ms/step
Epoch 529/1000
2023-10-25 01:27:36.749 
Epoch 529/1000 
	 loss: 27.3545, MinusLogProbMetric: 27.3545, val_loss: 28.1515, val_MinusLogProbMetric: 28.1515

Epoch 529: val_loss did not improve from 28.13330
196/196 - 30s - loss: 27.3545 - MinusLogProbMetric: 27.3545 - val_loss: 28.1515 - val_MinusLogProbMetric: 28.1515 - lr: 6.2500e-05 - 30s/epoch - 155ms/step
Epoch 530/1000
2023-10-25 01:28:08.189 
Epoch 530/1000 
	 loss: 27.3495, MinusLogProbMetric: 27.3495, val_loss: 28.1483, val_MinusLogProbMetric: 28.1483

Epoch 530: val_loss did not improve from 28.13330
196/196 - 31s - loss: 27.3495 - MinusLogProbMetric: 27.3495 - val_loss: 28.1483 - val_MinusLogProbMetric: 28.1483 - lr: 6.2500e-05 - 31s/epoch - 160ms/step
Epoch 531/1000
2023-10-25 01:28:40.634 
Epoch 531/1000 
	 loss: 27.3515, MinusLogProbMetric: 27.3515, val_loss: 28.1450, val_MinusLogProbMetric: 28.1450

Epoch 531: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3515 - MinusLogProbMetric: 27.3515 - val_loss: 28.1450 - val_MinusLogProbMetric: 28.1450 - lr: 6.2500e-05 - 32s/epoch - 166ms/step
Epoch 532/1000
2023-10-25 01:29:11.521 
Epoch 532/1000 
	 loss: 27.3530, MinusLogProbMetric: 27.3530, val_loss: 28.1387, val_MinusLogProbMetric: 28.1387

Epoch 532: val_loss did not improve from 28.13330
196/196 - 31s - loss: 27.3530 - MinusLogProbMetric: 27.3530 - val_loss: 28.1387 - val_MinusLogProbMetric: 28.1387 - lr: 6.2500e-05 - 31s/epoch - 158ms/step
Epoch 533/1000
2023-10-25 01:29:41.990 
Epoch 533/1000 
	 loss: 27.3512, MinusLogProbMetric: 27.3512, val_loss: 28.1400, val_MinusLogProbMetric: 28.1400

Epoch 533: val_loss did not improve from 28.13330
196/196 - 30s - loss: 27.3512 - MinusLogProbMetric: 27.3512 - val_loss: 28.1400 - val_MinusLogProbMetric: 28.1400 - lr: 6.2500e-05 - 30s/epoch - 155ms/step
Epoch 534/1000
2023-10-25 01:30:13.356 
Epoch 534/1000 
	 loss: 27.3510, MinusLogProbMetric: 27.3510, val_loss: 28.1464, val_MinusLogProbMetric: 28.1464

Epoch 534: val_loss did not improve from 28.13330
196/196 - 31s - loss: 27.3510 - MinusLogProbMetric: 27.3510 - val_loss: 28.1464 - val_MinusLogProbMetric: 28.1464 - lr: 6.2500e-05 - 31s/epoch - 160ms/step
Epoch 535/1000
2023-10-25 01:30:45.738 
Epoch 535/1000 
	 loss: 27.3503, MinusLogProbMetric: 27.3503, val_loss: 28.1484, val_MinusLogProbMetric: 28.1484

Epoch 535: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3503 - MinusLogProbMetric: 27.3503 - val_loss: 28.1484 - val_MinusLogProbMetric: 28.1484 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 536/1000
2023-10-25 01:31:17.738 
Epoch 536/1000 
	 loss: 27.3519, MinusLogProbMetric: 27.3519, val_loss: 28.1621, val_MinusLogProbMetric: 28.1621

Epoch 536: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3519 - MinusLogProbMetric: 27.3519 - val_loss: 28.1621 - val_MinusLogProbMetric: 28.1621 - lr: 6.2500e-05 - 32s/epoch - 163ms/step
Epoch 537/1000
2023-10-25 01:31:50.141 
Epoch 537/1000 
	 loss: 27.3524, MinusLogProbMetric: 27.3524, val_loss: 28.1376, val_MinusLogProbMetric: 28.1376

Epoch 537: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3524 - MinusLogProbMetric: 27.3524 - val_loss: 28.1376 - val_MinusLogProbMetric: 28.1376 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 538/1000
2023-10-25 01:32:21.730 
Epoch 538/1000 
	 loss: 27.3519, MinusLogProbMetric: 27.3519, val_loss: 28.1451, val_MinusLogProbMetric: 28.1451

Epoch 538: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3519 - MinusLogProbMetric: 27.3519 - val_loss: 28.1451 - val_MinusLogProbMetric: 28.1451 - lr: 6.2500e-05 - 32s/epoch - 161ms/step
Epoch 539/1000
2023-10-25 01:32:53.833 
Epoch 539/1000 
	 loss: 27.3483, MinusLogProbMetric: 27.3483, val_loss: 28.1500, val_MinusLogProbMetric: 28.1500

Epoch 539: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3483 - MinusLogProbMetric: 27.3483 - val_loss: 28.1500 - val_MinusLogProbMetric: 28.1500 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 540/1000
2023-10-25 01:33:23.828 
Epoch 540/1000 
	 loss: 27.3510, MinusLogProbMetric: 27.3510, val_loss: 28.1455, val_MinusLogProbMetric: 28.1455

Epoch 540: val_loss did not improve from 28.13330
196/196 - 30s - loss: 27.3510 - MinusLogProbMetric: 27.3510 - val_loss: 28.1455 - val_MinusLogProbMetric: 28.1455 - lr: 6.2500e-05 - 30s/epoch - 153ms/step
Epoch 541/1000
2023-10-25 01:33:54.272 
Epoch 541/1000 
	 loss: 27.3497, MinusLogProbMetric: 27.3497, val_loss: 28.1397, val_MinusLogProbMetric: 28.1397

Epoch 541: val_loss did not improve from 28.13330
196/196 - 30s - loss: 27.3497 - MinusLogProbMetric: 27.3497 - val_loss: 28.1397 - val_MinusLogProbMetric: 28.1397 - lr: 6.2500e-05 - 30s/epoch - 155ms/step
Epoch 542/1000
2023-10-25 01:34:26.483 
Epoch 542/1000 
	 loss: 27.3537, MinusLogProbMetric: 27.3537, val_loss: 28.1741, val_MinusLogProbMetric: 28.1741

Epoch 542: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3537 - MinusLogProbMetric: 27.3537 - val_loss: 28.1741 - val_MinusLogProbMetric: 28.1741 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 543/1000
2023-10-25 01:34:58.599 
Epoch 543/1000 
	 loss: 27.3519, MinusLogProbMetric: 27.3519, val_loss: 28.1445, val_MinusLogProbMetric: 28.1445

Epoch 543: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3519 - MinusLogProbMetric: 27.3519 - val_loss: 28.1445 - val_MinusLogProbMetric: 28.1445 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 544/1000
2023-10-25 01:35:30.525 
Epoch 544/1000 
	 loss: 27.3510, MinusLogProbMetric: 27.3510, val_loss: 28.1411, val_MinusLogProbMetric: 28.1411

Epoch 544: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3510 - MinusLogProbMetric: 27.3510 - val_loss: 28.1411 - val_MinusLogProbMetric: 28.1411 - lr: 6.2500e-05 - 32s/epoch - 163ms/step
Epoch 545/1000
2023-10-25 01:36:03.045 
Epoch 545/1000 
	 loss: 27.3493, MinusLogProbMetric: 27.3493, val_loss: 28.1395, val_MinusLogProbMetric: 28.1395

Epoch 545: val_loss did not improve from 28.13330
196/196 - 33s - loss: 27.3493 - MinusLogProbMetric: 27.3493 - val_loss: 28.1395 - val_MinusLogProbMetric: 28.1395 - lr: 6.2500e-05 - 33s/epoch - 166ms/step
Epoch 546/1000
2023-10-25 01:36:35.668 
Epoch 546/1000 
	 loss: 27.3515, MinusLogProbMetric: 27.3515, val_loss: 28.1490, val_MinusLogProbMetric: 28.1490

Epoch 546: val_loss did not improve from 28.13330
196/196 - 33s - loss: 27.3515 - MinusLogProbMetric: 27.3515 - val_loss: 28.1490 - val_MinusLogProbMetric: 28.1490 - lr: 6.2500e-05 - 33s/epoch - 166ms/step
Epoch 547/1000
2023-10-25 01:37:07.881 
Epoch 547/1000 
	 loss: 27.3510, MinusLogProbMetric: 27.3510, val_loss: 28.1575, val_MinusLogProbMetric: 28.1575

Epoch 547: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3510 - MinusLogProbMetric: 27.3510 - val_loss: 28.1575 - val_MinusLogProbMetric: 28.1575 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 548/1000
2023-10-25 01:37:40.245 
Epoch 548/1000 
	 loss: 27.3528, MinusLogProbMetric: 27.3528, val_loss: 28.1611, val_MinusLogProbMetric: 28.1611

Epoch 548: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3528 - MinusLogProbMetric: 27.3528 - val_loss: 28.1611 - val_MinusLogProbMetric: 28.1611 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 549/1000
2023-10-25 01:38:11.266 
Epoch 549/1000 
	 loss: 27.3485, MinusLogProbMetric: 27.3485, val_loss: 28.1567, val_MinusLogProbMetric: 28.1567

Epoch 549: val_loss did not improve from 28.13330
196/196 - 31s - loss: 27.3485 - MinusLogProbMetric: 27.3485 - val_loss: 28.1567 - val_MinusLogProbMetric: 28.1567 - lr: 6.2500e-05 - 31s/epoch - 158ms/step
Epoch 550/1000
2023-10-25 01:38:41.549 
Epoch 550/1000 
	 loss: 27.3471, MinusLogProbMetric: 27.3471, val_loss: 28.1558, val_MinusLogProbMetric: 28.1558

Epoch 550: val_loss did not improve from 28.13330
196/196 - 30s - loss: 27.3471 - MinusLogProbMetric: 27.3471 - val_loss: 28.1558 - val_MinusLogProbMetric: 28.1558 - lr: 6.2500e-05 - 30s/epoch - 154ms/step
Epoch 551/1000
2023-10-25 01:39:13.273 
Epoch 551/1000 
	 loss: 27.3485, MinusLogProbMetric: 27.3485, val_loss: 28.1355, val_MinusLogProbMetric: 28.1355

Epoch 551: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3485 - MinusLogProbMetric: 27.3485 - val_loss: 28.1355 - val_MinusLogProbMetric: 28.1355 - lr: 6.2500e-05 - 32s/epoch - 162ms/step
Epoch 552/1000
2023-10-25 01:39:44.376 
Epoch 552/1000 
	 loss: 27.3476, MinusLogProbMetric: 27.3476, val_loss: 28.1542, val_MinusLogProbMetric: 28.1542

Epoch 552: val_loss did not improve from 28.13330
196/196 - 31s - loss: 27.3476 - MinusLogProbMetric: 27.3476 - val_loss: 28.1542 - val_MinusLogProbMetric: 28.1542 - lr: 6.2500e-05 - 31s/epoch - 159ms/step
Epoch 553/1000
2023-10-25 01:40:16.493 
Epoch 553/1000 
	 loss: 27.3499, MinusLogProbMetric: 27.3499, val_loss: 28.1411, val_MinusLogProbMetric: 28.1411

Epoch 553: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3499 - MinusLogProbMetric: 27.3499 - val_loss: 28.1411 - val_MinusLogProbMetric: 28.1411 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 554/1000
2023-10-25 01:40:48.724 
Epoch 554/1000 
	 loss: 27.3486, MinusLogProbMetric: 27.3486, val_loss: 28.1665, val_MinusLogProbMetric: 28.1665

Epoch 554: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3486 - MinusLogProbMetric: 27.3486 - val_loss: 28.1665 - val_MinusLogProbMetric: 28.1665 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 555/1000
2023-10-25 01:41:18.396 
Epoch 555/1000 
	 loss: 27.3488, MinusLogProbMetric: 27.3488, val_loss: 28.1531, val_MinusLogProbMetric: 28.1531

Epoch 555: val_loss did not improve from 28.13330
196/196 - 30s - loss: 27.3488 - MinusLogProbMetric: 27.3488 - val_loss: 28.1531 - val_MinusLogProbMetric: 28.1531 - lr: 6.2500e-05 - 30s/epoch - 151ms/step
Epoch 556/1000
2023-10-25 01:41:49.757 
Epoch 556/1000 
	 loss: 27.3466, MinusLogProbMetric: 27.3466, val_loss: 28.1390, val_MinusLogProbMetric: 28.1390

Epoch 556: val_loss did not improve from 28.13330
196/196 - 31s - loss: 27.3466 - MinusLogProbMetric: 27.3466 - val_loss: 28.1390 - val_MinusLogProbMetric: 28.1390 - lr: 6.2500e-05 - 31s/epoch - 160ms/step
Epoch 557/1000
2023-10-25 01:42:20.163 
Epoch 557/1000 
	 loss: 27.3446, MinusLogProbMetric: 27.3446, val_loss: 28.1455, val_MinusLogProbMetric: 28.1455

Epoch 557: val_loss did not improve from 28.13330
196/196 - 30s - loss: 27.3446 - MinusLogProbMetric: 27.3446 - val_loss: 28.1455 - val_MinusLogProbMetric: 28.1455 - lr: 6.2500e-05 - 30s/epoch - 155ms/step
Epoch 558/1000
2023-10-25 01:42:52.564 
Epoch 558/1000 
	 loss: 27.3477, MinusLogProbMetric: 27.3477, val_loss: 28.1416, val_MinusLogProbMetric: 28.1416

Epoch 558: val_loss did not improve from 28.13330
196/196 - 32s - loss: 27.3477 - MinusLogProbMetric: 27.3477 - val_loss: 28.1416 - val_MinusLogProbMetric: 28.1416 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 559/1000
2023-10-25 01:43:25.244 
Epoch 559/1000 
	 loss: 27.3471, MinusLogProbMetric: 27.3471, val_loss: 28.1420, val_MinusLogProbMetric: 28.1420

Epoch 559: val_loss did not improve from 28.13330
196/196 - 33s - loss: 27.3471 - MinusLogProbMetric: 27.3471 - val_loss: 28.1420 - val_MinusLogProbMetric: 28.1420 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 560/1000
2023-10-25 01:43:56.343 
Epoch 560/1000 
	 loss: 27.3301, MinusLogProbMetric: 27.3301, val_loss: 28.1318, val_MinusLogProbMetric: 28.1318

Epoch 560: val_loss improved from 28.13330 to 28.13184, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 32s - loss: 27.3301 - MinusLogProbMetric: 27.3301 - val_loss: 28.1318 - val_MinusLogProbMetric: 28.1318 - lr: 3.1250e-05 - 32s/epoch - 162ms/step
Epoch 561/1000
2023-10-25 01:44:28.833 
Epoch 561/1000 
	 loss: 27.3301, MinusLogProbMetric: 27.3301, val_loss: 28.1386, val_MinusLogProbMetric: 28.1386

Epoch 561: val_loss did not improve from 28.13184
196/196 - 32s - loss: 27.3301 - MinusLogProbMetric: 27.3301 - val_loss: 28.1386 - val_MinusLogProbMetric: 28.1386 - lr: 3.1250e-05 - 32s/epoch - 163ms/step
Epoch 562/1000
2023-10-25 01:44:59.715 
Epoch 562/1000 
	 loss: 27.3309, MinusLogProbMetric: 27.3309, val_loss: 28.1377, val_MinusLogProbMetric: 28.1377

Epoch 562: val_loss did not improve from 28.13184
196/196 - 31s - loss: 27.3309 - MinusLogProbMetric: 27.3309 - val_loss: 28.1377 - val_MinusLogProbMetric: 28.1377 - lr: 3.1250e-05 - 31s/epoch - 158ms/step
Epoch 563/1000
2023-10-25 01:45:30.606 
Epoch 563/1000 
	 loss: 27.3323, MinusLogProbMetric: 27.3323, val_loss: 28.1382, val_MinusLogProbMetric: 28.1382

Epoch 563: val_loss did not improve from 28.13184
196/196 - 31s - loss: 27.3323 - MinusLogProbMetric: 27.3323 - val_loss: 28.1382 - val_MinusLogProbMetric: 28.1382 - lr: 3.1250e-05 - 31s/epoch - 158ms/step
Epoch 564/1000
2023-10-25 01:46:01.220 
Epoch 564/1000 
	 loss: 27.3300, MinusLogProbMetric: 27.3300, val_loss: 28.1365, val_MinusLogProbMetric: 28.1365

Epoch 564: val_loss did not improve from 28.13184
196/196 - 31s - loss: 27.3300 - MinusLogProbMetric: 27.3300 - val_loss: 28.1365 - val_MinusLogProbMetric: 28.1365 - lr: 3.1250e-05 - 31s/epoch - 156ms/step
Epoch 565/1000
2023-10-25 01:46:33.350 
Epoch 565/1000 
	 loss: 27.3300, MinusLogProbMetric: 27.3300, val_loss: 28.1393, val_MinusLogProbMetric: 28.1393

Epoch 565: val_loss did not improve from 28.13184
196/196 - 32s - loss: 27.3300 - MinusLogProbMetric: 27.3300 - val_loss: 28.1393 - val_MinusLogProbMetric: 28.1393 - lr: 3.1250e-05 - 32s/epoch - 164ms/step
Epoch 566/1000
2023-10-25 01:47:04.061 
Epoch 566/1000 
	 loss: 27.3294, MinusLogProbMetric: 27.3294, val_loss: 28.1373, val_MinusLogProbMetric: 28.1373

Epoch 566: val_loss did not improve from 28.13184
196/196 - 31s - loss: 27.3294 - MinusLogProbMetric: 27.3294 - val_loss: 28.1373 - val_MinusLogProbMetric: 28.1373 - lr: 3.1250e-05 - 31s/epoch - 157ms/step
Epoch 567/1000
2023-10-25 01:47:36.502 
Epoch 567/1000 
	 loss: 27.3297, MinusLogProbMetric: 27.3297, val_loss: 28.1274, val_MinusLogProbMetric: 28.1274

Epoch 567: val_loss improved from 28.13184 to 28.12737, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 27.3297 - MinusLogProbMetric: 27.3297 - val_loss: 28.1274 - val_MinusLogProbMetric: 28.1274 - lr: 3.1250e-05 - 33s/epoch - 168ms/step
Epoch 568/1000
2023-10-25 01:48:08.587 
Epoch 568/1000 
	 loss: 27.3283, MinusLogProbMetric: 27.3283, val_loss: 28.1514, val_MinusLogProbMetric: 28.1514

Epoch 568: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3283 - MinusLogProbMetric: 27.3283 - val_loss: 28.1514 - val_MinusLogProbMetric: 28.1514 - lr: 3.1250e-05 - 32s/epoch - 161ms/step
Epoch 569/1000
2023-10-25 01:48:39.838 
Epoch 569/1000 
	 loss: 27.3295, MinusLogProbMetric: 27.3295, val_loss: 28.1512, val_MinusLogProbMetric: 28.1512

Epoch 569: val_loss did not improve from 28.12737
196/196 - 31s - loss: 27.3295 - MinusLogProbMetric: 27.3295 - val_loss: 28.1512 - val_MinusLogProbMetric: 28.1512 - lr: 3.1250e-05 - 31s/epoch - 159ms/step
Epoch 570/1000
2023-10-25 01:49:09.820 
Epoch 570/1000 
	 loss: 27.3300, MinusLogProbMetric: 27.3300, val_loss: 28.1299, val_MinusLogProbMetric: 28.1299

Epoch 570: val_loss did not improve from 28.12737
196/196 - 30s - loss: 27.3300 - MinusLogProbMetric: 27.3300 - val_loss: 28.1299 - val_MinusLogProbMetric: 28.1299 - lr: 3.1250e-05 - 30s/epoch - 153ms/step
Epoch 571/1000
2023-10-25 01:49:41.765 
Epoch 571/1000 
	 loss: 27.3312, MinusLogProbMetric: 27.3312, val_loss: 28.1399, val_MinusLogProbMetric: 28.1399

Epoch 571: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3312 - MinusLogProbMetric: 27.3312 - val_loss: 28.1399 - val_MinusLogProbMetric: 28.1399 - lr: 3.1250e-05 - 32s/epoch - 163ms/step
Epoch 572/1000
2023-10-25 01:50:14.258 
Epoch 572/1000 
	 loss: 27.3289, MinusLogProbMetric: 27.3289, val_loss: 28.1414, val_MinusLogProbMetric: 28.1414

Epoch 572: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3289 - MinusLogProbMetric: 27.3289 - val_loss: 28.1414 - val_MinusLogProbMetric: 28.1414 - lr: 3.1250e-05 - 32s/epoch - 166ms/step
Epoch 573/1000
2023-10-25 01:50:45.856 
Epoch 573/1000 
	 loss: 27.3307, MinusLogProbMetric: 27.3307, val_loss: 28.1478, val_MinusLogProbMetric: 28.1478

Epoch 573: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3307 - MinusLogProbMetric: 27.3307 - val_loss: 28.1478 - val_MinusLogProbMetric: 28.1478 - lr: 3.1250e-05 - 32s/epoch - 161ms/step
Epoch 574/1000
2023-10-25 01:51:16.928 
Epoch 574/1000 
	 loss: 27.3303, MinusLogProbMetric: 27.3303, val_loss: 28.1328, val_MinusLogProbMetric: 28.1328

Epoch 574: val_loss did not improve from 28.12737
196/196 - 31s - loss: 27.3303 - MinusLogProbMetric: 27.3303 - val_loss: 28.1328 - val_MinusLogProbMetric: 28.1328 - lr: 3.1250e-05 - 31s/epoch - 159ms/step
Epoch 575/1000
2023-10-25 01:51:48.783 
Epoch 575/1000 
	 loss: 27.3313, MinusLogProbMetric: 27.3313, val_loss: 28.1379, val_MinusLogProbMetric: 28.1379

Epoch 575: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3313 - MinusLogProbMetric: 27.3313 - val_loss: 28.1379 - val_MinusLogProbMetric: 28.1379 - lr: 3.1250e-05 - 32s/epoch - 163ms/step
Epoch 576/1000
2023-10-25 01:52:20.831 
Epoch 576/1000 
	 loss: 27.3300, MinusLogProbMetric: 27.3300, val_loss: 28.1323, val_MinusLogProbMetric: 28.1323

Epoch 576: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3300 - MinusLogProbMetric: 27.3300 - val_loss: 28.1323 - val_MinusLogProbMetric: 28.1323 - lr: 3.1250e-05 - 32s/epoch - 163ms/step
Epoch 577/1000
2023-10-25 01:52:53.120 
Epoch 577/1000 
	 loss: 27.3314, MinusLogProbMetric: 27.3314, val_loss: 28.1379, val_MinusLogProbMetric: 28.1379

Epoch 577: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3314 - MinusLogProbMetric: 27.3314 - val_loss: 28.1379 - val_MinusLogProbMetric: 28.1379 - lr: 3.1250e-05 - 32s/epoch - 165ms/step
Epoch 578/1000
2023-10-25 01:53:25.680 
Epoch 578/1000 
	 loss: 27.3293, MinusLogProbMetric: 27.3293, val_loss: 28.1514, val_MinusLogProbMetric: 28.1514

Epoch 578: val_loss did not improve from 28.12737
196/196 - 33s - loss: 27.3293 - MinusLogProbMetric: 27.3293 - val_loss: 28.1514 - val_MinusLogProbMetric: 28.1514 - lr: 3.1250e-05 - 33s/epoch - 166ms/step
Epoch 579/1000
2023-10-25 01:53:57.153 
Epoch 579/1000 
	 loss: 27.3300, MinusLogProbMetric: 27.3300, val_loss: 28.1330, val_MinusLogProbMetric: 28.1330

Epoch 579: val_loss did not improve from 28.12737
196/196 - 31s - loss: 27.3300 - MinusLogProbMetric: 27.3300 - val_loss: 28.1330 - val_MinusLogProbMetric: 28.1330 - lr: 3.1250e-05 - 31s/epoch - 161ms/step
Epoch 580/1000
2023-10-25 01:54:26.332 
Epoch 580/1000 
	 loss: 27.3305, MinusLogProbMetric: 27.3305, val_loss: 28.1388, val_MinusLogProbMetric: 28.1388

Epoch 580: val_loss did not improve from 28.12737
196/196 - 29s - loss: 27.3305 - MinusLogProbMetric: 27.3305 - val_loss: 28.1388 - val_MinusLogProbMetric: 28.1388 - lr: 3.1250e-05 - 29s/epoch - 149ms/step
Epoch 581/1000
2023-10-25 01:54:55.907 
Epoch 581/1000 
	 loss: 27.3282, MinusLogProbMetric: 27.3282, val_loss: 28.1343, val_MinusLogProbMetric: 28.1343

Epoch 581: val_loss did not improve from 28.12737
196/196 - 30s - loss: 27.3282 - MinusLogProbMetric: 27.3282 - val_loss: 28.1343 - val_MinusLogProbMetric: 28.1343 - lr: 3.1250e-05 - 30s/epoch - 151ms/step
Epoch 582/1000
2023-10-25 01:55:28.721 
Epoch 582/1000 
	 loss: 27.3312, MinusLogProbMetric: 27.3312, val_loss: 28.1324, val_MinusLogProbMetric: 28.1324

Epoch 582: val_loss did not improve from 28.12737
196/196 - 33s - loss: 27.3312 - MinusLogProbMetric: 27.3312 - val_loss: 28.1324 - val_MinusLogProbMetric: 28.1324 - lr: 3.1250e-05 - 33s/epoch - 167ms/step
Epoch 583/1000
2023-10-25 01:56:00.438 
Epoch 583/1000 
	 loss: 27.3291, MinusLogProbMetric: 27.3291, val_loss: 28.1294, val_MinusLogProbMetric: 28.1294

Epoch 583: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3291 - MinusLogProbMetric: 27.3291 - val_loss: 28.1294 - val_MinusLogProbMetric: 28.1294 - lr: 3.1250e-05 - 32s/epoch - 162ms/step
Epoch 584/1000
2023-10-25 01:56:31.367 
Epoch 584/1000 
	 loss: 27.3282, MinusLogProbMetric: 27.3282, val_loss: 28.1368, val_MinusLogProbMetric: 28.1368

Epoch 584: val_loss did not improve from 28.12737
196/196 - 31s - loss: 27.3282 - MinusLogProbMetric: 27.3282 - val_loss: 28.1368 - val_MinusLogProbMetric: 28.1368 - lr: 3.1250e-05 - 31s/epoch - 158ms/step
Epoch 585/1000
2023-10-25 01:57:02.349 
Epoch 585/1000 
	 loss: 27.3299, MinusLogProbMetric: 27.3299, val_loss: 28.1447, val_MinusLogProbMetric: 28.1447

Epoch 585: val_loss did not improve from 28.12737
196/196 - 31s - loss: 27.3299 - MinusLogProbMetric: 27.3299 - val_loss: 28.1447 - val_MinusLogProbMetric: 28.1447 - lr: 3.1250e-05 - 31s/epoch - 158ms/step
Epoch 586/1000
2023-10-25 01:57:34.593 
Epoch 586/1000 
	 loss: 27.3277, MinusLogProbMetric: 27.3277, val_loss: 28.1368, val_MinusLogProbMetric: 28.1368

Epoch 586: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3277 - MinusLogProbMetric: 27.3277 - val_loss: 28.1368 - val_MinusLogProbMetric: 28.1368 - lr: 3.1250e-05 - 32s/epoch - 165ms/step
Epoch 587/1000
2023-10-25 01:58:04.713 
Epoch 587/1000 
	 loss: 27.3280, MinusLogProbMetric: 27.3280, val_loss: 28.1360, val_MinusLogProbMetric: 28.1360

Epoch 587: val_loss did not improve from 28.12737
196/196 - 30s - loss: 27.3280 - MinusLogProbMetric: 27.3280 - val_loss: 28.1360 - val_MinusLogProbMetric: 28.1360 - lr: 3.1250e-05 - 30s/epoch - 154ms/step
Epoch 588/1000
2023-10-25 01:58:35.963 
Epoch 588/1000 
	 loss: 27.3267, MinusLogProbMetric: 27.3267, val_loss: 28.1317, val_MinusLogProbMetric: 28.1317

Epoch 588: val_loss did not improve from 28.12737
196/196 - 31s - loss: 27.3267 - MinusLogProbMetric: 27.3267 - val_loss: 28.1317 - val_MinusLogProbMetric: 28.1317 - lr: 3.1250e-05 - 31s/epoch - 159ms/step
Epoch 589/1000
2023-10-25 01:59:06.692 
Epoch 589/1000 
	 loss: 27.3292, MinusLogProbMetric: 27.3292, val_loss: 28.1350, val_MinusLogProbMetric: 28.1350

Epoch 589: val_loss did not improve from 28.12737
196/196 - 31s - loss: 27.3292 - MinusLogProbMetric: 27.3292 - val_loss: 28.1350 - val_MinusLogProbMetric: 28.1350 - lr: 3.1250e-05 - 31s/epoch - 157ms/step
Epoch 590/1000
2023-10-25 01:59:38.230 
Epoch 590/1000 
	 loss: 27.3298, MinusLogProbMetric: 27.3298, val_loss: 28.1345, val_MinusLogProbMetric: 28.1345

Epoch 590: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3298 - MinusLogProbMetric: 27.3298 - val_loss: 28.1345 - val_MinusLogProbMetric: 28.1345 - lr: 3.1250e-05 - 32s/epoch - 161ms/step
Epoch 591/1000
2023-10-25 02:00:08.144 
Epoch 591/1000 
	 loss: 27.3295, MinusLogProbMetric: 27.3295, val_loss: 28.1310, val_MinusLogProbMetric: 28.1310

Epoch 591: val_loss did not improve from 28.12737
196/196 - 30s - loss: 27.3295 - MinusLogProbMetric: 27.3295 - val_loss: 28.1310 - val_MinusLogProbMetric: 28.1310 - lr: 3.1250e-05 - 30s/epoch - 153ms/step
Epoch 592/1000
2023-10-25 02:00:39.538 
Epoch 592/1000 
	 loss: 27.3277, MinusLogProbMetric: 27.3277, val_loss: 28.1327, val_MinusLogProbMetric: 28.1327

Epoch 592: val_loss did not improve from 28.12737
196/196 - 31s - loss: 27.3277 - MinusLogProbMetric: 27.3277 - val_loss: 28.1327 - val_MinusLogProbMetric: 28.1327 - lr: 3.1250e-05 - 31s/epoch - 160ms/step
Epoch 593/1000
2023-10-25 02:01:10.774 
Epoch 593/1000 
	 loss: 27.3282, MinusLogProbMetric: 27.3282, val_loss: 28.1313, val_MinusLogProbMetric: 28.1313

Epoch 593: val_loss did not improve from 28.12737
196/196 - 31s - loss: 27.3282 - MinusLogProbMetric: 27.3282 - val_loss: 28.1313 - val_MinusLogProbMetric: 28.1313 - lr: 3.1250e-05 - 31s/epoch - 159ms/step
Epoch 594/1000
2023-10-25 02:01:39.855 
Epoch 594/1000 
	 loss: 27.3277, MinusLogProbMetric: 27.3277, val_loss: 28.1309, val_MinusLogProbMetric: 28.1309

Epoch 594: val_loss did not improve from 28.12737
196/196 - 29s - loss: 27.3277 - MinusLogProbMetric: 27.3277 - val_loss: 28.1309 - val_MinusLogProbMetric: 28.1309 - lr: 3.1250e-05 - 29s/epoch - 148ms/step
Epoch 595/1000
2023-10-25 02:02:11.726 
Epoch 595/1000 
	 loss: 27.3294, MinusLogProbMetric: 27.3294, val_loss: 28.1385, val_MinusLogProbMetric: 28.1385

Epoch 595: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3294 - MinusLogProbMetric: 27.3294 - val_loss: 28.1385 - val_MinusLogProbMetric: 28.1385 - lr: 3.1250e-05 - 32s/epoch - 163ms/step
Epoch 596/1000
2023-10-25 02:02:43.416 
Epoch 596/1000 
	 loss: 27.3277, MinusLogProbMetric: 27.3277, val_loss: 28.1406, val_MinusLogProbMetric: 28.1406

Epoch 596: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3277 - MinusLogProbMetric: 27.3277 - val_loss: 28.1406 - val_MinusLogProbMetric: 28.1406 - lr: 3.1250e-05 - 32s/epoch - 162ms/step
Epoch 597/1000
2023-10-25 02:03:14.936 
Epoch 597/1000 
	 loss: 27.3272, MinusLogProbMetric: 27.3272, val_loss: 28.1408, val_MinusLogProbMetric: 28.1408

Epoch 597: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3272 - MinusLogProbMetric: 27.3272 - val_loss: 28.1408 - val_MinusLogProbMetric: 28.1408 - lr: 3.1250e-05 - 32s/epoch - 161ms/step
Epoch 598/1000
2023-10-25 02:03:47.038 
Epoch 598/1000 
	 loss: 27.3296, MinusLogProbMetric: 27.3296, val_loss: 28.1378, val_MinusLogProbMetric: 28.1378

Epoch 598: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3296 - MinusLogProbMetric: 27.3296 - val_loss: 28.1378 - val_MinusLogProbMetric: 28.1378 - lr: 3.1250e-05 - 32s/epoch - 164ms/step
Epoch 599/1000
2023-10-25 02:04:18.067 
Epoch 599/1000 
	 loss: 27.3289, MinusLogProbMetric: 27.3289, val_loss: 28.1376, val_MinusLogProbMetric: 28.1376

Epoch 599: val_loss did not improve from 28.12737
196/196 - 31s - loss: 27.3289 - MinusLogProbMetric: 27.3289 - val_loss: 28.1376 - val_MinusLogProbMetric: 28.1376 - lr: 3.1250e-05 - 31s/epoch - 158ms/step
Epoch 600/1000
2023-10-25 02:04:47.757 
Epoch 600/1000 
	 loss: 27.3281, MinusLogProbMetric: 27.3281, val_loss: 28.1393, val_MinusLogProbMetric: 28.1393

Epoch 600: val_loss did not improve from 28.12737
196/196 - 30s - loss: 27.3281 - MinusLogProbMetric: 27.3281 - val_loss: 28.1393 - val_MinusLogProbMetric: 28.1393 - lr: 3.1250e-05 - 30s/epoch - 151ms/step
Epoch 601/1000
2023-10-25 02:05:18.821 
Epoch 601/1000 
	 loss: 27.3277, MinusLogProbMetric: 27.3277, val_loss: 28.1323, val_MinusLogProbMetric: 28.1323

Epoch 601: val_loss did not improve from 28.12737
196/196 - 31s - loss: 27.3277 - MinusLogProbMetric: 27.3277 - val_loss: 28.1323 - val_MinusLogProbMetric: 28.1323 - lr: 3.1250e-05 - 31s/epoch - 158ms/step
Epoch 602/1000
2023-10-25 02:05:51.433 
Epoch 602/1000 
	 loss: 27.3267, MinusLogProbMetric: 27.3267, val_loss: 28.1348, val_MinusLogProbMetric: 28.1348

Epoch 602: val_loss did not improve from 28.12737
196/196 - 33s - loss: 27.3267 - MinusLogProbMetric: 27.3267 - val_loss: 28.1348 - val_MinusLogProbMetric: 28.1348 - lr: 3.1250e-05 - 33s/epoch - 166ms/step
Epoch 603/1000
2023-10-25 02:06:22.004 
Epoch 603/1000 
	 loss: 27.3275, MinusLogProbMetric: 27.3275, val_loss: 28.1376, val_MinusLogProbMetric: 28.1376

Epoch 603: val_loss did not improve from 28.12737
196/196 - 31s - loss: 27.3275 - MinusLogProbMetric: 27.3275 - val_loss: 28.1376 - val_MinusLogProbMetric: 28.1376 - lr: 3.1250e-05 - 31s/epoch - 156ms/step
Epoch 604/1000
2023-10-25 02:06:53.566 
Epoch 604/1000 
	 loss: 27.3285, MinusLogProbMetric: 27.3285, val_loss: 28.1365, val_MinusLogProbMetric: 28.1365

Epoch 604: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3285 - MinusLogProbMetric: 27.3285 - val_loss: 28.1365 - val_MinusLogProbMetric: 28.1365 - lr: 3.1250e-05 - 32s/epoch - 161ms/step
Epoch 605/1000
2023-10-25 02:07:26.246 
Epoch 605/1000 
	 loss: 27.3304, MinusLogProbMetric: 27.3304, val_loss: 28.1440, val_MinusLogProbMetric: 28.1440

Epoch 605: val_loss did not improve from 28.12737
196/196 - 33s - loss: 27.3304 - MinusLogProbMetric: 27.3304 - val_loss: 28.1440 - val_MinusLogProbMetric: 28.1440 - lr: 3.1250e-05 - 33s/epoch - 167ms/step
Epoch 606/1000
2023-10-25 02:07:59.129 
Epoch 606/1000 
	 loss: 27.3269, MinusLogProbMetric: 27.3269, val_loss: 28.1295, val_MinusLogProbMetric: 28.1295

Epoch 606: val_loss did not improve from 28.12737
196/196 - 33s - loss: 27.3269 - MinusLogProbMetric: 27.3269 - val_loss: 28.1295 - val_MinusLogProbMetric: 28.1295 - lr: 3.1250e-05 - 33s/epoch - 168ms/step
Epoch 607/1000
2023-10-25 02:08:31.040 
Epoch 607/1000 
	 loss: 27.3271, MinusLogProbMetric: 27.3271, val_loss: 28.1367, val_MinusLogProbMetric: 28.1367

Epoch 607: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3271 - MinusLogProbMetric: 27.3271 - val_loss: 28.1367 - val_MinusLogProbMetric: 28.1367 - lr: 3.1250e-05 - 32s/epoch - 163ms/step
Epoch 608/1000
2023-10-25 02:09:02.056 
Epoch 608/1000 
	 loss: 27.3275, MinusLogProbMetric: 27.3275, val_loss: 28.1367, val_MinusLogProbMetric: 28.1367

Epoch 608: val_loss did not improve from 28.12737
196/196 - 31s - loss: 27.3275 - MinusLogProbMetric: 27.3275 - val_loss: 28.1367 - val_MinusLogProbMetric: 28.1367 - lr: 3.1250e-05 - 31s/epoch - 158ms/step
Epoch 609/1000
2023-10-25 02:09:32.833 
Epoch 609/1000 
	 loss: 27.3261, MinusLogProbMetric: 27.3261, val_loss: 28.1446, val_MinusLogProbMetric: 28.1446

Epoch 609: val_loss did not improve from 28.12737
196/196 - 31s - loss: 27.3261 - MinusLogProbMetric: 27.3261 - val_loss: 28.1446 - val_MinusLogProbMetric: 28.1446 - lr: 3.1250e-05 - 31s/epoch - 157ms/step
Epoch 610/1000
2023-10-25 02:10:03.859 
Epoch 610/1000 
	 loss: 27.3285, MinusLogProbMetric: 27.3285, val_loss: 28.1341, val_MinusLogProbMetric: 28.1341

Epoch 610: val_loss did not improve from 28.12737
196/196 - 31s - loss: 27.3285 - MinusLogProbMetric: 27.3285 - val_loss: 28.1341 - val_MinusLogProbMetric: 28.1341 - lr: 3.1250e-05 - 31s/epoch - 158ms/step
Epoch 611/1000
2023-10-25 02:10:34.848 
Epoch 611/1000 
	 loss: 27.3275, MinusLogProbMetric: 27.3275, val_loss: 28.1320, val_MinusLogProbMetric: 28.1320

Epoch 611: val_loss did not improve from 28.12737
196/196 - 31s - loss: 27.3275 - MinusLogProbMetric: 27.3275 - val_loss: 28.1320 - val_MinusLogProbMetric: 28.1320 - lr: 3.1250e-05 - 31s/epoch - 158ms/step
Epoch 612/1000
2023-10-25 02:11:06.755 
Epoch 612/1000 
	 loss: 27.3288, MinusLogProbMetric: 27.3288, val_loss: 28.1396, val_MinusLogProbMetric: 28.1396

Epoch 612: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3288 - MinusLogProbMetric: 27.3288 - val_loss: 28.1396 - val_MinusLogProbMetric: 28.1396 - lr: 3.1250e-05 - 32s/epoch - 163ms/step
Epoch 613/1000
2023-10-25 02:11:38.081 
Epoch 613/1000 
	 loss: 27.3278, MinusLogProbMetric: 27.3278, val_loss: 28.1339, val_MinusLogProbMetric: 28.1339

Epoch 613: val_loss did not improve from 28.12737
196/196 - 31s - loss: 27.3278 - MinusLogProbMetric: 27.3278 - val_loss: 28.1339 - val_MinusLogProbMetric: 28.1339 - lr: 3.1250e-05 - 31s/epoch - 160ms/step
Epoch 614/1000
2023-10-25 02:12:10.093 
Epoch 614/1000 
	 loss: 27.3275, MinusLogProbMetric: 27.3275, val_loss: 28.1314, val_MinusLogProbMetric: 28.1314

Epoch 614: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3275 - MinusLogProbMetric: 27.3275 - val_loss: 28.1314 - val_MinusLogProbMetric: 28.1314 - lr: 3.1250e-05 - 32s/epoch - 163ms/step
Epoch 615/1000
2023-10-25 02:12:41.242 
Epoch 615/1000 
	 loss: 27.3270, MinusLogProbMetric: 27.3270, val_loss: 28.1388, val_MinusLogProbMetric: 28.1388

Epoch 615: val_loss did not improve from 28.12737
196/196 - 31s - loss: 27.3270 - MinusLogProbMetric: 27.3270 - val_loss: 28.1388 - val_MinusLogProbMetric: 28.1388 - lr: 3.1250e-05 - 31s/epoch - 159ms/step
Epoch 616/1000
2023-10-25 02:13:12.966 
Epoch 616/1000 
	 loss: 27.3263, MinusLogProbMetric: 27.3263, val_loss: 28.1350, val_MinusLogProbMetric: 28.1350

Epoch 616: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3263 - MinusLogProbMetric: 27.3263 - val_loss: 28.1350 - val_MinusLogProbMetric: 28.1350 - lr: 3.1250e-05 - 32s/epoch - 162ms/step
Epoch 617/1000
2023-10-25 02:13:43.669 
Epoch 617/1000 
	 loss: 27.3267, MinusLogProbMetric: 27.3267, val_loss: 28.1293, val_MinusLogProbMetric: 28.1293

Epoch 617: val_loss did not improve from 28.12737
196/196 - 31s - loss: 27.3267 - MinusLogProbMetric: 27.3267 - val_loss: 28.1293 - val_MinusLogProbMetric: 28.1293 - lr: 3.1250e-05 - 31s/epoch - 157ms/step
Epoch 618/1000
2023-10-25 02:14:15.651 
Epoch 618/1000 
	 loss: 27.3189, MinusLogProbMetric: 27.3189, val_loss: 28.1294, val_MinusLogProbMetric: 28.1294

Epoch 618: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3189 - MinusLogProbMetric: 27.3189 - val_loss: 28.1294 - val_MinusLogProbMetric: 28.1294 - lr: 1.5625e-05 - 32s/epoch - 163ms/step
Epoch 619/1000
2023-10-25 02:14:48.041 
Epoch 619/1000 
	 loss: 27.3180, MinusLogProbMetric: 27.3180, val_loss: 28.1345, val_MinusLogProbMetric: 28.1345

Epoch 619: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3180 - MinusLogProbMetric: 27.3180 - val_loss: 28.1345 - val_MinusLogProbMetric: 28.1345 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 620/1000
2023-10-25 02:15:20.432 
Epoch 620/1000 
	 loss: 27.3179, MinusLogProbMetric: 27.3179, val_loss: 28.1280, val_MinusLogProbMetric: 28.1280

Epoch 620: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3179 - MinusLogProbMetric: 27.3179 - val_loss: 28.1280 - val_MinusLogProbMetric: 28.1280 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 621/1000
2023-10-25 02:15:52.841 
Epoch 621/1000 
	 loss: 27.3182, MinusLogProbMetric: 27.3182, val_loss: 28.1377, val_MinusLogProbMetric: 28.1377

Epoch 621: val_loss did not improve from 28.12737
196/196 - 32s - loss: 27.3182 - MinusLogProbMetric: 27.3182 - val_loss: 28.1377 - val_MinusLogProbMetric: 28.1377 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 622/1000
2023-10-25 02:16:25.536 
Epoch 622/1000 
	 loss: 27.3188, MinusLogProbMetric: 27.3188, val_loss: 28.1310, val_MinusLogProbMetric: 28.1310

Epoch 622: val_loss did not improve from 28.12737
196/196 - 33s - loss: 27.3188 - MinusLogProbMetric: 27.3188 - val_loss: 28.1310 - val_MinusLogProbMetric: 28.1310 - lr: 1.5625e-05 - 33s/epoch - 167ms/step
Epoch 623/1000
2023-10-25 02:16:57.490 
Epoch 623/1000 
	 loss: 27.3180, MinusLogProbMetric: 27.3180, val_loss: 28.1260, val_MinusLogProbMetric: 28.1260

Epoch 623: val_loss improved from 28.12737 to 28.12598, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 33s - loss: 27.3180 - MinusLogProbMetric: 27.3180 - val_loss: 28.1260 - val_MinusLogProbMetric: 28.1260 - lr: 1.5625e-05 - 33s/epoch - 166ms/step
Epoch 624/1000
2023-10-25 02:17:30.497 
Epoch 624/1000 
	 loss: 27.3180, MinusLogProbMetric: 27.3180, val_loss: 28.1338, val_MinusLogProbMetric: 28.1338

Epoch 624: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3180 - MinusLogProbMetric: 27.3180 - val_loss: 28.1338 - val_MinusLogProbMetric: 28.1338 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 625/1000
2023-10-25 02:18:02.824 
Epoch 625/1000 
	 loss: 27.3185, MinusLogProbMetric: 27.3185, val_loss: 28.1342, val_MinusLogProbMetric: 28.1342

Epoch 625: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3185 - MinusLogProbMetric: 27.3185 - val_loss: 28.1342 - val_MinusLogProbMetric: 28.1342 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 626/1000
2023-10-25 02:18:35.159 
Epoch 626/1000 
	 loss: 27.3182, MinusLogProbMetric: 27.3182, val_loss: 28.1278, val_MinusLogProbMetric: 28.1278

Epoch 626: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3182 - MinusLogProbMetric: 27.3182 - val_loss: 28.1278 - val_MinusLogProbMetric: 28.1278 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 627/1000
2023-10-25 02:19:06.350 
Epoch 627/1000 
	 loss: 27.3187, MinusLogProbMetric: 27.3187, val_loss: 28.1287, val_MinusLogProbMetric: 28.1287

Epoch 627: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3187 - MinusLogProbMetric: 27.3187 - val_loss: 28.1287 - val_MinusLogProbMetric: 28.1287 - lr: 1.5625e-05 - 31s/epoch - 159ms/step
Epoch 628/1000
2023-10-25 02:19:38.341 
Epoch 628/1000 
	 loss: 27.3180, MinusLogProbMetric: 27.3180, val_loss: 28.1327, val_MinusLogProbMetric: 28.1327

Epoch 628: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3180 - MinusLogProbMetric: 27.3180 - val_loss: 28.1327 - val_MinusLogProbMetric: 28.1327 - lr: 1.5625e-05 - 32s/epoch - 163ms/step
Epoch 629/1000
2023-10-25 02:20:08.287 
Epoch 629/1000 
	 loss: 27.3190, MinusLogProbMetric: 27.3190, val_loss: 28.1318, val_MinusLogProbMetric: 28.1318

Epoch 629: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3190 - MinusLogProbMetric: 27.3190 - val_loss: 28.1318 - val_MinusLogProbMetric: 28.1318 - lr: 1.5625e-05 - 30s/epoch - 153ms/step
Epoch 630/1000
2023-10-25 02:20:39.209 
Epoch 630/1000 
	 loss: 27.3176, MinusLogProbMetric: 27.3176, val_loss: 28.1373, val_MinusLogProbMetric: 28.1373

Epoch 630: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3176 - MinusLogProbMetric: 27.3176 - val_loss: 28.1373 - val_MinusLogProbMetric: 28.1373 - lr: 1.5625e-05 - 31s/epoch - 158ms/step
Epoch 631/1000
2023-10-25 02:21:10.635 
Epoch 631/1000 
	 loss: 27.3183, MinusLogProbMetric: 27.3183, val_loss: 28.1340, val_MinusLogProbMetric: 28.1340

Epoch 631: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3183 - MinusLogProbMetric: 27.3183 - val_loss: 28.1340 - val_MinusLogProbMetric: 28.1340 - lr: 1.5625e-05 - 31s/epoch - 160ms/step
Epoch 632/1000
2023-10-25 02:21:40.515 
Epoch 632/1000 
	 loss: 27.3191, MinusLogProbMetric: 27.3191, val_loss: 28.1358, val_MinusLogProbMetric: 28.1358

Epoch 632: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3191 - MinusLogProbMetric: 27.3191 - val_loss: 28.1358 - val_MinusLogProbMetric: 28.1358 - lr: 1.5625e-05 - 30s/epoch - 152ms/step
Epoch 633/1000
2023-10-25 02:22:12.698 
Epoch 633/1000 
	 loss: 27.3187, MinusLogProbMetric: 27.3187, val_loss: 28.1341, val_MinusLogProbMetric: 28.1341

Epoch 633: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3187 - MinusLogProbMetric: 27.3187 - val_loss: 28.1341 - val_MinusLogProbMetric: 28.1341 - lr: 1.5625e-05 - 32s/epoch - 164ms/step
Epoch 634/1000
2023-10-25 02:22:44.993 
Epoch 634/1000 
	 loss: 27.3177, MinusLogProbMetric: 27.3177, val_loss: 28.1339, val_MinusLogProbMetric: 28.1339

Epoch 634: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3177 - MinusLogProbMetric: 27.3177 - val_loss: 28.1339 - val_MinusLogProbMetric: 28.1339 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 635/1000
2023-10-25 02:23:16.160 
Epoch 635/1000 
	 loss: 27.3180, MinusLogProbMetric: 27.3180, val_loss: 28.1329, val_MinusLogProbMetric: 28.1329

Epoch 635: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3180 - MinusLogProbMetric: 27.3180 - val_loss: 28.1329 - val_MinusLogProbMetric: 28.1329 - lr: 1.5625e-05 - 31s/epoch - 159ms/step
Epoch 636/1000
2023-10-25 02:23:46.554 
Epoch 636/1000 
	 loss: 27.3171, MinusLogProbMetric: 27.3171, val_loss: 28.1331, val_MinusLogProbMetric: 28.1331

Epoch 636: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3171 - MinusLogProbMetric: 27.3171 - val_loss: 28.1331 - val_MinusLogProbMetric: 28.1331 - lr: 1.5625e-05 - 30s/epoch - 155ms/step
Epoch 637/1000
2023-10-25 02:24:16.863 
Epoch 637/1000 
	 loss: 27.3175, MinusLogProbMetric: 27.3175, val_loss: 28.1334, val_MinusLogProbMetric: 28.1334

Epoch 637: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3175 - MinusLogProbMetric: 27.3175 - val_loss: 28.1334 - val_MinusLogProbMetric: 28.1334 - lr: 1.5625e-05 - 30s/epoch - 155ms/step
Epoch 638/1000
2023-10-25 02:24:46.838 
Epoch 638/1000 
	 loss: 27.3177, MinusLogProbMetric: 27.3177, val_loss: 28.1324, val_MinusLogProbMetric: 28.1324

Epoch 638: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3177 - MinusLogProbMetric: 27.3177 - val_loss: 28.1324 - val_MinusLogProbMetric: 28.1324 - lr: 1.5625e-05 - 30s/epoch - 153ms/step
Epoch 639/1000
2023-10-25 02:25:17.591 
Epoch 639/1000 
	 loss: 27.3174, MinusLogProbMetric: 27.3174, val_loss: 28.1342, val_MinusLogProbMetric: 28.1342

Epoch 639: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3174 - MinusLogProbMetric: 27.3174 - val_loss: 28.1342 - val_MinusLogProbMetric: 28.1342 - lr: 1.5625e-05 - 31s/epoch - 157ms/step
Epoch 640/1000
2023-10-25 02:25:47.865 
Epoch 640/1000 
	 loss: 27.3187, MinusLogProbMetric: 27.3187, val_loss: 28.1315, val_MinusLogProbMetric: 28.1315

Epoch 640: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3187 - MinusLogProbMetric: 27.3187 - val_loss: 28.1315 - val_MinusLogProbMetric: 28.1315 - lr: 1.5625e-05 - 30s/epoch - 154ms/step
Epoch 641/1000
2023-10-25 02:26:20.374 
Epoch 641/1000 
	 loss: 27.3176, MinusLogProbMetric: 27.3176, val_loss: 28.1271, val_MinusLogProbMetric: 28.1271

Epoch 641: val_loss did not improve from 28.12598
196/196 - 33s - loss: 27.3176 - MinusLogProbMetric: 27.3176 - val_loss: 28.1271 - val_MinusLogProbMetric: 28.1271 - lr: 1.5625e-05 - 33s/epoch - 166ms/step
Epoch 642/1000
2023-10-25 02:26:50.444 
Epoch 642/1000 
	 loss: 27.3181, MinusLogProbMetric: 27.3181, val_loss: 28.1329, val_MinusLogProbMetric: 28.1329

Epoch 642: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3181 - MinusLogProbMetric: 27.3181 - val_loss: 28.1329 - val_MinusLogProbMetric: 28.1329 - lr: 1.5625e-05 - 30s/epoch - 153ms/step
Epoch 643/1000
2023-10-25 02:27:20.748 
Epoch 643/1000 
	 loss: 27.3178, MinusLogProbMetric: 27.3178, val_loss: 28.1302, val_MinusLogProbMetric: 28.1302

Epoch 643: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3178 - MinusLogProbMetric: 27.3178 - val_loss: 28.1302 - val_MinusLogProbMetric: 28.1302 - lr: 1.5625e-05 - 30s/epoch - 155ms/step
Epoch 644/1000
2023-10-25 02:27:51.096 
Epoch 644/1000 
	 loss: 27.3171, MinusLogProbMetric: 27.3171, val_loss: 28.1291, val_MinusLogProbMetric: 28.1291

Epoch 644: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3171 - MinusLogProbMetric: 27.3171 - val_loss: 28.1291 - val_MinusLogProbMetric: 28.1291 - lr: 1.5625e-05 - 30s/epoch - 155ms/step
Epoch 645/1000
2023-10-25 02:28:23.027 
Epoch 645/1000 
	 loss: 27.3183, MinusLogProbMetric: 27.3183, val_loss: 28.1288, val_MinusLogProbMetric: 28.1288

Epoch 645: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3183 - MinusLogProbMetric: 27.3183 - val_loss: 28.1288 - val_MinusLogProbMetric: 28.1288 - lr: 1.5625e-05 - 32s/epoch - 163ms/step
Epoch 646/1000
2023-10-25 02:28:54.057 
Epoch 646/1000 
	 loss: 27.3177, MinusLogProbMetric: 27.3177, val_loss: 28.1309, val_MinusLogProbMetric: 28.1309

Epoch 646: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3177 - MinusLogProbMetric: 27.3177 - val_loss: 28.1309 - val_MinusLogProbMetric: 28.1309 - lr: 1.5625e-05 - 31s/epoch - 158ms/step
Epoch 647/1000
2023-10-25 02:29:26.591 
Epoch 647/1000 
	 loss: 27.3181, MinusLogProbMetric: 27.3181, val_loss: 28.1371, val_MinusLogProbMetric: 28.1371

Epoch 647: val_loss did not improve from 28.12598
196/196 - 33s - loss: 27.3181 - MinusLogProbMetric: 27.3181 - val_loss: 28.1371 - val_MinusLogProbMetric: 28.1371 - lr: 1.5625e-05 - 33s/epoch - 166ms/step
Epoch 648/1000
2023-10-25 02:29:58.849 
Epoch 648/1000 
	 loss: 27.3168, MinusLogProbMetric: 27.3168, val_loss: 28.1304, val_MinusLogProbMetric: 28.1304

Epoch 648: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3168 - MinusLogProbMetric: 27.3168 - val_loss: 28.1304 - val_MinusLogProbMetric: 28.1304 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 649/1000
2023-10-25 02:30:29.130 
Epoch 649/1000 
	 loss: 27.3185, MinusLogProbMetric: 27.3185, val_loss: 28.1336, val_MinusLogProbMetric: 28.1336

Epoch 649: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3185 - MinusLogProbMetric: 27.3185 - val_loss: 28.1336 - val_MinusLogProbMetric: 28.1336 - lr: 1.5625e-05 - 30s/epoch - 154ms/step
Epoch 650/1000
2023-10-25 02:31:00.675 
Epoch 650/1000 
	 loss: 27.3172, MinusLogProbMetric: 27.3172, val_loss: 28.1329, val_MinusLogProbMetric: 28.1329

Epoch 650: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3172 - MinusLogProbMetric: 27.3172 - val_loss: 28.1329 - val_MinusLogProbMetric: 28.1329 - lr: 1.5625e-05 - 32s/epoch - 161ms/step
Epoch 651/1000
2023-10-25 02:31:32.308 
Epoch 651/1000 
	 loss: 27.3167, MinusLogProbMetric: 27.3167, val_loss: 28.1380, val_MinusLogProbMetric: 28.1380

Epoch 651: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3167 - MinusLogProbMetric: 27.3167 - val_loss: 28.1380 - val_MinusLogProbMetric: 28.1380 - lr: 1.5625e-05 - 32s/epoch - 161ms/step
Epoch 652/1000
2023-10-25 02:32:02.478 
Epoch 652/1000 
	 loss: 27.3173, MinusLogProbMetric: 27.3173, val_loss: 28.1343, val_MinusLogProbMetric: 28.1343

Epoch 652: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3173 - MinusLogProbMetric: 27.3173 - val_loss: 28.1343 - val_MinusLogProbMetric: 28.1343 - lr: 1.5625e-05 - 30s/epoch - 154ms/step
Epoch 653/1000
2023-10-25 02:32:33.605 
Epoch 653/1000 
	 loss: 27.3172, MinusLogProbMetric: 27.3172, val_loss: 28.1282, val_MinusLogProbMetric: 28.1282

Epoch 653: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3172 - MinusLogProbMetric: 27.3172 - val_loss: 28.1282 - val_MinusLogProbMetric: 28.1282 - lr: 1.5625e-05 - 31s/epoch - 159ms/step
Epoch 654/1000
2023-10-25 02:33:03.779 
Epoch 654/1000 
	 loss: 27.3176, MinusLogProbMetric: 27.3176, val_loss: 28.1315, val_MinusLogProbMetric: 28.1315

Epoch 654: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3176 - MinusLogProbMetric: 27.3176 - val_loss: 28.1315 - val_MinusLogProbMetric: 28.1315 - lr: 1.5625e-05 - 30s/epoch - 154ms/step
Epoch 655/1000
2023-10-25 02:33:35.138 
Epoch 655/1000 
	 loss: 27.3173, MinusLogProbMetric: 27.3173, val_loss: 28.1315, val_MinusLogProbMetric: 28.1315

Epoch 655: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3173 - MinusLogProbMetric: 27.3173 - val_loss: 28.1315 - val_MinusLogProbMetric: 28.1315 - lr: 1.5625e-05 - 31s/epoch - 160ms/step
Epoch 656/1000
2023-10-25 02:34:06.650 
Epoch 656/1000 
	 loss: 27.3165, MinusLogProbMetric: 27.3165, val_loss: 28.1286, val_MinusLogProbMetric: 28.1286

Epoch 656: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3165 - MinusLogProbMetric: 27.3165 - val_loss: 28.1286 - val_MinusLogProbMetric: 28.1286 - lr: 1.5625e-05 - 32s/epoch - 161ms/step
Epoch 657/1000
2023-10-25 02:34:38.009 
Epoch 657/1000 
	 loss: 27.3174, MinusLogProbMetric: 27.3174, val_loss: 28.1337, val_MinusLogProbMetric: 28.1337

Epoch 657: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3174 - MinusLogProbMetric: 27.3174 - val_loss: 28.1337 - val_MinusLogProbMetric: 28.1337 - lr: 1.5625e-05 - 31s/epoch - 160ms/step
Epoch 658/1000
2023-10-25 02:35:09.978 
Epoch 658/1000 
	 loss: 27.3167, MinusLogProbMetric: 27.3167, val_loss: 28.1274, val_MinusLogProbMetric: 28.1274

Epoch 658: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3167 - MinusLogProbMetric: 27.3167 - val_loss: 28.1274 - val_MinusLogProbMetric: 28.1274 - lr: 1.5625e-05 - 32s/epoch - 163ms/step
Epoch 659/1000
2023-10-25 02:35:41.687 
Epoch 659/1000 
	 loss: 27.3167, MinusLogProbMetric: 27.3167, val_loss: 28.1370, val_MinusLogProbMetric: 28.1370

Epoch 659: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3167 - MinusLogProbMetric: 27.3167 - val_loss: 28.1370 - val_MinusLogProbMetric: 28.1370 - lr: 1.5625e-05 - 32s/epoch - 162ms/step
Epoch 660/1000
2023-10-25 02:36:14.232 
Epoch 660/1000 
	 loss: 27.3173, MinusLogProbMetric: 27.3173, val_loss: 28.1337, val_MinusLogProbMetric: 28.1337

Epoch 660: val_loss did not improve from 28.12598
196/196 - 33s - loss: 27.3173 - MinusLogProbMetric: 27.3173 - val_loss: 28.1337 - val_MinusLogProbMetric: 28.1337 - lr: 1.5625e-05 - 33s/epoch - 166ms/step
Epoch 661/1000
2023-10-25 02:36:46.070 
Epoch 661/1000 
	 loss: 27.3169, MinusLogProbMetric: 27.3169, val_loss: 28.1324, val_MinusLogProbMetric: 28.1324

Epoch 661: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3169 - MinusLogProbMetric: 27.3169 - val_loss: 28.1324 - val_MinusLogProbMetric: 28.1324 - lr: 1.5625e-05 - 32s/epoch - 162ms/step
Epoch 662/1000
2023-10-25 02:37:17.958 
Epoch 662/1000 
	 loss: 27.3168, MinusLogProbMetric: 27.3168, val_loss: 28.1379, val_MinusLogProbMetric: 28.1379

Epoch 662: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3168 - MinusLogProbMetric: 27.3168 - val_loss: 28.1379 - val_MinusLogProbMetric: 28.1379 - lr: 1.5625e-05 - 32s/epoch - 163ms/step
Epoch 663/1000
2023-10-25 02:37:48.237 
Epoch 663/1000 
	 loss: 27.3173, MinusLogProbMetric: 27.3173, val_loss: 28.1332, val_MinusLogProbMetric: 28.1332

Epoch 663: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3173 - MinusLogProbMetric: 27.3173 - val_loss: 28.1332 - val_MinusLogProbMetric: 28.1332 - lr: 1.5625e-05 - 30s/epoch - 154ms/step
Epoch 664/1000
2023-10-25 02:38:20.668 
Epoch 664/1000 
	 loss: 27.3171, MinusLogProbMetric: 27.3171, val_loss: 28.1263, val_MinusLogProbMetric: 28.1263

Epoch 664: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3171 - MinusLogProbMetric: 27.3171 - val_loss: 28.1263 - val_MinusLogProbMetric: 28.1263 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 665/1000
2023-10-25 02:38:52.866 
Epoch 665/1000 
	 loss: 27.3165, MinusLogProbMetric: 27.3165, val_loss: 28.1363, val_MinusLogProbMetric: 28.1363

Epoch 665: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3165 - MinusLogProbMetric: 27.3165 - val_loss: 28.1363 - val_MinusLogProbMetric: 28.1363 - lr: 1.5625e-05 - 32s/epoch - 164ms/step
Epoch 666/1000
2023-10-25 02:39:23.313 
Epoch 666/1000 
	 loss: 27.3173, MinusLogProbMetric: 27.3173, val_loss: 28.1305, val_MinusLogProbMetric: 28.1305

Epoch 666: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3173 - MinusLogProbMetric: 27.3173 - val_loss: 28.1305 - val_MinusLogProbMetric: 28.1305 - lr: 1.5625e-05 - 30s/epoch - 155ms/step
Epoch 667/1000
2023-10-25 02:39:53.848 
Epoch 667/1000 
	 loss: 27.3163, MinusLogProbMetric: 27.3163, val_loss: 28.1327, val_MinusLogProbMetric: 28.1327

Epoch 667: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3163 - MinusLogProbMetric: 27.3163 - val_loss: 28.1327 - val_MinusLogProbMetric: 28.1327 - lr: 1.5625e-05 - 31s/epoch - 156ms/step
Epoch 668/1000
2023-10-25 02:40:25.409 
Epoch 668/1000 
	 loss: 27.3176, MinusLogProbMetric: 27.3176, val_loss: 28.1363, val_MinusLogProbMetric: 28.1363

Epoch 668: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3176 - MinusLogProbMetric: 27.3176 - val_loss: 28.1363 - val_MinusLogProbMetric: 28.1363 - lr: 1.5625e-05 - 32s/epoch - 161ms/step
Epoch 669/1000
2023-10-25 02:40:57.211 
Epoch 669/1000 
	 loss: 27.3169, MinusLogProbMetric: 27.3169, val_loss: 28.1389, val_MinusLogProbMetric: 28.1389

Epoch 669: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3169 - MinusLogProbMetric: 27.3169 - val_loss: 28.1389 - val_MinusLogProbMetric: 28.1389 - lr: 1.5625e-05 - 32s/epoch - 162ms/step
Epoch 670/1000
2023-10-25 02:41:29.321 
Epoch 670/1000 
	 loss: 27.3163, MinusLogProbMetric: 27.3163, val_loss: 28.1308, val_MinusLogProbMetric: 28.1308

Epoch 670: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3163 - MinusLogProbMetric: 27.3163 - val_loss: 28.1308 - val_MinusLogProbMetric: 28.1308 - lr: 1.5625e-05 - 32s/epoch - 164ms/step
Epoch 671/1000
2023-10-25 02:42:01.733 
Epoch 671/1000 
	 loss: 27.3171, MinusLogProbMetric: 27.3171, val_loss: 28.1355, val_MinusLogProbMetric: 28.1355

Epoch 671: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3171 - MinusLogProbMetric: 27.3171 - val_loss: 28.1355 - val_MinusLogProbMetric: 28.1355 - lr: 1.5625e-05 - 32s/epoch - 165ms/step
Epoch 672/1000
2023-10-25 02:42:33.701 
Epoch 672/1000 
	 loss: 27.3174, MinusLogProbMetric: 27.3174, val_loss: 28.1308, val_MinusLogProbMetric: 28.1308

Epoch 672: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3174 - MinusLogProbMetric: 27.3174 - val_loss: 28.1308 - val_MinusLogProbMetric: 28.1308 - lr: 1.5625e-05 - 32s/epoch - 163ms/step
Epoch 673/1000
2023-10-25 02:43:05.018 
Epoch 673/1000 
	 loss: 27.3159, MinusLogProbMetric: 27.3159, val_loss: 28.1346, val_MinusLogProbMetric: 28.1346

Epoch 673: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3159 - MinusLogProbMetric: 27.3159 - val_loss: 28.1346 - val_MinusLogProbMetric: 28.1346 - lr: 1.5625e-05 - 31s/epoch - 160ms/step
Epoch 674/1000
2023-10-25 02:43:36.891 
Epoch 674/1000 
	 loss: 27.3126, MinusLogProbMetric: 27.3126, val_loss: 28.1309, val_MinusLogProbMetric: 28.1309

Epoch 674: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3126 - MinusLogProbMetric: 27.3126 - val_loss: 28.1309 - val_MinusLogProbMetric: 28.1309 - lr: 7.8125e-06 - 32s/epoch - 163ms/step
Epoch 675/1000
2023-10-25 02:44:07.482 
Epoch 675/1000 
	 loss: 27.3117, MinusLogProbMetric: 27.3117, val_loss: 28.1321, val_MinusLogProbMetric: 28.1321

Epoch 675: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3117 - MinusLogProbMetric: 27.3117 - val_loss: 28.1321 - val_MinusLogProbMetric: 28.1321 - lr: 7.8125e-06 - 31s/epoch - 156ms/step
Epoch 676/1000
2023-10-25 02:44:39.429 
Epoch 676/1000 
	 loss: 27.3121, MinusLogProbMetric: 27.3121, val_loss: 28.1307, val_MinusLogProbMetric: 28.1307

Epoch 676: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3121 - MinusLogProbMetric: 27.3121 - val_loss: 28.1307 - val_MinusLogProbMetric: 28.1307 - lr: 7.8125e-06 - 32s/epoch - 163ms/step
Epoch 677/1000
2023-10-25 02:45:11.116 
Epoch 677/1000 
	 loss: 27.3125, MinusLogProbMetric: 27.3125, val_loss: 28.1290, val_MinusLogProbMetric: 28.1290

Epoch 677: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3125 - MinusLogProbMetric: 27.3125 - val_loss: 28.1290 - val_MinusLogProbMetric: 28.1290 - lr: 7.8125e-06 - 32s/epoch - 162ms/step
Epoch 678/1000
2023-10-25 02:45:43.059 
Epoch 678/1000 
	 loss: 27.3115, MinusLogProbMetric: 27.3115, val_loss: 28.1309, val_MinusLogProbMetric: 28.1309

Epoch 678: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3115 - MinusLogProbMetric: 27.3115 - val_loss: 28.1309 - val_MinusLogProbMetric: 28.1309 - lr: 7.8125e-06 - 32s/epoch - 163ms/step
Epoch 679/1000
2023-10-25 02:46:14.793 
Epoch 679/1000 
	 loss: 27.3124, MinusLogProbMetric: 27.3124, val_loss: 28.1290, val_MinusLogProbMetric: 28.1290

Epoch 679: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3124 - MinusLogProbMetric: 27.3124 - val_loss: 28.1290 - val_MinusLogProbMetric: 28.1290 - lr: 7.8125e-06 - 32s/epoch - 162ms/step
Epoch 680/1000
2023-10-25 02:46:46.585 
Epoch 680/1000 
	 loss: 27.3121, MinusLogProbMetric: 27.3121, val_loss: 28.1339, val_MinusLogProbMetric: 28.1339

Epoch 680: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3121 - MinusLogProbMetric: 27.3121 - val_loss: 28.1339 - val_MinusLogProbMetric: 28.1339 - lr: 7.8125e-06 - 32s/epoch - 162ms/step
Epoch 681/1000
2023-10-25 02:47:17.937 
Epoch 681/1000 
	 loss: 27.3119, MinusLogProbMetric: 27.3119, val_loss: 28.1329, val_MinusLogProbMetric: 28.1329

Epoch 681: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3119 - MinusLogProbMetric: 27.3119 - val_loss: 28.1329 - val_MinusLogProbMetric: 28.1329 - lr: 7.8125e-06 - 31s/epoch - 160ms/step
Epoch 682/1000
2023-10-25 02:47:50.332 
Epoch 682/1000 
	 loss: 27.3123, MinusLogProbMetric: 27.3123, val_loss: 28.1307, val_MinusLogProbMetric: 28.1307

Epoch 682: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3123 - MinusLogProbMetric: 27.3123 - val_loss: 28.1307 - val_MinusLogProbMetric: 28.1307 - lr: 7.8125e-06 - 32s/epoch - 165ms/step
Epoch 683/1000
2023-10-25 02:48:21.545 
Epoch 683/1000 
	 loss: 27.3117, MinusLogProbMetric: 27.3117, val_loss: 28.1302, val_MinusLogProbMetric: 28.1302

Epoch 683: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3117 - MinusLogProbMetric: 27.3117 - val_loss: 28.1302 - val_MinusLogProbMetric: 28.1302 - lr: 7.8125e-06 - 31s/epoch - 159ms/step
Epoch 684/1000
2023-10-25 02:48:52.566 
Epoch 684/1000 
	 loss: 27.3118, MinusLogProbMetric: 27.3118, val_loss: 28.1295, val_MinusLogProbMetric: 28.1295

Epoch 684: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3118 - MinusLogProbMetric: 27.3118 - val_loss: 28.1295 - val_MinusLogProbMetric: 28.1295 - lr: 7.8125e-06 - 31s/epoch - 158ms/step
Epoch 685/1000
2023-10-25 02:49:24.474 
Epoch 685/1000 
	 loss: 27.3119, MinusLogProbMetric: 27.3119, val_loss: 28.1322, val_MinusLogProbMetric: 28.1322

Epoch 685: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3119 - MinusLogProbMetric: 27.3119 - val_loss: 28.1322 - val_MinusLogProbMetric: 28.1322 - lr: 7.8125e-06 - 32s/epoch - 163ms/step
Epoch 686/1000
2023-10-25 02:49:56.736 
Epoch 686/1000 
	 loss: 27.3120, MinusLogProbMetric: 27.3120, val_loss: 28.1296, val_MinusLogProbMetric: 28.1296

Epoch 686: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3120 - MinusLogProbMetric: 27.3120 - val_loss: 28.1296 - val_MinusLogProbMetric: 28.1296 - lr: 7.8125e-06 - 32s/epoch - 165ms/step
Epoch 687/1000
2023-10-25 02:50:29.789 
Epoch 687/1000 
	 loss: 27.3119, MinusLogProbMetric: 27.3119, val_loss: 28.1313, val_MinusLogProbMetric: 28.1313

Epoch 687: val_loss did not improve from 28.12598
196/196 - 33s - loss: 27.3119 - MinusLogProbMetric: 27.3119 - val_loss: 28.1313 - val_MinusLogProbMetric: 28.1313 - lr: 7.8125e-06 - 33s/epoch - 169ms/step
Epoch 688/1000
2023-10-25 02:51:01.029 
Epoch 688/1000 
	 loss: 27.3120, MinusLogProbMetric: 27.3120, val_loss: 28.1325, val_MinusLogProbMetric: 28.1325

Epoch 688: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3120 - MinusLogProbMetric: 27.3120 - val_loss: 28.1325 - val_MinusLogProbMetric: 28.1325 - lr: 7.8125e-06 - 31s/epoch - 159ms/step
Epoch 689/1000
2023-10-25 02:51:31.303 
Epoch 689/1000 
	 loss: 27.3116, MinusLogProbMetric: 27.3116, val_loss: 28.1339, val_MinusLogProbMetric: 28.1339

Epoch 689: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3116 - MinusLogProbMetric: 27.3116 - val_loss: 28.1339 - val_MinusLogProbMetric: 28.1339 - lr: 7.8125e-06 - 30s/epoch - 154ms/step
Epoch 690/1000
2023-10-25 02:52:02.667 
Epoch 690/1000 
	 loss: 27.3120, MinusLogProbMetric: 27.3120, val_loss: 28.1302, val_MinusLogProbMetric: 28.1302

Epoch 690: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3120 - MinusLogProbMetric: 27.3120 - val_loss: 28.1302 - val_MinusLogProbMetric: 28.1302 - lr: 7.8125e-06 - 31s/epoch - 160ms/step
Epoch 691/1000
2023-10-25 02:52:34.021 
Epoch 691/1000 
	 loss: 27.3119, MinusLogProbMetric: 27.3119, val_loss: 28.1319, val_MinusLogProbMetric: 28.1319

Epoch 691: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3119 - MinusLogProbMetric: 27.3119 - val_loss: 28.1319 - val_MinusLogProbMetric: 28.1319 - lr: 7.8125e-06 - 31s/epoch - 160ms/step
Epoch 692/1000
2023-10-25 02:53:06.278 
Epoch 692/1000 
	 loss: 27.3113, MinusLogProbMetric: 27.3113, val_loss: 28.1317, val_MinusLogProbMetric: 28.1317

Epoch 692: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3113 - MinusLogProbMetric: 27.3113 - val_loss: 28.1317 - val_MinusLogProbMetric: 28.1317 - lr: 7.8125e-06 - 32s/epoch - 165ms/step
Epoch 693/1000
2023-10-25 02:53:37.666 
Epoch 693/1000 
	 loss: 27.3118, MinusLogProbMetric: 27.3118, val_loss: 28.1313, val_MinusLogProbMetric: 28.1313

Epoch 693: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3118 - MinusLogProbMetric: 27.3118 - val_loss: 28.1313 - val_MinusLogProbMetric: 28.1313 - lr: 7.8125e-06 - 31s/epoch - 160ms/step
Epoch 694/1000
2023-10-25 02:54:09.654 
Epoch 694/1000 
	 loss: 27.3118, MinusLogProbMetric: 27.3118, val_loss: 28.1299, val_MinusLogProbMetric: 28.1299

Epoch 694: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3118 - MinusLogProbMetric: 27.3118 - val_loss: 28.1299 - val_MinusLogProbMetric: 28.1299 - lr: 7.8125e-06 - 32s/epoch - 163ms/step
Epoch 695/1000
2023-10-25 02:54:40.445 
Epoch 695/1000 
	 loss: 27.3116, MinusLogProbMetric: 27.3116, val_loss: 28.1331, val_MinusLogProbMetric: 28.1331

Epoch 695: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3116 - MinusLogProbMetric: 27.3116 - val_loss: 28.1331 - val_MinusLogProbMetric: 28.1331 - lr: 7.8125e-06 - 31s/epoch - 157ms/step
Epoch 696/1000
2023-10-25 02:55:10.634 
Epoch 696/1000 
	 loss: 27.3117, MinusLogProbMetric: 27.3117, val_loss: 28.1334, val_MinusLogProbMetric: 28.1334

Epoch 696: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3117 - MinusLogProbMetric: 27.3117 - val_loss: 28.1334 - val_MinusLogProbMetric: 28.1334 - lr: 7.8125e-06 - 30s/epoch - 154ms/step
Epoch 697/1000
2023-10-25 02:55:41.639 
Epoch 697/1000 
	 loss: 27.3115, MinusLogProbMetric: 27.3115, val_loss: 28.1300, val_MinusLogProbMetric: 28.1300

Epoch 697: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3115 - MinusLogProbMetric: 27.3115 - val_loss: 28.1300 - val_MinusLogProbMetric: 28.1300 - lr: 7.8125e-06 - 31s/epoch - 158ms/step
Epoch 698/1000
2023-10-25 02:56:11.721 
Epoch 698/1000 
	 loss: 27.3118, MinusLogProbMetric: 27.3118, val_loss: 28.1285, val_MinusLogProbMetric: 28.1285

Epoch 698: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3118 - MinusLogProbMetric: 27.3118 - val_loss: 28.1285 - val_MinusLogProbMetric: 28.1285 - lr: 7.8125e-06 - 30s/epoch - 153ms/step
Epoch 699/1000
2023-10-25 02:56:42.108 
Epoch 699/1000 
	 loss: 27.3119, MinusLogProbMetric: 27.3119, val_loss: 28.1314, val_MinusLogProbMetric: 28.1314

Epoch 699: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3119 - MinusLogProbMetric: 27.3119 - val_loss: 28.1314 - val_MinusLogProbMetric: 28.1314 - lr: 7.8125e-06 - 30s/epoch - 155ms/step
Epoch 700/1000
2023-10-25 02:57:13.626 
Epoch 700/1000 
	 loss: 27.3115, MinusLogProbMetric: 27.3115, val_loss: 28.1291, val_MinusLogProbMetric: 28.1291

Epoch 700: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3115 - MinusLogProbMetric: 27.3115 - val_loss: 28.1291 - val_MinusLogProbMetric: 28.1291 - lr: 7.8125e-06 - 32s/epoch - 161ms/step
Epoch 701/1000
2023-10-25 02:57:46.209 
Epoch 701/1000 
	 loss: 27.3115, MinusLogProbMetric: 27.3115, val_loss: 28.1293, val_MinusLogProbMetric: 28.1293

Epoch 701: val_loss did not improve from 28.12598
196/196 - 33s - loss: 27.3115 - MinusLogProbMetric: 27.3115 - val_loss: 28.1293 - val_MinusLogProbMetric: 28.1293 - lr: 7.8125e-06 - 33s/epoch - 166ms/step
Epoch 702/1000
2023-10-25 02:58:17.242 
Epoch 702/1000 
	 loss: 27.3112, MinusLogProbMetric: 27.3112, val_loss: 28.1324, val_MinusLogProbMetric: 28.1324

Epoch 702: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3112 - MinusLogProbMetric: 27.3112 - val_loss: 28.1324 - val_MinusLogProbMetric: 28.1324 - lr: 7.8125e-06 - 31s/epoch - 158ms/step
Epoch 703/1000
2023-10-25 02:58:49.319 
Epoch 703/1000 
	 loss: 27.3114, MinusLogProbMetric: 27.3114, val_loss: 28.1309, val_MinusLogProbMetric: 28.1309

Epoch 703: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3114 - MinusLogProbMetric: 27.3114 - val_loss: 28.1309 - val_MinusLogProbMetric: 28.1309 - lr: 7.8125e-06 - 32s/epoch - 164ms/step
Epoch 704/1000
2023-10-25 02:59:20.455 
Epoch 704/1000 
	 loss: 27.3110, MinusLogProbMetric: 27.3110, val_loss: 28.1312, val_MinusLogProbMetric: 28.1312

Epoch 704: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3110 - MinusLogProbMetric: 27.3110 - val_loss: 28.1312 - val_MinusLogProbMetric: 28.1312 - lr: 7.8125e-06 - 31s/epoch - 159ms/step
Epoch 705/1000
2023-10-25 02:59:52.302 
Epoch 705/1000 
	 loss: 27.3118, MinusLogProbMetric: 27.3118, val_loss: 28.1290, val_MinusLogProbMetric: 28.1290

Epoch 705: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3118 - MinusLogProbMetric: 27.3118 - val_loss: 28.1290 - val_MinusLogProbMetric: 28.1290 - lr: 7.8125e-06 - 32s/epoch - 162ms/step
Epoch 706/1000
2023-10-25 03:00:23.246 
Epoch 706/1000 
	 loss: 27.3118, MinusLogProbMetric: 27.3118, val_loss: 28.1339, val_MinusLogProbMetric: 28.1339

Epoch 706: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3118 - MinusLogProbMetric: 27.3118 - val_loss: 28.1339 - val_MinusLogProbMetric: 28.1339 - lr: 7.8125e-06 - 31s/epoch - 158ms/step
Epoch 707/1000
2023-10-25 03:00:55.675 
Epoch 707/1000 
	 loss: 27.3115, MinusLogProbMetric: 27.3115, val_loss: 28.1323, val_MinusLogProbMetric: 28.1323

Epoch 707: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3115 - MinusLogProbMetric: 27.3115 - val_loss: 28.1323 - val_MinusLogProbMetric: 28.1323 - lr: 7.8125e-06 - 32s/epoch - 165ms/step
Epoch 708/1000
2023-10-25 03:01:27.325 
Epoch 708/1000 
	 loss: 27.3112, MinusLogProbMetric: 27.3112, val_loss: 28.1308, val_MinusLogProbMetric: 28.1308

Epoch 708: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3112 - MinusLogProbMetric: 27.3112 - val_loss: 28.1308 - val_MinusLogProbMetric: 28.1308 - lr: 7.8125e-06 - 32s/epoch - 161ms/step
Epoch 709/1000
2023-10-25 03:01:58.785 
Epoch 709/1000 
	 loss: 27.3116, MinusLogProbMetric: 27.3116, val_loss: 28.1323, val_MinusLogProbMetric: 28.1323

Epoch 709: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3116 - MinusLogProbMetric: 27.3116 - val_loss: 28.1323 - val_MinusLogProbMetric: 28.1323 - lr: 7.8125e-06 - 31s/epoch - 160ms/step
Epoch 710/1000
2023-10-25 03:02:29.672 
Epoch 710/1000 
	 loss: 27.3115, MinusLogProbMetric: 27.3115, val_loss: 28.1355, val_MinusLogProbMetric: 28.1355

Epoch 710: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3115 - MinusLogProbMetric: 27.3115 - val_loss: 28.1355 - val_MinusLogProbMetric: 28.1355 - lr: 7.8125e-06 - 31s/epoch - 158ms/step
Epoch 711/1000
2023-10-25 03:03:02.155 
Epoch 711/1000 
	 loss: 27.3116, MinusLogProbMetric: 27.3116, val_loss: 28.1314, val_MinusLogProbMetric: 28.1314

Epoch 711: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3116 - MinusLogProbMetric: 27.3116 - val_loss: 28.1314 - val_MinusLogProbMetric: 28.1314 - lr: 7.8125e-06 - 32s/epoch - 166ms/step
Epoch 712/1000
2023-10-25 03:03:34.595 
Epoch 712/1000 
	 loss: 27.3117, MinusLogProbMetric: 27.3117, val_loss: 28.1315, val_MinusLogProbMetric: 28.1315

Epoch 712: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3117 - MinusLogProbMetric: 27.3117 - val_loss: 28.1315 - val_MinusLogProbMetric: 28.1315 - lr: 7.8125e-06 - 32s/epoch - 165ms/step
Epoch 713/1000
2023-10-25 03:04:06.486 
Epoch 713/1000 
	 loss: 27.3116, MinusLogProbMetric: 27.3116, val_loss: 28.1287, val_MinusLogProbMetric: 28.1287

Epoch 713: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3116 - MinusLogProbMetric: 27.3116 - val_loss: 28.1287 - val_MinusLogProbMetric: 28.1287 - lr: 7.8125e-06 - 32s/epoch - 163ms/step
Epoch 714/1000
2023-10-25 03:04:38.507 
Epoch 714/1000 
	 loss: 27.3113, MinusLogProbMetric: 27.3113, val_loss: 28.1316, val_MinusLogProbMetric: 28.1316

Epoch 714: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3113 - MinusLogProbMetric: 27.3113 - val_loss: 28.1316 - val_MinusLogProbMetric: 28.1316 - lr: 7.8125e-06 - 32s/epoch - 163ms/step
Epoch 715/1000
2023-10-25 03:05:08.826 
Epoch 715/1000 
	 loss: 27.3115, MinusLogProbMetric: 27.3115, val_loss: 28.1291, val_MinusLogProbMetric: 28.1291

Epoch 715: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3115 - MinusLogProbMetric: 27.3115 - val_loss: 28.1291 - val_MinusLogProbMetric: 28.1291 - lr: 7.8125e-06 - 30s/epoch - 155ms/step
Epoch 716/1000
2023-10-25 03:05:41.066 
Epoch 716/1000 
	 loss: 27.3113, MinusLogProbMetric: 27.3113, val_loss: 28.1314, val_MinusLogProbMetric: 28.1314

Epoch 716: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3113 - MinusLogProbMetric: 27.3113 - val_loss: 28.1314 - val_MinusLogProbMetric: 28.1314 - lr: 7.8125e-06 - 32s/epoch - 164ms/step
Epoch 717/1000
2023-10-25 03:06:11.480 
Epoch 717/1000 
	 loss: 27.3110, MinusLogProbMetric: 27.3110, val_loss: 28.1326, val_MinusLogProbMetric: 28.1326

Epoch 717: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3110 - MinusLogProbMetric: 27.3110 - val_loss: 28.1326 - val_MinusLogProbMetric: 28.1326 - lr: 7.8125e-06 - 30s/epoch - 155ms/step
Epoch 718/1000
2023-10-25 03:06:41.776 
Epoch 718/1000 
	 loss: 27.3116, MinusLogProbMetric: 27.3116, val_loss: 28.1316, val_MinusLogProbMetric: 28.1316

Epoch 718: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3116 - MinusLogProbMetric: 27.3116 - val_loss: 28.1316 - val_MinusLogProbMetric: 28.1316 - lr: 7.8125e-06 - 30s/epoch - 155ms/step
Epoch 719/1000
2023-10-25 03:07:13.400 
Epoch 719/1000 
	 loss: 27.3113, MinusLogProbMetric: 27.3113, val_loss: 28.1326, val_MinusLogProbMetric: 28.1326

Epoch 719: val_loss did not improve from 28.12598
196/196 - 32s - loss: 27.3113 - MinusLogProbMetric: 27.3113 - val_loss: 28.1326 - val_MinusLogProbMetric: 28.1326 - lr: 7.8125e-06 - 32s/epoch - 161ms/step
Epoch 720/1000
2023-10-25 03:07:44.815 
Epoch 720/1000 
	 loss: 27.3114, MinusLogProbMetric: 27.3114, val_loss: 28.1306, val_MinusLogProbMetric: 28.1306

Epoch 720: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3114 - MinusLogProbMetric: 27.3114 - val_loss: 28.1306 - val_MinusLogProbMetric: 28.1306 - lr: 7.8125e-06 - 31s/epoch - 160ms/step
Epoch 721/1000
2023-10-25 03:08:16.099 
Epoch 721/1000 
	 loss: 27.3113, MinusLogProbMetric: 27.3113, val_loss: 28.1304, val_MinusLogProbMetric: 28.1304

Epoch 721: val_loss did not improve from 28.12598
196/196 - 31s - loss: 27.3113 - MinusLogProbMetric: 27.3113 - val_loss: 28.1304 - val_MinusLogProbMetric: 28.1304 - lr: 7.8125e-06 - 31s/epoch - 160ms/step
Epoch 722/1000
2023-10-25 03:08:46.579 
Epoch 722/1000 
	 loss: 27.3115, MinusLogProbMetric: 27.3115, val_loss: 28.1318, val_MinusLogProbMetric: 28.1318

Epoch 722: val_loss did not improve from 28.12598
196/196 - 30s - loss: 27.3115 - MinusLogProbMetric: 27.3115 - val_loss: 28.1318 - val_MinusLogProbMetric: 28.1318 - lr: 7.8125e-06 - 30s/epoch - 155ms/step
Epoch 723/1000
2023-10-25 03:09:18.737 
Epoch 723/1000 
	 loss: 27.3114, MinusLogProbMetric: 27.3114, val_loss: 28.1330, val_MinusLogProbMetric: 28.1330

Epoch 723: val_loss did not improve from 28.12598
Restoring model weights from the end of the best epoch: 623.
196/196 - 32s - loss: 27.3114 - MinusLogProbMetric: 27.3114 - val_loss: 28.1330 - val_MinusLogProbMetric: 28.1330 - lr: 7.8125e-06 - 32s/epoch - 165ms/step
Epoch 723: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 440.
Model trained in 22862.47 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.67 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 0.88 s.
===========
Run 346/720 done in 22867.01 s.
===========

Directory ../../results/CsplineN_new/run_347/ already exists.
Skipping it.
===========
Run 347/720 already exists. Skipping it.
===========

===========
Generating train data for run 348.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_348/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_348/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_348/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_348
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_237"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_238 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  2139360   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,139,360
Trainable params: 2,139,360
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7fe6405ae410>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe64078e9b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe64078e9b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe7926ed990>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe67418e830>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe67418f100>, <keras.callbacks.ModelCheckpoint object at 0x7fe67418f4f0>, <keras.callbacks.EarlyStopping object at 0x7fe67418e8f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe67418eda0>, <keras.callbacks.TerminateOnNaN object at 0x7fe67418f130>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_348/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 348/720 with hyperparameters:
timestamp = 2023-10-25 03:09:25.052927
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2139360
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-10-25 03:11:09.778 
Epoch 1/1000 
	 loss: 367.4133, MinusLogProbMetric: 367.4133, val_loss: 94.7714, val_MinusLogProbMetric: 94.7714

Epoch 1: val_loss improved from inf to 94.77143, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 105s - loss: 367.4133 - MinusLogProbMetric: 367.4133 - val_loss: 94.7714 - val_MinusLogProbMetric: 94.7714 - lr: 0.0010 - 105s/epoch - 536ms/step
Epoch 2/1000
2023-10-25 03:11:45.783 
Epoch 2/1000 
	 loss: 71.7794, MinusLogProbMetric: 71.7794, val_loss: 57.6858, val_MinusLogProbMetric: 57.6858

Epoch 2: val_loss improved from 94.77143 to 57.68581, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 36s - loss: 71.7794 - MinusLogProbMetric: 71.7794 - val_loss: 57.6858 - val_MinusLogProbMetric: 57.6858 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 3/1000
2023-10-25 03:12:23.425 
Epoch 3/1000 
	 loss: 52.7946, MinusLogProbMetric: 52.7946, val_loss: 49.0160, val_MinusLogProbMetric: 49.0160

Epoch 3: val_loss improved from 57.68581 to 49.01599, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 38s - loss: 52.7946 - MinusLogProbMetric: 52.7946 - val_loss: 49.0160 - val_MinusLogProbMetric: 49.0160 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 4/1000
2023-10-25 03:13:02.508 
Epoch 4/1000 
	 loss: 46.2431, MinusLogProbMetric: 46.2431, val_loss: 44.8157, val_MinusLogProbMetric: 44.8157

Epoch 4: val_loss improved from 49.01599 to 44.81565, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 46.2431 - MinusLogProbMetric: 46.2431 - val_loss: 44.8157 - val_MinusLogProbMetric: 44.8157 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 5/1000
2023-10-25 03:13:41.227 
Epoch 5/1000 
	 loss: 42.6795, MinusLogProbMetric: 42.6795, val_loss: 43.4002, val_MinusLogProbMetric: 43.4002

Epoch 5: val_loss improved from 44.81565 to 43.40017, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 42.6795 - MinusLogProbMetric: 42.6795 - val_loss: 43.4002 - val_MinusLogProbMetric: 43.4002 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 6/1000
2023-10-25 03:14:20.821 
Epoch 6/1000 
	 loss: 40.6793, MinusLogProbMetric: 40.6793, val_loss: 39.0771, val_MinusLogProbMetric: 39.0771

Epoch 6: val_loss improved from 43.40017 to 39.07709, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 40s - loss: 40.6793 - MinusLogProbMetric: 40.6793 - val_loss: 39.0771 - val_MinusLogProbMetric: 39.0771 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 7/1000
2023-10-25 03:14:59.566 
Epoch 7/1000 
	 loss: 38.6565, MinusLogProbMetric: 38.6565, val_loss: 39.2834, val_MinusLogProbMetric: 39.2834

Epoch 7: val_loss did not improve from 39.07709
196/196 - 38s - loss: 38.6565 - MinusLogProbMetric: 38.6565 - val_loss: 39.2834 - val_MinusLogProbMetric: 39.2834 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 8/1000
2023-10-25 03:15:37.416 
Epoch 8/1000 
	 loss: 37.5260, MinusLogProbMetric: 37.5260, val_loss: 37.2484, val_MinusLogProbMetric: 37.2484

Epoch 8: val_loss improved from 39.07709 to 37.24839, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 37.5260 - MinusLogProbMetric: 37.5260 - val_loss: 37.2484 - val_MinusLogProbMetric: 37.2484 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 9/1000
2023-10-25 03:16:16.018 
Epoch 9/1000 
	 loss: 36.7665, MinusLogProbMetric: 36.7665, val_loss: 36.8924, val_MinusLogProbMetric: 36.8924

Epoch 9: val_loss improved from 37.24839 to 36.89243, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 36.7665 - MinusLogProbMetric: 36.7665 - val_loss: 36.8924 - val_MinusLogProbMetric: 36.8924 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 10/1000
2023-10-25 03:16:53.475 
Epoch 10/1000 
	 loss: 36.2066, MinusLogProbMetric: 36.2066, val_loss: 34.9549, val_MinusLogProbMetric: 34.9549

Epoch 10: val_loss improved from 36.89243 to 34.95490, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 38s - loss: 36.2066 - MinusLogProbMetric: 36.2066 - val_loss: 34.9549 - val_MinusLogProbMetric: 34.9549 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 11/1000
2023-10-25 03:17:31.045 
Epoch 11/1000 
	 loss: 35.6052, MinusLogProbMetric: 35.6052, val_loss: 35.4715, val_MinusLogProbMetric: 35.4715

Epoch 11: val_loss did not improve from 34.95490
196/196 - 37s - loss: 35.6052 - MinusLogProbMetric: 35.6052 - val_loss: 35.4715 - val_MinusLogProbMetric: 35.4715 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 12/1000
2023-10-25 03:18:08.626 
Epoch 12/1000 
	 loss: 35.0249, MinusLogProbMetric: 35.0249, val_loss: 35.6215, val_MinusLogProbMetric: 35.6215

Epoch 12: val_loss did not improve from 34.95490
196/196 - 38s - loss: 35.0249 - MinusLogProbMetric: 35.0249 - val_loss: 35.6215 - val_MinusLogProbMetric: 35.6215 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 13/1000
2023-10-25 03:18:46.679 
Epoch 13/1000 
	 loss: 34.6573, MinusLogProbMetric: 34.6573, val_loss: 34.3279, val_MinusLogProbMetric: 34.3279

Epoch 13: val_loss improved from 34.95490 to 34.32794, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 34.6573 - MinusLogProbMetric: 34.6573 - val_loss: 34.3279 - val_MinusLogProbMetric: 34.3279 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 14/1000
2023-10-25 03:19:26.052 
Epoch 14/1000 
	 loss: 34.1065, MinusLogProbMetric: 34.1065, val_loss: 36.9157, val_MinusLogProbMetric: 36.9157

Epoch 14: val_loss did not improve from 34.32794
196/196 - 39s - loss: 34.1065 - MinusLogProbMetric: 34.1065 - val_loss: 36.9157 - val_MinusLogProbMetric: 36.9157 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 15/1000
2023-10-25 03:20:03.497 
Epoch 15/1000 
	 loss: 33.8902, MinusLogProbMetric: 33.8902, val_loss: 33.2321, val_MinusLogProbMetric: 33.2321

Epoch 15: val_loss improved from 34.32794 to 33.23211, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 38s - loss: 33.8902 - MinusLogProbMetric: 33.8902 - val_loss: 33.2321 - val_MinusLogProbMetric: 33.2321 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 16/1000
2023-10-25 03:20:41.983 
Epoch 16/1000 
	 loss: 33.7919, MinusLogProbMetric: 33.7919, val_loss: 34.0532, val_MinusLogProbMetric: 34.0532

Epoch 16: val_loss did not improve from 33.23211
196/196 - 38s - loss: 33.7919 - MinusLogProbMetric: 33.7919 - val_loss: 34.0532 - val_MinusLogProbMetric: 34.0532 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 17/1000
2023-10-25 03:21:20.448 
Epoch 17/1000 
	 loss: 33.3958, MinusLogProbMetric: 33.3958, val_loss: 33.1129, val_MinusLogProbMetric: 33.1129

Epoch 17: val_loss improved from 33.23211 to 33.11292, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 33.3958 - MinusLogProbMetric: 33.3958 - val_loss: 33.1129 - val_MinusLogProbMetric: 33.1129 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 18/1000
2023-10-25 03:21:58.647 
Epoch 18/1000 
	 loss: 33.2678, MinusLogProbMetric: 33.2678, val_loss: 32.9567, val_MinusLogProbMetric: 32.9567

Epoch 18: val_loss improved from 33.11292 to 32.95669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 38s - loss: 33.2678 - MinusLogProbMetric: 33.2678 - val_loss: 32.9567 - val_MinusLogProbMetric: 32.9567 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 19/1000
2023-10-25 03:22:36.643 
Epoch 19/1000 
	 loss: 32.8151, MinusLogProbMetric: 32.8151, val_loss: 32.9825, val_MinusLogProbMetric: 32.9825

Epoch 19: val_loss did not improve from 32.95669
196/196 - 37s - loss: 32.8151 - MinusLogProbMetric: 32.8151 - val_loss: 32.9825 - val_MinusLogProbMetric: 32.9825 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 20/1000
2023-10-25 03:23:13.386 
Epoch 20/1000 
	 loss: 32.6725, MinusLogProbMetric: 32.6725, val_loss: 32.2996, val_MinusLogProbMetric: 32.2996

Epoch 20: val_loss improved from 32.95669 to 32.29961, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 38s - loss: 32.6725 - MinusLogProbMetric: 32.6725 - val_loss: 32.2996 - val_MinusLogProbMetric: 32.2996 - lr: 0.0010 - 38s/epoch - 191ms/step
Epoch 21/1000
2023-10-25 03:23:51.646 
Epoch 21/1000 
	 loss: 32.4966, MinusLogProbMetric: 32.4966, val_loss: 32.2929, val_MinusLogProbMetric: 32.2929

Epoch 21: val_loss improved from 32.29961 to 32.29291, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 38s - loss: 32.4966 - MinusLogProbMetric: 32.4966 - val_loss: 32.2929 - val_MinusLogProbMetric: 32.2929 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 22/1000
2023-10-25 03:24:29.903 
Epoch 22/1000 
	 loss: 32.2661, MinusLogProbMetric: 32.2661, val_loss: 31.7777, val_MinusLogProbMetric: 31.7777

Epoch 22: val_loss improved from 32.29291 to 31.77766, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 38s - loss: 32.2661 - MinusLogProbMetric: 32.2661 - val_loss: 31.7777 - val_MinusLogProbMetric: 31.7777 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 23/1000
2023-10-25 03:25:09.254 
Epoch 23/1000 
	 loss: 32.2409, MinusLogProbMetric: 32.2409, val_loss: 34.1635, val_MinusLogProbMetric: 34.1635

Epoch 23: val_loss did not improve from 31.77766
196/196 - 39s - loss: 32.2409 - MinusLogProbMetric: 32.2409 - val_loss: 34.1635 - val_MinusLogProbMetric: 34.1635 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 24/1000
2023-10-25 03:25:48.173 
Epoch 24/1000 
	 loss: 32.1059, MinusLogProbMetric: 32.1059, val_loss: 31.4453, val_MinusLogProbMetric: 31.4453

Epoch 24: val_loss improved from 31.77766 to 31.44527, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 40s - loss: 32.1059 - MinusLogProbMetric: 32.1059 - val_loss: 31.4453 - val_MinusLogProbMetric: 31.4453 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 25/1000
2023-10-25 03:26:25.982 
Epoch 25/1000 
	 loss: 31.9193, MinusLogProbMetric: 31.9193, val_loss: 31.3563, val_MinusLogProbMetric: 31.3563

Epoch 25: val_loss improved from 31.44527 to 31.35630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 38s - loss: 31.9193 - MinusLogProbMetric: 31.9193 - val_loss: 31.3563 - val_MinusLogProbMetric: 31.3563 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 26/1000
2023-10-25 03:27:03.629 
Epoch 26/1000 
	 loss: 32.0465, MinusLogProbMetric: 32.0465, val_loss: 32.3968, val_MinusLogProbMetric: 32.3968

Epoch 26: val_loss did not improve from 31.35630
196/196 - 37s - loss: 32.0465 - MinusLogProbMetric: 32.0465 - val_loss: 32.3968 - val_MinusLogProbMetric: 32.3968 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 27/1000
2023-10-25 03:27:41.802 
Epoch 27/1000 
	 loss: 31.7415, MinusLogProbMetric: 31.7415, val_loss: 32.4288, val_MinusLogProbMetric: 32.4288

Epoch 27: val_loss did not improve from 31.35630
196/196 - 38s - loss: 31.7415 - MinusLogProbMetric: 31.7415 - val_loss: 32.4288 - val_MinusLogProbMetric: 32.4288 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 28/1000
2023-10-25 03:28:20.513 
Epoch 28/1000 
	 loss: 31.7265, MinusLogProbMetric: 31.7265, val_loss: 31.9704, val_MinusLogProbMetric: 31.9704

Epoch 28: val_loss did not improve from 31.35630
196/196 - 39s - loss: 31.7265 - MinusLogProbMetric: 31.7265 - val_loss: 31.9704 - val_MinusLogProbMetric: 31.9704 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 29/1000
2023-10-25 03:28:59.177 
Epoch 29/1000 
	 loss: 31.4817, MinusLogProbMetric: 31.4817, val_loss: 31.7896, val_MinusLogProbMetric: 31.7896

Epoch 29: val_loss did not improve from 31.35630
196/196 - 39s - loss: 31.4817 - MinusLogProbMetric: 31.4817 - val_loss: 31.7896 - val_MinusLogProbMetric: 31.7896 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 30/1000
2023-10-25 03:29:36.953 
Epoch 30/1000 
	 loss: 31.3501, MinusLogProbMetric: 31.3501, val_loss: 32.1353, val_MinusLogProbMetric: 32.1353

Epoch 30: val_loss did not improve from 31.35630
196/196 - 38s - loss: 31.3501 - MinusLogProbMetric: 31.3501 - val_loss: 32.1353 - val_MinusLogProbMetric: 32.1353 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 31/1000
2023-10-25 03:30:14.390 
Epoch 31/1000 
	 loss: 31.5646, MinusLogProbMetric: 31.5646, val_loss: 32.7972, val_MinusLogProbMetric: 32.7972

Epoch 31: val_loss did not improve from 31.35630
196/196 - 37s - loss: 31.5646 - MinusLogProbMetric: 31.5646 - val_loss: 32.7972 - val_MinusLogProbMetric: 32.7972 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 32/1000
2023-10-25 03:30:52.727 
Epoch 32/1000 
	 loss: 31.2761, MinusLogProbMetric: 31.2761, val_loss: 32.3573, val_MinusLogProbMetric: 32.3573

Epoch 32: val_loss did not improve from 31.35630
196/196 - 38s - loss: 31.2761 - MinusLogProbMetric: 31.2761 - val_loss: 32.3573 - val_MinusLogProbMetric: 32.3573 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 33/1000
2023-10-25 03:31:31.216 
Epoch 33/1000 
	 loss: 31.0844, MinusLogProbMetric: 31.0844, val_loss: 31.6908, val_MinusLogProbMetric: 31.6908

Epoch 33: val_loss did not improve from 31.35630
196/196 - 38s - loss: 31.0844 - MinusLogProbMetric: 31.0844 - val_loss: 31.6908 - val_MinusLogProbMetric: 31.6908 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 34/1000
2023-10-25 03:32:09.980 
Epoch 34/1000 
	 loss: 31.1696, MinusLogProbMetric: 31.1696, val_loss: 30.9194, val_MinusLogProbMetric: 30.9194

Epoch 34: val_loss improved from 31.35630 to 30.91938, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 31.1696 - MinusLogProbMetric: 31.1696 - val_loss: 30.9194 - val_MinusLogProbMetric: 30.9194 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 35/1000
2023-10-25 03:32:48.468 
Epoch 35/1000 
	 loss: 31.0821, MinusLogProbMetric: 31.0821, val_loss: 33.5950, val_MinusLogProbMetric: 33.5950

Epoch 35: val_loss did not improve from 30.91938
196/196 - 38s - loss: 31.0821 - MinusLogProbMetric: 31.0821 - val_loss: 33.5950 - val_MinusLogProbMetric: 33.5950 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 36/1000
2023-10-25 03:33:26.233 
Epoch 36/1000 
	 loss: 31.1318, MinusLogProbMetric: 31.1318, val_loss: 31.4114, val_MinusLogProbMetric: 31.4114

Epoch 36: val_loss did not improve from 30.91938
196/196 - 38s - loss: 31.1318 - MinusLogProbMetric: 31.1318 - val_loss: 31.4114 - val_MinusLogProbMetric: 31.4114 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 37/1000
2023-10-25 03:34:04.534 
Epoch 37/1000 
	 loss: 30.7980, MinusLogProbMetric: 30.7980, val_loss: 30.7333, val_MinusLogProbMetric: 30.7333

Epoch 37: val_loss improved from 30.91938 to 30.73329, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 30.7980 - MinusLogProbMetric: 30.7980 - val_loss: 30.7333 - val_MinusLogProbMetric: 30.7333 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 38/1000
2023-10-25 03:34:42.533 
Epoch 38/1000 
	 loss: 30.8431, MinusLogProbMetric: 30.8431, val_loss: 30.7382, val_MinusLogProbMetric: 30.7382

Epoch 38: val_loss did not improve from 30.73329
196/196 - 37s - loss: 30.8431 - MinusLogProbMetric: 30.8431 - val_loss: 30.7382 - val_MinusLogProbMetric: 30.7382 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 39/1000
2023-10-25 03:35:19.553 
Epoch 39/1000 
	 loss: 30.8546, MinusLogProbMetric: 30.8546, val_loss: 31.4515, val_MinusLogProbMetric: 31.4515

Epoch 39: val_loss did not improve from 30.73329
196/196 - 37s - loss: 30.8546 - MinusLogProbMetric: 30.8546 - val_loss: 31.4515 - val_MinusLogProbMetric: 31.4515 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 40/1000
2023-10-25 03:35:57.247 
Epoch 40/1000 
	 loss: 30.7158, MinusLogProbMetric: 30.7158, val_loss: 31.1729, val_MinusLogProbMetric: 31.1729

Epoch 40: val_loss did not improve from 30.73329
196/196 - 38s - loss: 30.7158 - MinusLogProbMetric: 30.7158 - val_loss: 31.1729 - val_MinusLogProbMetric: 31.1729 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 41/1000
2023-10-25 03:36:34.891 
Epoch 41/1000 
	 loss: 30.7058, MinusLogProbMetric: 30.7058, val_loss: 31.2640, val_MinusLogProbMetric: 31.2640

Epoch 41: val_loss did not improve from 30.73329
196/196 - 38s - loss: 30.7058 - MinusLogProbMetric: 30.7058 - val_loss: 31.2640 - val_MinusLogProbMetric: 31.2640 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 42/1000
2023-10-25 03:37:11.405 
Epoch 42/1000 
	 loss: 30.5661, MinusLogProbMetric: 30.5661, val_loss: 30.9256, val_MinusLogProbMetric: 30.9256

Epoch 42: val_loss did not improve from 30.73329
196/196 - 37s - loss: 30.5661 - MinusLogProbMetric: 30.5661 - val_loss: 30.9256 - val_MinusLogProbMetric: 30.9256 - lr: 0.0010 - 37s/epoch - 186ms/step
Epoch 43/1000
2023-10-25 03:37:49.779 
Epoch 43/1000 
	 loss: 30.5994, MinusLogProbMetric: 30.5994, val_loss: 30.5120, val_MinusLogProbMetric: 30.5120

Epoch 43: val_loss improved from 30.73329 to 30.51204, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 30.5994 - MinusLogProbMetric: 30.5994 - val_loss: 30.5120 - val_MinusLogProbMetric: 30.5120 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 44/1000
2023-10-25 03:38:28.321 
Epoch 44/1000 
	 loss: 30.4670, MinusLogProbMetric: 30.4670, val_loss: 31.2388, val_MinusLogProbMetric: 31.2388

Epoch 44: val_loss did not improve from 30.51204
196/196 - 38s - loss: 30.4670 - MinusLogProbMetric: 30.4670 - val_loss: 31.2388 - val_MinusLogProbMetric: 31.2388 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 45/1000
2023-10-25 03:39:07.165 
Epoch 45/1000 
	 loss: 30.4910, MinusLogProbMetric: 30.4910, val_loss: 31.0466, val_MinusLogProbMetric: 31.0466

Epoch 45: val_loss did not improve from 30.51204
196/196 - 39s - loss: 30.4910 - MinusLogProbMetric: 30.4910 - val_loss: 31.0466 - val_MinusLogProbMetric: 31.0466 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 46/1000
2023-10-25 03:39:45.615 
Epoch 46/1000 
	 loss: 30.5188, MinusLogProbMetric: 30.5188, val_loss: 30.7247, val_MinusLogProbMetric: 30.7247

Epoch 46: val_loss did not improve from 30.51204
196/196 - 38s - loss: 30.5188 - MinusLogProbMetric: 30.5188 - val_loss: 30.7247 - val_MinusLogProbMetric: 30.7247 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 47/1000
2023-10-25 03:40:24.393 
Epoch 47/1000 
	 loss: 30.4126, MinusLogProbMetric: 30.4126, val_loss: 31.0169, val_MinusLogProbMetric: 31.0169

Epoch 47: val_loss did not improve from 30.51204
196/196 - 39s - loss: 30.4126 - MinusLogProbMetric: 30.4126 - val_loss: 31.0169 - val_MinusLogProbMetric: 31.0169 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 48/1000
2023-10-25 03:41:02.627 
Epoch 48/1000 
	 loss: 30.2439, MinusLogProbMetric: 30.2439, val_loss: 30.3817, val_MinusLogProbMetric: 30.3817

Epoch 48: val_loss improved from 30.51204 to 30.38174, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 30.2439 - MinusLogProbMetric: 30.2439 - val_loss: 30.3817 - val_MinusLogProbMetric: 30.3817 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 49/1000
2023-10-25 03:41:41.311 
Epoch 49/1000 
	 loss: 30.3864, MinusLogProbMetric: 30.3864, val_loss: 31.2642, val_MinusLogProbMetric: 31.2642

Epoch 49: val_loss did not improve from 30.38174
196/196 - 38s - loss: 30.3864 - MinusLogProbMetric: 30.3864 - val_loss: 31.2642 - val_MinusLogProbMetric: 31.2642 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 50/1000
2023-10-25 03:42:18.260 
Epoch 50/1000 
	 loss: 30.3154, MinusLogProbMetric: 30.3154, val_loss: 30.9663, val_MinusLogProbMetric: 30.9663

Epoch 50: val_loss did not improve from 30.38174
196/196 - 37s - loss: 30.3154 - MinusLogProbMetric: 30.3154 - val_loss: 30.9663 - val_MinusLogProbMetric: 30.9663 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 51/1000
2023-10-25 03:42:54.620 
Epoch 51/1000 
	 loss: 30.2221, MinusLogProbMetric: 30.2221, val_loss: 30.9904, val_MinusLogProbMetric: 30.9904

Epoch 51: val_loss did not improve from 30.38174
196/196 - 36s - loss: 30.2221 - MinusLogProbMetric: 30.2221 - val_loss: 30.9904 - val_MinusLogProbMetric: 30.9904 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 52/1000
2023-10-25 03:43:32.103 
Epoch 52/1000 
	 loss: 30.1770, MinusLogProbMetric: 30.1770, val_loss: 30.6184, val_MinusLogProbMetric: 30.6184

Epoch 52: val_loss did not improve from 30.38174
196/196 - 37s - loss: 30.1770 - MinusLogProbMetric: 30.1770 - val_loss: 30.6184 - val_MinusLogProbMetric: 30.6184 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 53/1000
2023-10-25 03:44:10.037 
Epoch 53/1000 
	 loss: 30.2071, MinusLogProbMetric: 30.2071, val_loss: 29.9931, val_MinusLogProbMetric: 29.9931

Epoch 53: val_loss improved from 30.38174 to 29.99309, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 30.2071 - MinusLogProbMetric: 30.2071 - val_loss: 29.9931 - val_MinusLogProbMetric: 29.9931 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 54/1000
2023-10-25 03:44:48.160 
Epoch 54/1000 
	 loss: 30.0941, MinusLogProbMetric: 30.0941, val_loss: 30.2324, val_MinusLogProbMetric: 30.2324

Epoch 54: val_loss did not improve from 29.99309
196/196 - 37s - loss: 30.0941 - MinusLogProbMetric: 30.0941 - val_loss: 30.2324 - val_MinusLogProbMetric: 30.2324 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 55/1000
2023-10-25 03:45:25.625 
Epoch 55/1000 
	 loss: 30.1835, MinusLogProbMetric: 30.1835, val_loss: 29.9815, val_MinusLogProbMetric: 29.9815

Epoch 55: val_loss improved from 29.99309 to 29.98149, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 38s - loss: 30.1835 - MinusLogProbMetric: 30.1835 - val_loss: 29.9815 - val_MinusLogProbMetric: 29.9815 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 56/1000
2023-10-25 03:46:03.960 
Epoch 56/1000 
	 loss: 29.9916, MinusLogProbMetric: 29.9916, val_loss: 30.2785, val_MinusLogProbMetric: 30.2785

Epoch 56: val_loss did not improve from 29.98149
196/196 - 38s - loss: 29.9916 - MinusLogProbMetric: 29.9916 - val_loss: 30.2785 - val_MinusLogProbMetric: 30.2785 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 57/1000
2023-10-25 03:46:42.767 
Epoch 57/1000 
	 loss: 30.1005, MinusLogProbMetric: 30.1005, val_loss: 30.1582, val_MinusLogProbMetric: 30.1582

Epoch 57: val_loss did not improve from 29.98149
196/196 - 39s - loss: 30.1005 - MinusLogProbMetric: 30.1005 - val_loss: 30.1582 - val_MinusLogProbMetric: 30.1582 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 58/1000
2023-10-25 03:47:20.041 
Epoch 58/1000 
	 loss: 29.9797, MinusLogProbMetric: 29.9797, val_loss: 30.4173, val_MinusLogProbMetric: 30.4173

Epoch 58: val_loss did not improve from 29.98149
196/196 - 37s - loss: 29.9797 - MinusLogProbMetric: 29.9797 - val_loss: 30.4173 - val_MinusLogProbMetric: 30.4173 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 59/1000
2023-10-25 03:47:55.385 
Epoch 59/1000 
	 loss: 29.9393, MinusLogProbMetric: 29.9393, val_loss: 29.9741, val_MinusLogProbMetric: 29.9741

Epoch 59: val_loss improved from 29.98149 to 29.97410, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 36s - loss: 29.9393 - MinusLogProbMetric: 29.9393 - val_loss: 29.9741 - val_MinusLogProbMetric: 29.9741 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 60/1000
2023-10-25 03:48:33.093 
Epoch 60/1000 
	 loss: 30.0509, MinusLogProbMetric: 30.0509, val_loss: 30.4014, val_MinusLogProbMetric: 30.4014

Epoch 60: val_loss did not improve from 29.97410
196/196 - 37s - loss: 30.0509 - MinusLogProbMetric: 30.0509 - val_loss: 30.4014 - val_MinusLogProbMetric: 30.4014 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 61/1000
2023-10-25 03:49:09.219 
Epoch 61/1000 
	 loss: 29.9767, MinusLogProbMetric: 29.9767, val_loss: 32.5868, val_MinusLogProbMetric: 32.5868

Epoch 61: val_loss did not improve from 29.97410
196/196 - 36s - loss: 29.9767 - MinusLogProbMetric: 29.9767 - val_loss: 32.5868 - val_MinusLogProbMetric: 32.5868 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 62/1000
2023-10-25 03:49:46.465 
Epoch 62/1000 
	 loss: 30.0480, MinusLogProbMetric: 30.0480, val_loss: 30.1873, val_MinusLogProbMetric: 30.1873

Epoch 62: val_loss did not improve from 29.97410
196/196 - 37s - loss: 30.0480 - MinusLogProbMetric: 30.0480 - val_loss: 30.1873 - val_MinusLogProbMetric: 30.1873 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 63/1000
2023-10-25 03:50:23.813 
Epoch 63/1000 
	 loss: 29.9754, MinusLogProbMetric: 29.9754, val_loss: 30.0335, val_MinusLogProbMetric: 30.0335

Epoch 63: val_loss did not improve from 29.97410
196/196 - 37s - loss: 29.9754 - MinusLogProbMetric: 29.9754 - val_loss: 30.0335 - val_MinusLogProbMetric: 30.0335 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 64/1000
2023-10-25 03:51:00.999 
Epoch 64/1000 
	 loss: 29.8113, MinusLogProbMetric: 29.8113, val_loss: 29.7724, val_MinusLogProbMetric: 29.7724

Epoch 64: val_loss improved from 29.97410 to 29.77244, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 38s - loss: 29.8113 - MinusLogProbMetric: 29.8113 - val_loss: 29.7724 - val_MinusLogProbMetric: 29.7724 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 65/1000
2023-10-25 03:51:38.452 
Epoch 65/1000 
	 loss: 29.9473, MinusLogProbMetric: 29.9473, val_loss: 29.8231, val_MinusLogProbMetric: 29.8231

Epoch 65: val_loss did not improve from 29.77244
196/196 - 37s - loss: 29.9473 - MinusLogProbMetric: 29.9473 - val_loss: 29.8231 - val_MinusLogProbMetric: 29.8231 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 66/1000
2023-10-25 03:52:15.899 
Epoch 66/1000 
	 loss: 29.8275, MinusLogProbMetric: 29.8275, val_loss: 29.7941, val_MinusLogProbMetric: 29.7941

Epoch 66: val_loss did not improve from 29.77244
196/196 - 37s - loss: 29.8275 - MinusLogProbMetric: 29.8275 - val_loss: 29.7941 - val_MinusLogProbMetric: 29.7941 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 67/1000
2023-10-25 03:52:53.994 
Epoch 67/1000 
	 loss: 29.8478, MinusLogProbMetric: 29.8478, val_loss: 30.1085, val_MinusLogProbMetric: 30.1085

Epoch 67: val_loss did not improve from 29.77244
196/196 - 38s - loss: 29.8478 - MinusLogProbMetric: 29.8478 - val_loss: 30.1085 - val_MinusLogProbMetric: 30.1085 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 68/1000
2023-10-25 03:53:32.623 
Epoch 68/1000 
	 loss: 29.7207, MinusLogProbMetric: 29.7207, val_loss: 29.5399, val_MinusLogProbMetric: 29.5399

Epoch 68: val_loss improved from 29.77244 to 29.53991, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 29.7207 - MinusLogProbMetric: 29.7207 - val_loss: 29.5399 - val_MinusLogProbMetric: 29.5399 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 69/1000
2023-10-25 03:54:10.208 
Epoch 69/1000 
	 loss: 29.8118, MinusLogProbMetric: 29.8118, val_loss: 30.0102, val_MinusLogProbMetric: 30.0102

Epoch 69: val_loss did not improve from 29.53991
196/196 - 37s - loss: 29.8118 - MinusLogProbMetric: 29.8118 - val_loss: 30.0102 - val_MinusLogProbMetric: 30.0102 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 70/1000
2023-10-25 03:54:48.548 
Epoch 70/1000 
	 loss: 29.6167, MinusLogProbMetric: 29.6167, val_loss: 30.1685, val_MinusLogProbMetric: 30.1685

Epoch 70: val_loss did not improve from 29.53991
196/196 - 38s - loss: 29.6167 - MinusLogProbMetric: 29.6167 - val_loss: 30.1685 - val_MinusLogProbMetric: 30.1685 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 71/1000
2023-10-25 03:55:25.533 
Epoch 71/1000 
	 loss: 29.7449, MinusLogProbMetric: 29.7449, val_loss: 31.2488, val_MinusLogProbMetric: 31.2488

Epoch 71: val_loss did not improve from 29.53991
196/196 - 37s - loss: 29.7449 - MinusLogProbMetric: 29.7449 - val_loss: 31.2488 - val_MinusLogProbMetric: 31.2488 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 72/1000
2023-10-25 03:56:02.852 
Epoch 72/1000 
	 loss: 29.6472, MinusLogProbMetric: 29.6472, val_loss: 29.7802, val_MinusLogProbMetric: 29.7802

Epoch 72: val_loss did not improve from 29.53991
196/196 - 37s - loss: 29.6472 - MinusLogProbMetric: 29.6472 - val_loss: 29.7802 - val_MinusLogProbMetric: 29.7802 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 73/1000
2023-10-25 03:56:40.455 
Epoch 73/1000 
	 loss: 29.6886, MinusLogProbMetric: 29.6886, val_loss: 30.1093, val_MinusLogProbMetric: 30.1093

Epoch 73: val_loss did not improve from 29.53991
196/196 - 38s - loss: 29.6886 - MinusLogProbMetric: 29.6886 - val_loss: 30.1093 - val_MinusLogProbMetric: 30.1093 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 74/1000
2023-10-25 03:57:18.532 
Epoch 74/1000 
	 loss: 29.6084, MinusLogProbMetric: 29.6084, val_loss: 29.4868, val_MinusLogProbMetric: 29.4868

Epoch 74: val_loss improved from 29.53991 to 29.48684, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 29.6084 - MinusLogProbMetric: 29.6084 - val_loss: 29.4868 - val_MinusLogProbMetric: 29.4868 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 75/1000
2023-10-25 03:57:56.216 
Epoch 75/1000 
	 loss: 29.6500, MinusLogProbMetric: 29.6500, val_loss: 29.5487, val_MinusLogProbMetric: 29.5487

Epoch 75: val_loss did not improve from 29.48684
196/196 - 37s - loss: 29.6500 - MinusLogProbMetric: 29.6500 - val_loss: 29.5487 - val_MinusLogProbMetric: 29.5487 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 76/1000
2023-10-25 03:58:33.391 
Epoch 76/1000 
	 loss: 29.5044, MinusLogProbMetric: 29.5044, val_loss: 29.3856, val_MinusLogProbMetric: 29.3856

Epoch 76: val_loss improved from 29.48684 to 29.38559, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 38s - loss: 29.5044 - MinusLogProbMetric: 29.5044 - val_loss: 29.3856 - val_MinusLogProbMetric: 29.3856 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 77/1000
2023-10-25 03:59:11.342 
Epoch 77/1000 
	 loss: 29.6263, MinusLogProbMetric: 29.6263, val_loss: 29.8504, val_MinusLogProbMetric: 29.8504

Epoch 77: val_loss did not improve from 29.38559
196/196 - 37s - loss: 29.6263 - MinusLogProbMetric: 29.6263 - val_loss: 29.8504 - val_MinusLogProbMetric: 29.8504 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 78/1000
2023-10-25 03:59:49.644 
Epoch 78/1000 
	 loss: 29.5168, MinusLogProbMetric: 29.5168, val_loss: 30.0567, val_MinusLogProbMetric: 30.0567

Epoch 78: val_loss did not improve from 29.38559
196/196 - 38s - loss: 29.5168 - MinusLogProbMetric: 29.5168 - val_loss: 30.0567 - val_MinusLogProbMetric: 30.0567 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 79/1000
2023-10-25 04:00:25.781 
Epoch 79/1000 
	 loss: 29.5895, MinusLogProbMetric: 29.5895, val_loss: 29.7852, val_MinusLogProbMetric: 29.7852

Epoch 79: val_loss did not improve from 29.38559
196/196 - 36s - loss: 29.5895 - MinusLogProbMetric: 29.5895 - val_loss: 29.7852 - val_MinusLogProbMetric: 29.7852 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 80/1000
2023-10-25 04:01:03.388 
Epoch 80/1000 
	 loss: 29.5624, MinusLogProbMetric: 29.5624, val_loss: 29.7431, val_MinusLogProbMetric: 29.7431

Epoch 80: val_loss did not improve from 29.38559
196/196 - 38s - loss: 29.5624 - MinusLogProbMetric: 29.5624 - val_loss: 29.7431 - val_MinusLogProbMetric: 29.7431 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 81/1000
2023-10-25 04:01:39.920 
Epoch 81/1000 
	 loss: 29.4717, MinusLogProbMetric: 29.4717, val_loss: 30.1320, val_MinusLogProbMetric: 30.1320

Epoch 81: val_loss did not improve from 29.38559
196/196 - 37s - loss: 29.4717 - MinusLogProbMetric: 29.4717 - val_loss: 30.1320 - val_MinusLogProbMetric: 30.1320 - lr: 0.0010 - 37s/epoch - 186ms/step
Epoch 82/1000
2023-10-25 04:02:17.058 
Epoch 82/1000 
	 loss: 29.3824, MinusLogProbMetric: 29.3824, val_loss: 29.6896, val_MinusLogProbMetric: 29.6896

Epoch 82: val_loss did not improve from 29.38559
196/196 - 37s - loss: 29.3824 - MinusLogProbMetric: 29.3824 - val_loss: 29.6896 - val_MinusLogProbMetric: 29.6896 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 83/1000
2023-10-25 04:02:54.927 
Epoch 83/1000 
	 loss: 29.6029, MinusLogProbMetric: 29.6029, val_loss: 29.6773, val_MinusLogProbMetric: 29.6773

Epoch 83: val_loss did not improve from 29.38559
196/196 - 38s - loss: 29.6029 - MinusLogProbMetric: 29.6029 - val_loss: 29.6773 - val_MinusLogProbMetric: 29.6773 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 84/1000
2023-10-25 04:03:30.674 
Epoch 84/1000 
	 loss: 29.3474, MinusLogProbMetric: 29.3474, val_loss: 29.6348, val_MinusLogProbMetric: 29.6348

Epoch 84: val_loss did not improve from 29.38559
196/196 - 36s - loss: 29.3474 - MinusLogProbMetric: 29.3474 - val_loss: 29.6348 - val_MinusLogProbMetric: 29.6348 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 85/1000
2023-10-25 04:04:08.819 
Epoch 85/1000 
	 loss: 29.4418, MinusLogProbMetric: 29.4418, val_loss: 29.6957, val_MinusLogProbMetric: 29.6957

Epoch 85: val_loss did not improve from 29.38559
196/196 - 38s - loss: 29.4418 - MinusLogProbMetric: 29.4418 - val_loss: 29.6957 - val_MinusLogProbMetric: 29.6957 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 86/1000
2023-10-25 04:04:47.119 
Epoch 86/1000 
	 loss: 29.4038, MinusLogProbMetric: 29.4038, val_loss: 30.5277, val_MinusLogProbMetric: 30.5277

Epoch 86: val_loss did not improve from 29.38559
196/196 - 38s - loss: 29.4038 - MinusLogProbMetric: 29.4038 - val_loss: 30.5277 - val_MinusLogProbMetric: 30.5277 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 87/1000
2023-10-25 04:05:25.585 
Epoch 87/1000 
	 loss: 29.4689, MinusLogProbMetric: 29.4689, val_loss: 30.3445, val_MinusLogProbMetric: 30.3445

Epoch 87: val_loss did not improve from 29.38559
196/196 - 38s - loss: 29.4689 - MinusLogProbMetric: 29.4689 - val_loss: 30.3445 - val_MinusLogProbMetric: 30.3445 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 88/1000
2023-10-25 04:06:03.090 
Epoch 88/1000 
	 loss: 29.3941, MinusLogProbMetric: 29.3941, val_loss: 29.8354, val_MinusLogProbMetric: 29.8354

Epoch 88: val_loss did not improve from 29.38559
196/196 - 38s - loss: 29.3941 - MinusLogProbMetric: 29.3941 - val_loss: 29.8354 - val_MinusLogProbMetric: 29.8354 - lr: 0.0010 - 38s/epoch - 191ms/step
Epoch 89/1000
2023-10-25 04:06:40.658 
Epoch 89/1000 
	 loss: 29.3358, MinusLogProbMetric: 29.3358, val_loss: 29.3396, val_MinusLogProbMetric: 29.3396

Epoch 89: val_loss improved from 29.38559 to 29.33961, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 38s - loss: 29.3358 - MinusLogProbMetric: 29.3358 - val_loss: 29.3396 - val_MinusLogProbMetric: 29.3396 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 90/1000
2023-10-25 04:07:17.753 
Epoch 90/1000 
	 loss: 29.3016, MinusLogProbMetric: 29.3016, val_loss: 29.7520, val_MinusLogProbMetric: 29.7520

Epoch 90: val_loss did not improve from 29.33961
196/196 - 36s - loss: 29.3016 - MinusLogProbMetric: 29.3016 - val_loss: 29.7520 - val_MinusLogProbMetric: 29.7520 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 91/1000
2023-10-25 04:07:54.335 
Epoch 91/1000 
	 loss: 29.2623, MinusLogProbMetric: 29.2623, val_loss: 31.6948, val_MinusLogProbMetric: 31.6948

Epoch 91: val_loss did not improve from 29.33961
196/196 - 37s - loss: 29.2623 - MinusLogProbMetric: 29.2623 - val_loss: 31.6948 - val_MinusLogProbMetric: 31.6948 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 92/1000
2023-10-25 04:08:30.219 
Epoch 92/1000 
	 loss: 29.2018, MinusLogProbMetric: 29.2018, val_loss: 30.1112, val_MinusLogProbMetric: 30.1112

Epoch 92: val_loss did not improve from 29.33961
196/196 - 36s - loss: 29.2018 - MinusLogProbMetric: 29.2018 - val_loss: 30.1112 - val_MinusLogProbMetric: 30.1112 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 93/1000
2023-10-25 04:09:05.740 
Epoch 93/1000 
	 loss: 29.2621, MinusLogProbMetric: 29.2621, val_loss: 29.9698, val_MinusLogProbMetric: 29.9698

Epoch 93: val_loss did not improve from 29.33961
196/196 - 36s - loss: 29.2621 - MinusLogProbMetric: 29.2621 - val_loss: 29.9698 - val_MinusLogProbMetric: 29.9698 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 94/1000
2023-10-25 04:09:43.662 
Epoch 94/1000 
	 loss: 29.3238, MinusLogProbMetric: 29.3238, val_loss: 30.1870, val_MinusLogProbMetric: 30.1870

Epoch 94: val_loss did not improve from 29.33961
196/196 - 38s - loss: 29.3238 - MinusLogProbMetric: 29.3238 - val_loss: 30.1870 - val_MinusLogProbMetric: 30.1870 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 95/1000
2023-10-25 04:10:22.139 
Epoch 95/1000 
	 loss: 29.2127, MinusLogProbMetric: 29.2127, val_loss: 29.7698, val_MinusLogProbMetric: 29.7698

Epoch 95: val_loss did not improve from 29.33961
196/196 - 38s - loss: 29.2127 - MinusLogProbMetric: 29.2127 - val_loss: 29.7698 - val_MinusLogProbMetric: 29.7698 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 96/1000
2023-10-25 04:11:00.171 
Epoch 96/1000 
	 loss: 29.2975, MinusLogProbMetric: 29.2975, val_loss: 29.8196, val_MinusLogProbMetric: 29.8196

Epoch 96: val_loss did not improve from 29.33961
196/196 - 38s - loss: 29.2975 - MinusLogProbMetric: 29.2975 - val_loss: 29.8196 - val_MinusLogProbMetric: 29.8196 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 97/1000
2023-10-25 04:11:37.211 
Epoch 97/1000 
	 loss: 29.3680, MinusLogProbMetric: 29.3680, val_loss: 29.6746, val_MinusLogProbMetric: 29.6746

Epoch 97: val_loss did not improve from 29.33961
196/196 - 37s - loss: 29.3680 - MinusLogProbMetric: 29.3680 - val_loss: 29.6746 - val_MinusLogProbMetric: 29.6746 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 98/1000
2023-10-25 04:12:14.074 
Epoch 98/1000 
	 loss: 29.1651, MinusLogProbMetric: 29.1651, val_loss: 29.7525, val_MinusLogProbMetric: 29.7525

Epoch 98: val_loss did not improve from 29.33961
196/196 - 37s - loss: 29.1651 - MinusLogProbMetric: 29.1651 - val_loss: 29.7525 - val_MinusLogProbMetric: 29.7525 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 99/1000
2023-10-25 04:12:51.078 
Epoch 99/1000 
	 loss: 29.2459, MinusLogProbMetric: 29.2459, val_loss: 29.5130, val_MinusLogProbMetric: 29.5130

Epoch 99: val_loss did not improve from 29.33961
196/196 - 37s - loss: 29.2459 - MinusLogProbMetric: 29.2459 - val_loss: 29.5130 - val_MinusLogProbMetric: 29.5130 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 100/1000
2023-10-25 04:13:29.428 
Epoch 100/1000 
	 loss: 29.3660, MinusLogProbMetric: 29.3660, val_loss: 29.4738, val_MinusLogProbMetric: 29.4738

Epoch 100: val_loss did not improve from 29.33961
196/196 - 38s - loss: 29.3660 - MinusLogProbMetric: 29.3660 - val_loss: 29.4738 - val_MinusLogProbMetric: 29.4738 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 101/1000
2023-10-25 04:14:07.412 
Epoch 101/1000 
	 loss: 29.0672, MinusLogProbMetric: 29.0672, val_loss: 29.8482, val_MinusLogProbMetric: 29.8482

Epoch 101: val_loss did not improve from 29.33961
196/196 - 38s - loss: 29.0672 - MinusLogProbMetric: 29.0672 - val_loss: 29.8482 - val_MinusLogProbMetric: 29.8482 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 102/1000
2023-10-25 04:14:44.105 
Epoch 102/1000 
	 loss: 29.2172, MinusLogProbMetric: 29.2172, val_loss: 33.9402, val_MinusLogProbMetric: 33.9402

Epoch 102: val_loss did not improve from 29.33961
196/196 - 37s - loss: 29.2172 - MinusLogProbMetric: 29.2172 - val_loss: 33.9402 - val_MinusLogProbMetric: 33.9402 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 103/1000
2023-10-25 04:15:19.953 
Epoch 103/1000 
	 loss: 29.2835, MinusLogProbMetric: 29.2835, val_loss: 29.1875, val_MinusLogProbMetric: 29.1875

Epoch 103: val_loss improved from 29.33961 to 29.18751, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 37s - loss: 29.2835 - MinusLogProbMetric: 29.2835 - val_loss: 29.1875 - val_MinusLogProbMetric: 29.1875 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 104/1000
2023-10-25 04:15:59.027 
Epoch 104/1000 
	 loss: 29.1548, MinusLogProbMetric: 29.1548, val_loss: 29.1334, val_MinusLogProbMetric: 29.1334

Epoch 104: val_loss improved from 29.18751 to 29.13340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 29.1548 - MinusLogProbMetric: 29.1548 - val_loss: 29.1334 - val_MinusLogProbMetric: 29.1334 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 105/1000
2023-10-25 04:16:38.839 
Epoch 105/1000 
	 loss: 29.2462, MinusLogProbMetric: 29.2462, val_loss: 29.5096, val_MinusLogProbMetric: 29.5096

Epoch 105: val_loss did not improve from 29.13340
196/196 - 39s - loss: 29.2462 - MinusLogProbMetric: 29.2462 - val_loss: 29.5096 - val_MinusLogProbMetric: 29.5096 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 106/1000
2023-10-25 04:17:17.432 
Epoch 106/1000 
	 loss: 29.0630, MinusLogProbMetric: 29.0630, val_loss: 29.4400, val_MinusLogProbMetric: 29.4400

Epoch 106: val_loss did not improve from 29.13340
196/196 - 39s - loss: 29.0630 - MinusLogProbMetric: 29.0630 - val_loss: 29.4400 - val_MinusLogProbMetric: 29.4400 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 107/1000
2023-10-25 04:17:56.370 
Epoch 107/1000 
	 loss: 29.0299, MinusLogProbMetric: 29.0299, val_loss: 30.1837, val_MinusLogProbMetric: 30.1837

Epoch 107: val_loss did not improve from 29.13340
196/196 - 39s - loss: 29.0299 - MinusLogProbMetric: 29.0299 - val_loss: 30.1837 - val_MinusLogProbMetric: 30.1837 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 108/1000
2023-10-25 04:18:33.567 
Epoch 108/1000 
	 loss: 29.0990, MinusLogProbMetric: 29.0990, val_loss: 29.5853, val_MinusLogProbMetric: 29.5853

Epoch 108: val_loss did not improve from 29.13340
196/196 - 37s - loss: 29.0990 - MinusLogProbMetric: 29.0990 - val_loss: 29.5853 - val_MinusLogProbMetric: 29.5853 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 109/1000
2023-10-25 04:19:11.744 
Epoch 109/1000 
	 loss: 29.0413, MinusLogProbMetric: 29.0413, val_loss: 29.0139, val_MinusLogProbMetric: 29.0139

Epoch 109: val_loss improved from 29.13340 to 29.01386, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 29.0413 - MinusLogProbMetric: 29.0413 - val_loss: 29.0139 - val_MinusLogProbMetric: 29.0139 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 110/1000
2023-10-25 04:19:51.256 
Epoch 110/1000 
	 loss: 29.1157, MinusLogProbMetric: 29.1157, val_loss: 29.5932, val_MinusLogProbMetric: 29.5932

Epoch 110: val_loss did not improve from 29.01386
196/196 - 39s - loss: 29.1157 - MinusLogProbMetric: 29.1157 - val_loss: 29.5932 - val_MinusLogProbMetric: 29.5932 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 111/1000
2023-10-25 04:20:28.330 
Epoch 111/1000 
	 loss: 29.0494, MinusLogProbMetric: 29.0494, val_loss: 29.8766, val_MinusLogProbMetric: 29.8766

Epoch 111: val_loss did not improve from 29.01386
196/196 - 37s - loss: 29.0494 - MinusLogProbMetric: 29.0494 - val_loss: 29.8766 - val_MinusLogProbMetric: 29.8766 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 112/1000
2023-10-25 04:21:06.827 
Epoch 112/1000 
	 loss: 28.9924, MinusLogProbMetric: 28.9924, val_loss: 30.0859, val_MinusLogProbMetric: 30.0859

Epoch 112: val_loss did not improve from 29.01386
196/196 - 38s - loss: 28.9924 - MinusLogProbMetric: 28.9924 - val_loss: 30.0859 - val_MinusLogProbMetric: 30.0859 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 113/1000
2023-10-25 04:21:44.336 
Epoch 113/1000 
	 loss: 29.0646, MinusLogProbMetric: 29.0646, val_loss: 29.4709, val_MinusLogProbMetric: 29.4709

Epoch 113: val_loss did not improve from 29.01386
196/196 - 38s - loss: 29.0646 - MinusLogProbMetric: 29.0646 - val_loss: 29.4709 - val_MinusLogProbMetric: 29.4709 - lr: 0.0010 - 38s/epoch - 191ms/step
Epoch 114/1000
2023-10-25 04:22:21.680 
Epoch 114/1000 
	 loss: 28.9608, MinusLogProbMetric: 28.9608, val_loss: 29.0985, val_MinusLogProbMetric: 29.0985

Epoch 114: val_loss did not improve from 29.01386
196/196 - 37s - loss: 28.9608 - MinusLogProbMetric: 28.9608 - val_loss: 29.0985 - val_MinusLogProbMetric: 29.0985 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 115/1000
2023-10-25 04:22:59.901 
Epoch 115/1000 
	 loss: 29.0785, MinusLogProbMetric: 29.0785, val_loss: 29.5855, val_MinusLogProbMetric: 29.5855

Epoch 115: val_loss did not improve from 29.01386
196/196 - 38s - loss: 29.0785 - MinusLogProbMetric: 29.0785 - val_loss: 29.5855 - val_MinusLogProbMetric: 29.5855 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 116/1000
2023-10-25 04:23:36.758 
Epoch 116/1000 
	 loss: 29.0549, MinusLogProbMetric: 29.0549, val_loss: 29.2979, val_MinusLogProbMetric: 29.2979

Epoch 116: val_loss did not improve from 29.01386
196/196 - 37s - loss: 29.0549 - MinusLogProbMetric: 29.0549 - val_loss: 29.2979 - val_MinusLogProbMetric: 29.2979 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 117/1000
2023-10-25 04:24:13.903 
Epoch 117/1000 
	 loss: 28.9610, MinusLogProbMetric: 28.9610, val_loss: 29.7854, val_MinusLogProbMetric: 29.7854

Epoch 117: val_loss did not improve from 29.01386
196/196 - 37s - loss: 28.9610 - MinusLogProbMetric: 28.9610 - val_loss: 29.7854 - val_MinusLogProbMetric: 29.7854 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 118/1000
2023-10-25 04:24:52.029 
Epoch 118/1000 
	 loss: 29.0027, MinusLogProbMetric: 29.0027, val_loss: 28.9169, val_MinusLogProbMetric: 28.9169

Epoch 118: val_loss improved from 29.01386 to 28.91693, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 29.0027 - MinusLogProbMetric: 29.0027 - val_loss: 28.9169 - val_MinusLogProbMetric: 28.9169 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 119/1000
2023-10-25 04:25:28.276 
Epoch 119/1000 
	 loss: 28.9163, MinusLogProbMetric: 28.9163, val_loss: 28.8553, val_MinusLogProbMetric: 28.8553

Epoch 119: val_loss improved from 28.91693 to 28.85535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 36s - loss: 28.9163 - MinusLogProbMetric: 28.9163 - val_loss: 28.8553 - val_MinusLogProbMetric: 28.8553 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 120/1000
2023-10-25 04:26:07.123 
Epoch 120/1000 
	 loss: 28.9160, MinusLogProbMetric: 28.9160, val_loss: 28.9067, val_MinusLogProbMetric: 28.9067

Epoch 120: val_loss did not improve from 28.85535
196/196 - 38s - loss: 28.9160 - MinusLogProbMetric: 28.9160 - val_loss: 28.9067 - val_MinusLogProbMetric: 28.9067 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 121/1000
2023-10-25 04:26:43.780 
Epoch 121/1000 
	 loss: 28.9241, MinusLogProbMetric: 28.9241, val_loss: 29.6231, val_MinusLogProbMetric: 29.6231

Epoch 121: val_loss did not improve from 28.85535
196/196 - 37s - loss: 28.9241 - MinusLogProbMetric: 28.9241 - val_loss: 29.6231 - val_MinusLogProbMetric: 29.6231 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 122/1000
2023-10-25 04:27:21.275 
Epoch 122/1000 
	 loss: 28.8297, MinusLogProbMetric: 28.8297, val_loss: 29.3114, val_MinusLogProbMetric: 29.3114

Epoch 122: val_loss did not improve from 28.85535
196/196 - 37s - loss: 28.8297 - MinusLogProbMetric: 28.8297 - val_loss: 29.3114 - val_MinusLogProbMetric: 29.3114 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 123/1000
2023-10-25 04:28:00.192 
Epoch 123/1000 
	 loss: 28.9461, MinusLogProbMetric: 28.9461, val_loss: 29.1450, val_MinusLogProbMetric: 29.1450

Epoch 123: val_loss did not improve from 28.85535
196/196 - 39s - loss: 28.9461 - MinusLogProbMetric: 28.9461 - val_loss: 29.1450 - val_MinusLogProbMetric: 29.1450 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 124/1000
2023-10-25 04:28:37.841 
Epoch 124/1000 
	 loss: 28.7976, MinusLogProbMetric: 28.7976, val_loss: 28.9856, val_MinusLogProbMetric: 28.9856

Epoch 124: val_loss did not improve from 28.85535
196/196 - 38s - loss: 28.7976 - MinusLogProbMetric: 28.7976 - val_loss: 28.9856 - val_MinusLogProbMetric: 28.9856 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 125/1000
2023-10-25 04:29:14.013 
Epoch 125/1000 
	 loss: 28.9835, MinusLogProbMetric: 28.9835, val_loss: 29.1424, val_MinusLogProbMetric: 29.1424

Epoch 125: val_loss did not improve from 28.85535
196/196 - 36s - loss: 28.9835 - MinusLogProbMetric: 28.9835 - val_loss: 29.1424 - val_MinusLogProbMetric: 29.1424 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 126/1000
2023-10-25 04:29:49.173 
Epoch 126/1000 
	 loss: 28.8159, MinusLogProbMetric: 28.8159, val_loss: 29.6139, val_MinusLogProbMetric: 29.6139

Epoch 126: val_loss did not improve from 28.85535
196/196 - 35s - loss: 28.8159 - MinusLogProbMetric: 28.8159 - val_loss: 29.6139 - val_MinusLogProbMetric: 29.6139 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 127/1000
2023-10-25 04:30:25.734 
Epoch 127/1000 
	 loss: 29.0105, MinusLogProbMetric: 29.0105, val_loss: 29.6971, val_MinusLogProbMetric: 29.6971

Epoch 127: val_loss did not improve from 28.85535
196/196 - 37s - loss: 29.0105 - MinusLogProbMetric: 29.0105 - val_loss: 29.6971 - val_MinusLogProbMetric: 29.6971 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 128/1000
2023-10-25 04:31:03.192 
Epoch 128/1000 
	 loss: 28.9212, MinusLogProbMetric: 28.9212, val_loss: 29.3231, val_MinusLogProbMetric: 29.3231

Epoch 128: val_loss did not improve from 28.85535
196/196 - 37s - loss: 28.9212 - MinusLogProbMetric: 28.9212 - val_loss: 29.3231 - val_MinusLogProbMetric: 29.3231 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 129/1000
2023-10-25 04:31:41.839 
Epoch 129/1000 
	 loss: 28.8271, MinusLogProbMetric: 28.8271, val_loss: 30.3328, val_MinusLogProbMetric: 30.3328

Epoch 129: val_loss did not improve from 28.85535
196/196 - 39s - loss: 28.8271 - MinusLogProbMetric: 28.8271 - val_loss: 30.3328 - val_MinusLogProbMetric: 30.3328 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 130/1000
2023-10-25 04:32:18.892 
Epoch 130/1000 
	 loss: 28.8706, MinusLogProbMetric: 28.8706, val_loss: 29.1391, val_MinusLogProbMetric: 29.1391

Epoch 130: val_loss did not improve from 28.85535
196/196 - 37s - loss: 28.8706 - MinusLogProbMetric: 28.8706 - val_loss: 29.1391 - val_MinusLogProbMetric: 29.1391 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 131/1000
2023-10-25 04:32:57.322 
Epoch 131/1000 
	 loss: 28.9629, MinusLogProbMetric: 28.9629, val_loss: 29.0538, val_MinusLogProbMetric: 29.0538

Epoch 131: val_loss did not improve from 28.85535
196/196 - 38s - loss: 28.9629 - MinusLogProbMetric: 28.9629 - val_loss: 29.0538 - val_MinusLogProbMetric: 29.0538 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 132/1000
2023-10-25 04:33:33.610 
Epoch 132/1000 
	 loss: 28.8243, MinusLogProbMetric: 28.8243, val_loss: 28.9744, val_MinusLogProbMetric: 28.9744

Epoch 132: val_loss did not improve from 28.85535
196/196 - 36s - loss: 28.8243 - MinusLogProbMetric: 28.8243 - val_loss: 28.9744 - val_MinusLogProbMetric: 28.9744 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 133/1000
2023-10-25 04:34:12.082 
Epoch 133/1000 
	 loss: 28.8185, MinusLogProbMetric: 28.8185, val_loss: 29.3638, val_MinusLogProbMetric: 29.3638

Epoch 133: val_loss did not improve from 28.85535
196/196 - 38s - loss: 28.8185 - MinusLogProbMetric: 28.8185 - val_loss: 29.3638 - val_MinusLogProbMetric: 29.3638 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 134/1000
2023-10-25 04:34:47.553 
Epoch 134/1000 
	 loss: 28.7804, MinusLogProbMetric: 28.7804, val_loss: 29.3889, val_MinusLogProbMetric: 29.3889

Epoch 134: val_loss did not improve from 28.85535
196/196 - 35s - loss: 28.7804 - MinusLogProbMetric: 28.7804 - val_loss: 29.3889 - val_MinusLogProbMetric: 29.3889 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 135/1000
2023-10-25 04:35:26.072 
Epoch 135/1000 
	 loss: 28.9035, MinusLogProbMetric: 28.9035, val_loss: 29.1170, val_MinusLogProbMetric: 29.1170

Epoch 135: val_loss did not improve from 28.85535
196/196 - 39s - loss: 28.9035 - MinusLogProbMetric: 28.9035 - val_loss: 29.1170 - val_MinusLogProbMetric: 29.1170 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 136/1000
2023-10-25 04:36:04.404 
Epoch 136/1000 
	 loss: 28.7630, MinusLogProbMetric: 28.7630, val_loss: 29.1194, val_MinusLogProbMetric: 29.1194

Epoch 136: val_loss did not improve from 28.85535
196/196 - 38s - loss: 28.7630 - MinusLogProbMetric: 28.7630 - val_loss: 29.1194 - val_MinusLogProbMetric: 29.1194 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 137/1000
2023-10-25 04:36:42.558 
Epoch 137/1000 
	 loss: 28.7698, MinusLogProbMetric: 28.7698, val_loss: 28.9601, val_MinusLogProbMetric: 28.9601

Epoch 137: val_loss did not improve from 28.85535
196/196 - 38s - loss: 28.7698 - MinusLogProbMetric: 28.7698 - val_loss: 28.9601 - val_MinusLogProbMetric: 28.9601 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 138/1000
2023-10-25 04:37:21.077 
Epoch 138/1000 
	 loss: 28.8213, MinusLogProbMetric: 28.8213, val_loss: 28.9659, val_MinusLogProbMetric: 28.9659

Epoch 138: val_loss did not improve from 28.85535
196/196 - 39s - loss: 28.8213 - MinusLogProbMetric: 28.8213 - val_loss: 28.9659 - val_MinusLogProbMetric: 28.9659 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 139/1000
2023-10-25 04:37:58.769 
Epoch 139/1000 
	 loss: 28.9677, MinusLogProbMetric: 28.9677, val_loss: 29.6047, val_MinusLogProbMetric: 29.6047

Epoch 139: val_loss did not improve from 28.85535
196/196 - 38s - loss: 28.9677 - MinusLogProbMetric: 28.9677 - val_loss: 29.6047 - val_MinusLogProbMetric: 29.6047 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 140/1000
2023-10-25 04:38:37.295 
Epoch 140/1000 
	 loss: 28.7633, MinusLogProbMetric: 28.7633, val_loss: 28.9885, val_MinusLogProbMetric: 28.9885

Epoch 140: val_loss did not improve from 28.85535
196/196 - 39s - loss: 28.7633 - MinusLogProbMetric: 28.7633 - val_loss: 28.9885 - val_MinusLogProbMetric: 28.9885 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 141/1000
2023-10-25 04:39:14.489 
Epoch 141/1000 
	 loss: 28.7077, MinusLogProbMetric: 28.7077, val_loss: 29.5049, val_MinusLogProbMetric: 29.5049

Epoch 141: val_loss did not improve from 28.85535
196/196 - 37s - loss: 28.7077 - MinusLogProbMetric: 28.7077 - val_loss: 29.5049 - val_MinusLogProbMetric: 29.5049 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 142/1000
2023-10-25 04:39:53.318 
Epoch 142/1000 
	 loss: 28.6977, MinusLogProbMetric: 28.6977, val_loss: 29.1300, val_MinusLogProbMetric: 29.1300

Epoch 142: val_loss did not improve from 28.85535
196/196 - 39s - loss: 28.6977 - MinusLogProbMetric: 28.6977 - val_loss: 29.1300 - val_MinusLogProbMetric: 29.1300 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 143/1000
2023-10-25 04:40:32.311 
Epoch 143/1000 
	 loss: 28.5970, MinusLogProbMetric: 28.5970, val_loss: 29.5995, val_MinusLogProbMetric: 29.5995

Epoch 143: val_loss did not improve from 28.85535
196/196 - 39s - loss: 28.5970 - MinusLogProbMetric: 28.5970 - val_loss: 29.5995 - val_MinusLogProbMetric: 29.5995 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 144/1000
2023-10-25 04:41:09.704 
Epoch 144/1000 
	 loss: 28.8441, MinusLogProbMetric: 28.8441, val_loss: 28.9592, val_MinusLogProbMetric: 28.9592

Epoch 144: val_loss did not improve from 28.85535
196/196 - 37s - loss: 28.8441 - MinusLogProbMetric: 28.8441 - val_loss: 28.9592 - val_MinusLogProbMetric: 28.9592 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 145/1000
2023-10-25 04:41:48.382 
Epoch 145/1000 
	 loss: 28.7107, MinusLogProbMetric: 28.7107, val_loss: 28.8950, val_MinusLogProbMetric: 28.8950

Epoch 145: val_loss did not improve from 28.85535
196/196 - 39s - loss: 28.7107 - MinusLogProbMetric: 28.7107 - val_loss: 28.8950 - val_MinusLogProbMetric: 28.8950 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 146/1000
2023-10-25 04:42:26.995 
Epoch 146/1000 
	 loss: 28.6587, MinusLogProbMetric: 28.6587, val_loss: 29.4696, val_MinusLogProbMetric: 29.4696

Epoch 146: val_loss did not improve from 28.85535
196/196 - 39s - loss: 28.6587 - MinusLogProbMetric: 28.6587 - val_loss: 29.4696 - val_MinusLogProbMetric: 29.4696 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 147/1000
2023-10-25 04:43:03.999 
Epoch 147/1000 
	 loss: 28.6810, MinusLogProbMetric: 28.6810, val_loss: 29.5586, val_MinusLogProbMetric: 29.5586

Epoch 147: val_loss did not improve from 28.85535
196/196 - 37s - loss: 28.6810 - MinusLogProbMetric: 28.6810 - val_loss: 29.5586 - val_MinusLogProbMetric: 29.5586 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 148/1000
2023-10-25 04:43:41.191 
Epoch 148/1000 
	 loss: 28.8044, MinusLogProbMetric: 28.8044, val_loss: 29.9206, val_MinusLogProbMetric: 29.9206

Epoch 148: val_loss did not improve from 28.85535
196/196 - 37s - loss: 28.8044 - MinusLogProbMetric: 28.8044 - val_loss: 29.9206 - val_MinusLogProbMetric: 29.9206 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 149/1000
2023-10-25 04:44:19.147 
Epoch 149/1000 
	 loss: 28.5956, MinusLogProbMetric: 28.5956, val_loss: 29.2922, val_MinusLogProbMetric: 29.2922

Epoch 149: val_loss did not improve from 28.85535
196/196 - 38s - loss: 28.5956 - MinusLogProbMetric: 28.5956 - val_loss: 29.2922 - val_MinusLogProbMetric: 29.2922 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 150/1000
2023-10-25 04:44:57.402 
Epoch 150/1000 
	 loss: 28.7186, MinusLogProbMetric: 28.7186, val_loss: 29.0735, val_MinusLogProbMetric: 29.0735

Epoch 150: val_loss did not improve from 28.85535
196/196 - 38s - loss: 28.7186 - MinusLogProbMetric: 28.7186 - val_loss: 29.0735 - val_MinusLogProbMetric: 29.0735 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 151/1000
2023-10-25 04:45:36.022 
Epoch 151/1000 
	 loss: 28.5878, MinusLogProbMetric: 28.5878, val_loss: 29.1966, val_MinusLogProbMetric: 29.1966

Epoch 151: val_loss did not improve from 28.85535
196/196 - 39s - loss: 28.5878 - MinusLogProbMetric: 28.5878 - val_loss: 29.1966 - val_MinusLogProbMetric: 29.1966 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 152/1000
2023-10-25 04:46:12.999 
Epoch 152/1000 
	 loss: 28.6657, MinusLogProbMetric: 28.6657, val_loss: 28.9313, val_MinusLogProbMetric: 28.9313

Epoch 152: val_loss did not improve from 28.85535
196/196 - 37s - loss: 28.6657 - MinusLogProbMetric: 28.6657 - val_loss: 28.9313 - val_MinusLogProbMetric: 28.9313 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 153/1000
2023-10-25 04:46:49.864 
Epoch 153/1000 
	 loss: 28.5993, MinusLogProbMetric: 28.5993, val_loss: 29.8616, val_MinusLogProbMetric: 29.8616

Epoch 153: val_loss did not improve from 28.85535
196/196 - 37s - loss: 28.5993 - MinusLogProbMetric: 28.5993 - val_loss: 29.8616 - val_MinusLogProbMetric: 29.8616 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 154/1000
2023-10-25 04:47:28.080 
Epoch 154/1000 
	 loss: 28.5793, MinusLogProbMetric: 28.5793, val_loss: 29.2521, val_MinusLogProbMetric: 29.2521

Epoch 154: val_loss did not improve from 28.85535
196/196 - 38s - loss: 28.5793 - MinusLogProbMetric: 28.5793 - val_loss: 29.2521 - val_MinusLogProbMetric: 29.2521 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 155/1000
2023-10-25 04:48:06.823 
Epoch 155/1000 
	 loss: 28.5602, MinusLogProbMetric: 28.5602, val_loss: 28.9691, val_MinusLogProbMetric: 28.9691

Epoch 155: val_loss did not improve from 28.85535
196/196 - 39s - loss: 28.5602 - MinusLogProbMetric: 28.5602 - val_loss: 28.9691 - val_MinusLogProbMetric: 28.9691 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 156/1000
2023-10-25 04:48:45.713 
Epoch 156/1000 
	 loss: 28.5355, MinusLogProbMetric: 28.5355, val_loss: 29.1757, val_MinusLogProbMetric: 29.1757

Epoch 156: val_loss did not improve from 28.85535
196/196 - 39s - loss: 28.5355 - MinusLogProbMetric: 28.5355 - val_loss: 29.1757 - val_MinusLogProbMetric: 29.1757 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 157/1000
2023-10-25 04:49:23.514 
Epoch 157/1000 
	 loss: 28.5869, MinusLogProbMetric: 28.5869, val_loss: 28.8000, val_MinusLogProbMetric: 28.8000

Epoch 157: val_loss improved from 28.85535 to 28.80002, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 28.5869 - MinusLogProbMetric: 28.5869 - val_loss: 28.8000 - val_MinusLogProbMetric: 28.8000 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 158/1000
2023-10-25 04:50:02.409 
Epoch 158/1000 
	 loss: 28.5946, MinusLogProbMetric: 28.5946, val_loss: 28.9837, val_MinusLogProbMetric: 28.9837

Epoch 158: val_loss did not improve from 28.80002
196/196 - 38s - loss: 28.5946 - MinusLogProbMetric: 28.5946 - val_loss: 28.9837 - val_MinusLogProbMetric: 28.9837 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 159/1000
2023-10-25 04:50:41.151 
Epoch 159/1000 
	 loss: 28.5835, MinusLogProbMetric: 28.5835, val_loss: 28.9501, val_MinusLogProbMetric: 28.9501

Epoch 159: val_loss did not improve from 28.80002
196/196 - 39s - loss: 28.5835 - MinusLogProbMetric: 28.5835 - val_loss: 28.9501 - val_MinusLogProbMetric: 28.9501 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 160/1000
2023-10-25 04:51:17.336 
Epoch 160/1000 
	 loss: 28.5092, MinusLogProbMetric: 28.5092, val_loss: 29.1934, val_MinusLogProbMetric: 29.1934

Epoch 160: val_loss did not improve from 28.80002
196/196 - 36s - loss: 28.5092 - MinusLogProbMetric: 28.5092 - val_loss: 29.1934 - val_MinusLogProbMetric: 29.1934 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 161/1000
2023-10-25 04:51:56.393 
Epoch 161/1000 
	 loss: 28.5326, MinusLogProbMetric: 28.5326, val_loss: 28.9726, val_MinusLogProbMetric: 28.9726

Epoch 161: val_loss did not improve from 28.80002
196/196 - 39s - loss: 28.5326 - MinusLogProbMetric: 28.5326 - val_loss: 28.9726 - val_MinusLogProbMetric: 28.9726 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 162/1000
2023-10-25 04:52:34.060 
Epoch 162/1000 
	 loss: 28.5364, MinusLogProbMetric: 28.5364, val_loss: 29.2917, val_MinusLogProbMetric: 29.2917

Epoch 162: val_loss did not improve from 28.80002
196/196 - 38s - loss: 28.5364 - MinusLogProbMetric: 28.5364 - val_loss: 29.2917 - val_MinusLogProbMetric: 29.2917 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 163/1000
2023-10-25 04:53:12.471 
Epoch 163/1000 
	 loss: 28.5051, MinusLogProbMetric: 28.5051, val_loss: 29.0548, val_MinusLogProbMetric: 29.0548

Epoch 163: val_loss did not improve from 28.80002
196/196 - 38s - loss: 28.5051 - MinusLogProbMetric: 28.5051 - val_loss: 29.0548 - val_MinusLogProbMetric: 29.0548 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 164/1000
2023-10-25 04:53:49.093 
Epoch 164/1000 
	 loss: 28.4644, MinusLogProbMetric: 28.4644, val_loss: 29.8250, val_MinusLogProbMetric: 29.8250

Epoch 164: val_loss did not improve from 28.80002
196/196 - 37s - loss: 28.4644 - MinusLogProbMetric: 28.4644 - val_loss: 29.8250 - val_MinusLogProbMetric: 29.8250 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 165/1000
2023-10-25 04:54:27.761 
Epoch 165/1000 
	 loss: 28.5738, MinusLogProbMetric: 28.5738, val_loss: 29.4206, val_MinusLogProbMetric: 29.4206

Epoch 165: val_loss did not improve from 28.80002
196/196 - 39s - loss: 28.5738 - MinusLogProbMetric: 28.5738 - val_loss: 29.4206 - val_MinusLogProbMetric: 29.4206 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 166/1000
2023-10-25 04:55:05.774 
Epoch 166/1000 
	 loss: 28.5728, MinusLogProbMetric: 28.5728, val_loss: 29.3927, val_MinusLogProbMetric: 29.3927

Epoch 166: val_loss did not improve from 28.80002
196/196 - 38s - loss: 28.5728 - MinusLogProbMetric: 28.5728 - val_loss: 29.3927 - val_MinusLogProbMetric: 29.3927 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 167/1000
2023-10-25 04:55:43.734 
Epoch 167/1000 
	 loss: 28.4468, MinusLogProbMetric: 28.4468, val_loss: 29.2529, val_MinusLogProbMetric: 29.2529

Epoch 167: val_loss did not improve from 28.80002
196/196 - 38s - loss: 28.4468 - MinusLogProbMetric: 28.4468 - val_loss: 29.2529 - val_MinusLogProbMetric: 29.2529 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 168/1000
2023-10-25 04:56:21.983 
Epoch 168/1000 
	 loss: 28.4983, MinusLogProbMetric: 28.4983, val_loss: 29.3903, val_MinusLogProbMetric: 29.3903

Epoch 168: val_loss did not improve from 28.80002
196/196 - 38s - loss: 28.4983 - MinusLogProbMetric: 28.4983 - val_loss: 29.3903 - val_MinusLogProbMetric: 29.3903 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 169/1000
2023-10-25 04:56:58.359 
Epoch 169/1000 
	 loss: 28.4366, MinusLogProbMetric: 28.4366, val_loss: 28.8751, val_MinusLogProbMetric: 28.8751

Epoch 169: val_loss did not improve from 28.80002
196/196 - 36s - loss: 28.4366 - MinusLogProbMetric: 28.4366 - val_loss: 28.8751 - val_MinusLogProbMetric: 28.8751 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 170/1000
2023-10-25 04:57:34.011 
Epoch 170/1000 
	 loss: 28.4822, MinusLogProbMetric: 28.4822, val_loss: 29.0930, val_MinusLogProbMetric: 29.0930

Epoch 170: val_loss did not improve from 28.80002
196/196 - 36s - loss: 28.4822 - MinusLogProbMetric: 28.4822 - val_loss: 29.0930 - val_MinusLogProbMetric: 29.0930 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 171/1000
2023-10-25 04:58:11.585 
Epoch 171/1000 
	 loss: 28.4459, MinusLogProbMetric: 28.4459, val_loss: 29.0215, val_MinusLogProbMetric: 29.0215

Epoch 171: val_loss did not improve from 28.80002
196/196 - 38s - loss: 28.4459 - MinusLogProbMetric: 28.4459 - val_loss: 29.0215 - val_MinusLogProbMetric: 29.0215 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 172/1000
2023-10-25 04:58:47.852 
Epoch 172/1000 
	 loss: 28.4750, MinusLogProbMetric: 28.4750, val_loss: 28.6468, val_MinusLogProbMetric: 28.6468

Epoch 172: val_loss improved from 28.80002 to 28.64679, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 37s - loss: 28.4750 - MinusLogProbMetric: 28.4750 - val_loss: 28.6468 - val_MinusLogProbMetric: 28.6468 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 173/1000
2023-10-25 04:59:26.998 
Epoch 173/1000 
	 loss: 28.4676, MinusLogProbMetric: 28.4676, val_loss: 29.4937, val_MinusLogProbMetric: 29.4937

Epoch 173: val_loss did not improve from 28.64679
196/196 - 38s - loss: 28.4676 - MinusLogProbMetric: 28.4676 - val_loss: 29.4937 - val_MinusLogProbMetric: 29.4937 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 174/1000
2023-10-25 05:00:03.902 
Epoch 174/1000 
	 loss: 28.4051, MinusLogProbMetric: 28.4051, val_loss: 29.4028, val_MinusLogProbMetric: 29.4028

Epoch 174: val_loss did not improve from 28.64679
196/196 - 37s - loss: 28.4051 - MinusLogProbMetric: 28.4051 - val_loss: 29.4028 - val_MinusLogProbMetric: 29.4028 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 175/1000
2023-10-25 05:00:41.509 
Epoch 175/1000 
	 loss: 28.4928, MinusLogProbMetric: 28.4928, val_loss: 29.0453, val_MinusLogProbMetric: 29.0453

Epoch 175: val_loss did not improve from 28.64679
196/196 - 38s - loss: 28.4928 - MinusLogProbMetric: 28.4928 - val_loss: 29.0453 - val_MinusLogProbMetric: 29.0453 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 176/1000
2023-10-25 05:01:18.295 
Epoch 176/1000 
	 loss: 28.4236, MinusLogProbMetric: 28.4236, val_loss: 28.9201, val_MinusLogProbMetric: 28.9201

Epoch 176: val_loss did not improve from 28.64679
196/196 - 37s - loss: 28.4236 - MinusLogProbMetric: 28.4236 - val_loss: 28.9201 - val_MinusLogProbMetric: 28.9201 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 177/1000
2023-10-25 05:01:55.709 
Epoch 177/1000 
	 loss: 28.4534, MinusLogProbMetric: 28.4534, val_loss: 29.0396, val_MinusLogProbMetric: 29.0396

Epoch 177: val_loss did not improve from 28.64679
196/196 - 37s - loss: 28.4534 - MinusLogProbMetric: 28.4534 - val_loss: 29.0396 - val_MinusLogProbMetric: 29.0396 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 178/1000
2023-10-25 05:02:32.693 
Epoch 178/1000 
	 loss: 28.4555, MinusLogProbMetric: 28.4555, val_loss: 28.9827, val_MinusLogProbMetric: 28.9827

Epoch 178: val_loss did not improve from 28.64679
196/196 - 37s - loss: 28.4555 - MinusLogProbMetric: 28.4555 - val_loss: 28.9827 - val_MinusLogProbMetric: 28.9827 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 179/1000
2023-10-25 05:03:11.539 
Epoch 179/1000 
	 loss: 28.5540, MinusLogProbMetric: 28.5540, val_loss: 29.2967, val_MinusLogProbMetric: 29.2967

Epoch 179: val_loss did not improve from 28.64679
196/196 - 39s - loss: 28.5540 - MinusLogProbMetric: 28.5540 - val_loss: 29.2967 - val_MinusLogProbMetric: 29.2967 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 180/1000
2023-10-25 05:03:49.574 
Epoch 180/1000 
	 loss: 28.4275, MinusLogProbMetric: 28.4275, val_loss: 28.8725, val_MinusLogProbMetric: 28.8725

Epoch 180: val_loss did not improve from 28.64679
196/196 - 38s - loss: 28.4275 - MinusLogProbMetric: 28.4275 - val_loss: 28.8725 - val_MinusLogProbMetric: 28.8725 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 181/1000
2023-10-25 05:04:26.604 
Epoch 181/1000 
	 loss: 28.3487, MinusLogProbMetric: 28.3487, val_loss: 29.0514, val_MinusLogProbMetric: 29.0514

Epoch 181: val_loss did not improve from 28.64679
196/196 - 37s - loss: 28.3487 - MinusLogProbMetric: 28.3487 - val_loss: 29.0514 - val_MinusLogProbMetric: 29.0514 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 182/1000
2023-10-25 05:05:03.565 
Epoch 182/1000 
	 loss: 28.4888, MinusLogProbMetric: 28.4888, val_loss: 29.3465, val_MinusLogProbMetric: 29.3465

Epoch 182: val_loss did not improve from 28.64679
196/196 - 37s - loss: 28.4888 - MinusLogProbMetric: 28.4888 - val_loss: 29.3465 - val_MinusLogProbMetric: 29.3465 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 183/1000
2023-10-25 05:05:41.097 
Epoch 183/1000 
	 loss: 28.3095, MinusLogProbMetric: 28.3095, val_loss: 28.6972, val_MinusLogProbMetric: 28.6972

Epoch 183: val_loss did not improve from 28.64679
196/196 - 38s - loss: 28.3095 - MinusLogProbMetric: 28.3095 - val_loss: 28.6972 - val_MinusLogProbMetric: 28.6972 - lr: 0.0010 - 38s/epoch - 191ms/step
Epoch 184/1000
2023-10-25 05:06:18.280 
Epoch 184/1000 
	 loss: 28.4877, MinusLogProbMetric: 28.4877, val_loss: 28.9295, val_MinusLogProbMetric: 28.9295

Epoch 184: val_loss did not improve from 28.64679
196/196 - 37s - loss: 28.4877 - MinusLogProbMetric: 28.4877 - val_loss: 28.9295 - val_MinusLogProbMetric: 28.9295 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 185/1000
2023-10-25 05:06:56.226 
Epoch 185/1000 
	 loss: 28.4334, MinusLogProbMetric: 28.4334, val_loss: 29.0382, val_MinusLogProbMetric: 29.0382

Epoch 185: val_loss did not improve from 28.64679
196/196 - 38s - loss: 28.4334 - MinusLogProbMetric: 28.4334 - val_loss: 29.0382 - val_MinusLogProbMetric: 29.0382 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 186/1000
2023-10-25 05:07:32.537 
Epoch 186/1000 
	 loss: 28.4456, MinusLogProbMetric: 28.4456, val_loss: 29.0274, val_MinusLogProbMetric: 29.0274

Epoch 186: val_loss did not improve from 28.64679
196/196 - 36s - loss: 28.4456 - MinusLogProbMetric: 28.4456 - val_loss: 29.0274 - val_MinusLogProbMetric: 29.0274 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 187/1000
2023-10-25 05:08:09.082 
Epoch 187/1000 
	 loss: 28.3674, MinusLogProbMetric: 28.3674, val_loss: 28.5922, val_MinusLogProbMetric: 28.5922

Epoch 187: val_loss improved from 28.64679 to 28.59220, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 37s - loss: 28.3674 - MinusLogProbMetric: 28.3674 - val_loss: 28.5922 - val_MinusLogProbMetric: 28.5922 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 188/1000
2023-10-25 05:08:48.111 
Epoch 188/1000 
	 loss: 28.3309, MinusLogProbMetric: 28.3309, val_loss: 29.9365, val_MinusLogProbMetric: 29.9365

Epoch 188: val_loss did not improve from 28.59220
196/196 - 38s - loss: 28.3309 - MinusLogProbMetric: 28.3309 - val_loss: 29.9365 - val_MinusLogProbMetric: 29.9365 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 189/1000
2023-10-25 05:09:26.637 
Epoch 189/1000 
	 loss: 28.4430, MinusLogProbMetric: 28.4430, val_loss: 29.0839, val_MinusLogProbMetric: 29.0839

Epoch 189: val_loss did not improve from 28.59220
196/196 - 39s - loss: 28.4430 - MinusLogProbMetric: 28.4430 - val_loss: 29.0839 - val_MinusLogProbMetric: 29.0839 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 190/1000
2023-10-25 05:10:03.851 
Epoch 190/1000 
	 loss: 28.3300, MinusLogProbMetric: 28.3300, val_loss: 28.8756, val_MinusLogProbMetric: 28.8756

Epoch 190: val_loss did not improve from 28.59220
196/196 - 37s - loss: 28.3300 - MinusLogProbMetric: 28.3300 - val_loss: 28.8756 - val_MinusLogProbMetric: 28.8756 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 191/1000
2023-10-25 05:10:42.203 
Epoch 191/1000 
	 loss: 28.3373, MinusLogProbMetric: 28.3373, val_loss: 28.8929, val_MinusLogProbMetric: 28.8929

Epoch 191: val_loss did not improve from 28.59220
196/196 - 38s - loss: 28.3373 - MinusLogProbMetric: 28.3373 - val_loss: 28.8929 - val_MinusLogProbMetric: 28.8929 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 192/1000
2023-10-25 05:11:19.542 
Epoch 192/1000 
	 loss: 28.2556, MinusLogProbMetric: 28.2556, val_loss: 28.9221, val_MinusLogProbMetric: 28.9221

Epoch 192: val_loss did not improve from 28.59220
196/196 - 37s - loss: 28.2556 - MinusLogProbMetric: 28.2556 - val_loss: 28.9221 - val_MinusLogProbMetric: 28.9221 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 193/1000
2023-10-25 05:11:57.145 
Epoch 193/1000 
	 loss: 28.4140, MinusLogProbMetric: 28.4140, val_loss: 30.8636, val_MinusLogProbMetric: 30.8636

Epoch 193: val_loss did not improve from 28.59220
196/196 - 38s - loss: 28.4140 - MinusLogProbMetric: 28.4140 - val_loss: 30.8636 - val_MinusLogProbMetric: 30.8636 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 194/1000
2023-10-25 05:12:33.975 
Epoch 194/1000 
	 loss: 28.3653, MinusLogProbMetric: 28.3653, val_loss: 28.5571, val_MinusLogProbMetric: 28.5571

Epoch 194: val_loss improved from 28.59220 to 28.55706, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 37s - loss: 28.3653 - MinusLogProbMetric: 28.3653 - val_loss: 28.5571 - val_MinusLogProbMetric: 28.5571 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 195/1000
2023-10-25 05:13:12.457 
Epoch 195/1000 
	 loss: 28.2707, MinusLogProbMetric: 28.2707, val_loss: 28.9003, val_MinusLogProbMetric: 28.9003

Epoch 195: val_loss did not improve from 28.55706
196/196 - 38s - loss: 28.2707 - MinusLogProbMetric: 28.2707 - val_loss: 28.9003 - val_MinusLogProbMetric: 28.9003 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 196/1000
2023-10-25 05:13:51.263 
Epoch 196/1000 
	 loss: 28.3112, MinusLogProbMetric: 28.3112, val_loss: 28.7230, val_MinusLogProbMetric: 28.7230

Epoch 196: val_loss did not improve from 28.55706
196/196 - 39s - loss: 28.3112 - MinusLogProbMetric: 28.3112 - val_loss: 28.7230 - val_MinusLogProbMetric: 28.7230 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 197/1000
2023-10-25 05:14:29.120 
Epoch 197/1000 
	 loss: 28.4472, MinusLogProbMetric: 28.4472, val_loss: 29.1759, val_MinusLogProbMetric: 29.1759

Epoch 197: val_loss did not improve from 28.55706
196/196 - 38s - loss: 28.4472 - MinusLogProbMetric: 28.4472 - val_loss: 29.1759 - val_MinusLogProbMetric: 29.1759 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 198/1000
2023-10-25 05:15:05.249 
Epoch 198/1000 
	 loss: 28.2584, MinusLogProbMetric: 28.2584, val_loss: 28.6962, val_MinusLogProbMetric: 28.6962

Epoch 198: val_loss did not improve from 28.55706
196/196 - 36s - loss: 28.2584 - MinusLogProbMetric: 28.2584 - val_loss: 28.6962 - val_MinusLogProbMetric: 28.6962 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 199/1000
2023-10-25 05:15:43.086 
Epoch 199/1000 
	 loss: 28.4761, MinusLogProbMetric: 28.4761, val_loss: 28.8729, val_MinusLogProbMetric: 28.8729

Epoch 199: val_loss did not improve from 28.55706
196/196 - 38s - loss: 28.4761 - MinusLogProbMetric: 28.4761 - val_loss: 28.8729 - val_MinusLogProbMetric: 28.8729 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 200/1000
2023-10-25 05:16:20.188 
Epoch 200/1000 
	 loss: 28.3827, MinusLogProbMetric: 28.3827, val_loss: 29.2092, val_MinusLogProbMetric: 29.2092

Epoch 200: val_loss did not improve from 28.55706
196/196 - 37s - loss: 28.3827 - MinusLogProbMetric: 28.3827 - val_loss: 29.2092 - val_MinusLogProbMetric: 29.2092 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 201/1000
2023-10-25 05:16:57.363 
Epoch 201/1000 
	 loss: 28.3228, MinusLogProbMetric: 28.3228, val_loss: 28.7486, val_MinusLogProbMetric: 28.7486

Epoch 201: val_loss did not improve from 28.55706
196/196 - 37s - loss: 28.3228 - MinusLogProbMetric: 28.3228 - val_loss: 28.7486 - val_MinusLogProbMetric: 28.7486 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 202/1000
2023-10-25 05:17:35.359 
Epoch 202/1000 
	 loss: 28.2514, MinusLogProbMetric: 28.2514, val_loss: 29.2409, val_MinusLogProbMetric: 29.2409

Epoch 202: val_loss did not improve from 28.55706
196/196 - 38s - loss: 28.2514 - MinusLogProbMetric: 28.2514 - val_loss: 29.2409 - val_MinusLogProbMetric: 29.2409 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 203/1000
2023-10-25 05:18:12.065 
Epoch 203/1000 
	 loss: 28.2579, MinusLogProbMetric: 28.2579, val_loss: 28.8121, val_MinusLogProbMetric: 28.8121

Epoch 203: val_loss did not improve from 28.55706
196/196 - 37s - loss: 28.2579 - MinusLogProbMetric: 28.2579 - val_loss: 28.8121 - val_MinusLogProbMetric: 28.8121 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 204/1000
2023-10-25 05:18:48.077 
Epoch 204/1000 
	 loss: 28.2323, MinusLogProbMetric: 28.2323, val_loss: 29.2405, val_MinusLogProbMetric: 29.2405

Epoch 204: val_loss did not improve from 28.55706
196/196 - 36s - loss: 28.2323 - MinusLogProbMetric: 28.2323 - val_loss: 29.2405 - val_MinusLogProbMetric: 29.2405 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 205/1000
2023-10-25 05:19:25.613 
Epoch 205/1000 
	 loss: 28.2099, MinusLogProbMetric: 28.2099, val_loss: 28.8851, val_MinusLogProbMetric: 28.8851

Epoch 205: val_loss did not improve from 28.55706
196/196 - 38s - loss: 28.2099 - MinusLogProbMetric: 28.2099 - val_loss: 28.8851 - val_MinusLogProbMetric: 28.8851 - lr: 0.0010 - 38s/epoch - 191ms/step
Epoch 206/1000
2023-10-25 05:20:03.685 
Epoch 206/1000 
	 loss: 28.3073, MinusLogProbMetric: 28.3073, val_loss: 29.2581, val_MinusLogProbMetric: 29.2581

Epoch 206: val_loss did not improve from 28.55706
196/196 - 38s - loss: 28.3073 - MinusLogProbMetric: 28.3073 - val_loss: 29.2581 - val_MinusLogProbMetric: 29.2581 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 207/1000
2023-10-25 05:20:42.412 
Epoch 207/1000 
	 loss: 28.3436, MinusLogProbMetric: 28.3436, val_loss: 28.9127, val_MinusLogProbMetric: 28.9127

Epoch 207: val_loss did not improve from 28.55706
196/196 - 39s - loss: 28.3436 - MinusLogProbMetric: 28.3436 - val_loss: 28.9127 - val_MinusLogProbMetric: 28.9127 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 208/1000
2023-10-25 05:21:21.433 
Epoch 208/1000 
	 loss: 28.3109, MinusLogProbMetric: 28.3109, val_loss: 28.9185, val_MinusLogProbMetric: 28.9185

Epoch 208: val_loss did not improve from 28.55706
196/196 - 39s - loss: 28.3109 - MinusLogProbMetric: 28.3109 - val_loss: 28.9185 - val_MinusLogProbMetric: 28.9185 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 209/1000
2023-10-25 05:22:00.224 
Epoch 209/1000 
	 loss: 28.1723, MinusLogProbMetric: 28.1723, val_loss: 29.0915, val_MinusLogProbMetric: 29.0915

Epoch 209: val_loss did not improve from 28.55706
196/196 - 39s - loss: 28.1723 - MinusLogProbMetric: 28.1723 - val_loss: 29.0915 - val_MinusLogProbMetric: 29.0915 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 210/1000
2023-10-25 05:22:38.723 
Epoch 210/1000 
	 loss: 28.2877, MinusLogProbMetric: 28.2877, val_loss: 29.5708, val_MinusLogProbMetric: 29.5708

Epoch 210: val_loss did not improve from 28.55706
196/196 - 38s - loss: 28.2877 - MinusLogProbMetric: 28.2877 - val_loss: 29.5708 - val_MinusLogProbMetric: 29.5708 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 211/1000
2023-10-25 05:23:17.360 
Epoch 211/1000 
	 loss: 28.4241, MinusLogProbMetric: 28.4241, val_loss: 28.8661, val_MinusLogProbMetric: 28.8661

Epoch 211: val_loss did not improve from 28.55706
196/196 - 39s - loss: 28.4241 - MinusLogProbMetric: 28.4241 - val_loss: 28.8661 - val_MinusLogProbMetric: 28.8661 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 212/1000
2023-10-25 05:23:55.993 
Epoch 212/1000 
	 loss: 28.1677, MinusLogProbMetric: 28.1677, val_loss: 28.9315, val_MinusLogProbMetric: 28.9315

Epoch 212: val_loss did not improve from 28.55706
196/196 - 39s - loss: 28.1677 - MinusLogProbMetric: 28.1677 - val_loss: 28.9315 - val_MinusLogProbMetric: 28.9315 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 213/1000
2023-10-25 05:24:34.451 
Epoch 213/1000 
	 loss: 28.2395, MinusLogProbMetric: 28.2395, val_loss: 28.5379, val_MinusLogProbMetric: 28.5379

Epoch 213: val_loss improved from 28.55706 to 28.53794, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 28.2395 - MinusLogProbMetric: 28.2395 - val_loss: 28.5379 - val_MinusLogProbMetric: 28.5379 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 214/1000
2023-10-25 05:25:13.860 
Epoch 214/1000 
	 loss: 28.1982, MinusLogProbMetric: 28.1982, val_loss: 29.2715, val_MinusLogProbMetric: 29.2715

Epoch 214: val_loss did not improve from 28.53794
196/196 - 39s - loss: 28.1982 - MinusLogProbMetric: 28.1982 - val_loss: 29.2715 - val_MinusLogProbMetric: 29.2715 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 215/1000
2023-10-25 05:25:50.446 
Epoch 215/1000 
	 loss: 28.1944, MinusLogProbMetric: 28.1944, val_loss: 29.0572, val_MinusLogProbMetric: 29.0572

Epoch 215: val_loss did not improve from 28.53794
196/196 - 37s - loss: 28.1944 - MinusLogProbMetric: 28.1944 - val_loss: 29.0572 - val_MinusLogProbMetric: 29.0572 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 216/1000
2023-10-25 05:26:27.232 
Epoch 216/1000 
	 loss: 28.2121, MinusLogProbMetric: 28.2121, val_loss: 28.6977, val_MinusLogProbMetric: 28.6977

Epoch 216: val_loss did not improve from 28.53794
196/196 - 37s - loss: 28.2121 - MinusLogProbMetric: 28.2121 - val_loss: 28.6977 - val_MinusLogProbMetric: 28.6977 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 217/1000
2023-10-25 05:27:04.443 
Epoch 217/1000 
	 loss: 28.2617, MinusLogProbMetric: 28.2617, val_loss: 29.1666, val_MinusLogProbMetric: 29.1666

Epoch 217: val_loss did not improve from 28.53794
196/196 - 37s - loss: 28.2617 - MinusLogProbMetric: 28.2617 - val_loss: 29.1666 - val_MinusLogProbMetric: 29.1666 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 218/1000
2023-10-25 05:27:42.913 
Epoch 218/1000 
	 loss: 28.2427, MinusLogProbMetric: 28.2427, val_loss: 28.8831, val_MinusLogProbMetric: 28.8831

Epoch 218: val_loss did not improve from 28.53794
196/196 - 38s - loss: 28.2427 - MinusLogProbMetric: 28.2427 - val_loss: 28.8831 - val_MinusLogProbMetric: 28.8831 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 219/1000
2023-10-25 05:28:20.219 
Epoch 219/1000 
	 loss: 28.1946, MinusLogProbMetric: 28.1946, val_loss: 28.6753, val_MinusLogProbMetric: 28.6753

Epoch 219: val_loss did not improve from 28.53794
196/196 - 37s - loss: 28.1946 - MinusLogProbMetric: 28.1946 - val_loss: 28.6753 - val_MinusLogProbMetric: 28.6753 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 220/1000
2023-10-25 05:28:57.731 
Epoch 220/1000 
	 loss: 28.2606, MinusLogProbMetric: 28.2606, val_loss: 29.0605, val_MinusLogProbMetric: 29.0605

Epoch 220: val_loss did not improve from 28.53794
196/196 - 38s - loss: 28.2606 - MinusLogProbMetric: 28.2606 - val_loss: 29.0605 - val_MinusLogProbMetric: 29.0605 - lr: 0.0010 - 38s/epoch - 191ms/step
Epoch 221/1000
2023-10-25 05:29:34.648 
Epoch 221/1000 
	 loss: 28.1996, MinusLogProbMetric: 28.1996, val_loss: 28.6755, val_MinusLogProbMetric: 28.6755

Epoch 221: val_loss did not improve from 28.53794
196/196 - 37s - loss: 28.1996 - MinusLogProbMetric: 28.1996 - val_loss: 28.6755 - val_MinusLogProbMetric: 28.6755 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 222/1000
2023-10-25 05:30:11.672 
Epoch 222/1000 
	 loss: 28.2160, MinusLogProbMetric: 28.2160, val_loss: 28.9933, val_MinusLogProbMetric: 28.9933

Epoch 222: val_loss did not improve from 28.53794
196/196 - 37s - loss: 28.2160 - MinusLogProbMetric: 28.2160 - val_loss: 28.9933 - val_MinusLogProbMetric: 28.9933 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 223/1000
2023-10-25 05:30:48.937 
Epoch 223/1000 
	 loss: 28.1544, MinusLogProbMetric: 28.1544, val_loss: 28.7307, val_MinusLogProbMetric: 28.7307

Epoch 223: val_loss did not improve from 28.53794
196/196 - 37s - loss: 28.1544 - MinusLogProbMetric: 28.1544 - val_loss: 28.7307 - val_MinusLogProbMetric: 28.7307 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 224/1000
2023-10-25 05:31:26.588 
Epoch 224/1000 
	 loss: 28.1788, MinusLogProbMetric: 28.1788, val_loss: 28.6967, val_MinusLogProbMetric: 28.6967

Epoch 224: val_loss did not improve from 28.53794
196/196 - 38s - loss: 28.1788 - MinusLogProbMetric: 28.1788 - val_loss: 28.6967 - val_MinusLogProbMetric: 28.6967 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 225/1000
2023-10-25 05:32:05.053 
Epoch 225/1000 
	 loss: 28.2096, MinusLogProbMetric: 28.2096, val_loss: 28.6321, val_MinusLogProbMetric: 28.6321

Epoch 225: val_loss did not improve from 28.53794
196/196 - 38s - loss: 28.2096 - MinusLogProbMetric: 28.2096 - val_loss: 28.6321 - val_MinusLogProbMetric: 28.6321 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 226/1000
2023-10-25 05:32:41.547 
Epoch 226/1000 
	 loss: 28.1907, MinusLogProbMetric: 28.1907, val_loss: 28.6992, val_MinusLogProbMetric: 28.6992

Epoch 226: val_loss did not improve from 28.53794
196/196 - 36s - loss: 28.1907 - MinusLogProbMetric: 28.1907 - val_loss: 28.6992 - val_MinusLogProbMetric: 28.6992 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 227/1000
2023-10-25 05:33:18.856 
Epoch 227/1000 
	 loss: 28.1401, MinusLogProbMetric: 28.1401, val_loss: 28.6875, val_MinusLogProbMetric: 28.6875

Epoch 227: val_loss did not improve from 28.53794
196/196 - 37s - loss: 28.1401 - MinusLogProbMetric: 28.1401 - val_loss: 28.6875 - val_MinusLogProbMetric: 28.6875 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 228/1000
2023-10-25 05:33:57.746 
Epoch 228/1000 
	 loss: 28.1432, MinusLogProbMetric: 28.1432, val_loss: 28.8665, val_MinusLogProbMetric: 28.8665

Epoch 228: val_loss did not improve from 28.53794
196/196 - 39s - loss: 28.1432 - MinusLogProbMetric: 28.1432 - val_loss: 28.8665 - val_MinusLogProbMetric: 28.8665 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 229/1000
2023-10-25 05:34:34.131 
Epoch 229/1000 
	 loss: 28.1330, MinusLogProbMetric: 28.1330, val_loss: 29.1569, val_MinusLogProbMetric: 29.1569

Epoch 229: val_loss did not improve from 28.53794
196/196 - 36s - loss: 28.1330 - MinusLogProbMetric: 28.1330 - val_loss: 29.1569 - val_MinusLogProbMetric: 29.1569 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 230/1000
2023-10-25 05:35:11.060 
Epoch 230/1000 
	 loss: 28.0926, MinusLogProbMetric: 28.0926, val_loss: 28.5100, val_MinusLogProbMetric: 28.5100

Epoch 230: val_loss improved from 28.53794 to 28.51001, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 37s - loss: 28.0926 - MinusLogProbMetric: 28.0926 - val_loss: 28.5100 - val_MinusLogProbMetric: 28.5100 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 231/1000
2023-10-25 05:35:49.233 
Epoch 231/1000 
	 loss: 28.1562, MinusLogProbMetric: 28.1562, val_loss: 28.6916, val_MinusLogProbMetric: 28.6916

Epoch 231: val_loss did not improve from 28.51001
196/196 - 38s - loss: 28.1562 - MinusLogProbMetric: 28.1562 - val_loss: 28.6916 - val_MinusLogProbMetric: 28.6916 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 232/1000
2023-10-25 05:36:28.206 
Epoch 232/1000 
	 loss: 28.1323, MinusLogProbMetric: 28.1323, val_loss: 28.8929, val_MinusLogProbMetric: 28.8929

Epoch 232: val_loss did not improve from 28.51001
196/196 - 39s - loss: 28.1323 - MinusLogProbMetric: 28.1323 - val_loss: 28.8929 - val_MinusLogProbMetric: 28.8929 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 233/1000
2023-10-25 05:37:05.736 
Epoch 233/1000 
	 loss: 28.0480, MinusLogProbMetric: 28.0480, val_loss: 28.9205, val_MinusLogProbMetric: 28.9205

Epoch 233: val_loss did not improve from 28.51001
196/196 - 38s - loss: 28.0480 - MinusLogProbMetric: 28.0480 - val_loss: 28.9205 - val_MinusLogProbMetric: 28.9205 - lr: 0.0010 - 38s/epoch - 191ms/step
Epoch 234/1000
2023-10-25 05:37:43.703 
Epoch 234/1000 
	 loss: 28.1095, MinusLogProbMetric: 28.1095, val_loss: 28.6455, val_MinusLogProbMetric: 28.6455

Epoch 234: val_loss did not improve from 28.51001
196/196 - 38s - loss: 28.1095 - MinusLogProbMetric: 28.1095 - val_loss: 28.6455 - val_MinusLogProbMetric: 28.6455 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 235/1000
2023-10-25 05:38:22.143 
Epoch 235/1000 
	 loss: 28.1832, MinusLogProbMetric: 28.1832, val_loss: 29.1037, val_MinusLogProbMetric: 29.1037

Epoch 235: val_loss did not improve from 28.51001
196/196 - 38s - loss: 28.1832 - MinusLogProbMetric: 28.1832 - val_loss: 29.1037 - val_MinusLogProbMetric: 29.1037 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 236/1000
2023-10-25 05:38:59.481 
Epoch 236/1000 
	 loss: 28.3205, MinusLogProbMetric: 28.3205, val_loss: 28.9825, val_MinusLogProbMetric: 28.9825

Epoch 236: val_loss did not improve from 28.51001
196/196 - 37s - loss: 28.3205 - MinusLogProbMetric: 28.3205 - val_loss: 28.9825 - val_MinusLogProbMetric: 28.9825 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 237/1000
2023-10-25 05:39:36.960 
Epoch 237/1000 
	 loss: 28.0125, MinusLogProbMetric: 28.0125, val_loss: 28.7037, val_MinusLogProbMetric: 28.7037

Epoch 237: val_loss did not improve from 28.51001
196/196 - 37s - loss: 28.0125 - MinusLogProbMetric: 28.0125 - val_loss: 28.7037 - val_MinusLogProbMetric: 28.7037 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 238/1000
2023-10-25 05:40:14.695 
Epoch 238/1000 
	 loss: 28.0629, MinusLogProbMetric: 28.0629, val_loss: 28.5417, val_MinusLogProbMetric: 28.5417

Epoch 238: val_loss did not improve from 28.51001
196/196 - 38s - loss: 28.0629 - MinusLogProbMetric: 28.0629 - val_loss: 28.5417 - val_MinusLogProbMetric: 28.5417 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 239/1000
2023-10-25 05:40:50.877 
Epoch 239/1000 
	 loss: 28.0754, MinusLogProbMetric: 28.0754, val_loss: 28.8669, val_MinusLogProbMetric: 28.8669

Epoch 239: val_loss did not improve from 28.51001
196/196 - 36s - loss: 28.0754 - MinusLogProbMetric: 28.0754 - val_loss: 28.8669 - val_MinusLogProbMetric: 28.8669 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 240/1000
2023-10-25 05:41:30.296 
Epoch 240/1000 
	 loss: 28.0412, MinusLogProbMetric: 28.0412, val_loss: 28.7668, val_MinusLogProbMetric: 28.7668

Epoch 240: val_loss did not improve from 28.51001
196/196 - 39s - loss: 28.0412 - MinusLogProbMetric: 28.0412 - val_loss: 28.7668 - val_MinusLogProbMetric: 28.7668 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 241/1000
2023-10-25 05:42:08.011 
Epoch 241/1000 
	 loss: 28.1928, MinusLogProbMetric: 28.1928, val_loss: 28.8241, val_MinusLogProbMetric: 28.8241

Epoch 241: val_loss did not improve from 28.51001
196/196 - 38s - loss: 28.1928 - MinusLogProbMetric: 28.1928 - val_loss: 28.8241 - val_MinusLogProbMetric: 28.8241 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 242/1000
2023-10-25 05:42:44.866 
Epoch 242/1000 
	 loss: 28.1508, MinusLogProbMetric: 28.1508, val_loss: 28.8145, val_MinusLogProbMetric: 28.8145

Epoch 242: val_loss did not improve from 28.51001
196/196 - 37s - loss: 28.1508 - MinusLogProbMetric: 28.1508 - val_loss: 28.8145 - val_MinusLogProbMetric: 28.8145 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 243/1000
2023-10-25 05:43:21.336 
Epoch 243/1000 
	 loss: 28.0389, MinusLogProbMetric: 28.0389, val_loss: 28.5704, val_MinusLogProbMetric: 28.5704

Epoch 243: val_loss did not improve from 28.51001
196/196 - 36s - loss: 28.0389 - MinusLogProbMetric: 28.0389 - val_loss: 28.5704 - val_MinusLogProbMetric: 28.5704 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 244/1000
2023-10-25 05:43:57.483 
Epoch 244/1000 
	 loss: 28.0962, MinusLogProbMetric: 28.0962, val_loss: 30.4892, val_MinusLogProbMetric: 30.4892

Epoch 244: val_loss did not improve from 28.51001
196/196 - 36s - loss: 28.0962 - MinusLogProbMetric: 28.0962 - val_loss: 30.4892 - val_MinusLogProbMetric: 30.4892 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 245/1000
2023-10-25 05:44:36.046 
Epoch 245/1000 
	 loss: 28.0782, MinusLogProbMetric: 28.0782, val_loss: 28.7344, val_MinusLogProbMetric: 28.7344

Epoch 245: val_loss did not improve from 28.51001
196/196 - 39s - loss: 28.0782 - MinusLogProbMetric: 28.0782 - val_loss: 28.7344 - val_MinusLogProbMetric: 28.7344 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 246/1000
2023-10-25 05:45:14.798 
Epoch 246/1000 
	 loss: 28.1417, MinusLogProbMetric: 28.1417, val_loss: 28.8570, val_MinusLogProbMetric: 28.8570

Epoch 246: val_loss did not improve from 28.51001
196/196 - 39s - loss: 28.1417 - MinusLogProbMetric: 28.1417 - val_loss: 28.8570 - val_MinusLogProbMetric: 28.8570 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 247/1000
2023-10-25 05:45:51.982 
Epoch 247/1000 
	 loss: 28.0390, MinusLogProbMetric: 28.0390, val_loss: 28.8934, val_MinusLogProbMetric: 28.8934

Epoch 247: val_loss did not improve from 28.51001
196/196 - 37s - loss: 28.0390 - MinusLogProbMetric: 28.0390 - val_loss: 28.8934 - val_MinusLogProbMetric: 28.8934 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 248/1000
2023-10-25 05:46:29.449 
Epoch 248/1000 
	 loss: 28.1778, MinusLogProbMetric: 28.1778, val_loss: 29.1709, val_MinusLogProbMetric: 29.1709

Epoch 248: val_loss did not improve from 28.51001
196/196 - 37s - loss: 28.1778 - MinusLogProbMetric: 28.1778 - val_loss: 29.1709 - val_MinusLogProbMetric: 29.1709 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 249/1000
2023-10-25 05:47:06.454 
Epoch 249/1000 
	 loss: 28.0611, MinusLogProbMetric: 28.0611, val_loss: 28.4969, val_MinusLogProbMetric: 28.4969

Epoch 249: val_loss improved from 28.51001 to 28.49688, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 38s - loss: 28.0611 - MinusLogProbMetric: 28.0611 - val_loss: 28.4969 - val_MinusLogProbMetric: 28.4969 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 250/1000
2023-10-25 05:47:45.837 
Epoch 250/1000 
	 loss: 28.0006, MinusLogProbMetric: 28.0006, val_loss: 28.5250, val_MinusLogProbMetric: 28.5250

Epoch 250: val_loss did not improve from 28.49688
196/196 - 39s - loss: 28.0006 - MinusLogProbMetric: 28.0006 - val_loss: 28.5250 - val_MinusLogProbMetric: 28.5250 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 251/1000
2023-10-25 05:48:21.531 
Epoch 251/1000 
	 loss: 28.0026, MinusLogProbMetric: 28.0026, val_loss: 29.1891, val_MinusLogProbMetric: 29.1891

Epoch 251: val_loss did not improve from 28.49688
196/196 - 36s - loss: 28.0026 - MinusLogProbMetric: 28.0026 - val_loss: 29.1891 - val_MinusLogProbMetric: 29.1891 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 252/1000
2023-10-25 05:49:00.276 
Epoch 252/1000 
	 loss: 28.0220, MinusLogProbMetric: 28.0220, val_loss: 28.9475, val_MinusLogProbMetric: 28.9475

Epoch 252: val_loss did not improve from 28.49688
196/196 - 39s - loss: 28.0220 - MinusLogProbMetric: 28.0220 - val_loss: 28.9475 - val_MinusLogProbMetric: 28.9475 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 253/1000
2023-10-25 05:49:38.718 
Epoch 253/1000 
	 loss: 28.0417, MinusLogProbMetric: 28.0417, val_loss: 28.4716, val_MinusLogProbMetric: 28.4716

Epoch 253: val_loss improved from 28.49688 to 28.47161, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 28.0417 - MinusLogProbMetric: 28.0417 - val_loss: 28.4716 - val_MinusLogProbMetric: 28.4716 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 254/1000
2023-10-25 05:50:16.624 
Epoch 254/1000 
	 loss: 28.1766, MinusLogProbMetric: 28.1766, val_loss: 28.5315, val_MinusLogProbMetric: 28.5315

Epoch 254: val_loss did not improve from 28.47161
196/196 - 37s - loss: 28.1766 - MinusLogProbMetric: 28.1766 - val_loss: 28.5315 - val_MinusLogProbMetric: 28.5315 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 255/1000
2023-10-25 05:50:53.720 
Epoch 255/1000 
	 loss: 27.9760, MinusLogProbMetric: 27.9760, val_loss: 29.4313, val_MinusLogProbMetric: 29.4313

Epoch 255: val_loss did not improve from 28.47161
196/196 - 37s - loss: 27.9760 - MinusLogProbMetric: 27.9760 - val_loss: 29.4313 - val_MinusLogProbMetric: 29.4313 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 256/1000
2023-10-25 05:51:31.644 
Epoch 256/1000 
	 loss: 27.9946, MinusLogProbMetric: 27.9946, val_loss: 29.1497, val_MinusLogProbMetric: 29.1497

Epoch 256: val_loss did not improve from 28.47161
196/196 - 38s - loss: 27.9946 - MinusLogProbMetric: 27.9946 - val_loss: 29.1497 - val_MinusLogProbMetric: 29.1497 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 257/1000
2023-10-25 05:52:10.284 
Epoch 257/1000 
	 loss: 28.0379, MinusLogProbMetric: 28.0379, val_loss: 29.0920, val_MinusLogProbMetric: 29.0920

Epoch 257: val_loss did not improve from 28.47161
196/196 - 39s - loss: 28.0379 - MinusLogProbMetric: 28.0379 - val_loss: 29.0920 - val_MinusLogProbMetric: 29.0920 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 258/1000
2023-10-25 05:52:45.614 
Epoch 258/1000 
	 loss: 27.9660, MinusLogProbMetric: 27.9660, val_loss: 28.5595, val_MinusLogProbMetric: 28.5595

Epoch 258: val_loss did not improve from 28.47161
196/196 - 35s - loss: 27.9660 - MinusLogProbMetric: 27.9660 - val_loss: 28.5595 - val_MinusLogProbMetric: 28.5595 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 259/1000
2023-10-25 05:53:23.438 
Epoch 259/1000 
	 loss: 27.9611, MinusLogProbMetric: 27.9611, val_loss: 28.7657, val_MinusLogProbMetric: 28.7657

Epoch 259: val_loss did not improve from 28.47161
196/196 - 38s - loss: 27.9611 - MinusLogProbMetric: 27.9611 - val_loss: 28.7657 - val_MinusLogProbMetric: 28.7657 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 260/1000
2023-10-25 05:54:00.758 
Epoch 260/1000 
	 loss: 28.0327, MinusLogProbMetric: 28.0327, val_loss: 28.9453, val_MinusLogProbMetric: 28.9453

Epoch 260: val_loss did not improve from 28.47161
196/196 - 37s - loss: 28.0327 - MinusLogProbMetric: 28.0327 - val_loss: 28.9453 - val_MinusLogProbMetric: 28.9453 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 261/1000
2023-10-25 05:54:38.098 
Epoch 261/1000 
	 loss: 27.9469, MinusLogProbMetric: 27.9469, val_loss: 28.6260, val_MinusLogProbMetric: 28.6260

Epoch 261: val_loss did not improve from 28.47161
196/196 - 37s - loss: 27.9469 - MinusLogProbMetric: 27.9469 - val_loss: 28.6260 - val_MinusLogProbMetric: 28.6260 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 262/1000
2023-10-25 05:55:16.825 
Epoch 262/1000 
	 loss: 27.9560, MinusLogProbMetric: 27.9560, val_loss: 28.7455, val_MinusLogProbMetric: 28.7455

Epoch 262: val_loss did not improve from 28.47161
196/196 - 39s - loss: 27.9560 - MinusLogProbMetric: 27.9560 - val_loss: 28.7455 - val_MinusLogProbMetric: 28.7455 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 263/1000
2023-10-25 05:55:53.694 
Epoch 263/1000 
	 loss: 28.0227, MinusLogProbMetric: 28.0227, val_loss: 28.4461, val_MinusLogProbMetric: 28.4461

Epoch 263: val_loss improved from 28.47161 to 28.44605, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 37s - loss: 28.0227 - MinusLogProbMetric: 28.0227 - val_loss: 28.4461 - val_MinusLogProbMetric: 28.4461 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 264/1000
2023-10-25 05:56:30.167 
Epoch 264/1000 
	 loss: 27.9392, MinusLogProbMetric: 27.9392, val_loss: 28.6052, val_MinusLogProbMetric: 28.6052

Epoch 264: val_loss did not improve from 28.44605
196/196 - 36s - loss: 27.9392 - MinusLogProbMetric: 27.9392 - val_loss: 28.6052 - val_MinusLogProbMetric: 28.6052 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 265/1000
2023-10-25 05:57:05.974 
Epoch 265/1000 
	 loss: 27.9494, MinusLogProbMetric: 27.9494, val_loss: 28.6422, val_MinusLogProbMetric: 28.6422

Epoch 265: val_loss did not improve from 28.44605
196/196 - 36s - loss: 27.9494 - MinusLogProbMetric: 27.9494 - val_loss: 28.6422 - val_MinusLogProbMetric: 28.6422 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 266/1000
2023-10-25 05:57:41.331 
Epoch 266/1000 
	 loss: 28.0056, MinusLogProbMetric: 28.0056, val_loss: 28.8211, val_MinusLogProbMetric: 28.8211

Epoch 266: val_loss did not improve from 28.44605
196/196 - 35s - loss: 28.0056 - MinusLogProbMetric: 28.0056 - val_loss: 28.8211 - val_MinusLogProbMetric: 28.8211 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 267/1000
2023-10-25 05:58:19.183 
Epoch 267/1000 
	 loss: 27.9786, MinusLogProbMetric: 27.9786, val_loss: 28.5059, val_MinusLogProbMetric: 28.5059

Epoch 267: val_loss did not improve from 28.44605
196/196 - 38s - loss: 27.9786 - MinusLogProbMetric: 27.9786 - val_loss: 28.5059 - val_MinusLogProbMetric: 28.5059 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 268/1000
2023-10-25 05:58:57.292 
Epoch 268/1000 
	 loss: 27.9369, MinusLogProbMetric: 27.9369, val_loss: 28.7103, val_MinusLogProbMetric: 28.7103

Epoch 268: val_loss did not improve from 28.44605
196/196 - 38s - loss: 27.9369 - MinusLogProbMetric: 27.9369 - val_loss: 28.7103 - val_MinusLogProbMetric: 28.7103 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 269/1000
2023-10-25 05:59:35.996 
Epoch 269/1000 
	 loss: 27.8811, MinusLogProbMetric: 27.8811, val_loss: 28.9919, val_MinusLogProbMetric: 28.9919

Epoch 269: val_loss did not improve from 28.44605
196/196 - 39s - loss: 27.8811 - MinusLogProbMetric: 27.8811 - val_loss: 28.9919 - val_MinusLogProbMetric: 28.9919 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 270/1000
2023-10-25 06:00:13.501 
Epoch 270/1000 
	 loss: 27.9061, MinusLogProbMetric: 27.9061, val_loss: 28.7321, val_MinusLogProbMetric: 28.7321

Epoch 270: val_loss did not improve from 28.44605
196/196 - 38s - loss: 27.9061 - MinusLogProbMetric: 27.9061 - val_loss: 28.7321 - val_MinusLogProbMetric: 28.7321 - lr: 0.0010 - 38s/epoch - 191ms/step
Epoch 271/1000
2023-10-25 06:00:51.330 
Epoch 271/1000 
	 loss: 27.9798, MinusLogProbMetric: 27.9798, val_loss: 28.7366, val_MinusLogProbMetric: 28.7366

Epoch 271: val_loss did not improve from 28.44605
196/196 - 38s - loss: 27.9798 - MinusLogProbMetric: 27.9798 - val_loss: 28.7366 - val_MinusLogProbMetric: 28.7366 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 272/1000
2023-10-25 06:01:28.143 
Epoch 272/1000 
	 loss: 27.9026, MinusLogProbMetric: 27.9026, val_loss: 28.5735, val_MinusLogProbMetric: 28.5735

Epoch 272: val_loss did not improve from 28.44605
196/196 - 37s - loss: 27.9026 - MinusLogProbMetric: 27.9026 - val_loss: 28.5735 - val_MinusLogProbMetric: 28.5735 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 273/1000
2023-10-25 06:02:05.441 
Epoch 273/1000 
	 loss: 27.8682, MinusLogProbMetric: 27.8682, val_loss: 28.6114, val_MinusLogProbMetric: 28.6114

Epoch 273: val_loss did not improve from 28.44605
196/196 - 37s - loss: 27.8682 - MinusLogProbMetric: 27.8682 - val_loss: 28.6114 - val_MinusLogProbMetric: 28.6114 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 274/1000
2023-10-25 06:02:43.584 
Epoch 274/1000 
	 loss: 27.9230, MinusLogProbMetric: 27.9230, val_loss: 28.8006, val_MinusLogProbMetric: 28.8006

Epoch 274: val_loss did not improve from 28.44605
196/196 - 38s - loss: 27.9230 - MinusLogProbMetric: 27.9230 - val_loss: 28.8006 - val_MinusLogProbMetric: 28.8006 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 275/1000
2023-10-25 06:03:20.200 
Epoch 275/1000 
	 loss: 27.9265, MinusLogProbMetric: 27.9265, val_loss: 29.8385, val_MinusLogProbMetric: 29.8385

Epoch 275: val_loss did not improve from 28.44605
196/196 - 37s - loss: 27.9265 - MinusLogProbMetric: 27.9265 - val_loss: 29.8385 - val_MinusLogProbMetric: 29.8385 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 276/1000
2023-10-25 06:03:57.508 
Epoch 276/1000 
	 loss: 27.9925, MinusLogProbMetric: 27.9925, val_loss: 28.5949, val_MinusLogProbMetric: 28.5949

Epoch 276: val_loss did not improve from 28.44605
196/196 - 37s - loss: 27.9925 - MinusLogProbMetric: 27.9925 - val_loss: 28.5949 - val_MinusLogProbMetric: 28.5949 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 277/1000
2023-10-25 06:04:35.243 
Epoch 277/1000 
	 loss: 28.0157, MinusLogProbMetric: 28.0157, val_loss: 28.8314, val_MinusLogProbMetric: 28.8314

Epoch 277: val_loss did not improve from 28.44605
196/196 - 38s - loss: 28.0157 - MinusLogProbMetric: 28.0157 - val_loss: 28.8314 - val_MinusLogProbMetric: 28.8314 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 278/1000
2023-10-25 06:05:12.450 
Epoch 278/1000 
	 loss: 27.8835, MinusLogProbMetric: 27.8835, val_loss: 28.8608, val_MinusLogProbMetric: 28.8608

Epoch 278: val_loss did not improve from 28.44605
196/196 - 37s - loss: 27.8835 - MinusLogProbMetric: 27.8835 - val_loss: 28.8608 - val_MinusLogProbMetric: 28.8608 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 279/1000
2023-10-25 06:05:49.959 
Epoch 279/1000 
	 loss: 27.9098, MinusLogProbMetric: 27.9098, val_loss: 28.6805, val_MinusLogProbMetric: 28.6805

Epoch 279: val_loss did not improve from 28.44605
196/196 - 38s - loss: 27.9098 - MinusLogProbMetric: 27.9098 - val_loss: 28.6805 - val_MinusLogProbMetric: 28.6805 - lr: 0.0010 - 38s/epoch - 191ms/step
Epoch 280/1000
2023-10-25 06:06:27.293 
Epoch 280/1000 
	 loss: 27.8663, MinusLogProbMetric: 27.8663, val_loss: 28.6529, val_MinusLogProbMetric: 28.6529

Epoch 280: val_loss did not improve from 28.44605
196/196 - 37s - loss: 27.8663 - MinusLogProbMetric: 27.8663 - val_loss: 28.6529 - val_MinusLogProbMetric: 28.6529 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 281/1000
2023-10-25 06:07:05.193 
Epoch 281/1000 
	 loss: 28.0097, MinusLogProbMetric: 28.0097, val_loss: 29.1978, val_MinusLogProbMetric: 29.1978

Epoch 281: val_loss did not improve from 28.44605
196/196 - 38s - loss: 28.0097 - MinusLogProbMetric: 28.0097 - val_loss: 29.1978 - val_MinusLogProbMetric: 29.1978 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 282/1000
2023-10-25 06:07:43.286 
Epoch 282/1000 
	 loss: 27.8932, MinusLogProbMetric: 27.8932, val_loss: 28.6591, val_MinusLogProbMetric: 28.6591

Epoch 282: val_loss did not improve from 28.44605
196/196 - 38s - loss: 27.8932 - MinusLogProbMetric: 27.8932 - val_loss: 28.6591 - val_MinusLogProbMetric: 28.6591 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 283/1000
2023-10-25 06:08:20.627 
Epoch 283/1000 
	 loss: 27.9136, MinusLogProbMetric: 27.9136, val_loss: 28.9009, val_MinusLogProbMetric: 28.9009

Epoch 283: val_loss did not improve from 28.44605
196/196 - 37s - loss: 27.9136 - MinusLogProbMetric: 27.9136 - val_loss: 28.9009 - val_MinusLogProbMetric: 28.9009 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 284/1000
2023-10-25 06:08:56.225 
Epoch 284/1000 
	 loss: 27.8917, MinusLogProbMetric: 27.8917, val_loss: 28.9216, val_MinusLogProbMetric: 28.9216

Epoch 284: val_loss did not improve from 28.44605
196/196 - 36s - loss: 27.8917 - MinusLogProbMetric: 27.8917 - val_loss: 28.9216 - val_MinusLogProbMetric: 28.9216 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 285/1000
2023-10-25 06:09:33.086 
Epoch 285/1000 
	 loss: 27.8757, MinusLogProbMetric: 27.8757, val_loss: 28.5937, val_MinusLogProbMetric: 28.5937

Epoch 285: val_loss did not improve from 28.44605
196/196 - 37s - loss: 27.8757 - MinusLogProbMetric: 27.8757 - val_loss: 28.5937 - val_MinusLogProbMetric: 28.5937 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 286/1000
2023-10-25 06:10:11.703 
Epoch 286/1000 
	 loss: 27.8855, MinusLogProbMetric: 27.8855, val_loss: 28.7453, val_MinusLogProbMetric: 28.7453

Epoch 286: val_loss did not improve from 28.44605
196/196 - 39s - loss: 27.8855 - MinusLogProbMetric: 27.8855 - val_loss: 28.7453 - val_MinusLogProbMetric: 28.7453 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 287/1000
2023-10-25 06:10:49.850 
Epoch 287/1000 
	 loss: 27.8292, MinusLogProbMetric: 27.8292, val_loss: 28.5669, val_MinusLogProbMetric: 28.5669

Epoch 287: val_loss did not improve from 28.44605
196/196 - 38s - loss: 27.8292 - MinusLogProbMetric: 27.8292 - val_loss: 28.5669 - val_MinusLogProbMetric: 28.5669 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 288/1000
2023-10-25 06:11:26.230 
Epoch 288/1000 
	 loss: 27.9677, MinusLogProbMetric: 27.9677, val_loss: 28.7704, val_MinusLogProbMetric: 28.7704

Epoch 288: val_loss did not improve from 28.44605
196/196 - 36s - loss: 27.9677 - MinusLogProbMetric: 27.9677 - val_loss: 28.7704 - val_MinusLogProbMetric: 28.7704 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 289/1000
2023-10-25 06:12:02.066 
Epoch 289/1000 
	 loss: 27.9521, MinusLogProbMetric: 27.9521, val_loss: 28.9217, val_MinusLogProbMetric: 28.9217

Epoch 289: val_loss did not improve from 28.44605
196/196 - 36s - loss: 27.9521 - MinusLogProbMetric: 27.9521 - val_loss: 28.9217 - val_MinusLogProbMetric: 28.9217 - lr: 0.0010 - 36s/epoch - 183ms/step
Epoch 290/1000
2023-10-25 06:12:38.331 
Epoch 290/1000 
	 loss: 27.8903, MinusLogProbMetric: 27.8903, val_loss: 28.5423, val_MinusLogProbMetric: 28.5423

Epoch 290: val_loss did not improve from 28.44605
196/196 - 36s - loss: 27.8903 - MinusLogProbMetric: 27.8903 - val_loss: 28.5423 - val_MinusLogProbMetric: 28.5423 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 291/1000
2023-10-25 06:13:15.978 
Epoch 291/1000 
	 loss: 27.7864, MinusLogProbMetric: 27.7864, val_loss: 28.5736, val_MinusLogProbMetric: 28.5736

Epoch 291: val_loss did not improve from 28.44605
196/196 - 38s - loss: 27.7864 - MinusLogProbMetric: 27.7864 - val_loss: 28.5736 - val_MinusLogProbMetric: 28.5736 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 292/1000
2023-10-25 06:13:53.444 
Epoch 292/1000 
	 loss: 27.8346, MinusLogProbMetric: 27.8346, val_loss: 28.5218, val_MinusLogProbMetric: 28.5218

Epoch 292: val_loss did not improve from 28.44605
196/196 - 37s - loss: 27.8346 - MinusLogProbMetric: 27.8346 - val_loss: 28.5218 - val_MinusLogProbMetric: 28.5218 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 293/1000
2023-10-25 06:14:31.747 
Epoch 293/1000 
	 loss: 27.9405, MinusLogProbMetric: 27.9405, val_loss: 28.8544, val_MinusLogProbMetric: 28.8544

Epoch 293: val_loss did not improve from 28.44605
196/196 - 38s - loss: 27.9405 - MinusLogProbMetric: 27.9405 - val_loss: 28.8544 - val_MinusLogProbMetric: 28.8544 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 294/1000
2023-10-25 06:15:09.595 
Epoch 294/1000 
	 loss: 27.8761, MinusLogProbMetric: 27.8761, val_loss: 28.9876, val_MinusLogProbMetric: 28.9876

Epoch 294: val_loss did not improve from 28.44605
196/196 - 38s - loss: 27.8761 - MinusLogProbMetric: 27.8761 - val_loss: 28.9876 - val_MinusLogProbMetric: 28.9876 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 295/1000
2023-10-25 06:15:47.487 
Epoch 295/1000 
	 loss: 27.9007, MinusLogProbMetric: 27.9007, val_loss: 29.0142, val_MinusLogProbMetric: 29.0142

Epoch 295: val_loss did not improve from 28.44605
196/196 - 38s - loss: 27.9007 - MinusLogProbMetric: 27.9007 - val_loss: 29.0142 - val_MinusLogProbMetric: 29.0142 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 296/1000
2023-10-25 06:16:25.457 
Epoch 296/1000 
	 loss: 27.8643, MinusLogProbMetric: 27.8643, val_loss: 28.6688, val_MinusLogProbMetric: 28.6688

Epoch 296: val_loss did not improve from 28.44605
196/196 - 38s - loss: 27.8643 - MinusLogProbMetric: 27.8643 - val_loss: 28.6688 - val_MinusLogProbMetric: 28.6688 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 297/1000
2023-10-25 06:17:04.204 
Epoch 297/1000 
	 loss: 27.8755, MinusLogProbMetric: 27.8755, val_loss: 28.6610, val_MinusLogProbMetric: 28.6610

Epoch 297: val_loss did not improve from 28.44605
196/196 - 39s - loss: 27.8755 - MinusLogProbMetric: 27.8755 - val_loss: 28.6610 - val_MinusLogProbMetric: 28.6610 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 298/1000
2023-10-25 06:17:43.108 
Epoch 298/1000 
	 loss: 27.8459, MinusLogProbMetric: 27.8459, val_loss: 28.8090, val_MinusLogProbMetric: 28.8090

Epoch 298: val_loss did not improve from 28.44605
196/196 - 39s - loss: 27.8459 - MinusLogProbMetric: 27.8459 - val_loss: 28.8090 - val_MinusLogProbMetric: 28.8090 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 299/1000
2023-10-25 06:18:20.461 
Epoch 299/1000 
	 loss: 27.8266, MinusLogProbMetric: 27.8266, val_loss: 28.5545, val_MinusLogProbMetric: 28.5545

Epoch 299: val_loss did not improve from 28.44605
196/196 - 37s - loss: 27.8266 - MinusLogProbMetric: 27.8266 - val_loss: 28.5545 - val_MinusLogProbMetric: 28.5545 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 300/1000
2023-10-25 06:18:59.001 
Epoch 300/1000 
	 loss: 27.9420, MinusLogProbMetric: 27.9420, val_loss: 28.9727, val_MinusLogProbMetric: 28.9727

Epoch 300: val_loss did not improve from 28.44605
196/196 - 39s - loss: 27.9420 - MinusLogProbMetric: 27.9420 - val_loss: 28.9727 - val_MinusLogProbMetric: 28.9727 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 301/1000
2023-10-25 06:19:36.992 
Epoch 301/1000 
	 loss: 27.8514, MinusLogProbMetric: 27.8514, val_loss: 28.5203, val_MinusLogProbMetric: 28.5203

Epoch 301: val_loss did not improve from 28.44605
196/196 - 38s - loss: 27.8514 - MinusLogProbMetric: 27.8514 - val_loss: 28.5203 - val_MinusLogProbMetric: 28.5203 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 302/1000
2023-10-25 06:20:15.646 
Epoch 302/1000 
	 loss: 27.7631, MinusLogProbMetric: 27.7631, val_loss: 29.2524, val_MinusLogProbMetric: 29.2524

Epoch 302: val_loss did not improve from 28.44605
196/196 - 39s - loss: 27.7631 - MinusLogProbMetric: 27.7631 - val_loss: 29.2524 - val_MinusLogProbMetric: 29.2524 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 303/1000
2023-10-25 06:20:53.700 
Epoch 303/1000 
	 loss: 27.8212, MinusLogProbMetric: 27.8212, val_loss: 29.1174, val_MinusLogProbMetric: 29.1174

Epoch 303: val_loss did not improve from 28.44605
196/196 - 38s - loss: 27.8212 - MinusLogProbMetric: 27.8212 - val_loss: 29.1174 - val_MinusLogProbMetric: 29.1174 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 304/1000
2023-10-25 06:21:30.447 
Epoch 304/1000 
	 loss: 27.7598, MinusLogProbMetric: 27.7598, val_loss: 29.4497, val_MinusLogProbMetric: 29.4497

Epoch 304: val_loss did not improve from 28.44605
196/196 - 37s - loss: 27.7598 - MinusLogProbMetric: 27.7598 - val_loss: 29.4497 - val_MinusLogProbMetric: 29.4497 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 305/1000
2023-10-25 06:22:08.225 
Epoch 305/1000 
	 loss: 27.8084, MinusLogProbMetric: 27.8084, val_loss: 28.5760, val_MinusLogProbMetric: 28.5760

Epoch 305: val_loss did not improve from 28.44605
196/196 - 38s - loss: 27.8084 - MinusLogProbMetric: 27.8084 - val_loss: 28.5760 - val_MinusLogProbMetric: 28.5760 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 306/1000
2023-10-25 06:22:46.281 
Epoch 306/1000 
	 loss: 27.8067, MinusLogProbMetric: 27.8067, val_loss: 29.0362, val_MinusLogProbMetric: 29.0362

Epoch 306: val_loss did not improve from 28.44605
196/196 - 38s - loss: 27.8067 - MinusLogProbMetric: 27.8067 - val_loss: 29.0362 - val_MinusLogProbMetric: 29.0362 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 307/1000
2023-10-25 06:23:23.918 
Epoch 307/1000 
	 loss: 27.7968, MinusLogProbMetric: 27.7968, val_loss: 29.1777, val_MinusLogProbMetric: 29.1777

Epoch 307: val_loss did not improve from 28.44605
196/196 - 38s - loss: 27.7968 - MinusLogProbMetric: 27.7968 - val_loss: 29.1777 - val_MinusLogProbMetric: 29.1777 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 308/1000
2023-10-25 06:24:00.806 
Epoch 308/1000 
	 loss: 27.7325, MinusLogProbMetric: 27.7325, val_loss: 28.7257, val_MinusLogProbMetric: 28.7257

Epoch 308: val_loss did not improve from 28.44605
196/196 - 37s - loss: 27.7325 - MinusLogProbMetric: 27.7325 - val_loss: 28.7257 - val_MinusLogProbMetric: 28.7257 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 309/1000
2023-10-25 06:24:38.238 
Epoch 309/1000 
	 loss: 27.8328, MinusLogProbMetric: 27.8328, val_loss: 29.1045, val_MinusLogProbMetric: 29.1045

Epoch 309: val_loss did not improve from 28.44605
196/196 - 37s - loss: 27.8328 - MinusLogProbMetric: 27.8328 - val_loss: 29.1045 - val_MinusLogProbMetric: 29.1045 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 310/1000
2023-10-25 06:25:15.574 
Epoch 310/1000 
	 loss: 27.7808, MinusLogProbMetric: 27.7808, val_loss: 28.6269, val_MinusLogProbMetric: 28.6269

Epoch 310: val_loss did not improve from 28.44605
196/196 - 37s - loss: 27.7808 - MinusLogProbMetric: 27.7808 - val_loss: 28.6269 - val_MinusLogProbMetric: 28.6269 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 311/1000
2023-10-25 06:25:54.308 
Epoch 311/1000 
	 loss: 27.7757, MinusLogProbMetric: 27.7757, val_loss: 29.7988, val_MinusLogProbMetric: 29.7988

Epoch 311: val_loss did not improve from 28.44605
196/196 - 39s - loss: 27.7757 - MinusLogProbMetric: 27.7757 - val_loss: 29.7988 - val_MinusLogProbMetric: 29.7988 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 312/1000
2023-10-25 06:26:33.005 
Epoch 312/1000 
	 loss: 27.8011, MinusLogProbMetric: 27.8011, val_loss: 28.7487, val_MinusLogProbMetric: 28.7487

Epoch 312: val_loss did not improve from 28.44605
196/196 - 39s - loss: 27.8011 - MinusLogProbMetric: 27.8011 - val_loss: 28.7487 - val_MinusLogProbMetric: 28.7487 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 313/1000
2023-10-25 06:27:10.821 
Epoch 313/1000 
	 loss: 27.8528, MinusLogProbMetric: 27.8528, val_loss: 28.6086, val_MinusLogProbMetric: 28.6086

Epoch 313: val_loss did not improve from 28.44605
196/196 - 38s - loss: 27.8528 - MinusLogProbMetric: 27.8528 - val_loss: 28.6086 - val_MinusLogProbMetric: 28.6086 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 314/1000
2023-10-25 06:27:46.956 
Epoch 314/1000 
	 loss: 27.2972, MinusLogProbMetric: 27.2972, val_loss: 28.3397, val_MinusLogProbMetric: 28.3397

Epoch 314: val_loss improved from 28.44605 to 28.33973, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 37s - loss: 27.2972 - MinusLogProbMetric: 27.2972 - val_loss: 28.3397 - val_MinusLogProbMetric: 28.3397 - lr: 5.0000e-04 - 37s/epoch - 188ms/step
Epoch 315/1000
2023-10-25 06:28:23.794 
Epoch 315/1000 
	 loss: 27.3011, MinusLogProbMetric: 27.3011, val_loss: 28.1986, val_MinusLogProbMetric: 28.1986

Epoch 315: val_loss improved from 28.33973 to 28.19860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 37s - loss: 27.3011 - MinusLogProbMetric: 27.3011 - val_loss: 28.1986 - val_MinusLogProbMetric: 28.1986 - lr: 5.0000e-04 - 37s/epoch - 188ms/step
Epoch 316/1000
2023-10-25 06:29:01.844 
Epoch 316/1000 
	 loss: 27.2780, MinusLogProbMetric: 27.2780, val_loss: 28.2631, val_MinusLogProbMetric: 28.2631

Epoch 316: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.2780 - MinusLogProbMetric: 27.2780 - val_loss: 28.2631 - val_MinusLogProbMetric: 28.2631 - lr: 5.0000e-04 - 37s/epoch - 191ms/step
Epoch 317/1000
2023-10-25 06:29:40.509 
Epoch 317/1000 
	 loss: 27.2652, MinusLogProbMetric: 27.2652, val_loss: 28.3696, val_MinusLogProbMetric: 28.3696

Epoch 317: val_loss did not improve from 28.19860
196/196 - 39s - loss: 27.2652 - MinusLogProbMetric: 27.2652 - val_loss: 28.3696 - val_MinusLogProbMetric: 28.3696 - lr: 5.0000e-04 - 39s/epoch - 197ms/step
Epoch 318/1000
2023-10-25 06:30:17.139 
Epoch 318/1000 
	 loss: 27.2826, MinusLogProbMetric: 27.2826, val_loss: 28.4338, val_MinusLogProbMetric: 28.4338

Epoch 318: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.2826 - MinusLogProbMetric: 27.2826 - val_loss: 28.4338 - val_MinusLogProbMetric: 28.4338 - lr: 5.0000e-04 - 37s/epoch - 187ms/step
Epoch 319/1000
2023-10-25 06:30:55.406 
Epoch 319/1000 
	 loss: 27.3141, MinusLogProbMetric: 27.3141, val_loss: 28.4479, val_MinusLogProbMetric: 28.4479

Epoch 319: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.3141 - MinusLogProbMetric: 27.3141 - val_loss: 28.4479 - val_MinusLogProbMetric: 28.4479 - lr: 5.0000e-04 - 38s/epoch - 195ms/step
Epoch 320/1000
2023-10-25 06:31:33.298 
Epoch 320/1000 
	 loss: 27.2885, MinusLogProbMetric: 27.2885, val_loss: 28.2345, val_MinusLogProbMetric: 28.2345

Epoch 320: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.2885 - MinusLogProbMetric: 27.2885 - val_loss: 28.2345 - val_MinusLogProbMetric: 28.2345 - lr: 5.0000e-04 - 38s/epoch - 193ms/step
Epoch 321/1000
2023-10-25 06:32:10.249 
Epoch 321/1000 
	 loss: 27.2942, MinusLogProbMetric: 27.2942, val_loss: 28.2999, val_MinusLogProbMetric: 28.2999

Epoch 321: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.2942 - MinusLogProbMetric: 27.2942 - val_loss: 28.2999 - val_MinusLogProbMetric: 28.2999 - lr: 5.0000e-04 - 37s/epoch - 189ms/step
Epoch 322/1000
2023-10-25 06:32:46.779 
Epoch 322/1000 
	 loss: 27.2910, MinusLogProbMetric: 27.2910, val_loss: 28.3924, val_MinusLogProbMetric: 28.3924

Epoch 322: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.2910 - MinusLogProbMetric: 27.2910 - val_loss: 28.3924 - val_MinusLogProbMetric: 28.3924 - lr: 5.0000e-04 - 37s/epoch - 186ms/step
Epoch 323/1000
2023-10-25 06:33:24.133 
Epoch 323/1000 
	 loss: 27.2876, MinusLogProbMetric: 27.2876, val_loss: 28.2580, val_MinusLogProbMetric: 28.2580

Epoch 323: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.2876 - MinusLogProbMetric: 27.2876 - val_loss: 28.2580 - val_MinusLogProbMetric: 28.2580 - lr: 5.0000e-04 - 37s/epoch - 191ms/step
Epoch 324/1000
2023-10-25 06:34:01.888 
Epoch 324/1000 
	 loss: 27.2639, MinusLogProbMetric: 27.2639, val_loss: 28.3369, val_MinusLogProbMetric: 28.3369

Epoch 324: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.2639 - MinusLogProbMetric: 27.2639 - val_loss: 28.3369 - val_MinusLogProbMetric: 28.3369 - lr: 5.0000e-04 - 38s/epoch - 193ms/step
Epoch 325/1000
2023-10-25 06:34:39.012 
Epoch 325/1000 
	 loss: 27.3062, MinusLogProbMetric: 27.3062, val_loss: 28.2842, val_MinusLogProbMetric: 28.2842

Epoch 325: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.3062 - MinusLogProbMetric: 27.3062 - val_loss: 28.2842 - val_MinusLogProbMetric: 28.2842 - lr: 5.0000e-04 - 37s/epoch - 189ms/step
Epoch 326/1000
2023-10-25 06:35:16.046 
Epoch 326/1000 
	 loss: 27.2774, MinusLogProbMetric: 27.2774, val_loss: 28.2377, val_MinusLogProbMetric: 28.2377

Epoch 326: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.2774 - MinusLogProbMetric: 27.2774 - val_loss: 28.2377 - val_MinusLogProbMetric: 28.2377 - lr: 5.0000e-04 - 37s/epoch - 189ms/step
Epoch 327/1000
2023-10-25 06:35:54.372 
Epoch 327/1000 
	 loss: 27.2598, MinusLogProbMetric: 27.2598, val_loss: 28.2924, val_MinusLogProbMetric: 28.2924

Epoch 327: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.2598 - MinusLogProbMetric: 27.2598 - val_loss: 28.2924 - val_MinusLogProbMetric: 28.2924 - lr: 5.0000e-04 - 38s/epoch - 196ms/step
Epoch 328/1000
2023-10-25 06:36:31.676 
Epoch 328/1000 
	 loss: 27.2626, MinusLogProbMetric: 27.2626, val_loss: 28.3159, val_MinusLogProbMetric: 28.3159

Epoch 328: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.2626 - MinusLogProbMetric: 27.2626 - val_loss: 28.3159 - val_MinusLogProbMetric: 28.3159 - lr: 5.0000e-04 - 37s/epoch - 190ms/step
Epoch 329/1000
2023-10-25 06:37:10.157 
Epoch 329/1000 
	 loss: 27.2844, MinusLogProbMetric: 27.2844, val_loss: 28.3971, val_MinusLogProbMetric: 28.3971

Epoch 329: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.2844 - MinusLogProbMetric: 27.2844 - val_loss: 28.3971 - val_MinusLogProbMetric: 28.3971 - lr: 5.0000e-04 - 38s/epoch - 196ms/step
Epoch 330/1000
2023-10-25 06:37:48.764 
Epoch 330/1000 
	 loss: 27.2568, MinusLogProbMetric: 27.2568, val_loss: 28.3575, val_MinusLogProbMetric: 28.3575

Epoch 330: val_loss did not improve from 28.19860
196/196 - 39s - loss: 27.2568 - MinusLogProbMetric: 27.2568 - val_loss: 28.3575 - val_MinusLogProbMetric: 28.3575 - lr: 5.0000e-04 - 39s/epoch - 197ms/step
Epoch 331/1000
2023-10-25 06:38:25.572 
Epoch 331/1000 
	 loss: 27.2630, MinusLogProbMetric: 27.2630, val_loss: 28.2700, val_MinusLogProbMetric: 28.2700

Epoch 331: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.2630 - MinusLogProbMetric: 27.2630 - val_loss: 28.2700 - val_MinusLogProbMetric: 28.2700 - lr: 5.0000e-04 - 37s/epoch - 188ms/step
Epoch 332/1000
2023-10-25 06:39:02.459 
Epoch 332/1000 
	 loss: 27.2591, MinusLogProbMetric: 27.2591, val_loss: 28.3281, val_MinusLogProbMetric: 28.3281

Epoch 332: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.2591 - MinusLogProbMetric: 27.2591 - val_loss: 28.3281 - val_MinusLogProbMetric: 28.3281 - lr: 5.0000e-04 - 37s/epoch - 188ms/step
Epoch 333/1000
2023-10-25 06:39:40.225 
Epoch 333/1000 
	 loss: 27.2661, MinusLogProbMetric: 27.2661, val_loss: 28.3382, val_MinusLogProbMetric: 28.3382

Epoch 333: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.2661 - MinusLogProbMetric: 27.2661 - val_loss: 28.3382 - val_MinusLogProbMetric: 28.3382 - lr: 5.0000e-04 - 38s/epoch - 193ms/step
Epoch 334/1000
2023-10-25 06:40:19.137 
Epoch 334/1000 
	 loss: 27.2696, MinusLogProbMetric: 27.2696, val_loss: 28.2318, val_MinusLogProbMetric: 28.2318

Epoch 334: val_loss did not improve from 28.19860
196/196 - 39s - loss: 27.2696 - MinusLogProbMetric: 27.2696 - val_loss: 28.2318 - val_MinusLogProbMetric: 28.2318 - lr: 5.0000e-04 - 39s/epoch - 199ms/step
Epoch 335/1000
2023-10-25 06:40:57.863 
Epoch 335/1000 
	 loss: 27.2760, MinusLogProbMetric: 27.2760, val_loss: 28.3117, val_MinusLogProbMetric: 28.3117

Epoch 335: val_loss did not improve from 28.19860
196/196 - 39s - loss: 27.2760 - MinusLogProbMetric: 27.2760 - val_loss: 28.3117 - val_MinusLogProbMetric: 28.3117 - lr: 5.0000e-04 - 39s/epoch - 198ms/step
Epoch 336/1000
2023-10-25 06:41:34.507 
Epoch 336/1000 
	 loss: 27.2547, MinusLogProbMetric: 27.2547, val_loss: 28.3566, val_MinusLogProbMetric: 28.3566

Epoch 336: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.2547 - MinusLogProbMetric: 27.2547 - val_loss: 28.3566 - val_MinusLogProbMetric: 28.3566 - lr: 5.0000e-04 - 37s/epoch - 187ms/step
Epoch 337/1000
2023-10-25 06:42:11.343 
Epoch 337/1000 
	 loss: 27.2892, MinusLogProbMetric: 27.2892, val_loss: 28.3191, val_MinusLogProbMetric: 28.3191

Epoch 337: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.2892 - MinusLogProbMetric: 27.2892 - val_loss: 28.3191 - val_MinusLogProbMetric: 28.3191 - lr: 5.0000e-04 - 37s/epoch - 188ms/step
Epoch 338/1000
2023-10-25 06:42:49.677 
Epoch 338/1000 
	 loss: 27.2721, MinusLogProbMetric: 27.2721, val_loss: 28.3445, val_MinusLogProbMetric: 28.3445

Epoch 338: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.2721 - MinusLogProbMetric: 27.2721 - val_loss: 28.3445 - val_MinusLogProbMetric: 28.3445 - lr: 5.0000e-04 - 38s/epoch - 196ms/step
Epoch 339/1000
2023-10-25 06:43:28.524 
Epoch 339/1000 
	 loss: 27.2638, MinusLogProbMetric: 27.2638, val_loss: 28.2887, val_MinusLogProbMetric: 28.2887

Epoch 339: val_loss did not improve from 28.19860
196/196 - 39s - loss: 27.2638 - MinusLogProbMetric: 27.2638 - val_loss: 28.2887 - val_MinusLogProbMetric: 28.2887 - lr: 5.0000e-04 - 39s/epoch - 198ms/step
Epoch 340/1000
2023-10-25 06:44:07.379 
Epoch 340/1000 
	 loss: 27.2375, MinusLogProbMetric: 27.2375, val_loss: 28.3518, val_MinusLogProbMetric: 28.3518

Epoch 340: val_loss did not improve from 28.19860
196/196 - 39s - loss: 27.2375 - MinusLogProbMetric: 27.2375 - val_loss: 28.3518 - val_MinusLogProbMetric: 28.3518 - lr: 5.0000e-04 - 39s/epoch - 198ms/step
Epoch 341/1000
2023-10-25 06:44:44.123 
Epoch 341/1000 
	 loss: 27.2541, MinusLogProbMetric: 27.2541, val_loss: 28.2622, val_MinusLogProbMetric: 28.2622

Epoch 341: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.2541 - MinusLogProbMetric: 27.2541 - val_loss: 28.2622 - val_MinusLogProbMetric: 28.2622 - lr: 5.0000e-04 - 37s/epoch - 187ms/step
Epoch 342/1000
2023-10-25 06:45:22.234 
Epoch 342/1000 
	 loss: 27.2537, MinusLogProbMetric: 27.2537, val_loss: 28.4372, val_MinusLogProbMetric: 28.4372

Epoch 342: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.2537 - MinusLogProbMetric: 27.2537 - val_loss: 28.4372 - val_MinusLogProbMetric: 28.4372 - lr: 5.0000e-04 - 38s/epoch - 194ms/step
Epoch 343/1000
2023-10-25 06:45:58.711 
Epoch 343/1000 
	 loss: 27.2374, MinusLogProbMetric: 27.2374, val_loss: 28.2698, val_MinusLogProbMetric: 28.2698

Epoch 343: val_loss did not improve from 28.19860
196/196 - 36s - loss: 27.2374 - MinusLogProbMetric: 27.2374 - val_loss: 28.2698 - val_MinusLogProbMetric: 28.2698 - lr: 5.0000e-04 - 36s/epoch - 186ms/step
Epoch 344/1000
2023-10-25 06:46:36.266 
Epoch 344/1000 
	 loss: 27.2699, MinusLogProbMetric: 27.2699, val_loss: 28.2521, val_MinusLogProbMetric: 28.2521

Epoch 344: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.2699 - MinusLogProbMetric: 27.2699 - val_loss: 28.2521 - val_MinusLogProbMetric: 28.2521 - lr: 5.0000e-04 - 38s/epoch - 192ms/step
Epoch 345/1000
2023-10-25 06:47:13.573 
Epoch 345/1000 
	 loss: 27.2943, MinusLogProbMetric: 27.2943, val_loss: 28.3326, val_MinusLogProbMetric: 28.3326

Epoch 345: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.2943 - MinusLogProbMetric: 27.2943 - val_loss: 28.3326 - val_MinusLogProbMetric: 28.3326 - lr: 5.0000e-04 - 37s/epoch - 190ms/step
Epoch 346/1000
2023-10-25 06:47:51.709 
Epoch 346/1000 
	 loss: 27.2467, MinusLogProbMetric: 27.2467, val_loss: 28.2026, val_MinusLogProbMetric: 28.2026

Epoch 346: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.2467 - MinusLogProbMetric: 27.2467 - val_loss: 28.2026 - val_MinusLogProbMetric: 28.2026 - lr: 5.0000e-04 - 38s/epoch - 195ms/step
Epoch 347/1000
2023-10-25 06:48:29.050 
Epoch 347/1000 
	 loss: 27.2380, MinusLogProbMetric: 27.2380, val_loss: 28.2550, val_MinusLogProbMetric: 28.2550

Epoch 347: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.2380 - MinusLogProbMetric: 27.2380 - val_loss: 28.2550 - val_MinusLogProbMetric: 28.2550 - lr: 5.0000e-04 - 37s/epoch - 190ms/step
Epoch 348/1000
2023-10-25 06:49:06.533 
Epoch 348/1000 
	 loss: 27.2319, MinusLogProbMetric: 27.2319, val_loss: 28.2795, val_MinusLogProbMetric: 28.2795

Epoch 348: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.2319 - MinusLogProbMetric: 27.2319 - val_loss: 28.2795 - val_MinusLogProbMetric: 28.2795 - lr: 5.0000e-04 - 37s/epoch - 191ms/step
Epoch 349/1000
2023-10-25 06:49:42.880 
Epoch 349/1000 
	 loss: 27.2481, MinusLogProbMetric: 27.2481, val_loss: 28.5140, val_MinusLogProbMetric: 28.5140

Epoch 349: val_loss did not improve from 28.19860
196/196 - 36s - loss: 27.2481 - MinusLogProbMetric: 27.2481 - val_loss: 28.5140 - val_MinusLogProbMetric: 28.5140 - lr: 5.0000e-04 - 36s/epoch - 185ms/step
Epoch 350/1000
2023-10-25 06:50:18.957 
Epoch 350/1000 
	 loss: 27.2399, MinusLogProbMetric: 27.2399, val_loss: 28.3444, val_MinusLogProbMetric: 28.3444

Epoch 350: val_loss did not improve from 28.19860
196/196 - 36s - loss: 27.2399 - MinusLogProbMetric: 27.2399 - val_loss: 28.3444 - val_MinusLogProbMetric: 28.3444 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 351/1000
2023-10-25 06:50:56.060 
Epoch 351/1000 
	 loss: 27.2469, MinusLogProbMetric: 27.2469, val_loss: 28.2684, val_MinusLogProbMetric: 28.2684

Epoch 351: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.2469 - MinusLogProbMetric: 27.2469 - val_loss: 28.2684 - val_MinusLogProbMetric: 28.2684 - lr: 5.0000e-04 - 37s/epoch - 189ms/step
Epoch 352/1000
2023-10-25 06:51:34.173 
Epoch 352/1000 
	 loss: 27.2250, MinusLogProbMetric: 27.2250, val_loss: 28.2923, val_MinusLogProbMetric: 28.2923

Epoch 352: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.2250 - MinusLogProbMetric: 27.2250 - val_loss: 28.2923 - val_MinusLogProbMetric: 28.2923 - lr: 5.0000e-04 - 38s/epoch - 194ms/step
Epoch 353/1000
2023-10-25 06:52:11.922 
Epoch 353/1000 
	 loss: 27.2154, MinusLogProbMetric: 27.2154, val_loss: 28.5115, val_MinusLogProbMetric: 28.5115

Epoch 353: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.2154 - MinusLogProbMetric: 27.2154 - val_loss: 28.5115 - val_MinusLogProbMetric: 28.5115 - lr: 5.0000e-04 - 38s/epoch - 193ms/step
Epoch 354/1000
2023-10-25 06:52:50.354 
Epoch 354/1000 
	 loss: 27.2191, MinusLogProbMetric: 27.2191, val_loss: 28.4937, val_MinusLogProbMetric: 28.4937

Epoch 354: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.2191 - MinusLogProbMetric: 27.2191 - val_loss: 28.4937 - val_MinusLogProbMetric: 28.4937 - lr: 5.0000e-04 - 38s/epoch - 196ms/step
Epoch 355/1000
2023-10-25 06:53:27.404 
Epoch 355/1000 
	 loss: 27.2354, MinusLogProbMetric: 27.2354, val_loss: 28.2787, val_MinusLogProbMetric: 28.2787

Epoch 355: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.2354 - MinusLogProbMetric: 27.2354 - val_loss: 28.2787 - val_MinusLogProbMetric: 28.2787 - lr: 5.0000e-04 - 37s/epoch - 189ms/step
Epoch 356/1000
2023-10-25 06:54:05.319 
Epoch 356/1000 
	 loss: 27.2201, MinusLogProbMetric: 27.2201, val_loss: 28.3938, val_MinusLogProbMetric: 28.3938

Epoch 356: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.2201 - MinusLogProbMetric: 27.2201 - val_loss: 28.3938 - val_MinusLogProbMetric: 28.3938 - lr: 5.0000e-04 - 38s/epoch - 193ms/step
Epoch 357/1000
2023-10-25 06:54:43.380 
Epoch 357/1000 
	 loss: 27.2369, MinusLogProbMetric: 27.2369, val_loss: 28.4177, val_MinusLogProbMetric: 28.4177

Epoch 357: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.2369 - MinusLogProbMetric: 27.2369 - val_loss: 28.4177 - val_MinusLogProbMetric: 28.4177 - lr: 5.0000e-04 - 38s/epoch - 194ms/step
Epoch 358/1000
2023-10-25 06:55:20.544 
Epoch 358/1000 
	 loss: 27.2863, MinusLogProbMetric: 27.2863, val_loss: 28.2503, val_MinusLogProbMetric: 28.2503

Epoch 358: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.2863 - MinusLogProbMetric: 27.2863 - val_loss: 28.2503 - val_MinusLogProbMetric: 28.2503 - lr: 5.0000e-04 - 37s/epoch - 190ms/step
Epoch 359/1000
2023-10-25 06:55:57.436 
Epoch 359/1000 
	 loss: 27.2055, MinusLogProbMetric: 27.2055, val_loss: 28.5036, val_MinusLogProbMetric: 28.5036

Epoch 359: val_loss did not improve from 28.19860
196/196 - 37s - loss: 27.2055 - MinusLogProbMetric: 27.2055 - val_loss: 28.5036 - val_MinusLogProbMetric: 28.5036 - lr: 5.0000e-04 - 37s/epoch - 188ms/step
Epoch 360/1000
2023-10-25 06:56:34.951 
Epoch 360/1000 
	 loss: 27.2392, MinusLogProbMetric: 27.2392, val_loss: 28.2807, val_MinusLogProbMetric: 28.2807

Epoch 360: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.2392 - MinusLogProbMetric: 27.2392 - val_loss: 28.2807 - val_MinusLogProbMetric: 28.2807 - lr: 5.0000e-04 - 38s/epoch - 191ms/step
Epoch 361/1000
2023-10-25 06:57:12.829 
Epoch 361/1000 
	 loss: 27.2413, MinusLogProbMetric: 27.2413, val_loss: 28.4131, val_MinusLogProbMetric: 28.4131

Epoch 361: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.2413 - MinusLogProbMetric: 27.2413 - val_loss: 28.4131 - val_MinusLogProbMetric: 28.4131 - lr: 5.0000e-04 - 38s/epoch - 193ms/step
Epoch 362/1000
2023-10-25 06:57:50.722 
Epoch 362/1000 
	 loss: 27.2472, MinusLogProbMetric: 27.2472, val_loss: 28.5889, val_MinusLogProbMetric: 28.5889

Epoch 362: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.2472 - MinusLogProbMetric: 27.2472 - val_loss: 28.5889 - val_MinusLogProbMetric: 28.5889 - lr: 5.0000e-04 - 38s/epoch - 193ms/step
Epoch 363/1000
2023-10-25 06:58:26.964 
Epoch 363/1000 
	 loss: 27.2177, MinusLogProbMetric: 27.2177, val_loss: 28.3099, val_MinusLogProbMetric: 28.3099

Epoch 363: val_loss did not improve from 28.19860
196/196 - 36s - loss: 27.2177 - MinusLogProbMetric: 27.2177 - val_loss: 28.3099 - val_MinusLogProbMetric: 28.3099 - lr: 5.0000e-04 - 36s/epoch - 185ms/step
Epoch 364/1000
2023-10-25 06:59:04.619 
Epoch 364/1000 
	 loss: 27.2450, MinusLogProbMetric: 27.2450, val_loss: 28.3598, val_MinusLogProbMetric: 28.3598

Epoch 364: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.2450 - MinusLogProbMetric: 27.2450 - val_loss: 28.3598 - val_MinusLogProbMetric: 28.3598 - lr: 5.0000e-04 - 38s/epoch - 192ms/step
Epoch 365/1000
2023-10-25 06:59:42.990 
Epoch 365/1000 
	 loss: 27.2260, MinusLogProbMetric: 27.2260, val_loss: 28.2549, val_MinusLogProbMetric: 28.2549

Epoch 365: val_loss did not improve from 28.19860
196/196 - 38s - loss: 27.2260 - MinusLogProbMetric: 27.2260 - val_loss: 28.2549 - val_MinusLogProbMetric: 28.2549 - lr: 5.0000e-04 - 38s/epoch - 196ms/step
Epoch 366/1000
2023-10-25 07:00:19.482 
Epoch 366/1000 
	 loss: 27.0451, MinusLogProbMetric: 27.0451, val_loss: 28.1838, val_MinusLogProbMetric: 28.1838

Epoch 366: val_loss improved from 28.19860 to 28.18377, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 37s - loss: 27.0451 - MinusLogProbMetric: 27.0451 - val_loss: 28.1838 - val_MinusLogProbMetric: 28.1838 - lr: 2.5000e-04 - 37s/epoch - 190ms/step
Epoch 367/1000
2023-10-25 07:00:58.972 
Epoch 367/1000 
	 loss: 27.0465, MinusLogProbMetric: 27.0465, val_loss: 28.1713, val_MinusLogProbMetric: 28.1713

Epoch 367: val_loss improved from 28.18377 to 28.17134, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 27.0465 - MinusLogProbMetric: 27.0465 - val_loss: 28.1713 - val_MinusLogProbMetric: 28.1713 - lr: 2.5000e-04 - 39s/epoch - 201ms/step
Epoch 368/1000
2023-10-25 07:01:38.391 
Epoch 368/1000 
	 loss: 27.0361, MinusLogProbMetric: 27.0361, val_loss: 28.2383, val_MinusLogProbMetric: 28.2383

Epoch 368: val_loss did not improve from 28.17134
196/196 - 39s - loss: 27.0361 - MinusLogProbMetric: 27.0361 - val_loss: 28.2383 - val_MinusLogProbMetric: 28.2383 - lr: 2.5000e-04 - 39s/epoch - 198ms/step
Epoch 369/1000
2023-10-25 07:02:16.250 
Epoch 369/1000 
	 loss: 27.0357, MinusLogProbMetric: 27.0357, val_loss: 28.1697, val_MinusLogProbMetric: 28.1697

Epoch 369: val_loss improved from 28.17134 to 28.16969, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 39s - loss: 27.0357 - MinusLogProbMetric: 27.0357 - val_loss: 28.1697 - val_MinusLogProbMetric: 28.1697 - lr: 2.5000e-04 - 39s/epoch - 197ms/step
Epoch 370/1000
2023-10-25 07:02:54.079 
Epoch 370/1000 
	 loss: 27.0419, MinusLogProbMetric: 27.0419, val_loss: 28.1796, val_MinusLogProbMetric: 28.1796

Epoch 370: val_loss did not improve from 28.16969
196/196 - 37s - loss: 27.0419 - MinusLogProbMetric: 27.0419 - val_loss: 28.1796 - val_MinusLogProbMetric: 28.1796 - lr: 2.5000e-04 - 37s/epoch - 189ms/step
Epoch 371/1000
2023-10-25 07:03:30.212 
Epoch 371/1000 
	 loss: 27.0408, MinusLogProbMetric: 27.0408, val_loss: 28.1755, val_MinusLogProbMetric: 28.1755

Epoch 371: val_loss did not improve from 28.16969
196/196 - 36s - loss: 27.0408 - MinusLogProbMetric: 27.0408 - val_loss: 28.1755 - val_MinusLogProbMetric: 28.1755 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 372/1000
2023-10-25 07:04:08.987 
Epoch 372/1000 
	 loss: 27.0428, MinusLogProbMetric: 27.0428, val_loss: 28.1976, val_MinusLogProbMetric: 28.1976

Epoch 372: val_loss did not improve from 28.16969
196/196 - 39s - loss: 27.0428 - MinusLogProbMetric: 27.0428 - val_loss: 28.1976 - val_MinusLogProbMetric: 28.1976 - lr: 2.5000e-04 - 39s/epoch - 198ms/step
Epoch 373/1000
2023-10-25 07:04:47.916 
Epoch 373/1000 
	 loss: 27.0497, MinusLogProbMetric: 27.0497, val_loss: 28.2422, val_MinusLogProbMetric: 28.2422

Epoch 373: val_loss did not improve from 28.16969
196/196 - 39s - loss: 27.0497 - MinusLogProbMetric: 27.0497 - val_loss: 28.2422 - val_MinusLogProbMetric: 28.2422 - lr: 2.5000e-04 - 39s/epoch - 199ms/step
Epoch 374/1000
2023-10-25 07:05:25.668 
Epoch 374/1000 
	 loss: 27.0396, MinusLogProbMetric: 27.0396, val_loss: 28.1968, val_MinusLogProbMetric: 28.1968

Epoch 374: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0396 - MinusLogProbMetric: 27.0396 - val_loss: 28.1968 - val_MinusLogProbMetric: 28.1968 - lr: 2.5000e-04 - 38s/epoch - 193ms/step
Epoch 375/1000
2023-10-25 07:06:02.911 
Epoch 375/1000 
	 loss: 27.0508, MinusLogProbMetric: 27.0508, val_loss: 28.2834, val_MinusLogProbMetric: 28.2834

Epoch 375: val_loss did not improve from 28.16969
196/196 - 37s - loss: 27.0508 - MinusLogProbMetric: 27.0508 - val_loss: 28.2834 - val_MinusLogProbMetric: 28.2834 - lr: 2.5000e-04 - 37s/epoch - 190ms/step
Epoch 376/1000
2023-10-25 07:06:41.183 
Epoch 376/1000 
	 loss: 27.0435, MinusLogProbMetric: 27.0435, val_loss: 28.2190, val_MinusLogProbMetric: 28.2190

Epoch 376: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0435 - MinusLogProbMetric: 27.0435 - val_loss: 28.2190 - val_MinusLogProbMetric: 28.2190 - lr: 2.5000e-04 - 38s/epoch - 195ms/step
Epoch 377/1000
2023-10-25 07:07:17.058 
Epoch 377/1000 
	 loss: 27.0414, MinusLogProbMetric: 27.0414, val_loss: 28.2121, val_MinusLogProbMetric: 28.2121

Epoch 377: val_loss did not improve from 28.16969
196/196 - 36s - loss: 27.0414 - MinusLogProbMetric: 27.0414 - val_loss: 28.2121 - val_MinusLogProbMetric: 28.2121 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 378/1000
2023-10-25 07:07:55.805 
Epoch 378/1000 
	 loss: 27.0311, MinusLogProbMetric: 27.0311, val_loss: 28.2086, val_MinusLogProbMetric: 28.2086

Epoch 378: val_loss did not improve from 28.16969
196/196 - 39s - loss: 27.0311 - MinusLogProbMetric: 27.0311 - val_loss: 28.2086 - val_MinusLogProbMetric: 28.2086 - lr: 2.5000e-04 - 39s/epoch - 198ms/step
Epoch 379/1000
2023-10-25 07:08:32.951 
Epoch 379/1000 
	 loss: 27.0348, MinusLogProbMetric: 27.0348, val_loss: 28.1830, val_MinusLogProbMetric: 28.1830

Epoch 379: val_loss did not improve from 28.16969
196/196 - 37s - loss: 27.0348 - MinusLogProbMetric: 27.0348 - val_loss: 28.1830 - val_MinusLogProbMetric: 28.1830 - lr: 2.5000e-04 - 37s/epoch - 190ms/step
Epoch 380/1000
2023-10-25 07:09:08.622 
Epoch 380/1000 
	 loss: 27.0369, MinusLogProbMetric: 27.0369, val_loss: 28.2006, val_MinusLogProbMetric: 28.2006

Epoch 380: val_loss did not improve from 28.16969
196/196 - 36s - loss: 27.0369 - MinusLogProbMetric: 27.0369 - val_loss: 28.2006 - val_MinusLogProbMetric: 28.2006 - lr: 2.5000e-04 - 36s/epoch - 182ms/step
Epoch 381/1000
2023-10-25 07:09:44.953 
Epoch 381/1000 
	 loss: 27.0342, MinusLogProbMetric: 27.0342, val_loss: 28.2386, val_MinusLogProbMetric: 28.2386

Epoch 381: val_loss did not improve from 28.16969
196/196 - 36s - loss: 27.0342 - MinusLogProbMetric: 27.0342 - val_loss: 28.2386 - val_MinusLogProbMetric: 28.2386 - lr: 2.5000e-04 - 36s/epoch - 185ms/step
Epoch 382/1000
2023-10-25 07:10:22.453 
Epoch 382/1000 
	 loss: 27.0479, MinusLogProbMetric: 27.0479, val_loss: 28.2247, val_MinusLogProbMetric: 28.2247

Epoch 382: val_loss did not improve from 28.16969
196/196 - 37s - loss: 27.0479 - MinusLogProbMetric: 27.0479 - val_loss: 28.2247 - val_MinusLogProbMetric: 28.2247 - lr: 2.5000e-04 - 37s/epoch - 191ms/step
Epoch 383/1000
2023-10-25 07:11:00.870 
Epoch 383/1000 
	 loss: 27.0362, MinusLogProbMetric: 27.0362, val_loss: 28.1819, val_MinusLogProbMetric: 28.1819

Epoch 383: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0362 - MinusLogProbMetric: 27.0362 - val_loss: 28.1819 - val_MinusLogProbMetric: 28.1819 - lr: 2.5000e-04 - 38s/epoch - 196ms/step
Epoch 384/1000
2023-10-25 07:11:37.561 
Epoch 384/1000 
	 loss: 27.0324, MinusLogProbMetric: 27.0324, val_loss: 28.2062, val_MinusLogProbMetric: 28.2062

Epoch 384: val_loss did not improve from 28.16969
196/196 - 37s - loss: 27.0324 - MinusLogProbMetric: 27.0324 - val_loss: 28.2062 - val_MinusLogProbMetric: 28.2062 - lr: 2.5000e-04 - 37s/epoch - 187ms/step
Epoch 385/1000
2023-10-25 07:12:16.255 
Epoch 385/1000 
	 loss: 27.0300, MinusLogProbMetric: 27.0300, val_loss: 28.1959, val_MinusLogProbMetric: 28.1959

Epoch 385: val_loss did not improve from 28.16969
196/196 - 39s - loss: 27.0300 - MinusLogProbMetric: 27.0300 - val_loss: 28.1959 - val_MinusLogProbMetric: 28.1959 - lr: 2.5000e-04 - 39s/epoch - 197ms/step
Epoch 386/1000
2023-10-25 07:12:54.550 
Epoch 386/1000 
	 loss: 27.0253, MinusLogProbMetric: 27.0253, val_loss: 28.1844, val_MinusLogProbMetric: 28.1844

Epoch 386: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0253 - MinusLogProbMetric: 27.0253 - val_loss: 28.1844 - val_MinusLogProbMetric: 28.1844 - lr: 2.5000e-04 - 38s/epoch - 195ms/step
Epoch 387/1000
2023-10-25 07:13:30.837 
Epoch 387/1000 
	 loss: 27.0339, MinusLogProbMetric: 27.0339, val_loss: 28.2909, val_MinusLogProbMetric: 28.2909

Epoch 387: val_loss did not improve from 28.16969
196/196 - 36s - loss: 27.0339 - MinusLogProbMetric: 27.0339 - val_loss: 28.2909 - val_MinusLogProbMetric: 28.2909 - lr: 2.5000e-04 - 36s/epoch - 185ms/step
Epoch 388/1000
2023-10-25 07:14:08.462 
Epoch 388/1000 
	 loss: 27.0250, MinusLogProbMetric: 27.0250, val_loss: 28.1914, val_MinusLogProbMetric: 28.1914

Epoch 388: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0250 - MinusLogProbMetric: 27.0250 - val_loss: 28.1914 - val_MinusLogProbMetric: 28.1914 - lr: 2.5000e-04 - 38s/epoch - 192ms/step
Epoch 389/1000
2023-10-25 07:14:45.657 
Epoch 389/1000 
	 loss: 27.0241, MinusLogProbMetric: 27.0241, val_loss: 28.2308, val_MinusLogProbMetric: 28.2308

Epoch 389: val_loss did not improve from 28.16969
196/196 - 37s - loss: 27.0241 - MinusLogProbMetric: 27.0241 - val_loss: 28.2308 - val_MinusLogProbMetric: 28.2308 - lr: 2.5000e-04 - 37s/epoch - 190ms/step
Epoch 390/1000
2023-10-25 07:15:23.244 
Epoch 390/1000 
	 loss: 27.0363, MinusLogProbMetric: 27.0363, val_loss: 28.2313, val_MinusLogProbMetric: 28.2313

Epoch 390: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0363 - MinusLogProbMetric: 27.0363 - val_loss: 28.2313 - val_MinusLogProbMetric: 28.2313 - lr: 2.5000e-04 - 38s/epoch - 192ms/step
Epoch 391/1000
2023-10-25 07:16:00.298 
Epoch 391/1000 
	 loss: 27.0332, MinusLogProbMetric: 27.0332, val_loss: 28.2719, val_MinusLogProbMetric: 28.2719

Epoch 391: val_loss did not improve from 28.16969
196/196 - 37s - loss: 27.0332 - MinusLogProbMetric: 27.0332 - val_loss: 28.2719 - val_MinusLogProbMetric: 28.2719 - lr: 2.5000e-04 - 37s/epoch - 189ms/step
Epoch 392/1000
2023-10-25 07:16:37.453 
Epoch 392/1000 
	 loss: 27.0262, MinusLogProbMetric: 27.0262, val_loss: 28.2118, val_MinusLogProbMetric: 28.2118

Epoch 392: val_loss did not improve from 28.16969
196/196 - 37s - loss: 27.0262 - MinusLogProbMetric: 27.0262 - val_loss: 28.2118 - val_MinusLogProbMetric: 28.2118 - lr: 2.5000e-04 - 37s/epoch - 190ms/step
Epoch 393/1000
2023-10-25 07:17:14.229 
Epoch 393/1000 
	 loss: 27.0207, MinusLogProbMetric: 27.0207, val_loss: 28.1959, val_MinusLogProbMetric: 28.1959

Epoch 393: val_loss did not improve from 28.16969
196/196 - 37s - loss: 27.0207 - MinusLogProbMetric: 27.0207 - val_loss: 28.1959 - val_MinusLogProbMetric: 28.1959 - lr: 2.5000e-04 - 37s/epoch - 188ms/step
Epoch 394/1000
2023-10-25 07:17:51.838 
Epoch 394/1000 
	 loss: 27.0241, MinusLogProbMetric: 27.0241, val_loss: 28.1935, val_MinusLogProbMetric: 28.1935

Epoch 394: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0241 - MinusLogProbMetric: 27.0241 - val_loss: 28.1935 - val_MinusLogProbMetric: 28.1935 - lr: 2.5000e-04 - 38s/epoch - 192ms/step
Epoch 395/1000
2023-10-25 07:18:29.608 
Epoch 395/1000 
	 loss: 27.0258, MinusLogProbMetric: 27.0258, val_loss: 28.2385, val_MinusLogProbMetric: 28.2385

Epoch 395: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0258 - MinusLogProbMetric: 27.0258 - val_loss: 28.2385 - val_MinusLogProbMetric: 28.2385 - lr: 2.5000e-04 - 38s/epoch - 193ms/step
Epoch 396/1000
2023-10-25 07:19:07.380 
Epoch 396/1000 
	 loss: 27.0216, MinusLogProbMetric: 27.0216, val_loss: 28.2198, val_MinusLogProbMetric: 28.2198

Epoch 396: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0216 - MinusLogProbMetric: 27.0216 - val_loss: 28.2198 - val_MinusLogProbMetric: 28.2198 - lr: 2.5000e-04 - 38s/epoch - 193ms/step
Epoch 397/1000
2023-10-25 07:19:45.316 
Epoch 397/1000 
	 loss: 27.0166, MinusLogProbMetric: 27.0166, val_loss: 28.1801, val_MinusLogProbMetric: 28.1801

Epoch 397: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0166 - MinusLogProbMetric: 27.0166 - val_loss: 28.1801 - val_MinusLogProbMetric: 28.1801 - lr: 2.5000e-04 - 38s/epoch - 194ms/step
Epoch 398/1000
2023-10-25 07:20:22.281 
Epoch 398/1000 
	 loss: 27.0123, MinusLogProbMetric: 27.0123, val_loss: 28.2346, val_MinusLogProbMetric: 28.2346

Epoch 398: val_loss did not improve from 28.16969
196/196 - 37s - loss: 27.0123 - MinusLogProbMetric: 27.0123 - val_loss: 28.2346 - val_MinusLogProbMetric: 28.2346 - lr: 2.5000e-04 - 37s/epoch - 189ms/step
Epoch 399/1000
2023-10-25 07:20:59.910 
Epoch 399/1000 
	 loss: 27.0190, MinusLogProbMetric: 27.0190, val_loss: 28.2130, val_MinusLogProbMetric: 28.2130

Epoch 399: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0190 - MinusLogProbMetric: 27.0190 - val_loss: 28.2130 - val_MinusLogProbMetric: 28.2130 - lr: 2.5000e-04 - 38s/epoch - 192ms/step
Epoch 400/1000
2023-10-25 07:21:37.305 
Epoch 400/1000 
	 loss: 27.0219, MinusLogProbMetric: 27.0219, val_loss: 28.3249, val_MinusLogProbMetric: 28.3249

Epoch 400: val_loss did not improve from 28.16969
196/196 - 37s - loss: 27.0219 - MinusLogProbMetric: 27.0219 - val_loss: 28.3249 - val_MinusLogProbMetric: 28.3249 - lr: 2.5000e-04 - 37s/epoch - 191ms/step
Epoch 401/1000
2023-10-25 07:22:15.588 
Epoch 401/1000 
	 loss: 27.0340, MinusLogProbMetric: 27.0340, val_loss: 28.1999, val_MinusLogProbMetric: 28.1999

Epoch 401: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0340 - MinusLogProbMetric: 27.0340 - val_loss: 28.1999 - val_MinusLogProbMetric: 28.1999 - lr: 2.5000e-04 - 38s/epoch - 195ms/step
Epoch 402/1000
2023-10-25 07:22:54.381 
Epoch 402/1000 
	 loss: 27.0125, MinusLogProbMetric: 27.0125, val_loss: 28.2880, val_MinusLogProbMetric: 28.2880

Epoch 402: val_loss did not improve from 28.16969
196/196 - 39s - loss: 27.0125 - MinusLogProbMetric: 27.0125 - val_loss: 28.2880 - val_MinusLogProbMetric: 28.2880 - lr: 2.5000e-04 - 39s/epoch - 198ms/step
Epoch 403/1000
2023-10-25 07:23:32.027 
Epoch 403/1000 
	 loss: 27.0123, MinusLogProbMetric: 27.0123, val_loss: 28.3060, val_MinusLogProbMetric: 28.3060

Epoch 403: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0123 - MinusLogProbMetric: 27.0123 - val_loss: 28.3060 - val_MinusLogProbMetric: 28.3060 - lr: 2.5000e-04 - 38s/epoch - 192ms/step
Epoch 404/1000
2023-10-25 07:24:09.947 
Epoch 404/1000 
	 loss: 27.0211, MinusLogProbMetric: 27.0211, val_loss: 28.2145, val_MinusLogProbMetric: 28.2145

Epoch 404: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0211 - MinusLogProbMetric: 27.0211 - val_loss: 28.2145 - val_MinusLogProbMetric: 28.2145 - lr: 2.5000e-04 - 38s/epoch - 193ms/step
Epoch 405/1000
2023-10-25 07:24:47.103 
Epoch 405/1000 
	 loss: 27.0303, MinusLogProbMetric: 27.0303, val_loss: 28.3173, val_MinusLogProbMetric: 28.3173

Epoch 405: val_loss did not improve from 28.16969
196/196 - 37s - loss: 27.0303 - MinusLogProbMetric: 27.0303 - val_loss: 28.3173 - val_MinusLogProbMetric: 28.3173 - lr: 2.5000e-04 - 37s/epoch - 190ms/step
Epoch 406/1000
2023-10-25 07:25:25.440 
Epoch 406/1000 
	 loss: 27.0276, MinusLogProbMetric: 27.0276, val_loss: 28.2658, val_MinusLogProbMetric: 28.2658

Epoch 406: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0276 - MinusLogProbMetric: 27.0276 - val_loss: 28.2658 - val_MinusLogProbMetric: 28.2658 - lr: 2.5000e-04 - 38s/epoch - 196ms/step
Epoch 407/1000
2023-10-25 07:26:03.121 
Epoch 407/1000 
	 loss: 27.0214, MinusLogProbMetric: 27.0214, val_loss: 28.1953, val_MinusLogProbMetric: 28.1953

Epoch 407: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0214 - MinusLogProbMetric: 27.0214 - val_loss: 28.1953 - val_MinusLogProbMetric: 28.1953 - lr: 2.5000e-04 - 38s/epoch - 192ms/step
Epoch 408/1000
2023-10-25 07:26:40.466 
Epoch 408/1000 
	 loss: 27.0134, MinusLogProbMetric: 27.0134, val_loss: 28.2015, val_MinusLogProbMetric: 28.2015

Epoch 408: val_loss did not improve from 28.16969
196/196 - 37s - loss: 27.0134 - MinusLogProbMetric: 27.0134 - val_loss: 28.2015 - val_MinusLogProbMetric: 28.2015 - lr: 2.5000e-04 - 37s/epoch - 191ms/step
Epoch 409/1000
2023-10-25 07:27:18.513 
Epoch 409/1000 
	 loss: 27.0230, MinusLogProbMetric: 27.0230, val_loss: 28.1879, val_MinusLogProbMetric: 28.1879

Epoch 409: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0230 - MinusLogProbMetric: 27.0230 - val_loss: 28.1879 - val_MinusLogProbMetric: 28.1879 - lr: 2.5000e-04 - 38s/epoch - 194ms/step
Epoch 410/1000
2023-10-25 07:27:57.202 
Epoch 410/1000 
	 loss: 27.0262, MinusLogProbMetric: 27.0262, val_loss: 28.2648, val_MinusLogProbMetric: 28.2648

Epoch 410: val_loss did not improve from 28.16969
196/196 - 39s - loss: 27.0262 - MinusLogProbMetric: 27.0262 - val_loss: 28.2648 - val_MinusLogProbMetric: 28.2648 - lr: 2.5000e-04 - 39s/epoch - 197ms/step
Epoch 411/1000
2023-10-25 07:28:32.611 
Epoch 411/1000 
	 loss: 27.0169, MinusLogProbMetric: 27.0169, val_loss: 28.2268, val_MinusLogProbMetric: 28.2268

Epoch 411: val_loss did not improve from 28.16969
196/196 - 35s - loss: 27.0169 - MinusLogProbMetric: 27.0169 - val_loss: 28.2268 - val_MinusLogProbMetric: 28.2268 - lr: 2.5000e-04 - 35s/epoch - 181ms/step
Epoch 412/1000
2023-10-25 07:29:10.515 
Epoch 412/1000 
	 loss: 27.0144, MinusLogProbMetric: 27.0144, val_loss: 28.3590, val_MinusLogProbMetric: 28.3590

Epoch 412: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0144 - MinusLogProbMetric: 27.0144 - val_loss: 28.3590 - val_MinusLogProbMetric: 28.3590 - lr: 2.5000e-04 - 38s/epoch - 193ms/step
Epoch 413/1000
2023-10-25 07:29:48.685 
Epoch 413/1000 
	 loss: 27.0184, MinusLogProbMetric: 27.0184, val_loss: 28.2989, val_MinusLogProbMetric: 28.2989

Epoch 413: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0184 - MinusLogProbMetric: 27.0184 - val_loss: 28.2989 - val_MinusLogProbMetric: 28.2989 - lr: 2.5000e-04 - 38s/epoch - 195ms/step
Epoch 414/1000
2023-10-25 07:30:24.622 
Epoch 414/1000 
	 loss: 27.0217, MinusLogProbMetric: 27.0217, val_loss: 28.2600, val_MinusLogProbMetric: 28.2600

Epoch 414: val_loss did not improve from 28.16969
196/196 - 36s - loss: 27.0217 - MinusLogProbMetric: 27.0217 - val_loss: 28.2600 - val_MinusLogProbMetric: 28.2600 - lr: 2.5000e-04 - 36s/epoch - 183ms/step
Epoch 415/1000
2023-10-25 07:31:00.770 
Epoch 415/1000 
	 loss: 27.0141, MinusLogProbMetric: 27.0141, val_loss: 28.2207, val_MinusLogProbMetric: 28.2207

Epoch 415: val_loss did not improve from 28.16969
196/196 - 36s - loss: 27.0141 - MinusLogProbMetric: 27.0141 - val_loss: 28.2207 - val_MinusLogProbMetric: 28.2207 - lr: 2.5000e-04 - 36s/epoch - 184ms/step
Epoch 416/1000
2023-10-25 07:31:38.198 
Epoch 416/1000 
	 loss: 27.0127, MinusLogProbMetric: 27.0127, val_loss: 28.2041, val_MinusLogProbMetric: 28.2041

Epoch 416: val_loss did not improve from 28.16969
196/196 - 37s - loss: 27.0127 - MinusLogProbMetric: 27.0127 - val_loss: 28.2041 - val_MinusLogProbMetric: 28.2041 - lr: 2.5000e-04 - 37s/epoch - 191ms/step
Epoch 417/1000
2023-10-25 07:32:16.188 
Epoch 417/1000 
	 loss: 27.0053, MinusLogProbMetric: 27.0053, val_loss: 28.2677, val_MinusLogProbMetric: 28.2677

Epoch 417: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0053 - MinusLogProbMetric: 27.0053 - val_loss: 28.2677 - val_MinusLogProbMetric: 28.2677 - lr: 2.5000e-04 - 38s/epoch - 194ms/step
Epoch 418/1000
2023-10-25 07:32:53.374 
Epoch 418/1000 
	 loss: 27.0076, MinusLogProbMetric: 27.0076, val_loss: 28.2281, val_MinusLogProbMetric: 28.2281

Epoch 418: val_loss did not improve from 28.16969
196/196 - 37s - loss: 27.0076 - MinusLogProbMetric: 27.0076 - val_loss: 28.2281 - val_MinusLogProbMetric: 28.2281 - lr: 2.5000e-04 - 37s/epoch - 190ms/step
Epoch 419/1000
2023-10-25 07:33:31.501 
Epoch 419/1000 
	 loss: 27.0063, MinusLogProbMetric: 27.0063, val_loss: 28.3133, val_MinusLogProbMetric: 28.3133

Epoch 419: val_loss did not improve from 28.16969
196/196 - 38s - loss: 27.0063 - MinusLogProbMetric: 27.0063 - val_loss: 28.3133 - val_MinusLogProbMetric: 28.3133 - lr: 2.5000e-04 - 38s/epoch - 195ms/step
Epoch 420/1000
2023-10-25 07:34:09.306 
Epoch 420/1000 
	 loss: 26.9441, MinusLogProbMetric: 26.9441, val_loss: 28.1742, val_MinusLogProbMetric: 28.1742

Epoch 420: val_loss did not improve from 28.16969
196/196 - 38s - loss: 26.9441 - MinusLogProbMetric: 26.9441 - val_loss: 28.1742 - val_MinusLogProbMetric: 28.1742 - lr: 1.2500e-04 - 38s/epoch - 193ms/step
Epoch 421/1000
2023-10-25 07:34:46.932 
Epoch 421/1000 
	 loss: 26.9451, MinusLogProbMetric: 26.9451, val_loss: 28.1814, val_MinusLogProbMetric: 28.1814

Epoch 421: val_loss did not improve from 28.16969
196/196 - 38s - loss: 26.9451 - MinusLogProbMetric: 26.9451 - val_loss: 28.1814 - val_MinusLogProbMetric: 28.1814 - lr: 1.2500e-04 - 38s/epoch - 192ms/step
Epoch 422/1000
2023-10-25 07:35:24.241 
Epoch 422/1000 
	 loss: 26.9450, MinusLogProbMetric: 26.9450, val_loss: 28.2059, val_MinusLogProbMetric: 28.2059

Epoch 422: val_loss did not improve from 28.16969
196/196 - 37s - loss: 26.9450 - MinusLogProbMetric: 26.9450 - val_loss: 28.2059 - val_MinusLogProbMetric: 28.2059 - lr: 1.2500e-04 - 37s/epoch - 190ms/step
Epoch 423/1000
2023-10-25 07:36:01.121 
Epoch 423/1000 
	 loss: 26.9370, MinusLogProbMetric: 26.9370, val_loss: 28.2041, val_MinusLogProbMetric: 28.2041

Epoch 423: val_loss did not improve from 28.16969
196/196 - 37s - loss: 26.9370 - MinusLogProbMetric: 26.9370 - val_loss: 28.2041 - val_MinusLogProbMetric: 28.2041 - lr: 1.2500e-04 - 37s/epoch - 188ms/step
Epoch 424/1000
2023-10-25 07:36:38.682 
Epoch 424/1000 
	 loss: 26.9406, MinusLogProbMetric: 26.9406, val_loss: 28.1744, val_MinusLogProbMetric: 28.1744

Epoch 424: val_loss did not improve from 28.16969
196/196 - 38s - loss: 26.9406 - MinusLogProbMetric: 26.9406 - val_loss: 28.1744 - val_MinusLogProbMetric: 28.1744 - lr: 1.2500e-04 - 38s/epoch - 192ms/step
Epoch 425/1000
2023-10-25 07:37:14.546 
Epoch 425/1000 
	 loss: 26.9382, MinusLogProbMetric: 26.9382, val_loss: 28.1905, val_MinusLogProbMetric: 28.1905

Epoch 425: val_loss did not improve from 28.16969
196/196 - 36s - loss: 26.9382 - MinusLogProbMetric: 26.9382 - val_loss: 28.1905 - val_MinusLogProbMetric: 28.1905 - lr: 1.2500e-04 - 36s/epoch - 183ms/step
Epoch 426/1000
2023-10-25 07:37:51.565 
Epoch 426/1000 
	 loss: 26.9327, MinusLogProbMetric: 26.9327, val_loss: 28.1650, val_MinusLogProbMetric: 28.1650

Epoch 426: val_loss improved from 28.16969 to 28.16503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_348/weights/best_weights.h5
196/196 - 38s - loss: 26.9327 - MinusLogProbMetric: 26.9327 - val_loss: 28.1650 - val_MinusLogProbMetric: 28.1650 - lr: 1.2500e-04 - 38s/epoch - 193ms/step
Epoch 427/1000
2023-10-25 07:38:30.694 
Epoch 427/1000 
	 loss: 26.9350, MinusLogProbMetric: 26.9350, val_loss: 28.1886, val_MinusLogProbMetric: 28.1886

Epoch 427: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.9350 - MinusLogProbMetric: 26.9350 - val_loss: 28.1886 - val_MinusLogProbMetric: 28.1886 - lr: 1.2500e-04 - 38s/epoch - 196ms/step
Epoch 428/1000
2023-10-25 07:39:08.950 
Epoch 428/1000 
	 loss: 26.9363, MinusLogProbMetric: 26.9363, val_loss: 28.2002, val_MinusLogProbMetric: 28.2002

Epoch 428: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.9363 - MinusLogProbMetric: 26.9363 - val_loss: 28.2002 - val_MinusLogProbMetric: 28.2002 - lr: 1.2500e-04 - 38s/epoch - 195ms/step
Epoch 429/1000
2023-10-25 07:39:47.587 
Epoch 429/1000 
	 loss: 26.9407, MinusLogProbMetric: 26.9407, val_loss: 28.1749, val_MinusLogProbMetric: 28.1749

Epoch 429: val_loss did not improve from 28.16503
196/196 - 39s - loss: 26.9407 - MinusLogProbMetric: 26.9407 - val_loss: 28.1749 - val_MinusLogProbMetric: 28.1749 - lr: 1.2500e-04 - 39s/epoch - 197ms/step
Epoch 430/1000
2023-10-25 07:40:26.205 
Epoch 430/1000 
	 loss: 26.9348, MinusLogProbMetric: 26.9348, val_loss: 28.1763, val_MinusLogProbMetric: 28.1763

Epoch 430: val_loss did not improve from 28.16503
196/196 - 39s - loss: 26.9348 - MinusLogProbMetric: 26.9348 - val_loss: 28.1763 - val_MinusLogProbMetric: 28.1763 - lr: 1.2500e-04 - 39s/epoch - 197ms/step
Epoch 431/1000
2023-10-25 07:41:04.376 
Epoch 431/1000 
	 loss: 26.9384, MinusLogProbMetric: 26.9384, val_loss: 28.1682, val_MinusLogProbMetric: 28.1682

Epoch 431: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.9384 - MinusLogProbMetric: 26.9384 - val_loss: 28.1682 - val_MinusLogProbMetric: 28.1682 - lr: 1.2500e-04 - 38s/epoch - 195ms/step
Epoch 432/1000
2023-10-25 07:41:42.024 
Epoch 432/1000 
	 loss: 26.9343, MinusLogProbMetric: 26.9343, val_loss: 28.1713, val_MinusLogProbMetric: 28.1713

Epoch 432: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.9343 - MinusLogProbMetric: 26.9343 - val_loss: 28.1713 - val_MinusLogProbMetric: 28.1713 - lr: 1.2500e-04 - 38s/epoch - 192ms/step
Epoch 433/1000
2023-10-25 07:42:19.555 
Epoch 433/1000 
	 loss: 26.9346, MinusLogProbMetric: 26.9346, val_loss: 28.1757, val_MinusLogProbMetric: 28.1757

Epoch 433: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.9346 - MinusLogProbMetric: 26.9346 - val_loss: 28.1757 - val_MinusLogProbMetric: 28.1757 - lr: 1.2500e-04 - 38s/epoch - 191ms/step
Epoch 434/1000
2023-10-25 07:42:55.559 
Epoch 434/1000 
	 loss: 26.9343, MinusLogProbMetric: 26.9343, val_loss: 28.2608, val_MinusLogProbMetric: 28.2608

Epoch 434: val_loss did not improve from 28.16503
196/196 - 36s - loss: 26.9343 - MinusLogProbMetric: 26.9343 - val_loss: 28.2608 - val_MinusLogProbMetric: 28.2608 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 435/1000
2023-10-25 07:43:32.547 
Epoch 435/1000 
	 loss: 26.9370, MinusLogProbMetric: 26.9370, val_loss: 28.1762, val_MinusLogProbMetric: 28.1762

Epoch 435: val_loss did not improve from 28.16503
196/196 - 37s - loss: 26.9370 - MinusLogProbMetric: 26.9370 - val_loss: 28.1762 - val_MinusLogProbMetric: 28.1762 - lr: 1.2500e-04 - 37s/epoch - 189ms/step
Epoch 436/1000
2023-10-25 07:44:09.829 
Epoch 436/1000 
	 loss: 26.9356, MinusLogProbMetric: 26.9356, val_loss: 28.1773, val_MinusLogProbMetric: 28.1773

Epoch 436: val_loss did not improve from 28.16503
196/196 - 37s - loss: 26.9356 - MinusLogProbMetric: 26.9356 - val_loss: 28.1773 - val_MinusLogProbMetric: 28.1773 - lr: 1.2500e-04 - 37s/epoch - 190ms/step
Epoch 437/1000
2023-10-25 07:44:45.893 
Epoch 437/1000 
	 loss: 26.9372, MinusLogProbMetric: 26.9372, val_loss: 28.1931, val_MinusLogProbMetric: 28.1931

Epoch 437: val_loss did not improve from 28.16503
196/196 - 36s - loss: 26.9372 - MinusLogProbMetric: 26.9372 - val_loss: 28.1931 - val_MinusLogProbMetric: 28.1931 - lr: 1.2500e-04 - 36s/epoch - 184ms/step
Epoch 438/1000
2023-10-25 07:45:24.421 
Epoch 438/1000 
	 loss: 26.9436, MinusLogProbMetric: 26.9436, val_loss: 28.1838, val_MinusLogProbMetric: 28.1838

Epoch 438: val_loss did not improve from 28.16503
196/196 - 39s - loss: 26.9436 - MinusLogProbMetric: 26.9436 - val_loss: 28.1838 - val_MinusLogProbMetric: 28.1838 - lr: 1.2500e-04 - 39s/epoch - 197ms/step
Epoch 439/1000
2023-10-25 07:46:01.466 
Epoch 439/1000 
	 loss: 26.9263, MinusLogProbMetric: 26.9263, val_loss: 28.1835, val_MinusLogProbMetric: 28.1835

Epoch 439: val_loss did not improve from 28.16503
196/196 - 37s - loss: 26.9263 - MinusLogProbMetric: 26.9263 - val_loss: 28.1835 - val_MinusLogProbMetric: 28.1835 - lr: 1.2500e-04 - 37s/epoch - 189ms/step
Epoch 440/1000
2023-10-25 07:46:39.000 
Epoch 440/1000 
	 loss: 26.9299, MinusLogProbMetric: 26.9299, val_loss: 28.1723, val_MinusLogProbMetric: 28.1723

Epoch 440: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.9299 - MinusLogProbMetric: 26.9299 - val_loss: 28.1723 - val_MinusLogProbMetric: 28.1723 - lr: 1.2500e-04 - 38s/epoch - 191ms/step
Epoch 441/1000
2023-10-25 07:47:16.242 
Epoch 441/1000 
	 loss: 26.9344, MinusLogProbMetric: 26.9344, val_loss: 28.2184, val_MinusLogProbMetric: 28.2184

Epoch 441: val_loss did not improve from 28.16503
196/196 - 37s - loss: 26.9344 - MinusLogProbMetric: 26.9344 - val_loss: 28.2184 - val_MinusLogProbMetric: 28.2184 - lr: 1.2500e-04 - 37s/epoch - 190ms/step
Epoch 442/1000
2023-10-25 07:47:54.926 
Epoch 442/1000 
	 loss: 26.9416, MinusLogProbMetric: 26.9416, val_loss: 28.2001, val_MinusLogProbMetric: 28.2001

Epoch 442: val_loss did not improve from 28.16503
196/196 - 39s - loss: 26.9416 - MinusLogProbMetric: 26.9416 - val_loss: 28.2001 - val_MinusLogProbMetric: 28.2001 - lr: 1.2500e-04 - 39s/epoch - 197ms/step
Epoch 443/1000
2023-10-25 07:48:32.666 
Epoch 443/1000 
	 loss: 26.9330, MinusLogProbMetric: 26.9330, val_loss: 28.2205, val_MinusLogProbMetric: 28.2205

Epoch 443: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.9330 - MinusLogProbMetric: 26.9330 - val_loss: 28.2205 - val_MinusLogProbMetric: 28.2205 - lr: 1.2500e-04 - 38s/epoch - 193ms/step
Epoch 444/1000
2023-10-25 07:49:10.988 
Epoch 444/1000 
	 loss: 26.9337, MinusLogProbMetric: 26.9337, val_loss: 28.1951, val_MinusLogProbMetric: 28.1951

Epoch 444: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.9337 - MinusLogProbMetric: 26.9337 - val_loss: 28.1951 - val_MinusLogProbMetric: 28.1951 - lr: 1.2500e-04 - 38s/epoch - 196ms/step
Epoch 445/1000
2023-10-25 07:49:49.805 
Epoch 445/1000 
	 loss: 26.9278, MinusLogProbMetric: 26.9278, val_loss: 28.1700, val_MinusLogProbMetric: 28.1700

Epoch 445: val_loss did not improve from 28.16503
196/196 - 39s - loss: 26.9278 - MinusLogProbMetric: 26.9278 - val_loss: 28.1700 - val_MinusLogProbMetric: 28.1700 - lr: 1.2500e-04 - 39s/epoch - 198ms/step
Epoch 446/1000
2023-10-25 07:50:25.041 
Epoch 446/1000 
	 loss: 26.9322, MinusLogProbMetric: 26.9322, val_loss: 28.1728, val_MinusLogProbMetric: 28.1728

Epoch 446: val_loss did not improve from 28.16503
196/196 - 35s - loss: 26.9322 - MinusLogProbMetric: 26.9322 - val_loss: 28.1728 - val_MinusLogProbMetric: 28.1728 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 447/1000
2023-10-25 07:50:58.931 
Epoch 447/1000 
	 loss: 26.9308, MinusLogProbMetric: 26.9308, val_loss: 28.1935, val_MinusLogProbMetric: 28.1935

Epoch 447: val_loss did not improve from 28.16503
196/196 - 34s - loss: 26.9308 - MinusLogProbMetric: 26.9308 - val_loss: 28.1935 - val_MinusLogProbMetric: 28.1935 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 448/1000
2023-10-25 07:51:34.491 
Epoch 448/1000 
	 loss: 26.9311, MinusLogProbMetric: 26.9311, val_loss: 28.2214, val_MinusLogProbMetric: 28.2214

Epoch 448: val_loss did not improve from 28.16503
196/196 - 36s - loss: 26.9311 - MinusLogProbMetric: 26.9311 - val_loss: 28.2214 - val_MinusLogProbMetric: 28.2214 - lr: 1.2500e-04 - 36s/epoch - 181ms/step
Epoch 449/1000
2023-10-25 07:52:12.051 
Epoch 449/1000 
	 loss: 26.9340, MinusLogProbMetric: 26.9340, val_loss: 28.1794, val_MinusLogProbMetric: 28.1794

Epoch 449: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.9340 - MinusLogProbMetric: 26.9340 - val_loss: 28.1794 - val_MinusLogProbMetric: 28.1794 - lr: 1.2500e-04 - 38s/epoch - 192ms/step
Epoch 450/1000
2023-10-25 07:52:45.811 
Epoch 450/1000 
	 loss: 26.9343, MinusLogProbMetric: 26.9343, val_loss: 28.1916, val_MinusLogProbMetric: 28.1916

Epoch 450: val_loss did not improve from 28.16503
196/196 - 34s - loss: 26.9343 - MinusLogProbMetric: 26.9343 - val_loss: 28.1916 - val_MinusLogProbMetric: 28.1916 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 451/1000
2023-10-25 07:53:20.241 
Epoch 451/1000 
	 loss: 26.9325, MinusLogProbMetric: 26.9325, val_loss: 28.2321, val_MinusLogProbMetric: 28.2321

Epoch 451: val_loss did not improve from 28.16503
196/196 - 34s - loss: 26.9325 - MinusLogProbMetric: 26.9325 - val_loss: 28.2321 - val_MinusLogProbMetric: 28.2321 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 452/1000
2023-10-25 07:53:57.209 
Epoch 452/1000 
	 loss: 26.9340, MinusLogProbMetric: 26.9340, val_loss: 28.2010, val_MinusLogProbMetric: 28.2010

Epoch 452: val_loss did not improve from 28.16503
196/196 - 37s - loss: 26.9340 - MinusLogProbMetric: 26.9340 - val_loss: 28.2010 - val_MinusLogProbMetric: 28.2010 - lr: 1.2500e-04 - 37s/epoch - 189ms/step
Epoch 453/1000
2023-10-25 07:54:31.851 
Epoch 453/1000 
	 loss: 26.9306, MinusLogProbMetric: 26.9306, val_loss: 28.1839, val_MinusLogProbMetric: 28.1839

Epoch 453: val_loss did not improve from 28.16503
196/196 - 35s - loss: 26.9306 - MinusLogProbMetric: 26.9306 - val_loss: 28.1839 - val_MinusLogProbMetric: 28.1839 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 454/1000
2023-10-25 07:55:03.453 
Epoch 454/1000 
	 loss: 26.9303, MinusLogProbMetric: 26.9303, val_loss: 28.1992, val_MinusLogProbMetric: 28.1992

Epoch 454: val_loss did not improve from 28.16503
196/196 - 32s - loss: 26.9303 - MinusLogProbMetric: 26.9303 - val_loss: 28.1992 - val_MinusLogProbMetric: 28.1992 - lr: 1.2500e-04 - 32s/epoch - 161ms/step
Epoch 455/1000
2023-10-25 07:55:37.111 
Epoch 455/1000 
	 loss: 26.9269, MinusLogProbMetric: 26.9269, val_loss: 28.1806, val_MinusLogProbMetric: 28.1806

Epoch 455: val_loss did not improve from 28.16503
196/196 - 34s - loss: 26.9269 - MinusLogProbMetric: 26.9269 - val_loss: 28.1806 - val_MinusLogProbMetric: 28.1806 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 456/1000
2023-10-25 07:56:12.068 
Epoch 456/1000 
	 loss: 26.9297, MinusLogProbMetric: 26.9297, val_loss: 28.1652, val_MinusLogProbMetric: 28.1652

Epoch 456: val_loss did not improve from 28.16503
196/196 - 35s - loss: 26.9297 - MinusLogProbMetric: 26.9297 - val_loss: 28.1652 - val_MinusLogProbMetric: 28.1652 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 457/1000
2023-10-25 07:56:50.280 
Epoch 457/1000 
	 loss: 26.9227, MinusLogProbMetric: 26.9227, val_loss: 28.1783, val_MinusLogProbMetric: 28.1783

Epoch 457: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.9227 - MinusLogProbMetric: 26.9227 - val_loss: 28.1783 - val_MinusLogProbMetric: 28.1783 - lr: 1.2500e-04 - 38s/epoch - 195ms/step
Epoch 458/1000
2023-10-25 07:57:25.881 
Epoch 458/1000 
	 loss: 26.9234, MinusLogProbMetric: 26.9234, val_loss: 28.1759, val_MinusLogProbMetric: 28.1759

Epoch 458: val_loss did not improve from 28.16503
196/196 - 36s - loss: 26.9234 - MinusLogProbMetric: 26.9234 - val_loss: 28.1759 - val_MinusLogProbMetric: 28.1759 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 459/1000
2023-10-25 07:58:01.113 
Epoch 459/1000 
	 loss: 26.9229, MinusLogProbMetric: 26.9229, val_loss: 28.1966, val_MinusLogProbMetric: 28.1966

Epoch 459: val_loss did not improve from 28.16503
196/196 - 35s - loss: 26.9229 - MinusLogProbMetric: 26.9229 - val_loss: 28.1966 - val_MinusLogProbMetric: 28.1966 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 460/1000
2023-10-25 07:58:36.369 
Epoch 460/1000 
	 loss: 26.9288, MinusLogProbMetric: 26.9288, val_loss: 28.1868, val_MinusLogProbMetric: 28.1868

Epoch 460: val_loss did not improve from 28.16503
196/196 - 35s - loss: 26.9288 - MinusLogProbMetric: 26.9288 - val_loss: 28.1868 - val_MinusLogProbMetric: 28.1868 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 461/1000
2023-10-25 07:59:14.433 
Epoch 461/1000 
	 loss: 26.9237, MinusLogProbMetric: 26.9237, val_loss: 28.2055, val_MinusLogProbMetric: 28.2055

Epoch 461: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.9237 - MinusLogProbMetric: 26.9237 - val_loss: 28.2055 - val_MinusLogProbMetric: 28.2055 - lr: 1.2500e-04 - 38s/epoch - 194ms/step
Epoch 462/1000
2023-10-25 07:59:48.964 
Epoch 462/1000 
	 loss: 26.9248, MinusLogProbMetric: 26.9248, val_loss: 28.1824, val_MinusLogProbMetric: 28.1824

Epoch 462: val_loss did not improve from 28.16503
196/196 - 35s - loss: 26.9248 - MinusLogProbMetric: 26.9248 - val_loss: 28.1824 - val_MinusLogProbMetric: 28.1824 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 463/1000
2023-10-25 08:00:23.256 
Epoch 463/1000 
	 loss: 26.9249, MinusLogProbMetric: 26.9249, val_loss: 28.1804, val_MinusLogProbMetric: 28.1804

Epoch 463: val_loss did not improve from 28.16503
196/196 - 34s - loss: 26.9249 - MinusLogProbMetric: 26.9249 - val_loss: 28.1804 - val_MinusLogProbMetric: 28.1804 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 464/1000
2023-10-25 08:00:58.298 
Epoch 464/1000 
	 loss: 26.9210, MinusLogProbMetric: 26.9210, val_loss: 28.2150, val_MinusLogProbMetric: 28.2150

Epoch 464: val_loss did not improve from 28.16503
196/196 - 35s - loss: 26.9210 - MinusLogProbMetric: 26.9210 - val_loss: 28.2150 - val_MinusLogProbMetric: 28.2150 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 465/1000
2023-10-25 08:01:33.981 
Epoch 465/1000 
	 loss: 26.9227, MinusLogProbMetric: 26.9227, val_loss: 28.2083, val_MinusLogProbMetric: 28.2083

Epoch 465: val_loss did not improve from 28.16503
196/196 - 36s - loss: 26.9227 - MinusLogProbMetric: 26.9227 - val_loss: 28.2083 - val_MinusLogProbMetric: 28.2083 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 466/1000
2023-10-25 08:02:08.514 
Epoch 466/1000 
	 loss: 26.9210, MinusLogProbMetric: 26.9210, val_loss: 28.1977, val_MinusLogProbMetric: 28.1977

Epoch 466: val_loss did not improve from 28.16503
196/196 - 35s - loss: 26.9210 - MinusLogProbMetric: 26.9210 - val_loss: 28.1977 - val_MinusLogProbMetric: 28.1977 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 467/1000
2023-10-25 08:02:43.259 
Epoch 467/1000 
	 loss: 26.9251, MinusLogProbMetric: 26.9251, val_loss: 28.1823, val_MinusLogProbMetric: 28.1823

Epoch 467: val_loss did not improve from 28.16503
196/196 - 35s - loss: 26.9251 - MinusLogProbMetric: 26.9251 - val_loss: 28.1823 - val_MinusLogProbMetric: 28.1823 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 468/1000
2023-10-25 08:03:18.927 
Epoch 468/1000 
	 loss: 26.9216, MinusLogProbMetric: 26.9216, val_loss: 28.1998, val_MinusLogProbMetric: 28.1998

Epoch 468: val_loss did not improve from 28.16503
196/196 - 36s - loss: 26.9216 - MinusLogProbMetric: 26.9216 - val_loss: 28.1998 - val_MinusLogProbMetric: 28.1998 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 469/1000
2023-10-25 08:03:56.558 
Epoch 469/1000 
	 loss: 26.9151, MinusLogProbMetric: 26.9151, val_loss: 28.2059, val_MinusLogProbMetric: 28.2059

Epoch 469: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.9151 - MinusLogProbMetric: 26.9151 - val_loss: 28.2059 - val_MinusLogProbMetric: 28.2059 - lr: 1.2500e-04 - 38s/epoch - 192ms/step
Epoch 470/1000
2023-10-25 08:04:28.372 
Epoch 470/1000 
	 loss: 26.9317, MinusLogProbMetric: 26.9317, val_loss: 28.2098, val_MinusLogProbMetric: 28.2098

Epoch 470: val_loss did not improve from 28.16503
196/196 - 32s - loss: 26.9317 - MinusLogProbMetric: 26.9317 - val_loss: 28.2098 - val_MinusLogProbMetric: 28.2098 - lr: 1.2500e-04 - 32s/epoch - 162ms/step
Epoch 471/1000
2023-10-25 08:05:01.853 
Epoch 471/1000 
	 loss: 26.9177, MinusLogProbMetric: 26.9177, val_loss: 28.1990, val_MinusLogProbMetric: 28.1990

Epoch 471: val_loss did not improve from 28.16503
196/196 - 33s - loss: 26.9177 - MinusLogProbMetric: 26.9177 - val_loss: 28.1990 - val_MinusLogProbMetric: 28.1990 - lr: 1.2500e-04 - 33s/epoch - 171ms/step
Epoch 472/1000
2023-10-25 08:05:35.770 
Epoch 472/1000 
	 loss: 26.9152, MinusLogProbMetric: 26.9152, val_loss: 28.1985, val_MinusLogProbMetric: 28.1985

Epoch 472: val_loss did not improve from 28.16503
196/196 - 34s - loss: 26.9152 - MinusLogProbMetric: 26.9152 - val_loss: 28.1985 - val_MinusLogProbMetric: 28.1985 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 473/1000
2023-10-25 08:06:10.915 
Epoch 473/1000 
	 loss: 26.9189, MinusLogProbMetric: 26.9189, val_loss: 28.1804, val_MinusLogProbMetric: 28.1804

Epoch 473: val_loss did not improve from 28.16503
196/196 - 35s - loss: 26.9189 - MinusLogProbMetric: 26.9189 - val_loss: 28.1804 - val_MinusLogProbMetric: 28.1804 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 474/1000
2023-10-25 08:06:42.441 
Epoch 474/1000 
	 loss: 26.9153, MinusLogProbMetric: 26.9153, val_loss: 28.1959, val_MinusLogProbMetric: 28.1959

Epoch 474: val_loss did not improve from 28.16503
196/196 - 32s - loss: 26.9153 - MinusLogProbMetric: 26.9153 - val_loss: 28.1959 - val_MinusLogProbMetric: 28.1959 - lr: 1.2500e-04 - 32s/epoch - 161ms/step
Epoch 475/1000
2023-10-25 08:07:13.836 
Epoch 475/1000 
	 loss: 26.9202, MinusLogProbMetric: 26.9202, val_loss: 28.1967, val_MinusLogProbMetric: 28.1967

Epoch 475: val_loss did not improve from 28.16503
196/196 - 31s - loss: 26.9202 - MinusLogProbMetric: 26.9202 - val_loss: 28.1967 - val_MinusLogProbMetric: 28.1967 - lr: 1.2500e-04 - 31s/epoch - 160ms/step
Epoch 476/1000
2023-10-25 08:07:45.284 
Epoch 476/1000 
	 loss: 26.9204, MinusLogProbMetric: 26.9204, val_loss: 28.1859, val_MinusLogProbMetric: 28.1859

Epoch 476: val_loss did not improve from 28.16503
196/196 - 31s - loss: 26.9204 - MinusLogProbMetric: 26.9204 - val_loss: 28.1859 - val_MinusLogProbMetric: 28.1859 - lr: 1.2500e-04 - 31s/epoch - 160ms/step
Epoch 477/1000
2023-10-25 08:08:24.073 
Epoch 477/1000 
	 loss: 26.8876, MinusLogProbMetric: 26.8876, val_loss: 28.1742, val_MinusLogProbMetric: 28.1742

Epoch 477: val_loss did not improve from 28.16503
196/196 - 39s - loss: 26.8876 - MinusLogProbMetric: 26.8876 - val_loss: 28.1742 - val_MinusLogProbMetric: 28.1742 - lr: 6.2500e-05 - 39s/epoch - 198ms/step
Epoch 478/1000
2023-10-25 08:09:00.907 
Epoch 478/1000 
	 loss: 26.8873, MinusLogProbMetric: 26.8873, val_loss: 28.1716, val_MinusLogProbMetric: 28.1716

Epoch 478: val_loss did not improve from 28.16503
196/196 - 37s - loss: 26.8873 - MinusLogProbMetric: 26.8873 - val_loss: 28.1716 - val_MinusLogProbMetric: 28.1716 - lr: 6.2500e-05 - 37s/epoch - 188ms/step
Epoch 479/1000
2023-10-25 08:09:33.075 
Epoch 479/1000 
	 loss: 26.8867, MinusLogProbMetric: 26.8867, val_loss: 28.1672, val_MinusLogProbMetric: 28.1672

Epoch 479: val_loss did not improve from 28.16503
196/196 - 32s - loss: 26.8867 - MinusLogProbMetric: 26.8867 - val_loss: 28.1672 - val_MinusLogProbMetric: 28.1672 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 480/1000
2023-10-25 08:10:03.591 
Epoch 480/1000 
	 loss: 26.8847, MinusLogProbMetric: 26.8847, val_loss: 28.1669, val_MinusLogProbMetric: 28.1669

Epoch 480: val_loss did not improve from 28.16503
196/196 - 31s - loss: 26.8847 - MinusLogProbMetric: 26.8847 - val_loss: 28.1669 - val_MinusLogProbMetric: 28.1669 - lr: 6.2500e-05 - 31s/epoch - 156ms/step
Epoch 481/1000
2023-10-25 08:10:37.635 
Epoch 481/1000 
	 loss: 26.8874, MinusLogProbMetric: 26.8874, val_loss: 28.1701, val_MinusLogProbMetric: 28.1701

Epoch 481: val_loss did not improve from 28.16503
196/196 - 34s - loss: 26.8874 - MinusLogProbMetric: 26.8874 - val_loss: 28.1701 - val_MinusLogProbMetric: 28.1701 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 482/1000
2023-10-25 08:11:11.727 
Epoch 482/1000 
	 loss: 26.8869, MinusLogProbMetric: 26.8869, val_loss: 28.1716, val_MinusLogProbMetric: 28.1716

Epoch 482: val_loss did not improve from 28.16503
196/196 - 34s - loss: 26.8869 - MinusLogProbMetric: 26.8869 - val_loss: 28.1716 - val_MinusLogProbMetric: 28.1716 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 483/1000
2023-10-25 08:11:42.048 
Epoch 483/1000 
	 loss: 26.8850, MinusLogProbMetric: 26.8850, val_loss: 28.1751, val_MinusLogProbMetric: 28.1751

Epoch 483: val_loss did not improve from 28.16503
196/196 - 30s - loss: 26.8850 - MinusLogProbMetric: 26.8850 - val_loss: 28.1751 - val_MinusLogProbMetric: 28.1751 - lr: 6.2500e-05 - 30s/epoch - 155ms/step
Epoch 484/1000
2023-10-25 08:12:14.285 
Epoch 484/1000 
	 loss: 26.8836, MinusLogProbMetric: 26.8836, val_loss: 28.1800, val_MinusLogProbMetric: 28.1800

Epoch 484: val_loss did not improve from 28.16503
196/196 - 32s - loss: 26.8836 - MinusLogProbMetric: 26.8836 - val_loss: 28.1800 - val_MinusLogProbMetric: 28.1800 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 485/1000
2023-10-25 08:12:48.771 
Epoch 485/1000 
	 loss: 26.8857, MinusLogProbMetric: 26.8857, val_loss: 28.1679, val_MinusLogProbMetric: 28.1679

Epoch 485: val_loss did not improve from 28.16503
196/196 - 34s - loss: 26.8857 - MinusLogProbMetric: 26.8857 - val_loss: 28.1679 - val_MinusLogProbMetric: 28.1679 - lr: 6.2500e-05 - 34s/epoch - 176ms/step
Epoch 486/1000
2023-10-25 08:13:23.630 
Epoch 486/1000 
	 loss: 26.8827, MinusLogProbMetric: 26.8827, val_loss: 28.1740, val_MinusLogProbMetric: 28.1740

Epoch 486: val_loss did not improve from 28.16503
196/196 - 35s - loss: 26.8827 - MinusLogProbMetric: 26.8827 - val_loss: 28.1740 - val_MinusLogProbMetric: 28.1740 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 487/1000
2023-10-25 08:13:54.433 
Epoch 487/1000 
	 loss: 26.8860, MinusLogProbMetric: 26.8860, val_loss: 28.1760, val_MinusLogProbMetric: 28.1760

Epoch 487: val_loss did not improve from 28.16503
196/196 - 31s - loss: 26.8860 - MinusLogProbMetric: 26.8860 - val_loss: 28.1760 - val_MinusLogProbMetric: 28.1760 - lr: 6.2500e-05 - 31s/epoch - 157ms/step
Epoch 488/1000
2023-10-25 08:14:25.688 
Epoch 488/1000 
	 loss: 26.8837, MinusLogProbMetric: 26.8837, val_loss: 28.1913, val_MinusLogProbMetric: 28.1913

Epoch 488: val_loss did not improve from 28.16503
196/196 - 31s - loss: 26.8837 - MinusLogProbMetric: 26.8837 - val_loss: 28.1913 - val_MinusLogProbMetric: 28.1913 - lr: 6.2500e-05 - 31s/epoch - 159ms/step
Epoch 489/1000
2023-10-25 08:14:56.198 
Epoch 489/1000 
	 loss: 26.8836, MinusLogProbMetric: 26.8836, val_loss: 28.1796, val_MinusLogProbMetric: 28.1796

Epoch 489: val_loss did not improve from 28.16503
196/196 - 31s - loss: 26.8836 - MinusLogProbMetric: 26.8836 - val_loss: 28.1796 - val_MinusLogProbMetric: 28.1796 - lr: 6.2500e-05 - 31s/epoch - 156ms/step
Epoch 490/1000
2023-10-25 08:15:31.686 
Epoch 490/1000 
	 loss: 26.8867, MinusLogProbMetric: 26.8867, val_loss: 28.1843, val_MinusLogProbMetric: 28.1843

Epoch 490: val_loss did not improve from 28.16503
196/196 - 35s - loss: 26.8867 - MinusLogProbMetric: 26.8867 - val_loss: 28.1843 - val_MinusLogProbMetric: 28.1843 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 491/1000
2023-10-25 08:16:05.534 
Epoch 491/1000 
	 loss: 26.8850, MinusLogProbMetric: 26.8850, val_loss: 28.1707, val_MinusLogProbMetric: 28.1707

Epoch 491: val_loss did not improve from 28.16503
196/196 - 34s - loss: 26.8850 - MinusLogProbMetric: 26.8850 - val_loss: 28.1707 - val_MinusLogProbMetric: 28.1707 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 492/1000
2023-10-25 08:16:35.984 
Epoch 492/1000 
	 loss: 26.8819, MinusLogProbMetric: 26.8819, val_loss: 28.1802, val_MinusLogProbMetric: 28.1802

Epoch 492: val_loss did not improve from 28.16503
196/196 - 30s - loss: 26.8819 - MinusLogProbMetric: 26.8819 - val_loss: 28.1802 - val_MinusLogProbMetric: 28.1802 - lr: 6.2500e-05 - 30s/epoch - 155ms/step
Epoch 493/1000
2023-10-25 08:17:06.879 
Epoch 493/1000 
	 loss: 26.8870, MinusLogProbMetric: 26.8870, val_loss: 28.1944, val_MinusLogProbMetric: 28.1944

Epoch 493: val_loss did not improve from 28.16503
196/196 - 31s - loss: 26.8870 - MinusLogProbMetric: 26.8870 - val_loss: 28.1944 - val_MinusLogProbMetric: 28.1944 - lr: 6.2500e-05 - 31s/epoch - 158ms/step
Epoch 494/1000
2023-10-25 08:17:42.817 
Epoch 494/1000 
	 loss: 26.8854, MinusLogProbMetric: 26.8854, val_loss: 28.1734, val_MinusLogProbMetric: 28.1734

Epoch 494: val_loss did not improve from 28.16503
196/196 - 36s - loss: 26.8854 - MinusLogProbMetric: 26.8854 - val_loss: 28.1734 - val_MinusLogProbMetric: 28.1734 - lr: 6.2500e-05 - 36s/epoch - 183ms/step
Epoch 495/1000
2023-10-25 08:18:18.199 
Epoch 495/1000 
	 loss: 26.8830, MinusLogProbMetric: 26.8830, val_loss: 28.1757, val_MinusLogProbMetric: 28.1757

Epoch 495: val_loss did not improve from 28.16503
196/196 - 35s - loss: 26.8830 - MinusLogProbMetric: 26.8830 - val_loss: 28.1757 - val_MinusLogProbMetric: 28.1757 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 496/1000
2023-10-25 08:18:51.038 
Epoch 496/1000 
	 loss: 26.8834, MinusLogProbMetric: 26.8834, val_loss: 28.1739, val_MinusLogProbMetric: 28.1739

Epoch 496: val_loss did not improve from 28.16503
196/196 - 33s - loss: 26.8834 - MinusLogProbMetric: 26.8834 - val_loss: 28.1739 - val_MinusLogProbMetric: 28.1739 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 497/1000
2023-10-25 08:19:22.956 
Epoch 497/1000 
	 loss: 26.8804, MinusLogProbMetric: 26.8804, val_loss: 28.1963, val_MinusLogProbMetric: 28.1963

Epoch 497: val_loss did not improve from 28.16503
196/196 - 32s - loss: 26.8804 - MinusLogProbMetric: 26.8804 - val_loss: 28.1963 - val_MinusLogProbMetric: 28.1963 - lr: 6.2500e-05 - 32s/epoch - 163ms/step
Epoch 498/1000
2023-10-25 08:19:55.328 
Epoch 498/1000 
	 loss: 26.8835, MinusLogProbMetric: 26.8835, val_loss: 28.1717, val_MinusLogProbMetric: 28.1717

Epoch 498: val_loss did not improve from 28.16503
196/196 - 32s - loss: 26.8835 - MinusLogProbMetric: 26.8835 - val_loss: 28.1717 - val_MinusLogProbMetric: 28.1717 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 499/1000
2023-10-25 08:20:29.444 
Epoch 499/1000 
	 loss: 26.8823, MinusLogProbMetric: 26.8823, val_loss: 28.1728, val_MinusLogProbMetric: 28.1728

Epoch 499: val_loss did not improve from 28.16503
196/196 - 34s - loss: 26.8823 - MinusLogProbMetric: 26.8823 - val_loss: 28.1728 - val_MinusLogProbMetric: 28.1728 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 500/1000
2023-10-25 08:21:01.310 
Epoch 500/1000 
	 loss: 26.8821, MinusLogProbMetric: 26.8821, val_loss: 28.1764, val_MinusLogProbMetric: 28.1764

Epoch 500: val_loss did not improve from 28.16503
196/196 - 32s - loss: 26.8821 - MinusLogProbMetric: 26.8821 - val_loss: 28.1764 - val_MinusLogProbMetric: 28.1764 - lr: 6.2500e-05 - 32s/epoch - 163ms/step
Epoch 501/1000
2023-10-25 08:21:31.855 
Epoch 501/1000 
	 loss: 26.8808, MinusLogProbMetric: 26.8808, val_loss: 28.1836, val_MinusLogProbMetric: 28.1836

Epoch 501: val_loss did not improve from 28.16503
196/196 - 31s - loss: 26.8808 - MinusLogProbMetric: 26.8808 - val_loss: 28.1836 - val_MinusLogProbMetric: 28.1836 - lr: 6.2500e-05 - 31s/epoch - 156ms/step
Epoch 502/1000
2023-10-25 08:22:03.465 
Epoch 502/1000 
	 loss: 26.8813, MinusLogProbMetric: 26.8813, val_loss: 28.1669, val_MinusLogProbMetric: 28.1669

Epoch 502: val_loss did not improve from 28.16503
196/196 - 32s - loss: 26.8813 - MinusLogProbMetric: 26.8813 - val_loss: 28.1669 - val_MinusLogProbMetric: 28.1669 - lr: 6.2500e-05 - 32s/epoch - 161ms/step
Epoch 503/1000
2023-10-25 08:22:41.302 
Epoch 503/1000 
	 loss: 26.8814, MinusLogProbMetric: 26.8814, val_loss: 28.1771, val_MinusLogProbMetric: 28.1771

Epoch 503: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.8814 - MinusLogProbMetric: 26.8814 - val_loss: 28.1771 - val_MinusLogProbMetric: 28.1771 - lr: 6.2500e-05 - 38s/epoch - 193ms/step
Epoch 504/1000
2023-10-25 08:23:19.938 
Epoch 504/1000 
	 loss: 26.8815, MinusLogProbMetric: 26.8815, val_loss: 28.1705, val_MinusLogProbMetric: 28.1705

Epoch 504: val_loss did not improve from 28.16503
196/196 - 39s - loss: 26.8815 - MinusLogProbMetric: 26.8815 - val_loss: 28.1705 - val_MinusLogProbMetric: 28.1705 - lr: 6.2500e-05 - 39s/epoch - 197ms/step
Epoch 505/1000
2023-10-25 08:23:55.200 
Epoch 505/1000 
	 loss: 26.8787, MinusLogProbMetric: 26.8787, val_loss: 28.1728, val_MinusLogProbMetric: 28.1728

Epoch 505: val_loss did not improve from 28.16503
196/196 - 35s - loss: 26.8787 - MinusLogProbMetric: 26.8787 - val_loss: 28.1728 - val_MinusLogProbMetric: 28.1728 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 506/1000
2023-10-25 08:24:32.564 
Epoch 506/1000 
	 loss: 26.8793, MinusLogProbMetric: 26.8793, val_loss: 28.1839, val_MinusLogProbMetric: 28.1839

Epoch 506: val_loss did not improve from 28.16503
196/196 - 37s - loss: 26.8793 - MinusLogProbMetric: 26.8793 - val_loss: 28.1839 - val_MinusLogProbMetric: 28.1839 - lr: 6.2500e-05 - 37s/epoch - 191ms/step
Epoch 507/1000
2023-10-25 08:25:10.338 
Epoch 507/1000 
	 loss: 26.8813, MinusLogProbMetric: 26.8813, val_loss: 28.1743, val_MinusLogProbMetric: 28.1743

Epoch 507: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.8813 - MinusLogProbMetric: 26.8813 - val_loss: 28.1743 - val_MinusLogProbMetric: 28.1743 - lr: 6.2500e-05 - 38s/epoch - 193ms/step
Epoch 508/1000
2023-10-25 08:25:47.891 
Epoch 508/1000 
	 loss: 26.8820, MinusLogProbMetric: 26.8820, val_loss: 28.1805, val_MinusLogProbMetric: 28.1805

Epoch 508: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.8820 - MinusLogProbMetric: 26.8820 - val_loss: 28.1805 - val_MinusLogProbMetric: 28.1805 - lr: 6.2500e-05 - 38s/epoch - 192ms/step
Epoch 509/1000
2023-10-25 08:26:25.324 
Epoch 509/1000 
	 loss: 26.8806, MinusLogProbMetric: 26.8806, val_loss: 28.1748, val_MinusLogProbMetric: 28.1748

Epoch 509: val_loss did not improve from 28.16503
196/196 - 37s - loss: 26.8806 - MinusLogProbMetric: 26.8806 - val_loss: 28.1748 - val_MinusLogProbMetric: 28.1748 - lr: 6.2500e-05 - 37s/epoch - 191ms/step
Epoch 510/1000
2023-10-25 08:27:00.383 
Epoch 510/1000 
	 loss: 26.8797, MinusLogProbMetric: 26.8797, val_loss: 28.1774, val_MinusLogProbMetric: 28.1774

Epoch 510: val_loss did not improve from 28.16503
196/196 - 35s - loss: 26.8797 - MinusLogProbMetric: 26.8797 - val_loss: 28.1774 - val_MinusLogProbMetric: 28.1774 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 511/1000
2023-10-25 08:27:38.789 
Epoch 511/1000 
	 loss: 26.8791, MinusLogProbMetric: 26.8791, val_loss: 28.1801, val_MinusLogProbMetric: 28.1801

Epoch 511: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.8791 - MinusLogProbMetric: 26.8791 - val_loss: 28.1801 - val_MinusLogProbMetric: 28.1801 - lr: 6.2500e-05 - 38s/epoch - 196ms/step
Epoch 512/1000
2023-10-25 08:28:16.202 
Epoch 512/1000 
	 loss: 26.8821, MinusLogProbMetric: 26.8821, val_loss: 28.1749, val_MinusLogProbMetric: 28.1749

Epoch 512: val_loss did not improve from 28.16503
196/196 - 37s - loss: 26.8821 - MinusLogProbMetric: 26.8821 - val_loss: 28.1749 - val_MinusLogProbMetric: 28.1749 - lr: 6.2500e-05 - 37s/epoch - 191ms/step
Epoch 513/1000
2023-10-25 08:28:54.685 
Epoch 513/1000 
	 loss: 26.8781, MinusLogProbMetric: 26.8781, val_loss: 28.1818, val_MinusLogProbMetric: 28.1818

Epoch 513: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.8781 - MinusLogProbMetric: 26.8781 - val_loss: 28.1818 - val_MinusLogProbMetric: 28.1818 - lr: 6.2500e-05 - 38s/epoch - 196ms/step
Epoch 514/1000
2023-10-25 08:29:32.190 
Epoch 514/1000 
	 loss: 26.8787, MinusLogProbMetric: 26.8787, val_loss: 28.1893, val_MinusLogProbMetric: 28.1893

Epoch 514: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.8787 - MinusLogProbMetric: 26.8787 - val_loss: 28.1893 - val_MinusLogProbMetric: 28.1893 - lr: 6.2500e-05 - 38s/epoch - 191ms/step
Epoch 515/1000
2023-10-25 08:30:08.960 
Epoch 515/1000 
	 loss: 26.8804, MinusLogProbMetric: 26.8804, val_loss: 28.1718, val_MinusLogProbMetric: 28.1718

Epoch 515: val_loss did not improve from 28.16503
196/196 - 37s - loss: 26.8804 - MinusLogProbMetric: 26.8804 - val_loss: 28.1718 - val_MinusLogProbMetric: 28.1718 - lr: 6.2500e-05 - 37s/epoch - 188ms/step
Epoch 516/1000
2023-10-25 08:30:45.799 
Epoch 516/1000 
	 loss: 26.8789, MinusLogProbMetric: 26.8789, val_loss: 28.1734, val_MinusLogProbMetric: 28.1734

Epoch 516: val_loss did not improve from 28.16503
196/196 - 37s - loss: 26.8789 - MinusLogProbMetric: 26.8789 - val_loss: 28.1734 - val_MinusLogProbMetric: 28.1734 - lr: 6.2500e-05 - 37s/epoch - 188ms/step
Epoch 517/1000
2023-10-25 08:31:23.615 
Epoch 517/1000 
	 loss: 26.8783, MinusLogProbMetric: 26.8783, val_loss: 28.1712, val_MinusLogProbMetric: 28.1712

Epoch 517: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.8783 - MinusLogProbMetric: 26.8783 - val_loss: 28.1712 - val_MinusLogProbMetric: 28.1712 - lr: 6.2500e-05 - 38s/epoch - 193ms/step
Epoch 518/1000
2023-10-25 08:32:01.245 
Epoch 518/1000 
	 loss: 26.8788, MinusLogProbMetric: 26.8788, val_loss: 28.1761, val_MinusLogProbMetric: 28.1761

Epoch 518: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.8788 - MinusLogProbMetric: 26.8788 - val_loss: 28.1761 - val_MinusLogProbMetric: 28.1761 - lr: 6.2500e-05 - 38s/epoch - 192ms/step
Epoch 519/1000
2023-10-25 08:32:39.593 
Epoch 519/1000 
	 loss: 26.8779, MinusLogProbMetric: 26.8779, val_loss: 28.1830, val_MinusLogProbMetric: 28.1830

Epoch 519: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.8779 - MinusLogProbMetric: 26.8779 - val_loss: 28.1830 - val_MinusLogProbMetric: 28.1830 - lr: 6.2500e-05 - 38s/epoch - 196ms/step
Epoch 520/1000
2023-10-25 08:33:17.551 
Epoch 520/1000 
	 loss: 26.8748, MinusLogProbMetric: 26.8748, val_loss: 28.1725, val_MinusLogProbMetric: 28.1725

Epoch 520: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.8748 - MinusLogProbMetric: 26.8748 - val_loss: 28.1725 - val_MinusLogProbMetric: 28.1725 - lr: 6.2500e-05 - 38s/epoch - 194ms/step
Epoch 521/1000
2023-10-25 08:33:53.995 
Epoch 521/1000 
	 loss: 26.8764, MinusLogProbMetric: 26.8764, val_loss: 28.1754, val_MinusLogProbMetric: 28.1754

Epoch 521: val_loss did not improve from 28.16503
196/196 - 36s - loss: 26.8764 - MinusLogProbMetric: 26.8764 - val_loss: 28.1754 - val_MinusLogProbMetric: 28.1754 - lr: 6.2500e-05 - 36s/epoch - 186ms/step
Epoch 522/1000
2023-10-25 08:34:32.169 
Epoch 522/1000 
	 loss: 26.8771, MinusLogProbMetric: 26.8771, val_loss: 28.1710, val_MinusLogProbMetric: 28.1710

Epoch 522: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.8771 - MinusLogProbMetric: 26.8771 - val_loss: 28.1710 - val_MinusLogProbMetric: 28.1710 - lr: 6.2500e-05 - 38s/epoch - 195ms/step
Epoch 523/1000
2023-10-25 08:35:07.226 
Epoch 523/1000 
	 loss: 26.8764, MinusLogProbMetric: 26.8764, val_loss: 28.1792, val_MinusLogProbMetric: 28.1792

Epoch 523: val_loss did not improve from 28.16503
196/196 - 35s - loss: 26.8764 - MinusLogProbMetric: 26.8764 - val_loss: 28.1792 - val_MinusLogProbMetric: 28.1792 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 524/1000
2023-10-25 08:35:45.547 
Epoch 524/1000 
	 loss: 26.8779, MinusLogProbMetric: 26.8779, val_loss: 28.1830, val_MinusLogProbMetric: 28.1830

Epoch 524: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.8779 - MinusLogProbMetric: 26.8779 - val_loss: 28.1830 - val_MinusLogProbMetric: 28.1830 - lr: 6.2500e-05 - 38s/epoch - 195ms/step
Epoch 525/1000
2023-10-25 08:36:23.695 
Epoch 525/1000 
	 loss: 26.8764, MinusLogProbMetric: 26.8764, val_loss: 28.1810, val_MinusLogProbMetric: 28.1810

Epoch 525: val_loss did not improve from 28.16503
196/196 - 38s - loss: 26.8764 - MinusLogProbMetric: 26.8764 - val_loss: 28.1810 - val_MinusLogProbMetric: 28.1810 - lr: 6.2500e-05 - 38s/epoch - 195ms/step
Epoch 526/1000
2023-10-25 08:37:01.833 
Epoch 526/1000 
	 loss: 26.8748, MinusLogProbMetric: 26.8748, val_loss: 28.1856, val_MinusLogProbMetric: 28.1856

Epoch 526: val_loss did not improve from 28.16503
Restoring model weights from the end of the best epoch: 426.
196/196 - 39s - loss: 26.8748 - MinusLogProbMetric: 26.8748 - val_loss: 28.1856 - val_MinusLogProbMetric: 28.1856 - lr: 6.2500e-05 - 39s/epoch - 196ms/step
Epoch 526: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 440.
Model trained in 19657.20 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.72 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 0.92 s.
===========
Run 348/720 done in 19663.25 s.
===========

Directory ../../results/CsplineN_new/run_349/ already exists.
Skipping it.
===========
Run 349/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_350/ already exists.
Skipping it.
===========
Run 350/720 already exists. Skipping it.
===========

===========
Generating train data for run 351.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_248"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_249 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7fe77b548ee0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe4b4298460>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe4b4298460>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe77bf89f60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe77aedd480>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe77aedd9f0>, <keras.callbacks.ModelCheckpoint object at 0x7fe77aeddab0>, <keras.callbacks.EarlyStopping object at 0x7fe77aeddd20>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe77aeddd50>, <keras.callbacks.TerminateOnNaN object at 0x7fe77aedd990>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-25 08:37:10.666803
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 08:39:43.606 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 153s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 153s/epoch - 780ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 351.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_259"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_260 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7fe4ec4def80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe4f62b65f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe4f62b65f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe4ef7313f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe4ec47f070>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe4ec47f5e0>, <keras.callbacks.ModelCheckpoint object at 0x7fe4ec47f6a0>, <keras.callbacks.EarlyStopping object at 0x7fe4ec47f910>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe4ec47f940>, <keras.callbacks.TerminateOnNaN object at 0x7fe4ec47f580>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-25 08:39:54.365520
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 08:42:29.176 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 155s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 155s/epoch - 790ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0001111111111111111.
===========
Generating train data for run 351.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_270"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_271 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7fe55461bc40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe5fc592410>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe5fc592410>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe54d05a140>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe55463ac50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe55463b1c0>, <keras.callbacks.ModelCheckpoint object at 0x7fe55463b280>, <keras.callbacks.EarlyStopping object at 0x7fe55463b4f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe55463b520>, <keras.callbacks.TerminateOnNaN object at 0x7fe55463b160>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-25 08:42:37.604508
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 08:45:12.383 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 155s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 155s/epoch - 789ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 3.703703703703703e-05.
===========
Generating train data for run 351.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_281"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_282 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7fe77a223310>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe77acf6590>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe77acf6590>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe4343f1900>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe4343f1cf0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe4343f27a0>, <keras.callbacks.ModelCheckpoint object at 0x7fe4343f3100>, <keras.callbacks.EarlyStopping object at 0x7fe4343f3c10>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe4343f3a90>, <keras.callbacks.TerminateOnNaN object at 0x7fe4343f2f50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-25 08:45:29.129829
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 08:48:01.488 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 152s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 152s/epoch - 777ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.2345679012345677e-05.
===========
Generating train data for run 351.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_292"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_293 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7fe778fa3ca0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe5058b8190>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe5058b8190>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe4debf4fa0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe778fdacb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe778fdb220>, <keras.callbacks.ModelCheckpoint object at 0x7fe778fdb2e0>, <keras.callbacks.EarlyStopping object at 0x7fe778fdb550>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe778fdb580>, <keras.callbacks.TerminateOnNaN object at 0x7fe778fdb1c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-25 08:48:12.324522
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 08:50:57.419 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 165s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 165s/epoch - 842ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.115226337448558e-06.
===========
Generating train data for run 351.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_303"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_304 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7fe505033610>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe49eed8fd0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe49eed8fd0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe54cb96080>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe50501fd60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe50501fe20>, <keras.callbacks.ModelCheckpoint object at 0x7fe50501d900>, <keras.callbacks.EarlyStopping object at 0x7fe50501dab0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe50501dc30>, <keras.callbacks.TerminateOnNaN object at 0x7fe50501ff40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-25 08:51:07.144593
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 08:53:49.632 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 162s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 162s/epoch - 828ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.3717421124828526e-06.
===========
Generating train data for run 351.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_314"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_315 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7fe5998439d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe50d4e5c00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe50d4e5c00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe61d346fb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe50ded2f20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe50ded03d0>, <keras.callbacks.ModelCheckpoint object at 0x7fe50ded01c0>, <keras.callbacks.EarlyStopping object at 0x7fe50ded1c60>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe50ded1ab0>, <keras.callbacks.TerminateOnNaN object at 0x7fe50ded25c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-25 08:54:00.867748
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 08:56:42.511 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 162s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 162s/epoch - 824ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.572473708276175e-07.
===========
Generating train data for run 351.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_325"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_326 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7fe437a03d30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe49fbda3e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe49fbda3e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe4de8d8f10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe486fd23b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe486fd2920>, <keras.callbacks.ModelCheckpoint object at 0x7fe486fd29e0>, <keras.callbacks.EarlyStopping object at 0x7fe486fd2c50>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe486fd2c80>, <keras.callbacks.TerminateOnNaN object at 0x7fe486fd28c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-25 08:56:52.329559
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 08:59:43.025 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 171s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 171s/epoch - 870ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.524157902758725e-07.
===========
Generating train data for run 351.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_336"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_337 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7fe486a21e10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe4e7ca98a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe4e7ca98a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe580a8e0e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe50d3c3490>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe50d3c3a00>, <keras.callbacks.ModelCheckpoint object at 0x7fe50d3c3ac0>, <keras.callbacks.EarlyStopping object at 0x7fe50d3c3d30>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe50d3c3d60>, <keras.callbacks.TerminateOnNaN object at 0x7fe50d3c39a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-25 08:59:51.146252
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 09:02:40.947 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 170s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 170s/epoch - 865ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.0805263425290834e-08.
===========
Generating train data for run 351.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_347"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_348 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7fe54c5c7df0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe5fc5820b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe5fc5820b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe5ad7eb010>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe54c8efa00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe54c8edfc0>, <keras.callbacks.ModelCheckpoint object at 0x7fe54c8ee2c0>, <keras.callbacks.EarlyStopping object at 0x7fe54c8edd80>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe54c8eda20>, <keras.callbacks.TerminateOnNaN object at 0x7fe54c8eeaa0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-25 09:02:50.459606
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 09:05:38.298 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 168s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 168s/epoch - 855ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.6935087808430278e-08.
===========
Generating train data for run 351.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_351/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_351
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_358"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_359 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7fe435e4eb00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe436732f80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe436732f80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe4663e36d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe4deaff670>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_351/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe4deafcc40>, <keras.callbacks.ModelCheckpoint object at 0x7fe4deafe8c0>, <keras.callbacks.EarlyStopping object at 0x7fe4deafd510>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe4deafe290>, <keras.callbacks.TerminateOnNaN object at 0x7fe4deafce80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_351/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 351/720 with hyperparameters:
timestamp = 2023-10-25 09:05:46.711965
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 09:08:25.439 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7750.5083, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 159s - loss: nan - MinusLogProbMetric: 7750.5083 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 159s/epoch - 809ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.645029269476759e-09.
===========
Run 351/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 352.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_352
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_369"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_370 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7fe732fe6fb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe732f2c8b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe732f2c8b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe3c44c29e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe732e216f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe732e21c60>, <keras.callbacks.ModelCheckpoint object at 0x7fe732e21d20>, <keras.callbacks.EarlyStopping object at 0x7fe732e21f90>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe732e21fc0>, <keras.callbacks.TerminateOnNaN object at 0x7fe732e21c00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_352/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 352/720 with hyperparameters:
timestamp = 2023-10-25 09:08:36.345686
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 8: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 09:11:29.269 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5961.3618, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 173s - loss: nan - MinusLogProbMetric: 5961.3618 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 173s/epoch - 882ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 352.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_352
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_380"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_381 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7fe97c31ad70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe5d56f75b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe5d56f75b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe5a47a1510>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe7e041b730>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe7e041b790>, <keras.callbacks.ModelCheckpoint object at 0x7fe7e041af20>, <keras.callbacks.EarlyStopping object at 0x7fe7e041a830>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe7e041ac80>, <keras.callbacks.TerminateOnNaN object at 0x7fe7e041b850>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_352/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 352/720 with hyperparameters:
timestamp = 2023-10-25 09:11:39.842874
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 42: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 09:14:50.909 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5119.7515, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 191s - loss: nan - MinusLogProbMetric: 5119.7515 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 191s/epoch - 975ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0001111111111111111.
===========
Generating train data for run 352.
===========
Train data generated in 0.36 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_352
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_391"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_392 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7fe8b829a230>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe7e04e6140>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe7e04e6140>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe78a5c83d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe9fc5ed420>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe9fc5ec370>, <keras.callbacks.ModelCheckpoint object at 0x7fe9fc5ecb20>, <keras.callbacks.EarlyStopping object at 0x7fe9fc5ed210>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe9fc5ed8d0>, <keras.callbacks.TerminateOnNaN object at 0x7fe9fc5efeb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_352/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 352/720 with hyperparameters:
timestamp = 2023-10-25 09:15:02.709055
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 90: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 09:18:20.789 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5191.5557, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 198s - loss: nan - MinusLogProbMetric: 5191.5557 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 198s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 3.703703703703703e-05.
===========
Generating train data for run 352.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_352
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_402"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_403 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7fe38dfefe50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe9201307c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe9201307c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe435ab1de0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe732aa4250>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe732aa47c0>, <keras.callbacks.ModelCheckpoint object at 0x7fe732aa4880>, <keras.callbacks.EarlyStopping object at 0x7fe732aa4af0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe732aa4b20>, <keras.callbacks.TerminateOnNaN object at 0x7fe732aa4760>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_352/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 352/720 with hyperparameters:
timestamp = 2023-10-25 09:18:32.719845
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-10-25 09:22:35.223 
Epoch 1/1000 
	 loss: 5381.3691, MinusLogProbMetric: 5381.3691, val_loss: 4349.3452, val_MinusLogProbMetric: 4349.3452

Epoch 1: val_loss improved from inf to 4349.34521, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 243s - loss: 5381.3691 - MinusLogProbMetric: 5381.3691 - val_loss: 4349.3452 - val_MinusLogProbMetric: 4349.3452 - lr: 3.7037e-05 - 243s/epoch - 1s/step
Epoch 2/1000
2023-10-25 09:23:53.972 
Epoch 2/1000 
	 loss: 3143.1775, MinusLogProbMetric: 3143.1775, val_loss: 2388.1311, val_MinusLogProbMetric: 2388.1311

Epoch 2: val_loss improved from 4349.34521 to 2388.13110, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 3143.1775 - MinusLogProbMetric: 3143.1775 - val_loss: 2388.1311 - val_MinusLogProbMetric: 2388.1311 - lr: 3.7037e-05 - 78s/epoch - 400ms/step
Epoch 3/1000
2023-10-25 09:25:11.240 
Epoch 3/1000 
	 loss: 2036.7092, MinusLogProbMetric: 2036.7092, val_loss: 2006.0768, val_MinusLogProbMetric: 2006.0768

Epoch 3: val_loss improved from 2388.13110 to 2006.07678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 2036.7092 - MinusLogProbMetric: 2036.7092 - val_loss: 2006.0768 - val_MinusLogProbMetric: 2006.0768 - lr: 3.7037e-05 - 77s/epoch - 394ms/step
Epoch 4/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-25 09:26:30.218 
Epoch 4/1000 
	 loss: 1734.3558, MinusLogProbMetric: 1734.3558, val_loss: 1407.3094, val_MinusLogProbMetric: 1407.3093

Epoch 4: val_loss improved from 2006.07678 to 1407.30945, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 1734.3558 - MinusLogProbMetric: 1734.3558 - val_loss: 1407.3094 - val_MinusLogProbMetric: 1407.3093 - lr: 3.7037e-05 - 79s/epoch - 403ms/step
Epoch 5/1000
2023-10-25 09:27:46.085 
Epoch 5/1000 
	 loss: 1364.0879, MinusLogProbMetric: 1364.0879, val_loss: 1246.6820, val_MinusLogProbMetric: 1246.6820

Epoch 5: val_loss improved from 1407.30945 to 1246.68201, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 76s - loss: 1364.0879 - MinusLogProbMetric: 1364.0879 - val_loss: 1246.6820 - val_MinusLogProbMetric: 1246.6820 - lr: 3.7037e-05 - 76s/epoch - 388ms/step
Epoch 6/1000
2023-10-25 09:29:04.862 
Epoch 6/1000 
	 loss: 1429.4646, MinusLogProbMetric: 1429.4646, val_loss: 1328.9896, val_MinusLogProbMetric: 1328.9896

Epoch 6: val_loss did not improve from 1246.68201
196/196 - 77s - loss: 1429.4646 - MinusLogProbMetric: 1429.4646 - val_loss: 1328.9896 - val_MinusLogProbMetric: 1328.9896 - lr: 3.7037e-05 - 77s/epoch - 395ms/step
Epoch 7/1000
2023-10-25 09:30:19.453 
Epoch 7/1000 
	 loss: 1199.2625, MinusLogProbMetric: 1199.2625, val_loss: 1082.4459, val_MinusLogProbMetric: 1082.4459

Epoch 7: val_loss improved from 1246.68201 to 1082.44592, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 76s - loss: 1199.2625 - MinusLogProbMetric: 1199.2625 - val_loss: 1082.4459 - val_MinusLogProbMetric: 1082.4459 - lr: 3.7037e-05 - 76s/epoch - 387ms/step
Epoch 8/1000
2023-10-25 09:31:37.814 
Epoch 8/1000 
	 loss: 1049.4906, MinusLogProbMetric: 1049.4906, val_loss: 1007.4413, val_MinusLogProbMetric: 1007.4413

Epoch 8: val_loss improved from 1082.44592 to 1007.44128, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 1049.4906 - MinusLogProbMetric: 1049.4906 - val_loss: 1007.4413 - val_MinusLogProbMetric: 1007.4413 - lr: 3.7037e-05 - 78s/epoch - 400ms/step
Epoch 9/1000
2023-10-25 09:32:56.746 
Epoch 9/1000 
	 loss: 1236.4391, MinusLogProbMetric: 1236.4391, val_loss: 1285.7788, val_MinusLogProbMetric: 1285.7788

Epoch 9: val_loss did not improve from 1007.44128
196/196 - 78s - loss: 1236.4391 - MinusLogProbMetric: 1236.4391 - val_loss: 1285.7788 - val_MinusLogProbMetric: 1285.7788 - lr: 3.7037e-05 - 78s/epoch - 396ms/step
Epoch 10/1000
2023-10-25 09:34:13.366 
Epoch 10/1000 
	 loss: 1191.7937, MinusLogProbMetric: 1191.7937, val_loss: 1183.9246, val_MinusLogProbMetric: 1183.9246

Epoch 10: val_loss did not improve from 1007.44128
196/196 - 77s - loss: 1191.7937 - MinusLogProbMetric: 1191.7937 - val_loss: 1183.9246 - val_MinusLogProbMetric: 1183.9246 - lr: 3.7037e-05 - 77s/epoch - 391ms/step
Epoch 11/1000
2023-10-25 09:35:28.865 
Epoch 11/1000 
	 loss: 1160.7856, MinusLogProbMetric: 1160.7856, val_loss: 1029.3593, val_MinusLogProbMetric: 1029.3593

Epoch 11: val_loss did not improve from 1007.44128
196/196 - 75s - loss: 1160.7856 - MinusLogProbMetric: 1160.7856 - val_loss: 1029.3593 - val_MinusLogProbMetric: 1029.3593 - lr: 3.7037e-05 - 75s/epoch - 385ms/step
Epoch 12/1000
2023-10-25 09:36:44.964 
Epoch 12/1000 
	 loss: 949.2451, MinusLogProbMetric: 949.2451, val_loss: 1092.7412, val_MinusLogProbMetric: 1092.7412

Epoch 12: val_loss did not improve from 1007.44128
196/196 - 76s - loss: 949.2451 - MinusLogProbMetric: 949.2451 - val_loss: 1092.7412 - val_MinusLogProbMetric: 1092.7412 - lr: 3.7037e-05 - 76s/epoch - 388ms/step
Epoch 13/1000
2023-10-25 09:38:02.658 
Epoch 13/1000 
	 loss: 990.2238, MinusLogProbMetric: 990.2238, val_loss: 906.4299, val_MinusLogProbMetric: 906.4299

Epoch 13: val_loss improved from 1007.44128 to 906.42987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 990.2238 - MinusLogProbMetric: 990.2238 - val_loss: 906.4299 - val_MinusLogProbMetric: 906.4299 - lr: 3.7037e-05 - 79s/epoch - 403ms/step
Epoch 14/1000
2023-10-25 09:39:22.244 
Epoch 14/1000 
	 loss: 890.3322, MinusLogProbMetric: 890.3322, val_loss: 860.1862, val_MinusLogProbMetric: 860.1862

Epoch 14: val_loss improved from 906.42987 to 860.18616, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 890.3322 - MinusLogProbMetric: 890.3322 - val_loss: 860.1862 - val_MinusLogProbMetric: 860.1862 - lr: 3.7037e-05 - 79s/epoch - 405ms/step
Epoch 15/1000
2023-10-25 09:40:41.024 
Epoch 15/1000 
	 loss: 884.7613, MinusLogProbMetric: 884.7613, val_loss: 921.0555, val_MinusLogProbMetric: 921.0555

Epoch 15: val_loss did not improve from 860.18616
196/196 - 78s - loss: 884.7613 - MinusLogProbMetric: 884.7613 - val_loss: 921.0555 - val_MinusLogProbMetric: 921.0555 - lr: 3.7037e-05 - 78s/epoch - 396ms/step
Epoch 16/1000
2023-10-25 09:42:00.109 
Epoch 16/1000 
	 loss: 938.4204, MinusLogProbMetric: 938.4204, val_loss: 853.7806, val_MinusLogProbMetric: 853.7806

Epoch 16: val_loss improved from 860.18616 to 853.78064, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 938.4204 - MinusLogProbMetric: 938.4204 - val_loss: 853.7806 - val_MinusLogProbMetric: 853.7806 - lr: 3.7037e-05 - 80s/epoch - 410ms/step
Epoch 17/1000
2023-10-25 09:43:19.852 
Epoch 17/1000 
	 loss: 837.1359, MinusLogProbMetric: 837.1359, val_loss: 843.0620, val_MinusLogProbMetric: 843.0620

Epoch 17: val_loss improved from 853.78064 to 843.06195, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 837.1359 - MinusLogProbMetric: 837.1359 - val_loss: 843.0620 - val_MinusLogProbMetric: 843.0620 - lr: 3.7037e-05 - 80s/epoch - 407ms/step
Epoch 18/1000
2023-10-25 09:44:40.805 
Epoch 18/1000 
	 loss: 842.2610, MinusLogProbMetric: 842.2610, val_loss: 853.0403, val_MinusLogProbMetric: 853.0403

Epoch 18: val_loss did not improve from 843.06195
196/196 - 80s - loss: 842.2610 - MinusLogProbMetric: 842.2610 - val_loss: 853.0403 - val_MinusLogProbMetric: 853.0403 - lr: 3.7037e-05 - 80s/epoch - 406ms/step
Epoch 19/1000
2023-10-25 09:45:56.937 
Epoch 19/1000 
	 loss: 814.8419, MinusLogProbMetric: 814.8419, val_loss: 845.0542, val_MinusLogProbMetric: 845.0542

Epoch 19: val_loss did not improve from 843.06195
196/196 - 76s - loss: 814.8419 - MinusLogProbMetric: 814.8419 - val_loss: 845.0542 - val_MinusLogProbMetric: 845.0542 - lr: 3.7037e-05 - 76s/epoch - 388ms/step
Epoch 20/1000
2023-10-25 09:47:14.289 
Epoch 20/1000 
	 loss: 895.5157, MinusLogProbMetric: 895.5157, val_loss: 850.0109, val_MinusLogProbMetric: 850.0109

Epoch 20: val_loss did not improve from 843.06195
196/196 - 77s - loss: 895.5157 - MinusLogProbMetric: 895.5157 - val_loss: 850.0109 - val_MinusLogProbMetric: 850.0109 - lr: 3.7037e-05 - 77s/epoch - 395ms/step
Epoch 21/1000
2023-10-25 09:48:32.593 
Epoch 21/1000 
	 loss: 863.4108, MinusLogProbMetric: 863.4108, val_loss: 1052.5259, val_MinusLogProbMetric: 1052.5259

Epoch 21: val_loss did not improve from 843.06195
196/196 - 78s - loss: 863.4108 - MinusLogProbMetric: 863.4108 - val_loss: 1052.5259 - val_MinusLogProbMetric: 1052.5259 - lr: 3.7037e-05 - 78s/epoch - 399ms/step
Epoch 22/1000
2023-10-25 09:49:49.483 
Epoch 22/1000 
	 loss: 975.0203, MinusLogProbMetric: 975.0203, val_loss: 909.0782, val_MinusLogProbMetric: 909.0782

Epoch 22: val_loss did not improve from 843.06195
196/196 - 77s - loss: 975.0203 - MinusLogProbMetric: 975.0203 - val_loss: 909.0782 - val_MinusLogProbMetric: 909.0782 - lr: 3.7037e-05 - 77s/epoch - 392ms/step
Epoch 23/1000
2023-10-25 09:51:07.751 
Epoch 23/1000 
	 loss: 936.5717, MinusLogProbMetric: 936.5717, val_loss: 868.2606, val_MinusLogProbMetric: 868.2606

Epoch 23: val_loss did not improve from 843.06195
196/196 - 78s - loss: 936.5717 - MinusLogProbMetric: 936.5717 - val_loss: 868.2606 - val_MinusLogProbMetric: 868.2606 - lr: 3.7037e-05 - 78s/epoch - 399ms/step
Epoch 24/1000
2023-10-25 09:52:25.714 
Epoch 24/1000 
	 loss: 1001.2023, MinusLogProbMetric: 1001.2023, val_loss: 917.9944, val_MinusLogProbMetric: 917.9944

Epoch 24: val_loss did not improve from 843.06195
196/196 - 78s - loss: 1001.2023 - MinusLogProbMetric: 1001.2023 - val_loss: 917.9944 - val_MinusLogProbMetric: 917.9944 - lr: 3.7037e-05 - 78s/epoch - 398ms/step
Epoch 25/1000
2023-10-25 09:53:43.580 
Epoch 25/1000 
	 loss: 870.5107, MinusLogProbMetric: 870.5107, val_loss: 855.0728, val_MinusLogProbMetric: 855.0728

Epoch 25: val_loss did not improve from 843.06195
196/196 - 78s - loss: 870.5107 - MinusLogProbMetric: 870.5107 - val_loss: 855.0728 - val_MinusLogProbMetric: 855.0728 - lr: 3.7037e-05 - 78s/epoch - 397ms/step
Epoch 26/1000
2023-10-25 09:55:01.070 
Epoch 26/1000 
	 loss: 809.8637, MinusLogProbMetric: 809.8637, val_loss: 820.0888, val_MinusLogProbMetric: 820.0888

Epoch 26: val_loss improved from 843.06195 to 820.08881, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 809.8637 - MinusLogProbMetric: 809.8637 - val_loss: 820.0888 - val_MinusLogProbMetric: 820.0888 - lr: 3.7037e-05 - 79s/epoch - 401ms/step
Epoch 27/1000
2023-10-25 09:56:19.603 
Epoch 27/1000 
	 loss: 816.4169, MinusLogProbMetric: 816.4169, val_loss: 871.1535, val_MinusLogProbMetric: 871.1535

Epoch 27: val_loss did not improve from 820.08881
196/196 - 77s - loss: 816.4169 - MinusLogProbMetric: 816.4169 - val_loss: 871.1535 - val_MinusLogProbMetric: 871.1535 - lr: 3.7037e-05 - 77s/epoch - 395ms/step
Epoch 28/1000
2023-10-25 09:57:37.059 
Epoch 28/1000 
	 loss: 784.7921, MinusLogProbMetric: 784.7921, val_loss: 748.6317, val_MinusLogProbMetric: 748.6317

Epoch 28: val_loss improved from 820.08881 to 748.63171, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 784.7921 - MinusLogProbMetric: 784.7921 - val_loss: 748.6317 - val_MinusLogProbMetric: 748.6317 - lr: 3.7037e-05 - 79s/epoch - 401ms/step
Epoch 29/1000
2023-10-25 09:58:55.230 
Epoch 29/1000 
	 loss: 740.4501, MinusLogProbMetric: 740.4501, val_loss: 729.8743, val_MinusLogProbMetric: 729.8743

Epoch 29: val_loss improved from 748.63171 to 729.87433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 740.4501 - MinusLogProbMetric: 740.4501 - val_loss: 729.8743 - val_MinusLogProbMetric: 729.8743 - lr: 3.7037e-05 - 78s/epoch - 399ms/step
Epoch 30/1000
2023-10-25 10:00:13.409 
Epoch 30/1000 
	 loss: 754.3147, MinusLogProbMetric: 754.3147, val_loss: 790.3138, val_MinusLogProbMetric: 790.3138

Epoch 30: val_loss did not improve from 729.87433
196/196 - 77s - loss: 754.3147 - MinusLogProbMetric: 754.3147 - val_loss: 790.3138 - val_MinusLogProbMetric: 790.3138 - lr: 3.7037e-05 - 77s/epoch - 393ms/step
Epoch 31/1000
2023-10-25 10:01:32.211 
Epoch 31/1000 
	 loss: 776.4617, MinusLogProbMetric: 776.4617, val_loss: 742.2987, val_MinusLogProbMetric: 742.2987

Epoch 31: val_loss did not improve from 729.87433
196/196 - 79s - loss: 776.4617 - MinusLogProbMetric: 776.4617 - val_loss: 742.2987 - val_MinusLogProbMetric: 742.2987 - lr: 3.7037e-05 - 79s/epoch - 402ms/step
Epoch 32/1000
2023-10-25 10:02:53.804 
Epoch 32/1000 
	 loss: 752.4866, MinusLogProbMetric: 752.4866, val_loss: 735.2958, val_MinusLogProbMetric: 735.2958

Epoch 32: val_loss did not improve from 729.87433
196/196 - 82s - loss: 752.4866 - MinusLogProbMetric: 752.4866 - val_loss: 735.2958 - val_MinusLogProbMetric: 735.2958 - lr: 3.7037e-05 - 82s/epoch - 416ms/step
Epoch 33/1000
2023-10-25 10:04:13.134 
Epoch 33/1000 
	 loss: 728.1650, MinusLogProbMetric: 728.1650, val_loss: 728.2368, val_MinusLogProbMetric: 728.2368

Epoch 33: val_loss improved from 729.87433 to 728.23682, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 728.1650 - MinusLogProbMetric: 728.1650 - val_loss: 728.2368 - val_MinusLogProbMetric: 728.2368 - lr: 3.7037e-05 - 80s/epoch - 410ms/step
Epoch 34/1000
2023-10-25 10:05:32.441 
Epoch 34/1000 
	 loss: 727.0080, MinusLogProbMetric: 727.0080, val_loss: 707.3376, val_MinusLogProbMetric: 707.3376

Epoch 34: val_loss improved from 728.23682 to 707.33759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 727.0080 - MinusLogProbMetric: 727.0080 - val_loss: 707.3376 - val_MinusLogProbMetric: 707.3376 - lr: 3.7037e-05 - 79s/epoch - 405ms/step
Epoch 35/1000
2023-10-25 10:06:55.273 
Epoch 35/1000 
	 loss: 699.9214, MinusLogProbMetric: 699.9214, val_loss: 695.1660, val_MinusLogProbMetric: 695.1660

Epoch 35: val_loss improved from 707.33759 to 695.16602, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 83s - loss: 699.9214 - MinusLogProbMetric: 699.9214 - val_loss: 695.1660 - val_MinusLogProbMetric: 695.1660 - lr: 3.7037e-05 - 83s/epoch - 423ms/step
Epoch 36/1000
2023-10-25 10:08:16.079 
Epoch 36/1000 
	 loss: 693.0242, MinusLogProbMetric: 693.0242, val_loss: 676.3354, val_MinusLogProbMetric: 676.3354

Epoch 36: val_loss improved from 695.16602 to 676.33539, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 81s - loss: 693.0242 - MinusLogProbMetric: 693.0242 - val_loss: 676.3354 - val_MinusLogProbMetric: 676.3354 - lr: 3.7037e-05 - 81s/epoch - 412ms/step
Epoch 37/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 64: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 10:08:48.024 
Epoch 37/1000 
	 loss: nan, MinusLogProbMetric: 926.1639, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 37: val_loss did not improve from 676.33539
196/196 - 31s - loss: nan - MinusLogProbMetric: 926.1639 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 31s/epoch - 156ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.2345679012345677e-05.
===========
Generating train data for run 352.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_352
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_413"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_414 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7fea4031b2e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fee59f7c0d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fee59f7c0d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe7928e9f90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe43477edd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe43477f340>, <keras.callbacks.ModelCheckpoint object at 0x7fe43477f400>, <keras.callbacks.EarlyStopping object at 0x7fe43477f670>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe43477f6a0>, <keras.callbacks.TerminateOnNaN object at 0x7fe43477f2e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 352/720 with hyperparameters:
timestamp = 2023-10-25 10:08:56.599102
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-10-25 10:12:51.499 
Epoch 1/1000 
	 loss: 690.6928, MinusLogProbMetric: 690.6928, val_loss: 690.0458, val_MinusLogProbMetric: 690.0458

Epoch 1: val_loss improved from inf to 690.04578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 236s - loss: 690.6928 - MinusLogProbMetric: 690.6928 - val_loss: 690.0458 - val_MinusLogProbMetric: 690.0458 - lr: 1.2346e-05 - 236s/epoch - 1s/step
Epoch 2/1000
2023-10-25 10:14:11.562 
Epoch 2/1000 
	 loss: 712.5474, MinusLogProbMetric: 712.5474, val_loss: 721.9987, val_MinusLogProbMetric: 721.9987

Epoch 2: val_loss did not improve from 690.04578
196/196 - 78s - loss: 712.5474 - MinusLogProbMetric: 712.5474 - val_loss: 721.9987 - val_MinusLogProbMetric: 721.9987 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 3/1000
2023-10-25 10:15:29.921 
Epoch 3/1000 
	 loss: 727.9807, MinusLogProbMetric: 727.9807, val_loss: 769.0800, val_MinusLogProbMetric: 769.0800

Epoch 3: val_loss did not improve from 690.04578
196/196 - 78s - loss: 727.9807 - MinusLogProbMetric: 727.9807 - val_loss: 769.0800 - val_MinusLogProbMetric: 769.0800 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 4/1000
2023-10-25 10:16:49.031 
Epoch 4/1000 
	 loss: 679.5325, MinusLogProbMetric: 679.5325, val_loss: 621.1075, val_MinusLogProbMetric: 621.1075

Epoch 4: val_loss improved from 690.04578 to 621.10754, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 679.5325 - MinusLogProbMetric: 679.5325 - val_loss: 621.1075 - val_MinusLogProbMetric: 621.1075 - lr: 1.2346e-05 - 80s/epoch - 410ms/step
Epoch 5/1000
2023-10-25 10:18:08.680 
Epoch 5/1000 
	 loss: 614.5194, MinusLogProbMetric: 614.5194, val_loss: 596.5858, val_MinusLogProbMetric: 596.5858

Epoch 5: val_loss improved from 621.10754 to 596.58575, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 614.5194 - MinusLogProbMetric: 614.5194 - val_loss: 596.5858 - val_MinusLogProbMetric: 596.5858 - lr: 1.2346e-05 - 80s/epoch - 406ms/step
Epoch 6/1000
2023-10-25 10:19:28.926 
Epoch 6/1000 
	 loss: 589.3925, MinusLogProbMetric: 589.3925, val_loss: 565.2073, val_MinusLogProbMetric: 565.2073

Epoch 6: val_loss improved from 596.58575 to 565.20734, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 589.3925 - MinusLogProbMetric: 589.3925 - val_loss: 565.2073 - val_MinusLogProbMetric: 565.2073 - lr: 1.2346e-05 - 80s/epoch - 409ms/step
Epoch 7/1000
2023-10-25 10:20:47.105 
Epoch 7/1000 
	 loss: 562.6547, MinusLogProbMetric: 562.6547, val_loss: 548.0036, val_MinusLogProbMetric: 548.0036

Epoch 7: val_loss improved from 565.20734 to 548.00360, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 562.6547 - MinusLogProbMetric: 562.6547 - val_loss: 548.0036 - val_MinusLogProbMetric: 548.0036 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 8/1000
2023-10-25 10:22:06.942 
Epoch 8/1000 
	 loss: 539.2628, MinusLogProbMetric: 539.2628, val_loss: 528.1072, val_MinusLogProbMetric: 528.1072

Epoch 8: val_loss improved from 548.00360 to 528.10718, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 539.2628 - MinusLogProbMetric: 539.2628 - val_loss: 528.1072 - val_MinusLogProbMetric: 528.1072 - lr: 1.2346e-05 - 80s/epoch - 408ms/step
Epoch 9/1000
2023-10-25 10:23:29.089 
Epoch 9/1000 
	 loss: 518.7313, MinusLogProbMetric: 518.7313, val_loss: 510.0109, val_MinusLogProbMetric: 510.0109

Epoch 9: val_loss improved from 528.10718 to 510.01089, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 82s - loss: 518.7313 - MinusLogProbMetric: 518.7313 - val_loss: 510.0109 - val_MinusLogProbMetric: 510.0109 - lr: 1.2346e-05 - 82s/epoch - 418ms/step
Epoch 10/1000
2023-10-25 10:24:49.688 
Epoch 10/1000 
	 loss: 503.2244, MinusLogProbMetric: 503.2244, val_loss: 492.9641, val_MinusLogProbMetric: 492.9641

Epoch 10: val_loss improved from 510.01089 to 492.96414, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 83s - loss: 503.2244 - MinusLogProbMetric: 503.2244 - val_loss: 492.9641 - val_MinusLogProbMetric: 492.9641 - lr: 1.2346e-05 - 83s/epoch - 422ms/step
Epoch 11/1000
2023-10-25 10:26:09.828 
Epoch 11/1000 
	 loss: 493.5770, MinusLogProbMetric: 493.5770, val_loss: 492.4163, val_MinusLogProbMetric: 492.4163

Epoch 11: val_loss improved from 492.96414 to 492.41626, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 493.5770 - MinusLogProbMetric: 493.5770 - val_loss: 492.4163 - val_MinusLogProbMetric: 492.4163 - lr: 1.2346e-05 - 78s/epoch - 398ms/step
Epoch 12/1000
2023-10-25 10:27:30.293 
Epoch 12/1000 
	 loss: 487.5428, MinusLogProbMetric: 487.5428, val_loss: 482.1496, val_MinusLogProbMetric: 482.1496

Epoch 12: val_loss improved from 492.41626 to 482.14957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 81s - loss: 487.5428 - MinusLogProbMetric: 487.5428 - val_loss: 482.1496 - val_MinusLogProbMetric: 482.1496 - lr: 1.2346e-05 - 81s/epoch - 411ms/step
Epoch 13/1000
2023-10-25 10:28:51.288 
Epoch 13/1000 
	 loss: 471.6690, MinusLogProbMetric: 471.6690, val_loss: 469.1712, val_MinusLogProbMetric: 469.1712

Epoch 13: val_loss improved from 482.14957 to 469.17123, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 81s - loss: 471.6690 - MinusLogProbMetric: 471.6690 - val_loss: 469.1712 - val_MinusLogProbMetric: 469.1712 - lr: 1.2346e-05 - 81s/epoch - 413ms/step
Epoch 14/1000
2023-10-25 10:30:10.630 
Epoch 14/1000 
	 loss: 466.1078, MinusLogProbMetric: 466.1078, val_loss: 457.1572, val_MinusLogProbMetric: 457.1572

Epoch 14: val_loss improved from 469.17123 to 457.15720, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 466.1078 - MinusLogProbMetric: 466.1078 - val_loss: 457.1572 - val_MinusLogProbMetric: 457.1572 - lr: 1.2346e-05 - 79s/epoch - 404ms/step
Epoch 15/1000
2023-10-25 10:31:29.162 
Epoch 15/1000 
	 loss: 476.0126, MinusLogProbMetric: 476.0126, val_loss: 469.1633, val_MinusLogProbMetric: 469.1633

Epoch 15: val_loss did not improve from 457.15720
196/196 - 77s - loss: 476.0126 - MinusLogProbMetric: 476.0126 - val_loss: 469.1633 - val_MinusLogProbMetric: 469.1633 - lr: 1.2346e-05 - 77s/epoch - 395ms/step
Epoch 16/1000
2023-10-25 10:32:48.763 
Epoch 16/1000 
	 loss: 455.0266, MinusLogProbMetric: 455.0266, val_loss: 444.8994, val_MinusLogProbMetric: 444.8994

Epoch 16: val_loss improved from 457.15720 to 444.89941, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 81s - loss: 455.0266 - MinusLogProbMetric: 455.0266 - val_loss: 444.8994 - val_MinusLogProbMetric: 444.8994 - lr: 1.2346e-05 - 81s/epoch - 413ms/step
Epoch 17/1000
2023-10-25 10:34:06.257 
Epoch 17/1000 
	 loss: 470.8682, MinusLogProbMetric: 470.8682, val_loss: 481.0242, val_MinusLogProbMetric: 481.0242

Epoch 17: val_loss did not improve from 444.89941
196/196 - 76s - loss: 470.8682 - MinusLogProbMetric: 470.8682 - val_loss: 481.0242 - val_MinusLogProbMetric: 481.0242 - lr: 1.2346e-05 - 76s/epoch - 388ms/step
Epoch 18/1000
2023-10-25 10:35:25.383 
Epoch 18/1000 
	 loss: 471.4082, MinusLogProbMetric: 471.4082, val_loss: 457.2637, val_MinusLogProbMetric: 457.2637

Epoch 18: val_loss did not improve from 444.89941
196/196 - 79s - loss: 471.4082 - MinusLogProbMetric: 471.4082 - val_loss: 457.2637 - val_MinusLogProbMetric: 457.2637 - lr: 1.2346e-05 - 79s/epoch - 404ms/step
Epoch 19/1000
2023-10-25 10:36:44.115 
Epoch 19/1000 
	 loss: 465.1727, MinusLogProbMetric: 465.1727, val_loss: 461.7305, val_MinusLogProbMetric: 461.7305

Epoch 19: val_loss did not improve from 444.89941
196/196 - 79s - loss: 465.1727 - MinusLogProbMetric: 465.1727 - val_loss: 461.7305 - val_MinusLogProbMetric: 461.7305 - lr: 1.2346e-05 - 79s/epoch - 402ms/step
Epoch 20/1000
2023-10-25 10:38:02.633 
Epoch 20/1000 
	 loss: 454.2903, MinusLogProbMetric: 454.2903, val_loss: 444.7403, val_MinusLogProbMetric: 444.7403

Epoch 20: val_loss improved from 444.89941 to 444.74033, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 454.2903 - MinusLogProbMetric: 454.2903 - val_loss: 444.7403 - val_MinusLogProbMetric: 444.7403 - lr: 1.2346e-05 - 80s/epoch - 407ms/step
Epoch 21/1000
2023-10-25 10:39:21.635 
Epoch 21/1000 
	 loss: 438.4238, MinusLogProbMetric: 438.4238, val_loss: 427.1307, val_MinusLogProbMetric: 427.1307

Epoch 21: val_loss improved from 444.74033 to 427.13068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 438.4238 - MinusLogProbMetric: 438.4238 - val_loss: 427.1307 - val_MinusLogProbMetric: 427.1307 - lr: 1.2346e-05 - 79s/epoch - 404ms/step
Epoch 22/1000
2023-10-25 10:40:40.596 
Epoch 22/1000 
	 loss: 434.4117, MinusLogProbMetric: 434.4117, val_loss: 432.1260, val_MinusLogProbMetric: 432.1260

Epoch 22: val_loss did not improve from 427.13068
196/196 - 78s - loss: 434.4117 - MinusLogProbMetric: 434.4117 - val_loss: 432.1260 - val_MinusLogProbMetric: 432.1260 - lr: 1.2346e-05 - 78s/epoch - 396ms/step
Epoch 23/1000
2023-10-25 10:42:01.219 
Epoch 23/1000 
	 loss: 463.0265, MinusLogProbMetric: 463.0265, val_loss: 449.2352, val_MinusLogProbMetric: 449.2352

Epoch 23: val_loss did not improve from 427.13068
196/196 - 81s - loss: 463.0265 - MinusLogProbMetric: 463.0265 - val_loss: 449.2352 - val_MinusLogProbMetric: 449.2352 - lr: 1.2346e-05 - 81s/epoch - 411ms/step
Epoch 24/1000
2023-10-25 10:43:18.467 
Epoch 24/1000 
	 loss: 462.2166, MinusLogProbMetric: 462.2166, val_loss: 457.4469, val_MinusLogProbMetric: 457.4469

Epoch 24: val_loss did not improve from 427.13068
196/196 - 77s - loss: 462.2166 - MinusLogProbMetric: 462.2166 - val_loss: 457.4469 - val_MinusLogProbMetric: 457.4469 - lr: 1.2346e-05 - 77s/epoch - 394ms/step
Epoch 25/1000
2023-10-25 10:44:35.458 
Epoch 25/1000 
	 loss: 437.1663, MinusLogProbMetric: 437.1663, val_loss: 421.6813, val_MinusLogProbMetric: 421.6813

Epoch 25: val_loss improved from 427.13068 to 421.68127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 437.1663 - MinusLogProbMetric: 437.1663 - val_loss: 421.6813 - val_MinusLogProbMetric: 421.6813 - lr: 1.2346e-05 - 78s/epoch - 399ms/step
Epoch 26/1000
2023-10-25 10:45:52.824 
Epoch 26/1000 
	 loss: 411.9786, MinusLogProbMetric: 411.9786, val_loss: 403.5798, val_MinusLogProbMetric: 403.5798

Epoch 26: val_loss improved from 421.68127 to 403.57983, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 411.9786 - MinusLogProbMetric: 411.9786 - val_loss: 403.5798 - val_MinusLogProbMetric: 403.5798 - lr: 1.2346e-05 - 77s/epoch - 395ms/step
Epoch 27/1000
2023-10-25 10:47:09.226 
Epoch 27/1000 
	 loss: 398.3900, MinusLogProbMetric: 398.3900, val_loss: 392.4880, val_MinusLogProbMetric: 392.4880

Epoch 27: val_loss improved from 403.57983 to 392.48801, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 76s - loss: 398.3900 - MinusLogProbMetric: 398.3900 - val_loss: 392.4880 - val_MinusLogProbMetric: 392.4880 - lr: 1.2346e-05 - 76s/epoch - 390ms/step
Epoch 28/1000
2023-10-25 10:48:27.012 
Epoch 28/1000 
	 loss: 445.6610, MinusLogProbMetric: 445.6610, val_loss: 481.9642, val_MinusLogProbMetric: 481.9642

Epoch 28: val_loss did not improve from 392.48801
196/196 - 77s - loss: 445.6610 - MinusLogProbMetric: 445.6610 - val_loss: 481.9642 - val_MinusLogProbMetric: 481.9642 - lr: 1.2346e-05 - 77s/epoch - 391ms/step
Epoch 29/1000
2023-10-25 10:49:43.589 
Epoch 29/1000 
	 loss: 457.4967, MinusLogProbMetric: 457.4967, val_loss: 444.4806, val_MinusLogProbMetric: 444.4806

Epoch 29: val_loss did not improve from 392.48801
196/196 - 77s - loss: 457.4967 - MinusLogProbMetric: 457.4967 - val_loss: 444.4806 - val_MinusLogProbMetric: 444.4806 - lr: 1.2346e-05 - 77s/epoch - 391ms/step
Epoch 30/1000
2023-10-25 10:51:01.294 
Epoch 30/1000 
	 loss: 429.2953, MinusLogProbMetric: 429.2953, val_loss: 423.7378, val_MinusLogProbMetric: 423.7378

Epoch 30: val_loss did not improve from 392.48801
196/196 - 78s - loss: 429.2953 - MinusLogProbMetric: 429.2953 - val_loss: 423.7378 - val_MinusLogProbMetric: 423.7378 - lr: 1.2346e-05 - 78s/epoch - 396ms/step
Epoch 31/1000
2023-10-25 10:52:19.006 
Epoch 31/1000 
	 loss: 410.9478, MinusLogProbMetric: 410.9478, val_loss: 404.7439, val_MinusLogProbMetric: 404.7439

Epoch 31: val_loss did not improve from 392.48801
196/196 - 78s - loss: 410.9478 - MinusLogProbMetric: 410.9478 - val_loss: 404.7439 - val_MinusLogProbMetric: 404.7439 - lr: 1.2346e-05 - 78s/epoch - 396ms/step
Epoch 32/1000
2023-10-25 10:53:33.636 
Epoch 32/1000 
	 loss: 397.7281, MinusLogProbMetric: 397.7281, val_loss: 391.8508, val_MinusLogProbMetric: 391.8508

Epoch 32: val_loss improved from 392.48801 to 391.85077, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 76s - loss: 397.7281 - MinusLogProbMetric: 397.7281 - val_loss: 391.8508 - val_MinusLogProbMetric: 391.8508 - lr: 1.2346e-05 - 76s/epoch - 387ms/step
Epoch 33/1000
2023-10-25 10:54:51.308 
Epoch 33/1000 
	 loss: 385.9123, MinusLogProbMetric: 385.9123, val_loss: 380.0063, val_MinusLogProbMetric: 380.0063

Epoch 33: val_loss improved from 391.85077 to 380.00629, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 385.9123 - MinusLogProbMetric: 385.9123 - val_loss: 380.0063 - val_MinusLogProbMetric: 380.0063 - lr: 1.2346e-05 - 78s/epoch - 397ms/step
Epoch 34/1000
2023-10-25 10:56:10.238 
Epoch 34/1000 
	 loss: 380.3427, MinusLogProbMetric: 380.3427, val_loss: 375.7635, val_MinusLogProbMetric: 375.7635

Epoch 34: val_loss improved from 380.00629 to 375.76346, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 380.3427 - MinusLogProbMetric: 380.3427 - val_loss: 375.7635 - val_MinusLogProbMetric: 375.7635 - lr: 1.2346e-05 - 79s/epoch - 402ms/step
Epoch 35/1000
2023-10-25 10:57:27.426 
Epoch 35/1000 
	 loss: 371.0270, MinusLogProbMetric: 371.0270, val_loss: 366.1409, val_MinusLogProbMetric: 366.1409

Epoch 35: val_loss improved from 375.76346 to 366.14093, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 371.0270 - MinusLogProbMetric: 371.0270 - val_loss: 366.1409 - val_MinusLogProbMetric: 366.1409 - lr: 1.2346e-05 - 77s/epoch - 394ms/step
Epoch 36/1000
2023-10-25 10:58:46.676 
Epoch 36/1000 
	 loss: 362.9531, MinusLogProbMetric: 362.9531, val_loss: 360.1869, val_MinusLogProbMetric: 360.1869

Epoch 36: val_loss improved from 366.14093 to 360.18689, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 362.9531 - MinusLogProbMetric: 362.9531 - val_loss: 360.1869 - val_MinusLogProbMetric: 360.1869 - lr: 1.2346e-05 - 79s/epoch - 405ms/step
Epoch 37/1000
2023-10-25 11:00:04.275 
Epoch 37/1000 
	 loss: 357.0667, MinusLogProbMetric: 357.0667, val_loss: 354.2249, val_MinusLogProbMetric: 354.2249

Epoch 37: val_loss improved from 360.18689 to 354.22495, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 357.0667 - MinusLogProbMetric: 357.0667 - val_loss: 354.2249 - val_MinusLogProbMetric: 354.2249 - lr: 1.2346e-05 - 78s/epoch - 395ms/step
Epoch 38/1000
2023-10-25 11:01:24.665 
Epoch 38/1000 
	 loss: 351.6129, MinusLogProbMetric: 351.6129, val_loss: 349.0360, val_MinusLogProbMetric: 349.0360

Epoch 38: val_loss improved from 354.22495 to 349.03598, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 351.6129 - MinusLogProbMetric: 351.6129 - val_loss: 349.0360 - val_MinusLogProbMetric: 349.0360 - lr: 1.2346e-05 - 80s/epoch - 411ms/step
Epoch 39/1000
2023-10-25 11:02:43.177 
Epoch 39/1000 
	 loss: 347.1500, MinusLogProbMetric: 347.1500, val_loss: 345.2750, val_MinusLogProbMetric: 345.2750

Epoch 39: val_loss improved from 349.03598 to 345.27496, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 347.1500 - MinusLogProbMetric: 347.1500 - val_loss: 345.2750 - val_MinusLogProbMetric: 345.2750 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 40/1000
2023-10-25 11:04:03.013 
Epoch 40/1000 
	 loss: 344.4811, MinusLogProbMetric: 344.4811, val_loss: 342.1992, val_MinusLogProbMetric: 342.1992

Epoch 40: val_loss improved from 345.27496 to 342.19922, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 344.4811 - MinusLogProbMetric: 344.4811 - val_loss: 342.1992 - val_MinusLogProbMetric: 342.1992 - lr: 1.2346e-05 - 80s/epoch - 407ms/step
Epoch 41/1000
2023-10-25 11:05:25.098 
Epoch 41/1000 
	 loss: 339.6426, MinusLogProbMetric: 339.6426, val_loss: 337.4024, val_MinusLogProbMetric: 337.4024

Epoch 41: val_loss improved from 342.19922 to 337.40244, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 82s - loss: 339.6426 - MinusLogProbMetric: 339.6426 - val_loss: 337.4024 - val_MinusLogProbMetric: 337.4024 - lr: 1.2346e-05 - 82s/epoch - 419ms/step
Epoch 42/1000
2023-10-25 11:06:42.683 
Epoch 42/1000 
	 loss: 337.2554, MinusLogProbMetric: 337.2554, val_loss: 332.3327, val_MinusLogProbMetric: 332.3327

Epoch 42: val_loss improved from 337.40244 to 332.33273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 337.2554 - MinusLogProbMetric: 337.2554 - val_loss: 332.3327 - val_MinusLogProbMetric: 332.3327 - lr: 1.2346e-05 - 78s/epoch - 396ms/step
Epoch 43/1000
2023-10-25 11:08:04.924 
Epoch 43/1000 
	 loss: 330.3930, MinusLogProbMetric: 330.3930, val_loss: 329.5221, val_MinusLogProbMetric: 329.5221

Epoch 43: val_loss improved from 332.33273 to 329.52206, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 82s - loss: 330.3930 - MinusLogProbMetric: 330.3930 - val_loss: 329.5221 - val_MinusLogProbMetric: 329.5221 - lr: 1.2346e-05 - 82s/epoch - 420ms/step
Epoch 44/1000
2023-10-25 11:09:25.385 
Epoch 44/1000 
	 loss: 327.2010, MinusLogProbMetric: 327.2010, val_loss: 324.6284, val_MinusLogProbMetric: 324.6284

Epoch 44: val_loss improved from 329.52206 to 324.62836, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 327.2010 - MinusLogProbMetric: 327.2010 - val_loss: 324.6284 - val_MinusLogProbMetric: 324.6284 - lr: 1.2346e-05 - 80s/epoch - 410ms/step
Epoch 45/1000
2023-10-25 11:10:44.418 
Epoch 45/1000 
	 loss: 323.3194, MinusLogProbMetric: 323.3194, val_loss: 321.3707, val_MinusLogProbMetric: 321.3707

Epoch 45: val_loss improved from 324.62836 to 321.37070, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 323.3194 - MinusLogProbMetric: 323.3194 - val_loss: 321.3707 - val_MinusLogProbMetric: 321.3707 - lr: 1.2346e-05 - 79s/epoch - 403ms/step
Epoch 46/1000
2023-10-25 11:12:01.464 
Epoch 46/1000 
	 loss: 320.1115, MinusLogProbMetric: 320.1115, val_loss: 318.9138, val_MinusLogProbMetric: 318.9138

Epoch 46: val_loss improved from 321.37070 to 318.91382, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 320.1115 - MinusLogProbMetric: 320.1115 - val_loss: 318.9138 - val_MinusLogProbMetric: 318.9138 - lr: 1.2346e-05 - 77s/epoch - 393ms/step
Epoch 47/1000
2023-10-25 11:13:20.039 
Epoch 47/1000 
	 loss: 316.6113, MinusLogProbMetric: 316.6113, val_loss: 314.6272, val_MinusLogProbMetric: 314.6272

Epoch 47: val_loss improved from 318.91382 to 314.62723, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 316.6113 - MinusLogProbMetric: 316.6113 - val_loss: 314.6272 - val_MinusLogProbMetric: 314.6272 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 48/1000
2023-10-25 11:14:39.224 
Epoch 48/1000 
	 loss: 316.8362, MinusLogProbMetric: 316.8362, val_loss: 312.6196, val_MinusLogProbMetric: 312.6196

Epoch 48: val_loss improved from 314.62723 to 312.61957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 316.8362 - MinusLogProbMetric: 316.8362 - val_loss: 312.6196 - val_MinusLogProbMetric: 312.6196 - lr: 1.2346e-05 - 79s/epoch - 404ms/step
Epoch 49/1000
2023-10-25 11:15:56.469 
Epoch 49/1000 
	 loss: 310.9283, MinusLogProbMetric: 310.9283, val_loss: 309.3182, val_MinusLogProbMetric: 309.3182

Epoch 49: val_loss improved from 312.61957 to 309.31824, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 310.9283 - MinusLogProbMetric: 310.9283 - val_loss: 309.3182 - val_MinusLogProbMetric: 309.3182 - lr: 1.2346e-05 - 77s/epoch - 393ms/step
Epoch 50/1000
2023-10-25 11:17:14.672 
Epoch 50/1000 
	 loss: 308.3653, MinusLogProbMetric: 308.3653, val_loss: 306.8475, val_MinusLogProbMetric: 306.8475

Epoch 50: val_loss improved from 309.31824 to 306.84747, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 308.3653 - MinusLogProbMetric: 308.3653 - val_loss: 306.8475 - val_MinusLogProbMetric: 306.8475 - lr: 1.2346e-05 - 79s/epoch - 402ms/step
Epoch 51/1000
2023-10-25 11:18:36.389 
Epoch 51/1000 
	 loss: 305.6717, MinusLogProbMetric: 305.6717, val_loss: 304.2837, val_MinusLogProbMetric: 304.2837

Epoch 51: val_loss improved from 306.84747 to 304.28369, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 82s - loss: 305.6717 - MinusLogProbMetric: 305.6717 - val_loss: 304.2837 - val_MinusLogProbMetric: 304.2837 - lr: 1.2346e-05 - 82s/epoch - 416ms/step
Epoch 52/1000
2023-10-25 11:19:55.578 
Epoch 52/1000 
	 loss: 303.3339, MinusLogProbMetric: 303.3339, val_loss: 302.7341, val_MinusLogProbMetric: 302.7341

Epoch 52: val_loss improved from 304.28369 to 302.73413, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 303.3339 - MinusLogProbMetric: 303.3339 - val_loss: 302.7341 - val_MinusLogProbMetric: 302.7341 - lr: 1.2346e-05 - 79s/epoch - 404ms/step
Epoch 53/1000
2023-10-25 11:21:17.078 
Epoch 53/1000 
	 loss: 300.9678, MinusLogProbMetric: 300.9678, val_loss: 299.5078, val_MinusLogProbMetric: 299.5078

Epoch 53: val_loss improved from 302.73413 to 299.50775, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 82s - loss: 300.9678 - MinusLogProbMetric: 300.9678 - val_loss: 299.5078 - val_MinusLogProbMetric: 299.5078 - lr: 1.2346e-05 - 82s/epoch - 416ms/step
Epoch 54/1000
2023-10-25 11:22:38.536 
Epoch 54/1000 
	 loss: 298.4557, MinusLogProbMetric: 298.4557, val_loss: 297.1707, val_MinusLogProbMetric: 297.1707

Epoch 54: val_loss improved from 299.50775 to 297.17068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 81s - loss: 298.4557 - MinusLogProbMetric: 298.4557 - val_loss: 297.1707 - val_MinusLogProbMetric: 297.1707 - lr: 1.2346e-05 - 81s/epoch - 415ms/step
Epoch 55/1000
2023-10-25 11:23:59.392 
Epoch 55/1000 
	 loss: 296.0732, MinusLogProbMetric: 296.0732, val_loss: 294.5859, val_MinusLogProbMetric: 294.5859

Epoch 55: val_loss improved from 297.17068 to 294.58591, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 81s - loss: 296.0732 - MinusLogProbMetric: 296.0732 - val_loss: 294.5859 - val_MinusLogProbMetric: 294.5859 - lr: 1.2346e-05 - 81s/epoch - 412ms/step
Epoch 56/1000
2023-10-25 11:25:16.071 
Epoch 56/1000 
	 loss: 293.7554, MinusLogProbMetric: 293.7554, val_loss: 292.2831, val_MinusLogProbMetric: 292.2831

Epoch 56: val_loss improved from 294.58591 to 292.28308, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 293.7554 - MinusLogProbMetric: 293.7554 - val_loss: 292.2831 - val_MinusLogProbMetric: 292.2831 - lr: 1.2346e-05 - 77s/epoch - 392ms/step
Epoch 57/1000
2023-10-25 11:26:36.595 
Epoch 57/1000 
	 loss: 291.8462, MinusLogProbMetric: 291.8462, val_loss: 290.6702, val_MinusLogProbMetric: 290.6702

Epoch 57: val_loss improved from 292.28308 to 290.67017, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 291.8462 - MinusLogProbMetric: 291.8462 - val_loss: 290.6702 - val_MinusLogProbMetric: 290.6702 - lr: 1.2346e-05 - 80s/epoch - 409ms/step
Epoch 58/1000
2023-10-25 11:27:57.193 
Epoch 58/1000 
	 loss: 289.3810, MinusLogProbMetric: 289.3810, val_loss: 287.9742, val_MinusLogProbMetric: 287.9742

Epoch 58: val_loss improved from 290.67017 to 287.97421, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 81s - loss: 289.3810 - MinusLogProbMetric: 289.3810 - val_loss: 287.9742 - val_MinusLogProbMetric: 287.9742 - lr: 1.2346e-05 - 81s/epoch - 412ms/step
Epoch 59/1000
2023-10-25 11:29:15.873 
Epoch 59/1000 
	 loss: 325.9213, MinusLogProbMetric: 325.9213, val_loss: 409.4816, val_MinusLogProbMetric: 409.4816

Epoch 59: val_loss did not improve from 287.97421
196/196 - 77s - loss: 325.9213 - MinusLogProbMetric: 325.9213 - val_loss: 409.4816 - val_MinusLogProbMetric: 409.4816 - lr: 1.2346e-05 - 77s/epoch - 395ms/step
Epoch 60/1000
2023-10-25 11:30:32.704 
Epoch 60/1000 
	 loss: 418.0726, MinusLogProbMetric: 418.0726, val_loss: 382.9088, val_MinusLogProbMetric: 382.9088

Epoch 60: val_loss did not improve from 287.97421
196/196 - 77s - loss: 418.0726 - MinusLogProbMetric: 418.0726 - val_loss: 382.9088 - val_MinusLogProbMetric: 382.9088 - lr: 1.2346e-05 - 77s/epoch - 392ms/step
Epoch 61/1000
2023-10-25 11:31:50.990 
Epoch 61/1000 
	 loss: 365.6868, MinusLogProbMetric: 365.6868, val_loss: 358.4606, val_MinusLogProbMetric: 358.4606

Epoch 61: val_loss did not improve from 287.97421
196/196 - 78s - loss: 365.6868 - MinusLogProbMetric: 365.6868 - val_loss: 358.4606 - val_MinusLogProbMetric: 358.4606 - lr: 1.2346e-05 - 78s/epoch - 399ms/step
Epoch 62/1000
2023-10-25 11:33:04.453 
Epoch 62/1000 
	 loss: 345.8764, MinusLogProbMetric: 345.8764, val_loss: 339.5933, val_MinusLogProbMetric: 339.5933

Epoch 62: val_loss did not improve from 287.97421
196/196 - 73s - loss: 345.8764 - MinusLogProbMetric: 345.8764 - val_loss: 339.5933 - val_MinusLogProbMetric: 339.5933 - lr: 1.2346e-05 - 73s/epoch - 375ms/step
Epoch 63/1000
2023-10-25 11:34:19.911 
Epoch 63/1000 
	 loss: 334.4861, MinusLogProbMetric: 334.4861, val_loss: 331.7141, val_MinusLogProbMetric: 331.7141

Epoch 63: val_loss did not improve from 287.97421
196/196 - 75s - loss: 334.4861 - MinusLogProbMetric: 334.4861 - val_loss: 331.7141 - val_MinusLogProbMetric: 331.7141 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 64/1000
2023-10-25 11:35:38.442 
Epoch 64/1000 
	 loss: 326.5632, MinusLogProbMetric: 326.5632, val_loss: 322.1541, val_MinusLogProbMetric: 322.1541

Epoch 64: val_loss did not improve from 287.97421
196/196 - 79s - loss: 326.5632 - MinusLogProbMetric: 326.5632 - val_loss: 322.1541 - val_MinusLogProbMetric: 322.1541 - lr: 1.2346e-05 - 79s/epoch - 401ms/step
Epoch 65/1000
2023-10-25 11:37:00.553 
Epoch 65/1000 
	 loss: 318.8362, MinusLogProbMetric: 318.8362, val_loss: 316.1539, val_MinusLogProbMetric: 316.1539

Epoch 65: val_loss did not improve from 287.97421
196/196 - 82s - loss: 318.8362 - MinusLogProbMetric: 318.8362 - val_loss: 316.1539 - val_MinusLogProbMetric: 316.1539 - lr: 1.2346e-05 - 82s/epoch - 419ms/step
Epoch 66/1000
2023-10-25 11:38:19.215 
Epoch 66/1000 
	 loss: 313.6871, MinusLogProbMetric: 313.6871, val_loss: 311.1559, val_MinusLogProbMetric: 311.1559

Epoch 66: val_loss did not improve from 287.97421
196/196 - 79s - loss: 313.6871 - MinusLogProbMetric: 313.6871 - val_loss: 311.1559 - val_MinusLogProbMetric: 311.1559 - lr: 1.2346e-05 - 79s/epoch - 401ms/step
Epoch 67/1000
2023-10-25 11:39:36.901 
Epoch 67/1000 
	 loss: 308.9057, MinusLogProbMetric: 308.9057, val_loss: 306.2023, val_MinusLogProbMetric: 306.2023

Epoch 67: val_loss did not improve from 287.97421
196/196 - 78s - loss: 308.9057 - MinusLogProbMetric: 308.9057 - val_loss: 306.2023 - val_MinusLogProbMetric: 306.2023 - lr: 1.2346e-05 - 78s/epoch - 396ms/step
Epoch 68/1000
2023-10-25 11:40:54.014 
Epoch 68/1000 
	 loss: 304.4529, MinusLogProbMetric: 304.4529, val_loss: 302.2877, val_MinusLogProbMetric: 302.2877

Epoch 68: val_loss did not improve from 287.97421
196/196 - 77s - loss: 304.4529 - MinusLogProbMetric: 304.4529 - val_loss: 302.2877 - val_MinusLogProbMetric: 302.2877 - lr: 1.2346e-05 - 77s/epoch - 393ms/step
Epoch 69/1000
2023-10-25 11:42:12.131 
Epoch 69/1000 
	 loss: 300.3868, MinusLogProbMetric: 300.3868, val_loss: 298.5989, val_MinusLogProbMetric: 298.5989

Epoch 69: val_loss did not improve from 287.97421
196/196 - 78s - loss: 300.3868 - MinusLogProbMetric: 300.3868 - val_loss: 298.5989 - val_MinusLogProbMetric: 298.5989 - lr: 1.2346e-05 - 78s/epoch - 399ms/step
Epoch 70/1000
2023-10-25 11:43:29.457 
Epoch 70/1000 
	 loss: 299.6960, MinusLogProbMetric: 299.6960, val_loss: 381.0410, val_MinusLogProbMetric: 381.0410

Epoch 70: val_loss did not improve from 287.97421
196/196 - 77s - loss: 299.6960 - MinusLogProbMetric: 299.6960 - val_loss: 381.0410 - val_MinusLogProbMetric: 381.0410 - lr: 1.2346e-05 - 77s/epoch - 395ms/step
Epoch 71/1000
2023-10-25 11:44:47.135 
Epoch 71/1000 
	 loss: 362.5043, MinusLogProbMetric: 362.5043, val_loss: 349.1129, val_MinusLogProbMetric: 349.1129

Epoch 71: val_loss did not improve from 287.97421
196/196 - 78s - loss: 362.5043 - MinusLogProbMetric: 362.5043 - val_loss: 349.1129 - val_MinusLogProbMetric: 349.1129 - lr: 1.2346e-05 - 78s/epoch - 396ms/step
Epoch 72/1000
2023-10-25 11:46:03.514 
Epoch 72/1000 
	 loss: 333.2579, MinusLogProbMetric: 333.2579, val_loss: 318.2474, val_MinusLogProbMetric: 318.2474

Epoch 72: val_loss did not improve from 287.97421
196/196 - 76s - loss: 333.2579 - MinusLogProbMetric: 333.2579 - val_loss: 318.2474 - val_MinusLogProbMetric: 318.2474 - lr: 1.2346e-05 - 76s/epoch - 390ms/step
Epoch 73/1000
2023-10-25 11:47:25.029 
Epoch 73/1000 
	 loss: 312.4342, MinusLogProbMetric: 312.4342, val_loss: 307.4623, val_MinusLogProbMetric: 307.4623

Epoch 73: val_loss did not improve from 287.97421
196/196 - 82s - loss: 312.4342 - MinusLogProbMetric: 312.4342 - val_loss: 307.4623 - val_MinusLogProbMetric: 307.4623 - lr: 1.2346e-05 - 82s/epoch - 416ms/step
Epoch 74/1000
2023-10-25 11:48:44.563 
Epoch 74/1000 
	 loss: 309.5973, MinusLogProbMetric: 309.5973, val_loss: 301.3366, val_MinusLogProbMetric: 301.3366

Epoch 74: val_loss did not improve from 287.97421
196/196 - 80s - loss: 309.5973 - MinusLogProbMetric: 309.5973 - val_loss: 301.3366 - val_MinusLogProbMetric: 301.3366 - lr: 1.2346e-05 - 80s/epoch - 406ms/step
Epoch 75/1000
2023-10-25 11:50:03.317 
Epoch 75/1000 
	 loss: 298.8939, MinusLogProbMetric: 298.8939, val_loss: 295.4432, val_MinusLogProbMetric: 295.4432

Epoch 75: val_loss did not improve from 287.97421
196/196 - 79s - loss: 298.8939 - MinusLogProbMetric: 298.8939 - val_loss: 295.4432 - val_MinusLogProbMetric: 295.4432 - lr: 1.2346e-05 - 79s/epoch - 402ms/step
Epoch 76/1000
2023-10-25 11:51:24.769 
Epoch 76/1000 
	 loss: 292.0316, MinusLogProbMetric: 292.0316, val_loss: 289.9401, val_MinusLogProbMetric: 289.9401

Epoch 76: val_loss did not improve from 287.97421
196/196 - 81s - loss: 292.0316 - MinusLogProbMetric: 292.0316 - val_loss: 289.9401 - val_MinusLogProbMetric: 289.9401 - lr: 1.2346e-05 - 81s/epoch - 416ms/step
Epoch 77/1000
2023-10-25 11:52:43.627 
Epoch 77/1000 
	 loss: 287.0259, MinusLogProbMetric: 287.0259, val_loss: 285.8844, val_MinusLogProbMetric: 285.8844

Epoch 77: val_loss improved from 287.97421 to 285.88443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 287.0259 - MinusLogProbMetric: 287.0259 - val_loss: 285.8844 - val_MinusLogProbMetric: 285.8844 - lr: 1.2346e-05 - 80s/epoch - 408ms/step
Epoch 78/1000
2023-10-25 11:53:59.896 
Epoch 78/1000 
	 loss: 283.0383, MinusLogProbMetric: 283.0383, val_loss: 281.4639, val_MinusLogProbMetric: 281.4639

Epoch 78: val_loss improved from 285.88443 to 281.46393, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 76s - loss: 283.0383 - MinusLogProbMetric: 283.0383 - val_loss: 281.4639 - val_MinusLogProbMetric: 281.4639 - lr: 1.2346e-05 - 76s/epoch - 389ms/step
Epoch 79/1000
2023-10-25 11:55:19.768 
Epoch 79/1000 
	 loss: 279.6405, MinusLogProbMetric: 279.6405, val_loss: 278.2362, val_MinusLogProbMetric: 278.2362

Epoch 79: val_loss improved from 281.46393 to 278.23621, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 279.6405 - MinusLogProbMetric: 279.6405 - val_loss: 278.2362 - val_MinusLogProbMetric: 278.2362 - lr: 1.2346e-05 - 80s/epoch - 408ms/step
Epoch 80/1000
2023-10-25 11:56:38.266 
Epoch 80/1000 
	 loss: 276.6755, MinusLogProbMetric: 276.6755, val_loss: 275.4367, val_MinusLogProbMetric: 275.4367

Epoch 80: val_loss improved from 278.23621 to 275.43674, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 276.6755 - MinusLogProbMetric: 276.6755 - val_loss: 275.4367 - val_MinusLogProbMetric: 275.4367 - lr: 1.2346e-05 - 79s/epoch - 401ms/step
Epoch 81/1000
2023-10-25 11:57:57.399 
Epoch 81/1000 
	 loss: 273.9488, MinusLogProbMetric: 273.9488, val_loss: 273.2410, val_MinusLogProbMetric: 273.2410

Epoch 81: val_loss improved from 275.43674 to 273.24103, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 273.9488 - MinusLogProbMetric: 273.9488 - val_loss: 273.2410 - val_MinusLogProbMetric: 273.2410 - lr: 1.2346e-05 - 79s/epoch - 403ms/step
Epoch 82/1000
2023-10-25 11:59:17.367 
Epoch 82/1000 
	 loss: 271.2826, MinusLogProbMetric: 271.2826, val_loss: 270.4136, val_MinusLogProbMetric: 270.4136

Epoch 82: val_loss improved from 273.24103 to 270.41360, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 271.2826 - MinusLogProbMetric: 271.2826 - val_loss: 270.4136 - val_MinusLogProbMetric: 270.4136 - lr: 1.2346e-05 - 80s/epoch - 408ms/step
Epoch 83/1000
2023-10-25 12:00:36.405 
Epoch 83/1000 
	 loss: 269.9575, MinusLogProbMetric: 269.9575, val_loss: 270.8408, val_MinusLogProbMetric: 270.8408

Epoch 83: val_loss did not improve from 270.41360
196/196 - 78s - loss: 269.9575 - MinusLogProbMetric: 269.9575 - val_loss: 270.8408 - val_MinusLogProbMetric: 270.8408 - lr: 1.2346e-05 - 78s/epoch - 397ms/step
Epoch 84/1000
2023-10-25 12:01:54.642 
Epoch 84/1000 
	 loss: 266.9482, MinusLogProbMetric: 266.9482, val_loss: 266.0518, val_MinusLogProbMetric: 266.0518

Epoch 84: val_loss improved from 270.41360 to 266.05176, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 266.9482 - MinusLogProbMetric: 266.9482 - val_loss: 266.0518 - val_MinusLogProbMetric: 266.0518 - lr: 1.2346e-05 - 79s/epoch - 405ms/step
Epoch 85/1000
2023-10-25 12:03:13.273 
Epoch 85/1000 
	 loss: 264.0806, MinusLogProbMetric: 264.0806, val_loss: 263.0602, val_MinusLogProbMetric: 263.0602

Epoch 85: val_loss improved from 266.05176 to 263.06015, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 264.0806 - MinusLogProbMetric: 264.0806 - val_loss: 263.0602 - val_MinusLogProbMetric: 263.0602 - lr: 1.2346e-05 - 79s/epoch - 402ms/step
Epoch 86/1000
2023-10-25 12:04:32.514 
Epoch 86/1000 
	 loss: 264.0582, MinusLogProbMetric: 264.0582, val_loss: 260.9586, val_MinusLogProbMetric: 260.9586

Epoch 86: val_loss improved from 263.06015 to 260.95856, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 264.0582 - MinusLogProbMetric: 264.0582 - val_loss: 260.9586 - val_MinusLogProbMetric: 260.9586 - lr: 1.2346e-05 - 79s/epoch - 404ms/step
Epoch 87/1000
2023-10-25 12:05:49.839 
Epoch 87/1000 
	 loss: 260.0372, MinusLogProbMetric: 260.0372, val_loss: 258.9162, val_MinusLogProbMetric: 258.9162

Epoch 87: val_loss improved from 260.95856 to 258.91623, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 260.0372 - MinusLogProbMetric: 260.0372 - val_loss: 258.9162 - val_MinusLogProbMetric: 258.9162 - lr: 1.2346e-05 - 77s/epoch - 395ms/step
Epoch 88/1000
2023-10-25 12:07:08.221 
Epoch 88/1000 
	 loss: 257.9740, MinusLogProbMetric: 257.9740, val_loss: 257.1896, val_MinusLogProbMetric: 257.1896

Epoch 88: val_loss improved from 258.91623 to 257.18961, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 257.9740 - MinusLogProbMetric: 257.9740 - val_loss: 257.1896 - val_MinusLogProbMetric: 257.1896 - lr: 1.2346e-05 - 78s/epoch - 399ms/step
Epoch 89/1000
2023-10-25 12:08:25.901 
Epoch 89/1000 
	 loss: 269.1608, MinusLogProbMetric: 269.1608, val_loss: 259.5065, val_MinusLogProbMetric: 259.5065

Epoch 89: val_loss did not improve from 257.18961
196/196 - 77s - loss: 269.1608 - MinusLogProbMetric: 269.1608 - val_loss: 259.5065 - val_MinusLogProbMetric: 259.5065 - lr: 1.2346e-05 - 77s/epoch - 390ms/step
Epoch 90/1000
2023-10-25 12:09:43.623 
Epoch 90/1000 
	 loss: 257.3336, MinusLogProbMetric: 257.3336, val_loss: 257.1823, val_MinusLogProbMetric: 257.1823

Epoch 90: val_loss improved from 257.18961 to 257.18228, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 257.3336 - MinusLogProbMetric: 257.3336 - val_loss: 257.1823 - val_MinusLogProbMetric: 257.1823 - lr: 1.2346e-05 - 79s/epoch - 402ms/step
Epoch 91/1000
2023-10-25 12:11:00.817 
Epoch 91/1000 
	 loss: 255.5704, MinusLogProbMetric: 255.5704, val_loss: 253.8425, val_MinusLogProbMetric: 253.8425

Epoch 91: val_loss improved from 257.18228 to 253.84253, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 255.5704 - MinusLogProbMetric: 255.5704 - val_loss: 253.8425 - val_MinusLogProbMetric: 253.8425 - lr: 1.2346e-05 - 77s/epoch - 395ms/step
Epoch 92/1000
2023-10-25 12:12:20.422 
Epoch 92/1000 
	 loss: 252.6124, MinusLogProbMetric: 252.6124, val_loss: 252.2621, val_MinusLogProbMetric: 252.2621

Epoch 92: val_loss improved from 253.84253 to 252.26208, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 252.6124 - MinusLogProbMetric: 252.6124 - val_loss: 252.2621 - val_MinusLogProbMetric: 252.2621 - lr: 1.2346e-05 - 79s/epoch - 405ms/step
Epoch 93/1000
2023-10-25 12:13:37.827 
Epoch 93/1000 
	 loss: 251.8442, MinusLogProbMetric: 251.8442, val_loss: 251.6622, val_MinusLogProbMetric: 251.6622

Epoch 93: val_loss improved from 252.26208 to 251.66225, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 251.8442 - MinusLogProbMetric: 251.8442 - val_loss: 251.6622 - val_MinusLogProbMetric: 251.6622 - lr: 1.2346e-05 - 77s/epoch - 395ms/step
Epoch 94/1000
2023-10-25 12:14:56.732 
Epoch 94/1000 
	 loss: 249.6391, MinusLogProbMetric: 249.6391, val_loss: 248.8356, val_MinusLogProbMetric: 248.8356

Epoch 94: val_loss improved from 251.66225 to 248.83559, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 249.6391 - MinusLogProbMetric: 249.6391 - val_loss: 248.8356 - val_MinusLogProbMetric: 248.8356 - lr: 1.2346e-05 - 79s/epoch - 403ms/step
Epoch 95/1000
2023-10-25 12:16:16.339 
Epoch 95/1000 
	 loss: 247.6998, MinusLogProbMetric: 247.6998, val_loss: 246.9702, val_MinusLogProbMetric: 246.9702

Epoch 95: val_loss improved from 248.83559 to 246.97017, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 247.6998 - MinusLogProbMetric: 247.6998 - val_loss: 246.9702 - val_MinusLogProbMetric: 246.9702 - lr: 1.2346e-05 - 80s/epoch - 406ms/step
Epoch 96/1000
2023-10-25 12:17:34.783 
Epoch 96/1000 
	 loss: 246.7334, MinusLogProbMetric: 246.7334, val_loss: 246.8996, val_MinusLogProbMetric: 246.8996

Epoch 96: val_loss improved from 246.97017 to 246.89961, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 246.7334 - MinusLogProbMetric: 246.7334 - val_loss: 246.8996 - val_MinusLogProbMetric: 246.8996 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 97/1000
2023-10-25 12:18:52.666 
Epoch 97/1000 
	 loss: 245.0407, MinusLogProbMetric: 245.0407, val_loss: 244.6297, val_MinusLogProbMetric: 244.6297

Epoch 97: val_loss improved from 246.89961 to 244.62973, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 245.0407 - MinusLogProbMetric: 245.0407 - val_loss: 244.6297 - val_MinusLogProbMetric: 244.6297 - lr: 1.2346e-05 - 78s/epoch - 397ms/step
Epoch 98/1000
2023-10-25 12:20:09.874 
Epoch 98/1000 
	 loss: 243.3912, MinusLogProbMetric: 243.3912, val_loss: 242.3121, val_MinusLogProbMetric: 242.3121

Epoch 98: val_loss improved from 244.62973 to 242.31207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 243.3912 - MinusLogProbMetric: 243.3912 - val_loss: 242.3121 - val_MinusLogProbMetric: 242.3121 - lr: 1.2346e-05 - 77s/epoch - 394ms/step
Epoch 99/1000
2023-10-25 12:21:29.842 
Epoch 99/1000 
	 loss: 241.5451, MinusLogProbMetric: 241.5451, val_loss: 241.1655, val_MinusLogProbMetric: 241.1655

Epoch 99: val_loss improved from 242.31207 to 241.16550, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 241.5451 - MinusLogProbMetric: 241.5451 - val_loss: 241.1655 - val_MinusLogProbMetric: 241.1655 - lr: 1.2346e-05 - 80s/epoch - 408ms/step
Epoch 100/1000
2023-10-25 12:22:48.775 
Epoch 100/1000 
	 loss: 240.1378, MinusLogProbMetric: 240.1378, val_loss: 239.8144, val_MinusLogProbMetric: 239.8144

Epoch 100: val_loss improved from 241.16550 to 239.81436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 240.1378 - MinusLogProbMetric: 240.1378 - val_loss: 239.8144 - val_MinusLogProbMetric: 239.8144 - lr: 1.2346e-05 - 79s/epoch - 404ms/step
Epoch 101/1000
2023-10-25 12:24:07.687 
Epoch 101/1000 
	 loss: 238.8254, MinusLogProbMetric: 238.8254, val_loss: 238.1085, val_MinusLogProbMetric: 238.1085

Epoch 101: val_loss improved from 239.81436 to 238.10851, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 238.8254 - MinusLogProbMetric: 238.8254 - val_loss: 238.1085 - val_MinusLogProbMetric: 238.1085 - lr: 1.2346e-05 - 79s/epoch - 402ms/step
Epoch 102/1000
2023-10-25 12:25:24.046 
Epoch 102/1000 
	 loss: 238.0894, MinusLogProbMetric: 238.0894, val_loss: 237.9509, val_MinusLogProbMetric: 237.9509

Epoch 102: val_loss improved from 238.10851 to 237.95087, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 76s - loss: 238.0894 - MinusLogProbMetric: 238.0894 - val_loss: 237.9509 - val_MinusLogProbMetric: 237.9509 - lr: 1.2346e-05 - 76s/epoch - 390ms/step
Epoch 103/1000
2023-10-25 12:26:44.516 
Epoch 103/1000 
	 loss: 238.0896, MinusLogProbMetric: 238.0896, val_loss: 244.3122, val_MinusLogProbMetric: 244.3122

Epoch 103: val_loss did not improve from 237.95087
196/196 - 79s - loss: 238.0896 - MinusLogProbMetric: 238.0896 - val_loss: 244.3122 - val_MinusLogProbMetric: 244.3122 - lr: 1.2346e-05 - 79s/epoch - 404ms/step
Epoch 104/1000
2023-10-25 12:28:00.383 
Epoch 104/1000 
	 loss: 236.0901, MinusLogProbMetric: 236.0901, val_loss: 235.0978, val_MinusLogProbMetric: 235.0978

Epoch 104: val_loss improved from 237.95087 to 235.09778, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 236.0901 - MinusLogProbMetric: 236.0901 - val_loss: 235.0978 - val_MinusLogProbMetric: 235.0978 - lr: 1.2346e-05 - 77s/epoch - 393ms/step
Epoch 105/1000
2023-10-25 12:29:18.219 
Epoch 105/1000 
	 loss: 244.8697, MinusLogProbMetric: 244.8697, val_loss: 243.8820, val_MinusLogProbMetric: 243.8820

Epoch 105: val_loss did not improve from 235.09778
196/196 - 77s - loss: 244.8697 - MinusLogProbMetric: 244.8697 - val_loss: 243.8820 - val_MinusLogProbMetric: 243.8820 - lr: 1.2346e-05 - 77s/epoch - 391ms/step
Epoch 106/1000
2023-10-25 12:30:38.274 
Epoch 106/1000 
	 loss: 236.9038, MinusLogProbMetric: 236.9038, val_loss: 234.5998, val_MinusLogProbMetric: 234.5998

Epoch 106: val_loss improved from 235.09778 to 234.59979, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 81s - loss: 236.9038 - MinusLogProbMetric: 236.9038 - val_loss: 234.5998 - val_MinusLogProbMetric: 234.5998 - lr: 1.2346e-05 - 81s/epoch - 414ms/step
Epoch 107/1000
2023-10-25 12:31:57.927 
Epoch 107/1000 
	 loss: 233.2940, MinusLogProbMetric: 233.2940, val_loss: 232.0888, val_MinusLogProbMetric: 232.0888

Epoch 107: val_loss improved from 234.59979 to 232.08879, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 233.2940 - MinusLogProbMetric: 233.2940 - val_loss: 232.0888 - val_MinusLogProbMetric: 232.0888 - lr: 1.2346e-05 - 80s/epoch - 407ms/step
Epoch 108/1000
2023-10-25 12:33:16.064 
Epoch 108/1000 
	 loss: 231.1246, MinusLogProbMetric: 231.1246, val_loss: 230.5102, val_MinusLogProbMetric: 230.5102

Epoch 108: val_loss improved from 232.08879 to 230.51019, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 231.1246 - MinusLogProbMetric: 231.1246 - val_loss: 230.5102 - val_MinusLogProbMetric: 230.5102 - lr: 1.2346e-05 - 78s/epoch - 399ms/step
Epoch 109/1000
2023-10-25 12:34:31.755 
Epoch 109/1000 
	 loss: 230.2511, MinusLogProbMetric: 230.2511, val_loss: 229.4956, val_MinusLogProbMetric: 229.4956

Epoch 109: val_loss improved from 230.51019 to 229.49562, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 76s - loss: 230.2511 - MinusLogProbMetric: 230.2511 - val_loss: 229.4956 - val_MinusLogProbMetric: 229.4956 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 110/1000
2023-10-25 12:35:50.848 
Epoch 110/1000 
	 loss: 228.9770, MinusLogProbMetric: 228.9770, val_loss: 228.3363, val_MinusLogProbMetric: 228.3363

Epoch 110: val_loss improved from 229.49562 to 228.33635, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 228.9770 - MinusLogProbMetric: 228.9770 - val_loss: 228.3363 - val_MinusLogProbMetric: 228.3363 - lr: 1.2346e-05 - 79s/epoch - 404ms/step
Epoch 111/1000
2023-10-25 12:37:03.229 
Epoch 111/1000 
	 loss: 228.0444, MinusLogProbMetric: 228.0444, val_loss: 227.4435, val_MinusLogProbMetric: 227.4435

Epoch 111: val_loss improved from 228.33635 to 227.44348, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 72s - loss: 228.0444 - MinusLogProbMetric: 228.0444 - val_loss: 227.4435 - val_MinusLogProbMetric: 227.4435 - lr: 1.2346e-05 - 72s/epoch - 368ms/step
Epoch 112/1000
2023-10-25 12:38:07.227 
Epoch 112/1000 
	 loss: 227.4104, MinusLogProbMetric: 227.4104, val_loss: 226.9189, val_MinusLogProbMetric: 226.9189

Epoch 112: val_loss improved from 227.44348 to 226.91895, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 64s - loss: 227.4104 - MinusLogProbMetric: 227.4104 - val_loss: 226.9189 - val_MinusLogProbMetric: 226.9189 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 113/1000
2023-10-25 12:39:15.322 
Epoch 113/1000 
	 loss: 226.9366, MinusLogProbMetric: 226.9366, val_loss: 226.5217, val_MinusLogProbMetric: 226.5217

Epoch 113: val_loss improved from 226.91895 to 226.52174, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 68s - loss: 226.9366 - MinusLogProbMetric: 226.9366 - val_loss: 226.5217 - val_MinusLogProbMetric: 226.5217 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 114/1000
2023-10-25 12:40:22.945 
Epoch 114/1000 
	 loss: 225.5030, MinusLogProbMetric: 225.5030, val_loss: 224.8895, val_MinusLogProbMetric: 224.8895

Epoch 114: val_loss improved from 226.52174 to 224.88945, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 68s - loss: 225.5030 - MinusLogProbMetric: 225.5030 - val_loss: 224.8895 - val_MinusLogProbMetric: 224.8895 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 115/1000
2023-10-25 12:41:30.712 
Epoch 115/1000 
	 loss: 224.3955, MinusLogProbMetric: 224.3955, val_loss: 224.4561, val_MinusLogProbMetric: 224.4561

Epoch 115: val_loss improved from 224.88945 to 224.45610, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 68s - loss: 224.3955 - MinusLogProbMetric: 224.3955 - val_loss: 224.4561 - val_MinusLogProbMetric: 224.4561 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 116/1000
2023-10-25 12:42:37.330 
Epoch 116/1000 
	 loss: 223.6723, MinusLogProbMetric: 223.6723, val_loss: 222.9860, val_MinusLogProbMetric: 222.9860

Epoch 116: val_loss improved from 224.45610 to 222.98604, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 67s - loss: 223.6723 - MinusLogProbMetric: 223.6723 - val_loss: 222.9860 - val_MinusLogProbMetric: 222.9860 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 117/1000
2023-10-25 12:43:43.952 
Epoch 117/1000 
	 loss: 222.1636, MinusLogProbMetric: 222.1636, val_loss: 221.7139, val_MinusLogProbMetric: 221.7139

Epoch 117: val_loss improved from 222.98604 to 221.71388, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 66s - loss: 222.1636 - MinusLogProbMetric: 222.1636 - val_loss: 221.7139 - val_MinusLogProbMetric: 221.7139 - lr: 1.2346e-05 - 66s/epoch - 339ms/step
Epoch 118/1000
2023-10-25 12:44:51.291 
Epoch 118/1000 
	 loss: 221.3159, MinusLogProbMetric: 221.3159, val_loss: 221.0014, val_MinusLogProbMetric: 221.0014

Epoch 118: val_loss improved from 221.71388 to 221.00140, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 67s - loss: 221.3159 - MinusLogProbMetric: 221.3159 - val_loss: 221.0014 - val_MinusLogProbMetric: 221.0014 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 119/1000
2023-10-25 12:45:55.322 
Epoch 119/1000 
	 loss: 220.2305, MinusLogProbMetric: 220.2305, val_loss: 220.3180, val_MinusLogProbMetric: 220.3180

Epoch 119: val_loss improved from 221.00140 to 220.31802, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 64s - loss: 220.2305 - MinusLogProbMetric: 220.2305 - val_loss: 220.3180 - val_MinusLogProbMetric: 220.3180 - lr: 1.2346e-05 - 64s/epoch - 328ms/step
Epoch 120/1000
2023-10-25 12:47:00.280 
Epoch 120/1000 
	 loss: 220.1071, MinusLogProbMetric: 220.1071, val_loss: 219.1550, val_MinusLogProbMetric: 219.1550

Epoch 120: val_loss improved from 220.31802 to 219.15501, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 65s - loss: 220.1071 - MinusLogProbMetric: 220.1071 - val_loss: 219.1550 - val_MinusLogProbMetric: 219.1550 - lr: 1.2346e-05 - 65s/epoch - 330ms/step
Epoch 121/1000
2023-10-25 12:48:06.021 
Epoch 121/1000 
	 loss: 219.1235, MinusLogProbMetric: 219.1235, val_loss: 218.5380, val_MinusLogProbMetric: 218.5380

Epoch 121: val_loss improved from 219.15501 to 218.53799, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 66s - loss: 219.1235 - MinusLogProbMetric: 219.1235 - val_loss: 218.5380 - val_MinusLogProbMetric: 218.5380 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 122/1000
2023-10-25 12:49:23.474 
Epoch 122/1000 
	 loss: 217.8720, MinusLogProbMetric: 217.8720, val_loss: 217.6147, val_MinusLogProbMetric: 217.6147

Epoch 122: val_loss improved from 218.53799 to 217.61465, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 217.8720 - MinusLogProbMetric: 217.8720 - val_loss: 217.6147 - val_MinusLogProbMetric: 217.6147 - lr: 1.2346e-05 - 78s/epoch - 396ms/step
Epoch 123/1000
2023-10-25 12:50:42.449 
Epoch 123/1000 
	 loss: 217.1067, MinusLogProbMetric: 217.1067, val_loss: 217.8103, val_MinusLogProbMetric: 217.8103

Epoch 123: val_loss did not improve from 217.61465
196/196 - 78s - loss: 217.1067 - MinusLogProbMetric: 217.1067 - val_loss: 217.8103 - val_MinusLogProbMetric: 217.8103 - lr: 1.2346e-05 - 78s/epoch - 396ms/step
Epoch 124/1000
2023-10-25 12:51:59.490 
Epoch 124/1000 
	 loss: 216.7276, MinusLogProbMetric: 216.7276, val_loss: 215.7625, val_MinusLogProbMetric: 215.7625

Epoch 124: val_loss improved from 217.61465 to 215.76253, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 216.7276 - MinusLogProbMetric: 216.7276 - val_loss: 215.7625 - val_MinusLogProbMetric: 215.7625 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 125/1000
2023-10-25 12:53:18.786 
Epoch 125/1000 
	 loss: 215.5182, MinusLogProbMetric: 215.5182, val_loss: 215.2301, val_MinusLogProbMetric: 215.2301

Epoch 125: val_loss improved from 215.76253 to 215.23010, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 215.5182 - MinusLogProbMetric: 215.5182 - val_loss: 215.2301 - val_MinusLogProbMetric: 215.2301 - lr: 1.2346e-05 - 79s/epoch - 405ms/step
Epoch 126/1000
2023-10-25 12:54:32.082 
Epoch 126/1000 
	 loss: 214.6442, MinusLogProbMetric: 214.6442, val_loss: 214.3708, val_MinusLogProbMetric: 214.3708

Epoch 126: val_loss improved from 215.23010 to 214.37082, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 73s - loss: 214.6442 - MinusLogProbMetric: 214.6442 - val_loss: 214.3708 - val_MinusLogProbMetric: 214.3708 - lr: 1.2346e-05 - 73s/epoch - 372ms/step
Epoch 127/1000
2023-10-25 12:55:48.871 
Epoch 127/1000 
	 loss: 213.8304, MinusLogProbMetric: 213.8304, val_loss: 213.4073, val_MinusLogProbMetric: 213.4073

Epoch 127: val_loss improved from 214.37082 to 213.40726, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 213.8304 - MinusLogProbMetric: 213.8304 - val_loss: 213.4073 - val_MinusLogProbMetric: 213.4073 - lr: 1.2346e-05 - 77s/epoch - 393ms/step
Epoch 128/1000
2023-10-25 12:57:03.467 
Epoch 128/1000 
	 loss: 213.2738, MinusLogProbMetric: 213.2738, val_loss: 213.0078, val_MinusLogProbMetric: 213.0078

Epoch 128: val_loss improved from 213.40726 to 213.00780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 74s - loss: 213.2738 - MinusLogProbMetric: 213.2738 - val_loss: 213.0078 - val_MinusLogProbMetric: 213.0078 - lr: 1.2346e-05 - 74s/epoch - 378ms/step
Epoch 129/1000
2023-10-25 12:58:18.581 
Epoch 129/1000 
	 loss: 214.2835, MinusLogProbMetric: 214.2835, val_loss: 214.5901, val_MinusLogProbMetric: 214.5901

Epoch 129: val_loss did not improve from 213.00780
196/196 - 74s - loss: 214.2835 - MinusLogProbMetric: 214.2835 - val_loss: 214.5901 - val_MinusLogProbMetric: 214.5901 - lr: 1.2346e-05 - 74s/epoch - 379ms/step
Epoch 130/1000
2023-10-25 12:59:34.326 
Epoch 130/1000 
	 loss: 213.3729, MinusLogProbMetric: 213.3729, val_loss: 212.3462, val_MinusLogProbMetric: 212.3462

Epoch 130: val_loss improved from 213.00780 to 212.34616, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 213.3729 - MinusLogProbMetric: 213.3729 - val_loss: 212.3462 - val_MinusLogProbMetric: 212.3462 - lr: 1.2346e-05 - 77s/epoch - 393ms/step
Epoch 131/1000
2023-10-25 13:00:50.694 
Epoch 131/1000 
	 loss: 214.1111, MinusLogProbMetric: 214.1111, val_loss: 219.5168, val_MinusLogProbMetric: 219.5168

Epoch 131: val_loss did not improve from 212.34616
196/196 - 75s - loss: 214.1111 - MinusLogProbMetric: 214.1111 - val_loss: 219.5168 - val_MinusLogProbMetric: 219.5168 - lr: 1.2346e-05 - 75s/epoch - 383ms/step
Epoch 132/1000
2023-10-25 13:02:04.054 
Epoch 132/1000 
	 loss: 221.7456, MinusLogProbMetric: 221.7456, val_loss: 219.9699, val_MinusLogProbMetric: 219.9699

Epoch 132: val_loss did not improve from 212.34616
196/196 - 73s - loss: 221.7456 - MinusLogProbMetric: 221.7456 - val_loss: 219.9699 - val_MinusLogProbMetric: 219.9699 - lr: 1.2346e-05 - 73s/epoch - 374ms/step
Epoch 133/1000
2023-10-25 13:03:21.522 
Epoch 133/1000 
	 loss: 220.6179, MinusLogProbMetric: 220.6179, val_loss: 210.9422, val_MinusLogProbMetric: 210.9422

Epoch 133: val_loss improved from 212.34616 to 210.94218, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 220.6179 - MinusLogProbMetric: 220.6179 - val_loss: 210.9422 - val_MinusLogProbMetric: 210.9422 - lr: 1.2346e-05 - 79s/epoch - 402ms/step
Epoch 134/1000
2023-10-25 13:04:39.101 
Epoch 134/1000 
	 loss: 209.6769, MinusLogProbMetric: 209.6769, val_loss: 208.9272, val_MinusLogProbMetric: 208.9272

Epoch 134: val_loss improved from 210.94218 to 208.92720, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 209.6769 - MinusLogProbMetric: 209.6769 - val_loss: 208.9272 - val_MinusLogProbMetric: 208.9272 - lr: 1.2346e-05 - 77s/epoch - 395ms/step
Epoch 135/1000
2023-10-25 13:05:58.091 
Epoch 135/1000 
	 loss: 210.1770, MinusLogProbMetric: 210.1770, val_loss: 213.2185, val_MinusLogProbMetric: 213.2185

Epoch 135: val_loss did not improve from 208.92720
196/196 - 78s - loss: 210.1770 - MinusLogProbMetric: 210.1770 - val_loss: 213.2185 - val_MinusLogProbMetric: 213.2185 - lr: 1.2346e-05 - 78s/epoch - 398ms/step
Epoch 136/1000
2023-10-25 13:07:16.684 
Epoch 136/1000 
	 loss: 208.5677, MinusLogProbMetric: 208.5677, val_loss: 207.6418, val_MinusLogProbMetric: 207.6418

Epoch 136: val_loss improved from 208.92720 to 207.64178, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 208.5677 - MinusLogProbMetric: 208.5677 - val_loss: 207.6418 - val_MinusLogProbMetric: 207.6418 - lr: 1.2346e-05 - 80s/epoch - 407ms/step
Epoch 137/1000
2023-10-25 13:08:32.897 
Epoch 137/1000 
	 loss: 207.5344, MinusLogProbMetric: 207.5344, val_loss: 207.7145, val_MinusLogProbMetric: 207.7145

Epoch 137: val_loss did not improve from 207.64178
196/196 - 75s - loss: 207.5344 - MinusLogProbMetric: 207.5344 - val_loss: 207.7145 - val_MinusLogProbMetric: 207.7145 - lr: 1.2346e-05 - 75s/epoch - 382ms/step
Epoch 138/1000
2023-10-25 13:09:51.058 
Epoch 138/1000 
	 loss: 206.3026, MinusLogProbMetric: 206.3026, val_loss: 205.7966, val_MinusLogProbMetric: 205.7966

Epoch 138: val_loss improved from 207.64178 to 205.79665, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 206.3026 - MinusLogProbMetric: 206.3026 - val_loss: 205.7966 - val_MinusLogProbMetric: 205.7966 - lr: 1.2346e-05 - 79s/epoch - 404ms/step
Epoch 139/1000
2023-10-25 13:11:06.615 
Epoch 139/1000 
	 loss: 205.5124, MinusLogProbMetric: 205.5124, val_loss: 205.4091, val_MinusLogProbMetric: 205.4091

Epoch 139: val_loss improved from 205.79665 to 205.40906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 76s - loss: 205.5124 - MinusLogProbMetric: 205.5124 - val_loss: 205.4091 - val_MinusLogProbMetric: 205.4091 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 140/1000
2023-10-25 13:12:24.028 
Epoch 140/1000 
	 loss: 204.7466, MinusLogProbMetric: 204.7466, val_loss: 204.3919, val_MinusLogProbMetric: 204.3919

Epoch 140: val_loss improved from 205.40906 to 204.39191, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 204.7466 - MinusLogProbMetric: 204.7466 - val_loss: 204.3919 - val_MinusLogProbMetric: 204.3919 - lr: 1.2346e-05 - 77s/epoch - 395ms/step
Epoch 141/1000
2023-10-25 13:13:39.353 
Epoch 141/1000 
	 loss: 203.9419, MinusLogProbMetric: 203.9419, val_loss: 203.9542, val_MinusLogProbMetric: 203.9542

Epoch 141: val_loss improved from 204.39191 to 203.95419, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 75s - loss: 203.9419 - MinusLogProbMetric: 203.9419 - val_loss: 203.9542 - val_MinusLogProbMetric: 203.9542 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 142/1000
2023-10-25 13:14:51.879 
Epoch 142/1000 
	 loss: 203.2539, MinusLogProbMetric: 203.2539, val_loss: 203.0553, val_MinusLogProbMetric: 203.0553

Epoch 142: val_loss improved from 203.95419 to 203.05533, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 73s - loss: 203.2539 - MinusLogProbMetric: 203.2539 - val_loss: 203.0553 - val_MinusLogProbMetric: 203.0553 - lr: 1.2346e-05 - 73s/epoch - 370ms/step
Epoch 143/1000
2023-10-25 13:16:07.936 
Epoch 143/1000 
	 loss: 203.2840, MinusLogProbMetric: 203.2840, val_loss: 202.2305, val_MinusLogProbMetric: 202.2305

Epoch 143: val_loss improved from 203.05533 to 202.23053, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 76s - loss: 203.2840 - MinusLogProbMetric: 203.2840 - val_loss: 202.2305 - val_MinusLogProbMetric: 202.2305 - lr: 1.2346e-05 - 76s/epoch - 387ms/step
Epoch 144/1000
2023-10-25 13:17:21.379 
Epoch 144/1000 
	 loss: 201.8186, MinusLogProbMetric: 201.8186, val_loss: 202.5843, val_MinusLogProbMetric: 202.5843

Epoch 144: val_loss did not improve from 202.23053
196/196 - 72s - loss: 201.8186 - MinusLogProbMetric: 201.8186 - val_loss: 202.5843 - val_MinusLogProbMetric: 202.5843 - lr: 1.2346e-05 - 72s/epoch - 369ms/step
Epoch 145/1000
2023-10-25 13:18:37.728 
Epoch 145/1000 
	 loss: 201.0940, MinusLogProbMetric: 201.0940, val_loss: 200.9344, val_MinusLogProbMetric: 200.9344

Epoch 145: val_loss improved from 202.23053 to 200.93439, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 201.0940 - MinusLogProbMetric: 201.0940 - val_loss: 200.9344 - val_MinusLogProbMetric: 200.9344 - lr: 1.2346e-05 - 77s/epoch - 394ms/step
Epoch 146/1000
2023-10-25 13:19:52.354 
Epoch 146/1000 
	 loss: 200.7036, MinusLogProbMetric: 200.7036, val_loss: 200.2964, val_MinusLogProbMetric: 200.2964

Epoch 146: val_loss improved from 200.93439 to 200.29642, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 75s - loss: 200.7036 - MinusLogProbMetric: 200.7036 - val_loss: 200.2964 - val_MinusLogProbMetric: 200.2964 - lr: 1.2346e-05 - 75s/epoch - 383ms/step
Epoch 147/1000
2023-10-25 13:21:07.976 
Epoch 147/1000 
	 loss: 200.7721, MinusLogProbMetric: 200.7721, val_loss: 199.8827, val_MinusLogProbMetric: 199.8827

Epoch 147: val_loss improved from 200.29642 to 199.88269, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 75s - loss: 200.7721 - MinusLogProbMetric: 200.7721 - val_loss: 199.8827 - val_MinusLogProbMetric: 199.8827 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 148/1000
2023-10-25 13:22:26.392 
Epoch 148/1000 
	 loss: 199.0656, MinusLogProbMetric: 199.0656, val_loss: 219.3069, val_MinusLogProbMetric: 219.3069

Epoch 148: val_loss did not improve from 199.88269
196/196 - 77s - loss: 199.0656 - MinusLogProbMetric: 199.0656 - val_loss: 219.3069 - val_MinusLogProbMetric: 219.3069 - lr: 1.2346e-05 - 77s/epoch - 394ms/step
Epoch 149/1000
2023-10-25 13:23:41.369 
Epoch 149/1000 
	 loss: 203.8788, MinusLogProbMetric: 203.8788, val_loss: 198.8205, val_MinusLogProbMetric: 198.8205

Epoch 149: val_loss improved from 199.88269 to 198.82047, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 76s - loss: 203.8788 - MinusLogProbMetric: 203.8788 - val_loss: 198.8205 - val_MinusLogProbMetric: 198.8205 - lr: 1.2346e-05 - 76s/epoch - 389ms/step
Epoch 150/1000
2023-10-25 13:25:02.922 
Epoch 150/1000 
	 loss: 197.9992, MinusLogProbMetric: 197.9992, val_loss: 197.9294, val_MinusLogProbMetric: 197.9294

Epoch 150: val_loss improved from 198.82047 to 197.92937, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 81s - loss: 197.9992 - MinusLogProbMetric: 197.9992 - val_loss: 197.9294 - val_MinusLogProbMetric: 197.9294 - lr: 1.2346e-05 - 81s/epoch - 415ms/step
Epoch 151/1000
2023-10-25 13:26:19.049 
Epoch 151/1000 
	 loss: 197.4988, MinusLogProbMetric: 197.4988, val_loss: 197.3240, val_MinusLogProbMetric: 197.3240

Epoch 151: val_loss improved from 197.92937 to 197.32401, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 76s - loss: 197.4988 - MinusLogProbMetric: 197.4988 - val_loss: 197.3240 - val_MinusLogProbMetric: 197.3240 - lr: 1.2346e-05 - 76s/epoch - 388ms/step
Epoch 152/1000
2023-10-25 13:27:35.601 
Epoch 152/1000 
	 loss: 198.4564, MinusLogProbMetric: 198.4564, val_loss: 199.3220, val_MinusLogProbMetric: 199.3220

Epoch 152: val_loss did not improve from 197.32401
196/196 - 75s - loss: 198.4564 - MinusLogProbMetric: 198.4564 - val_loss: 199.3220 - val_MinusLogProbMetric: 199.3220 - lr: 1.2346e-05 - 75s/epoch - 385ms/step
Epoch 153/1000
2023-10-25 13:28:49.529 
Epoch 153/1000 
	 loss: 197.2271, MinusLogProbMetric: 197.2271, val_loss: 196.5965, val_MinusLogProbMetric: 196.5965

Epoch 153: val_loss improved from 197.32401 to 196.59648, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 75s - loss: 197.2271 - MinusLogProbMetric: 197.2271 - val_loss: 196.5965 - val_MinusLogProbMetric: 196.5965 - lr: 1.2346e-05 - 75s/epoch - 384ms/step
Epoch 154/1000
2023-10-25 13:30:05.623 
Epoch 154/1000 
	 loss: 196.0641, MinusLogProbMetric: 196.0641, val_loss: 195.9135, val_MinusLogProbMetric: 195.9135

Epoch 154: val_loss improved from 196.59648 to 195.91347, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 76s - loss: 196.0641 - MinusLogProbMetric: 196.0641 - val_loss: 195.9135 - val_MinusLogProbMetric: 195.9135 - lr: 1.2346e-05 - 76s/epoch - 387ms/step
Epoch 155/1000
2023-10-25 13:31:19.879 
Epoch 155/1000 
	 loss: 195.1247, MinusLogProbMetric: 195.1247, val_loss: 195.3103, val_MinusLogProbMetric: 195.3103

Epoch 155: val_loss improved from 195.91347 to 195.31026, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 74s - loss: 195.1247 - MinusLogProbMetric: 195.1247 - val_loss: 195.3103 - val_MinusLogProbMetric: 195.3103 - lr: 1.2346e-05 - 74s/epoch - 379ms/step
Epoch 156/1000
2023-10-25 13:32:34.354 
Epoch 156/1000 
	 loss: 201.2468, MinusLogProbMetric: 201.2468, val_loss: 200.1380, val_MinusLogProbMetric: 200.1380

Epoch 156: val_loss did not improve from 195.31026
196/196 - 73s - loss: 201.2468 - MinusLogProbMetric: 201.2468 - val_loss: 200.1380 - val_MinusLogProbMetric: 200.1380 - lr: 1.2346e-05 - 73s/epoch - 374ms/step
Epoch 157/1000
2023-10-25 13:33:47.154 
Epoch 157/1000 
	 loss: 195.5853, MinusLogProbMetric: 195.5853, val_loss: 195.4064, val_MinusLogProbMetric: 195.4064

Epoch 157: val_loss did not improve from 195.31026
196/196 - 73s - loss: 195.5853 - MinusLogProbMetric: 195.5853 - val_loss: 195.4064 - val_MinusLogProbMetric: 195.4064 - lr: 1.2346e-05 - 73s/epoch - 371ms/step
Epoch 158/1000
2023-10-25 13:35:05.220 
Epoch 158/1000 
	 loss: 194.2491, MinusLogProbMetric: 194.2491, val_loss: 193.5891, val_MinusLogProbMetric: 193.5891

Epoch 158: val_loss improved from 195.31026 to 193.58913, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 194.2491 - MinusLogProbMetric: 194.2491 - val_loss: 193.5891 - val_MinusLogProbMetric: 193.5891 - lr: 1.2346e-05 - 79s/epoch - 404ms/step
Epoch 159/1000
2023-10-25 13:36:21.833 
Epoch 159/1000 
	 loss: 192.7339, MinusLogProbMetric: 192.7339, val_loss: 192.4713, val_MinusLogProbMetric: 192.4713

Epoch 159: val_loss improved from 193.58913 to 192.47125, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 192.7339 - MinusLogProbMetric: 192.7339 - val_loss: 192.4713 - val_MinusLogProbMetric: 192.4713 - lr: 1.2346e-05 - 77s/epoch - 391ms/step
Epoch 160/1000
2023-10-25 13:37:38.583 
Epoch 160/1000 
	 loss: 191.8538, MinusLogProbMetric: 191.8538, val_loss: 191.5627, val_MinusLogProbMetric: 191.5627

Epoch 160: val_loss improved from 192.47125 to 191.56274, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 191.8538 - MinusLogProbMetric: 191.8538 - val_loss: 191.5627 - val_MinusLogProbMetric: 191.5627 - lr: 1.2346e-05 - 77s/epoch - 393ms/step
Epoch 161/1000
2023-10-25 13:38:53.904 
Epoch 161/1000 
	 loss: 191.0419, MinusLogProbMetric: 191.0419, val_loss: 191.1026, val_MinusLogProbMetric: 191.1026

Epoch 161: val_loss improved from 191.56274 to 191.10260, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 75s - loss: 191.0419 - MinusLogProbMetric: 191.0419 - val_loss: 191.1026 - val_MinusLogProbMetric: 191.1026 - lr: 1.2346e-05 - 75s/epoch - 383ms/step
Epoch 162/1000
2023-10-25 13:40:13.601 
Epoch 162/1000 
	 loss: 190.6922, MinusLogProbMetric: 190.6922, val_loss: 193.1843, val_MinusLogProbMetric: 193.1843

Epoch 162: val_loss did not improve from 191.10260
196/196 - 78s - loss: 190.6922 - MinusLogProbMetric: 190.6922 - val_loss: 193.1843 - val_MinusLogProbMetric: 193.1843 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 163/1000
2023-10-25 13:41:28.080 
Epoch 163/1000 
	 loss: 193.2898, MinusLogProbMetric: 193.2898, val_loss: 190.1112, val_MinusLogProbMetric: 190.1112

Epoch 163: val_loss improved from 191.10260 to 190.11122, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 76s - loss: 193.2898 - MinusLogProbMetric: 193.2898 - val_loss: 190.1112 - val_MinusLogProbMetric: 190.1112 - lr: 1.2346e-05 - 76s/epoch - 385ms/step
Epoch 164/1000
2023-10-25 13:42:45.900 
Epoch 164/1000 
	 loss: 189.3564, MinusLogProbMetric: 189.3564, val_loss: 189.1032, val_MinusLogProbMetric: 189.1032

Epoch 164: val_loss improved from 190.11122 to 189.10323, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 189.3564 - MinusLogProbMetric: 189.3564 - val_loss: 189.1032 - val_MinusLogProbMetric: 189.1032 - lr: 1.2346e-05 - 78s/epoch - 398ms/step
Epoch 165/1000
2023-10-25 13:44:02.736 
Epoch 165/1000 
	 loss: 190.6676, MinusLogProbMetric: 190.6676, val_loss: 189.4972, val_MinusLogProbMetric: 189.4972

Epoch 165: val_loss did not improve from 189.10323
196/196 - 76s - loss: 190.6676 - MinusLogProbMetric: 190.6676 - val_loss: 189.4972 - val_MinusLogProbMetric: 189.4972 - lr: 1.2346e-05 - 76s/epoch - 386ms/step
Epoch 166/1000
2023-10-25 13:45:21.124 
Epoch 166/1000 
	 loss: 188.6945, MinusLogProbMetric: 188.6945, val_loss: 188.5451, val_MinusLogProbMetric: 188.5451

Epoch 166: val_loss improved from 189.10323 to 188.54509, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 188.6945 - MinusLogProbMetric: 188.6945 - val_loss: 188.5451 - val_MinusLogProbMetric: 188.5451 - lr: 1.2346e-05 - 80s/epoch - 406ms/step
Epoch 167/1000
2023-10-25 13:46:38.365 
Epoch 167/1000 
	 loss: 188.2843, MinusLogProbMetric: 188.2843, val_loss: 188.1942, val_MinusLogProbMetric: 188.1942

Epoch 167: val_loss improved from 188.54509 to 188.19421, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 188.2843 - MinusLogProbMetric: 188.2843 - val_loss: 188.1942 - val_MinusLogProbMetric: 188.1942 - lr: 1.2346e-05 - 77s/epoch - 394ms/step
Epoch 168/1000
2023-10-25 13:47:57.129 
Epoch 168/1000 
	 loss: 187.5288, MinusLogProbMetric: 187.5288, val_loss: 187.2563, val_MinusLogProbMetric: 187.2563

Epoch 168: val_loss improved from 188.19421 to 187.25627, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 187.5288 - MinusLogProbMetric: 187.5288 - val_loss: 187.2563 - val_MinusLogProbMetric: 187.2563 - lr: 1.2346e-05 - 79s/epoch - 402ms/step
Epoch 169/1000
2023-10-25 13:49:16.163 
Epoch 169/1000 
	 loss: 186.8625, MinusLogProbMetric: 186.8625, val_loss: 186.8627, val_MinusLogProbMetric: 186.8627

Epoch 169: val_loss improved from 187.25627 to 186.86269, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 186.8625 - MinusLogProbMetric: 186.8625 - val_loss: 186.8627 - val_MinusLogProbMetric: 186.8627 - lr: 1.2346e-05 - 79s/epoch - 403ms/step
Epoch 170/1000
2023-10-25 13:50:34.977 
Epoch 170/1000 
	 loss: 186.2068, MinusLogProbMetric: 186.2068, val_loss: 186.1307, val_MinusLogProbMetric: 186.1307

Epoch 170: val_loss improved from 186.86269 to 186.13066, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 186.2068 - MinusLogProbMetric: 186.2068 - val_loss: 186.1307 - val_MinusLogProbMetric: 186.1307 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 171/1000
2023-10-25 13:51:52.670 
Epoch 171/1000 
	 loss: 186.1828, MinusLogProbMetric: 186.1828, val_loss: 185.7645, val_MinusLogProbMetric: 185.7645

Epoch 171: val_loss improved from 186.13066 to 185.76451, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 186.1828 - MinusLogProbMetric: 186.1828 - val_loss: 185.7645 - val_MinusLogProbMetric: 185.7645 - lr: 1.2346e-05 - 78s/epoch - 398ms/step
Epoch 172/1000
2023-10-25 13:53:11.832 
Epoch 172/1000 
	 loss: 185.1748, MinusLogProbMetric: 185.1748, val_loss: 185.1649, val_MinusLogProbMetric: 185.1649

Epoch 172: val_loss improved from 185.76451 to 185.16495, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 185.1748 - MinusLogProbMetric: 185.1748 - val_loss: 185.1649 - val_MinusLogProbMetric: 185.1649 - lr: 1.2346e-05 - 79s/epoch - 402ms/step
Epoch 173/1000
2023-10-25 13:54:31.427 
Epoch 173/1000 
	 loss: 184.4498, MinusLogProbMetric: 184.4498, val_loss: 184.5399, val_MinusLogProbMetric: 184.5399

Epoch 173: val_loss improved from 185.16495 to 184.53987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 184.4498 - MinusLogProbMetric: 184.4498 - val_loss: 184.5399 - val_MinusLogProbMetric: 184.5399 - lr: 1.2346e-05 - 80s/epoch - 408ms/step
Epoch 174/1000
2023-10-25 13:55:50.307 
Epoch 174/1000 
	 loss: 184.1472, MinusLogProbMetric: 184.1472, val_loss: 184.6022, val_MinusLogProbMetric: 184.6022

Epoch 174: val_loss did not improve from 184.53987
196/196 - 77s - loss: 184.1472 - MinusLogProbMetric: 184.1472 - val_loss: 184.6022 - val_MinusLogProbMetric: 184.6022 - lr: 1.2346e-05 - 77s/epoch - 395ms/step
Epoch 175/1000
2023-10-25 13:57:06.746 
Epoch 175/1000 
	 loss: 185.5978, MinusLogProbMetric: 185.5978, val_loss: 184.4027, val_MinusLogProbMetric: 184.4027

Epoch 175: val_loss improved from 184.53987 to 184.40266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 185.5978 - MinusLogProbMetric: 185.5978 - val_loss: 184.4027 - val_MinusLogProbMetric: 184.4027 - lr: 1.2346e-05 - 77s/epoch - 395ms/step
Epoch 176/1000
2023-10-25 13:58:23.367 
Epoch 176/1000 
	 loss: 183.5163, MinusLogProbMetric: 183.5163, val_loss: 183.5326, val_MinusLogProbMetric: 183.5326

Epoch 176: val_loss improved from 184.40266 to 183.53258, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 183.5163 - MinusLogProbMetric: 183.5163 - val_loss: 183.5326 - val_MinusLogProbMetric: 183.5326 - lr: 1.2346e-05 - 77s/epoch - 391ms/step
Epoch 177/1000
2023-10-25 13:59:40.233 
Epoch 177/1000 
	 loss: 183.0651, MinusLogProbMetric: 183.0651, val_loss: 183.3533, val_MinusLogProbMetric: 183.3533

Epoch 177: val_loss improved from 183.53258 to 183.35333, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 77s - loss: 183.0651 - MinusLogProbMetric: 183.0651 - val_loss: 183.3533 - val_MinusLogProbMetric: 183.3533 - lr: 1.2346e-05 - 77s/epoch - 393ms/step
Epoch 178/1000
2023-10-25 14:00:59.067 
Epoch 178/1000 
	 loss: 182.5958, MinusLogProbMetric: 182.5958, val_loss: 182.9464, val_MinusLogProbMetric: 182.9464

Epoch 178: val_loss improved from 183.35333 to 182.94635, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 182.5958 - MinusLogProbMetric: 182.5958 - val_loss: 182.9464 - val_MinusLogProbMetric: 182.9464 - lr: 1.2346e-05 - 79s/epoch - 401ms/step
Epoch 179/1000
2023-10-25 14:02:17.528 
Epoch 179/1000 
	 loss: 181.7647, MinusLogProbMetric: 181.7647, val_loss: 181.7487, val_MinusLogProbMetric: 181.7487

Epoch 179: val_loss improved from 182.94635 to 181.74866, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 181.7647 - MinusLogProbMetric: 181.7647 - val_loss: 181.7487 - val_MinusLogProbMetric: 181.7487 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 180/1000
2023-10-25 14:03:36.742 
Epoch 180/1000 
	 loss: 181.2303, MinusLogProbMetric: 181.2303, val_loss: 181.1281, val_MinusLogProbMetric: 181.1281

Epoch 180: val_loss improved from 181.74866 to 181.12808, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 181.2303 - MinusLogProbMetric: 181.2303 - val_loss: 181.1281 - val_MinusLogProbMetric: 181.1281 - lr: 1.2346e-05 - 79s/epoch - 406ms/step
Epoch 181/1000
2023-10-25 14:04:56.511 
Epoch 181/1000 
	 loss: 180.7896, MinusLogProbMetric: 180.7896, val_loss: 180.8201, val_MinusLogProbMetric: 180.8201

Epoch 181: val_loss improved from 181.12808 to 180.82011, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 180.7896 - MinusLogProbMetric: 180.7896 - val_loss: 180.8201 - val_MinusLogProbMetric: 180.8201 - lr: 1.2346e-05 - 79s/epoch - 405ms/step
Epoch 182/1000
2023-10-25 14:06:14.287 
Epoch 182/1000 
	 loss: 180.2522, MinusLogProbMetric: 180.2522, val_loss: 180.2737, val_MinusLogProbMetric: 180.2737

Epoch 182: val_loss improved from 180.82011 to 180.27373, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 180.2522 - MinusLogProbMetric: 180.2522 - val_loss: 180.2737 - val_MinusLogProbMetric: 180.2737 - lr: 1.2346e-05 - 78s/epoch - 397ms/step
Epoch 183/1000
2023-10-25 14:07:33.478 
Epoch 183/1000 
	 loss: 180.5241, MinusLogProbMetric: 180.5241, val_loss: 181.0228, val_MinusLogProbMetric: 181.0228

Epoch 183: val_loss did not improve from 180.27373
196/196 - 78s - loss: 180.5241 - MinusLogProbMetric: 180.5241 - val_loss: 181.0228 - val_MinusLogProbMetric: 181.0228 - lr: 1.2346e-05 - 78s/epoch - 399ms/step
Epoch 184/1000
2023-10-25 14:08:53.560 
Epoch 184/1000 
	 loss: 196.5156, MinusLogProbMetric: 196.5156, val_loss: 269.9596, val_MinusLogProbMetric: 269.9596

Epoch 184: val_loss did not improve from 180.27373
196/196 - 80s - loss: 196.5156 - MinusLogProbMetric: 196.5156 - val_loss: 269.9596 - val_MinusLogProbMetric: 269.9596 - lr: 1.2346e-05 - 80s/epoch - 409ms/step
Epoch 185/1000
2023-10-25 14:10:12.715 
Epoch 185/1000 
	 loss: 225.5272, MinusLogProbMetric: 225.5272, val_loss: 208.9061, val_MinusLogProbMetric: 208.9061

Epoch 185: val_loss did not improve from 180.27373
196/196 - 79s - loss: 225.5272 - MinusLogProbMetric: 225.5272 - val_loss: 208.9061 - val_MinusLogProbMetric: 208.9061 - lr: 1.2346e-05 - 79s/epoch - 404ms/step
Epoch 186/1000
2023-10-25 14:11:29.806 
Epoch 186/1000 
	 loss: 202.0109, MinusLogProbMetric: 202.0109, val_loss: 195.6713, val_MinusLogProbMetric: 195.6713

Epoch 186: val_loss did not improve from 180.27373
196/196 - 77s - loss: 202.0109 - MinusLogProbMetric: 202.0109 - val_loss: 195.6713 - val_MinusLogProbMetric: 195.6713 - lr: 1.2346e-05 - 77s/epoch - 393ms/step
Epoch 187/1000
2023-10-25 14:12:47.206 
Epoch 187/1000 
	 loss: 193.1656, MinusLogProbMetric: 193.1656, val_loss: 207.6464, val_MinusLogProbMetric: 207.6464

Epoch 187: val_loss did not improve from 180.27373
196/196 - 77s - loss: 193.1656 - MinusLogProbMetric: 193.1656 - val_loss: 207.6464 - val_MinusLogProbMetric: 207.6464 - lr: 1.2346e-05 - 77s/epoch - 395ms/step
Epoch 188/1000
2023-10-25 14:14:05.561 
Epoch 188/1000 
	 loss: 191.4418, MinusLogProbMetric: 191.4418, val_loss: 189.1075, val_MinusLogProbMetric: 189.1075

Epoch 188: val_loss did not improve from 180.27373
196/196 - 78s - loss: 191.4418 - MinusLogProbMetric: 191.4418 - val_loss: 189.1075 - val_MinusLogProbMetric: 189.1075 - lr: 1.2346e-05 - 78s/epoch - 400ms/step
Epoch 189/1000
2023-10-25 14:15:25.277 
Epoch 189/1000 
	 loss: 187.3598, MinusLogProbMetric: 187.3598, val_loss: 186.4570, val_MinusLogProbMetric: 186.4570

Epoch 189: val_loss did not improve from 180.27373
196/196 - 80s - loss: 187.3598 - MinusLogProbMetric: 187.3598 - val_loss: 186.4570 - val_MinusLogProbMetric: 186.4570 - lr: 1.2346e-05 - 80s/epoch - 407ms/step
Epoch 190/1000
2023-10-25 14:16:43.328 
Epoch 190/1000 
	 loss: 185.4889, MinusLogProbMetric: 185.4889, val_loss: 184.5087, val_MinusLogProbMetric: 184.5087

Epoch 190: val_loss did not improve from 180.27373
196/196 - 78s - loss: 185.4889 - MinusLogProbMetric: 185.4889 - val_loss: 184.5087 - val_MinusLogProbMetric: 184.5087 - lr: 1.2346e-05 - 78s/epoch - 398ms/step
Epoch 191/1000
2023-10-25 14:18:01.073 
Epoch 191/1000 
	 loss: 183.4849, MinusLogProbMetric: 183.4849, val_loss: 183.2640, val_MinusLogProbMetric: 183.2640

Epoch 191: val_loss did not improve from 180.27373
196/196 - 78s - loss: 183.4849 - MinusLogProbMetric: 183.4849 - val_loss: 183.2640 - val_MinusLogProbMetric: 183.2640 - lr: 1.2346e-05 - 78s/epoch - 397ms/step
Epoch 192/1000
2023-10-25 14:19:20.848 
Epoch 192/1000 
	 loss: 182.6580, MinusLogProbMetric: 182.6580, val_loss: 182.4863, val_MinusLogProbMetric: 182.4863

Epoch 192: val_loss did not improve from 180.27373
196/196 - 80s - loss: 182.6580 - MinusLogProbMetric: 182.6580 - val_loss: 182.4863 - val_MinusLogProbMetric: 182.4863 - lr: 1.2346e-05 - 80s/epoch - 407ms/step
Epoch 193/1000
2023-10-25 14:20:40.532 
Epoch 193/1000 
	 loss: 181.6609, MinusLogProbMetric: 181.6609, val_loss: 182.9379, val_MinusLogProbMetric: 182.9379

Epoch 193: val_loss did not improve from 180.27373
196/196 - 80s - loss: 181.6609 - MinusLogProbMetric: 181.6609 - val_loss: 182.9379 - val_MinusLogProbMetric: 182.9379 - lr: 1.2346e-05 - 80s/epoch - 407ms/step
Epoch 194/1000
2023-10-25 14:21:59.200 
Epoch 194/1000 
	 loss: 180.7100, MinusLogProbMetric: 180.7100, val_loss: 180.7807, val_MinusLogProbMetric: 180.7807

Epoch 194: val_loss did not improve from 180.27373
196/196 - 79s - loss: 180.7100 - MinusLogProbMetric: 180.7100 - val_loss: 180.7807 - val_MinusLogProbMetric: 180.7807 - lr: 1.2346e-05 - 79s/epoch - 401ms/step
Epoch 195/1000
2023-10-25 14:23:16.844 
Epoch 195/1000 
	 loss: 179.9694, MinusLogProbMetric: 179.9694, val_loss: 179.9279, val_MinusLogProbMetric: 179.9279

Epoch 195: val_loss improved from 180.27373 to 179.92792, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 179.9694 - MinusLogProbMetric: 179.9694 - val_loss: 179.9279 - val_MinusLogProbMetric: 179.9279 - lr: 1.2346e-05 - 79s/epoch - 402ms/step
Epoch 196/1000
2023-10-25 14:24:37.248 
Epoch 196/1000 
	 loss: 179.2772, MinusLogProbMetric: 179.2772, val_loss: 179.2429, val_MinusLogProbMetric: 179.2429

Epoch 196: val_loss improved from 179.92792 to 179.24287, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 179.2772 - MinusLogProbMetric: 179.2772 - val_loss: 179.2429 - val_MinusLogProbMetric: 179.2429 - lr: 1.2346e-05 - 80s/epoch - 410ms/step
Epoch 197/1000
2023-10-25 14:25:57.274 
Epoch 197/1000 
	 loss: 178.5104, MinusLogProbMetric: 178.5104, val_loss: 178.3829, val_MinusLogProbMetric: 178.3829

Epoch 197: val_loss improved from 179.24287 to 178.38293, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 178.5104 - MinusLogProbMetric: 178.5104 - val_loss: 178.3829 - val_MinusLogProbMetric: 178.3829 - lr: 1.2346e-05 - 80s/epoch - 410ms/step
Epoch 198/1000
2023-10-25 14:27:17.711 
Epoch 198/1000 
	 loss: 178.1756, MinusLogProbMetric: 178.1756, val_loss: 178.1793, val_MinusLogProbMetric: 178.1793

Epoch 198: val_loss improved from 178.38293 to 178.17926, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 178.1756 - MinusLogProbMetric: 178.1756 - val_loss: 178.1793 - val_MinusLogProbMetric: 178.1793 - lr: 1.2346e-05 - 80s/epoch - 409ms/step
Epoch 199/1000
2023-10-25 14:28:23.692 
Epoch 199/1000 
	 loss: 184.3269, MinusLogProbMetric: 184.3269, val_loss: 179.1651, val_MinusLogProbMetric: 179.1651

Epoch 199: val_loss did not improve from 178.17926
196/196 - 65s - loss: 184.3269 - MinusLogProbMetric: 184.3269 - val_loss: 179.1651 - val_MinusLogProbMetric: 179.1651 - lr: 1.2346e-05 - 65s/epoch - 331ms/step
Epoch 200/1000
2023-10-25 14:29:27.878 
Epoch 200/1000 
	 loss: 177.8040, MinusLogProbMetric: 177.8040, val_loss: 177.5569, val_MinusLogProbMetric: 177.5569

Epoch 200: val_loss improved from 178.17926 to 177.55690, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 65s - loss: 177.8040 - MinusLogProbMetric: 177.8040 - val_loss: 177.5569 - val_MinusLogProbMetric: 177.5569 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 201/1000
2023-10-25 14:30:38.465 
Epoch 201/1000 
	 loss: 177.3535, MinusLogProbMetric: 177.3535, val_loss: 177.2133, val_MinusLogProbMetric: 177.2133

Epoch 201: val_loss improved from 177.55690 to 177.21335, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 71s - loss: 177.3535 - MinusLogProbMetric: 177.3535 - val_loss: 177.2133 - val_MinusLogProbMetric: 177.2133 - lr: 1.2346e-05 - 71s/epoch - 360ms/step
Epoch 202/1000
2023-10-25 14:31:45.323 
Epoch 202/1000 
	 loss: 176.2667, MinusLogProbMetric: 176.2667, val_loss: 176.2106, val_MinusLogProbMetric: 176.2106

Epoch 202: val_loss improved from 177.21335 to 176.21056, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 67s - loss: 176.2667 - MinusLogProbMetric: 176.2667 - val_loss: 176.2106 - val_MinusLogProbMetric: 176.2106 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 203/1000
2023-10-25 14:32:53.860 
Epoch 203/1000 
	 loss: 175.7143, MinusLogProbMetric: 175.7143, val_loss: 175.7291, val_MinusLogProbMetric: 175.7291

Epoch 203: val_loss improved from 176.21056 to 175.72908, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 69s - loss: 175.7143 - MinusLogProbMetric: 175.7143 - val_loss: 175.7291 - val_MinusLogProbMetric: 175.7291 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 204/1000
2023-10-25 14:34:02.665 
Epoch 204/1000 
	 loss: 175.0810, MinusLogProbMetric: 175.0810, val_loss: 175.1066, val_MinusLogProbMetric: 175.1066

Epoch 204: val_loss improved from 175.72908 to 175.10655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 69s - loss: 175.0810 - MinusLogProbMetric: 175.0810 - val_loss: 175.1066 - val_MinusLogProbMetric: 175.1066 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 205/1000
2023-10-25 14:35:07.546 
Epoch 205/1000 
	 loss: 174.9247, MinusLogProbMetric: 174.9247, val_loss: 175.5898, val_MinusLogProbMetric: 175.5898

Epoch 205: val_loss did not improve from 175.10655
196/196 - 64s - loss: 174.9247 - MinusLogProbMetric: 174.9247 - val_loss: 175.5898 - val_MinusLogProbMetric: 175.5898 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 206/1000
2023-10-25 14:36:13.531 
Epoch 206/1000 
	 loss: 174.5340, MinusLogProbMetric: 174.5340, val_loss: 174.0286, val_MinusLogProbMetric: 174.0286

Epoch 206: val_loss improved from 175.10655 to 174.02856, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 67s - loss: 174.5340 - MinusLogProbMetric: 174.5340 - val_loss: 174.0286 - val_MinusLogProbMetric: 174.0286 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 207/1000
2023-10-25 14:37:19.405 
Epoch 207/1000 
	 loss: 174.3693, MinusLogProbMetric: 174.3693, val_loss: 173.7400, val_MinusLogProbMetric: 173.7400

Epoch 207: val_loss improved from 174.02856 to 173.74002, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 66s - loss: 174.3693 - MinusLogProbMetric: 174.3693 - val_loss: 173.7400 - val_MinusLogProbMetric: 173.7400 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 208/1000
2023-10-25 14:38:24.974 
Epoch 208/1000 
	 loss: 173.1674, MinusLogProbMetric: 173.1674, val_loss: 173.1938, val_MinusLogProbMetric: 173.1938

Epoch 208: val_loss improved from 173.74002 to 173.19385, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 65s - loss: 173.1674 - MinusLogProbMetric: 173.1674 - val_loss: 173.1938 - val_MinusLogProbMetric: 173.1938 - lr: 1.2346e-05 - 65s/epoch - 334ms/step
Epoch 209/1000
2023-10-25 14:39:31.270 
Epoch 209/1000 
	 loss: 173.0059, MinusLogProbMetric: 173.0059, val_loss: 173.1205, val_MinusLogProbMetric: 173.1205

Epoch 209: val_loss improved from 173.19385 to 173.12048, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 66s - loss: 173.0059 - MinusLogProbMetric: 173.0059 - val_loss: 173.1205 - val_MinusLogProbMetric: 173.1205 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 210/1000
2023-10-25 14:40:38.941 
Epoch 210/1000 
	 loss: 172.3761, MinusLogProbMetric: 172.3761, val_loss: 172.3127, val_MinusLogProbMetric: 172.3127

Epoch 210: val_loss improved from 173.12048 to 172.31271, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 68s - loss: 172.3761 - MinusLogProbMetric: 172.3761 - val_loss: 172.3127 - val_MinusLogProbMetric: 172.3127 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 211/1000
2023-10-25 14:41:46.787 
Epoch 211/1000 
	 loss: 195.5348, MinusLogProbMetric: 195.5348, val_loss: 248.4897, val_MinusLogProbMetric: 248.4897

Epoch 211: val_loss did not improve from 172.31271
196/196 - 67s - loss: 195.5348 - MinusLogProbMetric: 195.5348 - val_loss: 248.4897 - val_MinusLogProbMetric: 248.4897 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 212/1000
2023-10-25 14:42:51.005 
Epoch 212/1000 
	 loss: 213.0563, MinusLogProbMetric: 213.0563, val_loss: 190.8608, val_MinusLogProbMetric: 190.8608

Epoch 212: val_loss did not improve from 172.31271
196/196 - 64s - loss: 213.0563 - MinusLogProbMetric: 213.0563 - val_loss: 190.8608 - val_MinusLogProbMetric: 190.8608 - lr: 1.2346e-05 - 64s/epoch - 328ms/step
Epoch 213/1000
2023-10-25 14:44:08.788 
Epoch 213/1000 
	 loss: 184.7108, MinusLogProbMetric: 184.7108, val_loss: 179.4865, val_MinusLogProbMetric: 179.4865

Epoch 213: val_loss did not improve from 172.31271
196/196 - 78s - loss: 184.7108 - MinusLogProbMetric: 184.7108 - val_loss: 179.4865 - val_MinusLogProbMetric: 179.4865 - lr: 1.2346e-05 - 78s/epoch - 397ms/step
Epoch 214/1000
2023-10-25 14:45:27.864 
Epoch 214/1000 
	 loss: 175.9687, MinusLogProbMetric: 175.9687, val_loss: 174.2585, val_MinusLogProbMetric: 174.2585

Epoch 214: val_loss did not improve from 172.31271
196/196 - 79s - loss: 175.9687 - MinusLogProbMetric: 175.9687 - val_loss: 174.2585 - val_MinusLogProbMetric: 174.2585 - lr: 1.2346e-05 - 79s/epoch - 403ms/step
Epoch 215/1000
2023-10-25 14:46:49.640 
Epoch 215/1000 
	 loss: 173.1789, MinusLogProbMetric: 173.1789, val_loss: 172.7778, val_MinusLogProbMetric: 172.7778

Epoch 215: val_loss did not improve from 172.31271
196/196 - 82s - loss: 173.1789 - MinusLogProbMetric: 173.1789 - val_loss: 172.7778 - val_MinusLogProbMetric: 172.7778 - lr: 1.2346e-05 - 82s/epoch - 417ms/step
Epoch 216/1000
2023-10-25 14:48:07.530 
Epoch 216/1000 
	 loss: 172.2116, MinusLogProbMetric: 172.2116, val_loss: 172.0949, val_MinusLogProbMetric: 172.0949

Epoch 216: val_loss improved from 172.31271 to 172.09494, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 172.2116 - MinusLogProbMetric: 172.2116 - val_loss: 172.0949 - val_MinusLogProbMetric: 172.0949 - lr: 1.2346e-05 - 79s/epoch - 404ms/step
Epoch 217/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 158: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 14:49:14.471 
Epoch 217/1000 
	 loss: nan, MinusLogProbMetric: 201.0752, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 217: val_loss did not improve from 172.09494
196/196 - 66s - loss: nan - MinusLogProbMetric: 201.0752 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 66s/epoch - 335ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.115226337448558e-06.
===========
Generating train data for run 352.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_352/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_352
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_424"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_425 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7fe4dfe6a620>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe4b4a40e50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe4b4a40e50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe7e04e6470>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe7e04e74f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe7e04e41f0>, <keras.callbacks.ModelCheckpoint object at 0x7fe7e04e50f0>, <keras.callbacks.EarlyStopping object at 0x7fe7e04e6ef0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe7e04e43a0>, <keras.callbacks.TerminateOnNaN object at 0x7fe7e04e59c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 352/720 with hyperparameters:
timestamp = 2023-10-25 14:49:35.773595
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-10-25 14:53:38.969 
Epoch 1/1000 
	 loss: 169.7068, MinusLogProbMetric: 169.7068, val_loss: 175.9625, val_MinusLogProbMetric: 175.9625

Epoch 1: val_loss improved from inf to 175.96249, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 244s - loss: 169.7068 - MinusLogProbMetric: 169.7068 - val_loss: 175.9625 - val_MinusLogProbMetric: 175.9625 - lr: 4.1152e-06 - 244s/epoch - 1s/step
Epoch 2/1000
2023-10-25 14:55:00.522 
Epoch 2/1000 
	 loss: 163.2745, MinusLogProbMetric: 163.2745, val_loss: 167.9901, val_MinusLogProbMetric: 167.9901

Epoch 2: val_loss improved from 175.96249 to 167.99005, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 81s - loss: 163.2745 - MinusLogProbMetric: 163.2745 - val_loss: 167.9901 - val_MinusLogProbMetric: 167.9901 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 3/1000
2023-10-25 14:56:20.776 
Epoch 3/1000 
	 loss: 155.9861, MinusLogProbMetric: 155.9861, val_loss: 151.5929, val_MinusLogProbMetric: 151.5929

Epoch 3: val_loss improved from 167.99005 to 151.59290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 155.9861 - MinusLogProbMetric: 155.9861 - val_loss: 151.5929 - val_MinusLogProbMetric: 151.5929 - lr: 4.1152e-06 - 80s/epoch - 409ms/step
Epoch 4/1000
2023-10-25 14:57:42.117 
Epoch 4/1000 
	 loss: 149.2599, MinusLogProbMetric: 149.2599, val_loss: 146.3564, val_MinusLogProbMetric: 146.3564

Epoch 4: val_loss improved from 151.59290 to 146.35635, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 82s - loss: 149.2599 - MinusLogProbMetric: 149.2599 - val_loss: 146.3564 - val_MinusLogProbMetric: 146.3564 - lr: 4.1152e-06 - 82s/epoch - 416ms/step
Epoch 5/1000
2023-10-25 14:59:03.682 
Epoch 5/1000 
	 loss: 144.8351, MinusLogProbMetric: 144.8351, val_loss: 142.7655, val_MinusLogProbMetric: 142.7655

Epoch 5: val_loss improved from 146.35635 to 142.76547, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 81s - loss: 144.8351 - MinusLogProbMetric: 144.8351 - val_loss: 142.7655 - val_MinusLogProbMetric: 142.7655 - lr: 4.1152e-06 - 81s/epoch - 415ms/step
Epoch 6/1000
2023-10-25 15:00:24.446 
Epoch 6/1000 
	 loss: 141.7359, MinusLogProbMetric: 141.7359, val_loss: 138.5546, val_MinusLogProbMetric: 138.5546

Epoch 6: val_loss improved from 142.76547 to 138.55463, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 81s - loss: 141.7359 - MinusLogProbMetric: 141.7359 - val_loss: 138.5546 - val_MinusLogProbMetric: 138.5546 - lr: 4.1152e-06 - 81s/epoch - 413ms/step
Epoch 7/1000
2023-10-25 15:01:47.870 
Epoch 7/1000 
	 loss: 142.8571, MinusLogProbMetric: 142.8571, val_loss: 138.7554, val_MinusLogProbMetric: 138.7554

Epoch 7: val_loss did not improve from 138.55463
196/196 - 82s - loss: 142.8571 - MinusLogProbMetric: 142.8571 - val_loss: 138.7554 - val_MinusLogProbMetric: 138.7554 - lr: 4.1152e-06 - 82s/epoch - 419ms/step
Epoch 8/1000
2023-10-25 15:03:07.866 
Epoch 8/1000 
	 loss: 136.4556, MinusLogProbMetric: 136.4556, val_loss: 135.1410, val_MinusLogProbMetric: 135.1410

Epoch 8: val_loss improved from 138.55463 to 135.14102, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 81s - loss: 136.4556 - MinusLogProbMetric: 136.4556 - val_loss: 135.1410 - val_MinusLogProbMetric: 135.1410 - lr: 4.1152e-06 - 81s/epoch - 415ms/step
Epoch 9/1000
2023-10-25 15:04:29.097 
Epoch 9/1000 
	 loss: 135.7845, MinusLogProbMetric: 135.7845, val_loss: 135.2327, val_MinusLogProbMetric: 135.2327

Epoch 9: val_loss did not improve from 135.14102
196/196 - 80s - loss: 135.7845 - MinusLogProbMetric: 135.7845 - val_loss: 135.2327 - val_MinusLogProbMetric: 135.2327 - lr: 4.1152e-06 - 80s/epoch - 408ms/step
Epoch 10/1000
2023-10-25 15:05:51.472 
Epoch 10/1000 
	 loss: 132.7094, MinusLogProbMetric: 132.7094, val_loss: 131.8599, val_MinusLogProbMetric: 131.8599

Epoch 10: val_loss improved from 135.14102 to 131.85989, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 84s - loss: 132.7094 - MinusLogProbMetric: 132.7094 - val_loss: 131.8599 - val_MinusLogProbMetric: 131.8599 - lr: 4.1152e-06 - 84s/epoch - 427ms/step
Epoch 11/1000
2023-10-25 15:07:12.137 
Epoch 11/1000 
	 loss: 155.6218, MinusLogProbMetric: 155.6218, val_loss: 154.6561, val_MinusLogProbMetric: 154.6561

Epoch 11: val_loss did not improve from 131.85989
196/196 - 79s - loss: 155.6218 - MinusLogProbMetric: 155.6218 - val_loss: 154.6561 - val_MinusLogProbMetric: 154.6561 - lr: 4.1152e-06 - 79s/epoch - 405ms/step
Epoch 12/1000
2023-10-25 15:08:32.594 
Epoch 12/1000 
	 loss: 147.4391, MinusLogProbMetric: 147.4391, val_loss: 142.2076, val_MinusLogProbMetric: 142.2076

Epoch 12: val_loss did not improve from 131.85989
196/196 - 80s - loss: 147.4391 - MinusLogProbMetric: 147.4391 - val_loss: 142.2076 - val_MinusLogProbMetric: 142.2076 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 13/1000
2023-10-25 15:09:52.254 
Epoch 13/1000 
	 loss: 139.0986, MinusLogProbMetric: 139.0986, val_loss: 135.3436, val_MinusLogProbMetric: 135.3436

Epoch 13: val_loss did not improve from 131.85989
196/196 - 80s - loss: 139.0986 - MinusLogProbMetric: 139.0986 - val_loss: 135.3436 - val_MinusLogProbMetric: 135.3436 - lr: 4.1152e-06 - 80s/epoch - 406ms/step
Epoch 14/1000
2023-10-25 15:11:11.152 
Epoch 14/1000 
	 loss: 135.0468, MinusLogProbMetric: 135.0468, val_loss: 133.5650, val_MinusLogProbMetric: 133.5650

Epoch 14: val_loss did not improve from 131.85989
196/196 - 79s - loss: 135.0468 - MinusLogProbMetric: 135.0468 - val_loss: 133.5650 - val_MinusLogProbMetric: 133.5650 - lr: 4.1152e-06 - 79s/epoch - 403ms/step
Epoch 15/1000
2023-10-25 15:12:28.469 
Epoch 15/1000 
	 loss: 131.8867, MinusLogProbMetric: 131.8867, val_loss: 131.0650, val_MinusLogProbMetric: 131.0650

Epoch 15: val_loss improved from 131.85989 to 131.06496, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 131.8867 - MinusLogProbMetric: 131.8867 - val_loss: 131.0650 - val_MinusLogProbMetric: 131.0650 - lr: 4.1152e-06 - 79s/epoch - 401ms/step
Epoch 16/1000
2023-10-25 15:13:46.335 
Epoch 16/1000 
	 loss: 129.7667, MinusLogProbMetric: 129.7667, val_loss: 127.6090, val_MinusLogProbMetric: 127.6090

Epoch 16: val_loss improved from 131.06496 to 127.60896, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 129.7667 - MinusLogProbMetric: 129.7667 - val_loss: 127.6090 - val_MinusLogProbMetric: 127.6090 - lr: 4.1152e-06 - 78s/epoch - 396ms/step
Epoch 17/1000
2023-10-25 15:15:06.412 
Epoch 17/1000 
	 loss: 126.9557, MinusLogProbMetric: 126.9557, val_loss: 125.8851, val_MinusLogProbMetric: 125.8851

Epoch 17: val_loss improved from 127.60896 to 125.88509, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 126.9557 - MinusLogProbMetric: 126.9557 - val_loss: 125.8851 - val_MinusLogProbMetric: 125.8851 - lr: 4.1152e-06 - 80s/epoch - 410ms/step
Epoch 18/1000
2023-10-25 15:16:28.321 
Epoch 18/1000 
	 loss: 124.7514, MinusLogProbMetric: 124.7514, val_loss: 125.0295, val_MinusLogProbMetric: 125.0295

Epoch 18: val_loss improved from 125.88509 to 125.02954, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 82s - loss: 124.7514 - MinusLogProbMetric: 124.7514 - val_loss: 125.0295 - val_MinusLogProbMetric: 125.0295 - lr: 4.1152e-06 - 82s/epoch - 418ms/step
Epoch 19/1000
2023-10-25 15:17:50.051 
Epoch 19/1000 
	 loss: 124.4596, MinusLogProbMetric: 124.4596, val_loss: 122.3326, val_MinusLogProbMetric: 122.3326

Epoch 19: val_loss improved from 125.02954 to 122.33259, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 82s - loss: 124.4596 - MinusLogProbMetric: 124.4596 - val_loss: 122.3326 - val_MinusLogProbMetric: 122.3326 - lr: 4.1152e-06 - 82s/epoch - 416ms/step
Epoch 20/1000
2023-10-25 15:19:09.975 
Epoch 20/1000 
	 loss: 121.4344, MinusLogProbMetric: 121.4344, val_loss: 120.3199, val_MinusLogProbMetric: 120.3199

Epoch 20: val_loss improved from 122.33259 to 120.31992, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 121.4344 - MinusLogProbMetric: 121.4344 - val_loss: 120.3199 - val_MinusLogProbMetric: 120.3199 - lr: 4.1152e-06 - 80s/epoch - 408ms/step
Epoch 21/1000
2023-10-25 15:20:28.062 
Epoch 21/1000 
	 loss: 119.2621, MinusLogProbMetric: 119.2621, val_loss: 118.6387, val_MinusLogProbMetric: 118.6387

Epoch 21: val_loss improved from 120.31992 to 118.63872, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 119.2621 - MinusLogProbMetric: 119.2621 - val_loss: 118.6387 - val_MinusLogProbMetric: 118.6387 - lr: 4.1152e-06 - 78s/epoch - 399ms/step
Epoch 22/1000
2023-10-25 15:21:47.408 
Epoch 22/1000 
	 loss: 117.8387, MinusLogProbMetric: 117.8387, val_loss: 118.5117, val_MinusLogProbMetric: 118.5117

Epoch 22: val_loss improved from 118.63872 to 118.51166, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 117.8387 - MinusLogProbMetric: 117.8387 - val_loss: 118.5117 - val_MinusLogProbMetric: 118.5117 - lr: 4.1152e-06 - 79s/epoch - 405ms/step
Epoch 23/1000
2023-10-25 15:23:09.072 
Epoch 23/1000 
	 loss: 117.2349, MinusLogProbMetric: 117.2349, val_loss: 130.3273, val_MinusLogProbMetric: 130.3273

Epoch 23: val_loss did not improve from 118.51166
196/196 - 80s - loss: 117.2349 - MinusLogProbMetric: 117.2349 - val_loss: 130.3273 - val_MinusLogProbMetric: 130.3273 - lr: 4.1152e-06 - 80s/epoch - 411ms/step
Epoch 24/1000
2023-10-25 15:24:27.623 
Epoch 24/1000 
	 loss: 116.9074, MinusLogProbMetric: 116.9074, val_loss: 115.2164, val_MinusLogProbMetric: 115.2164

Epoch 24: val_loss improved from 118.51166 to 115.21636, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 116.9074 - MinusLogProbMetric: 116.9074 - val_loss: 115.2164 - val_MinusLogProbMetric: 115.2164 - lr: 4.1152e-06 - 80s/epoch - 406ms/step
Epoch 25/1000
2023-10-25 15:25:46.859 
Epoch 25/1000 
	 loss: 145.4916, MinusLogProbMetric: 145.4916, val_loss: 221.5363, val_MinusLogProbMetric: 221.5363

Epoch 25: val_loss did not improve from 115.21636
196/196 - 78s - loss: 145.4916 - MinusLogProbMetric: 145.4916 - val_loss: 221.5363 - val_MinusLogProbMetric: 221.5363 - lr: 4.1152e-06 - 78s/epoch - 399ms/step
Epoch 26/1000
2023-10-25 15:27:08.962 
Epoch 26/1000 
	 loss: 189.2080, MinusLogProbMetric: 189.2080, val_loss: 159.1995, val_MinusLogProbMetric: 159.1995

Epoch 26: val_loss did not improve from 115.21636
196/196 - 82s - loss: 189.2080 - MinusLogProbMetric: 189.2080 - val_loss: 159.1995 - val_MinusLogProbMetric: 159.1995 - lr: 4.1152e-06 - 82s/epoch - 419ms/step
Epoch 27/1000
2023-10-25 15:28:30.731 
Epoch 27/1000 
	 loss: 150.4966, MinusLogProbMetric: 150.4966, val_loss: 144.7567, val_MinusLogProbMetric: 144.7567

Epoch 27: val_loss did not improve from 115.21636
196/196 - 82s - loss: 150.4966 - MinusLogProbMetric: 150.4966 - val_loss: 144.7567 - val_MinusLogProbMetric: 144.7567 - lr: 4.1152e-06 - 82s/epoch - 417ms/step
Epoch 28/1000
2023-10-25 15:29:52.948 
Epoch 28/1000 
	 loss: 141.4059, MinusLogProbMetric: 141.4059, val_loss: 139.9391, val_MinusLogProbMetric: 139.9391

Epoch 28: val_loss did not improve from 115.21636
196/196 - 82s - loss: 141.4059 - MinusLogProbMetric: 141.4059 - val_loss: 139.9391 - val_MinusLogProbMetric: 139.9391 - lr: 4.1152e-06 - 82s/epoch - 419ms/step
Epoch 29/1000
2023-10-25 15:31:14.447 
Epoch 29/1000 
	 loss: 140.5807, MinusLogProbMetric: 140.5807, val_loss: 138.5397, val_MinusLogProbMetric: 138.5397

Epoch 29: val_loss did not improve from 115.21636
196/196 - 81s - loss: 140.5807 - MinusLogProbMetric: 140.5807 - val_loss: 138.5397 - val_MinusLogProbMetric: 138.5397 - lr: 4.1152e-06 - 81s/epoch - 416ms/step
Epoch 30/1000
2023-10-25 15:32:37.161 
Epoch 30/1000 
	 loss: 144.4332, MinusLogProbMetric: 144.4332, val_loss: 245.0329, val_MinusLogProbMetric: 245.0329

Epoch 30: val_loss did not improve from 115.21636
196/196 - 83s - loss: 144.4332 - MinusLogProbMetric: 144.4332 - val_loss: 245.0329 - val_MinusLogProbMetric: 245.0329 - lr: 4.1152e-06 - 83s/epoch - 422ms/step
Epoch 31/1000
2023-10-25 15:33:58.632 
Epoch 31/1000 
	 loss: 183.7987, MinusLogProbMetric: 183.7987, val_loss: 164.5060, val_MinusLogProbMetric: 164.5060

Epoch 31: val_loss did not improve from 115.21636
196/196 - 81s - loss: 183.7987 - MinusLogProbMetric: 183.7987 - val_loss: 164.5060 - val_MinusLogProbMetric: 164.5060 - lr: 4.1152e-06 - 81s/epoch - 416ms/step
Epoch 32/1000
2023-10-25 15:35:18.331 
Epoch 32/1000 
	 loss: 157.9048, MinusLogProbMetric: 157.9048, val_loss: 155.8475, val_MinusLogProbMetric: 155.8475

Epoch 32: val_loss did not improve from 115.21636
196/196 - 80s - loss: 157.9048 - MinusLogProbMetric: 157.9048 - val_loss: 155.8475 - val_MinusLogProbMetric: 155.8475 - lr: 4.1152e-06 - 80s/epoch - 407ms/step
Epoch 33/1000
2023-10-25 15:36:34.369 
Epoch 33/1000 
	 loss: 150.1654, MinusLogProbMetric: 150.1654, val_loss: 146.7106, val_MinusLogProbMetric: 146.7106

Epoch 33: val_loss did not improve from 115.21636
196/196 - 76s - loss: 150.1654 - MinusLogProbMetric: 150.1654 - val_loss: 146.7106 - val_MinusLogProbMetric: 146.7106 - lr: 4.1152e-06 - 76s/epoch - 388ms/step
Epoch 34/1000
2023-10-25 15:37:52.147 
Epoch 34/1000 
	 loss: 143.5596, MinusLogProbMetric: 143.5596, val_loss: 141.3310, val_MinusLogProbMetric: 141.3310

Epoch 34: val_loss did not improve from 115.21636
196/196 - 78s - loss: 143.5596 - MinusLogProbMetric: 143.5596 - val_loss: 141.3310 - val_MinusLogProbMetric: 141.3310 - lr: 4.1152e-06 - 78s/epoch - 397ms/step
Epoch 35/1000
2023-10-25 15:39:10.072 
Epoch 35/1000 
	 loss: 139.1373, MinusLogProbMetric: 139.1373, val_loss: 138.2495, val_MinusLogProbMetric: 138.2495

Epoch 35: val_loss did not improve from 115.21636
196/196 - 78s - loss: 139.1373 - MinusLogProbMetric: 139.1373 - val_loss: 138.2495 - val_MinusLogProbMetric: 138.2495 - lr: 4.1152e-06 - 78s/epoch - 398ms/step
Epoch 36/1000
2023-10-25 15:40:27.918 
Epoch 36/1000 
	 loss: 135.9657, MinusLogProbMetric: 135.9657, val_loss: 134.5887, val_MinusLogProbMetric: 134.5887

Epoch 36: val_loss did not improve from 115.21636
196/196 - 78s - loss: 135.9657 - MinusLogProbMetric: 135.9657 - val_loss: 134.5887 - val_MinusLogProbMetric: 134.5887 - lr: 4.1152e-06 - 78s/epoch - 397ms/step
Epoch 37/1000
2023-10-25 15:41:46.638 
Epoch 37/1000 
	 loss: 133.6166, MinusLogProbMetric: 133.6166, val_loss: 133.0415, val_MinusLogProbMetric: 133.0415

Epoch 37: val_loss did not improve from 115.21636
196/196 - 79s - loss: 133.6166 - MinusLogProbMetric: 133.6166 - val_loss: 133.0415 - val_MinusLogProbMetric: 133.0415 - lr: 4.1152e-06 - 79s/epoch - 402ms/step
Epoch 38/1000
2023-10-25 15:43:05.386 
Epoch 38/1000 
	 loss: 131.9682, MinusLogProbMetric: 131.9682, val_loss: 130.0133, val_MinusLogProbMetric: 130.0133

Epoch 38: val_loss did not improve from 115.21636
196/196 - 79s - loss: 131.9682 - MinusLogProbMetric: 131.9682 - val_loss: 130.0133 - val_MinusLogProbMetric: 130.0133 - lr: 4.1152e-06 - 79s/epoch - 402ms/step
Epoch 39/1000
2023-10-25 15:44:26.847 
Epoch 39/1000 
	 loss: 129.3680, MinusLogProbMetric: 129.3680, val_loss: 127.6940, val_MinusLogProbMetric: 127.6940

Epoch 39: val_loss did not improve from 115.21636
196/196 - 81s - loss: 129.3680 - MinusLogProbMetric: 129.3680 - val_loss: 127.6940 - val_MinusLogProbMetric: 127.6940 - lr: 4.1152e-06 - 81s/epoch - 416ms/step
Epoch 40/1000
2023-10-25 15:45:46.692 
Epoch 40/1000 
	 loss: 126.2601, MinusLogProbMetric: 126.2601, val_loss: 125.3742, val_MinusLogProbMetric: 125.3742

Epoch 40: val_loss did not improve from 115.21636
196/196 - 80s - loss: 126.2601 - MinusLogProbMetric: 126.2601 - val_loss: 125.3742 - val_MinusLogProbMetric: 125.3742 - lr: 4.1152e-06 - 80s/epoch - 407ms/step
Epoch 41/1000
2023-10-25 15:47:03.184 
Epoch 41/1000 
	 loss: 124.2682, MinusLogProbMetric: 124.2682, val_loss: 123.6925, val_MinusLogProbMetric: 123.6925

Epoch 41: val_loss did not improve from 115.21636
196/196 - 76s - loss: 124.2682 - MinusLogProbMetric: 124.2682 - val_loss: 123.6925 - val_MinusLogProbMetric: 123.6925 - lr: 4.1152e-06 - 76s/epoch - 390ms/step
Epoch 42/1000
2023-10-25 15:48:20.776 
Epoch 42/1000 
	 loss: 122.6605, MinusLogProbMetric: 122.6605, val_loss: 122.2282, val_MinusLogProbMetric: 122.2282

Epoch 42: val_loss did not improve from 115.21636
196/196 - 78s - loss: 122.6605 - MinusLogProbMetric: 122.6605 - val_loss: 122.2282 - val_MinusLogProbMetric: 122.2282 - lr: 4.1152e-06 - 78s/epoch - 396ms/step
Epoch 43/1000
2023-10-25 15:49:42.285 
Epoch 43/1000 
	 loss: 131.0595, MinusLogProbMetric: 131.0595, val_loss: 123.9910, val_MinusLogProbMetric: 123.9910

Epoch 43: val_loss did not improve from 115.21636
196/196 - 82s - loss: 131.0595 - MinusLogProbMetric: 131.0595 - val_loss: 123.9910 - val_MinusLogProbMetric: 123.9910 - lr: 4.1152e-06 - 82s/epoch - 416ms/step
Epoch 44/1000
2023-10-25 15:51:00.136 
Epoch 44/1000 
	 loss: 121.9610, MinusLogProbMetric: 121.9610, val_loss: 120.5608, val_MinusLogProbMetric: 120.5608

Epoch 44: val_loss did not improve from 115.21636
196/196 - 78s - loss: 121.9610 - MinusLogProbMetric: 121.9610 - val_loss: 120.5608 - val_MinusLogProbMetric: 120.5608 - lr: 4.1152e-06 - 78s/epoch - 397ms/step
Epoch 45/1000
2023-10-25 15:52:19.536 
Epoch 45/1000 
	 loss: 119.3730, MinusLogProbMetric: 119.3730, val_loss: 118.8801, val_MinusLogProbMetric: 118.8801

Epoch 45: val_loss did not improve from 115.21636
196/196 - 79s - loss: 119.3730 - MinusLogProbMetric: 119.3730 - val_loss: 118.8801 - val_MinusLogProbMetric: 118.8801 - lr: 4.1152e-06 - 79s/epoch - 405ms/step
Epoch 46/1000
2023-10-25 15:53:37.318 
Epoch 46/1000 
	 loss: 117.8324, MinusLogProbMetric: 117.8324, val_loss: 117.4479, val_MinusLogProbMetric: 117.4479

Epoch 46: val_loss did not improve from 115.21636
196/196 - 78s - loss: 117.8324 - MinusLogProbMetric: 117.8324 - val_loss: 117.4479 - val_MinusLogProbMetric: 117.4479 - lr: 4.1152e-06 - 78s/epoch - 397ms/step
Epoch 47/1000
2023-10-25 15:54:53.244 
Epoch 47/1000 
	 loss: 116.5937, MinusLogProbMetric: 116.5937, val_loss: 116.2792, val_MinusLogProbMetric: 116.2792

Epoch 47: val_loss did not improve from 115.21636
196/196 - 76s - loss: 116.5937 - MinusLogProbMetric: 116.5937 - val_loss: 116.2792 - val_MinusLogProbMetric: 116.2792 - lr: 4.1152e-06 - 76s/epoch - 387ms/step
Epoch 48/1000
2023-10-25 15:56:10.862 
Epoch 48/1000 
	 loss: 115.4216, MinusLogProbMetric: 115.4216, val_loss: 115.1292, val_MinusLogProbMetric: 115.1292

Epoch 48: val_loss improved from 115.21636 to 115.12925, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 115.4216 - MinusLogProbMetric: 115.4216 - val_loss: 115.1292 - val_MinusLogProbMetric: 115.1292 - lr: 4.1152e-06 - 79s/epoch - 402ms/step
Epoch 49/1000
2023-10-25 15:57:29.352 
Epoch 49/1000 
	 loss: 114.3987, MinusLogProbMetric: 114.3987, val_loss: 114.1972, val_MinusLogProbMetric: 114.1972

Epoch 49: val_loss improved from 115.12925 to 114.19724, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 114.3987 - MinusLogProbMetric: 114.3987 - val_loss: 114.1972 - val_MinusLogProbMetric: 114.1972 - lr: 4.1152e-06 - 78s/epoch - 400ms/step
Epoch 50/1000
2023-10-25 15:58:47.634 
Epoch 50/1000 
	 loss: 113.6260, MinusLogProbMetric: 113.6260, val_loss: 113.5194, val_MinusLogProbMetric: 113.5194

Epoch 50: val_loss improved from 114.19724 to 113.51939, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 113.6260 - MinusLogProbMetric: 113.6260 - val_loss: 113.5194 - val_MinusLogProbMetric: 113.5194 - lr: 4.1152e-06 - 78s/epoch - 400ms/step
Epoch 51/1000
2023-10-25 16:00:06.084 
Epoch 51/1000 
	 loss: 112.9649, MinusLogProbMetric: 112.9649, val_loss: 112.4564, val_MinusLogProbMetric: 112.4564

Epoch 51: val_loss improved from 113.51939 to 112.45643, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 78s - loss: 112.9649 - MinusLogProbMetric: 112.9649 - val_loss: 112.4564 - val_MinusLogProbMetric: 112.4564 - lr: 4.1152e-06 - 78s/epoch - 399ms/step
Epoch 52/1000
2023-10-25 16:01:25.490 
Epoch 52/1000 
	 loss: 111.8600, MinusLogProbMetric: 111.8600, val_loss: 111.8790, val_MinusLogProbMetric: 111.8790

Epoch 52: val_loss improved from 112.45643 to 111.87901, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 111.8600 - MinusLogProbMetric: 111.8600 - val_loss: 111.8790 - val_MinusLogProbMetric: 111.8790 - lr: 4.1152e-06 - 80s/epoch - 406ms/step
Epoch 53/1000
2023-10-25 16:02:46.228 
Epoch 53/1000 
	 loss: 111.2793, MinusLogProbMetric: 111.2793, val_loss: 110.9150, val_MinusLogProbMetric: 110.9150

Epoch 53: val_loss improved from 111.87901 to 110.91499, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 81s - loss: 111.2793 - MinusLogProbMetric: 111.2793 - val_loss: 110.9150 - val_MinusLogProbMetric: 110.9150 - lr: 4.1152e-06 - 81s/epoch - 412ms/step
Epoch 54/1000
2023-10-25 16:04:05.336 
Epoch 54/1000 
	 loss: 110.2629, MinusLogProbMetric: 110.2629, val_loss: 110.0765, val_MinusLogProbMetric: 110.0765

Epoch 54: val_loss improved from 110.91499 to 110.07651, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 79s - loss: 110.2629 - MinusLogProbMetric: 110.2629 - val_loss: 110.0765 - val_MinusLogProbMetric: 110.0765 - lr: 4.1152e-06 - 79s/epoch - 405ms/step
Epoch 55/1000
2023-10-25 16:05:25.595 
Epoch 55/1000 
	 loss: 109.3803, MinusLogProbMetric: 109.3803, val_loss: 109.2112, val_MinusLogProbMetric: 109.2112

Epoch 55: val_loss improved from 110.07651 to 109.21124, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 80s - loss: 109.3803 - MinusLogProbMetric: 109.3803 - val_loss: 109.2112 - val_MinusLogProbMetric: 109.2112 - lr: 4.1152e-06 - 80s/epoch - 408ms/step
Epoch 56/1000
2023-10-25 16:06:29.567 
Epoch 56/1000 
	 loss: 108.6486, MinusLogProbMetric: 108.6486, val_loss: 108.7252, val_MinusLogProbMetric: 108.7252

Epoch 56: val_loss improved from 109.21124 to 108.72523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 64s - loss: 108.6486 - MinusLogProbMetric: 108.6486 - val_loss: 108.7252 - val_MinusLogProbMetric: 108.7252 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 57/1000
2023-10-25 16:07:36.016 
Epoch 57/1000 
	 loss: 107.9379, MinusLogProbMetric: 107.9379, val_loss: 108.1202, val_MinusLogProbMetric: 108.1202

Epoch 57: val_loss improved from 108.72523 to 108.12020, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 66s - loss: 107.9379 - MinusLogProbMetric: 107.9379 - val_loss: 108.1202 - val_MinusLogProbMetric: 108.1202 - lr: 4.1152e-06 - 66s/epoch - 338ms/step
Epoch 58/1000
2023-10-25 16:08:40.171 
Epoch 58/1000 
	 loss: 107.6545, MinusLogProbMetric: 107.6545, val_loss: 107.4336, val_MinusLogProbMetric: 107.4336

Epoch 58: val_loss improved from 108.12020 to 107.43362, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 64s - loss: 107.6545 - MinusLogProbMetric: 107.6545 - val_loss: 107.4336 - val_MinusLogProbMetric: 107.4336 - lr: 4.1152e-06 - 64s/epoch - 327ms/step
Epoch 59/1000
2023-10-25 16:09:44.061 
Epoch 59/1000 
	 loss: 106.7813, MinusLogProbMetric: 106.7813, val_loss: 106.7891, val_MinusLogProbMetric: 106.7891

Epoch 59: val_loss improved from 107.43362 to 106.78915, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 64s - loss: 106.7813 - MinusLogProbMetric: 106.7813 - val_loss: 106.7891 - val_MinusLogProbMetric: 106.7891 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 60/1000
2023-10-25 16:10:49.237 
Epoch 60/1000 
	 loss: 106.1209, MinusLogProbMetric: 106.1209, val_loss: 105.9007, val_MinusLogProbMetric: 105.9007

Epoch 60: val_loss improved from 106.78915 to 105.90069, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 65s - loss: 106.1209 - MinusLogProbMetric: 106.1209 - val_loss: 105.9007 - val_MinusLogProbMetric: 105.9007 - lr: 4.1152e-06 - 65s/epoch - 332ms/step
Epoch 61/1000
2023-10-25 16:11:56.090 
Epoch 61/1000 
	 loss: 105.8886, MinusLogProbMetric: 105.8886, val_loss: 106.2641, val_MinusLogProbMetric: 106.2641

Epoch 61: val_loss did not improve from 105.90069
196/196 - 66s - loss: 105.8886 - MinusLogProbMetric: 105.8886 - val_loss: 106.2641 - val_MinusLogProbMetric: 106.2641 - lr: 4.1152e-06 - 66s/epoch - 335ms/step
Epoch 62/1000
2023-10-25 16:13:03.300 
Epoch 62/1000 
	 loss: 105.1244, MinusLogProbMetric: 105.1244, val_loss: 105.0004, val_MinusLogProbMetric: 105.0004

Epoch 62: val_loss improved from 105.90069 to 105.00040, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 68s - loss: 105.1244 - MinusLogProbMetric: 105.1244 - val_loss: 105.0004 - val_MinusLogProbMetric: 105.0004 - lr: 4.1152e-06 - 68s/epoch - 349ms/step
Epoch 63/1000
2023-10-25 16:14:11.227 
Epoch 63/1000 
	 loss: 108.7869, MinusLogProbMetric: 108.7869, val_loss: 131.7742, val_MinusLogProbMetric: 131.7742

Epoch 63: val_loss did not improve from 105.00040
196/196 - 67s - loss: 108.7869 - MinusLogProbMetric: 108.7869 - val_loss: 131.7742 - val_MinusLogProbMetric: 131.7742 - lr: 4.1152e-06 - 67s/epoch - 341ms/step
Epoch 64/1000
2023-10-25 16:15:19.516 
Epoch 64/1000 
	 loss: 110.6464, MinusLogProbMetric: 110.6464, val_loss: 106.5468, val_MinusLogProbMetric: 106.5468

Epoch 64: val_loss did not improve from 105.00040
196/196 - 68s - loss: 110.6464 - MinusLogProbMetric: 110.6464 - val_loss: 106.5468 - val_MinusLogProbMetric: 106.5468 - lr: 4.1152e-06 - 68s/epoch - 348ms/step
Epoch 65/1000
2023-10-25 16:16:28.155 
Epoch 65/1000 
	 loss: 105.1606, MinusLogProbMetric: 105.1606, val_loss: 104.7421, val_MinusLogProbMetric: 104.7421

Epoch 65: val_loss improved from 105.00040 to 104.74215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 70s - loss: 105.1606 - MinusLogProbMetric: 105.1606 - val_loss: 104.7421 - val_MinusLogProbMetric: 104.7421 - lr: 4.1152e-06 - 70s/epoch - 356ms/step
Epoch 66/1000
2023-10-25 16:17:35.674 
Epoch 66/1000 
	 loss: 103.7965, MinusLogProbMetric: 103.7965, val_loss: 103.5230, val_MinusLogProbMetric: 103.5230

Epoch 66: val_loss improved from 104.74215 to 103.52302, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 68s - loss: 103.7965 - MinusLogProbMetric: 103.7965 - val_loss: 103.5230 - val_MinusLogProbMetric: 103.5230 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 67/1000
2023-10-25 16:18:47.456 
Epoch 67/1000 
	 loss: 102.9232, MinusLogProbMetric: 102.9232, val_loss: 102.7463, val_MinusLogProbMetric: 102.7463

Epoch 67: val_loss improved from 103.52302 to 102.74634, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 72s - loss: 102.9232 - MinusLogProbMetric: 102.9232 - val_loss: 102.7463 - val_MinusLogProbMetric: 102.7463 - lr: 4.1152e-06 - 72s/epoch - 366ms/step
Epoch 68/1000
2023-10-25 16:19:55.283 
Epoch 68/1000 
	 loss: 102.1730, MinusLogProbMetric: 102.1730, val_loss: 102.3278, val_MinusLogProbMetric: 102.3278

Epoch 68: val_loss improved from 102.74634 to 102.32780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 68s - loss: 102.1730 - MinusLogProbMetric: 102.1730 - val_loss: 102.3278 - val_MinusLogProbMetric: 102.3278 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 69/1000
2023-10-25 16:21:04.732 
Epoch 69/1000 
	 loss: 101.6759, MinusLogProbMetric: 101.6759, val_loss: 102.1146, val_MinusLogProbMetric: 102.1146

Epoch 69: val_loss improved from 102.32780 to 102.11462, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 70s - loss: 101.6759 - MinusLogProbMetric: 101.6759 - val_loss: 102.1146 - val_MinusLogProbMetric: 102.1146 - lr: 4.1152e-06 - 70s/epoch - 355ms/step
Epoch 70/1000
2023-10-25 16:22:15.401 
Epoch 70/1000 
	 loss: 101.1793, MinusLogProbMetric: 101.1793, val_loss: 100.9561, val_MinusLogProbMetric: 100.9561

Epoch 70: val_loss improved from 102.11462 to 100.95608, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 71s - loss: 101.1793 - MinusLogProbMetric: 101.1793 - val_loss: 100.9561 - val_MinusLogProbMetric: 100.9561 - lr: 4.1152e-06 - 71s/epoch - 361ms/step
Epoch 71/1000
2023-10-25 16:23:24.629 
Epoch 71/1000 
	 loss: 100.6533, MinusLogProbMetric: 100.6533, val_loss: 100.7600, val_MinusLogProbMetric: 100.7600

Epoch 71: val_loss improved from 100.95608 to 100.76003, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 69s - loss: 100.6533 - MinusLogProbMetric: 100.6533 - val_loss: 100.7600 - val_MinusLogProbMetric: 100.7600 - lr: 4.1152e-06 - 69s/epoch - 353ms/step
Epoch 72/1000
2023-10-25 16:24:32.117 
Epoch 72/1000 
	 loss: 100.5131, MinusLogProbMetric: 100.5131, val_loss: 100.1938, val_MinusLogProbMetric: 100.1938

Epoch 72: val_loss improved from 100.76003 to 100.19376, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 68s - loss: 100.5131 - MinusLogProbMetric: 100.5131 - val_loss: 100.1938 - val_MinusLogProbMetric: 100.1938 - lr: 4.1152e-06 - 68s/epoch - 345ms/step
Epoch 73/1000
2023-10-25 16:25:45.862 
Epoch 73/1000 
	 loss: 99.7255, MinusLogProbMetric: 99.7255, val_loss: 99.6443, val_MinusLogProbMetric: 99.6443

Epoch 73: val_loss improved from 100.19376 to 99.64426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 74s - loss: 99.7255 - MinusLogProbMetric: 99.7255 - val_loss: 99.6443 - val_MinusLogProbMetric: 99.6443 - lr: 4.1152e-06 - 74s/epoch - 377ms/step
Epoch 74/1000
2023-10-25 16:26:55.635 
Epoch 74/1000 
	 loss: 99.1494, MinusLogProbMetric: 99.1494, val_loss: 99.2721, val_MinusLogProbMetric: 99.2721

Epoch 74: val_loss improved from 99.64426 to 99.27206, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 70s - loss: 99.1494 - MinusLogProbMetric: 99.1494 - val_loss: 99.2721 - val_MinusLogProbMetric: 99.2721 - lr: 4.1152e-06 - 70s/epoch - 355ms/step
Epoch 75/1000
2023-10-25 16:28:06.766 
Epoch 75/1000 
	 loss: 98.9141, MinusLogProbMetric: 98.9141, val_loss: 98.6891, val_MinusLogProbMetric: 98.6891

Epoch 75: val_loss improved from 99.27206 to 98.68910, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 71s - loss: 98.9141 - MinusLogProbMetric: 98.9141 - val_loss: 98.6891 - val_MinusLogProbMetric: 98.6891 - lr: 4.1152e-06 - 71s/epoch - 363ms/step
Epoch 76/1000
2023-10-25 16:29:14.147 
Epoch 76/1000 
	 loss: 98.4350, MinusLogProbMetric: 98.4350, val_loss: 98.5387, val_MinusLogProbMetric: 98.5387

Epoch 76: val_loss improved from 98.68910 to 98.53872, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_352/weights/best_weights.h5
196/196 - 67s - loss: 98.4350 - MinusLogProbMetric: 98.4350 - val_loss: 98.5387 - val_MinusLogProbMetric: 98.5387 - lr: 4.1152e-06 - 67s/epoch - 343ms/step
Epoch 77/1000
2023-10-25 16:30:21.750 
Epoch 77/1000 
	 loss: 117.5458, MinusLogProbMetric: 117.5458, val_loss: 116.2331, val_MinusLogProbMetric: 116.2331

Epoch 77: val_loss did not improve from 98.53872
196/196 - 67s - loss: 117.5458 - MinusLogProbMetric: 117.5458 - val_loss: 116.2331 - val_MinusLogProbMetric: 116.2331 - lr: 4.1152e-06 - 67s/epoch - 339ms/step
Epoch 78/1000
2023-10-25 16:31:29.570 
Epoch 78/1000 
	 loss: 111.0238, MinusLogProbMetric: 111.0238, val_loss: 108.8615, val_MinusLogProbMetric: 108.8615

Epoch 78: val_loss did not improve from 98.53872
196/196 - 68s - loss: 111.0238 - MinusLogProbMetric: 111.0238 - val_loss: 108.8615 - val_MinusLogProbMetric: 108.8615 - lr: 4.1152e-06 - 68s/epoch - 346ms/step
Epoch 79/1000
2023-10-25 16:32:51.809 
Epoch 79/1000 
	 loss: 236.0668, MinusLogProbMetric: 236.0668, val_loss: 292.7560, val_MinusLogProbMetric: 292.7560

Epoch 79: val_loss did not improve from 98.53872
196/196 - 82s - loss: 236.0668 - MinusLogProbMetric: 236.0668 - val_loss: 292.7560 - val_MinusLogProbMetric: 292.7560 - lr: 4.1152e-06 - 82s/epoch - 420ms/step
Epoch 80/1000
2023-10-25 16:34:04.881 
Epoch 80/1000 
	 loss: 236.3619, MinusLogProbMetric: 236.3619, val_loss: 207.4794, val_MinusLogProbMetric: 207.4794

Epoch 80: val_loss did not improve from 98.53872
196/196 - 73s - loss: 236.3619 - MinusLogProbMetric: 236.3619 - val_loss: 207.4794 - val_MinusLogProbMetric: 207.4794 - lr: 4.1152e-06 - 73s/epoch - 373ms/step
Epoch 81/1000
2023-10-25 16:35:16.804 
Epoch 81/1000 
	 loss: 192.3996, MinusLogProbMetric: 192.3996, val_loss: 180.9437, val_MinusLogProbMetric: 180.9437

Epoch 81: val_loss did not improve from 98.53872
196/196 - 72s - loss: 192.3996 - MinusLogProbMetric: 192.3996 - val_loss: 180.9437 - val_MinusLogProbMetric: 180.9437 - lr: 4.1152e-06 - 72s/epoch - 367ms/step
Epoch 82/1000
2023-10-25 16:36:27.189 
Epoch 82/1000 
	 loss: 174.9410, MinusLogProbMetric: 174.9410, val_loss: 167.8293, val_MinusLogProbMetric: 167.8293

Epoch 82: val_loss did not improve from 98.53872
196/196 - 70s - loss: 174.9410 - MinusLogProbMetric: 174.9410 - val_loss: 167.8293 - val_MinusLogProbMetric: 167.8293 - lr: 4.1152e-06 - 70s/epoch - 359ms/step
Epoch 83/1000
2023-10-25 16:37:40.273 
Epoch 83/1000 
	 loss: 163.5427, MinusLogProbMetric: 163.5427, val_loss: 159.3174, val_MinusLogProbMetric: 159.3174

Epoch 83: val_loss did not improve from 98.53872
196/196 - 73s - loss: 163.5427 - MinusLogProbMetric: 163.5427 - val_loss: 159.3174 - val_MinusLogProbMetric: 159.3174 - lr: 4.1152e-06 - 73s/epoch - 373ms/step
Epoch 84/1000
2023-10-25 16:38:51.022 
Epoch 84/1000 
	 loss: 172.4181, MinusLogProbMetric: 172.4181, val_loss: 163.2264, val_MinusLogProbMetric: 163.2264

Epoch 84: val_loss did not improve from 98.53872
196/196 - 71s - loss: 172.4181 - MinusLogProbMetric: 172.4181 - val_loss: 163.2264 - val_MinusLogProbMetric: 163.2264 - lr: 4.1152e-06 - 71s/epoch - 361ms/step
Epoch 85/1000
2023-10-25 16:40:02.269 
Epoch 85/1000 
	 loss: 157.5475, MinusLogProbMetric: 157.5475, val_loss: 153.8560, val_MinusLogProbMetric: 153.8560

Epoch 85: val_loss did not improve from 98.53872
196/196 - 71s - loss: 157.5475 - MinusLogProbMetric: 157.5475 - val_loss: 153.8560 - val_MinusLogProbMetric: 153.8560 - lr: 4.1152e-06 - 71s/epoch - 363ms/step
Epoch 86/1000
2023-10-25 16:41:22.041 
Epoch 86/1000 
	 loss: 151.3287, MinusLogProbMetric: 151.3287, val_loss: 149.4478, val_MinusLogProbMetric: 149.4478

Epoch 86: val_loss did not improve from 98.53872
196/196 - 80s - loss: 151.3287 - MinusLogProbMetric: 151.3287 - val_loss: 149.4478 - val_MinusLogProbMetric: 149.4478 - lr: 4.1152e-06 - 80s/epoch - 407ms/step
Epoch 87/1000
2023-10-25 16:42:34.131 
Epoch 87/1000 
	 loss: 147.4323, MinusLogProbMetric: 147.4323, val_loss: 146.0753, val_MinusLogProbMetric: 146.0753

Epoch 87: val_loss did not improve from 98.53872
196/196 - 72s - loss: 147.4323 - MinusLogProbMetric: 147.4323 - val_loss: 146.0753 - val_MinusLogProbMetric: 146.0753 - lr: 4.1152e-06 - 72s/epoch - 368ms/step
Epoch 88/1000
2023-10-25 16:43:52.974 
Epoch 88/1000 
	 loss: 144.6225, MinusLogProbMetric: 144.6225, val_loss: 143.6101, val_MinusLogProbMetric: 143.6101

Epoch 88: val_loss did not improve from 98.53872
196/196 - 79s - loss: 144.6225 - MinusLogProbMetric: 144.6225 - val_loss: 143.6101 - val_MinusLogProbMetric: 143.6101 - lr: 4.1152e-06 - 79s/epoch - 402ms/step
Epoch 89/1000
2023-10-25 16:45:19.264 
Epoch 89/1000 
	 loss: 142.4266, MinusLogProbMetric: 142.4266, val_loss: 141.5470, val_MinusLogProbMetric: 141.5470

Epoch 89: val_loss did not improve from 98.53872
196/196 - 86s - loss: 142.4266 - MinusLogProbMetric: 142.4266 - val_loss: 141.5470 - val_MinusLogProbMetric: 141.5470 - lr: 4.1152e-06 - 86s/epoch - 440ms/step
Epoch 90/1000
2023-10-25 16:46:45.928 
Epoch 90/1000 
	 loss: 140.4232, MinusLogProbMetric: 140.4232, val_loss: 139.9178, val_MinusLogProbMetric: 139.9178

Epoch 90: val_loss did not improve from 98.53872
196/196 - 87s - loss: 140.4232 - MinusLogProbMetric: 140.4232 - val_loss: 139.9178 - val_MinusLogProbMetric: 139.9178 - lr: 4.1152e-06 - 87s/epoch - 442ms/step
Epoch 91/1000
2023-10-25 16:48:12.450 
Epoch 91/1000 
	 loss: 138.0616, MinusLogProbMetric: 138.0616, val_loss: 136.8492, val_MinusLogProbMetric: 136.8492

Epoch 91: val_loss did not improve from 98.53872
196/196 - 87s - loss: 138.0616 - MinusLogProbMetric: 138.0616 - val_loss: 136.8492 - val_MinusLogProbMetric: 136.8492 - lr: 4.1152e-06 - 87s/epoch - 441ms/step
Epoch 92/1000
2023-10-25 16:49:37.375 
Epoch 92/1000 
	 loss: 137.9269, MinusLogProbMetric: 137.9269, val_loss: 137.1122, val_MinusLogProbMetric: 137.1122

Epoch 92: val_loss did not improve from 98.53872
196/196 - 85s - loss: 137.9269 - MinusLogProbMetric: 137.9269 - val_loss: 137.1122 - val_MinusLogProbMetric: 137.1122 - lr: 4.1152e-06 - 85s/epoch - 433ms/step
Epoch 93/1000
2023-10-25 16:50:59.539 
Epoch 93/1000 
	 loss: 136.3542, MinusLogProbMetric: 136.3542, val_loss: 136.1229, val_MinusLogProbMetric: 136.1229

Epoch 93: val_loss did not improve from 98.53872
196/196 - 82s - loss: 136.3542 - MinusLogProbMetric: 136.3542 - val_loss: 136.1229 - val_MinusLogProbMetric: 136.1229 - lr: 4.1152e-06 - 82s/epoch - 419ms/step
Epoch 94/1000
2023-10-25 16:52:19.228 
Epoch 94/1000 
	 loss: 135.1444, MinusLogProbMetric: 135.1444, val_loss: 134.7894, val_MinusLogProbMetric: 134.7894

Epoch 94: val_loss did not improve from 98.53872
196/196 - 80s - loss: 135.1444 - MinusLogProbMetric: 135.1444 - val_loss: 134.7894 - val_MinusLogProbMetric: 134.7894 - lr: 4.1152e-06 - 80s/epoch - 407ms/step
Epoch 95/1000
2023-10-25 16:53:43.282 
Epoch 95/1000 
	 loss: 136.1877, MinusLogProbMetric: 136.1877, val_loss: 141.7954, val_MinusLogProbMetric: 141.7954

Epoch 95: val_loss did not improve from 98.53872
196/196 - 84s - loss: 136.1877 - MinusLogProbMetric: 136.1877 - val_loss: 141.7954 - val_MinusLogProbMetric: 141.7954 - lr: 4.1152e-06 - 84s/epoch - 429ms/step
Epoch 96/1000
2023-10-25 16:55:09.305 
Epoch 96/1000 
	 loss: 134.1483, MinusLogProbMetric: 134.1483, val_loss: 132.1305, val_MinusLogProbMetric: 132.1305

Epoch 96: val_loss did not improve from 98.53872
196/196 - 86s - loss: 134.1483 - MinusLogProbMetric: 134.1483 - val_loss: 132.1305 - val_MinusLogProbMetric: 132.1305 - lr: 4.1152e-06 - 86s/epoch - 439ms/step
Epoch 97/1000
2023-10-25 16:56:30.838 
Epoch 97/1000 
	 loss: 131.3013, MinusLogProbMetric: 131.3013, val_loss: 130.5526, val_MinusLogProbMetric: 130.5526

Epoch 97: val_loss did not improve from 98.53872
196/196 - 82s - loss: 131.3013 - MinusLogProbMetric: 131.3013 - val_loss: 130.5526 - val_MinusLogProbMetric: 130.5526 - lr: 4.1152e-06 - 82s/epoch - 416ms/step
Epoch 98/1000
2023-10-25 16:57:53.973 
Epoch 98/1000 
	 loss: 133.2755, MinusLogProbMetric: 133.2755, val_loss: 130.0350, val_MinusLogProbMetric: 130.0350

Epoch 98: val_loss did not improve from 98.53872
196/196 - 83s - loss: 133.2755 - MinusLogProbMetric: 133.2755 - val_loss: 130.0350 - val_MinusLogProbMetric: 130.0350 - lr: 4.1152e-06 - 83s/epoch - 424ms/step
Epoch 99/1000
2023-10-25 16:59:19.788 
Epoch 99/1000 
	 loss: 131.3034, MinusLogProbMetric: 131.3034, val_loss: 130.7190, val_MinusLogProbMetric: 130.7190

Epoch 99: val_loss did not improve from 98.53872
196/196 - 86s - loss: 131.3034 - MinusLogProbMetric: 131.3034 - val_loss: 130.7190 - val_MinusLogProbMetric: 130.7190 - lr: 4.1152e-06 - 86s/epoch - 438ms/step
Epoch 100/1000
2023-10-25 17:00:43.916 
Epoch 100/1000 
	 loss: 130.5912, MinusLogProbMetric: 130.5912, val_loss: 130.0048, val_MinusLogProbMetric: 130.0048

Epoch 100: val_loss did not improve from 98.53872
196/196 - 84s - loss: 130.5912 - MinusLogProbMetric: 130.5912 - val_loss: 130.0048 - val_MinusLogProbMetric: 130.0048 - lr: 4.1152e-06 - 84s/epoch - 429ms/step
Epoch 101/1000
2023-10-25 17:02:08.424 
Epoch 101/1000 
	 loss: 129.3120, MinusLogProbMetric: 129.3120, val_loss: 128.6947, val_MinusLogProbMetric: 128.6947

Epoch 101: val_loss did not improve from 98.53872
196/196 - 85s - loss: 129.3120 - MinusLogProbMetric: 129.3120 - val_loss: 128.6947 - val_MinusLogProbMetric: 128.6947 - lr: 4.1152e-06 - 85s/epoch - 431ms/step
Epoch 102/1000
2023-10-25 17:03:34.492 
Epoch 102/1000 
	 loss: 127.7496, MinusLogProbMetric: 127.7496, val_loss: 127.2902, val_MinusLogProbMetric: 127.2902

Epoch 102: val_loss did not improve from 98.53872
196/196 - 86s - loss: 127.7496 - MinusLogProbMetric: 127.7496 - val_loss: 127.2902 - val_MinusLogProbMetric: 127.2902 - lr: 4.1152e-06 - 86s/epoch - 439ms/step
Epoch 103/1000
2023-10-25 17:04:56.164 
Epoch 103/1000 
	 loss: 126.0913, MinusLogProbMetric: 126.0913, val_loss: 125.7369, val_MinusLogProbMetric: 125.7369

Epoch 103: val_loss did not improve from 98.53872
196/196 - 82s - loss: 126.0913 - MinusLogProbMetric: 126.0913 - val_loss: 125.7369 - val_MinusLogProbMetric: 125.7369 - lr: 4.1152e-06 - 82s/epoch - 417ms/step
Epoch 104/1000
2023-10-25 17:06:20.437 
Epoch 104/1000 
	 loss: 123.8783, MinusLogProbMetric: 123.8783, val_loss: 122.8809, val_MinusLogProbMetric: 122.8809

Epoch 104: val_loss did not improve from 98.53872
196/196 - 84s - loss: 123.8783 - MinusLogProbMetric: 123.8783 - val_loss: 122.8809 - val_MinusLogProbMetric: 122.8809 - lr: 4.1152e-06 - 84s/epoch - 430ms/step
Epoch 105/1000
2023-10-25 17:07:45.601 
Epoch 105/1000 
	 loss: 122.0584, MinusLogProbMetric: 122.0584, val_loss: 122.0865, val_MinusLogProbMetric: 122.0865

Epoch 105: val_loss did not improve from 98.53872
196/196 - 85s - loss: 122.0584 - MinusLogProbMetric: 122.0584 - val_loss: 122.0865 - val_MinusLogProbMetric: 122.0865 - lr: 4.1152e-06 - 85s/epoch - 434ms/step
Epoch 106/1000
2023-10-25 17:09:08.729 
Epoch 106/1000 
	 loss: 121.1454, MinusLogProbMetric: 121.1454, val_loss: 120.8960, val_MinusLogProbMetric: 120.8960

Epoch 106: val_loss did not improve from 98.53872
196/196 - 83s - loss: 121.1454 - MinusLogProbMetric: 121.1454 - val_loss: 120.8960 - val_MinusLogProbMetric: 120.8960 - lr: 4.1152e-06 - 83s/epoch - 424ms/step
Epoch 107/1000
2023-10-25 17:10:32.361 
Epoch 107/1000 
	 loss: 120.2163, MinusLogProbMetric: 120.2163, val_loss: 119.8939, val_MinusLogProbMetric: 119.8939

Epoch 107: val_loss did not improve from 98.53872
196/196 - 84s - loss: 120.2163 - MinusLogProbMetric: 120.2163 - val_loss: 119.8939 - val_MinusLogProbMetric: 119.8939 - lr: 4.1152e-06 - 84s/epoch - 427ms/step
Epoch 108/1000
2023-10-25 17:11:51.236 
Epoch 108/1000 
	 loss: 119.1611, MinusLogProbMetric: 119.1611, val_loss: 118.8446, val_MinusLogProbMetric: 118.8446

Epoch 108: val_loss did not improve from 98.53872
196/196 - 79s - loss: 119.1611 - MinusLogProbMetric: 119.1611 - val_loss: 118.8446 - val_MinusLogProbMetric: 118.8446 - lr: 4.1152e-06 - 79s/epoch - 402ms/step
Epoch 109/1000
2023-10-25 17:13:14.764 
Epoch 109/1000 
	 loss: 118.2827, MinusLogProbMetric: 118.2827, val_loss: 118.6100, val_MinusLogProbMetric: 118.6100

Epoch 109: val_loss did not improve from 98.53872
196/196 - 84s - loss: 118.2827 - MinusLogProbMetric: 118.2827 - val_loss: 118.6100 - val_MinusLogProbMetric: 118.6100 - lr: 4.1152e-06 - 84s/epoch - 426ms/step
Epoch 110/1000
2023-10-25 17:14:37.765 
Epoch 110/1000 
	 loss: 125.7408, MinusLogProbMetric: 125.7408, val_loss: 132.4929, val_MinusLogProbMetric: 132.4929

Epoch 110: val_loss did not improve from 98.53872
196/196 - 83s - loss: 125.7408 - MinusLogProbMetric: 125.7408 - val_loss: 132.4929 - val_MinusLogProbMetric: 132.4929 - lr: 4.1152e-06 - 83s/epoch - 423ms/step
Epoch 111/1000
2023-10-25 17:15:59.958 
Epoch 111/1000 
	 loss: 120.2165, MinusLogProbMetric: 120.2165, val_loss: 115.7752, val_MinusLogProbMetric: 115.7752

Epoch 111: val_loss did not improve from 98.53872
196/196 - 82s - loss: 120.2165 - MinusLogProbMetric: 120.2165 - val_loss: 115.7752 - val_MinusLogProbMetric: 115.7752 - lr: 4.1152e-06 - 82s/epoch - 419ms/step
Epoch 112/1000
2023-10-25 17:17:24.134 
Epoch 112/1000 
	 loss: 114.7653, MinusLogProbMetric: 114.7653, val_loss: 114.2371, val_MinusLogProbMetric: 114.2371

Epoch 112: val_loss did not improve from 98.53872
196/196 - 84s - loss: 114.7653 - MinusLogProbMetric: 114.7653 - val_loss: 114.2371 - val_MinusLogProbMetric: 114.2371 - lr: 4.1152e-06 - 84s/epoch - 429ms/step
Epoch 113/1000
2023-10-25 17:18:46.973 
Epoch 113/1000 
	 loss: 114.9682, MinusLogProbMetric: 114.9682, val_loss: 114.2710, val_MinusLogProbMetric: 114.2710

Epoch 113: val_loss did not improve from 98.53872
196/196 - 83s - loss: 114.9682 - MinusLogProbMetric: 114.9682 - val_loss: 114.2710 - val_MinusLogProbMetric: 114.2710 - lr: 4.1152e-06 - 83s/epoch - 423ms/step
Epoch 114/1000
2023-10-25 17:20:10.581 
Epoch 114/1000 
	 loss: 118.8000, MinusLogProbMetric: 118.8000, val_loss: 190.4446, val_MinusLogProbMetric: 190.4446

Epoch 114: val_loss did not improve from 98.53872
196/196 - 84s - loss: 118.8000 - MinusLogProbMetric: 118.8000 - val_loss: 190.4446 - val_MinusLogProbMetric: 190.4446 - lr: 4.1152e-06 - 84s/epoch - 427ms/step
Epoch 115/1000
2023-10-25 17:21:34.242 
Epoch 115/1000 
	 loss: 138.1056, MinusLogProbMetric: 138.1056, val_loss: 125.3161, val_MinusLogProbMetric: 125.3161

Epoch 115: val_loss did not improve from 98.53872
196/196 - 84s - loss: 138.1056 - MinusLogProbMetric: 138.1056 - val_loss: 125.3161 - val_MinusLogProbMetric: 125.3161 - lr: 4.1152e-06 - 84s/epoch - 427ms/step
Epoch 116/1000
2023-10-25 17:22:57.088 
Epoch 116/1000 
	 loss: 122.3947, MinusLogProbMetric: 122.3947, val_loss: 120.9905, val_MinusLogProbMetric: 120.9905

Epoch 116: val_loss did not improve from 98.53872
196/196 - 83s - loss: 122.3947 - MinusLogProbMetric: 122.3947 - val_loss: 120.9905 - val_MinusLogProbMetric: 120.9905 - lr: 4.1152e-06 - 83s/epoch - 423ms/step
Epoch 117/1000
2023-10-25 17:24:19.873 
Epoch 117/1000 
	 loss: 119.7927, MinusLogProbMetric: 119.7927, val_loss: 119.0135, val_MinusLogProbMetric: 119.0135

Epoch 117: val_loss did not improve from 98.53872
196/196 - 83s - loss: 119.7927 - MinusLogProbMetric: 119.7927 - val_loss: 119.0135 - val_MinusLogProbMetric: 119.0135 - lr: 4.1152e-06 - 83s/epoch - 422ms/step
Epoch 118/1000
2023-10-25 17:25:42.946 
Epoch 118/1000 
	 loss: 118.1134, MinusLogProbMetric: 118.1134, val_loss: 118.4646, val_MinusLogProbMetric: 118.4646

Epoch 118: val_loss did not improve from 98.53872
196/196 - 83s - loss: 118.1134 - MinusLogProbMetric: 118.1134 - val_loss: 118.4646 - val_MinusLogProbMetric: 118.4646 - lr: 4.1152e-06 - 83s/epoch - 424ms/step
Epoch 119/1000
2023-10-25 17:27:06.337 
Epoch 119/1000 
	 loss: 117.4812, MinusLogProbMetric: 117.4812, val_loss: 147.2726, val_MinusLogProbMetric: 147.2726

Epoch 119: val_loss did not improve from 98.53872
196/196 - 83s - loss: 117.4812 - MinusLogProbMetric: 117.4812 - val_loss: 147.2726 - val_MinusLogProbMetric: 147.2726 - lr: 4.1152e-06 - 83s/epoch - 425ms/step
Epoch 120/1000
2023-10-25 17:28:30.475 
Epoch 120/1000 
	 loss: 123.2262, MinusLogProbMetric: 123.2262, val_loss: 116.5472, val_MinusLogProbMetric: 116.5472

Epoch 120: val_loss did not improve from 98.53872
196/196 - 84s - loss: 123.2262 - MinusLogProbMetric: 123.2262 - val_loss: 116.5472 - val_MinusLogProbMetric: 116.5472 - lr: 4.1152e-06 - 84s/epoch - 429ms/step
Epoch 121/1000
2023-10-25 17:29:53.248 
Epoch 121/1000 
	 loss: 115.8938, MinusLogProbMetric: 115.8938, val_loss: 115.4499, val_MinusLogProbMetric: 115.4499

Epoch 121: val_loss did not improve from 98.53872
196/196 - 83s - loss: 115.8938 - MinusLogProbMetric: 115.8938 - val_loss: 115.4499 - val_MinusLogProbMetric: 115.4499 - lr: 4.1152e-06 - 83s/epoch - 422ms/step
Epoch 122/1000
2023-10-25 17:31:17.228 
Epoch 122/1000 
	 loss: 116.1293, MinusLogProbMetric: 116.1293, val_loss: 115.5477, val_MinusLogProbMetric: 115.5477

Epoch 122: val_loss did not improve from 98.53872
196/196 - 84s - loss: 116.1293 - MinusLogProbMetric: 116.1293 - val_loss: 115.5477 - val_MinusLogProbMetric: 115.5477 - lr: 4.1152e-06 - 84s/epoch - 428ms/step
Epoch 123/1000
2023-10-25 17:32:40.951 
Epoch 123/1000 
	 loss: 114.6871, MinusLogProbMetric: 114.6871, val_loss: 114.2851, val_MinusLogProbMetric: 114.2851

Epoch 123: val_loss did not improve from 98.53872
196/196 - 84s - loss: 114.6871 - MinusLogProbMetric: 114.6871 - val_loss: 114.2851 - val_MinusLogProbMetric: 114.2851 - lr: 4.1152e-06 - 84s/epoch - 427ms/step
Epoch 124/1000
2023-10-25 17:34:04.067 
Epoch 124/1000 
	 loss: 113.9416, MinusLogProbMetric: 113.9416, val_loss: 113.8549, val_MinusLogProbMetric: 113.8549

Epoch 124: val_loss did not improve from 98.53872
196/196 - 83s - loss: 113.9416 - MinusLogProbMetric: 113.9416 - val_loss: 113.8549 - val_MinusLogProbMetric: 113.8549 - lr: 4.1152e-06 - 83s/epoch - 424ms/step
Epoch 125/1000
2023-10-25 17:35:26.902 
Epoch 125/1000 
	 loss: 113.5598, MinusLogProbMetric: 113.5598, val_loss: 114.0267, val_MinusLogProbMetric: 114.0267

Epoch 125: val_loss did not improve from 98.53872
196/196 - 83s - loss: 113.5598 - MinusLogProbMetric: 113.5598 - val_loss: 114.0267 - val_MinusLogProbMetric: 114.0267 - lr: 4.1152e-06 - 83s/epoch - 423ms/step
Epoch 126/1000
2023-10-25 17:36:51.397 
Epoch 126/1000 
	 loss: 112.9596, MinusLogProbMetric: 112.9596, val_loss: 113.7897, val_MinusLogProbMetric: 113.7897

Epoch 126: val_loss did not improve from 98.53872
196/196 - 84s - loss: 112.9596 - MinusLogProbMetric: 112.9596 - val_loss: 113.7897 - val_MinusLogProbMetric: 113.7897 - lr: 4.1152e-06 - 84s/epoch - 431ms/step
Epoch 127/1000
2023-10-25 17:38:13.789 
Epoch 127/1000 
	 loss: 114.1547, MinusLogProbMetric: 114.1547, val_loss: 113.7608, val_MinusLogProbMetric: 113.7608

Epoch 127: val_loss did not improve from 98.53872
196/196 - 82s - loss: 114.1547 - MinusLogProbMetric: 114.1547 - val_loss: 113.7608 - val_MinusLogProbMetric: 113.7608 - lr: 2.0576e-06 - 82s/epoch - 420ms/step
Epoch 128/1000
2023-10-25 17:39:36.543 
Epoch 128/1000 
	 loss: 113.0611, MinusLogProbMetric: 113.0611, val_loss: 113.6728, val_MinusLogProbMetric: 113.6728

Epoch 128: val_loss did not improve from 98.53872
196/196 - 83s - loss: 113.0611 - MinusLogProbMetric: 113.0611 - val_loss: 113.6728 - val_MinusLogProbMetric: 113.6728 - lr: 2.0576e-06 - 83s/epoch - 422ms/step
Epoch 129/1000
2023-10-25 17:41:00.952 
Epoch 129/1000 
	 loss: 112.6786, MinusLogProbMetric: 112.6786, val_loss: 111.9921, val_MinusLogProbMetric: 111.9921

Epoch 129: val_loss did not improve from 98.53872
196/196 - 84s - loss: 112.6786 - MinusLogProbMetric: 112.6786 - val_loss: 111.9921 - val_MinusLogProbMetric: 111.9921 - lr: 2.0576e-06 - 84s/epoch - 431ms/step
Epoch 130/1000
2023-10-25 17:42:24.146 
Epoch 130/1000 
	 loss: 111.6259, MinusLogProbMetric: 111.6259, val_loss: 112.9884, val_MinusLogProbMetric: 112.9884

Epoch 130: val_loss did not improve from 98.53872
196/196 - 83s - loss: 111.6259 - MinusLogProbMetric: 111.6259 - val_loss: 112.9884 - val_MinusLogProbMetric: 112.9884 - lr: 2.0576e-06 - 83s/epoch - 424ms/step
Epoch 131/1000
2023-10-25 17:43:47.447 
Epoch 131/1000 
	 loss: 125.2414, MinusLogProbMetric: 125.2414, val_loss: 118.5634, val_MinusLogProbMetric: 118.5634

Epoch 131: val_loss did not improve from 98.53872
196/196 - 83s - loss: 125.2414 - MinusLogProbMetric: 125.2414 - val_loss: 118.5634 - val_MinusLogProbMetric: 118.5634 - lr: 2.0576e-06 - 83s/epoch - 425ms/step
Epoch 132/1000
2023-10-25 17:45:09.918 
Epoch 132/1000 
	 loss: 116.9559, MinusLogProbMetric: 116.9559, val_loss: 116.1357, val_MinusLogProbMetric: 116.1357

Epoch 132: val_loss did not improve from 98.53872
196/196 - 82s - loss: 116.9559 - MinusLogProbMetric: 116.9559 - val_loss: 116.1357 - val_MinusLogProbMetric: 116.1357 - lr: 2.0576e-06 - 82s/epoch - 421ms/step
Epoch 133/1000
2023-10-25 17:46:33.855 
Epoch 133/1000 
	 loss: 114.7374, MinusLogProbMetric: 114.7374, val_loss: 113.7285, val_MinusLogProbMetric: 113.7285

Epoch 133: val_loss did not improve from 98.53872
196/196 - 84s - loss: 114.7374 - MinusLogProbMetric: 114.7374 - val_loss: 113.7285 - val_MinusLogProbMetric: 113.7285 - lr: 2.0576e-06 - 84s/epoch - 428ms/step
Epoch 134/1000
2023-10-25 17:47:56.600 
Epoch 134/1000 
	 loss: 112.5716, MinusLogProbMetric: 112.5716, val_loss: 111.9894, val_MinusLogProbMetric: 111.9894

Epoch 134: val_loss did not improve from 98.53872
196/196 - 83s - loss: 112.5716 - MinusLogProbMetric: 112.5716 - val_loss: 111.9894 - val_MinusLogProbMetric: 111.9894 - lr: 2.0576e-06 - 83s/epoch - 422ms/step
Epoch 135/1000
2023-10-25 17:49:19.470 
Epoch 135/1000 
	 loss: 111.1884, MinusLogProbMetric: 111.1884, val_loss: 111.0780, val_MinusLogProbMetric: 111.0780

Epoch 135: val_loss did not improve from 98.53872
196/196 - 83s - loss: 111.1884 - MinusLogProbMetric: 111.1884 - val_loss: 111.0780 - val_MinusLogProbMetric: 111.0780 - lr: 2.0576e-06 - 83s/epoch - 423ms/step
Epoch 136/1000
2023-10-25 17:50:42.189 
Epoch 136/1000 
	 loss: 110.7135, MinusLogProbMetric: 110.7135, val_loss: 110.7765, val_MinusLogProbMetric: 110.7765

Epoch 136: val_loss did not improve from 98.53872
196/196 - 83s - loss: 110.7135 - MinusLogProbMetric: 110.7135 - val_loss: 110.7765 - val_MinusLogProbMetric: 110.7765 - lr: 2.0576e-06 - 83s/epoch - 422ms/step
Epoch 137/1000
2023-10-25 17:52:05.269 
Epoch 137/1000 
	 loss: 110.3588, MinusLogProbMetric: 110.3588, val_loss: 110.3880, val_MinusLogProbMetric: 110.3880

Epoch 137: val_loss did not improve from 98.53872
196/196 - 83s - loss: 110.3588 - MinusLogProbMetric: 110.3588 - val_loss: 110.3880 - val_MinusLogProbMetric: 110.3880 - lr: 2.0576e-06 - 83s/epoch - 424ms/step
Epoch 138/1000
2023-10-25 17:53:27.734 
Epoch 138/1000 
	 loss: 110.9585, MinusLogProbMetric: 110.9585, val_loss: 111.7558, val_MinusLogProbMetric: 111.7558

Epoch 138: val_loss did not improve from 98.53872
196/196 - 82s - loss: 110.9585 - MinusLogProbMetric: 110.9585 - val_loss: 111.7558 - val_MinusLogProbMetric: 111.7558 - lr: 2.0576e-06 - 82s/epoch - 421ms/step
Epoch 139/1000
2023-10-25 17:54:51.396 
Epoch 139/1000 
	 loss: 110.0773, MinusLogProbMetric: 110.0773, val_loss: 109.9574, val_MinusLogProbMetric: 109.9574

Epoch 139: val_loss did not improve from 98.53872
196/196 - 84s - loss: 110.0773 - MinusLogProbMetric: 110.0773 - val_loss: 109.9574 - val_MinusLogProbMetric: 109.9574 - lr: 2.0576e-06 - 84s/epoch - 427ms/step
Epoch 140/1000
2023-10-25 17:56:07.008 
Epoch 140/1000 
	 loss: 110.3067, MinusLogProbMetric: 110.3067, val_loss: 110.0399, val_MinusLogProbMetric: 110.0399

Epoch 140: val_loss did not improve from 98.53872
196/196 - 76s - loss: 110.3067 - MinusLogProbMetric: 110.3067 - val_loss: 110.0399 - val_MinusLogProbMetric: 110.0399 - lr: 2.0576e-06 - 76s/epoch - 386ms/step
Epoch 141/1000
2023-10-25 17:57:29.819 
Epoch 141/1000 
	 loss: 109.5776, MinusLogProbMetric: 109.5776, val_loss: 111.5423, val_MinusLogProbMetric: 111.5423

Epoch 141: val_loss did not improve from 98.53872
196/196 - 83s - loss: 109.5776 - MinusLogProbMetric: 109.5776 - val_loss: 111.5423 - val_MinusLogProbMetric: 111.5423 - lr: 2.0576e-06 - 83s/epoch - 422ms/step
Epoch 142/1000
2023-10-25 17:58:50.686 
Epoch 142/1000 
	 loss: 110.1045, MinusLogProbMetric: 110.1045, val_loss: 110.1450, val_MinusLogProbMetric: 110.1450

Epoch 142: val_loss did not improve from 98.53872
196/196 - 81s - loss: 110.1045 - MinusLogProbMetric: 110.1045 - val_loss: 110.1450 - val_MinusLogProbMetric: 110.1450 - lr: 2.0576e-06 - 81s/epoch - 413ms/step
Epoch 143/1000
2023-10-25 18:00:14.028 
Epoch 143/1000 
	 loss: 109.5397, MinusLogProbMetric: 109.5397, val_loss: 109.2728, val_MinusLogProbMetric: 109.2728

Epoch 143: val_loss did not improve from 98.53872
196/196 - 83s - loss: 109.5397 - MinusLogProbMetric: 109.5397 - val_loss: 109.2728 - val_MinusLogProbMetric: 109.2728 - lr: 2.0576e-06 - 83s/epoch - 425ms/step
Epoch 144/1000
2023-10-25 18:01:35.322 
Epoch 144/1000 
	 loss: 108.9749, MinusLogProbMetric: 108.9749, val_loss: 109.0842, val_MinusLogProbMetric: 109.0842

Epoch 144: val_loss did not improve from 98.53872
196/196 - 81s - loss: 108.9749 - MinusLogProbMetric: 108.9749 - val_loss: 109.0842 - val_MinusLogProbMetric: 109.0842 - lr: 2.0576e-06 - 81s/epoch - 415ms/step
Epoch 145/1000
2023-10-25 18:02:53.241 
Epoch 145/1000 
	 loss: 108.6897, MinusLogProbMetric: 108.6897, val_loss: 108.8577, val_MinusLogProbMetric: 108.8577

Epoch 145: val_loss did not improve from 98.53872
196/196 - 78s - loss: 108.6897 - MinusLogProbMetric: 108.6897 - val_loss: 108.8577 - val_MinusLogProbMetric: 108.8577 - lr: 2.0576e-06 - 78s/epoch - 398ms/step
Epoch 146/1000
2023-10-25 18:04:14.543 
Epoch 146/1000 
	 loss: 108.9028, MinusLogProbMetric: 108.9028, val_loss: 108.7830, val_MinusLogProbMetric: 108.7830

Epoch 146: val_loss did not improve from 98.53872
196/196 - 81s - loss: 108.9028 - MinusLogProbMetric: 108.9028 - val_loss: 108.7830 - val_MinusLogProbMetric: 108.7830 - lr: 2.0576e-06 - 81s/epoch - 415ms/step
Epoch 147/1000
2023-10-25 18:05:34.553 
Epoch 147/1000 
	 loss: 108.3634, MinusLogProbMetric: 108.3634, val_loss: 108.6936, val_MinusLogProbMetric: 108.6936

Epoch 147: val_loss did not improve from 98.53872
196/196 - 80s - loss: 108.3634 - MinusLogProbMetric: 108.3634 - val_loss: 108.6936 - val_MinusLogProbMetric: 108.6936 - lr: 2.0576e-06 - 80s/epoch - 408ms/step
Epoch 148/1000
2023-10-25 18:06:58.025 
Epoch 148/1000 
	 loss: 108.2022, MinusLogProbMetric: 108.2022, val_loss: 108.4003, val_MinusLogProbMetric: 108.4003

Epoch 148: val_loss did not improve from 98.53872
196/196 - 83s - loss: 108.2022 - MinusLogProbMetric: 108.2022 - val_loss: 108.4003 - val_MinusLogProbMetric: 108.4003 - lr: 2.0576e-06 - 83s/epoch - 426ms/step
Epoch 149/1000
2023-10-25 18:08:22.374 
Epoch 149/1000 
	 loss: 108.1501, MinusLogProbMetric: 108.1501, val_loss: 108.3258, val_MinusLogProbMetric: 108.3258

Epoch 149: val_loss did not improve from 98.53872
196/196 - 84s - loss: 108.1501 - MinusLogProbMetric: 108.1501 - val_loss: 108.3258 - val_MinusLogProbMetric: 108.3258 - lr: 2.0576e-06 - 84s/epoch - 430ms/step
Epoch 150/1000
2023-10-25 18:09:45.984 
Epoch 150/1000 
	 loss: 107.9342, MinusLogProbMetric: 107.9342, val_loss: 108.1224, val_MinusLogProbMetric: 108.1224

Epoch 150: val_loss did not improve from 98.53872
196/196 - 84s - loss: 107.9342 - MinusLogProbMetric: 107.9342 - val_loss: 108.1224 - val_MinusLogProbMetric: 108.1224 - lr: 2.0576e-06 - 84s/epoch - 427ms/step
Epoch 151/1000
2023-10-25 18:11:05.054 
Epoch 151/1000 
	 loss: 107.7264, MinusLogProbMetric: 107.7264, val_loss: 107.9879, val_MinusLogProbMetric: 107.9879

Epoch 151: val_loss did not improve from 98.53872
196/196 - 79s - loss: 107.7264 - MinusLogProbMetric: 107.7264 - val_loss: 107.9879 - val_MinusLogProbMetric: 107.9879 - lr: 2.0576e-06 - 79s/epoch - 403ms/step
Epoch 152/1000
2023-10-25 18:12:27.901 
Epoch 152/1000 
	 loss: 107.6409, MinusLogProbMetric: 107.6409, val_loss: 107.8414, val_MinusLogProbMetric: 107.8414

Epoch 152: val_loss did not improve from 98.53872
196/196 - 83s - loss: 107.6409 - MinusLogProbMetric: 107.6409 - val_loss: 107.8414 - val_MinusLogProbMetric: 107.8414 - lr: 2.0576e-06 - 83s/epoch - 423ms/step
Epoch 153/1000
2023-10-25 18:13:44.402 
Epoch 153/1000 
	 loss: 107.5438, MinusLogProbMetric: 107.5438, val_loss: 107.7036, val_MinusLogProbMetric: 107.7036

Epoch 153: val_loss did not improve from 98.53872
196/196 - 77s - loss: 107.5438 - MinusLogProbMetric: 107.5438 - val_loss: 107.7036 - val_MinusLogProbMetric: 107.7036 - lr: 2.0576e-06 - 77s/epoch - 390ms/step
Epoch 154/1000
2023-10-25 18:15:05.866 
Epoch 154/1000 
	 loss: 107.3721, MinusLogProbMetric: 107.3721, val_loss: 107.6797, val_MinusLogProbMetric: 107.6797

Epoch 154: val_loss did not improve from 98.53872
196/196 - 81s - loss: 107.3721 - MinusLogProbMetric: 107.3721 - val_loss: 107.6797 - val_MinusLogProbMetric: 107.6797 - lr: 2.0576e-06 - 81s/epoch - 416ms/step
Epoch 155/1000
2023-10-25 18:16:25.721 
Epoch 155/1000 
	 loss: 107.1644, MinusLogProbMetric: 107.1644, val_loss: 107.4267, val_MinusLogProbMetric: 107.4267

Epoch 155: val_loss did not improve from 98.53872
196/196 - 80s - loss: 107.1644 - MinusLogProbMetric: 107.1644 - val_loss: 107.4267 - val_MinusLogProbMetric: 107.4267 - lr: 2.0576e-06 - 80s/epoch - 407ms/step
Epoch 156/1000
2023-10-25 18:17:47.223 
Epoch 156/1000 
	 loss: 107.0201, MinusLogProbMetric: 107.0201, val_loss: 107.3841, val_MinusLogProbMetric: 107.3841

Epoch 156: val_loss did not improve from 98.53872
196/196 - 81s - loss: 107.0201 - MinusLogProbMetric: 107.0201 - val_loss: 107.3841 - val_MinusLogProbMetric: 107.3841 - lr: 2.0576e-06 - 81s/epoch - 416ms/step
Epoch 157/1000
2023-10-25 18:19:08.554 
Epoch 157/1000 
	 loss: 106.8833, MinusLogProbMetric: 106.8833, val_loss: 107.1453, val_MinusLogProbMetric: 107.1453

Epoch 157: val_loss did not improve from 98.53872
196/196 - 81s - loss: 106.8833 - MinusLogProbMetric: 106.8833 - val_loss: 107.1453 - val_MinusLogProbMetric: 107.1453 - lr: 2.0576e-06 - 81s/epoch - 415ms/step
Epoch 158/1000
2023-10-25 18:20:31.079 
Epoch 158/1000 
	 loss: 106.7287, MinusLogProbMetric: 106.7287, val_loss: 107.0051, val_MinusLogProbMetric: 107.0051

Epoch 158: val_loss did not improve from 98.53872
196/196 - 83s - loss: 106.7287 - MinusLogProbMetric: 106.7287 - val_loss: 107.0051 - val_MinusLogProbMetric: 107.0051 - lr: 2.0576e-06 - 83s/epoch - 421ms/step
Epoch 159/1000
2023-10-25 18:21:53.796 
Epoch 159/1000 
	 loss: 106.6255, MinusLogProbMetric: 106.6255, val_loss: 106.8993, val_MinusLogProbMetric: 106.8993

Epoch 159: val_loss did not improve from 98.53872
196/196 - 83s - loss: 106.6255 - MinusLogProbMetric: 106.6255 - val_loss: 106.8993 - val_MinusLogProbMetric: 106.8993 - lr: 2.0576e-06 - 83s/epoch - 422ms/step
Epoch 160/1000
2023-10-25 18:23:11.511 
Epoch 160/1000 
	 loss: 106.7427, MinusLogProbMetric: 106.7427, val_loss: 106.7627, val_MinusLogProbMetric: 106.7627

Epoch 160: val_loss did not improve from 98.53872
196/196 - 78s - loss: 106.7427 - MinusLogProbMetric: 106.7427 - val_loss: 106.7627 - val_MinusLogProbMetric: 106.7627 - lr: 2.0576e-06 - 78s/epoch - 396ms/step
Epoch 161/1000
2023-10-25 18:24:36.724 
Epoch 161/1000 
	 loss: 106.3603, MinusLogProbMetric: 106.3603, val_loss: 106.5904, val_MinusLogProbMetric: 106.5904

Epoch 161: val_loss did not improve from 98.53872
196/196 - 85s - loss: 106.3603 - MinusLogProbMetric: 106.3603 - val_loss: 106.5904 - val_MinusLogProbMetric: 106.5904 - lr: 2.0576e-06 - 85s/epoch - 435ms/step
Epoch 162/1000
2023-10-25 18:25:54.312 
Epoch 162/1000 
	 loss: 106.2151, MinusLogProbMetric: 106.2151, val_loss: 106.4535, val_MinusLogProbMetric: 106.4535

Epoch 162: val_loss did not improve from 98.53872
196/196 - 78s - loss: 106.2151 - MinusLogProbMetric: 106.2151 - val_loss: 106.4535 - val_MinusLogProbMetric: 106.4535 - lr: 2.0576e-06 - 78s/epoch - 396ms/step
Epoch 163/1000
2023-10-25 18:27:17.668 
Epoch 163/1000 
	 loss: 106.0978, MinusLogProbMetric: 106.0978, val_loss: 106.4224, val_MinusLogProbMetric: 106.4224

Epoch 163: val_loss did not improve from 98.53872
196/196 - 83s - loss: 106.0978 - MinusLogProbMetric: 106.0978 - val_loss: 106.4224 - val_MinusLogProbMetric: 106.4224 - lr: 2.0576e-06 - 83s/epoch - 425ms/step
Epoch 164/1000
2023-10-25 18:28:36.788 
Epoch 164/1000 
	 loss: 105.9580, MinusLogProbMetric: 105.9580, val_loss: 106.2330, val_MinusLogProbMetric: 106.2330

Epoch 164: val_loss did not improve from 98.53872
196/196 - 79s - loss: 105.9580 - MinusLogProbMetric: 105.9580 - val_loss: 106.2330 - val_MinusLogProbMetric: 106.2330 - lr: 2.0576e-06 - 79s/epoch - 404ms/step
Epoch 165/1000
2023-10-25 18:30:00.621 
Epoch 165/1000 
	 loss: 105.8165, MinusLogProbMetric: 105.8165, val_loss: 106.1045, val_MinusLogProbMetric: 106.1045

Epoch 165: val_loss did not improve from 98.53872
196/196 - 84s - loss: 105.8165 - MinusLogProbMetric: 105.8165 - val_loss: 106.1045 - val_MinusLogProbMetric: 106.1045 - lr: 2.0576e-06 - 84s/epoch - 428ms/step
Epoch 166/1000
2023-10-25 18:31:18.390 
Epoch 166/1000 
	 loss: 105.7140, MinusLogProbMetric: 105.7140, val_loss: 105.9466, val_MinusLogProbMetric: 105.9466

Epoch 166: val_loss did not improve from 98.53872
196/196 - 78s - loss: 105.7140 - MinusLogProbMetric: 105.7140 - val_loss: 105.9466 - val_MinusLogProbMetric: 105.9466 - lr: 2.0576e-06 - 78s/epoch - 397ms/step
Epoch 167/1000
2023-10-25 18:32:40.601 
Epoch 167/1000 
	 loss: 105.5485, MinusLogProbMetric: 105.5485, val_loss: 105.8097, val_MinusLogProbMetric: 105.8097

Epoch 167: val_loss did not improve from 98.53872
196/196 - 82s - loss: 105.5485 - MinusLogProbMetric: 105.5485 - val_loss: 105.8097 - val_MinusLogProbMetric: 105.8097 - lr: 2.0576e-06 - 82s/epoch - 419ms/step
Epoch 168/1000
2023-10-25 18:34:04.545 
Epoch 168/1000 
	 loss: 105.4120, MinusLogProbMetric: 105.4120, val_loss: 105.6665, val_MinusLogProbMetric: 105.6665

Epoch 168: val_loss did not improve from 98.53872
196/196 - 84s - loss: 105.4120 - MinusLogProbMetric: 105.4120 - val_loss: 105.6665 - val_MinusLogProbMetric: 105.6665 - lr: 2.0576e-06 - 84s/epoch - 428ms/step
Epoch 169/1000
2023-10-25 18:35:26.098 
Epoch 169/1000 
	 loss: 105.2895, MinusLogProbMetric: 105.2895, val_loss: 105.5685, val_MinusLogProbMetric: 105.5685

Epoch 169: val_loss did not improve from 98.53872
196/196 - 82s - loss: 105.2895 - MinusLogProbMetric: 105.2895 - val_loss: 105.5685 - val_MinusLogProbMetric: 105.5685 - lr: 2.0576e-06 - 82s/epoch - 416ms/step
Epoch 170/1000
2023-10-25 18:36:48.612 
Epoch 170/1000 
	 loss: 105.1577, MinusLogProbMetric: 105.1577, val_loss: 105.5825, val_MinusLogProbMetric: 105.5825

Epoch 170: val_loss did not improve from 98.53872
196/196 - 83s - loss: 105.1577 - MinusLogProbMetric: 105.1577 - val_loss: 105.5825 - val_MinusLogProbMetric: 105.5825 - lr: 2.0576e-06 - 83s/epoch - 421ms/step
Epoch 171/1000
2023-10-25 18:38:07.991 
Epoch 171/1000 
	 loss: 105.0317, MinusLogProbMetric: 105.0317, val_loss: 105.3156, val_MinusLogProbMetric: 105.3156

Epoch 171: val_loss did not improve from 98.53872
196/196 - 79s - loss: 105.0317 - MinusLogProbMetric: 105.0317 - val_loss: 105.3156 - val_MinusLogProbMetric: 105.3156 - lr: 2.0576e-06 - 79s/epoch - 405ms/step
Epoch 172/1000
2023-10-25 18:39:30.283 
Epoch 172/1000 
	 loss: 104.9541, MinusLogProbMetric: 104.9541, val_loss: 105.2313, val_MinusLogProbMetric: 105.2313

Epoch 172: val_loss did not improve from 98.53872
196/196 - 82s - loss: 104.9541 - MinusLogProbMetric: 104.9541 - val_loss: 105.2313 - val_MinusLogProbMetric: 105.2313 - lr: 2.0576e-06 - 82s/epoch - 420ms/step
Epoch 173/1000
2023-10-25 18:40:52.607 
Epoch 173/1000 
	 loss: 104.7973, MinusLogProbMetric: 104.7973, val_loss: 105.0806, val_MinusLogProbMetric: 105.0806

Epoch 173: val_loss did not improve from 98.53872
196/196 - 82s - loss: 104.7973 - MinusLogProbMetric: 104.7973 - val_loss: 105.0806 - val_MinusLogProbMetric: 105.0806 - lr: 2.0576e-06 - 82s/epoch - 420ms/step
Epoch 174/1000
2023-10-25 18:42:15.798 
Epoch 174/1000 
	 loss: 104.6417, MinusLogProbMetric: 104.6417, val_loss: 104.9597, val_MinusLogProbMetric: 104.9597

Epoch 174: val_loss did not improve from 98.53872
196/196 - 83s - loss: 104.6417 - MinusLogProbMetric: 104.6417 - val_loss: 104.9597 - val_MinusLogProbMetric: 104.9597 - lr: 2.0576e-06 - 83s/epoch - 424ms/step
Epoch 175/1000
2023-10-25 18:43:37.050 
Epoch 175/1000 
	 loss: 104.7063, MinusLogProbMetric: 104.7063, val_loss: 105.0433, val_MinusLogProbMetric: 105.0433

Epoch 175: val_loss did not improve from 98.53872
196/196 - 81s - loss: 104.7063 - MinusLogProbMetric: 104.7063 - val_loss: 105.0433 - val_MinusLogProbMetric: 105.0433 - lr: 2.0576e-06 - 81s/epoch - 415ms/step
Epoch 176/1000
2023-10-25 18:44:57.862 
Epoch 176/1000 
	 loss: 104.4616, MinusLogProbMetric: 104.4616, val_loss: 104.7954, val_MinusLogProbMetric: 104.7954

Epoch 176: val_loss did not improve from 98.53872
Restoring model weights from the end of the best epoch: 76.
196/196 - 81s - loss: 104.4616 - MinusLogProbMetric: 104.4616 - val_loss: 104.7954 - val_MinusLogProbMetric: 104.7954 - lr: 2.0576e-06 - 81s/epoch - 416ms/step
Epoch 176: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 440.
Model trained in 14122.86 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.65 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.92 s.
===========
Run 352/720 done in 34594.59 s.
===========

Directory ../../results/CsplineN_new/run_353/ already exists.
Skipping it.
===========
Run 353/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_354/ already exists.
Skipping it.
===========
Run 354/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_355/ already exists.
Skipping it.
===========
Run 355/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_356/ already exists.
Skipping it.
===========
Run 356/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_357/ already exists.
Skipping it.
===========
Run 357/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_358/ already exists.
Skipping it.
===========
Run 358/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_359/ already exists.
Skipping it.
===========
Run 359/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_360/ already exists.
Skipping it.
===========
Run 360/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_361/ already exists.
Skipping it.
===========
Run 361/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_362/ already exists.
Skipping it.
===========
Run 362/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_363/ already exists.
Skipping it.
===========
Run 363/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_364/ already exists.
Skipping it.
===========
Run 364/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_365/ already exists.
Skipping it.
===========
Run 365/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_366/ already exists.
Skipping it.
===========
Run 366/720 already exists. Skipping it.
===========

===========
Generating train data for run 367.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_367/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_367/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_367/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_367
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_435"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_436 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7fe3842230a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe3874e61a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe3874e61a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe7311ebc70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe3849e9a50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_367/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe3849e9fc0>, <keras.callbacks.ModelCheckpoint object at 0x7fe3849ea080>, <keras.callbacks.EarlyStopping object at 0x7fe3849ea2f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe3849ea320>, <keras.callbacks.TerminateOnNaN object at 0x7fe3849e9f60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_367/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 367/720 with hyperparameters:
timestamp = 2023-10-25 18:45:13.445032
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:48:51.329 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7703.5332, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 218s - loss: nan - MinusLogProbMetric: 7703.5332 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 218s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0003333333333333333.
===========
Generating train data for run 367.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_367/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_367/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_367/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_367
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_446"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_447 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7fe732ee1ba0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe732e57910>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe732e57910>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe49f8c2890>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe4f744cbb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_367/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe4f744d120>, <keras.callbacks.ModelCheckpoint object at 0x7fe4f744d1e0>, <keras.callbacks.EarlyStopping object at 0x7fe4f744d450>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe4f744d480>, <keras.callbacks.TerminateOnNaN object at 0x7fe4f744d0c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_367/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 367/720 with hyperparameters:
timestamp = 2023-10-25 18:49:03.464436
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:52:17.472 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7703.5332, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 194s - loss: nan - MinusLogProbMetric: 7703.5332 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 194s/epoch - 988ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0001111111111111111.
===========
Generating train data for run 367.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_367/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_367/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_367/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_367
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_457"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_458 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_42 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_42/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_42'")
self.model: <keras.engine.functional.Functional object at 0x7fe4dc2bdf00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe67c35a7d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe67c35a7d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe79220b400>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe6142afaf0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_367/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe6142affd0>, <keras.callbacks.ModelCheckpoint object at 0x7fe6142f4160>, <keras.callbacks.EarlyStopping object at 0x7fe6142f43d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe6142f4400>, <keras.callbacks.TerminateOnNaN object at 0x7fe6142f4040>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_367/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 367/720 with hyperparameters:
timestamp = 2023-10-25 18:52:30.576673
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:55:53.386 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7703.5332, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 203s - loss: nan - MinusLogProbMetric: 7703.5332 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 203s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 3.703703703703703e-05.
===========
Generating train data for run 367.
===========
Train data generated in 0.36 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_367/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_367/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_367/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_367
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_468"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_469 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_43 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_43/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_43'")
self.model: <keras.engine.functional.Functional object at 0x7fe4dd2ab430>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe792455ed0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe792455ed0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fee37dc7b20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe4e4887e50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_367/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe4e4886c80>, <keras.callbacks.ModelCheckpoint object at 0x7fe4e4886f50>, <keras.callbacks.EarlyStopping object at 0x7fe4e48866b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe4e4886aa0>, <keras.callbacks.TerminateOnNaN object at 0x7fe4e4886fb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_367/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 367/720 with hyperparameters:
timestamp = 2023-10-25 18:56:07.279373
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:59:24.867 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7703.5332, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 197s - loss: nan - MinusLogProbMetric: 7703.5332 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 197s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 1.2345679012345677e-05.
===========
Generating train data for run 367.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_367/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_367/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_367/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_367
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_479"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_480 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_44 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_44/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_44'")
self.model: <keras.engine.functional.Functional object at 0x7fe73231f670>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe37ced52d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe37ced52d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe731a90af0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe731af66e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_367/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe731af6c50>, <keras.callbacks.ModelCheckpoint object at 0x7fe731af6d10>, <keras.callbacks.EarlyStopping object at 0x7fe731af6f80>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe731af6fb0>, <keras.callbacks.TerminateOnNaN object at 0x7fe731af6bf0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_367/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 367/720 with hyperparameters:
timestamp = 2023-10-25 18:59:37.783749
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 19:03:21.901 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7703.5332, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 224s - loss: nan - MinusLogProbMetric: 7703.5332 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 224s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 4.115226337448558e-06.
===========
Generating train data for run 367.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_367/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_367/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_367/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_367
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_490"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_491 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_45 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_45/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_45'")
self.model: <keras.engine.functional.Functional object at 0x7fe4b4bd76a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe4e460a290>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe4e460a290>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe5d5bfa620>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe5f466e830>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_367/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe5f466eda0>, <keras.callbacks.ModelCheckpoint object at 0x7fe5f466ee60>, <keras.callbacks.EarlyStopping object at 0x7fe5f466f0d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe5f466f100>, <keras.callbacks.TerminateOnNaN object at 0x7fe5f466ed40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_367/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 367/720 with hyperparameters:
timestamp = 2023-10-25 19:03:36.350124
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 19:06:37.768 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7703.5332, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 181s - loss: nan - MinusLogProbMetric: 7703.5332 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 181s/epoch - 925ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 1.3717421124828526e-06.
===========
Generating train data for run 367.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_367/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_367/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_367/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_367
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_501"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_502 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_46 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_46/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_46'")
self.model: <keras.engine.functional.Functional object at 0x7fe43477f460>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe43477f6a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe43477f6a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe50534cee0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe3c54ed9f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_367/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe3c54341c0>, <keras.callbacks.ModelCheckpoint object at 0x7fe384664be0>, <keras.callbacks.EarlyStopping object at 0x7fe3846642b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe384664eb0>, <keras.callbacks.TerminateOnNaN object at 0x7fe3846656f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_367/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 367/720 with hyperparameters:
timestamp = 2023-10-25 19:07:01.891947
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 19:10:29.406 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7703.5332, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 207s - loss: nan - MinusLogProbMetric: 7703.5332 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 207s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 4.572473708276175e-07.
===========
Generating train data for run 367.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_367/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_367/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_367/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_367
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_512"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_513 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_47 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_47/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_47'")
self.model: <keras.engine.functional.Functional object at 0x7fe703b554b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fe7031c5090>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fe7031c5090>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fe703734940>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fe7030c0d00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_367/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fe7030c1270>, <keras.callbacks.ModelCheckpoint object at 0x7fe7030c1330>, <keras.callbacks.EarlyStopping object at 0x7fe7030c15a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fe7030c15d0>, <keras.callbacks.TerminateOnNaN object at 0x7fe7030c1210>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_367/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 367/720 with hyperparameters:
timestamp = 2023-10-25 19:10:43.289901
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
LLVM ERROR: pthread_create failed: Resource temporarily unavailable
