2023-09-26 13:23:17.101761: Importing os...
2023-09-26 13:23:17.101822: Importing sys...
2023-09-26 13:23:17.101836: Importing and initializing argparse...
Visible devices: [2]
2023-09-26 13:23:17.117803: Importing timer from timeit...
2023-09-26 13:23:17.118353: Setting env variables for tf import (only device [2] will be available)...
2023-09-26 13:23:17.118398: Importing numpy...
2023-09-26 13:23:17.295704: Importing pandas...
2023-09-26 13:23:17.478236: Importing shutil...
2023-09-26 13:23:17.478259: Importing subprocess...
2023-09-26 13:23:17.478267: Importing tensorflow...
Tensorflow version: 2.12.0
2023-09-26 13:23:19.584959: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-09-26 13:23:19.919693: Importing textwrap...
2023-09-26 13:23:19.919719: Importing timeit...
2023-09-26 13:23:19.919727: Importing traceback...
2023-09-26 13:23:19.919733: Importing typing...
2023-09-26 13:23:19.919741: Setting tf configs...
2023-09-26 13:23:20.180678: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-09-26 13:23:21.268828: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

===========
Generating train data for run 287.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_287/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_287/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.4544215 ,  3.75662   ,  8.587604  , ...,  7.2717385 ,
         2.2797804 ,  2.224709  ],
       [ 1.3902065 ,  3.433224  ,  7.9205    , ...,  7.378508  ,
         3.6055362 ,  1.7885203 ],
       [ 2.5590267 ,  3.4328253 ,  7.4715605 , ...,  7.496273  ,
         3.511426  ,  1.7360344 ],
       ...,
       [ 5.120887  ,  6.0070558 , -0.50225353, ...,  0.7586615 ,
         6.522565  ,  1.2877582 ],
       [ 4.695989  ,  6.091708  ,  0.05890873, ...,  1.4941858 ,
         6.0310183 ,  1.5105633 ],
       [ 5.3161554 ,  7.1853566 ,  7.6864433 , ...,  4.199462  ,
         2.6094391 ,  7.597809  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_287/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_287
self.data_kwargs: {'seed': 541}
self.x_data: [[ 3.5424666   4.160326    8.947518   ...  7.3513613   3.2319064
   2.0238907 ]
 [ 1.223466    3.6614168   7.44358    ...  7.1275983   2.889486
   2.1263318 ]
 [ 2.8113456   3.2582755   7.9323573  ...  7.1118317   2.55147
   1.6773428 ]
 ...
 [ 4.8257422   5.760828    0.99088824 ...  1.1221942   6.224573
   1.3870366 ]
 [ 2.8796694   4.5901284   7.9563813  ...  7.221806    3.399475
   1.8678449 ]
 [ 4.724557    5.899211   -0.68043184 ...  1.4107605   6.846419
   1.3825576 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  1074400   
 r)                                                              
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7ff2205e6a40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff28817ece0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff28817ece0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff25817c4c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff220421270>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff2204217e0>, <keras.callbacks.ModelCheckpoint object at 0x7ff220421930>, <keras.callbacks.EarlyStopping object at 0x7ff220421b40>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff220421b70>, <keras.callbacks.TerminateOnNaN object at 0x7ff2204218a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.4544215 ,  3.75662   ,  8.587604  , ...,  7.2717385 ,
         2.2797804 ,  2.224709  ],
       [ 1.3902065 ,  3.433224  ,  7.9205    , ...,  7.378508  ,
         3.6055362 ,  1.7885203 ],
       [ 2.5590267 ,  3.4328253 ,  7.4715605 , ...,  7.496273  ,
         3.511426  ,  1.7360344 ],
       ...,
       [ 5.120887  ,  6.0070558 , -0.50225353, ...,  0.7586615 ,
         6.522565  ,  1.2877582 ],
       [ 4.695989  ,  6.091708  ,  0.05890873, ...,  1.4941858 ,
         6.0310183 ,  1.5105633 ],
       [ 5.3161554 ,  7.1853566 ,  7.6864433 , ...,  4.199462  ,
         2.6094391 ,  7.597809  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_287/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 287/720 with hyperparameters:
timestamp = 2023-09-26 13:23:29.187567
ndims = 32
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.5424666   4.160326    8.947518    1.0939208   7.617727   -0.37824863
  9.755269    4.70193     9.9485855   6.084102    7.916266    0.31372035
  2.6670911   1.177147    2.493954    1.2494209   3.149602    6.587495
  1.3128376   6.9324527   5.4146237   2.7642589   4.1612015   1.0985487
  4.086754    9.263963    3.806251    5.6873355   2.3875675   7.3513613
  3.2319064   2.0238907 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 42: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-26 13:26:17.416 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2101.5671, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 168s - loss: nan - MinusLogProbMetric: 2101.5671 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 168s/epoch - 858ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0003333333333333333.
===========
Generating train data for run 287.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_287/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_287/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.4544215 ,  3.75662   ,  8.587604  , ...,  7.2717385 ,
         2.2797804 ,  2.224709  ],
       [ 1.3902065 ,  3.433224  ,  7.9205    , ...,  7.378508  ,
         3.6055362 ,  1.7885203 ],
       [ 2.5590267 ,  3.4328253 ,  7.4715605 , ...,  7.496273  ,
         3.511426  ,  1.7360344 ],
       ...,
       [ 5.120887  ,  6.0070558 , -0.50225353, ...,  0.7586615 ,
         6.522565  ,  1.2877582 ],
       [ 4.695989  ,  6.091708  ,  0.05890873, ...,  1.4941858 ,
         6.0310183 ,  1.5105633 ],
       [ 5.3161554 ,  7.1853566 ,  7.6864433 , ...,  4.199462  ,
         2.6094391 ,  7.597809  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_287/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_287
self.data_kwargs: {'seed': 541}
self.x_data: [[ 3.5424666   4.160326    8.947518   ...  7.3513613   3.2319064
   2.0238907 ]
 [ 1.223466    3.6614168   7.44358    ...  7.1275983   2.889486
   2.1263318 ]
 [ 2.8113456   3.2582755   7.9323573  ...  7.1118317   2.55147
   1.6773428 ]
 ...
 [ 4.8257422   5.760828    0.99088824 ...  1.1221942   6.224573
   1.3870366 ]
 [ 2.8796694   4.5901284   7.9563813  ...  7.221806    3.399475
   1.8678449 ]
 [ 4.724557    5.899211   -0.68043184 ...  1.4107605   6.846419
   1.3825576 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7ff634e2fa00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff6357abd30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff6357abd30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fefb84b37f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff634560820>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff634560d90>, <keras.callbacks.ModelCheckpoint object at 0x7ff634560e50>, <keras.callbacks.EarlyStopping object at 0x7ff6345610c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff6345610f0>, <keras.callbacks.TerminateOnNaN object at 0x7ff634560d30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.4544215 ,  3.75662   ,  8.587604  , ...,  7.2717385 ,
         2.2797804 ,  2.224709  ],
       [ 1.3902065 ,  3.433224  ,  7.9205    , ...,  7.378508  ,
         3.6055362 ,  1.7885203 ],
       [ 2.5590267 ,  3.4328253 ,  7.4715605 , ...,  7.496273  ,
         3.511426  ,  1.7360344 ],
       ...,
       [ 5.120887  ,  6.0070558 , -0.50225353, ...,  0.7586615 ,
         6.522565  ,  1.2877582 ],
       [ 4.695989  ,  6.091708  ,  0.05890873, ...,  1.4941858 ,
         6.0310183 ,  1.5105633 ],
       [ 5.3161554 ,  7.1853566 ,  7.6864433 , ...,  4.199462  ,
         2.6094391 ,  7.597809  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_287/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 287/720 with hyperparameters:
timestamp = 2023-09-26 13:26:27.827519
ndims = 32
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.5424666   4.160326    8.947518    1.0939208   7.617727   -0.37824863
  9.755269    4.70193     9.9485855   6.084102    7.916266    0.31372035
  2.6670911   1.177147    2.493954    1.2494209   3.149602    6.587495
  1.3128376   6.9324527   5.4146237   2.7642589   4.1612015   1.0985487
  4.086754    9.263963    3.806251    5.6873355   2.3875675   7.3513613
  3.2319064   2.0238907 ]
Epoch 1/1000
2023-09-26 13:30:35.065 
Epoch 1/1000 
	 loss: 1168.3126, MinusLogProbMetric: 1168.3126, val_loss: 522.8401, val_MinusLogProbMetric: 522.8401

Epoch 1: val_loss improved from inf to 522.84009, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 248s - loss: 1168.3126 - MinusLogProbMetric: 1168.3126 - val_loss: 522.8401 - val_MinusLogProbMetric: 522.8401 - lr: 3.3333e-04 - 248s/epoch - 1s/step
Epoch 2/1000
2023-09-26 13:31:53.304 
Epoch 2/1000 
	 loss: 477.6354, MinusLogProbMetric: 477.6354, val_loss: 368.2291, val_MinusLogProbMetric: 368.2291

Epoch 2: val_loss improved from 522.84009 to 368.22906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 477.6354 - MinusLogProbMetric: 477.6354 - val_loss: 368.2291 - val_MinusLogProbMetric: 368.2291 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 3/1000
2023-09-26 13:33:11.209 
Epoch 3/1000 
	 loss: 244.4537, MinusLogProbMetric: 244.4537, val_loss: 183.2103, val_MinusLogProbMetric: 183.2103

Epoch 3: val_loss improved from 368.22906 to 183.21034, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 244.4537 - MinusLogProbMetric: 244.4537 - val_loss: 183.2103 - val_MinusLogProbMetric: 183.2103 - lr: 3.3333e-04 - 78s/epoch - 398ms/step
Epoch 4/1000
2023-09-26 13:34:28.333 
Epoch 4/1000 
	 loss: 159.0978, MinusLogProbMetric: 159.0978, val_loss: 158.8983, val_MinusLogProbMetric: 158.8983

Epoch 4: val_loss improved from 183.21034 to 158.89832, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 159.0978 - MinusLogProbMetric: 159.0978 - val_loss: 158.8983 - val_MinusLogProbMetric: 158.8983 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 5/1000
2023-09-26 13:35:46.832 
Epoch 5/1000 
	 loss: 214.4514, MinusLogProbMetric: 214.4514, val_loss: 214.5529, val_MinusLogProbMetric: 214.5529

Epoch 5: val_loss did not improve from 158.89832
196/196 - 77s - loss: 214.4514 - MinusLogProbMetric: 214.4514 - val_loss: 214.5529 - val_MinusLogProbMetric: 214.5529 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 6/1000
2023-09-26 13:37:02.604 
Epoch 6/1000 
	 loss: 167.4017, MinusLogProbMetric: 167.4017, val_loss: 130.1620, val_MinusLogProbMetric: 130.1620

Epoch 6: val_loss improved from 158.89832 to 130.16196, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 167.4017 - MinusLogProbMetric: 167.4017 - val_loss: 130.1620 - val_MinusLogProbMetric: 130.1620 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 7/1000
2023-09-26 13:38:20.013 
Epoch 7/1000 
	 loss: 117.0281, MinusLogProbMetric: 117.0281, val_loss: 105.0790, val_MinusLogProbMetric: 105.0790

Epoch 7: val_loss improved from 130.16196 to 105.07902, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 117.0281 - MinusLogProbMetric: 117.0281 - val_loss: 105.0790 - val_MinusLogProbMetric: 105.0790 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 8/1000
2023-09-26 13:39:37.464 
Epoch 8/1000 
	 loss: 127.7891, MinusLogProbMetric: 127.7891, val_loss: 127.1866, val_MinusLogProbMetric: 127.1866

Epoch 8: val_loss did not improve from 105.07902
196/196 - 76s - loss: 127.7891 - MinusLogProbMetric: 127.7891 - val_loss: 127.1866 - val_MinusLogProbMetric: 127.1866 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 9/1000
2023-09-26 13:40:53.779 
Epoch 9/1000 
	 loss: 104.9298, MinusLogProbMetric: 104.9298, val_loss: 93.9334, val_MinusLogProbMetric: 93.9334

Epoch 9: val_loss improved from 105.07902 to 93.93340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 104.9298 - MinusLogProbMetric: 104.9298 - val_loss: 93.9334 - val_MinusLogProbMetric: 93.9334 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 10/1000
2023-09-26 13:42:10.683 
Epoch 10/1000 
	 loss: 85.0320, MinusLogProbMetric: 85.0320, val_loss: 80.0483, val_MinusLogProbMetric: 80.0483

Epoch 10: val_loss improved from 93.93340 to 80.04829, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 85.0320 - MinusLogProbMetric: 85.0320 - val_loss: 80.0483 - val_MinusLogProbMetric: 80.0483 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 11/1000
2023-09-26 13:43:28.089 
Epoch 11/1000 
	 loss: 74.1065, MinusLogProbMetric: 74.1065, val_loss: 71.4690, val_MinusLogProbMetric: 71.4690

Epoch 11: val_loss improved from 80.04829 to 71.46899, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 74.1065 - MinusLogProbMetric: 74.1065 - val_loss: 71.4690 - val_MinusLogProbMetric: 71.4690 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 12/1000
2023-09-26 13:44:45.300 
Epoch 12/1000 
	 loss: 73.1860, MinusLogProbMetric: 73.1860, val_loss: 84.1674, val_MinusLogProbMetric: 84.1674

Epoch 12: val_loss did not improve from 71.46899
196/196 - 76s - loss: 73.1860 - MinusLogProbMetric: 73.1860 - val_loss: 84.1674 - val_MinusLogProbMetric: 84.1674 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 13/1000
2023-09-26 13:46:01.323 
Epoch 13/1000 
	 loss: 67.0948, MinusLogProbMetric: 67.0948, val_loss: 60.3875, val_MinusLogProbMetric: 60.3875

Epoch 13: val_loss improved from 71.46899 to 60.38750, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 67.0948 - MinusLogProbMetric: 67.0948 - val_loss: 60.3875 - val_MinusLogProbMetric: 60.3875 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 14/1000
2023-09-26 13:47:18.975 
Epoch 14/1000 
	 loss: 60.0237, MinusLogProbMetric: 60.0237, val_loss: 56.8241, val_MinusLogProbMetric: 56.8241

Epoch 14: val_loss improved from 60.38750 to 56.82406, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 60.0237 - MinusLogProbMetric: 60.0237 - val_loss: 56.8241 - val_MinusLogProbMetric: 56.8241 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 15/1000
2023-09-26 13:48:37.452 
Epoch 15/1000 
	 loss: 54.5424, MinusLogProbMetric: 54.5424, val_loss: 52.4734, val_MinusLogProbMetric: 52.4734

Epoch 15: val_loss improved from 56.82406 to 52.47338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 54.5424 - MinusLogProbMetric: 54.5424 - val_loss: 52.4734 - val_MinusLogProbMetric: 52.4734 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 16/1000
2023-09-26 13:49:54.669 
Epoch 16/1000 
	 loss: 51.0600, MinusLogProbMetric: 51.0600, val_loss: 49.4233, val_MinusLogProbMetric: 49.4233

Epoch 16: val_loss improved from 52.47338 to 49.42329, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 51.0600 - MinusLogProbMetric: 51.0600 - val_loss: 49.4233 - val_MinusLogProbMetric: 49.4233 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 17/1000
2023-09-26 13:51:11.570 
Epoch 17/1000 
	 loss: 49.6954, MinusLogProbMetric: 49.6954, val_loss: 47.1945, val_MinusLogProbMetric: 47.1945

Epoch 17: val_loss improved from 49.42329 to 47.19453, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 49.6954 - MinusLogProbMetric: 49.6954 - val_loss: 47.1945 - val_MinusLogProbMetric: 47.1945 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 18/1000
2023-09-26 13:52:29.100 
Epoch 18/1000 
	 loss: 45.8042, MinusLogProbMetric: 45.8042, val_loss: 44.3563, val_MinusLogProbMetric: 44.3563

Epoch 18: val_loss improved from 47.19453 to 44.35633, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 45.8042 - MinusLogProbMetric: 45.8042 - val_loss: 44.3563 - val_MinusLogProbMetric: 44.3563 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 19/1000
2023-09-26 13:53:46.555 
Epoch 19/1000 
	 loss: 93.9512, MinusLogProbMetric: 93.9512, val_loss: 105.7490, val_MinusLogProbMetric: 105.7490

Epoch 19: val_loss did not improve from 44.35633
196/196 - 76s - loss: 93.9512 - MinusLogProbMetric: 93.9512 - val_loss: 105.7490 - val_MinusLogProbMetric: 105.7490 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 20/1000
2023-09-26 13:55:02.976 
Epoch 20/1000 
	 loss: 78.9761, MinusLogProbMetric: 78.9761, val_loss: 64.5933, val_MinusLogProbMetric: 64.5933

Epoch 20: val_loss did not improve from 44.35633
196/196 - 76s - loss: 78.9761 - MinusLogProbMetric: 78.9761 - val_loss: 64.5933 - val_MinusLogProbMetric: 64.5933 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 21/1000
2023-09-26 13:56:18.373 
Epoch 21/1000 
	 loss: 57.4820, MinusLogProbMetric: 57.4820, val_loss: 52.7916, val_MinusLogProbMetric: 52.7916

Epoch 21: val_loss did not improve from 44.35633
196/196 - 75s - loss: 57.4820 - MinusLogProbMetric: 57.4820 - val_loss: 52.7916 - val_MinusLogProbMetric: 52.7916 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 22/1000
2023-09-26 13:57:34.614 
Epoch 22/1000 
	 loss: 48.9170, MinusLogProbMetric: 48.9170, val_loss: 46.4825, val_MinusLogProbMetric: 46.4825

Epoch 22: val_loss did not improve from 44.35633
196/196 - 76s - loss: 48.9170 - MinusLogProbMetric: 48.9170 - val_loss: 46.4825 - val_MinusLogProbMetric: 46.4825 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 23/1000
2023-09-26 13:58:52.438 
Epoch 23/1000 
	 loss: 44.2827, MinusLogProbMetric: 44.2827, val_loss: 43.5045, val_MinusLogProbMetric: 43.5045

Epoch 23: val_loss improved from 44.35633 to 43.50450, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 79s - loss: 44.2827 - MinusLogProbMetric: 44.2827 - val_loss: 43.5045 - val_MinusLogProbMetric: 43.5045 - lr: 3.3333e-04 - 79s/epoch - 404ms/step
Epoch 24/1000
2023-09-26 14:00:10.402 
Epoch 24/1000 
	 loss: 44.6975, MinusLogProbMetric: 44.6975, val_loss: 42.8953, val_MinusLogProbMetric: 42.8953

Epoch 24: val_loss improved from 43.50450 to 42.89535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 44.6975 - MinusLogProbMetric: 44.6975 - val_loss: 42.8953 - val_MinusLogProbMetric: 42.8953 - lr: 3.3333e-04 - 78s/epoch - 398ms/step
Epoch 25/1000
2023-09-26 14:01:27.899 
Epoch 25/1000 
	 loss: 39.9170, MinusLogProbMetric: 39.9170, val_loss: 40.4008, val_MinusLogProbMetric: 40.4008

Epoch 25: val_loss improved from 42.89535 to 40.40083, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 39.9170 - MinusLogProbMetric: 39.9170 - val_loss: 40.4008 - val_MinusLogProbMetric: 40.4008 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 26/1000
2023-09-26 14:02:44.802 
Epoch 26/1000 
	 loss: 38.2863, MinusLogProbMetric: 38.2863, val_loss: 37.7613, val_MinusLogProbMetric: 37.7613

Epoch 26: val_loss improved from 40.40083 to 37.76133, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 38.2863 - MinusLogProbMetric: 38.2863 - val_loss: 37.7613 - val_MinusLogProbMetric: 37.7613 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 27/1000
2023-09-26 14:04:01.870 
Epoch 27/1000 
	 loss: 36.8051, MinusLogProbMetric: 36.8051, val_loss: 35.7346, val_MinusLogProbMetric: 35.7346

Epoch 27: val_loss improved from 37.76133 to 35.73459, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 36.8051 - MinusLogProbMetric: 36.8051 - val_loss: 35.7346 - val_MinusLogProbMetric: 35.7346 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 28/1000
2023-09-26 14:05:19.101 
Epoch 28/1000 
	 loss: 35.4330, MinusLogProbMetric: 35.4330, val_loss: 34.7839, val_MinusLogProbMetric: 34.7839

Epoch 28: val_loss improved from 35.73459 to 34.78388, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 35.4330 - MinusLogProbMetric: 35.4330 - val_loss: 34.7839 - val_MinusLogProbMetric: 34.7839 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 29/1000
2023-09-26 14:06:36.969 
Epoch 29/1000 
	 loss: 35.6417, MinusLogProbMetric: 35.6417, val_loss: 34.5802, val_MinusLogProbMetric: 34.5802

Epoch 29: val_loss improved from 34.78388 to 34.58023, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 35.6417 - MinusLogProbMetric: 35.6417 - val_loss: 34.5802 - val_MinusLogProbMetric: 34.5802 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 30/1000
2023-09-26 14:07:55.067 
Epoch 30/1000 
	 loss: 34.0130, MinusLogProbMetric: 34.0130, val_loss: 33.1709, val_MinusLogProbMetric: 33.1709

Epoch 30: val_loss improved from 34.58023 to 33.17085, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 34.0130 - MinusLogProbMetric: 34.0130 - val_loss: 33.1709 - val_MinusLogProbMetric: 33.1709 - lr: 3.3333e-04 - 78s/epoch - 398ms/step
Epoch 31/1000
2023-09-26 14:09:12.409 
Epoch 31/1000 
	 loss: 33.5184, MinusLogProbMetric: 33.5184, val_loss: 34.5894, val_MinusLogProbMetric: 34.5894

Epoch 31: val_loss did not improve from 33.17085
196/196 - 76s - loss: 33.5184 - MinusLogProbMetric: 33.5184 - val_loss: 34.5894 - val_MinusLogProbMetric: 34.5894 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 32/1000
2023-09-26 14:10:28.356 
Epoch 32/1000 
	 loss: 32.6271, MinusLogProbMetric: 32.6271, val_loss: 33.4779, val_MinusLogProbMetric: 33.4779

Epoch 32: val_loss did not improve from 33.17085
196/196 - 76s - loss: 32.6271 - MinusLogProbMetric: 32.6271 - val_loss: 33.4779 - val_MinusLogProbMetric: 33.4779 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 33/1000
2023-09-26 14:11:44.339 
Epoch 33/1000 
	 loss: 35.8177, MinusLogProbMetric: 35.8177, val_loss: 33.3651, val_MinusLogProbMetric: 33.3651

Epoch 33: val_loss did not improve from 33.17085
196/196 - 76s - loss: 35.8177 - MinusLogProbMetric: 35.8177 - val_loss: 33.3651 - val_MinusLogProbMetric: 33.3651 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 34/1000
2023-09-26 14:13:00.542 
Epoch 34/1000 
	 loss: 32.7455, MinusLogProbMetric: 32.7455, val_loss: 32.8011, val_MinusLogProbMetric: 32.8011

Epoch 34: val_loss improved from 33.17085 to 32.80114, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 32.7455 - MinusLogProbMetric: 32.7455 - val_loss: 32.8011 - val_MinusLogProbMetric: 32.8011 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 35/1000
2023-09-26 14:14:17.721 
Epoch 35/1000 
	 loss: 32.0832, MinusLogProbMetric: 32.0832, val_loss: 31.3618, val_MinusLogProbMetric: 31.3618

Epoch 35: val_loss improved from 32.80114 to 31.36184, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 32.0832 - MinusLogProbMetric: 32.0832 - val_loss: 31.3618 - val_MinusLogProbMetric: 31.3618 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 36/1000
2023-09-26 14:15:35.318 
Epoch 36/1000 
	 loss: 31.5218, MinusLogProbMetric: 31.5218, val_loss: 31.5082, val_MinusLogProbMetric: 31.5082

Epoch 36: val_loss did not improve from 31.36184
196/196 - 76s - loss: 31.5218 - MinusLogProbMetric: 31.5218 - val_loss: 31.5082 - val_MinusLogProbMetric: 31.5082 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 37/1000
2023-09-26 14:16:51.228 
Epoch 37/1000 
	 loss: 30.7995, MinusLogProbMetric: 30.7995, val_loss: 30.1627, val_MinusLogProbMetric: 30.1627

Epoch 37: val_loss improved from 31.36184 to 30.16269, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 30.7995 - MinusLogProbMetric: 30.7995 - val_loss: 30.1627 - val_MinusLogProbMetric: 30.1627 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 38/1000
2023-09-26 14:18:08.835 
Epoch 38/1000 
	 loss: 30.5215, MinusLogProbMetric: 30.5215, val_loss: 30.2862, val_MinusLogProbMetric: 30.2862

Epoch 38: val_loss did not improve from 30.16269
196/196 - 76s - loss: 30.5215 - MinusLogProbMetric: 30.5215 - val_loss: 30.2862 - val_MinusLogProbMetric: 30.2862 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 39/1000
2023-09-26 14:19:25.247 
Epoch 39/1000 
	 loss: 29.8944, MinusLogProbMetric: 29.8944, val_loss: 29.7360, val_MinusLogProbMetric: 29.7360

Epoch 39: val_loss improved from 30.16269 to 29.73602, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 29.8944 - MinusLogProbMetric: 29.8944 - val_loss: 29.7360 - val_MinusLogProbMetric: 29.7360 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 40/1000
2023-09-26 14:20:46.604 
Epoch 40/1000 
	 loss: 29.6750, MinusLogProbMetric: 29.6750, val_loss: 30.0667, val_MinusLogProbMetric: 30.0667

Epoch 40: val_loss did not improve from 29.73602
196/196 - 80s - loss: 29.6750 - MinusLogProbMetric: 29.6750 - val_loss: 30.0667 - val_MinusLogProbMetric: 30.0667 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 41/1000
2023-09-26 14:22:05.862 
Epoch 41/1000 
	 loss: 29.2256, MinusLogProbMetric: 29.2256, val_loss: 29.4300, val_MinusLogProbMetric: 29.4300

Epoch 41: val_loss improved from 29.73602 to 29.43002, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 81s - loss: 29.2256 - MinusLogProbMetric: 29.2256 - val_loss: 29.4300 - val_MinusLogProbMetric: 29.4300 - lr: 3.3333e-04 - 81s/epoch - 412ms/step
Epoch 42/1000
2023-09-26 14:23:24.609 
Epoch 42/1000 
	 loss: 28.9755, MinusLogProbMetric: 28.9755, val_loss: 28.9802, val_MinusLogProbMetric: 28.9802

Epoch 42: val_loss improved from 29.43002 to 28.98017, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 79s - loss: 28.9755 - MinusLogProbMetric: 28.9755 - val_loss: 28.9802 - val_MinusLogProbMetric: 28.9802 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 43/1000
2023-09-26 14:24:42.363 
Epoch 43/1000 
	 loss: 28.5387, MinusLogProbMetric: 28.5387, val_loss: 28.5885, val_MinusLogProbMetric: 28.5885

Epoch 43: val_loss improved from 28.98017 to 28.58849, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 28.5387 - MinusLogProbMetric: 28.5387 - val_loss: 28.5885 - val_MinusLogProbMetric: 28.5885 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 44/1000
2023-09-26 14:26:00.652 
Epoch 44/1000 
	 loss: 28.2815, MinusLogProbMetric: 28.2815, val_loss: 28.2003, val_MinusLogProbMetric: 28.2003

Epoch 44: val_loss improved from 28.58849 to 28.20029, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 79s - loss: 28.2815 - MinusLogProbMetric: 28.2815 - val_loss: 28.2003 - val_MinusLogProbMetric: 28.2003 - lr: 3.3333e-04 - 79s/epoch - 401ms/step
Epoch 45/1000
2023-09-26 14:27:19.169 
Epoch 45/1000 
	 loss: 30.0496, MinusLogProbMetric: 30.0496, val_loss: 29.9589, val_MinusLogProbMetric: 29.9589

Epoch 45: val_loss did not improve from 28.20029
196/196 - 77s - loss: 30.0496 - MinusLogProbMetric: 30.0496 - val_loss: 29.9589 - val_MinusLogProbMetric: 29.9589 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 46/1000
2023-09-26 14:28:36.041 
Epoch 46/1000 
	 loss: 27.8283, MinusLogProbMetric: 27.8283, val_loss: 27.2535, val_MinusLogProbMetric: 27.2535

Epoch 46: val_loss improved from 28.20029 to 27.25350, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 27.8283 - MinusLogProbMetric: 27.8283 - val_loss: 27.2535 - val_MinusLogProbMetric: 27.2535 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 47/1000
2023-09-26 14:29:54.388 
Epoch 47/1000 
	 loss: 27.3805, MinusLogProbMetric: 27.3805, val_loss: 27.5193, val_MinusLogProbMetric: 27.5193

Epoch 47: val_loss did not improve from 27.25350
196/196 - 77s - loss: 27.3805 - MinusLogProbMetric: 27.3805 - val_loss: 27.5193 - val_MinusLogProbMetric: 27.5193 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 48/1000
2023-09-26 14:31:10.718 
Epoch 48/1000 
	 loss: 27.1925, MinusLogProbMetric: 27.1925, val_loss: 26.9480, val_MinusLogProbMetric: 26.9480

Epoch 48: val_loss improved from 27.25350 to 26.94799, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 27.1925 - MinusLogProbMetric: 27.1925 - val_loss: 26.9480 - val_MinusLogProbMetric: 26.9480 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 49/1000
2023-09-26 14:32:29.014 
Epoch 49/1000 
	 loss: 26.8977, MinusLogProbMetric: 26.8977, val_loss: 26.7642, val_MinusLogProbMetric: 26.7642

Epoch 49: val_loss improved from 26.94799 to 26.76422, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 26.8977 - MinusLogProbMetric: 26.8977 - val_loss: 26.7642 - val_MinusLogProbMetric: 26.7642 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 50/1000
2023-09-26 14:33:45.675 
Epoch 50/1000 
	 loss: 26.5424, MinusLogProbMetric: 26.5424, val_loss: 27.8809, val_MinusLogProbMetric: 27.8809

Epoch 50: val_loss did not improve from 26.76422
196/196 - 75s - loss: 26.5424 - MinusLogProbMetric: 26.5424 - val_loss: 27.8809 - val_MinusLogProbMetric: 27.8809 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 51/1000
2023-09-26 14:35:01.390 
Epoch 51/1000 
	 loss: 26.4528, MinusLogProbMetric: 26.4528, val_loss: 26.0598, val_MinusLogProbMetric: 26.0598

Epoch 51: val_loss improved from 26.76422 to 26.05979, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 26.4528 - MinusLogProbMetric: 26.4528 - val_loss: 26.0598 - val_MinusLogProbMetric: 26.0598 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 52/1000
2023-09-26 14:36:18.626 
Epoch 52/1000 
	 loss: 26.3384, MinusLogProbMetric: 26.3384, val_loss: 28.2775, val_MinusLogProbMetric: 28.2775

Epoch 52: val_loss did not improve from 26.05979
196/196 - 76s - loss: 26.3384 - MinusLogProbMetric: 26.3384 - val_loss: 28.2775 - val_MinusLogProbMetric: 28.2775 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 53/1000
2023-09-26 14:37:35.121 
Epoch 53/1000 
	 loss: 25.8762, MinusLogProbMetric: 25.8762, val_loss: 27.0808, val_MinusLogProbMetric: 27.0808

Epoch 53: val_loss did not improve from 26.05979
196/196 - 76s - loss: 25.8762 - MinusLogProbMetric: 25.8762 - val_loss: 27.0808 - val_MinusLogProbMetric: 27.0808 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 54/1000
2023-09-26 14:38:50.704 
Epoch 54/1000 
	 loss: 62.3414, MinusLogProbMetric: 62.3414, val_loss: 51.0946, val_MinusLogProbMetric: 51.0946

Epoch 54: val_loss did not improve from 26.05979
196/196 - 76s - loss: 62.3414 - MinusLogProbMetric: 62.3414 - val_loss: 51.0946 - val_MinusLogProbMetric: 51.0946 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 55/1000
2023-09-26 14:40:07.060 
Epoch 55/1000 
	 loss: 40.5575, MinusLogProbMetric: 40.5575, val_loss: 36.0796, val_MinusLogProbMetric: 36.0796

Epoch 55: val_loss did not improve from 26.05979
196/196 - 76s - loss: 40.5575 - MinusLogProbMetric: 40.5575 - val_loss: 36.0796 - val_MinusLogProbMetric: 36.0796 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 56/1000
2023-09-26 14:41:23.182 
Epoch 56/1000 
	 loss: 33.6153, MinusLogProbMetric: 33.6153, val_loss: 32.3144, val_MinusLogProbMetric: 32.3144

Epoch 56: val_loss did not improve from 26.05979
196/196 - 76s - loss: 33.6153 - MinusLogProbMetric: 33.6153 - val_loss: 32.3144 - val_MinusLogProbMetric: 32.3144 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 57/1000
2023-09-26 14:42:39.209 
Epoch 57/1000 
	 loss: 30.9363, MinusLogProbMetric: 30.9363, val_loss: 30.3654, val_MinusLogProbMetric: 30.3654

Epoch 57: val_loss did not improve from 26.05979
196/196 - 76s - loss: 30.9363 - MinusLogProbMetric: 30.9363 - val_loss: 30.3654 - val_MinusLogProbMetric: 30.3654 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 58/1000
2023-09-26 14:43:55.398 
Epoch 58/1000 
	 loss: 29.6598, MinusLogProbMetric: 29.6598, val_loss: 29.1426, val_MinusLogProbMetric: 29.1426

Epoch 58: val_loss did not improve from 26.05979
196/196 - 76s - loss: 29.6598 - MinusLogProbMetric: 29.6598 - val_loss: 29.1426 - val_MinusLogProbMetric: 29.1426 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 59/1000
2023-09-26 14:45:11.275 
Epoch 59/1000 
	 loss: 28.3610, MinusLogProbMetric: 28.3610, val_loss: 28.2983, val_MinusLogProbMetric: 28.2983

Epoch 59: val_loss did not improve from 26.05979
196/196 - 76s - loss: 28.3610 - MinusLogProbMetric: 28.3610 - val_loss: 28.2983 - val_MinusLogProbMetric: 28.2983 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 60/1000
2023-09-26 14:46:27.614 
Epoch 60/1000 
	 loss: 27.7320, MinusLogProbMetric: 27.7320, val_loss: 27.7771, val_MinusLogProbMetric: 27.7771

Epoch 60: val_loss did not improve from 26.05979
196/196 - 76s - loss: 27.7320 - MinusLogProbMetric: 27.7320 - val_loss: 27.7771 - val_MinusLogProbMetric: 27.7771 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 61/1000
2023-09-26 14:47:43.643 
Epoch 61/1000 
	 loss: 27.0370, MinusLogProbMetric: 27.0370, val_loss: 26.7066, val_MinusLogProbMetric: 26.7066

Epoch 61: val_loss did not improve from 26.05979
196/196 - 76s - loss: 27.0370 - MinusLogProbMetric: 27.0370 - val_loss: 26.7066 - val_MinusLogProbMetric: 26.7066 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 62/1000
2023-09-26 14:48:59.151 
Epoch 62/1000 
	 loss: 28.2975, MinusLogProbMetric: 28.2975, val_loss: 27.5223, val_MinusLogProbMetric: 27.5223

Epoch 62: val_loss did not improve from 26.05979
196/196 - 76s - loss: 28.2975 - MinusLogProbMetric: 28.2975 - val_loss: 27.5223 - val_MinusLogProbMetric: 27.5223 - lr: 3.3333e-04 - 76s/epoch - 385ms/step
Epoch 63/1000
2023-09-26 14:50:15.591 
Epoch 63/1000 
	 loss: 27.0089, MinusLogProbMetric: 27.0089, val_loss: 26.8099, val_MinusLogProbMetric: 26.8099

Epoch 63: val_loss did not improve from 26.05979
196/196 - 76s - loss: 27.0089 - MinusLogProbMetric: 27.0089 - val_loss: 26.8099 - val_MinusLogProbMetric: 26.8099 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 64/1000
2023-09-26 14:51:31.938 
Epoch 64/1000 
	 loss: 26.4305, MinusLogProbMetric: 26.4305, val_loss: 25.7439, val_MinusLogProbMetric: 25.7439

Epoch 64: val_loss improved from 26.05979 to 25.74387, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 26.4305 - MinusLogProbMetric: 26.4305 - val_loss: 25.7439 - val_MinusLogProbMetric: 25.7439 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 65/1000
2023-09-26 14:52:49.054 
Epoch 65/1000 
	 loss: 25.9461, MinusLogProbMetric: 25.9461, val_loss: 26.0964, val_MinusLogProbMetric: 26.0964

Epoch 65: val_loss did not improve from 25.74387
196/196 - 76s - loss: 25.9461 - MinusLogProbMetric: 25.9461 - val_loss: 26.0964 - val_MinusLogProbMetric: 26.0964 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 66/1000
2023-09-26 14:54:05.216 
Epoch 66/1000 
	 loss: 25.6983, MinusLogProbMetric: 25.6983, val_loss: 26.0387, val_MinusLogProbMetric: 26.0387

Epoch 66: val_loss did not improve from 25.74387
196/196 - 76s - loss: 25.6983 - MinusLogProbMetric: 25.6983 - val_loss: 26.0387 - val_MinusLogProbMetric: 26.0387 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 67/1000
2023-09-26 14:55:21.732 
Epoch 67/1000 
	 loss: 25.3460, MinusLogProbMetric: 25.3460, val_loss: 25.5002, val_MinusLogProbMetric: 25.5002

Epoch 67: val_loss improved from 25.74387 to 25.50017, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 25.3460 - MinusLogProbMetric: 25.3460 - val_loss: 25.5002 - val_MinusLogProbMetric: 25.5002 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 68/1000
2023-09-26 14:56:39.784 
Epoch 68/1000 
	 loss: 25.1083, MinusLogProbMetric: 25.1083, val_loss: 25.0589, val_MinusLogProbMetric: 25.0589

Epoch 68: val_loss improved from 25.50017 to 25.05890, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 25.1083 - MinusLogProbMetric: 25.1083 - val_loss: 25.0589 - val_MinusLogProbMetric: 25.0589 - lr: 3.3333e-04 - 78s/epoch - 398ms/step
Epoch 69/1000
2023-09-26 14:57:56.362 
Epoch 69/1000 
	 loss: 24.7740, MinusLogProbMetric: 24.7740, val_loss: 24.2929, val_MinusLogProbMetric: 24.2929

Epoch 69: val_loss improved from 25.05890 to 24.29290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 24.7740 - MinusLogProbMetric: 24.7740 - val_loss: 24.2929 - val_MinusLogProbMetric: 24.2929 - lr: 3.3333e-04 - 77s/epoch - 390ms/step
Epoch 70/1000
2023-09-26 14:59:14.205 
Epoch 70/1000 
	 loss: 24.4823, MinusLogProbMetric: 24.4823, val_loss: 24.3256, val_MinusLogProbMetric: 24.3256

Epoch 70: val_loss did not improve from 24.29290
196/196 - 77s - loss: 24.4823 - MinusLogProbMetric: 24.4823 - val_loss: 24.3256 - val_MinusLogProbMetric: 24.3256 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 71/1000
2023-09-26 15:00:30.228 
Epoch 71/1000 
	 loss: 24.3143, MinusLogProbMetric: 24.3143, val_loss: 24.4027, val_MinusLogProbMetric: 24.4027

Epoch 71: val_loss did not improve from 24.29290
196/196 - 76s - loss: 24.3143 - MinusLogProbMetric: 24.3143 - val_loss: 24.4027 - val_MinusLogProbMetric: 24.4027 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 72/1000
2023-09-26 15:01:46.020 
Epoch 72/1000 
	 loss: 24.1247, MinusLogProbMetric: 24.1247, val_loss: 24.5611, val_MinusLogProbMetric: 24.5611

Epoch 72: val_loss did not improve from 24.29290
196/196 - 76s - loss: 24.1247 - MinusLogProbMetric: 24.1247 - val_loss: 24.5611 - val_MinusLogProbMetric: 24.5611 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 73/1000
2023-09-26 15:03:01.749 
Epoch 73/1000 
	 loss: 24.0245, MinusLogProbMetric: 24.0245, val_loss: 23.5284, val_MinusLogProbMetric: 23.5284

Epoch 73: val_loss improved from 24.29290 to 23.52844, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 24.0245 - MinusLogProbMetric: 24.0245 - val_loss: 23.5284 - val_MinusLogProbMetric: 23.5284 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 74/1000
2023-09-26 15:04:19.475 
Epoch 74/1000 
	 loss: 23.7348, MinusLogProbMetric: 23.7348, val_loss: 23.5712, val_MinusLogProbMetric: 23.5712

Epoch 74: val_loss did not improve from 23.52844
196/196 - 76s - loss: 23.7348 - MinusLogProbMetric: 23.7348 - val_loss: 23.5712 - val_MinusLogProbMetric: 23.5712 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 75/1000
2023-09-26 15:05:36.290 
Epoch 75/1000 
	 loss: 23.7825, MinusLogProbMetric: 23.7825, val_loss: 23.6701, val_MinusLogProbMetric: 23.6701

Epoch 75: val_loss did not improve from 23.52844
196/196 - 77s - loss: 23.7825 - MinusLogProbMetric: 23.7825 - val_loss: 23.6701 - val_MinusLogProbMetric: 23.6701 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 76/1000
2023-09-26 15:06:52.023 
Epoch 76/1000 
	 loss: 23.5312, MinusLogProbMetric: 23.5312, val_loss: 23.3332, val_MinusLogProbMetric: 23.3332

Epoch 76: val_loss improved from 23.52844 to 23.33322, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 23.5312 - MinusLogProbMetric: 23.5312 - val_loss: 23.3332 - val_MinusLogProbMetric: 23.3332 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 77/1000
2023-09-26 15:08:09.180 
Epoch 77/1000 
	 loss: 23.3023, MinusLogProbMetric: 23.3023, val_loss: 23.9456, val_MinusLogProbMetric: 23.9456

Epoch 77: val_loss did not improve from 23.33322
196/196 - 76s - loss: 23.3023 - MinusLogProbMetric: 23.3023 - val_loss: 23.9456 - val_MinusLogProbMetric: 23.9456 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 78/1000
2023-09-26 15:09:25.154 
Epoch 78/1000 
	 loss: 23.2694, MinusLogProbMetric: 23.2694, val_loss: 23.2407, val_MinusLogProbMetric: 23.2407

Epoch 78: val_loss improved from 23.33322 to 23.24073, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 23.2694 - MinusLogProbMetric: 23.2694 - val_loss: 23.2407 - val_MinusLogProbMetric: 23.2407 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 79/1000
2023-09-26 15:10:42.314 
Epoch 79/1000 
	 loss: 23.2784, MinusLogProbMetric: 23.2784, val_loss: 23.7226, val_MinusLogProbMetric: 23.7226

Epoch 79: val_loss did not improve from 23.24073
196/196 - 76s - loss: 23.2784 - MinusLogProbMetric: 23.2784 - val_loss: 23.7226 - val_MinusLogProbMetric: 23.7226 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 80/1000
2023-09-26 15:11:58.075 
Epoch 80/1000 
	 loss: 23.0296, MinusLogProbMetric: 23.0296, val_loss: 23.3054, val_MinusLogProbMetric: 23.3054

Epoch 80: val_loss did not improve from 23.24073
196/196 - 76s - loss: 23.0296 - MinusLogProbMetric: 23.0296 - val_loss: 23.3054 - val_MinusLogProbMetric: 23.3054 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 81/1000
2023-09-26 15:13:14.282 
Epoch 81/1000 
	 loss: 22.9536, MinusLogProbMetric: 22.9536, val_loss: 22.4195, val_MinusLogProbMetric: 22.4195

Epoch 81: val_loss improved from 23.24073 to 22.41953, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 22.9536 - MinusLogProbMetric: 22.9536 - val_loss: 22.4195 - val_MinusLogProbMetric: 22.4195 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 82/1000
2023-09-26 15:14:31.987 
Epoch 82/1000 
	 loss: 22.7700, MinusLogProbMetric: 22.7700, val_loss: 22.6005, val_MinusLogProbMetric: 22.6005

Epoch 82: val_loss did not improve from 22.41953
196/196 - 76s - loss: 22.7700 - MinusLogProbMetric: 22.7700 - val_loss: 22.6005 - val_MinusLogProbMetric: 22.6005 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 83/1000
2023-09-26 15:15:48.261 
Epoch 83/1000 
	 loss: 22.7629, MinusLogProbMetric: 22.7629, val_loss: 22.4900, val_MinusLogProbMetric: 22.4900

Epoch 83: val_loss did not improve from 22.41953
196/196 - 76s - loss: 22.7629 - MinusLogProbMetric: 22.7629 - val_loss: 22.4900 - val_MinusLogProbMetric: 22.4900 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 84/1000
2023-09-26 15:17:04.909 
Epoch 84/1000 
	 loss: 22.6013, MinusLogProbMetric: 22.6013, val_loss: 22.4449, val_MinusLogProbMetric: 22.4449

Epoch 84: val_loss did not improve from 22.41953
196/196 - 77s - loss: 22.6013 - MinusLogProbMetric: 22.6013 - val_loss: 22.4449 - val_MinusLogProbMetric: 22.4449 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 85/1000
2023-09-26 15:18:21.027 
Epoch 85/1000 
	 loss: 22.4886, MinusLogProbMetric: 22.4886, val_loss: 22.8915, val_MinusLogProbMetric: 22.8915

Epoch 85: val_loss did not improve from 22.41953
196/196 - 76s - loss: 22.4886 - MinusLogProbMetric: 22.4886 - val_loss: 22.8915 - val_MinusLogProbMetric: 22.8915 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 86/1000
2023-09-26 15:19:37.043 
Epoch 86/1000 
	 loss: 22.3245, MinusLogProbMetric: 22.3245, val_loss: 22.2923, val_MinusLogProbMetric: 22.2923

Epoch 86: val_loss improved from 22.41953 to 22.29232, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 22.3245 - MinusLogProbMetric: 22.3245 - val_loss: 22.2923 - val_MinusLogProbMetric: 22.2923 - lr: 3.3333e-04 - 78s/epoch - 395ms/step
Epoch 87/1000
2023-09-26 15:20:58.519 
Epoch 87/1000 
	 loss: 22.4031, MinusLogProbMetric: 22.4031, val_loss: 22.3505, val_MinusLogProbMetric: 22.3505

Epoch 87: val_loss did not improve from 22.29232
196/196 - 80s - loss: 22.4031 - MinusLogProbMetric: 22.4031 - val_loss: 22.3505 - val_MinusLogProbMetric: 22.3505 - lr: 3.3333e-04 - 80s/epoch - 408ms/step
Epoch 88/1000
2023-09-26 15:22:14.433 
Epoch 88/1000 
	 loss: 22.1236, MinusLogProbMetric: 22.1236, val_loss: 22.2021, val_MinusLogProbMetric: 22.2021

Epoch 88: val_loss improved from 22.29232 to 22.20213, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 22.1236 - MinusLogProbMetric: 22.1236 - val_loss: 22.2021 - val_MinusLogProbMetric: 22.2021 - lr: 3.3333e-04 - 77s/epoch - 395ms/step
Epoch 89/1000
2023-09-26 15:23:31.919 
Epoch 89/1000 
	 loss: 22.1744, MinusLogProbMetric: 22.1744, val_loss: 24.2641, val_MinusLogProbMetric: 24.2641

Epoch 89: val_loss did not improve from 22.20213
196/196 - 76s - loss: 22.1744 - MinusLogProbMetric: 22.1744 - val_loss: 24.2641 - val_MinusLogProbMetric: 24.2641 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 90/1000
2023-09-26 15:24:49.158 
Epoch 90/1000 
	 loss: 22.0735, MinusLogProbMetric: 22.0735, val_loss: 22.2140, val_MinusLogProbMetric: 22.2140

Epoch 90: val_loss did not improve from 22.20213
196/196 - 77s - loss: 22.0735 - MinusLogProbMetric: 22.0735 - val_loss: 22.2140 - val_MinusLogProbMetric: 22.2140 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 91/1000
2023-09-26 15:26:01.235 
Epoch 91/1000 
	 loss: 21.9072, MinusLogProbMetric: 21.9072, val_loss: 21.7975, val_MinusLogProbMetric: 21.7975

Epoch 91: val_loss improved from 22.20213 to 21.79753, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 73s - loss: 21.9072 - MinusLogProbMetric: 21.9072 - val_loss: 21.7975 - val_MinusLogProbMetric: 21.7975 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 92/1000
2023-09-26 15:27:07.738 
Epoch 92/1000 
	 loss: 22.0402, MinusLogProbMetric: 22.0402, val_loss: 21.9253, val_MinusLogProbMetric: 21.9253

Epoch 92: val_loss did not improve from 21.79753
196/196 - 65s - loss: 22.0402 - MinusLogProbMetric: 22.0402 - val_loss: 21.9253 - val_MinusLogProbMetric: 21.9253 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 93/1000
2023-09-26 15:28:21.866 
Epoch 93/1000 
	 loss: 21.9671, MinusLogProbMetric: 21.9671, val_loss: 22.3016, val_MinusLogProbMetric: 22.3016

Epoch 93: val_loss did not improve from 21.79753
196/196 - 74s - loss: 21.9671 - MinusLogProbMetric: 21.9671 - val_loss: 22.3016 - val_MinusLogProbMetric: 22.3016 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 94/1000
2023-09-26 15:29:31.098 
Epoch 94/1000 
	 loss: 21.7264, MinusLogProbMetric: 21.7264, val_loss: 21.8002, val_MinusLogProbMetric: 21.8002

Epoch 94: val_loss did not improve from 21.79753
196/196 - 69s - loss: 21.7264 - MinusLogProbMetric: 21.7264 - val_loss: 21.8002 - val_MinusLogProbMetric: 21.8002 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 95/1000
2023-09-26 15:30:38.974 
Epoch 95/1000 
	 loss: 21.6400, MinusLogProbMetric: 21.6400, val_loss: 21.7197, val_MinusLogProbMetric: 21.7197

Epoch 95: val_loss improved from 21.79753 to 21.71975, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 69s - loss: 21.6400 - MinusLogProbMetric: 21.6400 - val_loss: 21.7197 - val_MinusLogProbMetric: 21.7197 - lr: 3.3333e-04 - 69s/epoch - 352ms/step
Epoch 96/1000
2023-09-26 15:31:55.922 
Epoch 96/1000 
	 loss: 25.5172, MinusLogProbMetric: 25.5172, val_loss: 23.6328, val_MinusLogProbMetric: 23.6328

Epoch 96: val_loss did not improve from 21.71975
196/196 - 76s - loss: 25.5172 - MinusLogProbMetric: 25.5172 - val_loss: 23.6328 - val_MinusLogProbMetric: 23.6328 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 97/1000
2023-09-26 15:33:07.148 
Epoch 97/1000 
	 loss: 22.8368, MinusLogProbMetric: 22.8368, val_loss: 22.3891, val_MinusLogProbMetric: 22.3891

Epoch 97: val_loss did not improve from 21.71975
196/196 - 71s - loss: 22.8368 - MinusLogProbMetric: 22.8368 - val_loss: 22.3891 - val_MinusLogProbMetric: 22.3891 - lr: 3.3333e-04 - 71s/epoch - 363ms/step
Epoch 98/1000
2023-09-26 15:34:21.300 
Epoch 98/1000 
	 loss: 22.1910, MinusLogProbMetric: 22.1910, val_loss: 23.6812, val_MinusLogProbMetric: 23.6812

Epoch 98: val_loss did not improve from 21.71975
196/196 - 74s - loss: 22.1910 - MinusLogProbMetric: 22.1910 - val_loss: 23.6812 - val_MinusLogProbMetric: 23.6812 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 99/1000
2023-09-26 15:35:36.574 
Epoch 99/1000 
	 loss: 22.0236, MinusLogProbMetric: 22.0236, val_loss: 22.0381, val_MinusLogProbMetric: 22.0381

Epoch 99: val_loss did not improve from 21.71975
196/196 - 75s - loss: 22.0236 - MinusLogProbMetric: 22.0236 - val_loss: 22.0381 - val_MinusLogProbMetric: 22.0381 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 100/1000
2023-09-26 15:36:52.490 
Epoch 100/1000 
	 loss: 21.9016, MinusLogProbMetric: 21.9016, val_loss: 21.8456, val_MinusLogProbMetric: 21.8456

Epoch 100: val_loss did not improve from 21.71975
196/196 - 76s - loss: 21.9016 - MinusLogProbMetric: 21.9016 - val_loss: 21.8456 - val_MinusLogProbMetric: 21.8456 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 101/1000
2023-09-26 15:38:08.371 
Epoch 101/1000 
	 loss: 21.6373, MinusLogProbMetric: 21.6373, val_loss: 21.6865, val_MinusLogProbMetric: 21.6865

Epoch 101: val_loss improved from 21.71975 to 21.68647, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 21.6373 - MinusLogProbMetric: 21.6373 - val_loss: 21.6865 - val_MinusLogProbMetric: 21.6865 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 102/1000
2023-09-26 15:39:25.293 
Epoch 102/1000 
	 loss: 21.4649, MinusLogProbMetric: 21.4649, val_loss: 21.8485, val_MinusLogProbMetric: 21.8485

Epoch 102: val_loss did not improve from 21.68647
196/196 - 76s - loss: 21.4649 - MinusLogProbMetric: 21.4649 - val_loss: 21.8485 - val_MinusLogProbMetric: 21.8485 - lr: 3.3333e-04 - 76s/epoch - 385ms/step
Epoch 103/1000
2023-09-26 15:40:41.173 
Epoch 103/1000 
	 loss: 21.4601, MinusLogProbMetric: 21.4601, val_loss: 21.0071, val_MinusLogProbMetric: 21.0071

Epoch 103: val_loss improved from 21.68647 to 21.00710, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 21.4601 - MinusLogProbMetric: 21.4601 - val_loss: 21.0071 - val_MinusLogProbMetric: 21.0071 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 104/1000
2023-09-26 15:41:58.970 
Epoch 104/1000 
	 loss: 21.3448, MinusLogProbMetric: 21.3448, val_loss: 21.9776, val_MinusLogProbMetric: 21.9776

Epoch 104: val_loss did not improve from 21.00710
196/196 - 76s - loss: 21.3448 - MinusLogProbMetric: 21.3448 - val_loss: 21.9776 - val_MinusLogProbMetric: 21.9776 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 105/1000
2023-09-26 15:43:16.805 
Epoch 105/1000 
	 loss: 21.3656, MinusLogProbMetric: 21.3656, val_loss: 20.8662, val_MinusLogProbMetric: 20.8662

Epoch 105: val_loss improved from 21.00710 to 20.86625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 79s - loss: 21.3656 - MinusLogProbMetric: 21.3656 - val_loss: 20.8662 - val_MinusLogProbMetric: 20.8662 - lr: 3.3333e-04 - 79s/epoch - 403ms/step
Epoch 106/1000
2023-09-26 15:44:34.734 
Epoch 106/1000 
	 loss: 21.2450, MinusLogProbMetric: 21.2450, val_loss: 23.0661, val_MinusLogProbMetric: 23.0661

Epoch 106: val_loss did not improve from 20.86625
196/196 - 77s - loss: 21.2450 - MinusLogProbMetric: 21.2450 - val_loss: 23.0661 - val_MinusLogProbMetric: 23.0661 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 107/1000
2023-09-26 15:45:50.705 
Epoch 107/1000 
	 loss: 21.2242, MinusLogProbMetric: 21.2242, val_loss: 21.5511, val_MinusLogProbMetric: 21.5511

Epoch 107: val_loss did not improve from 20.86625
196/196 - 76s - loss: 21.2242 - MinusLogProbMetric: 21.2242 - val_loss: 21.5511 - val_MinusLogProbMetric: 21.5511 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 108/1000
2023-09-26 15:47:06.252 
Epoch 108/1000 
	 loss: 21.1328, MinusLogProbMetric: 21.1328, val_loss: 21.4594, val_MinusLogProbMetric: 21.4594

Epoch 108: val_loss did not improve from 20.86625
196/196 - 76s - loss: 21.1328 - MinusLogProbMetric: 21.1328 - val_loss: 21.4594 - val_MinusLogProbMetric: 21.4594 - lr: 3.3333e-04 - 76s/epoch - 385ms/step
Epoch 109/1000
2023-09-26 15:48:21.891 
Epoch 109/1000 
	 loss: 21.0299, MinusLogProbMetric: 21.0299, val_loss: 20.8796, val_MinusLogProbMetric: 20.8796

Epoch 109: val_loss did not improve from 20.86625
196/196 - 76s - loss: 21.0299 - MinusLogProbMetric: 21.0299 - val_loss: 20.8796 - val_MinusLogProbMetric: 20.8796 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 110/1000
2023-09-26 15:49:37.631 
Epoch 110/1000 
	 loss: 21.0081, MinusLogProbMetric: 21.0081, val_loss: 21.1158, val_MinusLogProbMetric: 21.1158

Epoch 110: val_loss did not improve from 20.86625
196/196 - 76s - loss: 21.0081 - MinusLogProbMetric: 21.0081 - val_loss: 21.1158 - val_MinusLogProbMetric: 21.1158 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 111/1000
2023-09-26 15:50:53.029 
Epoch 111/1000 
	 loss: 21.0592, MinusLogProbMetric: 21.0592, val_loss: 20.7973, val_MinusLogProbMetric: 20.7973

Epoch 111: val_loss improved from 20.86625 to 20.79726, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 21.0592 - MinusLogProbMetric: 21.0592 - val_loss: 20.7973 - val_MinusLogProbMetric: 20.7973 - lr: 3.3333e-04 - 77s/epoch - 390ms/step
Epoch 112/1000
2023-09-26 15:52:09.371 
Epoch 112/1000 
	 loss: 21.0469, MinusLogProbMetric: 21.0469, val_loss: 20.9927, val_MinusLogProbMetric: 20.9927

Epoch 112: val_loss did not improve from 20.79726
196/196 - 75s - loss: 21.0469 - MinusLogProbMetric: 21.0469 - val_loss: 20.9927 - val_MinusLogProbMetric: 20.9927 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 113/1000
2023-09-26 15:53:24.426 
Epoch 113/1000 
	 loss: 20.7502, MinusLogProbMetric: 20.7502, val_loss: 21.5992, val_MinusLogProbMetric: 21.5992

Epoch 113: val_loss did not improve from 20.79726
196/196 - 75s - loss: 20.7502 - MinusLogProbMetric: 20.7502 - val_loss: 21.5992 - val_MinusLogProbMetric: 21.5992 - lr: 3.3333e-04 - 75s/epoch - 383ms/step
Epoch 114/1000
2023-09-26 15:54:39.723 
Epoch 114/1000 
	 loss: 20.8726, MinusLogProbMetric: 20.8726, val_loss: 20.9458, val_MinusLogProbMetric: 20.9458

Epoch 114: val_loss did not improve from 20.79726
196/196 - 75s - loss: 20.8726 - MinusLogProbMetric: 20.8726 - val_loss: 20.9458 - val_MinusLogProbMetric: 20.9458 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 115/1000
2023-09-26 15:55:54.184 
Epoch 115/1000 
	 loss: 20.7869, MinusLogProbMetric: 20.7869, val_loss: 20.4847, val_MinusLogProbMetric: 20.4847

Epoch 115: val_loss improved from 20.79726 to 20.48470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 76s - loss: 20.7869 - MinusLogProbMetric: 20.7869 - val_loss: 20.4847 - val_MinusLogProbMetric: 20.4847 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 116/1000
2023-09-26 15:57:05.156 
Epoch 116/1000 
	 loss: 20.6961, MinusLogProbMetric: 20.6961, val_loss: 21.2569, val_MinusLogProbMetric: 21.2569

Epoch 116: val_loss did not improve from 20.48470
196/196 - 70s - loss: 20.6961 - MinusLogProbMetric: 20.6961 - val_loss: 21.2569 - val_MinusLogProbMetric: 21.2569 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 117/1000
2023-09-26 15:58:14.789 
Epoch 117/1000 
	 loss: 20.6692, MinusLogProbMetric: 20.6692, val_loss: 21.8923, val_MinusLogProbMetric: 21.8923

Epoch 117: val_loss did not improve from 20.48470
196/196 - 70s - loss: 20.6692 - MinusLogProbMetric: 20.6692 - val_loss: 21.8923 - val_MinusLogProbMetric: 21.8923 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 118/1000
2023-09-26 15:59:29.807 
Epoch 118/1000 
	 loss: 20.6934, MinusLogProbMetric: 20.6934, val_loss: 20.4803, val_MinusLogProbMetric: 20.4803

Epoch 118: val_loss improved from 20.48470 to 20.48033, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 76s - loss: 20.6934 - MinusLogProbMetric: 20.6934 - val_loss: 20.4803 - val_MinusLogProbMetric: 20.4803 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 119/1000
2023-09-26 16:00:46.498 
Epoch 119/1000 
	 loss: 20.6268, MinusLogProbMetric: 20.6268, val_loss: 21.0436, val_MinusLogProbMetric: 21.0436

Epoch 119: val_loss did not improve from 20.48033
196/196 - 76s - loss: 20.6268 - MinusLogProbMetric: 20.6268 - val_loss: 21.0436 - val_MinusLogProbMetric: 21.0436 - lr: 3.3333e-04 - 76s/epoch - 385ms/step
Epoch 120/1000
2023-09-26 16:02:01.244 
Epoch 120/1000 
	 loss: 20.5738, MinusLogProbMetric: 20.5738, val_loss: 21.1260, val_MinusLogProbMetric: 21.1260

Epoch 120: val_loss did not improve from 20.48033
196/196 - 75s - loss: 20.5738 - MinusLogProbMetric: 20.5738 - val_loss: 21.1260 - val_MinusLogProbMetric: 21.1260 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 121/1000
2023-09-26 16:03:16.604 
Epoch 121/1000 
	 loss: 20.5316, MinusLogProbMetric: 20.5316, val_loss: 20.5440, val_MinusLogProbMetric: 20.5440

Epoch 121: val_loss did not improve from 20.48033
196/196 - 75s - loss: 20.5316 - MinusLogProbMetric: 20.5316 - val_loss: 20.5440 - val_MinusLogProbMetric: 20.5440 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 122/1000
2023-09-26 16:04:32.511 
Epoch 122/1000 
	 loss: 20.3956, MinusLogProbMetric: 20.3956, val_loss: 20.6947, val_MinusLogProbMetric: 20.6947

Epoch 122: val_loss did not improve from 20.48033
196/196 - 76s - loss: 20.3956 - MinusLogProbMetric: 20.3956 - val_loss: 20.6947 - val_MinusLogProbMetric: 20.6947 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 123/1000
2023-09-26 16:05:48.602 
Epoch 123/1000 
	 loss: 20.3826, MinusLogProbMetric: 20.3826, val_loss: 20.2055, val_MinusLogProbMetric: 20.2055

Epoch 123: val_loss improved from 20.48033 to 20.20549, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 20.3826 - MinusLogProbMetric: 20.3826 - val_loss: 20.2055 - val_MinusLogProbMetric: 20.2055 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 124/1000
2023-09-26 16:07:05.809 
Epoch 124/1000 
	 loss: 20.4189, MinusLogProbMetric: 20.4189, val_loss: 20.1026, val_MinusLogProbMetric: 20.1026

Epoch 124: val_loss improved from 20.20549 to 20.10256, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 20.4189 - MinusLogProbMetric: 20.4189 - val_loss: 20.1026 - val_MinusLogProbMetric: 20.1026 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 125/1000
2023-09-26 16:08:21.754 
Epoch 125/1000 
	 loss: 20.4625, MinusLogProbMetric: 20.4625, val_loss: 20.8637, val_MinusLogProbMetric: 20.8637

Epoch 125: val_loss did not improve from 20.10256
196/196 - 75s - loss: 20.4625 - MinusLogProbMetric: 20.4625 - val_loss: 20.8637 - val_MinusLogProbMetric: 20.8637 - lr: 3.3333e-04 - 75s/epoch - 382ms/step
Epoch 126/1000
2023-09-26 16:09:37.543 
Epoch 126/1000 
	 loss: 20.4287, MinusLogProbMetric: 20.4287, val_loss: 20.4057, val_MinusLogProbMetric: 20.4057

Epoch 126: val_loss did not improve from 20.10256
196/196 - 76s - loss: 20.4287 - MinusLogProbMetric: 20.4287 - val_loss: 20.4057 - val_MinusLogProbMetric: 20.4057 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 127/1000
2023-09-26 16:10:53.217 
Epoch 127/1000 
	 loss: 20.3236, MinusLogProbMetric: 20.3236, val_loss: 20.1821, val_MinusLogProbMetric: 20.1821

Epoch 127: val_loss did not improve from 20.10256
196/196 - 76s - loss: 20.3236 - MinusLogProbMetric: 20.3236 - val_loss: 20.1821 - val_MinusLogProbMetric: 20.1821 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 128/1000
2023-09-26 16:12:08.451 
Epoch 128/1000 
	 loss: 20.3140, MinusLogProbMetric: 20.3140, val_loss: 20.5501, val_MinusLogProbMetric: 20.5501

Epoch 128: val_loss did not improve from 20.10256
196/196 - 75s - loss: 20.3140 - MinusLogProbMetric: 20.3140 - val_loss: 20.5501 - val_MinusLogProbMetric: 20.5501 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 129/1000
2023-09-26 16:13:23.586 
Epoch 129/1000 
	 loss: 20.2187, MinusLogProbMetric: 20.2187, val_loss: 20.9492, val_MinusLogProbMetric: 20.9492

Epoch 129: val_loss did not improve from 20.10256
196/196 - 75s - loss: 20.2187 - MinusLogProbMetric: 20.2187 - val_loss: 20.9492 - val_MinusLogProbMetric: 20.9492 - lr: 3.3333e-04 - 75s/epoch - 383ms/step
Epoch 130/1000
2023-09-26 16:14:39.263 
Epoch 130/1000 
	 loss: 20.2702, MinusLogProbMetric: 20.2702, val_loss: 20.9859, val_MinusLogProbMetric: 20.9859

Epoch 130: val_loss did not improve from 20.10256
196/196 - 76s - loss: 20.2702 - MinusLogProbMetric: 20.2702 - val_loss: 20.9859 - val_MinusLogProbMetric: 20.9859 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 131/1000
2023-09-26 16:15:54.769 
Epoch 131/1000 
	 loss: 20.2385, MinusLogProbMetric: 20.2385, val_loss: 20.2327, val_MinusLogProbMetric: 20.2327

Epoch 131: val_loss did not improve from 20.10256
196/196 - 76s - loss: 20.2385 - MinusLogProbMetric: 20.2385 - val_loss: 20.2327 - val_MinusLogProbMetric: 20.2327 - lr: 3.3333e-04 - 76s/epoch - 385ms/step
Epoch 132/1000
2023-09-26 16:17:11.484 
Epoch 132/1000 
	 loss: 20.6843, MinusLogProbMetric: 20.6843, val_loss: 21.8284, val_MinusLogProbMetric: 21.8284

Epoch 132: val_loss did not improve from 20.10256
196/196 - 77s - loss: 20.6843 - MinusLogProbMetric: 20.6843 - val_loss: 21.8284 - val_MinusLogProbMetric: 21.8284 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 133/1000
2023-09-26 16:18:27.928 
Epoch 133/1000 
	 loss: 20.4918, MinusLogProbMetric: 20.4918, val_loss: 20.4473, val_MinusLogProbMetric: 20.4473

Epoch 133: val_loss did not improve from 20.10256
196/196 - 76s - loss: 20.4918 - MinusLogProbMetric: 20.4918 - val_loss: 20.4473 - val_MinusLogProbMetric: 20.4473 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 134/1000
2023-09-26 16:19:44.305 
Epoch 134/1000 
	 loss: 20.1643, MinusLogProbMetric: 20.1643, val_loss: 20.2114, val_MinusLogProbMetric: 20.2114

Epoch 134: val_loss did not improve from 20.10256
196/196 - 76s - loss: 20.1643 - MinusLogProbMetric: 20.1643 - val_loss: 20.2114 - val_MinusLogProbMetric: 20.2114 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 135/1000
2023-09-26 16:21:01.317 
Epoch 135/1000 
	 loss: 20.1891, MinusLogProbMetric: 20.1891, val_loss: 20.0630, val_MinusLogProbMetric: 20.0630

Epoch 135: val_loss improved from 20.10256 to 20.06295, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 20.1891 - MinusLogProbMetric: 20.1891 - val_loss: 20.0630 - val_MinusLogProbMetric: 20.0630 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 136/1000
2023-09-26 16:22:19.583 
Epoch 136/1000 
	 loss: 20.0530, MinusLogProbMetric: 20.0530, val_loss: 20.0115, val_MinusLogProbMetric: 20.0115

Epoch 136: val_loss improved from 20.06295 to 20.01152, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 20.0530 - MinusLogProbMetric: 20.0530 - val_loss: 20.0115 - val_MinusLogProbMetric: 20.0115 - lr: 3.3333e-04 - 78s/epoch - 400ms/step
Epoch 137/1000
2023-09-26 16:23:37.592 
Epoch 137/1000 
	 loss: 20.0377, MinusLogProbMetric: 20.0377, val_loss: 19.9469, val_MinusLogProbMetric: 19.9469

Epoch 137: val_loss improved from 20.01152 to 19.94687, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 20.0377 - MinusLogProbMetric: 20.0377 - val_loss: 19.9469 - val_MinusLogProbMetric: 19.9469 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 138/1000
2023-09-26 16:24:55.781 
Epoch 138/1000 
	 loss: 19.9412, MinusLogProbMetric: 19.9412, val_loss: 19.7333, val_MinusLogProbMetric: 19.7333

Epoch 138: val_loss improved from 19.94687 to 19.73331, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 19.9412 - MinusLogProbMetric: 19.9412 - val_loss: 19.7333 - val_MinusLogProbMetric: 19.7333 - lr: 3.3333e-04 - 78s/epoch - 399ms/step
Epoch 139/1000
2023-09-26 16:26:14.329 
Epoch 139/1000 
	 loss: 20.0284, MinusLogProbMetric: 20.0284, val_loss: 20.0466, val_MinusLogProbMetric: 20.0466

Epoch 139: val_loss did not improve from 19.73331
196/196 - 77s - loss: 20.0284 - MinusLogProbMetric: 20.0284 - val_loss: 20.0466 - val_MinusLogProbMetric: 20.0466 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 140/1000
2023-09-26 16:27:30.881 
Epoch 140/1000 
	 loss: 19.9795, MinusLogProbMetric: 19.9795, val_loss: 20.7237, val_MinusLogProbMetric: 20.7237

Epoch 140: val_loss did not improve from 19.73331
196/196 - 77s - loss: 19.9795 - MinusLogProbMetric: 19.9795 - val_loss: 20.7237 - val_MinusLogProbMetric: 20.7237 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 141/1000
2023-09-26 16:28:48.083 
Epoch 141/1000 
	 loss: 19.8524, MinusLogProbMetric: 19.8524, val_loss: 20.2580, val_MinusLogProbMetric: 20.2580

Epoch 141: val_loss did not improve from 19.73331
196/196 - 77s - loss: 19.8524 - MinusLogProbMetric: 19.8524 - val_loss: 20.2580 - val_MinusLogProbMetric: 20.2580 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 142/1000
2023-09-26 16:30:04.324 
Epoch 142/1000 
	 loss: 19.8777, MinusLogProbMetric: 19.8777, val_loss: 20.6530, val_MinusLogProbMetric: 20.6530

Epoch 142: val_loss did not improve from 19.73331
196/196 - 76s - loss: 19.8777 - MinusLogProbMetric: 19.8777 - val_loss: 20.6530 - val_MinusLogProbMetric: 20.6530 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 143/1000
2023-09-26 16:31:21.312 
Epoch 143/1000 
	 loss: 19.8470, MinusLogProbMetric: 19.8470, val_loss: 20.2955, val_MinusLogProbMetric: 20.2955

Epoch 143: val_loss did not improve from 19.73331
196/196 - 77s - loss: 19.8470 - MinusLogProbMetric: 19.8470 - val_loss: 20.2955 - val_MinusLogProbMetric: 20.2955 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 144/1000
2023-09-26 16:32:37.695 
Epoch 144/1000 
	 loss: 19.8287, MinusLogProbMetric: 19.8287, val_loss: 20.5952, val_MinusLogProbMetric: 20.5952

Epoch 144: val_loss did not improve from 19.73331
196/196 - 76s - loss: 19.8287 - MinusLogProbMetric: 19.8287 - val_loss: 20.5952 - val_MinusLogProbMetric: 20.5952 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 145/1000
2023-09-26 16:33:53.491 
Epoch 145/1000 
	 loss: 19.8170, MinusLogProbMetric: 19.8170, val_loss: 20.1851, val_MinusLogProbMetric: 20.1851

Epoch 145: val_loss did not improve from 19.73331
196/196 - 76s - loss: 19.8170 - MinusLogProbMetric: 19.8170 - val_loss: 20.1851 - val_MinusLogProbMetric: 20.1851 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 146/1000
2023-09-26 16:35:09.407 
Epoch 146/1000 
	 loss: 19.8385, MinusLogProbMetric: 19.8385, val_loss: 20.3220, val_MinusLogProbMetric: 20.3220

Epoch 146: val_loss did not improve from 19.73331
196/196 - 76s - loss: 19.8385 - MinusLogProbMetric: 19.8385 - val_loss: 20.3220 - val_MinusLogProbMetric: 20.3220 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 147/1000
2023-09-26 16:36:26.056 
Epoch 147/1000 
	 loss: 19.9039, MinusLogProbMetric: 19.9039, val_loss: 20.1977, val_MinusLogProbMetric: 20.1977

Epoch 147: val_loss did not improve from 19.73331
196/196 - 77s - loss: 19.9039 - MinusLogProbMetric: 19.9039 - val_loss: 20.1977 - val_MinusLogProbMetric: 20.1977 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 148/1000
2023-09-26 16:37:41.551 
Epoch 148/1000 
	 loss: 19.8676, MinusLogProbMetric: 19.8676, val_loss: 20.3034, val_MinusLogProbMetric: 20.3034

Epoch 148: val_loss did not improve from 19.73331
196/196 - 75s - loss: 19.8676 - MinusLogProbMetric: 19.8676 - val_loss: 20.3034 - val_MinusLogProbMetric: 20.3034 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 149/1000
2023-09-26 16:38:58.108 
Epoch 149/1000 
	 loss: 19.7698, MinusLogProbMetric: 19.7698, val_loss: 20.0391, val_MinusLogProbMetric: 20.0391

Epoch 149: val_loss did not improve from 19.73331
196/196 - 77s - loss: 19.7698 - MinusLogProbMetric: 19.7698 - val_loss: 20.0391 - val_MinusLogProbMetric: 20.0391 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 150/1000
2023-09-26 16:40:14.494 
Epoch 150/1000 
	 loss: 19.7233, MinusLogProbMetric: 19.7233, val_loss: 19.5644, val_MinusLogProbMetric: 19.5644

Epoch 150: val_loss improved from 19.73331 to 19.56441, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 19.7233 - MinusLogProbMetric: 19.7233 - val_loss: 19.5644 - val_MinusLogProbMetric: 19.5644 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 151/1000
2023-09-26 16:41:31.770 
Epoch 151/1000 
	 loss: 19.7573, MinusLogProbMetric: 19.7573, val_loss: 20.6926, val_MinusLogProbMetric: 20.6926

Epoch 151: val_loss did not improve from 19.56441
196/196 - 76s - loss: 19.7573 - MinusLogProbMetric: 19.7573 - val_loss: 20.6926 - val_MinusLogProbMetric: 20.6926 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 152/1000
2023-09-26 16:42:47.706 
Epoch 152/1000 
	 loss: 19.7333, MinusLogProbMetric: 19.7333, val_loss: 19.8192, val_MinusLogProbMetric: 19.8192

Epoch 152: val_loss did not improve from 19.56441
196/196 - 76s - loss: 19.7333 - MinusLogProbMetric: 19.7333 - val_loss: 19.8192 - val_MinusLogProbMetric: 19.8192 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 153/1000
2023-09-26 16:44:03.120 
Epoch 153/1000 
	 loss: 19.7076, MinusLogProbMetric: 19.7076, val_loss: 20.2778, val_MinusLogProbMetric: 20.2778

Epoch 153: val_loss did not improve from 19.56441
196/196 - 75s - loss: 19.7076 - MinusLogProbMetric: 19.7076 - val_loss: 20.2778 - val_MinusLogProbMetric: 20.2778 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 154/1000
2023-09-26 16:45:19.324 
Epoch 154/1000 
	 loss: 19.7325, MinusLogProbMetric: 19.7325, val_loss: 19.9395, val_MinusLogProbMetric: 19.9395

Epoch 154: val_loss did not improve from 19.56441
196/196 - 76s - loss: 19.7325 - MinusLogProbMetric: 19.7325 - val_loss: 19.9395 - val_MinusLogProbMetric: 19.9395 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 155/1000
2023-09-26 16:46:34.922 
Epoch 155/1000 
	 loss: 19.6388, MinusLogProbMetric: 19.6388, val_loss: 19.4953, val_MinusLogProbMetric: 19.4953

Epoch 155: val_loss improved from 19.56441 to 19.49529, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 19.6388 - MinusLogProbMetric: 19.6388 - val_loss: 19.4953 - val_MinusLogProbMetric: 19.4953 - lr: 3.3333e-04 - 77s/epoch - 393ms/step
Epoch 156/1000
2023-09-26 16:47:52.227 
Epoch 156/1000 
	 loss: 19.5580, MinusLogProbMetric: 19.5580, val_loss: 19.8417, val_MinusLogProbMetric: 19.8417

Epoch 156: val_loss did not improve from 19.49529
196/196 - 76s - loss: 19.5580 - MinusLogProbMetric: 19.5580 - val_loss: 19.8417 - val_MinusLogProbMetric: 19.8417 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 157/1000
2023-09-26 16:49:08.334 
Epoch 157/1000 
	 loss: 19.5534, MinusLogProbMetric: 19.5534, val_loss: 19.5717, val_MinusLogProbMetric: 19.5717

Epoch 157: val_loss did not improve from 19.49529
196/196 - 76s - loss: 19.5534 - MinusLogProbMetric: 19.5534 - val_loss: 19.5717 - val_MinusLogProbMetric: 19.5717 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 158/1000
2023-09-26 16:50:24.875 
Epoch 158/1000 
	 loss: 19.5501, MinusLogProbMetric: 19.5501, val_loss: 19.7789, val_MinusLogProbMetric: 19.7789

Epoch 158: val_loss did not improve from 19.49529
196/196 - 77s - loss: 19.5501 - MinusLogProbMetric: 19.5501 - val_loss: 19.7789 - val_MinusLogProbMetric: 19.7789 - lr: 3.3333e-04 - 77s/epoch - 390ms/step
Epoch 159/1000
2023-09-26 16:51:41.381 
Epoch 159/1000 
	 loss: 19.4917, MinusLogProbMetric: 19.4917, val_loss: 19.3028, val_MinusLogProbMetric: 19.3028

Epoch 159: val_loss improved from 19.49529 to 19.30276, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 19.4917 - MinusLogProbMetric: 19.4917 - val_loss: 19.3028 - val_MinusLogProbMetric: 19.3028 - lr: 3.3333e-04 - 78s/epoch - 396ms/step
Epoch 160/1000
2023-09-26 16:52:58.552 
Epoch 160/1000 
	 loss: 19.5364, MinusLogProbMetric: 19.5364, val_loss: 19.5645, val_MinusLogProbMetric: 19.5645

Epoch 160: val_loss did not improve from 19.30276
196/196 - 76s - loss: 19.5364 - MinusLogProbMetric: 19.5364 - val_loss: 19.5645 - val_MinusLogProbMetric: 19.5645 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 161/1000
2023-09-26 16:54:14.519 
Epoch 161/1000 
	 loss: 19.5075, MinusLogProbMetric: 19.5075, val_loss: 19.6027, val_MinusLogProbMetric: 19.6027

Epoch 161: val_loss did not improve from 19.30276
196/196 - 76s - loss: 19.5075 - MinusLogProbMetric: 19.5075 - val_loss: 19.6027 - val_MinusLogProbMetric: 19.6027 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 162/1000
2023-09-26 16:55:30.166 
Epoch 162/1000 
	 loss: 19.3930, MinusLogProbMetric: 19.3930, val_loss: 19.4189, val_MinusLogProbMetric: 19.4189

Epoch 162: val_loss did not improve from 19.30276
196/196 - 76s - loss: 19.3930 - MinusLogProbMetric: 19.3930 - val_loss: 19.4189 - val_MinusLogProbMetric: 19.4189 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 163/1000
2023-09-26 16:56:46.221 
Epoch 163/1000 
	 loss: 19.4437, MinusLogProbMetric: 19.4437, val_loss: 19.3443, val_MinusLogProbMetric: 19.3443

Epoch 163: val_loss did not improve from 19.30276
196/196 - 76s - loss: 19.4437 - MinusLogProbMetric: 19.4437 - val_loss: 19.3443 - val_MinusLogProbMetric: 19.3443 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 164/1000
2023-09-26 16:58:01.887 
Epoch 164/1000 
	 loss: 19.3996, MinusLogProbMetric: 19.3996, val_loss: 19.7946, val_MinusLogProbMetric: 19.7946

Epoch 164: val_loss did not improve from 19.30276
196/196 - 76s - loss: 19.3996 - MinusLogProbMetric: 19.3996 - val_loss: 19.7946 - val_MinusLogProbMetric: 19.7946 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 165/1000
2023-09-26 16:59:18.034 
Epoch 165/1000 
	 loss: 19.4323, MinusLogProbMetric: 19.4323, val_loss: 19.6867, val_MinusLogProbMetric: 19.6867

Epoch 165: val_loss did not improve from 19.30276
196/196 - 76s - loss: 19.4323 - MinusLogProbMetric: 19.4323 - val_loss: 19.6867 - val_MinusLogProbMetric: 19.6867 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 166/1000
2023-09-26 17:00:34.418 
Epoch 166/1000 
	 loss: 19.4248, MinusLogProbMetric: 19.4248, val_loss: 19.5654, val_MinusLogProbMetric: 19.5654

Epoch 166: val_loss did not improve from 19.30276
196/196 - 76s - loss: 19.4248 - MinusLogProbMetric: 19.4248 - val_loss: 19.5654 - val_MinusLogProbMetric: 19.5654 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 167/1000
2023-09-26 17:01:51.222 
Epoch 167/1000 
	 loss: 19.2931, MinusLogProbMetric: 19.2931, val_loss: 19.8387, val_MinusLogProbMetric: 19.8387

Epoch 167: val_loss did not improve from 19.30276
196/196 - 77s - loss: 19.2931 - MinusLogProbMetric: 19.2931 - val_loss: 19.8387 - val_MinusLogProbMetric: 19.8387 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 168/1000
2023-09-26 17:03:07.028 
Epoch 168/1000 
	 loss: 19.3115, MinusLogProbMetric: 19.3115, val_loss: 19.0440, val_MinusLogProbMetric: 19.0440

Epoch 168: val_loss improved from 19.30276 to 19.04403, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 19.3115 - MinusLogProbMetric: 19.3115 - val_loss: 19.0440 - val_MinusLogProbMetric: 19.0440 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 169/1000
2023-09-26 17:04:23.832 
Epoch 169/1000 
	 loss: 20.0853, MinusLogProbMetric: 20.0853, val_loss: 19.5306, val_MinusLogProbMetric: 19.5306

Epoch 169: val_loss did not improve from 19.04403
196/196 - 75s - loss: 20.0853 - MinusLogProbMetric: 20.0853 - val_loss: 19.5306 - val_MinusLogProbMetric: 19.5306 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 170/1000
2023-09-26 17:05:39.956 
Epoch 170/1000 
	 loss: 19.3477, MinusLogProbMetric: 19.3477, val_loss: 19.2728, val_MinusLogProbMetric: 19.2728

Epoch 170: val_loss did not improve from 19.04403
196/196 - 76s - loss: 19.3477 - MinusLogProbMetric: 19.3477 - val_loss: 19.2728 - val_MinusLogProbMetric: 19.2728 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 171/1000
2023-09-26 17:06:55.455 
Epoch 171/1000 
	 loss: 19.2486, MinusLogProbMetric: 19.2486, val_loss: 19.3184, val_MinusLogProbMetric: 19.3184

Epoch 171: val_loss did not improve from 19.04403
196/196 - 75s - loss: 19.2486 - MinusLogProbMetric: 19.2486 - val_loss: 19.3184 - val_MinusLogProbMetric: 19.3184 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 172/1000
2023-09-26 17:08:11.191 
Epoch 172/1000 
	 loss: 19.4077, MinusLogProbMetric: 19.4077, val_loss: 19.4479, val_MinusLogProbMetric: 19.4479

Epoch 172: val_loss did not improve from 19.04403
196/196 - 76s - loss: 19.4077 - MinusLogProbMetric: 19.4077 - val_loss: 19.4479 - val_MinusLogProbMetric: 19.4479 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 173/1000
2023-09-26 17:09:26.823 
Epoch 173/1000 
	 loss: 19.3382, MinusLogProbMetric: 19.3382, val_loss: 19.2392, val_MinusLogProbMetric: 19.2392

Epoch 173: val_loss did not improve from 19.04403
196/196 - 76s - loss: 19.3382 - MinusLogProbMetric: 19.3382 - val_loss: 19.2392 - val_MinusLogProbMetric: 19.2392 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 174/1000
2023-09-26 17:10:43.426 
Epoch 174/1000 
	 loss: 19.2693, MinusLogProbMetric: 19.2693, val_loss: 19.5316, val_MinusLogProbMetric: 19.5316

Epoch 174: val_loss did not improve from 19.04403
196/196 - 77s - loss: 19.2693 - MinusLogProbMetric: 19.2693 - val_loss: 19.5316 - val_MinusLogProbMetric: 19.5316 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 175/1000
2023-09-26 17:12:00.296 
Epoch 175/1000 
	 loss: 19.2620, MinusLogProbMetric: 19.2620, val_loss: 19.4940, val_MinusLogProbMetric: 19.4940

Epoch 175: val_loss did not improve from 19.04403
196/196 - 77s - loss: 19.2620 - MinusLogProbMetric: 19.2620 - val_loss: 19.4940 - val_MinusLogProbMetric: 19.4940 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 176/1000
2023-09-26 17:13:16.844 
Epoch 176/1000 
	 loss: 19.2063, MinusLogProbMetric: 19.2063, val_loss: 19.3403, val_MinusLogProbMetric: 19.3403

Epoch 176: val_loss did not improve from 19.04403
196/196 - 77s - loss: 19.2063 - MinusLogProbMetric: 19.2063 - val_loss: 19.3403 - val_MinusLogProbMetric: 19.3403 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 177/1000
2023-09-26 17:14:32.670 
Epoch 177/1000 
	 loss: 19.2255, MinusLogProbMetric: 19.2255, val_loss: 19.4985, val_MinusLogProbMetric: 19.4985

Epoch 177: val_loss did not improve from 19.04403
196/196 - 76s - loss: 19.2255 - MinusLogProbMetric: 19.2255 - val_loss: 19.4985 - val_MinusLogProbMetric: 19.4985 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 178/1000
2023-09-26 17:15:48.260 
Epoch 178/1000 
	 loss: 19.1614, MinusLogProbMetric: 19.1614, val_loss: 19.6818, val_MinusLogProbMetric: 19.6818

Epoch 178: val_loss did not improve from 19.04403
196/196 - 76s - loss: 19.1614 - MinusLogProbMetric: 19.1614 - val_loss: 19.6818 - val_MinusLogProbMetric: 19.6818 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 179/1000
2023-09-26 17:17:04.669 
Epoch 179/1000 
	 loss: 19.1419, MinusLogProbMetric: 19.1419, val_loss: 19.2692, val_MinusLogProbMetric: 19.2692

Epoch 179: val_loss did not improve from 19.04403
196/196 - 76s - loss: 19.1419 - MinusLogProbMetric: 19.1419 - val_loss: 19.2692 - val_MinusLogProbMetric: 19.2692 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 180/1000
2023-09-26 17:18:20.314 
Epoch 180/1000 
	 loss: 19.2698, MinusLogProbMetric: 19.2698, val_loss: 19.2452, val_MinusLogProbMetric: 19.2452

Epoch 180: val_loss did not improve from 19.04403
196/196 - 76s - loss: 19.2698 - MinusLogProbMetric: 19.2698 - val_loss: 19.2452 - val_MinusLogProbMetric: 19.2452 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 181/1000
2023-09-26 17:19:36.140 
Epoch 181/1000 
	 loss: 19.1425, MinusLogProbMetric: 19.1425, val_loss: 19.5349, val_MinusLogProbMetric: 19.5349

Epoch 181: val_loss did not improve from 19.04403
196/196 - 76s - loss: 19.1425 - MinusLogProbMetric: 19.1425 - val_loss: 19.5349 - val_MinusLogProbMetric: 19.5349 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 182/1000
2023-09-26 17:20:51.947 
Epoch 182/1000 
	 loss: 19.1294, MinusLogProbMetric: 19.1294, val_loss: 19.1806, val_MinusLogProbMetric: 19.1806

Epoch 182: val_loss did not improve from 19.04403
196/196 - 76s - loss: 19.1294 - MinusLogProbMetric: 19.1294 - val_loss: 19.1806 - val_MinusLogProbMetric: 19.1806 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 183/1000
2023-09-26 17:22:07.151 
Epoch 183/1000 
	 loss: 19.1445, MinusLogProbMetric: 19.1445, val_loss: 19.0582, val_MinusLogProbMetric: 19.0582

Epoch 183: val_loss did not improve from 19.04403
196/196 - 75s - loss: 19.1445 - MinusLogProbMetric: 19.1445 - val_loss: 19.0582 - val_MinusLogProbMetric: 19.0582 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 184/1000
2023-09-26 17:23:23.584 
Epoch 184/1000 
	 loss: 19.0620, MinusLogProbMetric: 19.0620, val_loss: 18.7318, val_MinusLogProbMetric: 18.7318

Epoch 184: val_loss improved from 19.04403 to 18.73184, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 78s - loss: 19.0620 - MinusLogProbMetric: 19.0620 - val_loss: 18.7318 - val_MinusLogProbMetric: 18.7318 - lr: 3.3333e-04 - 78s/epoch - 397ms/step
Epoch 185/1000
2023-09-26 17:24:41.590 
Epoch 185/1000 
	 loss: 19.0920, MinusLogProbMetric: 19.0920, val_loss: 19.7005, val_MinusLogProbMetric: 19.7005

Epoch 185: val_loss did not improve from 18.73184
196/196 - 77s - loss: 19.0920 - MinusLogProbMetric: 19.0920 - val_loss: 19.7005 - val_MinusLogProbMetric: 19.7005 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 186/1000
2023-09-26 17:25:57.985 
Epoch 186/1000 
	 loss: 19.0858, MinusLogProbMetric: 19.0858, val_loss: 19.1255, val_MinusLogProbMetric: 19.1255

Epoch 186: val_loss did not improve from 18.73184
196/196 - 76s - loss: 19.0858 - MinusLogProbMetric: 19.0858 - val_loss: 19.1255 - val_MinusLogProbMetric: 19.1255 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 187/1000
2023-09-26 17:27:14.234 
Epoch 187/1000 
	 loss: 19.1413, MinusLogProbMetric: 19.1413, val_loss: 19.0503, val_MinusLogProbMetric: 19.0503

Epoch 187: val_loss did not improve from 18.73184
196/196 - 76s - loss: 19.1413 - MinusLogProbMetric: 19.1413 - val_loss: 19.0503 - val_MinusLogProbMetric: 19.0503 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 188/1000
2023-09-26 17:28:30.688 
Epoch 188/1000 
	 loss: 19.0315, MinusLogProbMetric: 19.0315, val_loss: 19.4670, val_MinusLogProbMetric: 19.4670

Epoch 188: val_loss did not improve from 18.73184
196/196 - 76s - loss: 19.0315 - MinusLogProbMetric: 19.0315 - val_loss: 19.4670 - val_MinusLogProbMetric: 19.4670 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 189/1000
2023-09-26 17:29:46.983 
Epoch 189/1000 
	 loss: 19.0670, MinusLogProbMetric: 19.0670, val_loss: 19.1463, val_MinusLogProbMetric: 19.1463

Epoch 189: val_loss did not improve from 18.73184
196/196 - 76s - loss: 19.0670 - MinusLogProbMetric: 19.0670 - val_loss: 19.1463 - val_MinusLogProbMetric: 19.1463 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 190/1000
2023-09-26 17:31:03.104 
Epoch 190/1000 
	 loss: 19.0477, MinusLogProbMetric: 19.0477, val_loss: 19.2175, val_MinusLogProbMetric: 19.2175

Epoch 190: val_loss did not improve from 18.73184
196/196 - 76s - loss: 19.0477 - MinusLogProbMetric: 19.0477 - val_loss: 19.2175 - val_MinusLogProbMetric: 19.2175 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 191/1000
2023-09-26 17:32:19.495 
Epoch 191/1000 
	 loss: 19.0742, MinusLogProbMetric: 19.0742, val_loss: 19.4861, val_MinusLogProbMetric: 19.4861

Epoch 191: val_loss did not improve from 18.73184
196/196 - 76s - loss: 19.0742 - MinusLogProbMetric: 19.0742 - val_loss: 19.4861 - val_MinusLogProbMetric: 19.4861 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 192/1000
2023-09-26 17:33:36.217 
Epoch 192/1000 
	 loss: 19.0442, MinusLogProbMetric: 19.0442, val_loss: 19.1789, val_MinusLogProbMetric: 19.1789

Epoch 192: val_loss did not improve from 18.73184
196/196 - 77s - loss: 19.0442 - MinusLogProbMetric: 19.0442 - val_loss: 19.1789 - val_MinusLogProbMetric: 19.1789 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 193/1000
2023-09-26 17:34:51.364 
Epoch 193/1000 
	 loss: 19.0342, MinusLogProbMetric: 19.0342, val_loss: 19.2328, val_MinusLogProbMetric: 19.2328

Epoch 193: val_loss did not improve from 18.73184
196/196 - 75s - loss: 19.0342 - MinusLogProbMetric: 19.0342 - val_loss: 19.2328 - val_MinusLogProbMetric: 19.2328 - lr: 3.3333e-04 - 75s/epoch - 383ms/step
Epoch 194/1000
2023-09-26 17:36:07.293 
Epoch 194/1000 
	 loss: 18.9678, MinusLogProbMetric: 18.9678, val_loss: 19.2710, val_MinusLogProbMetric: 19.2710

Epoch 194: val_loss did not improve from 18.73184
196/196 - 76s - loss: 18.9678 - MinusLogProbMetric: 18.9678 - val_loss: 19.2710 - val_MinusLogProbMetric: 19.2710 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 195/1000
2023-09-26 17:37:23.184 
Epoch 195/1000 
	 loss: 19.0419, MinusLogProbMetric: 19.0419, val_loss: 19.2669, val_MinusLogProbMetric: 19.2669

Epoch 195: val_loss did not improve from 18.73184
196/196 - 76s - loss: 19.0419 - MinusLogProbMetric: 19.0419 - val_loss: 19.2669 - val_MinusLogProbMetric: 19.2669 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 196/1000
2023-09-26 17:38:39.750 
Epoch 196/1000 
	 loss: 18.9929, MinusLogProbMetric: 18.9929, val_loss: 18.9448, val_MinusLogProbMetric: 18.9448

Epoch 196: val_loss did not improve from 18.73184
196/196 - 77s - loss: 18.9929 - MinusLogProbMetric: 18.9929 - val_loss: 18.9448 - val_MinusLogProbMetric: 18.9448 - lr: 3.3333e-04 - 77s/epoch - 391ms/step
Epoch 197/1000
2023-09-26 17:39:56.501 
Epoch 197/1000 
	 loss: 18.9064, MinusLogProbMetric: 18.9064, val_loss: 18.9550, val_MinusLogProbMetric: 18.9550

Epoch 197: val_loss did not improve from 18.73184
196/196 - 77s - loss: 18.9064 - MinusLogProbMetric: 18.9064 - val_loss: 18.9550 - val_MinusLogProbMetric: 18.9550 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 198/1000
2023-09-26 17:41:11.972 
Epoch 198/1000 
	 loss: 18.8851, MinusLogProbMetric: 18.8851, val_loss: 19.2125, val_MinusLogProbMetric: 19.2125

Epoch 198: val_loss did not improve from 18.73184
196/196 - 75s - loss: 18.8851 - MinusLogProbMetric: 18.8851 - val_loss: 19.2125 - val_MinusLogProbMetric: 19.2125 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 199/1000
2023-09-26 17:42:27.767 
Epoch 199/1000 
	 loss: 18.8982, MinusLogProbMetric: 18.8982, val_loss: 18.9728, val_MinusLogProbMetric: 18.9728

Epoch 199: val_loss did not improve from 18.73184
196/196 - 76s - loss: 18.8982 - MinusLogProbMetric: 18.8982 - val_loss: 18.9728 - val_MinusLogProbMetric: 18.9728 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 200/1000
2023-09-26 17:43:43.215 
Epoch 200/1000 
	 loss: 18.9743, MinusLogProbMetric: 18.9743, val_loss: 18.8970, val_MinusLogProbMetric: 18.8970

Epoch 200: val_loss did not improve from 18.73184
196/196 - 75s - loss: 18.9743 - MinusLogProbMetric: 18.9743 - val_loss: 18.8970 - val_MinusLogProbMetric: 18.8970 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 201/1000
2023-09-26 17:44:59.393 
Epoch 201/1000 
	 loss: 18.8817, MinusLogProbMetric: 18.8817, val_loss: 19.4298, val_MinusLogProbMetric: 19.4298

Epoch 201: val_loss did not improve from 18.73184
196/196 - 76s - loss: 18.8817 - MinusLogProbMetric: 18.8817 - val_loss: 19.4298 - val_MinusLogProbMetric: 19.4298 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 202/1000
2023-09-26 17:46:15.039 
Epoch 202/1000 
	 loss: 18.9051, MinusLogProbMetric: 18.9051, val_loss: 19.0918, val_MinusLogProbMetric: 19.0918

Epoch 202: val_loss did not improve from 18.73184
196/196 - 76s - loss: 18.9051 - MinusLogProbMetric: 18.9051 - val_loss: 19.0918 - val_MinusLogProbMetric: 19.0918 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 203/1000
2023-09-26 17:47:30.498 
Epoch 203/1000 
	 loss: 18.8786, MinusLogProbMetric: 18.8786, val_loss: 18.9727, val_MinusLogProbMetric: 18.9727

Epoch 203: val_loss did not improve from 18.73184
196/196 - 75s - loss: 18.8786 - MinusLogProbMetric: 18.8786 - val_loss: 18.9727 - val_MinusLogProbMetric: 18.9727 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 204/1000
2023-09-26 17:48:46.219 
Epoch 204/1000 
	 loss: 18.8947, MinusLogProbMetric: 18.8947, val_loss: 18.8586, val_MinusLogProbMetric: 18.8586

Epoch 204: val_loss did not improve from 18.73184
196/196 - 76s - loss: 18.8947 - MinusLogProbMetric: 18.8947 - val_loss: 18.8586 - val_MinusLogProbMetric: 18.8586 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 205/1000
2023-09-26 17:50:02.643 
Epoch 205/1000 
	 loss: 18.8905, MinusLogProbMetric: 18.8905, val_loss: 19.0364, val_MinusLogProbMetric: 19.0364

Epoch 205: val_loss did not improve from 18.73184
196/196 - 76s - loss: 18.8905 - MinusLogProbMetric: 18.8905 - val_loss: 19.0364 - val_MinusLogProbMetric: 19.0364 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 206/1000
2023-09-26 17:51:19.552 
Epoch 206/1000 
	 loss: 18.8908, MinusLogProbMetric: 18.8908, val_loss: 18.7720, val_MinusLogProbMetric: 18.7720

Epoch 206: val_loss did not improve from 18.73184
196/196 - 77s - loss: 18.8908 - MinusLogProbMetric: 18.8908 - val_loss: 18.7720 - val_MinusLogProbMetric: 18.7720 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 207/1000
2023-09-26 17:52:35.145 
Epoch 207/1000 
	 loss: 18.9217, MinusLogProbMetric: 18.9217, val_loss: 20.4864, val_MinusLogProbMetric: 20.4864

Epoch 207: val_loss did not improve from 18.73184
196/196 - 76s - loss: 18.9217 - MinusLogProbMetric: 18.9217 - val_loss: 20.4864 - val_MinusLogProbMetric: 20.4864 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 208/1000
2023-09-26 17:53:51.330 
Epoch 208/1000 
	 loss: 18.8367, MinusLogProbMetric: 18.8367, val_loss: 19.0200, val_MinusLogProbMetric: 19.0200

Epoch 208: val_loss did not improve from 18.73184
196/196 - 76s - loss: 18.8367 - MinusLogProbMetric: 18.8367 - val_loss: 19.0200 - val_MinusLogProbMetric: 19.0200 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 209/1000
2023-09-26 17:55:07.511 
Epoch 209/1000 
	 loss: 18.8442, MinusLogProbMetric: 18.8442, val_loss: 18.8192, val_MinusLogProbMetric: 18.8192

Epoch 209: val_loss did not improve from 18.73184
196/196 - 76s - loss: 18.8442 - MinusLogProbMetric: 18.8442 - val_loss: 18.8192 - val_MinusLogProbMetric: 18.8192 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 210/1000
2023-09-26 17:56:23.574 
Epoch 210/1000 
	 loss: 18.8646, MinusLogProbMetric: 18.8646, val_loss: 18.9930, val_MinusLogProbMetric: 18.9930

Epoch 210: val_loss did not improve from 18.73184
196/196 - 76s - loss: 18.8646 - MinusLogProbMetric: 18.8646 - val_loss: 18.9930 - val_MinusLogProbMetric: 18.9930 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 211/1000
2023-09-26 17:57:39.897 
Epoch 211/1000 
	 loss: 18.7835, MinusLogProbMetric: 18.7835, val_loss: 18.9035, val_MinusLogProbMetric: 18.9035

Epoch 211: val_loss did not improve from 18.73184
196/196 - 76s - loss: 18.7835 - MinusLogProbMetric: 18.7835 - val_loss: 18.9035 - val_MinusLogProbMetric: 18.9035 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 212/1000
2023-09-26 17:58:55.209 
Epoch 212/1000 
	 loss: 18.8671, MinusLogProbMetric: 18.8671, val_loss: 19.0720, val_MinusLogProbMetric: 19.0720

Epoch 212: val_loss did not improve from 18.73184
196/196 - 75s - loss: 18.8671 - MinusLogProbMetric: 18.8671 - val_loss: 19.0720 - val_MinusLogProbMetric: 19.0720 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 213/1000
2023-09-26 18:00:11.121 
Epoch 213/1000 
	 loss: 18.7306, MinusLogProbMetric: 18.7306, val_loss: 18.6119, val_MinusLogProbMetric: 18.6119

Epoch 213: val_loss improved from 18.73184 to 18.61194, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 18.7306 - MinusLogProbMetric: 18.7306 - val_loss: 18.6119 - val_MinusLogProbMetric: 18.6119 - lr: 3.3333e-04 - 77s/epoch - 394ms/step
Epoch 214/1000
2023-09-26 18:01:28.641 
Epoch 214/1000 
	 loss: 18.7896, MinusLogProbMetric: 18.7896, val_loss: 19.1598, val_MinusLogProbMetric: 19.1598

Epoch 214: val_loss did not improve from 18.61194
196/196 - 76s - loss: 18.7896 - MinusLogProbMetric: 18.7896 - val_loss: 19.1598 - val_MinusLogProbMetric: 19.1598 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 215/1000
2023-09-26 18:02:43.974 
Epoch 215/1000 
	 loss: 18.7845, MinusLogProbMetric: 18.7845, val_loss: 20.1897, val_MinusLogProbMetric: 20.1897

Epoch 215: val_loss did not improve from 18.61194
196/196 - 75s - loss: 18.7845 - MinusLogProbMetric: 18.7845 - val_loss: 20.1897 - val_MinusLogProbMetric: 20.1897 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 216/1000
2023-09-26 18:03:59.334 
Epoch 216/1000 
	 loss: 18.8823, MinusLogProbMetric: 18.8823, val_loss: 18.8524, val_MinusLogProbMetric: 18.8524

Epoch 216: val_loss did not improve from 18.61194
196/196 - 75s - loss: 18.8823 - MinusLogProbMetric: 18.8823 - val_loss: 18.8524 - val_MinusLogProbMetric: 18.8524 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 217/1000
2023-09-26 18:05:15.274 
Epoch 217/1000 
	 loss: 18.7601, MinusLogProbMetric: 18.7601, val_loss: 18.6812, val_MinusLogProbMetric: 18.6812

Epoch 217: val_loss did not improve from 18.61194
196/196 - 76s - loss: 18.7601 - MinusLogProbMetric: 18.7601 - val_loss: 18.6812 - val_MinusLogProbMetric: 18.6812 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 218/1000
2023-09-26 18:06:30.942 
Epoch 218/1000 
	 loss: 18.7671, MinusLogProbMetric: 18.7671, val_loss: 18.7443, val_MinusLogProbMetric: 18.7443

Epoch 218: val_loss did not improve from 18.61194
196/196 - 76s - loss: 18.7671 - MinusLogProbMetric: 18.7671 - val_loss: 18.7443 - val_MinusLogProbMetric: 18.7443 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 219/1000
2023-09-26 18:07:46.446 
Epoch 219/1000 
	 loss: 18.6899, MinusLogProbMetric: 18.6899, val_loss: 18.6606, val_MinusLogProbMetric: 18.6606

Epoch 219: val_loss did not improve from 18.61194
196/196 - 76s - loss: 18.6899 - MinusLogProbMetric: 18.6899 - val_loss: 18.6606 - val_MinusLogProbMetric: 18.6606 - lr: 3.3333e-04 - 76s/epoch - 385ms/step
Epoch 220/1000
2023-09-26 18:09:01.860 
Epoch 220/1000 
	 loss: 18.7329, MinusLogProbMetric: 18.7329, val_loss: 18.7650, val_MinusLogProbMetric: 18.7650

Epoch 220: val_loss did not improve from 18.61194
196/196 - 75s - loss: 18.7329 - MinusLogProbMetric: 18.7329 - val_loss: 18.7650 - val_MinusLogProbMetric: 18.7650 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 221/1000
2023-09-26 18:10:17.384 
Epoch 221/1000 
	 loss: 18.7020, MinusLogProbMetric: 18.7020, val_loss: 19.5790, val_MinusLogProbMetric: 19.5790

Epoch 221: val_loss did not improve from 18.61194
196/196 - 76s - loss: 18.7020 - MinusLogProbMetric: 18.7020 - val_loss: 19.5790 - val_MinusLogProbMetric: 19.5790 - lr: 3.3333e-04 - 76s/epoch - 385ms/step
Epoch 222/1000
2023-09-26 18:11:32.968 
Epoch 222/1000 
	 loss: 18.7897, MinusLogProbMetric: 18.7897, val_loss: 19.1380, val_MinusLogProbMetric: 19.1380

Epoch 222: val_loss did not improve from 18.61194
196/196 - 76s - loss: 18.7897 - MinusLogProbMetric: 18.7897 - val_loss: 19.1380 - val_MinusLogProbMetric: 19.1380 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 223/1000
2023-09-26 18:12:48.676 
Epoch 223/1000 
	 loss: 18.7273, MinusLogProbMetric: 18.7273, val_loss: 18.6846, val_MinusLogProbMetric: 18.6846

Epoch 223: val_loss did not improve from 18.61194
196/196 - 76s - loss: 18.7273 - MinusLogProbMetric: 18.7273 - val_loss: 18.6846 - val_MinusLogProbMetric: 18.6846 - lr: 3.3333e-04 - 76s/epoch - 386ms/step
Epoch 224/1000
2023-09-26 18:14:03.478 
Epoch 224/1000 
	 loss: 18.7784, MinusLogProbMetric: 18.7784, val_loss: 19.0171, val_MinusLogProbMetric: 19.0171

Epoch 224: val_loss did not improve from 18.61194
196/196 - 75s - loss: 18.7784 - MinusLogProbMetric: 18.7784 - val_loss: 19.0171 - val_MinusLogProbMetric: 19.0171 - lr: 3.3333e-04 - 75s/epoch - 382ms/step
Epoch 225/1000
2023-09-26 18:15:18.591 
Epoch 225/1000 
	 loss: 18.6771, MinusLogProbMetric: 18.6771, val_loss: 18.6636, val_MinusLogProbMetric: 18.6636

Epoch 225: val_loss did not improve from 18.61194
196/196 - 75s - loss: 18.6771 - MinusLogProbMetric: 18.6771 - val_loss: 18.6636 - val_MinusLogProbMetric: 18.6636 - lr: 3.3333e-04 - 75s/epoch - 383ms/step
Epoch 226/1000
2023-09-26 18:16:33.663 
Epoch 226/1000 
	 loss: 18.7107, MinusLogProbMetric: 18.7107, val_loss: 19.1760, val_MinusLogProbMetric: 19.1760

Epoch 226: val_loss did not improve from 18.61194
196/196 - 75s - loss: 18.7107 - MinusLogProbMetric: 18.7107 - val_loss: 19.1760 - val_MinusLogProbMetric: 19.1760 - lr: 3.3333e-04 - 75s/epoch - 383ms/step
Epoch 227/1000
2023-09-26 18:17:49.175 
Epoch 227/1000 
	 loss: 18.7488, MinusLogProbMetric: 18.7488, val_loss: 18.6065, val_MinusLogProbMetric: 18.6065

Epoch 227: val_loss improved from 18.61194 to 18.60647, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 77s - loss: 18.7488 - MinusLogProbMetric: 18.7488 - val_loss: 18.6065 - val_MinusLogProbMetric: 18.6065 - lr: 3.3333e-04 - 77s/epoch - 392ms/step
Epoch 228/1000
2023-09-26 18:19:05.690 
Epoch 228/1000 
	 loss: 18.6796, MinusLogProbMetric: 18.6796, val_loss: 18.6208, val_MinusLogProbMetric: 18.6208

Epoch 228: val_loss did not improve from 18.60647
196/196 - 75s - loss: 18.6796 - MinusLogProbMetric: 18.6796 - val_loss: 18.6208 - val_MinusLogProbMetric: 18.6208 - lr: 3.3333e-04 - 75s/epoch - 384ms/step
Epoch 229/1000
2023-09-26 18:20:20.152 
Epoch 229/1000 
	 loss: 18.6739, MinusLogProbMetric: 18.6739, val_loss: 18.9163, val_MinusLogProbMetric: 18.9163

Epoch 229: val_loss did not improve from 18.60647
196/196 - 74s - loss: 18.6739 - MinusLogProbMetric: 18.6739 - val_loss: 18.9163 - val_MinusLogProbMetric: 18.9163 - lr: 3.3333e-04 - 74s/epoch - 380ms/step
Epoch 230/1000
2023-09-26 18:21:35.006 
Epoch 230/1000 
	 loss: 18.6368, MinusLogProbMetric: 18.6368, val_loss: 18.5455, val_MinusLogProbMetric: 18.5455

Epoch 230: val_loss improved from 18.60647 to 18.54545, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 76s - loss: 18.6368 - MinusLogProbMetric: 18.6368 - val_loss: 18.5455 - val_MinusLogProbMetric: 18.5455 - lr: 3.3333e-04 - 76s/epoch - 388ms/step
Epoch 231/1000
2023-09-26 18:22:51.391 
Epoch 231/1000 
	 loss: 18.6268, MinusLogProbMetric: 18.6268, val_loss: 18.7866, val_MinusLogProbMetric: 18.7866

Epoch 231: val_loss did not improve from 18.54545
196/196 - 75s - loss: 18.6268 - MinusLogProbMetric: 18.6268 - val_loss: 18.7866 - val_MinusLogProbMetric: 18.7866 - lr: 3.3333e-04 - 75s/epoch - 383ms/step
Epoch 232/1000
2023-09-26 18:24:06.040 
Epoch 232/1000 
	 loss: 18.6760, MinusLogProbMetric: 18.6760, val_loss: 18.6583, val_MinusLogProbMetric: 18.6583

Epoch 232: val_loss did not improve from 18.54545
196/196 - 75s - loss: 18.6760 - MinusLogProbMetric: 18.6760 - val_loss: 18.6583 - val_MinusLogProbMetric: 18.6583 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 233/1000
2023-09-26 18:25:20.684 
Epoch 233/1000 
	 loss: 18.6170, MinusLogProbMetric: 18.6170, val_loss: 18.6783, val_MinusLogProbMetric: 18.6783

Epoch 233: val_loss did not improve from 18.54545
196/196 - 75s - loss: 18.6170 - MinusLogProbMetric: 18.6170 - val_loss: 18.6783 - val_MinusLogProbMetric: 18.6783 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 234/1000
2023-09-26 18:26:35.011 
Epoch 234/1000 
	 loss: 18.6267, MinusLogProbMetric: 18.6267, val_loss: 18.5402, val_MinusLogProbMetric: 18.5402

Epoch 234: val_loss improved from 18.54545 to 18.54025, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 18.6267 - MinusLogProbMetric: 18.6267 - val_loss: 18.5402 - val_MinusLogProbMetric: 18.5402 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 235/1000
2023-09-26 18:27:50.519 
Epoch 235/1000 
	 loss: 18.6488, MinusLogProbMetric: 18.6488, val_loss: 18.5186, val_MinusLogProbMetric: 18.5186

Epoch 235: val_loss improved from 18.54025 to 18.51857, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 76s - loss: 18.6488 - MinusLogProbMetric: 18.6488 - val_loss: 18.5186 - val_MinusLogProbMetric: 18.5186 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 236/1000
2023-09-26 18:29:06.592 
Epoch 236/1000 
	 loss: 18.5834, MinusLogProbMetric: 18.5834, val_loss: 18.5720, val_MinusLogProbMetric: 18.5720

Epoch 236: val_loss did not improve from 18.51857
196/196 - 75s - loss: 18.5834 - MinusLogProbMetric: 18.5834 - val_loss: 18.5720 - val_MinusLogProbMetric: 18.5720 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 237/1000
2023-09-26 18:30:20.821 
Epoch 237/1000 
	 loss: 18.5982, MinusLogProbMetric: 18.5982, val_loss: 18.5415, val_MinusLogProbMetric: 18.5415

Epoch 237: val_loss did not improve from 18.51857
196/196 - 74s - loss: 18.5982 - MinusLogProbMetric: 18.5982 - val_loss: 18.5415 - val_MinusLogProbMetric: 18.5415 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 238/1000
2023-09-26 18:31:35.462 
Epoch 238/1000 
	 loss: 18.6206, MinusLogProbMetric: 18.6206, val_loss: 18.4607, val_MinusLogProbMetric: 18.4607

Epoch 238: val_loss improved from 18.51857 to 18.46067, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 76s - loss: 18.6206 - MinusLogProbMetric: 18.6206 - val_loss: 18.4607 - val_MinusLogProbMetric: 18.4607 - lr: 3.3333e-04 - 76s/epoch - 387ms/step
Epoch 239/1000
2023-09-26 18:32:50.959 
Epoch 239/1000 
	 loss: 18.5430, MinusLogProbMetric: 18.5430, val_loss: 18.7219, val_MinusLogProbMetric: 18.7219

Epoch 239: val_loss did not improve from 18.46067
196/196 - 74s - loss: 18.5430 - MinusLogProbMetric: 18.5430 - val_loss: 18.7219 - val_MinusLogProbMetric: 18.7219 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 240/1000
2023-09-26 18:34:05.349 
Epoch 240/1000 
	 loss: 18.5597, MinusLogProbMetric: 18.5597, val_loss: 18.3954, val_MinusLogProbMetric: 18.3954

Epoch 240: val_loss improved from 18.46067 to 18.39537, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 18.5597 - MinusLogProbMetric: 18.5597 - val_loss: 18.3954 - val_MinusLogProbMetric: 18.3954 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 241/1000
2023-09-26 18:35:20.917 
Epoch 241/1000 
	 loss: 18.5780, MinusLogProbMetric: 18.5780, val_loss: 18.6385, val_MinusLogProbMetric: 18.6385

Epoch 241: val_loss did not improve from 18.39537
196/196 - 74s - loss: 18.5780 - MinusLogProbMetric: 18.5780 - val_loss: 18.6385 - val_MinusLogProbMetric: 18.6385 - lr: 3.3333e-04 - 74s/epoch - 380ms/step
Epoch 242/1000
2023-09-26 18:36:35.209 
Epoch 242/1000 
	 loss: 18.6271, MinusLogProbMetric: 18.6271, val_loss: 19.0586, val_MinusLogProbMetric: 19.0586

Epoch 242: val_loss did not improve from 18.39537
196/196 - 74s - loss: 18.6271 - MinusLogProbMetric: 18.6271 - val_loss: 19.0586 - val_MinusLogProbMetric: 19.0586 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 243/1000
2023-09-26 18:37:49.434 
Epoch 243/1000 
	 loss: 18.5303, MinusLogProbMetric: 18.5303, val_loss: 18.7058, val_MinusLogProbMetric: 18.7058

Epoch 243: val_loss did not improve from 18.39537
196/196 - 74s - loss: 18.5303 - MinusLogProbMetric: 18.5303 - val_loss: 18.7058 - val_MinusLogProbMetric: 18.7058 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 244/1000
2023-09-26 18:39:03.524 
Epoch 244/1000 
	 loss: 18.6885, MinusLogProbMetric: 18.6885, val_loss: 18.8623, val_MinusLogProbMetric: 18.8623

Epoch 244: val_loss did not improve from 18.39537
196/196 - 74s - loss: 18.6885 - MinusLogProbMetric: 18.6885 - val_loss: 18.8623 - val_MinusLogProbMetric: 18.8623 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 245/1000
2023-09-26 18:40:18.595 
Epoch 245/1000 
	 loss: 18.4852, MinusLogProbMetric: 18.4852, val_loss: 18.4896, val_MinusLogProbMetric: 18.4896

Epoch 245: val_loss did not improve from 18.39537
196/196 - 75s - loss: 18.4852 - MinusLogProbMetric: 18.4852 - val_loss: 18.4896 - val_MinusLogProbMetric: 18.4896 - lr: 3.3333e-04 - 75s/epoch - 383ms/step
Epoch 246/1000
2023-09-26 18:41:32.362 
Epoch 246/1000 
	 loss: 18.5331, MinusLogProbMetric: 18.5331, val_loss: 18.6974, val_MinusLogProbMetric: 18.6974

Epoch 246: val_loss did not improve from 18.39537
196/196 - 74s - loss: 18.5331 - MinusLogProbMetric: 18.5331 - val_loss: 18.6974 - val_MinusLogProbMetric: 18.6974 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 247/1000
2023-09-26 18:42:46.867 
Epoch 247/1000 
	 loss: 18.6174, MinusLogProbMetric: 18.6174, val_loss: 18.4758, val_MinusLogProbMetric: 18.4758

Epoch 247: val_loss did not improve from 18.39537
196/196 - 75s - loss: 18.6174 - MinusLogProbMetric: 18.6174 - val_loss: 18.4758 - val_MinusLogProbMetric: 18.4758 - lr: 3.3333e-04 - 75s/epoch - 380ms/step
Epoch 248/1000
2023-09-26 18:44:00.869 
Epoch 248/1000 
	 loss: 18.5938, MinusLogProbMetric: 18.5938, val_loss: 19.0307, val_MinusLogProbMetric: 19.0307

Epoch 248: val_loss did not improve from 18.39537
196/196 - 74s - loss: 18.5938 - MinusLogProbMetric: 18.5938 - val_loss: 19.0307 - val_MinusLogProbMetric: 19.0307 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 249/1000
2023-09-26 18:45:15.390 
Epoch 249/1000 
	 loss: 18.4520, MinusLogProbMetric: 18.4520, val_loss: 18.8272, val_MinusLogProbMetric: 18.8272

Epoch 249: val_loss did not improve from 18.39537
196/196 - 75s - loss: 18.4520 - MinusLogProbMetric: 18.4520 - val_loss: 18.8272 - val_MinusLogProbMetric: 18.8272 - lr: 3.3333e-04 - 75s/epoch - 380ms/step
Epoch 250/1000
2023-09-26 18:46:29.172 
Epoch 250/1000 
	 loss: 18.5405, MinusLogProbMetric: 18.5405, val_loss: 19.0077, val_MinusLogProbMetric: 19.0077

Epoch 250: val_loss did not improve from 18.39537
196/196 - 74s - loss: 18.5405 - MinusLogProbMetric: 18.5405 - val_loss: 19.0077 - val_MinusLogProbMetric: 19.0077 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 251/1000
2023-09-26 18:47:44.248 
Epoch 251/1000 
	 loss: 18.4963, MinusLogProbMetric: 18.4963, val_loss: 18.3709, val_MinusLogProbMetric: 18.3709

Epoch 251: val_loss improved from 18.39537 to 18.37093, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 76s - loss: 18.4963 - MinusLogProbMetric: 18.4963 - val_loss: 18.3709 - val_MinusLogProbMetric: 18.3709 - lr: 3.3333e-04 - 76s/epoch - 390ms/step
Epoch 252/1000
2023-09-26 18:48:59.659 
Epoch 252/1000 
	 loss: 18.4929, MinusLogProbMetric: 18.4929, val_loss: 18.5841, val_MinusLogProbMetric: 18.5841

Epoch 252: val_loss did not improve from 18.37093
196/196 - 74s - loss: 18.4929 - MinusLogProbMetric: 18.4929 - val_loss: 18.5841 - val_MinusLogProbMetric: 18.5841 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 253/1000
2023-09-26 18:50:14.216 
Epoch 253/1000 
	 loss: 18.5161, MinusLogProbMetric: 18.5161, val_loss: 19.1761, val_MinusLogProbMetric: 19.1761

Epoch 253: val_loss did not improve from 18.37093
196/196 - 75s - loss: 18.5161 - MinusLogProbMetric: 18.5161 - val_loss: 19.1761 - val_MinusLogProbMetric: 19.1761 - lr: 3.3333e-04 - 75s/epoch - 380ms/step
Epoch 254/1000
2023-09-26 18:51:28.687 
Epoch 254/1000 
	 loss: 18.5074, MinusLogProbMetric: 18.5074, val_loss: 18.7186, val_MinusLogProbMetric: 18.7186

Epoch 254: val_loss did not improve from 18.37093
196/196 - 74s - loss: 18.5074 - MinusLogProbMetric: 18.5074 - val_loss: 18.7186 - val_MinusLogProbMetric: 18.7186 - lr: 3.3333e-04 - 74s/epoch - 380ms/step
Epoch 255/1000
2023-09-26 18:52:43.565 
Epoch 255/1000 
	 loss: 18.5093, MinusLogProbMetric: 18.5093, val_loss: 18.2079, val_MinusLogProbMetric: 18.2079

Epoch 255: val_loss improved from 18.37093 to 18.20786, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 76s - loss: 18.5093 - MinusLogProbMetric: 18.5093 - val_loss: 18.2079 - val_MinusLogProbMetric: 18.2079 - lr: 3.3333e-04 - 76s/epoch - 389ms/step
Epoch 256/1000
2023-09-26 18:53:58.963 
Epoch 256/1000 
	 loss: 18.4996, MinusLogProbMetric: 18.4996, val_loss: 18.4111, val_MinusLogProbMetric: 18.4111

Epoch 256: val_loss did not improve from 18.20786
196/196 - 74s - loss: 18.4996 - MinusLogProbMetric: 18.4996 - val_loss: 18.4111 - val_MinusLogProbMetric: 18.4111 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 257/1000
2023-09-26 18:55:13.290 
Epoch 257/1000 
	 loss: 18.4659, MinusLogProbMetric: 18.4659, val_loss: 18.6618, val_MinusLogProbMetric: 18.6618

Epoch 257: val_loss did not improve from 18.20786
196/196 - 74s - loss: 18.4659 - MinusLogProbMetric: 18.4659 - val_loss: 18.6618 - val_MinusLogProbMetric: 18.6618 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 258/1000
2023-09-26 18:56:27.801 
Epoch 258/1000 
	 loss: 18.5761, MinusLogProbMetric: 18.5761, val_loss: 18.4458, val_MinusLogProbMetric: 18.4458

Epoch 258: val_loss did not improve from 18.20786
196/196 - 75s - loss: 18.5761 - MinusLogProbMetric: 18.5761 - val_loss: 18.4458 - val_MinusLogProbMetric: 18.4458 - lr: 3.3333e-04 - 75s/epoch - 380ms/step
Epoch 259/1000
2023-09-26 18:57:41.814 
Epoch 259/1000 
	 loss: 18.4535, MinusLogProbMetric: 18.4535, val_loss: 18.5754, val_MinusLogProbMetric: 18.5754

Epoch 259: val_loss did not improve from 18.20786
196/196 - 74s - loss: 18.4535 - MinusLogProbMetric: 18.4535 - val_loss: 18.5754 - val_MinusLogProbMetric: 18.5754 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 260/1000
2023-09-26 18:58:55.370 
Epoch 260/1000 
	 loss: 18.5159, MinusLogProbMetric: 18.5159, val_loss: 18.3705, val_MinusLogProbMetric: 18.3705

Epoch 260: val_loss did not improve from 18.20786
196/196 - 74s - loss: 18.5159 - MinusLogProbMetric: 18.5159 - val_loss: 18.3705 - val_MinusLogProbMetric: 18.3705 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 261/1000
2023-09-26 19:00:09.822 
Epoch 261/1000 
	 loss: 18.3742, MinusLogProbMetric: 18.3742, val_loss: 18.7661, val_MinusLogProbMetric: 18.7661

Epoch 261: val_loss did not improve from 18.20786
196/196 - 74s - loss: 18.3742 - MinusLogProbMetric: 18.3742 - val_loss: 18.7661 - val_MinusLogProbMetric: 18.7661 - lr: 3.3333e-04 - 74s/epoch - 380ms/step
Epoch 262/1000
2023-09-26 19:01:24.046 
Epoch 262/1000 
	 loss: 18.4061, MinusLogProbMetric: 18.4061, val_loss: 18.5208, val_MinusLogProbMetric: 18.5208

Epoch 262: val_loss did not improve from 18.20786
196/196 - 74s - loss: 18.4061 - MinusLogProbMetric: 18.4061 - val_loss: 18.5208 - val_MinusLogProbMetric: 18.5208 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 263/1000
2023-09-26 19:02:38.123 
Epoch 263/1000 
	 loss: 18.3640, MinusLogProbMetric: 18.3640, val_loss: 18.5319, val_MinusLogProbMetric: 18.5319

Epoch 263: val_loss did not improve from 18.20786
196/196 - 74s - loss: 18.3640 - MinusLogProbMetric: 18.3640 - val_loss: 18.5319 - val_MinusLogProbMetric: 18.5319 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 264/1000
2023-09-26 19:03:52.660 
Epoch 264/1000 
	 loss: 18.5102, MinusLogProbMetric: 18.5102, val_loss: 18.3222, val_MinusLogProbMetric: 18.3222

Epoch 264: val_loss did not improve from 18.20786
196/196 - 75s - loss: 18.5102 - MinusLogProbMetric: 18.5102 - val_loss: 18.3222 - val_MinusLogProbMetric: 18.3222 - lr: 3.3333e-04 - 75s/epoch - 380ms/step
Epoch 265/1000
2023-09-26 19:05:06.635 
Epoch 265/1000 
	 loss: 18.4080, MinusLogProbMetric: 18.4080, val_loss: 18.6413, val_MinusLogProbMetric: 18.6413

Epoch 265: val_loss did not improve from 18.20786
196/196 - 74s - loss: 18.4080 - MinusLogProbMetric: 18.4080 - val_loss: 18.6413 - val_MinusLogProbMetric: 18.6413 - lr: 3.3333e-04 - 74s/epoch - 377ms/step
Epoch 266/1000
2023-09-26 19:06:20.161 
Epoch 266/1000 
	 loss: 18.4004, MinusLogProbMetric: 18.4004, val_loss: 18.5667, val_MinusLogProbMetric: 18.5667

Epoch 266: val_loss did not improve from 18.20786
196/196 - 74s - loss: 18.4004 - MinusLogProbMetric: 18.4004 - val_loss: 18.5667 - val_MinusLogProbMetric: 18.5667 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 267/1000
2023-09-26 19:07:34.390 
Epoch 267/1000 
	 loss: 18.4458, MinusLogProbMetric: 18.4458, val_loss: 19.0175, val_MinusLogProbMetric: 19.0175

Epoch 267: val_loss did not improve from 18.20786
196/196 - 74s - loss: 18.4458 - MinusLogProbMetric: 18.4458 - val_loss: 19.0175 - val_MinusLogProbMetric: 19.0175 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 268/1000
2023-09-26 19:08:48.094 
Epoch 268/1000 
	 loss: 18.3264, MinusLogProbMetric: 18.3264, val_loss: 18.2141, val_MinusLogProbMetric: 18.2141

Epoch 268: val_loss did not improve from 18.20786
196/196 - 74s - loss: 18.3264 - MinusLogProbMetric: 18.3264 - val_loss: 18.2141 - val_MinusLogProbMetric: 18.2141 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 269/1000
2023-09-26 19:10:01.422 
Epoch 269/1000 
	 loss: 18.4248, MinusLogProbMetric: 18.4248, val_loss: 18.4611, val_MinusLogProbMetric: 18.4611

Epoch 269: val_loss did not improve from 18.20786
196/196 - 73s - loss: 18.4248 - MinusLogProbMetric: 18.4248 - val_loss: 18.4611 - val_MinusLogProbMetric: 18.4611 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 270/1000
2023-09-26 19:11:15.534 
Epoch 270/1000 
	 loss: 18.3736, MinusLogProbMetric: 18.3736, val_loss: 18.2054, val_MinusLogProbMetric: 18.2054

Epoch 270: val_loss improved from 18.20786 to 18.20539, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 18.3736 - MinusLogProbMetric: 18.3736 - val_loss: 18.2054 - val_MinusLogProbMetric: 18.2054 - lr: 3.3333e-04 - 75s/epoch - 385ms/step
Epoch 271/1000
2023-09-26 19:12:31.285 
Epoch 271/1000 
	 loss: 18.3047, MinusLogProbMetric: 18.3047, val_loss: 18.9221, val_MinusLogProbMetric: 18.9221

Epoch 271: val_loss did not improve from 18.20539
196/196 - 74s - loss: 18.3047 - MinusLogProbMetric: 18.3047 - val_loss: 18.9221 - val_MinusLogProbMetric: 18.9221 - lr: 3.3333e-04 - 74s/epoch - 380ms/step
Epoch 272/1000
2023-09-26 19:13:45.313 
Epoch 272/1000 
	 loss: 18.4093, MinusLogProbMetric: 18.4093, val_loss: 19.0487, val_MinusLogProbMetric: 19.0487

Epoch 272: val_loss did not improve from 18.20539
196/196 - 74s - loss: 18.4093 - MinusLogProbMetric: 18.4093 - val_loss: 19.0487 - val_MinusLogProbMetric: 19.0487 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 273/1000
2023-09-26 19:14:59.476 
Epoch 273/1000 
	 loss: 18.3726, MinusLogProbMetric: 18.3726, val_loss: 18.7022, val_MinusLogProbMetric: 18.7022

Epoch 273: val_loss did not improve from 18.20539
196/196 - 74s - loss: 18.3726 - MinusLogProbMetric: 18.3726 - val_loss: 18.7022 - val_MinusLogProbMetric: 18.7022 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 274/1000
2023-09-26 19:16:13.997 
Epoch 274/1000 
	 loss: 18.3919, MinusLogProbMetric: 18.3919, val_loss: 18.7853, val_MinusLogProbMetric: 18.7853

Epoch 274: val_loss did not improve from 18.20539
196/196 - 75s - loss: 18.3919 - MinusLogProbMetric: 18.3919 - val_loss: 18.7853 - val_MinusLogProbMetric: 18.7853 - lr: 3.3333e-04 - 75s/epoch - 380ms/step
Epoch 275/1000
2023-09-26 19:17:27.748 
Epoch 275/1000 
	 loss: 18.4383, MinusLogProbMetric: 18.4383, val_loss: 18.2800, val_MinusLogProbMetric: 18.2800

Epoch 275: val_loss did not improve from 18.20539
196/196 - 74s - loss: 18.4383 - MinusLogProbMetric: 18.4383 - val_loss: 18.2800 - val_MinusLogProbMetric: 18.2800 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 276/1000
2023-09-26 19:18:37.480 
Epoch 276/1000 
	 loss: 18.3811, MinusLogProbMetric: 18.3811, val_loss: 18.2704, val_MinusLogProbMetric: 18.2704

Epoch 276: val_loss did not improve from 18.20539
196/196 - 70s - loss: 18.3811 - MinusLogProbMetric: 18.3811 - val_loss: 18.2704 - val_MinusLogProbMetric: 18.2704 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 277/1000
2023-09-26 19:19:38.129 
Epoch 277/1000 
	 loss: 18.4039, MinusLogProbMetric: 18.4039, val_loss: 18.8708, val_MinusLogProbMetric: 18.8708

Epoch 277: val_loss did not improve from 18.20539
196/196 - 61s - loss: 18.4039 - MinusLogProbMetric: 18.4039 - val_loss: 18.8708 - val_MinusLogProbMetric: 18.8708 - lr: 3.3333e-04 - 61s/epoch - 309ms/step
Epoch 278/1000
2023-09-26 19:20:45.267 
Epoch 278/1000 
	 loss: 18.3620, MinusLogProbMetric: 18.3620, val_loss: 18.2968, val_MinusLogProbMetric: 18.2968

Epoch 278: val_loss did not improve from 18.20539
196/196 - 67s - loss: 18.3620 - MinusLogProbMetric: 18.3620 - val_loss: 18.2968 - val_MinusLogProbMetric: 18.2968 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 279/1000
2023-09-26 19:21:53.426 
Epoch 279/1000 
	 loss: 18.3838, MinusLogProbMetric: 18.3838, val_loss: 18.2797, val_MinusLogProbMetric: 18.2797

Epoch 279: val_loss did not improve from 18.20539
196/196 - 68s - loss: 18.3838 - MinusLogProbMetric: 18.3838 - val_loss: 18.2797 - val_MinusLogProbMetric: 18.2797 - lr: 3.3333e-04 - 68s/epoch - 348ms/step
Epoch 280/1000
2023-09-26 19:22:57.384 
Epoch 280/1000 
	 loss: 18.2867, MinusLogProbMetric: 18.2867, val_loss: 18.1872, val_MinusLogProbMetric: 18.1872

Epoch 280: val_loss improved from 18.20539 to 18.18719, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 65s - loss: 18.2867 - MinusLogProbMetric: 18.2867 - val_loss: 18.1872 - val_MinusLogProbMetric: 18.1872 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 281/1000
2023-09-26 19:24:04.898 
Epoch 281/1000 
	 loss: 18.2866, MinusLogProbMetric: 18.2866, val_loss: 18.5493, val_MinusLogProbMetric: 18.5493

Epoch 281: val_loss did not improve from 18.18719
196/196 - 67s - loss: 18.2866 - MinusLogProbMetric: 18.2866 - val_loss: 18.5493 - val_MinusLogProbMetric: 18.5493 - lr: 3.3333e-04 - 67s/epoch - 339ms/step
Epoch 282/1000
2023-09-26 19:25:18.609 
Epoch 282/1000 
	 loss: 18.4603, MinusLogProbMetric: 18.4603, val_loss: 18.5057, val_MinusLogProbMetric: 18.5057

Epoch 282: val_loss did not improve from 18.18719
196/196 - 74s - loss: 18.4603 - MinusLogProbMetric: 18.4603 - val_loss: 18.5057 - val_MinusLogProbMetric: 18.5057 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 283/1000
2023-09-26 19:26:31.336 
Epoch 283/1000 
	 loss: 18.3604, MinusLogProbMetric: 18.3604, val_loss: 18.9913, val_MinusLogProbMetric: 18.9913

Epoch 283: val_loss did not improve from 18.18719
196/196 - 73s - loss: 18.3604 - MinusLogProbMetric: 18.3604 - val_loss: 18.9913 - val_MinusLogProbMetric: 18.9913 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 284/1000
2023-09-26 19:27:44.277 
Epoch 284/1000 
	 loss: 18.2780, MinusLogProbMetric: 18.2780, val_loss: 18.8403, val_MinusLogProbMetric: 18.8403

Epoch 284: val_loss did not improve from 18.18719
196/196 - 73s - loss: 18.2780 - MinusLogProbMetric: 18.2780 - val_loss: 18.8403 - val_MinusLogProbMetric: 18.8403 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 285/1000
2023-09-26 19:28:47.512 
Epoch 285/1000 
	 loss: 18.4186, MinusLogProbMetric: 18.4186, val_loss: 18.6830, val_MinusLogProbMetric: 18.6830

Epoch 285: val_loss did not improve from 18.18719
196/196 - 63s - loss: 18.4186 - MinusLogProbMetric: 18.4186 - val_loss: 18.6830 - val_MinusLogProbMetric: 18.6830 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 286/1000
2023-09-26 19:29:50.693 
Epoch 286/1000 
	 loss: 18.3480, MinusLogProbMetric: 18.3480, val_loss: 18.3738, val_MinusLogProbMetric: 18.3738

Epoch 286: val_loss did not improve from 18.18719
196/196 - 63s - loss: 18.3480 - MinusLogProbMetric: 18.3480 - val_loss: 18.3738 - val_MinusLogProbMetric: 18.3738 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 287/1000
2023-09-26 19:31:03.711 
Epoch 287/1000 
	 loss: 18.2636, MinusLogProbMetric: 18.2636, val_loss: 18.3812, val_MinusLogProbMetric: 18.3812

Epoch 287: val_loss did not improve from 18.18719
196/196 - 73s - loss: 18.2636 - MinusLogProbMetric: 18.2636 - val_loss: 18.3812 - val_MinusLogProbMetric: 18.3812 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 288/1000
2023-09-26 19:32:15.493 
Epoch 288/1000 
	 loss: 18.3458, MinusLogProbMetric: 18.3458, val_loss: 18.2249, val_MinusLogProbMetric: 18.2249

Epoch 288: val_loss did not improve from 18.18719
196/196 - 72s - loss: 18.3458 - MinusLogProbMetric: 18.3458 - val_loss: 18.2249 - val_MinusLogProbMetric: 18.2249 - lr: 3.3333e-04 - 72s/epoch - 366ms/step
Epoch 289/1000
2023-09-26 19:33:28.780 
Epoch 289/1000 
	 loss: 18.3918, MinusLogProbMetric: 18.3918, val_loss: 18.3267, val_MinusLogProbMetric: 18.3267

Epoch 289: val_loss did not improve from 18.18719
196/196 - 73s - loss: 18.3918 - MinusLogProbMetric: 18.3918 - val_loss: 18.3267 - val_MinusLogProbMetric: 18.3267 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 290/1000
2023-09-26 19:34:41.848 
Epoch 290/1000 
	 loss: 18.2526, MinusLogProbMetric: 18.2526, val_loss: 18.5916, val_MinusLogProbMetric: 18.5916

Epoch 290: val_loss did not improve from 18.18719
196/196 - 73s - loss: 18.2526 - MinusLogProbMetric: 18.2526 - val_loss: 18.5916 - val_MinusLogProbMetric: 18.5916 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 291/1000
2023-09-26 19:35:54.986 
Epoch 291/1000 
	 loss: 18.3833, MinusLogProbMetric: 18.3833, val_loss: 18.2186, val_MinusLogProbMetric: 18.2186

Epoch 291: val_loss did not improve from 18.18719
196/196 - 73s - loss: 18.3833 - MinusLogProbMetric: 18.3833 - val_loss: 18.2186 - val_MinusLogProbMetric: 18.2186 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 292/1000
2023-09-26 19:37:08.476 
Epoch 292/1000 
	 loss: 18.2667, MinusLogProbMetric: 18.2667, val_loss: 18.7825, val_MinusLogProbMetric: 18.7825

Epoch 292: val_loss did not improve from 18.18719
196/196 - 73s - loss: 18.2667 - MinusLogProbMetric: 18.2667 - val_loss: 18.7825 - val_MinusLogProbMetric: 18.7825 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 293/1000
2023-09-26 19:38:22.178 
Epoch 293/1000 
	 loss: 18.2713, MinusLogProbMetric: 18.2713, val_loss: 18.3023, val_MinusLogProbMetric: 18.3023

Epoch 293: val_loss did not improve from 18.18719
196/196 - 74s - loss: 18.2713 - MinusLogProbMetric: 18.2713 - val_loss: 18.3023 - val_MinusLogProbMetric: 18.3023 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 294/1000
2023-09-26 19:39:35.710 
Epoch 294/1000 
	 loss: 18.2400, MinusLogProbMetric: 18.2400, val_loss: 18.6396, val_MinusLogProbMetric: 18.6396

Epoch 294: val_loss did not improve from 18.18719
196/196 - 74s - loss: 18.2400 - MinusLogProbMetric: 18.2400 - val_loss: 18.6396 - val_MinusLogProbMetric: 18.6396 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 295/1000
2023-09-26 19:40:49.058 
Epoch 295/1000 
	 loss: 18.2425, MinusLogProbMetric: 18.2425, val_loss: 18.1592, val_MinusLogProbMetric: 18.1592

Epoch 295: val_loss improved from 18.18719 to 18.15925, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 74s - loss: 18.2425 - MinusLogProbMetric: 18.2425 - val_loss: 18.1592 - val_MinusLogProbMetric: 18.1592 - lr: 3.3333e-04 - 74s/epoch - 380ms/step
Epoch 296/1000
2023-09-26 19:42:02.830 
Epoch 296/1000 
	 loss: 18.2819, MinusLogProbMetric: 18.2819, val_loss: 18.8021, val_MinusLogProbMetric: 18.8021

Epoch 296: val_loss did not improve from 18.15925
196/196 - 73s - loss: 18.2819 - MinusLogProbMetric: 18.2819 - val_loss: 18.8021 - val_MinusLogProbMetric: 18.8021 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 297/1000
2023-09-26 19:43:15.965 
Epoch 297/1000 
	 loss: 18.3296, MinusLogProbMetric: 18.3296, val_loss: 18.3779, val_MinusLogProbMetric: 18.3779

Epoch 297: val_loss did not improve from 18.15925
196/196 - 73s - loss: 18.3296 - MinusLogProbMetric: 18.3296 - val_loss: 18.3779 - val_MinusLogProbMetric: 18.3779 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 298/1000
2023-09-26 19:44:29.340 
Epoch 298/1000 
	 loss: 18.2211, MinusLogProbMetric: 18.2211, val_loss: 18.1332, val_MinusLogProbMetric: 18.1332

Epoch 298: val_loss improved from 18.15925 to 18.13321, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 18.2211 - MinusLogProbMetric: 18.2211 - val_loss: 18.1332 - val_MinusLogProbMetric: 18.1332 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 299/1000
2023-09-26 19:45:43.746 
Epoch 299/1000 
	 loss: 18.2235, MinusLogProbMetric: 18.2235, val_loss: 18.3365, val_MinusLogProbMetric: 18.3365

Epoch 299: val_loss did not improve from 18.13321
196/196 - 73s - loss: 18.2235 - MinusLogProbMetric: 18.2235 - val_loss: 18.3365 - val_MinusLogProbMetric: 18.3365 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 300/1000
2023-09-26 19:46:56.515 
Epoch 300/1000 
	 loss: 18.3170, MinusLogProbMetric: 18.3170, val_loss: 18.2542, val_MinusLogProbMetric: 18.2542

Epoch 300: val_loss did not improve from 18.13321
196/196 - 73s - loss: 18.3170 - MinusLogProbMetric: 18.3170 - val_loss: 18.2542 - val_MinusLogProbMetric: 18.2542 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 301/1000
2023-09-26 19:48:09.795 
Epoch 301/1000 
	 loss: 18.2653, MinusLogProbMetric: 18.2653, val_loss: 18.4158, val_MinusLogProbMetric: 18.4158

Epoch 301: val_loss did not improve from 18.13321
196/196 - 73s - loss: 18.2653 - MinusLogProbMetric: 18.2653 - val_loss: 18.4158 - val_MinusLogProbMetric: 18.4158 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 302/1000
2023-09-26 19:49:23.433 
Epoch 302/1000 
	 loss: 18.1628, MinusLogProbMetric: 18.1628, val_loss: 18.4181, val_MinusLogProbMetric: 18.4181

Epoch 302: val_loss did not improve from 18.13321
196/196 - 74s - loss: 18.1628 - MinusLogProbMetric: 18.1628 - val_loss: 18.4181 - val_MinusLogProbMetric: 18.4181 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 303/1000
2023-09-26 19:50:36.039 
Epoch 303/1000 
	 loss: 18.2272, MinusLogProbMetric: 18.2272, val_loss: 18.1558, val_MinusLogProbMetric: 18.1558

Epoch 303: val_loss did not improve from 18.13321
196/196 - 73s - loss: 18.2272 - MinusLogProbMetric: 18.2272 - val_loss: 18.1558 - val_MinusLogProbMetric: 18.1558 - lr: 3.3333e-04 - 73s/epoch - 370ms/step
Epoch 304/1000
2023-09-26 19:51:48.859 
Epoch 304/1000 
	 loss: 18.2410, MinusLogProbMetric: 18.2410, val_loss: 18.2141, val_MinusLogProbMetric: 18.2141

Epoch 304: val_loss did not improve from 18.13321
196/196 - 73s - loss: 18.2410 - MinusLogProbMetric: 18.2410 - val_loss: 18.2141 - val_MinusLogProbMetric: 18.2141 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 305/1000
2023-09-26 19:53:01.984 
Epoch 305/1000 
	 loss: 18.1903, MinusLogProbMetric: 18.1903, val_loss: 18.5996, val_MinusLogProbMetric: 18.5996

Epoch 305: val_loss did not improve from 18.13321
196/196 - 73s - loss: 18.1903 - MinusLogProbMetric: 18.1903 - val_loss: 18.5996 - val_MinusLogProbMetric: 18.5996 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 306/1000
2023-09-26 19:54:14.990 
Epoch 306/1000 
	 loss: 18.1865, MinusLogProbMetric: 18.1865, val_loss: 19.0782, val_MinusLogProbMetric: 19.0782

Epoch 306: val_loss did not improve from 18.13321
196/196 - 73s - loss: 18.1865 - MinusLogProbMetric: 18.1865 - val_loss: 19.0782 - val_MinusLogProbMetric: 19.0782 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 307/1000
2023-09-26 19:55:28.521 
Epoch 307/1000 
	 loss: 18.2529, MinusLogProbMetric: 18.2529, val_loss: 18.1220, val_MinusLogProbMetric: 18.1220

Epoch 307: val_loss improved from 18.13321 to 18.12204, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 18.2529 - MinusLogProbMetric: 18.2529 - val_loss: 18.1220 - val_MinusLogProbMetric: 18.1220 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 308/1000
2023-09-26 19:56:43.265 
Epoch 308/1000 
	 loss: 18.2187, MinusLogProbMetric: 18.2187, val_loss: 18.7021, val_MinusLogProbMetric: 18.7021

Epoch 308: val_loss did not improve from 18.12204
196/196 - 74s - loss: 18.2187 - MinusLogProbMetric: 18.2187 - val_loss: 18.7021 - val_MinusLogProbMetric: 18.7021 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 309/1000
2023-09-26 19:57:56.266 
Epoch 309/1000 
	 loss: 18.2553, MinusLogProbMetric: 18.2553, val_loss: 18.1192, val_MinusLogProbMetric: 18.1192

Epoch 309: val_loss improved from 18.12204 to 18.11917, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 74s - loss: 18.2553 - MinusLogProbMetric: 18.2553 - val_loss: 18.1192 - val_MinusLogProbMetric: 18.1192 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 310/1000
2023-09-26 19:59:10.844 
Epoch 310/1000 
	 loss: 18.1246, MinusLogProbMetric: 18.1246, val_loss: 18.2779, val_MinusLogProbMetric: 18.2779

Epoch 310: val_loss did not improve from 18.11917
196/196 - 73s - loss: 18.1246 - MinusLogProbMetric: 18.1246 - val_loss: 18.2779 - val_MinusLogProbMetric: 18.2779 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 311/1000
2023-09-26 20:00:24.555 
Epoch 311/1000 
	 loss: 18.1448, MinusLogProbMetric: 18.1448, val_loss: 18.1975, val_MinusLogProbMetric: 18.1975

Epoch 311: val_loss did not improve from 18.11917
196/196 - 74s - loss: 18.1448 - MinusLogProbMetric: 18.1448 - val_loss: 18.1975 - val_MinusLogProbMetric: 18.1975 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 312/1000
2023-09-26 20:01:37.549 
Epoch 312/1000 
	 loss: 18.1867, MinusLogProbMetric: 18.1867, val_loss: 18.2384, val_MinusLogProbMetric: 18.2384

Epoch 312: val_loss did not improve from 18.11917
196/196 - 73s - loss: 18.1867 - MinusLogProbMetric: 18.1867 - val_loss: 18.2384 - val_MinusLogProbMetric: 18.2384 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 313/1000
2023-09-26 20:02:50.771 
Epoch 313/1000 
	 loss: 18.2293, MinusLogProbMetric: 18.2293, val_loss: 18.4577, val_MinusLogProbMetric: 18.4577

Epoch 313: val_loss did not improve from 18.11917
196/196 - 73s - loss: 18.2293 - MinusLogProbMetric: 18.2293 - val_loss: 18.4577 - val_MinusLogProbMetric: 18.4577 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 314/1000
2023-09-26 20:04:03.502 
Epoch 314/1000 
	 loss: 18.2819, MinusLogProbMetric: 18.2819, val_loss: 18.5036, val_MinusLogProbMetric: 18.5036

Epoch 314: val_loss did not improve from 18.11917
196/196 - 73s - loss: 18.2819 - MinusLogProbMetric: 18.2819 - val_loss: 18.5036 - val_MinusLogProbMetric: 18.5036 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 315/1000
2023-09-26 20:05:17.543 
Epoch 315/1000 
	 loss: 18.2306, MinusLogProbMetric: 18.2306, val_loss: 18.2924, val_MinusLogProbMetric: 18.2924

Epoch 315: val_loss did not improve from 18.11917
196/196 - 74s - loss: 18.2306 - MinusLogProbMetric: 18.2306 - val_loss: 18.2924 - val_MinusLogProbMetric: 18.2924 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 316/1000
2023-09-26 20:06:30.797 
Epoch 316/1000 
	 loss: 18.2137, MinusLogProbMetric: 18.2137, val_loss: 18.3337, val_MinusLogProbMetric: 18.3337

Epoch 316: val_loss did not improve from 18.11917
196/196 - 73s - loss: 18.2137 - MinusLogProbMetric: 18.2137 - val_loss: 18.3337 - val_MinusLogProbMetric: 18.3337 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 317/1000
2023-09-26 20:07:42.970 
Epoch 317/1000 
	 loss: 18.1660, MinusLogProbMetric: 18.1660, val_loss: 18.2816, val_MinusLogProbMetric: 18.2816

Epoch 317: val_loss did not improve from 18.11917
196/196 - 72s - loss: 18.1660 - MinusLogProbMetric: 18.1660 - val_loss: 18.2816 - val_MinusLogProbMetric: 18.2816 - lr: 3.3333e-04 - 72s/epoch - 368ms/step
Epoch 318/1000
2023-09-26 20:08:55.741 
Epoch 318/1000 
	 loss: 18.1960, MinusLogProbMetric: 18.1960, val_loss: 18.5189, val_MinusLogProbMetric: 18.5189

Epoch 318: val_loss did not improve from 18.11917
196/196 - 73s - loss: 18.1960 - MinusLogProbMetric: 18.1960 - val_loss: 18.5189 - val_MinusLogProbMetric: 18.5189 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 319/1000
2023-09-26 20:10:08.928 
Epoch 319/1000 
	 loss: 18.1813, MinusLogProbMetric: 18.1813, val_loss: 18.2755, val_MinusLogProbMetric: 18.2755

Epoch 319: val_loss did not improve from 18.11917
196/196 - 73s - loss: 18.1813 - MinusLogProbMetric: 18.1813 - val_loss: 18.2755 - val_MinusLogProbMetric: 18.2755 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 320/1000
2023-09-26 20:11:22.385 
Epoch 320/1000 
	 loss: 18.0882, MinusLogProbMetric: 18.0882, val_loss: 18.3438, val_MinusLogProbMetric: 18.3438

Epoch 320: val_loss did not improve from 18.11917
196/196 - 73s - loss: 18.0882 - MinusLogProbMetric: 18.0882 - val_loss: 18.3438 - val_MinusLogProbMetric: 18.3438 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 321/1000
2023-09-26 20:12:35.649 
Epoch 321/1000 
	 loss: 18.1398, MinusLogProbMetric: 18.1398, val_loss: 18.3789, val_MinusLogProbMetric: 18.3789

Epoch 321: val_loss did not improve from 18.11917
196/196 - 73s - loss: 18.1398 - MinusLogProbMetric: 18.1398 - val_loss: 18.3789 - val_MinusLogProbMetric: 18.3789 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 322/1000
2023-09-26 20:13:47.896 
Epoch 322/1000 
	 loss: 18.1707, MinusLogProbMetric: 18.1707, val_loss: 18.2262, val_MinusLogProbMetric: 18.2262

Epoch 322: val_loss did not improve from 18.11917
196/196 - 72s - loss: 18.1707 - MinusLogProbMetric: 18.1707 - val_loss: 18.2262 - val_MinusLogProbMetric: 18.2262 - lr: 3.3333e-04 - 72s/epoch - 369ms/step
Epoch 323/1000
2023-09-26 20:15:01.311 
Epoch 323/1000 
	 loss: 18.2255, MinusLogProbMetric: 18.2255, val_loss: 18.6159, val_MinusLogProbMetric: 18.6159

Epoch 323: val_loss did not improve from 18.11917
196/196 - 73s - loss: 18.2255 - MinusLogProbMetric: 18.2255 - val_loss: 18.6159 - val_MinusLogProbMetric: 18.6159 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 324/1000
2023-09-26 20:16:14.391 
Epoch 324/1000 
	 loss: 18.1024, MinusLogProbMetric: 18.1024, val_loss: 18.3214, val_MinusLogProbMetric: 18.3214

Epoch 324: val_loss did not improve from 18.11917
196/196 - 73s - loss: 18.1024 - MinusLogProbMetric: 18.1024 - val_loss: 18.3214 - val_MinusLogProbMetric: 18.3214 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 325/1000
2023-09-26 20:17:27.507 
Epoch 325/1000 
	 loss: 18.1347, MinusLogProbMetric: 18.1347, val_loss: 17.9652, val_MinusLogProbMetric: 17.9652

Epoch 325: val_loss improved from 18.11917 to 17.96519, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 74s - loss: 18.1347 - MinusLogProbMetric: 18.1347 - val_loss: 17.9652 - val_MinusLogProbMetric: 17.9652 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 326/1000
2023-09-26 20:18:41.275 
Epoch 326/1000 
	 loss: 18.1908, MinusLogProbMetric: 18.1908, val_loss: 18.5361, val_MinusLogProbMetric: 18.5361

Epoch 326: val_loss did not improve from 17.96519
196/196 - 73s - loss: 18.1908 - MinusLogProbMetric: 18.1908 - val_loss: 18.5361 - val_MinusLogProbMetric: 18.5361 - lr: 3.3333e-04 - 73s/epoch - 370ms/step
Epoch 327/1000
2023-09-26 20:19:54.448 
Epoch 327/1000 
	 loss: 18.2655, MinusLogProbMetric: 18.2655, val_loss: 18.5178, val_MinusLogProbMetric: 18.5178

Epoch 327: val_loss did not improve from 17.96519
196/196 - 73s - loss: 18.2655 - MinusLogProbMetric: 18.2655 - val_loss: 18.5178 - val_MinusLogProbMetric: 18.5178 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 328/1000
2023-09-26 20:21:07.728 
Epoch 328/1000 
	 loss: 18.1115, MinusLogProbMetric: 18.1115, val_loss: 18.0460, val_MinusLogProbMetric: 18.0460

Epoch 328: val_loss did not improve from 17.96519
196/196 - 73s - loss: 18.1115 - MinusLogProbMetric: 18.1115 - val_loss: 18.0460 - val_MinusLogProbMetric: 18.0460 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 329/1000
2023-09-26 20:22:21.357 
Epoch 329/1000 
	 loss: 18.1339, MinusLogProbMetric: 18.1339, val_loss: 18.2455, val_MinusLogProbMetric: 18.2455

Epoch 329: val_loss did not improve from 17.96519
196/196 - 74s - loss: 18.1339 - MinusLogProbMetric: 18.1339 - val_loss: 18.2455 - val_MinusLogProbMetric: 18.2455 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 330/1000
2023-09-26 20:23:34.281 
Epoch 330/1000 
	 loss: 18.0722, MinusLogProbMetric: 18.0722, val_loss: 18.1457, val_MinusLogProbMetric: 18.1457

Epoch 330: val_loss did not improve from 17.96519
196/196 - 73s - loss: 18.0722 - MinusLogProbMetric: 18.0722 - val_loss: 18.1457 - val_MinusLogProbMetric: 18.1457 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 331/1000
2023-09-26 20:24:47.999 
Epoch 331/1000 
	 loss: 18.1486, MinusLogProbMetric: 18.1486, val_loss: 18.1115, val_MinusLogProbMetric: 18.1115

Epoch 331: val_loss did not improve from 17.96519
196/196 - 74s - loss: 18.1486 - MinusLogProbMetric: 18.1486 - val_loss: 18.1115 - val_MinusLogProbMetric: 18.1115 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 332/1000
2023-09-26 20:26:01.148 
Epoch 332/1000 
	 loss: 18.1858, MinusLogProbMetric: 18.1858, val_loss: 18.2381, val_MinusLogProbMetric: 18.2381

Epoch 332: val_loss did not improve from 17.96519
196/196 - 73s - loss: 18.1858 - MinusLogProbMetric: 18.1858 - val_loss: 18.2381 - val_MinusLogProbMetric: 18.2381 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 333/1000
2023-09-26 20:27:14.282 
Epoch 333/1000 
	 loss: 18.0597, MinusLogProbMetric: 18.0597, val_loss: 18.3648, val_MinusLogProbMetric: 18.3648

Epoch 333: val_loss did not improve from 17.96519
196/196 - 73s - loss: 18.0597 - MinusLogProbMetric: 18.0597 - val_loss: 18.3648 - val_MinusLogProbMetric: 18.3648 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 334/1000
2023-09-26 20:28:28.122 
Epoch 334/1000 
	 loss: 18.1623, MinusLogProbMetric: 18.1623, val_loss: 18.3092, val_MinusLogProbMetric: 18.3092

Epoch 334: val_loss did not improve from 17.96519
196/196 - 74s - loss: 18.1623 - MinusLogProbMetric: 18.1623 - val_loss: 18.3092 - val_MinusLogProbMetric: 18.3092 - lr: 3.3333e-04 - 74s/epoch - 377ms/step
Epoch 335/1000
2023-09-26 20:29:41.766 
Epoch 335/1000 
	 loss: 18.0417, MinusLogProbMetric: 18.0417, val_loss: 18.3260, val_MinusLogProbMetric: 18.3260

Epoch 335: val_loss did not improve from 17.96519
196/196 - 74s - loss: 18.0417 - MinusLogProbMetric: 18.0417 - val_loss: 18.3260 - val_MinusLogProbMetric: 18.3260 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 336/1000
2023-09-26 20:30:55.081 
Epoch 336/1000 
	 loss: 18.0438, MinusLogProbMetric: 18.0438, val_loss: 18.2188, val_MinusLogProbMetric: 18.2188

Epoch 336: val_loss did not improve from 17.96519
196/196 - 73s - loss: 18.0438 - MinusLogProbMetric: 18.0438 - val_loss: 18.2188 - val_MinusLogProbMetric: 18.2188 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 337/1000
2023-09-26 20:32:08.525 
Epoch 337/1000 
	 loss: 18.1120, MinusLogProbMetric: 18.1120, val_loss: 18.3302, val_MinusLogProbMetric: 18.3302

Epoch 337: val_loss did not improve from 17.96519
196/196 - 73s - loss: 18.1120 - MinusLogProbMetric: 18.1120 - val_loss: 18.3302 - val_MinusLogProbMetric: 18.3302 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 338/1000
2023-09-26 20:33:21.665 
Epoch 338/1000 
	 loss: 18.1016, MinusLogProbMetric: 18.1016, val_loss: 18.3739, val_MinusLogProbMetric: 18.3739

Epoch 338: val_loss did not improve from 17.96519
196/196 - 73s - loss: 18.1016 - MinusLogProbMetric: 18.1016 - val_loss: 18.3739 - val_MinusLogProbMetric: 18.3739 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 339/1000
2023-09-26 20:34:34.681 
Epoch 339/1000 
	 loss: 18.1855, MinusLogProbMetric: 18.1855, val_loss: 18.2558, val_MinusLogProbMetric: 18.2558

Epoch 339: val_loss did not improve from 17.96519
196/196 - 73s - loss: 18.1855 - MinusLogProbMetric: 18.1855 - val_loss: 18.2558 - val_MinusLogProbMetric: 18.2558 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 340/1000
2023-09-26 20:35:47.849 
Epoch 340/1000 
	 loss: 18.1123, MinusLogProbMetric: 18.1123, val_loss: 18.4459, val_MinusLogProbMetric: 18.4459

Epoch 340: val_loss did not improve from 17.96519
196/196 - 73s - loss: 18.1123 - MinusLogProbMetric: 18.1123 - val_loss: 18.4459 - val_MinusLogProbMetric: 18.4459 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 341/1000
2023-09-26 20:37:01.817 
Epoch 341/1000 
	 loss: 18.0103, MinusLogProbMetric: 18.0103, val_loss: 18.1608, val_MinusLogProbMetric: 18.1608

Epoch 341: val_loss did not improve from 17.96519
196/196 - 74s - loss: 18.0103 - MinusLogProbMetric: 18.0103 - val_loss: 18.1608 - val_MinusLogProbMetric: 18.1608 - lr: 3.3333e-04 - 74s/epoch - 377ms/step
Epoch 342/1000
2023-09-26 20:38:15.177 
Epoch 342/1000 
	 loss: 18.0602, MinusLogProbMetric: 18.0602, val_loss: 18.0460, val_MinusLogProbMetric: 18.0460

Epoch 342: val_loss did not improve from 17.96519
196/196 - 73s - loss: 18.0602 - MinusLogProbMetric: 18.0602 - val_loss: 18.0460 - val_MinusLogProbMetric: 18.0460 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 343/1000
2023-09-26 20:39:28.197 
Epoch 343/1000 
	 loss: 18.1021, MinusLogProbMetric: 18.1021, val_loss: 18.1081, val_MinusLogProbMetric: 18.1081

Epoch 343: val_loss did not improve from 17.96519
196/196 - 73s - loss: 18.1021 - MinusLogProbMetric: 18.1021 - val_loss: 18.1081 - val_MinusLogProbMetric: 18.1081 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 344/1000
2023-09-26 20:40:41.958 
Epoch 344/1000 
	 loss: 18.0475, MinusLogProbMetric: 18.0475, val_loss: 18.0196, val_MinusLogProbMetric: 18.0196

Epoch 344: val_loss did not improve from 17.96519
196/196 - 74s - loss: 18.0475 - MinusLogProbMetric: 18.0475 - val_loss: 18.0196 - val_MinusLogProbMetric: 18.0196 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 345/1000
2023-09-26 20:41:55.044 
Epoch 345/1000 
	 loss: 18.0751, MinusLogProbMetric: 18.0751, val_loss: 18.2108, val_MinusLogProbMetric: 18.2108

Epoch 345: val_loss did not improve from 17.96519
196/196 - 73s - loss: 18.0751 - MinusLogProbMetric: 18.0751 - val_loss: 18.2108 - val_MinusLogProbMetric: 18.2108 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 346/1000
2023-09-26 20:43:08.415 
Epoch 346/1000 
	 loss: 17.9664, MinusLogProbMetric: 17.9664, val_loss: 18.3305, val_MinusLogProbMetric: 18.3305

Epoch 346: val_loss did not improve from 17.96519
196/196 - 73s - loss: 17.9664 - MinusLogProbMetric: 17.9664 - val_loss: 18.3305 - val_MinusLogProbMetric: 18.3305 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 347/1000
2023-09-26 20:44:21.125 
Epoch 347/1000 
	 loss: 18.1152, MinusLogProbMetric: 18.1152, val_loss: 18.5952, val_MinusLogProbMetric: 18.5952

Epoch 347: val_loss did not improve from 17.96519
196/196 - 73s - loss: 18.1152 - MinusLogProbMetric: 18.1152 - val_loss: 18.5952 - val_MinusLogProbMetric: 18.5952 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 348/1000
2023-09-26 20:45:34.055 
Epoch 348/1000 
	 loss: 18.1373, MinusLogProbMetric: 18.1373, val_loss: 17.9765, val_MinusLogProbMetric: 17.9765

Epoch 348: val_loss did not improve from 17.96519
196/196 - 73s - loss: 18.1373 - MinusLogProbMetric: 18.1373 - val_loss: 17.9765 - val_MinusLogProbMetric: 17.9765 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 349/1000
2023-09-26 20:46:47.420 
Epoch 349/1000 
	 loss: 18.0322, MinusLogProbMetric: 18.0322, val_loss: 18.1532, val_MinusLogProbMetric: 18.1532

Epoch 349: val_loss did not improve from 17.96519
196/196 - 73s - loss: 18.0322 - MinusLogProbMetric: 18.0322 - val_loss: 18.1532 - val_MinusLogProbMetric: 18.1532 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 350/1000
2023-09-26 20:48:00.525 
Epoch 350/1000 
	 loss: 18.0283, MinusLogProbMetric: 18.0283, val_loss: 17.9345, val_MinusLogProbMetric: 17.9345

Epoch 350: val_loss improved from 17.96519 to 17.93451, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 74s - loss: 18.0283 - MinusLogProbMetric: 18.0283 - val_loss: 17.9345 - val_MinusLogProbMetric: 17.9345 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 351/1000
2023-09-26 20:49:14.601 
Epoch 351/1000 
	 loss: 18.0393, MinusLogProbMetric: 18.0393, val_loss: 18.2704, val_MinusLogProbMetric: 18.2704

Epoch 351: val_loss did not improve from 17.93451
196/196 - 73s - loss: 18.0393 - MinusLogProbMetric: 18.0393 - val_loss: 18.2704 - val_MinusLogProbMetric: 18.2704 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 352/1000
2023-09-26 20:50:26.813 
Epoch 352/1000 
	 loss: 18.1328, MinusLogProbMetric: 18.1328, val_loss: 18.0512, val_MinusLogProbMetric: 18.0512

Epoch 352: val_loss did not improve from 17.93451
196/196 - 72s - loss: 18.1328 - MinusLogProbMetric: 18.1328 - val_loss: 18.0512 - val_MinusLogProbMetric: 18.0512 - lr: 3.3333e-04 - 72s/epoch - 368ms/step
Epoch 353/1000
2023-09-26 20:51:40.042 
Epoch 353/1000 
	 loss: 18.0349, MinusLogProbMetric: 18.0349, val_loss: 18.1010, val_MinusLogProbMetric: 18.1010

Epoch 353: val_loss did not improve from 17.93451
196/196 - 73s - loss: 18.0349 - MinusLogProbMetric: 18.0349 - val_loss: 18.1010 - val_MinusLogProbMetric: 18.1010 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 354/1000
2023-09-26 20:52:52.693 
Epoch 354/1000 
	 loss: 18.0488, MinusLogProbMetric: 18.0488, val_loss: 18.1113, val_MinusLogProbMetric: 18.1113

Epoch 354: val_loss did not improve from 17.93451
196/196 - 73s - loss: 18.0488 - MinusLogProbMetric: 18.0488 - val_loss: 18.1113 - val_MinusLogProbMetric: 18.1113 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 355/1000
2023-09-26 20:54:05.689 
Epoch 355/1000 
	 loss: 18.0218, MinusLogProbMetric: 18.0218, val_loss: 18.6047, val_MinusLogProbMetric: 18.6047

Epoch 355: val_loss did not improve from 17.93451
196/196 - 73s - loss: 18.0218 - MinusLogProbMetric: 18.0218 - val_loss: 18.6047 - val_MinusLogProbMetric: 18.6047 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 356/1000
2023-09-26 20:55:18.748 
Epoch 356/1000 
	 loss: 18.1313, MinusLogProbMetric: 18.1313, val_loss: 17.8795, val_MinusLogProbMetric: 17.8795

Epoch 356: val_loss improved from 17.93451 to 17.87954, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 74s - loss: 18.1313 - MinusLogProbMetric: 18.1313 - val_loss: 17.8795 - val_MinusLogProbMetric: 17.8795 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 357/1000
2023-09-26 20:56:33.396 
Epoch 357/1000 
	 loss: 18.0457, MinusLogProbMetric: 18.0457, val_loss: 18.0979, val_MinusLogProbMetric: 18.0979

Epoch 357: val_loss did not improve from 17.87954
196/196 - 73s - loss: 18.0457 - MinusLogProbMetric: 18.0457 - val_loss: 18.0979 - val_MinusLogProbMetric: 18.0979 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 358/1000
2023-09-26 20:57:46.340 
Epoch 358/1000 
	 loss: 18.0194, MinusLogProbMetric: 18.0194, val_loss: 18.1323, val_MinusLogProbMetric: 18.1323

Epoch 358: val_loss did not improve from 17.87954
196/196 - 73s - loss: 18.0194 - MinusLogProbMetric: 18.0194 - val_loss: 18.1323 - val_MinusLogProbMetric: 18.1323 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 359/1000
2023-09-26 20:58:59.884 
Epoch 359/1000 
	 loss: 17.9691, MinusLogProbMetric: 17.9691, val_loss: 17.9968, val_MinusLogProbMetric: 17.9968

Epoch 359: val_loss did not improve from 17.87954
196/196 - 74s - loss: 17.9691 - MinusLogProbMetric: 17.9691 - val_loss: 17.9968 - val_MinusLogProbMetric: 17.9968 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 360/1000
2023-09-26 21:00:13.047 
Epoch 360/1000 
	 loss: 17.9878, MinusLogProbMetric: 17.9878, val_loss: 18.5548, val_MinusLogProbMetric: 18.5548

Epoch 360: val_loss did not improve from 17.87954
196/196 - 73s - loss: 17.9878 - MinusLogProbMetric: 17.9878 - val_loss: 18.5548 - val_MinusLogProbMetric: 18.5548 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 361/1000
2023-09-26 21:01:25.332 
Epoch 361/1000 
	 loss: 18.0576, MinusLogProbMetric: 18.0576, val_loss: 18.2800, val_MinusLogProbMetric: 18.2800

Epoch 361: val_loss did not improve from 17.87954
196/196 - 72s - loss: 18.0576 - MinusLogProbMetric: 18.0576 - val_loss: 18.2800 - val_MinusLogProbMetric: 18.2800 - lr: 3.3333e-04 - 72s/epoch - 369ms/step
Epoch 362/1000
2023-09-26 21:02:38.042 
Epoch 362/1000 
	 loss: 18.0704, MinusLogProbMetric: 18.0704, val_loss: 18.3064, val_MinusLogProbMetric: 18.3064

Epoch 362: val_loss did not improve from 17.87954
196/196 - 73s - loss: 18.0704 - MinusLogProbMetric: 18.0704 - val_loss: 18.3064 - val_MinusLogProbMetric: 18.3064 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 363/1000
2023-09-26 21:03:51.551 
Epoch 363/1000 
	 loss: 17.9871, MinusLogProbMetric: 17.9871, val_loss: 18.1169, val_MinusLogProbMetric: 18.1169

Epoch 363: val_loss did not improve from 17.87954
196/196 - 74s - loss: 17.9871 - MinusLogProbMetric: 17.9871 - val_loss: 18.1169 - val_MinusLogProbMetric: 18.1169 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 364/1000
2023-09-26 21:05:04.278 
Epoch 364/1000 
	 loss: 17.9550, MinusLogProbMetric: 17.9550, val_loss: 17.8929, val_MinusLogProbMetric: 17.8929

Epoch 364: val_loss did not improve from 17.87954
196/196 - 73s - loss: 17.9550 - MinusLogProbMetric: 17.9550 - val_loss: 17.8929 - val_MinusLogProbMetric: 17.8929 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 365/1000
2023-09-26 21:06:18.561 
Epoch 365/1000 
	 loss: 17.9870, MinusLogProbMetric: 17.9870, val_loss: 18.2029, val_MinusLogProbMetric: 18.2029

Epoch 365: val_loss did not improve from 17.87954
196/196 - 74s - loss: 17.9870 - MinusLogProbMetric: 17.9870 - val_loss: 18.2029 - val_MinusLogProbMetric: 18.2029 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 366/1000
2023-09-26 21:07:32.304 
Epoch 366/1000 
	 loss: 18.0659, MinusLogProbMetric: 18.0659, val_loss: 18.8829, val_MinusLogProbMetric: 18.8829

Epoch 366: val_loss did not improve from 17.87954
196/196 - 74s - loss: 18.0659 - MinusLogProbMetric: 18.0659 - val_loss: 18.8829 - val_MinusLogProbMetric: 18.8829 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 367/1000
2023-09-26 21:08:45.826 
Epoch 367/1000 
	 loss: 18.0229, MinusLogProbMetric: 18.0229, val_loss: 17.8435, val_MinusLogProbMetric: 17.8435

Epoch 367: val_loss improved from 17.87954 to 17.84354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 18.0229 - MinusLogProbMetric: 18.0229 - val_loss: 17.8435 - val_MinusLogProbMetric: 17.8435 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 368/1000
2023-09-26 21:10:00.188 
Epoch 368/1000 
	 loss: 18.0333, MinusLogProbMetric: 18.0333, val_loss: 18.2287, val_MinusLogProbMetric: 18.2287

Epoch 368: val_loss did not improve from 17.84354
196/196 - 73s - loss: 18.0333 - MinusLogProbMetric: 18.0333 - val_loss: 18.2287 - val_MinusLogProbMetric: 18.2287 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 369/1000
2023-09-26 21:11:13.409 
Epoch 369/1000 
	 loss: 18.0441, MinusLogProbMetric: 18.0441, val_loss: 18.1207, val_MinusLogProbMetric: 18.1207

Epoch 369: val_loss did not improve from 17.84354
196/196 - 73s - loss: 18.0441 - MinusLogProbMetric: 18.0441 - val_loss: 18.1207 - val_MinusLogProbMetric: 18.1207 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 370/1000
2023-09-26 21:12:26.377 
Epoch 370/1000 
	 loss: 17.9870, MinusLogProbMetric: 17.9870, val_loss: 18.3656, val_MinusLogProbMetric: 18.3656

Epoch 370: val_loss did not improve from 17.84354
196/196 - 73s - loss: 17.9870 - MinusLogProbMetric: 17.9870 - val_loss: 18.3656 - val_MinusLogProbMetric: 18.3656 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 371/1000
2023-09-26 21:13:39.636 
Epoch 371/1000 
	 loss: 18.0004, MinusLogProbMetric: 18.0004, val_loss: 18.8668, val_MinusLogProbMetric: 18.8668

Epoch 371: val_loss did not improve from 17.84354
196/196 - 73s - loss: 18.0004 - MinusLogProbMetric: 18.0004 - val_loss: 18.8668 - val_MinusLogProbMetric: 18.8668 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 372/1000
2023-09-26 21:14:53.214 
Epoch 372/1000 
	 loss: 17.9972, MinusLogProbMetric: 17.9972, val_loss: 17.8310, val_MinusLogProbMetric: 17.8310

Epoch 372: val_loss improved from 17.84354 to 17.83098, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 17.9972 - MinusLogProbMetric: 17.9972 - val_loss: 17.8310 - val_MinusLogProbMetric: 17.8310 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 373/1000
2023-09-26 21:16:07.523 
Epoch 373/1000 
	 loss: 17.9393, MinusLogProbMetric: 17.9393, val_loss: 17.9853, val_MinusLogProbMetric: 17.9853

Epoch 373: val_loss did not improve from 17.83098
196/196 - 73s - loss: 17.9393 - MinusLogProbMetric: 17.9393 - val_loss: 17.9853 - val_MinusLogProbMetric: 17.9853 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 374/1000
2023-09-26 21:17:20.993 
Epoch 374/1000 
	 loss: 18.0528, MinusLogProbMetric: 18.0528, val_loss: 17.9065, val_MinusLogProbMetric: 17.9065

Epoch 374: val_loss did not improve from 17.83098
196/196 - 73s - loss: 18.0528 - MinusLogProbMetric: 18.0528 - val_loss: 17.9065 - val_MinusLogProbMetric: 17.9065 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 375/1000
2023-09-26 21:18:34.025 
Epoch 375/1000 
	 loss: 18.0075, MinusLogProbMetric: 18.0075, val_loss: 18.2775, val_MinusLogProbMetric: 18.2775

Epoch 375: val_loss did not improve from 17.83098
196/196 - 73s - loss: 18.0075 - MinusLogProbMetric: 18.0075 - val_loss: 18.2775 - val_MinusLogProbMetric: 18.2775 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 376/1000
2023-09-26 21:19:47.486 
Epoch 376/1000 
	 loss: 17.9411, MinusLogProbMetric: 17.9411, val_loss: 18.2693, val_MinusLogProbMetric: 18.2693

Epoch 376: val_loss did not improve from 17.83098
196/196 - 73s - loss: 17.9411 - MinusLogProbMetric: 17.9411 - val_loss: 18.2693 - val_MinusLogProbMetric: 18.2693 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 377/1000
2023-09-26 21:20:59.892 
Epoch 377/1000 
	 loss: 17.9469, MinusLogProbMetric: 17.9469, val_loss: 18.4654, val_MinusLogProbMetric: 18.4654

Epoch 377: val_loss did not improve from 17.83098
196/196 - 72s - loss: 17.9469 - MinusLogProbMetric: 17.9469 - val_loss: 18.4654 - val_MinusLogProbMetric: 18.4654 - lr: 3.3333e-04 - 72s/epoch - 369ms/step
Epoch 378/1000
2023-09-26 21:22:13.729 
Epoch 378/1000 
	 loss: 17.9196, MinusLogProbMetric: 17.9196, val_loss: 18.1620, val_MinusLogProbMetric: 18.1620

Epoch 378: val_loss did not improve from 17.83098
196/196 - 74s - loss: 17.9196 - MinusLogProbMetric: 17.9196 - val_loss: 18.1620 - val_MinusLogProbMetric: 18.1620 - lr: 3.3333e-04 - 74s/epoch - 377ms/step
Epoch 379/1000
2023-09-26 21:23:27.359 
Epoch 379/1000 
	 loss: 17.9299, MinusLogProbMetric: 17.9299, val_loss: 18.1037, val_MinusLogProbMetric: 18.1037

Epoch 379: val_loss did not improve from 17.83098
196/196 - 74s - loss: 17.9299 - MinusLogProbMetric: 17.9299 - val_loss: 18.1037 - val_MinusLogProbMetric: 18.1037 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 380/1000
2023-09-26 21:24:40.395 
Epoch 380/1000 
	 loss: 18.0337, MinusLogProbMetric: 18.0337, val_loss: 17.9093, val_MinusLogProbMetric: 17.9093

Epoch 380: val_loss did not improve from 17.83098
196/196 - 73s - loss: 18.0337 - MinusLogProbMetric: 18.0337 - val_loss: 17.9093 - val_MinusLogProbMetric: 17.9093 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 381/1000
2023-09-26 21:25:52.862 
Epoch 381/1000 
	 loss: 17.8949, MinusLogProbMetric: 17.8949, val_loss: 18.2305, val_MinusLogProbMetric: 18.2305

Epoch 381: val_loss did not improve from 17.83098
196/196 - 72s - loss: 17.8949 - MinusLogProbMetric: 17.8949 - val_loss: 18.2305 - val_MinusLogProbMetric: 18.2305 - lr: 3.3333e-04 - 72s/epoch - 370ms/step
Epoch 382/1000
2023-09-26 21:27:06.407 
Epoch 382/1000 
	 loss: 17.9234, MinusLogProbMetric: 17.9234, val_loss: 18.2301, val_MinusLogProbMetric: 18.2301

Epoch 382: val_loss did not improve from 17.83098
196/196 - 74s - loss: 17.9234 - MinusLogProbMetric: 17.9234 - val_loss: 18.2301 - val_MinusLogProbMetric: 18.2301 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 383/1000
2023-09-26 21:28:19.665 
Epoch 383/1000 
	 loss: 17.9927, MinusLogProbMetric: 17.9927, val_loss: 18.1904, val_MinusLogProbMetric: 18.1904

Epoch 383: val_loss did not improve from 17.83098
196/196 - 73s - loss: 17.9927 - MinusLogProbMetric: 17.9927 - val_loss: 18.1904 - val_MinusLogProbMetric: 18.1904 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 384/1000
2023-09-26 21:29:32.105 
Epoch 384/1000 
	 loss: 17.9188, MinusLogProbMetric: 17.9188, val_loss: 17.9972, val_MinusLogProbMetric: 17.9972

Epoch 384: val_loss did not improve from 17.83098
196/196 - 72s - loss: 17.9188 - MinusLogProbMetric: 17.9188 - val_loss: 17.9972 - val_MinusLogProbMetric: 17.9972 - lr: 3.3333e-04 - 72s/epoch - 370ms/step
Epoch 385/1000
2023-09-26 21:30:46.082 
Epoch 385/1000 
	 loss: 17.9632, MinusLogProbMetric: 17.9632, val_loss: 18.0702, val_MinusLogProbMetric: 18.0702

Epoch 385: val_loss did not improve from 17.83098
196/196 - 74s - loss: 17.9632 - MinusLogProbMetric: 17.9632 - val_loss: 18.0702 - val_MinusLogProbMetric: 18.0702 - lr: 3.3333e-04 - 74s/epoch - 377ms/step
Epoch 386/1000
2023-09-26 21:31:59.950 
Epoch 386/1000 
	 loss: 17.9201, MinusLogProbMetric: 17.9201, val_loss: 18.0730, val_MinusLogProbMetric: 18.0730

Epoch 386: val_loss did not improve from 17.83098
196/196 - 74s - loss: 17.9201 - MinusLogProbMetric: 17.9201 - val_loss: 18.0730 - val_MinusLogProbMetric: 18.0730 - lr: 3.3333e-04 - 74s/epoch - 377ms/step
Epoch 387/1000
2023-09-26 21:33:13.638 
Epoch 387/1000 
	 loss: 18.0126, MinusLogProbMetric: 18.0126, val_loss: 17.8522, val_MinusLogProbMetric: 17.8522

Epoch 387: val_loss did not improve from 17.83098
196/196 - 74s - loss: 18.0126 - MinusLogProbMetric: 18.0126 - val_loss: 17.8522 - val_MinusLogProbMetric: 17.8522 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 388/1000
2023-09-26 21:34:27.222 
Epoch 388/1000 
	 loss: 17.9689, MinusLogProbMetric: 17.9689, val_loss: 18.2073, val_MinusLogProbMetric: 18.2073

Epoch 388: val_loss did not improve from 17.83098
196/196 - 74s - loss: 17.9689 - MinusLogProbMetric: 17.9689 - val_loss: 18.2073 - val_MinusLogProbMetric: 18.2073 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 389/1000
2023-09-26 21:35:40.975 
Epoch 389/1000 
	 loss: 17.9778, MinusLogProbMetric: 17.9778, val_loss: 18.0793, val_MinusLogProbMetric: 18.0793

Epoch 389: val_loss did not improve from 17.83098
196/196 - 74s - loss: 17.9778 - MinusLogProbMetric: 17.9778 - val_loss: 18.0793 - val_MinusLogProbMetric: 18.0793 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 390/1000
2023-09-26 21:36:54.413 
Epoch 390/1000 
	 loss: 17.9152, MinusLogProbMetric: 17.9152, val_loss: 18.1714, val_MinusLogProbMetric: 18.1714

Epoch 390: val_loss did not improve from 17.83098
196/196 - 73s - loss: 17.9152 - MinusLogProbMetric: 17.9152 - val_loss: 18.1714 - val_MinusLogProbMetric: 18.1714 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 391/1000
2023-09-26 21:38:07.583 
Epoch 391/1000 
	 loss: 17.9822, MinusLogProbMetric: 17.9822, val_loss: 18.5650, val_MinusLogProbMetric: 18.5650

Epoch 391: val_loss did not improve from 17.83098
196/196 - 73s - loss: 17.9822 - MinusLogProbMetric: 17.9822 - val_loss: 18.5650 - val_MinusLogProbMetric: 18.5650 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 392/1000
2023-09-26 21:39:21.199 
Epoch 392/1000 
	 loss: 17.8853, MinusLogProbMetric: 17.8853, val_loss: 17.9331, val_MinusLogProbMetric: 17.9331

Epoch 392: val_loss did not improve from 17.83098
196/196 - 74s - loss: 17.8853 - MinusLogProbMetric: 17.8853 - val_loss: 17.9331 - val_MinusLogProbMetric: 17.9331 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 393/1000
2023-09-26 21:40:34.741 
Epoch 393/1000 
	 loss: 17.9693, MinusLogProbMetric: 17.9693, val_loss: 18.0627, val_MinusLogProbMetric: 18.0627

Epoch 393: val_loss did not improve from 17.83098
196/196 - 74s - loss: 17.9693 - MinusLogProbMetric: 17.9693 - val_loss: 18.0627 - val_MinusLogProbMetric: 18.0627 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 394/1000
2023-09-26 21:41:48.744 
Epoch 394/1000 
	 loss: 17.9314, MinusLogProbMetric: 17.9314, val_loss: 18.0387, val_MinusLogProbMetric: 18.0387

Epoch 394: val_loss did not improve from 17.83098
196/196 - 74s - loss: 17.9314 - MinusLogProbMetric: 17.9314 - val_loss: 18.0387 - val_MinusLogProbMetric: 18.0387 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 395/1000
2023-09-26 21:43:01.999 
Epoch 395/1000 
	 loss: 17.9469, MinusLogProbMetric: 17.9469, val_loss: 20.0916, val_MinusLogProbMetric: 20.0916

Epoch 395: val_loss did not improve from 17.83098
196/196 - 73s - loss: 17.9469 - MinusLogProbMetric: 17.9469 - val_loss: 20.0916 - val_MinusLogProbMetric: 20.0916 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 396/1000
2023-09-26 21:44:14.655 
Epoch 396/1000 
	 loss: 17.9332, MinusLogProbMetric: 17.9332, val_loss: 18.1701, val_MinusLogProbMetric: 18.1701

Epoch 396: val_loss did not improve from 17.83098
196/196 - 73s - loss: 17.9332 - MinusLogProbMetric: 17.9332 - val_loss: 18.1701 - val_MinusLogProbMetric: 18.1701 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 397/1000
2023-09-26 21:45:27.975 
Epoch 397/1000 
	 loss: 17.9108, MinusLogProbMetric: 17.9108, val_loss: 18.1684, val_MinusLogProbMetric: 18.1684

Epoch 397: val_loss did not improve from 17.83098
196/196 - 73s - loss: 17.9108 - MinusLogProbMetric: 17.9108 - val_loss: 18.1684 - val_MinusLogProbMetric: 18.1684 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 398/1000
2023-09-26 21:46:41.737 
Epoch 398/1000 
	 loss: 17.9180, MinusLogProbMetric: 17.9180, val_loss: 18.0504, val_MinusLogProbMetric: 18.0504

Epoch 398: val_loss did not improve from 17.83098
196/196 - 74s - loss: 17.9180 - MinusLogProbMetric: 17.9180 - val_loss: 18.0504 - val_MinusLogProbMetric: 18.0504 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 399/1000
2023-09-26 21:47:54.978 
Epoch 399/1000 
	 loss: 17.9587, MinusLogProbMetric: 17.9587, val_loss: 17.9793, val_MinusLogProbMetric: 17.9793

Epoch 399: val_loss did not improve from 17.83098
196/196 - 73s - loss: 17.9587 - MinusLogProbMetric: 17.9587 - val_loss: 17.9793 - val_MinusLogProbMetric: 17.9793 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 400/1000
2023-09-26 21:49:07.962 
Epoch 400/1000 
	 loss: 17.9447, MinusLogProbMetric: 17.9447, val_loss: 17.9244, val_MinusLogProbMetric: 17.9244

Epoch 400: val_loss did not improve from 17.83098
196/196 - 73s - loss: 17.9447 - MinusLogProbMetric: 17.9447 - val_loss: 17.9244 - val_MinusLogProbMetric: 17.9244 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 401/1000
2023-09-26 21:50:21.186 
Epoch 401/1000 
	 loss: 17.8442, MinusLogProbMetric: 17.8442, val_loss: 17.9683, val_MinusLogProbMetric: 17.9683

Epoch 401: val_loss did not improve from 17.83098
196/196 - 73s - loss: 17.8442 - MinusLogProbMetric: 17.8442 - val_loss: 17.9683 - val_MinusLogProbMetric: 17.9683 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 402/1000
2023-09-26 21:51:34.676 
Epoch 402/1000 
	 loss: 17.8909, MinusLogProbMetric: 17.8909, val_loss: 17.9818, val_MinusLogProbMetric: 17.9818

Epoch 402: val_loss did not improve from 17.83098
196/196 - 73s - loss: 17.8909 - MinusLogProbMetric: 17.8909 - val_loss: 17.9818 - val_MinusLogProbMetric: 17.9818 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 403/1000
2023-09-26 21:52:48.877 
Epoch 403/1000 
	 loss: 17.9342, MinusLogProbMetric: 17.9342, val_loss: 18.2646, val_MinusLogProbMetric: 18.2646

Epoch 403: val_loss did not improve from 17.83098
196/196 - 74s - loss: 17.9342 - MinusLogProbMetric: 17.9342 - val_loss: 18.2646 - val_MinusLogProbMetric: 18.2646 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 404/1000
2023-09-26 21:54:01.721 
Epoch 404/1000 
	 loss: 17.9375, MinusLogProbMetric: 17.9375, val_loss: 17.9819, val_MinusLogProbMetric: 17.9819

Epoch 404: val_loss did not improve from 17.83098
196/196 - 73s - loss: 17.9375 - MinusLogProbMetric: 17.9375 - val_loss: 17.9819 - val_MinusLogProbMetric: 17.9819 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 405/1000
2023-09-26 21:55:15.293 
Epoch 405/1000 
	 loss: 17.8946, MinusLogProbMetric: 17.8946, val_loss: 17.9430, val_MinusLogProbMetric: 17.9430

Epoch 405: val_loss did not improve from 17.83098
196/196 - 74s - loss: 17.8946 - MinusLogProbMetric: 17.8946 - val_loss: 17.9430 - val_MinusLogProbMetric: 17.9430 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 406/1000
2023-09-26 21:56:28.940 
Epoch 406/1000 
	 loss: 17.8023, MinusLogProbMetric: 17.8023, val_loss: 18.2359, val_MinusLogProbMetric: 18.2359

Epoch 406: val_loss did not improve from 17.83098
196/196 - 74s - loss: 17.8023 - MinusLogProbMetric: 17.8023 - val_loss: 18.2359 - val_MinusLogProbMetric: 18.2359 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 407/1000
2023-09-26 21:57:42.483 
Epoch 407/1000 
	 loss: 17.9395, MinusLogProbMetric: 17.9395, val_loss: 18.1157, val_MinusLogProbMetric: 18.1157

Epoch 407: val_loss did not improve from 17.83098
196/196 - 74s - loss: 17.9395 - MinusLogProbMetric: 17.9395 - val_loss: 18.1157 - val_MinusLogProbMetric: 18.1157 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 408/1000
2023-09-26 21:58:55.060 
Epoch 408/1000 
	 loss: 17.8843, MinusLogProbMetric: 17.8843, val_loss: 17.9222, val_MinusLogProbMetric: 17.9222

Epoch 408: val_loss did not improve from 17.83098
196/196 - 73s - loss: 17.8843 - MinusLogProbMetric: 17.8843 - val_loss: 17.9222 - val_MinusLogProbMetric: 17.9222 - lr: 3.3333e-04 - 73s/epoch - 370ms/step
Epoch 409/1000
2023-09-26 22:00:08.296 
Epoch 409/1000 
	 loss: 17.8500, MinusLogProbMetric: 17.8500, val_loss: 17.7653, val_MinusLogProbMetric: 17.7653

Epoch 409: val_loss improved from 17.83098 to 17.76531, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 74s - loss: 17.8500 - MinusLogProbMetric: 17.8500 - val_loss: 17.7653 - val_MinusLogProbMetric: 17.7653 - lr: 3.3333e-04 - 74s/epoch - 380ms/step
Epoch 410/1000
2023-09-26 22:01:22.681 
Epoch 410/1000 
	 loss: 17.9092, MinusLogProbMetric: 17.9092, val_loss: 18.0579, val_MinusLogProbMetric: 18.0579

Epoch 410: val_loss did not improve from 17.76531
196/196 - 73s - loss: 17.9092 - MinusLogProbMetric: 17.9092 - val_loss: 18.0579 - val_MinusLogProbMetric: 18.0579 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 411/1000
2023-09-26 22:02:35.546 
Epoch 411/1000 
	 loss: 17.8263, MinusLogProbMetric: 17.8263, val_loss: 17.8531, val_MinusLogProbMetric: 17.8531

Epoch 411: val_loss did not improve from 17.76531
196/196 - 73s - loss: 17.8263 - MinusLogProbMetric: 17.8263 - val_loss: 17.8531 - val_MinusLogProbMetric: 17.8531 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 412/1000
2023-09-26 22:03:49.587 
Epoch 412/1000 
	 loss: 17.9119, MinusLogProbMetric: 17.9119, val_loss: 18.0117, val_MinusLogProbMetric: 18.0117

Epoch 412: val_loss did not improve from 17.76531
196/196 - 74s - loss: 17.9119 - MinusLogProbMetric: 17.9119 - val_loss: 18.0117 - val_MinusLogProbMetric: 18.0117 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 413/1000
2023-09-26 22:05:03.200 
Epoch 413/1000 
	 loss: 17.8676, MinusLogProbMetric: 17.8676, val_loss: 18.2514, val_MinusLogProbMetric: 18.2514

Epoch 413: val_loss did not improve from 17.76531
196/196 - 74s - loss: 17.8676 - MinusLogProbMetric: 17.8676 - val_loss: 18.2514 - val_MinusLogProbMetric: 18.2514 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 414/1000
2023-09-26 22:06:16.453 
Epoch 414/1000 
	 loss: 18.3856, MinusLogProbMetric: 18.3856, val_loss: 18.8040, val_MinusLogProbMetric: 18.8040

Epoch 414: val_loss did not improve from 17.76531
196/196 - 73s - loss: 18.3856 - MinusLogProbMetric: 18.3856 - val_loss: 18.8040 - val_MinusLogProbMetric: 18.8040 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 415/1000
2023-09-26 22:07:29.410 
Epoch 415/1000 
	 loss: 18.2100, MinusLogProbMetric: 18.2100, val_loss: 18.4485, val_MinusLogProbMetric: 18.4485

Epoch 415: val_loss did not improve from 17.76531
196/196 - 73s - loss: 18.2100 - MinusLogProbMetric: 18.2100 - val_loss: 18.4485 - val_MinusLogProbMetric: 18.4485 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 416/1000
2023-09-26 22:08:42.763 
Epoch 416/1000 
	 loss: 18.0925, MinusLogProbMetric: 18.0925, val_loss: 18.2624, val_MinusLogProbMetric: 18.2624

Epoch 416: val_loss did not improve from 17.76531
196/196 - 73s - loss: 18.0925 - MinusLogProbMetric: 18.0925 - val_loss: 18.2624 - val_MinusLogProbMetric: 18.2624 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 417/1000
2023-09-26 22:09:56.335 
Epoch 417/1000 
	 loss: 18.0260, MinusLogProbMetric: 18.0260, val_loss: 18.0898, val_MinusLogProbMetric: 18.0898

Epoch 417: val_loss did not improve from 17.76531
196/196 - 74s - loss: 18.0260 - MinusLogProbMetric: 18.0260 - val_loss: 18.0898 - val_MinusLogProbMetric: 18.0898 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 418/1000
2023-09-26 22:11:10.086 
Epoch 418/1000 
	 loss: 17.9217, MinusLogProbMetric: 17.9217, val_loss: 17.8884, val_MinusLogProbMetric: 17.8884

Epoch 418: val_loss did not improve from 17.76531
196/196 - 74s - loss: 17.9217 - MinusLogProbMetric: 17.9217 - val_loss: 17.8884 - val_MinusLogProbMetric: 17.8884 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 419/1000
2023-09-26 22:12:23.723 
Epoch 419/1000 
	 loss: 17.8466, MinusLogProbMetric: 17.8466, val_loss: 17.9640, val_MinusLogProbMetric: 17.9640

Epoch 419: val_loss did not improve from 17.76531
196/196 - 74s - loss: 17.8466 - MinusLogProbMetric: 17.8466 - val_loss: 17.9640 - val_MinusLogProbMetric: 17.9640 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 420/1000
2023-09-26 22:13:36.704 
Epoch 420/1000 
	 loss: 18.0650, MinusLogProbMetric: 18.0650, val_loss: 17.8620, val_MinusLogProbMetric: 17.8620

Epoch 420: val_loss did not improve from 17.76531
196/196 - 73s - loss: 18.0650 - MinusLogProbMetric: 18.0650 - val_loss: 17.8620 - val_MinusLogProbMetric: 17.8620 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 421/1000
2023-09-26 22:14:50.589 
Epoch 421/1000 
	 loss: 17.9311, MinusLogProbMetric: 17.9311, val_loss: 17.8762, val_MinusLogProbMetric: 17.8762

Epoch 421: val_loss did not improve from 17.76531
196/196 - 74s - loss: 17.9311 - MinusLogProbMetric: 17.9311 - val_loss: 17.8762 - val_MinusLogProbMetric: 17.8762 - lr: 3.3333e-04 - 74s/epoch - 377ms/step
Epoch 422/1000
2023-09-26 22:16:04.746 
Epoch 422/1000 
	 loss: 17.8624, MinusLogProbMetric: 17.8624, val_loss: 17.9881, val_MinusLogProbMetric: 17.9881

Epoch 422: val_loss did not improve from 17.76531
196/196 - 74s - loss: 17.8624 - MinusLogProbMetric: 17.8624 - val_loss: 17.9881 - val_MinusLogProbMetric: 17.9881 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 423/1000
2023-09-26 22:17:17.471 
Epoch 423/1000 
	 loss: 17.9644, MinusLogProbMetric: 17.9644, val_loss: 18.2634, val_MinusLogProbMetric: 18.2634

Epoch 423: val_loss did not improve from 17.76531
196/196 - 73s - loss: 17.9644 - MinusLogProbMetric: 17.9644 - val_loss: 18.2634 - val_MinusLogProbMetric: 18.2634 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 424/1000
2023-09-26 22:18:30.797 
Epoch 424/1000 
	 loss: 17.9251, MinusLogProbMetric: 17.9251, val_loss: 18.2603, val_MinusLogProbMetric: 18.2603

Epoch 424: val_loss did not improve from 17.76531
196/196 - 73s - loss: 17.9251 - MinusLogProbMetric: 17.9251 - val_loss: 18.2603 - val_MinusLogProbMetric: 18.2603 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 425/1000
2023-09-26 22:19:44.816 
Epoch 425/1000 
	 loss: 17.9357, MinusLogProbMetric: 17.9357, val_loss: 17.8607, val_MinusLogProbMetric: 17.8607

Epoch 425: val_loss did not improve from 17.76531
196/196 - 74s - loss: 17.9357 - MinusLogProbMetric: 17.9357 - val_loss: 17.8607 - val_MinusLogProbMetric: 17.8607 - lr: 3.3333e-04 - 74s/epoch - 378ms/step
Epoch 426/1000
2023-09-26 22:20:58.422 
Epoch 426/1000 
	 loss: 17.8198, MinusLogProbMetric: 17.8198, val_loss: 17.9734, val_MinusLogProbMetric: 17.9734

Epoch 426: val_loss did not improve from 17.76531
196/196 - 74s - loss: 17.8198 - MinusLogProbMetric: 17.8198 - val_loss: 17.9734 - val_MinusLogProbMetric: 17.9734 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 427/1000
2023-09-26 22:22:11.209 
Epoch 427/1000 
	 loss: 17.8501, MinusLogProbMetric: 17.8501, val_loss: 17.9805, val_MinusLogProbMetric: 17.9805

Epoch 427: val_loss did not improve from 17.76531
196/196 - 73s - loss: 17.8501 - MinusLogProbMetric: 17.8501 - val_loss: 17.9805 - val_MinusLogProbMetric: 17.9805 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 428/1000
2023-09-26 22:23:24.175 
Epoch 428/1000 
	 loss: 17.8769, MinusLogProbMetric: 17.8769, val_loss: 18.1097, val_MinusLogProbMetric: 18.1097

Epoch 428: val_loss did not improve from 17.76531
196/196 - 73s - loss: 17.8769 - MinusLogProbMetric: 17.8769 - val_loss: 18.1097 - val_MinusLogProbMetric: 18.1097 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 429/1000
2023-09-26 22:24:37.186 
Epoch 429/1000 
	 loss: 17.8641, MinusLogProbMetric: 17.8641, val_loss: 18.1241, val_MinusLogProbMetric: 18.1241

Epoch 429: val_loss did not improve from 17.76531
196/196 - 73s - loss: 17.8641 - MinusLogProbMetric: 17.8641 - val_loss: 18.1241 - val_MinusLogProbMetric: 18.1241 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 430/1000
2023-09-26 22:25:50.073 
Epoch 430/1000 
	 loss: 17.9553, MinusLogProbMetric: 17.9553, val_loss: 17.8486, val_MinusLogProbMetric: 17.8486

Epoch 430: val_loss did not improve from 17.76531
196/196 - 73s - loss: 17.9553 - MinusLogProbMetric: 17.9553 - val_loss: 17.8486 - val_MinusLogProbMetric: 17.8486 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 431/1000
2023-09-26 22:27:03.273 
Epoch 431/1000 
	 loss: 17.8470, MinusLogProbMetric: 17.8470, val_loss: 18.1072, val_MinusLogProbMetric: 18.1072

Epoch 431: val_loss did not improve from 17.76531
196/196 - 73s - loss: 17.8470 - MinusLogProbMetric: 17.8470 - val_loss: 18.1072 - val_MinusLogProbMetric: 18.1072 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 432/1000
2023-09-26 22:28:16.648 
Epoch 432/1000 
	 loss: 17.8844, MinusLogProbMetric: 17.8844, val_loss: 17.8690, val_MinusLogProbMetric: 17.8690

Epoch 432: val_loss did not improve from 17.76531
196/196 - 73s - loss: 17.8844 - MinusLogProbMetric: 17.8844 - val_loss: 17.8690 - val_MinusLogProbMetric: 17.8690 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 433/1000
2023-09-26 22:29:30.034 
Epoch 433/1000 
	 loss: 17.9964, MinusLogProbMetric: 17.9964, val_loss: 17.7663, val_MinusLogProbMetric: 17.7663

Epoch 433: val_loss did not improve from 17.76531
196/196 - 73s - loss: 17.9964 - MinusLogProbMetric: 17.9964 - val_loss: 17.7663 - val_MinusLogProbMetric: 17.7663 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 434/1000
2023-09-26 22:30:43.329 
Epoch 434/1000 
	 loss: 17.8095, MinusLogProbMetric: 17.8095, val_loss: 18.0214, val_MinusLogProbMetric: 18.0214

Epoch 434: val_loss did not improve from 17.76531
196/196 - 73s - loss: 17.8095 - MinusLogProbMetric: 17.8095 - val_loss: 18.0214 - val_MinusLogProbMetric: 18.0214 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 435/1000
2023-09-26 22:31:56.098 
Epoch 435/1000 
	 loss: 17.9048, MinusLogProbMetric: 17.9048, val_loss: 17.9953, val_MinusLogProbMetric: 17.9953

Epoch 435: val_loss did not improve from 17.76531
196/196 - 73s - loss: 17.9048 - MinusLogProbMetric: 17.9048 - val_loss: 17.9953 - val_MinusLogProbMetric: 17.9953 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 436/1000
2023-09-26 22:33:05.066 
Epoch 436/1000 
	 loss: 17.8648, MinusLogProbMetric: 17.8648, val_loss: 17.8574, val_MinusLogProbMetric: 17.8574

Epoch 436: val_loss did not improve from 17.76531
196/196 - 69s - loss: 17.8648 - MinusLogProbMetric: 17.8648 - val_loss: 17.8574 - val_MinusLogProbMetric: 17.8574 - lr: 3.3333e-04 - 69s/epoch - 352ms/step
Epoch 437/1000
2023-09-26 22:34:09.042 
Epoch 437/1000 
	 loss: 17.8398, MinusLogProbMetric: 17.8398, val_loss: 18.1524, val_MinusLogProbMetric: 18.1524

Epoch 437: val_loss did not improve from 17.76531
196/196 - 64s - loss: 17.8398 - MinusLogProbMetric: 17.8398 - val_loss: 18.1524 - val_MinusLogProbMetric: 18.1524 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 438/1000
2023-09-26 22:35:21.195 
Epoch 438/1000 
	 loss: 17.9396, MinusLogProbMetric: 17.9396, val_loss: 17.9717, val_MinusLogProbMetric: 17.9717

Epoch 438: val_loss did not improve from 17.76531
196/196 - 72s - loss: 17.9396 - MinusLogProbMetric: 17.9396 - val_loss: 17.9717 - val_MinusLogProbMetric: 17.9717 - lr: 3.3333e-04 - 72s/epoch - 368ms/step
Epoch 439/1000
2023-09-26 22:36:27.475 
Epoch 439/1000 
	 loss: 17.7678, MinusLogProbMetric: 17.7678, val_loss: 17.6892, val_MinusLogProbMetric: 17.6892

Epoch 439: val_loss improved from 17.76531 to 17.68919, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 67s - loss: 17.7678 - MinusLogProbMetric: 17.7678 - val_loss: 17.6892 - val_MinusLogProbMetric: 17.6892 - lr: 3.3333e-04 - 67s/epoch - 344ms/step
Epoch 440/1000
2023-09-26 22:37:33.645 
Epoch 440/1000 
	 loss: 17.7827, MinusLogProbMetric: 17.7827, val_loss: 17.8314, val_MinusLogProbMetric: 17.8314

Epoch 440: val_loss did not improve from 17.68919
196/196 - 65s - loss: 17.7827 - MinusLogProbMetric: 17.7827 - val_loss: 17.8314 - val_MinusLogProbMetric: 17.8314 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 441/1000
2023-09-26 22:38:45.444 
Epoch 441/1000 
	 loss: 17.8177, MinusLogProbMetric: 17.8177, val_loss: 17.9759, val_MinusLogProbMetric: 17.9759

Epoch 441: val_loss did not improve from 17.68919
196/196 - 72s - loss: 17.8177 - MinusLogProbMetric: 17.8177 - val_loss: 17.9759 - val_MinusLogProbMetric: 17.9759 - lr: 3.3333e-04 - 72s/epoch - 366ms/step
Epoch 442/1000
2023-09-26 22:39:58.012 
Epoch 442/1000 
	 loss: 17.8447, MinusLogProbMetric: 17.8447, val_loss: 18.0290, val_MinusLogProbMetric: 18.0290

Epoch 442: val_loss did not improve from 17.68919
196/196 - 73s - loss: 17.8447 - MinusLogProbMetric: 17.8447 - val_loss: 18.0290 - val_MinusLogProbMetric: 18.0290 - lr: 3.3333e-04 - 73s/epoch - 370ms/step
Epoch 443/1000
2023-09-26 22:41:11.346 
Epoch 443/1000 
	 loss: 17.8107, MinusLogProbMetric: 17.8107, val_loss: 18.4145, val_MinusLogProbMetric: 18.4145

Epoch 443: val_loss did not improve from 17.68919
196/196 - 73s - loss: 17.8107 - MinusLogProbMetric: 17.8107 - val_loss: 18.4145 - val_MinusLogProbMetric: 18.4145 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 444/1000
2023-09-26 22:42:25.077 
Epoch 444/1000 
	 loss: 17.9300, MinusLogProbMetric: 17.9300, val_loss: 17.8172, val_MinusLogProbMetric: 17.8172

Epoch 444: val_loss did not improve from 17.68919
196/196 - 74s - loss: 17.9300 - MinusLogProbMetric: 17.9300 - val_loss: 17.8172 - val_MinusLogProbMetric: 17.8172 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 445/1000
2023-09-26 22:43:38.694 
Epoch 445/1000 
	 loss: 17.8145, MinusLogProbMetric: 17.8145, val_loss: 18.0034, val_MinusLogProbMetric: 18.0034

Epoch 445: val_loss did not improve from 17.68919
196/196 - 74s - loss: 17.8145 - MinusLogProbMetric: 17.8145 - val_loss: 18.0034 - val_MinusLogProbMetric: 18.0034 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 446/1000
2023-09-26 22:44:52.257 
Epoch 446/1000 
	 loss: 17.8264, MinusLogProbMetric: 17.8264, val_loss: 18.3668, val_MinusLogProbMetric: 18.3668

Epoch 446: val_loss did not improve from 17.68919
196/196 - 74s - loss: 17.8264 - MinusLogProbMetric: 17.8264 - val_loss: 18.3668 - val_MinusLogProbMetric: 18.3668 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 447/1000
2023-09-26 22:46:05.392 
Epoch 447/1000 
	 loss: 17.8417, MinusLogProbMetric: 17.8417, val_loss: 18.2059, val_MinusLogProbMetric: 18.2059

Epoch 447: val_loss did not improve from 17.68919
196/196 - 73s - loss: 17.8417 - MinusLogProbMetric: 17.8417 - val_loss: 18.2059 - val_MinusLogProbMetric: 18.2059 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 448/1000
2023-09-26 22:47:18.483 
Epoch 448/1000 
	 loss: 17.7352, MinusLogProbMetric: 17.7352, val_loss: 17.7564, val_MinusLogProbMetric: 17.7564

Epoch 448: val_loss did not improve from 17.68919
196/196 - 73s - loss: 17.7352 - MinusLogProbMetric: 17.7352 - val_loss: 17.7564 - val_MinusLogProbMetric: 17.7564 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 449/1000
2023-09-26 22:48:31.515 
Epoch 449/1000 
	 loss: 17.7952, MinusLogProbMetric: 17.7952, val_loss: 17.8127, val_MinusLogProbMetric: 17.8127

Epoch 449: val_loss did not improve from 17.68919
196/196 - 73s - loss: 17.7952 - MinusLogProbMetric: 17.7952 - val_loss: 17.8127 - val_MinusLogProbMetric: 17.8127 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 450/1000
2023-09-26 22:49:45.106 
Epoch 450/1000 
	 loss: 17.8149, MinusLogProbMetric: 17.8149, val_loss: 17.8715, val_MinusLogProbMetric: 17.8715

Epoch 450: val_loss did not improve from 17.68919
196/196 - 74s - loss: 17.8149 - MinusLogProbMetric: 17.8149 - val_loss: 17.8715 - val_MinusLogProbMetric: 17.8715 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 451/1000
2023-09-26 22:50:58.483 
Epoch 451/1000 
	 loss: 17.8502, MinusLogProbMetric: 17.8502, val_loss: 17.8715, val_MinusLogProbMetric: 17.8715

Epoch 451: val_loss did not improve from 17.68919
196/196 - 73s - loss: 17.8502 - MinusLogProbMetric: 17.8502 - val_loss: 17.8715 - val_MinusLogProbMetric: 17.8715 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 452/1000
2023-09-26 22:52:11.408 
Epoch 452/1000 
	 loss: 17.8078, MinusLogProbMetric: 17.8078, val_loss: 17.8592, val_MinusLogProbMetric: 17.8592

Epoch 452: val_loss did not improve from 17.68919
196/196 - 73s - loss: 17.8078 - MinusLogProbMetric: 17.8078 - val_loss: 17.8592 - val_MinusLogProbMetric: 17.8592 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 453/1000
2023-09-26 22:53:24.299 
Epoch 453/1000 
	 loss: 17.7628, MinusLogProbMetric: 17.7628, val_loss: 18.1679, val_MinusLogProbMetric: 18.1679

Epoch 453: val_loss did not improve from 17.68919
196/196 - 73s - loss: 17.7628 - MinusLogProbMetric: 17.7628 - val_loss: 18.1679 - val_MinusLogProbMetric: 18.1679 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 454/1000
2023-09-26 22:54:37.450 
Epoch 454/1000 
	 loss: 17.8213, MinusLogProbMetric: 17.8213, val_loss: 18.1065, val_MinusLogProbMetric: 18.1065

Epoch 454: val_loss did not improve from 17.68919
196/196 - 73s - loss: 17.8213 - MinusLogProbMetric: 17.8213 - val_loss: 18.1065 - val_MinusLogProbMetric: 18.1065 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 455/1000
2023-09-26 22:55:50.592 
Epoch 455/1000 
	 loss: 17.7631, MinusLogProbMetric: 17.7631, val_loss: 17.7990, val_MinusLogProbMetric: 17.7990

Epoch 455: val_loss did not improve from 17.68919
196/196 - 73s - loss: 17.7631 - MinusLogProbMetric: 17.7631 - val_loss: 17.7990 - val_MinusLogProbMetric: 17.7990 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 456/1000
2023-09-26 22:57:04.201 
Epoch 456/1000 
	 loss: 17.7983, MinusLogProbMetric: 17.7983, val_loss: 17.6461, val_MinusLogProbMetric: 17.6461

Epoch 456: val_loss improved from 17.68919 to 17.64610, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 17.7983 - MinusLogProbMetric: 17.7983 - val_loss: 17.6461 - val_MinusLogProbMetric: 17.6461 - lr: 3.3333e-04 - 75s/epoch - 382ms/step
Epoch 457/1000
2023-09-26 22:58:18.335 
Epoch 457/1000 
	 loss: 17.8432, MinusLogProbMetric: 17.8432, val_loss: 18.6705, val_MinusLogProbMetric: 18.6705

Epoch 457: val_loss did not improve from 17.64610
196/196 - 73s - loss: 17.8432 - MinusLogProbMetric: 17.8432 - val_loss: 18.6705 - val_MinusLogProbMetric: 18.6705 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 458/1000
2023-09-26 22:59:31.337 
Epoch 458/1000 
	 loss: 17.8395, MinusLogProbMetric: 17.8395, val_loss: 18.0834, val_MinusLogProbMetric: 18.0834

Epoch 458: val_loss did not improve from 17.64610
196/196 - 73s - loss: 17.8395 - MinusLogProbMetric: 17.8395 - val_loss: 18.0834 - val_MinusLogProbMetric: 18.0834 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 459/1000
2023-09-26 23:00:44.422 
Epoch 459/1000 
	 loss: 17.8089, MinusLogProbMetric: 17.8089, val_loss: 19.5856, val_MinusLogProbMetric: 19.5856

Epoch 459: val_loss did not improve from 17.64610
196/196 - 73s - loss: 17.8089 - MinusLogProbMetric: 17.8089 - val_loss: 19.5856 - val_MinusLogProbMetric: 19.5856 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 460/1000
2023-09-26 23:01:57.139 
Epoch 460/1000 
	 loss: 17.8690, MinusLogProbMetric: 17.8690, val_loss: 18.0411, val_MinusLogProbMetric: 18.0411

Epoch 460: val_loss did not improve from 17.64610
196/196 - 73s - loss: 17.8690 - MinusLogProbMetric: 17.8690 - val_loss: 18.0411 - val_MinusLogProbMetric: 18.0411 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 461/1000
2023-09-26 23:03:10.856 
Epoch 461/1000 
	 loss: 17.8097, MinusLogProbMetric: 17.8097, val_loss: 17.6731, val_MinusLogProbMetric: 17.6731

Epoch 461: val_loss did not improve from 17.64610
196/196 - 74s - loss: 17.8097 - MinusLogProbMetric: 17.8097 - val_loss: 17.6731 - val_MinusLogProbMetric: 17.6731 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 462/1000
2023-09-26 23:04:24.028 
Epoch 462/1000 
	 loss: 17.7587, MinusLogProbMetric: 17.7587, val_loss: 17.8511, val_MinusLogProbMetric: 17.8511

Epoch 462: val_loss did not improve from 17.64610
196/196 - 73s - loss: 17.7587 - MinusLogProbMetric: 17.7587 - val_loss: 17.8511 - val_MinusLogProbMetric: 17.8511 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 463/1000
2023-09-26 23:05:37.307 
Epoch 463/1000 
	 loss: 17.7645, MinusLogProbMetric: 17.7645, val_loss: 17.8793, val_MinusLogProbMetric: 17.8793

Epoch 463: val_loss did not improve from 17.64610
196/196 - 73s - loss: 17.7645 - MinusLogProbMetric: 17.7645 - val_loss: 17.8793 - val_MinusLogProbMetric: 17.8793 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 464/1000
2023-09-26 23:06:50.508 
Epoch 464/1000 
	 loss: 17.8283, MinusLogProbMetric: 17.8283, val_loss: 17.6804, val_MinusLogProbMetric: 17.6804

Epoch 464: val_loss did not improve from 17.64610
196/196 - 73s - loss: 17.8283 - MinusLogProbMetric: 17.8283 - val_loss: 17.6804 - val_MinusLogProbMetric: 17.6804 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 465/1000
2023-09-26 23:08:03.099 
Epoch 465/1000 
	 loss: 17.7235, MinusLogProbMetric: 17.7235, val_loss: 17.8064, val_MinusLogProbMetric: 17.8064

Epoch 465: val_loss did not improve from 17.64610
196/196 - 73s - loss: 17.7235 - MinusLogProbMetric: 17.7235 - val_loss: 17.8064 - val_MinusLogProbMetric: 17.8064 - lr: 3.3333e-04 - 73s/epoch - 370ms/step
Epoch 466/1000
2023-09-26 23:09:16.598 
Epoch 466/1000 
	 loss: 17.7513, MinusLogProbMetric: 17.7513, val_loss: 17.9975, val_MinusLogProbMetric: 17.9975

Epoch 466: val_loss did not improve from 17.64610
196/196 - 73s - loss: 17.7513 - MinusLogProbMetric: 17.7513 - val_loss: 17.9975 - val_MinusLogProbMetric: 17.9975 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 467/1000
2023-09-26 23:10:30.090 
Epoch 467/1000 
	 loss: 17.7454, MinusLogProbMetric: 17.7454, val_loss: 17.7144, val_MinusLogProbMetric: 17.7144

Epoch 467: val_loss did not improve from 17.64610
196/196 - 73s - loss: 17.7454 - MinusLogProbMetric: 17.7454 - val_loss: 17.7144 - val_MinusLogProbMetric: 17.7144 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 468/1000
2023-09-26 23:11:43.688 
Epoch 468/1000 
	 loss: 17.7141, MinusLogProbMetric: 17.7141, val_loss: 17.6942, val_MinusLogProbMetric: 17.6942

Epoch 468: val_loss did not improve from 17.64610
196/196 - 74s - loss: 17.7141 - MinusLogProbMetric: 17.7141 - val_loss: 17.6942 - val_MinusLogProbMetric: 17.6942 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 469/1000
2023-09-26 23:12:57.135 
Epoch 469/1000 
	 loss: 17.8295, MinusLogProbMetric: 17.8295, val_loss: 17.9874, val_MinusLogProbMetric: 17.9874

Epoch 469: val_loss did not improve from 17.64610
196/196 - 73s - loss: 17.8295 - MinusLogProbMetric: 17.8295 - val_loss: 17.9874 - val_MinusLogProbMetric: 17.9874 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 470/1000
2023-09-26 23:14:10.334 
Epoch 470/1000 
	 loss: 17.6934, MinusLogProbMetric: 17.6934, val_loss: 17.9902, val_MinusLogProbMetric: 17.9902

Epoch 470: val_loss did not improve from 17.64610
196/196 - 73s - loss: 17.6934 - MinusLogProbMetric: 17.6934 - val_loss: 17.9902 - val_MinusLogProbMetric: 17.9902 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 471/1000
2023-09-26 23:15:24.047 
Epoch 471/1000 
	 loss: 17.7424, MinusLogProbMetric: 17.7424, val_loss: 18.0256, val_MinusLogProbMetric: 18.0256

Epoch 471: val_loss did not improve from 17.64610
196/196 - 74s - loss: 17.7424 - MinusLogProbMetric: 17.7424 - val_loss: 18.0256 - val_MinusLogProbMetric: 18.0256 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 472/1000
2023-09-26 23:16:36.784 
Epoch 472/1000 
	 loss: 17.7654, MinusLogProbMetric: 17.7654, val_loss: 18.4416, val_MinusLogProbMetric: 18.4416

Epoch 472: val_loss did not improve from 17.64610
196/196 - 73s - loss: 17.7654 - MinusLogProbMetric: 17.7654 - val_loss: 18.4416 - val_MinusLogProbMetric: 18.4416 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 473/1000
2023-09-26 23:17:50.527 
Epoch 473/1000 
	 loss: 17.7520, MinusLogProbMetric: 17.7520, val_loss: 17.8513, val_MinusLogProbMetric: 17.8513

Epoch 473: val_loss did not improve from 17.64610
196/196 - 74s - loss: 17.7520 - MinusLogProbMetric: 17.7520 - val_loss: 17.8513 - val_MinusLogProbMetric: 17.8513 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 474/1000
2023-09-26 23:19:03.422 
Epoch 474/1000 
	 loss: 17.7760, MinusLogProbMetric: 17.7760, val_loss: 18.1638, val_MinusLogProbMetric: 18.1638

Epoch 474: val_loss did not improve from 17.64610
196/196 - 73s - loss: 17.7760 - MinusLogProbMetric: 17.7760 - val_loss: 18.1638 - val_MinusLogProbMetric: 18.1638 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 475/1000
2023-09-26 23:20:16.362 
Epoch 475/1000 
	 loss: 17.6968, MinusLogProbMetric: 17.6968, val_loss: 18.2933, val_MinusLogProbMetric: 18.2933

Epoch 475: val_loss did not improve from 17.64610
196/196 - 73s - loss: 17.6968 - MinusLogProbMetric: 17.6968 - val_loss: 18.2933 - val_MinusLogProbMetric: 18.2933 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 476/1000
2023-09-26 23:21:29.616 
Epoch 476/1000 
	 loss: 17.7822, MinusLogProbMetric: 17.7822, val_loss: 17.8559, val_MinusLogProbMetric: 17.8559

Epoch 476: val_loss did not improve from 17.64610
196/196 - 73s - loss: 17.7822 - MinusLogProbMetric: 17.7822 - val_loss: 17.8559 - val_MinusLogProbMetric: 17.8559 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 477/1000
2023-09-26 23:22:42.747 
Epoch 477/1000 
	 loss: 17.7253, MinusLogProbMetric: 17.7253, val_loss: 18.1389, val_MinusLogProbMetric: 18.1389

Epoch 477: val_loss did not improve from 17.64610
196/196 - 73s - loss: 17.7253 - MinusLogProbMetric: 17.7253 - val_loss: 18.1389 - val_MinusLogProbMetric: 18.1389 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 478/1000
2023-09-26 23:23:55.921 
Epoch 478/1000 
	 loss: 17.7586, MinusLogProbMetric: 17.7586, val_loss: 18.0186, val_MinusLogProbMetric: 18.0186

Epoch 478: val_loss did not improve from 17.64610
196/196 - 73s - loss: 17.7586 - MinusLogProbMetric: 17.7586 - val_loss: 18.0186 - val_MinusLogProbMetric: 18.0186 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 479/1000
2023-09-26 23:25:09.168 
Epoch 479/1000 
	 loss: 17.7061, MinusLogProbMetric: 17.7061, val_loss: 18.0886, val_MinusLogProbMetric: 18.0886

Epoch 479: val_loss did not improve from 17.64610
196/196 - 73s - loss: 17.7061 - MinusLogProbMetric: 17.7061 - val_loss: 18.0886 - val_MinusLogProbMetric: 18.0886 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 480/1000
2023-09-26 23:26:22.651 
Epoch 480/1000 
	 loss: 17.7029, MinusLogProbMetric: 17.7029, val_loss: 17.6305, val_MinusLogProbMetric: 17.6305

Epoch 480: val_loss improved from 17.64610 to 17.63054, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 17.7029 - MinusLogProbMetric: 17.7029 - val_loss: 17.6305 - val_MinusLogProbMetric: 17.6305 - lr: 3.3333e-04 - 75s/epoch - 382ms/step
Epoch 481/1000
2023-09-26 23:27:37.196 
Epoch 481/1000 
	 loss: 17.7452, MinusLogProbMetric: 17.7452, val_loss: 17.5518, val_MinusLogProbMetric: 17.5518

Epoch 481: val_loss improved from 17.63054 to 17.55178, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 74s - loss: 17.7452 - MinusLogProbMetric: 17.7452 - val_loss: 17.5518 - val_MinusLogProbMetric: 17.5518 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 482/1000
2023-09-26 23:28:51.615 
Epoch 482/1000 
	 loss: 17.7989, MinusLogProbMetric: 17.7989, val_loss: 17.5643, val_MinusLogProbMetric: 17.5643

Epoch 482: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7989 - MinusLogProbMetric: 17.7989 - val_loss: 17.5643 - val_MinusLogProbMetric: 17.5643 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 483/1000
2023-09-26 23:30:04.983 
Epoch 483/1000 
	 loss: 17.7532, MinusLogProbMetric: 17.7532, val_loss: 17.8884, val_MinusLogProbMetric: 17.8884

Epoch 483: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7532 - MinusLogProbMetric: 17.7532 - val_loss: 17.8884 - val_MinusLogProbMetric: 17.8884 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 484/1000
2023-09-26 23:31:17.740 
Epoch 484/1000 
	 loss: 17.7343, MinusLogProbMetric: 17.7343, val_loss: 17.6575, val_MinusLogProbMetric: 17.6575

Epoch 484: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7343 - MinusLogProbMetric: 17.7343 - val_loss: 17.6575 - val_MinusLogProbMetric: 17.6575 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 485/1000
2023-09-26 23:32:30.948 
Epoch 485/1000 
	 loss: 17.7604, MinusLogProbMetric: 17.7604, val_loss: 18.1383, val_MinusLogProbMetric: 18.1383

Epoch 485: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7604 - MinusLogProbMetric: 17.7604 - val_loss: 18.1383 - val_MinusLogProbMetric: 18.1383 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 486/1000
2023-09-26 23:33:44.508 
Epoch 486/1000 
	 loss: 17.7270, MinusLogProbMetric: 17.7270, val_loss: 17.8538, val_MinusLogProbMetric: 17.8538

Epoch 486: val_loss did not improve from 17.55178
196/196 - 74s - loss: 17.7270 - MinusLogProbMetric: 17.7270 - val_loss: 17.8538 - val_MinusLogProbMetric: 17.8538 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 487/1000
2023-09-26 23:34:58.080 
Epoch 487/1000 
	 loss: 17.6938, MinusLogProbMetric: 17.6938, val_loss: 17.8161, val_MinusLogProbMetric: 17.8161

Epoch 487: val_loss did not improve from 17.55178
196/196 - 74s - loss: 17.6938 - MinusLogProbMetric: 17.6938 - val_loss: 17.8161 - val_MinusLogProbMetric: 17.8161 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 488/1000
2023-09-26 23:36:11.144 
Epoch 488/1000 
	 loss: 17.7911, MinusLogProbMetric: 17.7911, val_loss: 17.9623, val_MinusLogProbMetric: 17.9623

Epoch 488: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7911 - MinusLogProbMetric: 17.7911 - val_loss: 17.9623 - val_MinusLogProbMetric: 17.9623 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 489/1000
2023-09-26 23:37:24.651 
Epoch 489/1000 
	 loss: 17.7633, MinusLogProbMetric: 17.7633, val_loss: 17.8458, val_MinusLogProbMetric: 17.8458

Epoch 489: val_loss did not improve from 17.55178
196/196 - 74s - loss: 17.7633 - MinusLogProbMetric: 17.7633 - val_loss: 17.8458 - val_MinusLogProbMetric: 17.8458 - lr: 3.3333e-04 - 74s/epoch - 375ms/step
Epoch 490/1000
2023-09-26 23:38:38.008 
Epoch 490/1000 
	 loss: 17.7463, MinusLogProbMetric: 17.7463, val_loss: 18.1982, val_MinusLogProbMetric: 18.1982

Epoch 490: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7463 - MinusLogProbMetric: 17.7463 - val_loss: 18.1982 - val_MinusLogProbMetric: 18.1982 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 491/1000
2023-09-26 23:39:51.376 
Epoch 491/1000 
	 loss: 17.6938, MinusLogProbMetric: 17.6938, val_loss: 17.8166, val_MinusLogProbMetric: 17.8166

Epoch 491: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.6938 - MinusLogProbMetric: 17.6938 - val_loss: 17.8166 - val_MinusLogProbMetric: 17.8166 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 492/1000
2023-09-26 23:41:05.249 
Epoch 492/1000 
	 loss: 17.6481, MinusLogProbMetric: 17.6481, val_loss: 17.8469, val_MinusLogProbMetric: 17.8469

Epoch 492: val_loss did not improve from 17.55178
196/196 - 74s - loss: 17.6481 - MinusLogProbMetric: 17.6481 - val_loss: 17.8469 - val_MinusLogProbMetric: 17.8469 - lr: 3.3333e-04 - 74s/epoch - 377ms/step
Epoch 493/1000
2023-09-26 23:42:18.420 
Epoch 493/1000 
	 loss: 17.8730, MinusLogProbMetric: 17.8730, val_loss: 17.9273, val_MinusLogProbMetric: 17.9273

Epoch 493: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.8730 - MinusLogProbMetric: 17.8730 - val_loss: 17.9273 - val_MinusLogProbMetric: 17.9273 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 494/1000
2023-09-26 23:43:31.255 
Epoch 494/1000 
	 loss: 17.6358, MinusLogProbMetric: 17.6358, val_loss: 17.8094, val_MinusLogProbMetric: 17.8094

Epoch 494: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.6358 - MinusLogProbMetric: 17.6358 - val_loss: 17.8094 - val_MinusLogProbMetric: 17.8094 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 495/1000
2023-09-26 23:44:44.466 
Epoch 495/1000 
	 loss: 17.7783, MinusLogProbMetric: 17.7783, val_loss: 17.6142, val_MinusLogProbMetric: 17.6142

Epoch 495: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7783 - MinusLogProbMetric: 17.7783 - val_loss: 17.6142 - val_MinusLogProbMetric: 17.6142 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 496/1000
2023-09-26 23:45:57.521 
Epoch 496/1000 
	 loss: 17.6703, MinusLogProbMetric: 17.6703, val_loss: 17.7419, val_MinusLogProbMetric: 17.7419

Epoch 496: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.6703 - MinusLogProbMetric: 17.6703 - val_loss: 17.7419 - val_MinusLogProbMetric: 17.7419 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 497/1000
2023-09-26 23:47:10.359 
Epoch 497/1000 
	 loss: 17.7196, MinusLogProbMetric: 17.7196, val_loss: 17.8271, val_MinusLogProbMetric: 17.8271

Epoch 497: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7196 - MinusLogProbMetric: 17.7196 - val_loss: 17.8271 - val_MinusLogProbMetric: 17.8271 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 498/1000
2023-09-26 23:48:23.196 
Epoch 498/1000 
	 loss: 17.7000, MinusLogProbMetric: 17.7000, val_loss: 18.0131, val_MinusLogProbMetric: 18.0131

Epoch 498: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7000 - MinusLogProbMetric: 17.7000 - val_loss: 18.0131 - val_MinusLogProbMetric: 18.0131 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 499/1000
2023-09-26 23:49:35.734 
Epoch 499/1000 
	 loss: 17.7493, MinusLogProbMetric: 17.7493, val_loss: 17.8950, val_MinusLogProbMetric: 17.8950

Epoch 499: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7493 - MinusLogProbMetric: 17.7493 - val_loss: 17.8950 - val_MinusLogProbMetric: 17.8950 - lr: 3.3333e-04 - 73s/epoch - 370ms/step
Epoch 500/1000
2023-09-26 23:50:48.908 
Epoch 500/1000 
	 loss: 17.6860, MinusLogProbMetric: 17.6860, val_loss: 17.9279, val_MinusLogProbMetric: 17.9279

Epoch 500: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.6860 - MinusLogProbMetric: 17.6860 - val_loss: 17.9279 - val_MinusLogProbMetric: 17.9279 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 501/1000
2023-09-26 23:52:01.699 
Epoch 501/1000 
	 loss: 17.7035, MinusLogProbMetric: 17.7035, val_loss: 17.7105, val_MinusLogProbMetric: 17.7105

Epoch 501: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7035 - MinusLogProbMetric: 17.7035 - val_loss: 17.7105 - val_MinusLogProbMetric: 17.7105 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 502/1000
2023-09-26 23:53:14.377 
Epoch 502/1000 
	 loss: 17.6728, MinusLogProbMetric: 17.6728, val_loss: 18.0753, val_MinusLogProbMetric: 18.0753

Epoch 502: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.6728 - MinusLogProbMetric: 17.6728 - val_loss: 18.0753 - val_MinusLogProbMetric: 18.0753 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 503/1000
2023-09-26 23:54:26.901 
Epoch 503/1000 
	 loss: 17.7064, MinusLogProbMetric: 17.7064, val_loss: 17.6600, val_MinusLogProbMetric: 17.6600

Epoch 503: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7064 - MinusLogProbMetric: 17.7064 - val_loss: 17.6600 - val_MinusLogProbMetric: 17.6600 - lr: 3.3333e-04 - 73s/epoch - 370ms/step
Epoch 504/1000
2023-09-26 23:55:40.312 
Epoch 504/1000 
	 loss: 17.7214, MinusLogProbMetric: 17.7214, val_loss: 17.8714, val_MinusLogProbMetric: 17.8714

Epoch 504: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7214 - MinusLogProbMetric: 17.7214 - val_loss: 17.8714 - val_MinusLogProbMetric: 17.8714 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 505/1000
2023-09-26 23:56:53.641 
Epoch 505/1000 
	 loss: 17.6697, MinusLogProbMetric: 17.6697, val_loss: 18.3786, val_MinusLogProbMetric: 18.3786

Epoch 505: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.6697 - MinusLogProbMetric: 17.6697 - val_loss: 18.3786 - val_MinusLogProbMetric: 18.3786 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 506/1000
2023-09-26 23:58:06.748 
Epoch 506/1000 
	 loss: 17.7547, MinusLogProbMetric: 17.7547, val_loss: 17.6677, val_MinusLogProbMetric: 17.6677

Epoch 506: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7547 - MinusLogProbMetric: 17.7547 - val_loss: 17.6677 - val_MinusLogProbMetric: 17.6677 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 507/1000
2023-09-26 23:59:20.055 
Epoch 507/1000 
	 loss: 17.7141, MinusLogProbMetric: 17.7141, val_loss: 17.6997, val_MinusLogProbMetric: 17.6997

Epoch 507: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7141 - MinusLogProbMetric: 17.7141 - val_loss: 17.6997 - val_MinusLogProbMetric: 17.6997 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 508/1000
2023-09-27 00:00:32.920 
Epoch 508/1000 
	 loss: 17.7004, MinusLogProbMetric: 17.7004, val_loss: 17.6383, val_MinusLogProbMetric: 17.6383

Epoch 508: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7004 - MinusLogProbMetric: 17.7004 - val_loss: 17.6383 - val_MinusLogProbMetric: 17.6383 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 509/1000
2023-09-27 00:01:45.859 
Epoch 509/1000 
	 loss: 17.6265, MinusLogProbMetric: 17.6265, val_loss: 17.9944, val_MinusLogProbMetric: 17.9944

Epoch 509: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.6265 - MinusLogProbMetric: 17.6265 - val_loss: 17.9944 - val_MinusLogProbMetric: 17.9944 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 510/1000
2023-09-27 00:02:59.078 
Epoch 510/1000 
	 loss: 17.7069, MinusLogProbMetric: 17.7069, val_loss: 17.7386, val_MinusLogProbMetric: 17.7386

Epoch 510: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7069 - MinusLogProbMetric: 17.7069 - val_loss: 17.7386 - val_MinusLogProbMetric: 17.7386 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 511/1000
2023-09-27 00:04:12.092 
Epoch 511/1000 
	 loss: 17.6816, MinusLogProbMetric: 17.6816, val_loss: 18.5190, val_MinusLogProbMetric: 18.5190

Epoch 511: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.6816 - MinusLogProbMetric: 17.6816 - val_loss: 18.5190 - val_MinusLogProbMetric: 18.5190 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 512/1000
2023-09-27 00:05:25.307 
Epoch 512/1000 
	 loss: 17.6477, MinusLogProbMetric: 17.6477, val_loss: 17.5740, val_MinusLogProbMetric: 17.5740

Epoch 512: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.6477 - MinusLogProbMetric: 17.6477 - val_loss: 17.5740 - val_MinusLogProbMetric: 17.5740 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 513/1000
2023-09-27 00:06:39.219 
Epoch 513/1000 
	 loss: 17.8061, MinusLogProbMetric: 17.8061, val_loss: 18.0269, val_MinusLogProbMetric: 18.0269

Epoch 513: val_loss did not improve from 17.55178
196/196 - 74s - loss: 17.8061 - MinusLogProbMetric: 17.8061 - val_loss: 18.0269 - val_MinusLogProbMetric: 18.0269 - lr: 3.3333e-04 - 74s/epoch - 377ms/step
Epoch 514/1000
2023-09-27 00:07:51.923 
Epoch 514/1000 
	 loss: 17.7373, MinusLogProbMetric: 17.7373, val_loss: 17.7645, val_MinusLogProbMetric: 17.7645

Epoch 514: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7373 - MinusLogProbMetric: 17.7373 - val_loss: 17.7645 - val_MinusLogProbMetric: 17.7645 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 515/1000
2023-09-27 00:09:05.546 
Epoch 515/1000 
	 loss: 17.6464, MinusLogProbMetric: 17.6464, val_loss: 18.2831, val_MinusLogProbMetric: 18.2831

Epoch 515: val_loss did not improve from 17.55178
196/196 - 74s - loss: 17.6464 - MinusLogProbMetric: 17.6464 - val_loss: 18.2831 - val_MinusLogProbMetric: 18.2831 - lr: 3.3333e-04 - 74s/epoch - 376ms/step
Epoch 516/1000
2023-09-27 00:10:18.398 
Epoch 516/1000 
	 loss: 17.7323, MinusLogProbMetric: 17.7323, val_loss: 17.7064, val_MinusLogProbMetric: 17.7064

Epoch 516: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7323 - MinusLogProbMetric: 17.7323 - val_loss: 17.7064 - val_MinusLogProbMetric: 17.7064 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 517/1000
2023-09-27 00:11:31.852 
Epoch 517/1000 
	 loss: 17.7067, MinusLogProbMetric: 17.7067, val_loss: 17.8596, val_MinusLogProbMetric: 17.8596

Epoch 517: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7067 - MinusLogProbMetric: 17.7067 - val_loss: 17.8596 - val_MinusLogProbMetric: 17.8596 - lr: 3.3333e-04 - 73s/epoch - 375ms/step
Epoch 518/1000
2023-09-27 00:12:45.039 
Epoch 518/1000 
	 loss: 17.6970, MinusLogProbMetric: 17.6970, val_loss: 17.9424, val_MinusLogProbMetric: 17.9424

Epoch 518: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.6970 - MinusLogProbMetric: 17.6970 - val_loss: 17.9424 - val_MinusLogProbMetric: 17.9424 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 519/1000
2023-09-27 00:13:58.307 
Epoch 519/1000 
	 loss: 17.7282, MinusLogProbMetric: 17.7282, val_loss: 17.6443, val_MinusLogProbMetric: 17.6443

Epoch 519: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7282 - MinusLogProbMetric: 17.7282 - val_loss: 17.6443 - val_MinusLogProbMetric: 17.6443 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 520/1000
2023-09-27 00:15:11.701 
Epoch 520/1000 
	 loss: 17.6096, MinusLogProbMetric: 17.6096, val_loss: 17.7397, val_MinusLogProbMetric: 17.7397

Epoch 520: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.6096 - MinusLogProbMetric: 17.6096 - val_loss: 17.7397 - val_MinusLogProbMetric: 17.7397 - lr: 3.3333e-04 - 73s/epoch - 374ms/step
Epoch 521/1000
2023-09-27 00:16:24.828 
Epoch 521/1000 
	 loss: 17.7617, MinusLogProbMetric: 17.7617, val_loss: 17.9707, val_MinusLogProbMetric: 17.9707

Epoch 521: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.7617 - MinusLogProbMetric: 17.7617 - val_loss: 17.9707 - val_MinusLogProbMetric: 17.9707 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 522/1000
2023-09-27 00:17:38.009 
Epoch 522/1000 
	 loss: 17.6675, MinusLogProbMetric: 17.6675, val_loss: 17.6064, val_MinusLogProbMetric: 17.6064

Epoch 522: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.6675 - MinusLogProbMetric: 17.6675 - val_loss: 17.6064 - val_MinusLogProbMetric: 17.6064 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 523/1000
2023-09-27 00:18:50.695 
Epoch 523/1000 
	 loss: 17.6866, MinusLogProbMetric: 17.6866, val_loss: 17.6083, val_MinusLogProbMetric: 17.6083

Epoch 523: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.6866 - MinusLogProbMetric: 17.6866 - val_loss: 17.6083 - val_MinusLogProbMetric: 17.6083 - lr: 3.3333e-04 - 73s/epoch - 371ms/step
Epoch 524/1000
2023-09-27 00:20:03.554 
Epoch 524/1000 
	 loss: 17.6610, MinusLogProbMetric: 17.6610, val_loss: 17.9778, val_MinusLogProbMetric: 17.9778

Epoch 524: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.6610 - MinusLogProbMetric: 17.6610 - val_loss: 17.9778 - val_MinusLogProbMetric: 17.9778 - lr: 3.3333e-04 - 73s/epoch - 372ms/step
Epoch 525/1000
2023-09-27 00:21:16.586 
Epoch 525/1000 
	 loss: 17.6737, MinusLogProbMetric: 17.6737, val_loss: 17.9162, val_MinusLogProbMetric: 17.9162

Epoch 525: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.6737 - MinusLogProbMetric: 17.6737 - val_loss: 17.9162 - val_MinusLogProbMetric: 17.9162 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 526/1000
2023-09-27 00:22:29.768 
Epoch 526/1000 
	 loss: 17.6811, MinusLogProbMetric: 17.6811, val_loss: 17.6983, val_MinusLogProbMetric: 17.6983

Epoch 526: val_loss did not improve from 17.55178
196/196 - 73s - loss: 17.6811 - MinusLogProbMetric: 17.6811 - val_loss: 17.6983 - val_MinusLogProbMetric: 17.6983 - lr: 3.3333e-04 - 73s/epoch - 373ms/step
Epoch 527/1000
2023-09-27 00:23:41.000 
Epoch 527/1000 
	 loss: 17.6072, MinusLogProbMetric: 17.6072, val_loss: 17.6410, val_MinusLogProbMetric: 17.6410

Epoch 527: val_loss did not improve from 17.55178
196/196 - 71s - loss: 17.6072 - MinusLogProbMetric: 17.6072 - val_loss: 17.6410 - val_MinusLogProbMetric: 17.6410 - lr: 3.3333e-04 - 71s/epoch - 363ms/step
Epoch 528/1000
2023-09-27 00:24:49.900 
Epoch 528/1000 
	 loss: 17.6614, MinusLogProbMetric: 17.6614, val_loss: 17.7509, val_MinusLogProbMetric: 17.7509

Epoch 528: val_loss did not improve from 17.55178
196/196 - 69s - loss: 17.6614 - MinusLogProbMetric: 17.6614 - val_loss: 17.7509 - val_MinusLogProbMetric: 17.7509 - lr: 3.3333e-04 - 69s/epoch - 352ms/step
Epoch 529/1000
2023-09-27 00:25:57.622 
Epoch 529/1000 
	 loss: 17.6429, MinusLogProbMetric: 17.6429, val_loss: 17.8905, val_MinusLogProbMetric: 17.8905

Epoch 529: val_loss did not improve from 17.55178
196/196 - 68s - loss: 17.6429 - MinusLogProbMetric: 17.6429 - val_loss: 17.8905 - val_MinusLogProbMetric: 17.8905 - lr: 3.3333e-04 - 68s/epoch - 346ms/step
Epoch 530/1000
2023-09-27 00:27:11.898 
Epoch 530/1000 
	 loss: 17.6436, MinusLogProbMetric: 17.6436, val_loss: 17.9055, val_MinusLogProbMetric: 17.9055

Epoch 530: val_loss did not improve from 17.55178
196/196 - 74s - loss: 17.6436 - MinusLogProbMetric: 17.6436 - val_loss: 17.9055 - val_MinusLogProbMetric: 17.9055 - lr: 3.3333e-04 - 74s/epoch - 379ms/step
Epoch 531/1000
2023-09-27 00:28:26.602 
Epoch 531/1000 
	 loss: 17.6907, MinusLogProbMetric: 17.6907, val_loss: 17.6933, val_MinusLogProbMetric: 17.6933

Epoch 531: val_loss did not improve from 17.55178
196/196 - 75s - loss: 17.6907 - MinusLogProbMetric: 17.6907 - val_loss: 17.6933 - val_MinusLogProbMetric: 17.6933 - lr: 3.3333e-04 - 75s/epoch - 381ms/step
Epoch 532/1000
2023-09-27 00:29:41.145 
Epoch 532/1000 
	 loss: 17.3525, MinusLogProbMetric: 17.3525, val_loss: 17.3936, val_MinusLogProbMetric: 17.3936

Epoch 532: val_loss improved from 17.55178 to 17.39365, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 76s - loss: 17.3525 - MinusLogProbMetric: 17.3525 - val_loss: 17.3936 - val_MinusLogProbMetric: 17.3936 - lr: 1.6667e-04 - 76s/epoch - 387ms/step
Epoch 533/1000
2023-09-27 00:30:57.010 
Epoch 533/1000 
	 loss: 17.3369, MinusLogProbMetric: 17.3369, val_loss: 17.4243, val_MinusLogProbMetric: 17.4243

Epoch 533: val_loss did not improve from 17.39365
196/196 - 75s - loss: 17.3369 - MinusLogProbMetric: 17.3369 - val_loss: 17.4243 - val_MinusLogProbMetric: 17.4243 - lr: 1.6667e-04 - 75s/epoch - 381ms/step
Epoch 534/1000
2023-09-27 00:32:11.515 
Epoch 534/1000 
	 loss: 17.3361, MinusLogProbMetric: 17.3361, val_loss: 17.3854, val_MinusLogProbMetric: 17.3854

Epoch 534: val_loss improved from 17.39365 to 17.38536, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 76s - loss: 17.3361 - MinusLogProbMetric: 17.3361 - val_loss: 17.3854 - val_MinusLogProbMetric: 17.3854 - lr: 1.6667e-04 - 76s/epoch - 387ms/step
Epoch 535/1000
2023-09-27 00:33:26.975 
Epoch 535/1000 
	 loss: 17.3454, MinusLogProbMetric: 17.3454, val_loss: 17.4083, val_MinusLogProbMetric: 17.4083

Epoch 535: val_loss did not improve from 17.38536
196/196 - 74s - loss: 17.3454 - MinusLogProbMetric: 17.3454 - val_loss: 17.4083 - val_MinusLogProbMetric: 17.4083 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 536/1000
2023-09-27 00:34:41.904 
Epoch 536/1000 
	 loss: 17.3371, MinusLogProbMetric: 17.3371, val_loss: 17.3618, val_MinusLogProbMetric: 17.3618

Epoch 536: val_loss improved from 17.38536 to 17.36178, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 76s - loss: 17.3371 - MinusLogProbMetric: 17.3371 - val_loss: 17.3618 - val_MinusLogProbMetric: 17.3618 - lr: 1.6667e-04 - 76s/epoch - 388ms/step
Epoch 537/1000
2023-09-27 00:35:59.886 
Epoch 537/1000 
	 loss: 17.3255, MinusLogProbMetric: 17.3255, val_loss: 17.3630, val_MinusLogProbMetric: 17.3630

Epoch 537: val_loss did not improve from 17.36178
196/196 - 77s - loss: 17.3255 - MinusLogProbMetric: 17.3255 - val_loss: 17.3630 - val_MinusLogProbMetric: 17.3630 - lr: 1.6667e-04 - 77s/epoch - 392ms/step
Epoch 538/1000
2023-09-27 00:37:15.244 
Epoch 538/1000 
	 loss: 17.3175, MinusLogProbMetric: 17.3175, val_loss: 17.4046, val_MinusLogProbMetric: 17.4046

Epoch 538: val_loss did not improve from 17.36178
196/196 - 75s - loss: 17.3175 - MinusLogProbMetric: 17.3175 - val_loss: 17.4046 - val_MinusLogProbMetric: 17.4046 - lr: 1.6667e-04 - 75s/epoch - 384ms/step
Epoch 539/1000
2023-09-27 00:38:30.241 
Epoch 539/1000 
	 loss: 17.3371, MinusLogProbMetric: 17.3371, val_loss: 17.4844, val_MinusLogProbMetric: 17.4844

Epoch 539: val_loss did not improve from 17.36178
196/196 - 75s - loss: 17.3371 - MinusLogProbMetric: 17.3371 - val_loss: 17.4844 - val_MinusLogProbMetric: 17.4844 - lr: 1.6667e-04 - 75s/epoch - 383ms/step
Epoch 540/1000
2023-09-27 00:39:44.872 
Epoch 540/1000 
	 loss: 17.3623, MinusLogProbMetric: 17.3623, val_loss: 17.3393, val_MinusLogProbMetric: 17.3393

Epoch 540: val_loss improved from 17.36178 to 17.33928, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 76s - loss: 17.3623 - MinusLogProbMetric: 17.3623 - val_loss: 17.3393 - val_MinusLogProbMetric: 17.3393 - lr: 1.6667e-04 - 76s/epoch - 388ms/step
Epoch 541/1000
2023-09-27 00:41:01.338 
Epoch 541/1000 
	 loss: 17.3349, MinusLogProbMetric: 17.3349, val_loss: 17.3787, val_MinusLogProbMetric: 17.3787

Epoch 541: val_loss did not improve from 17.33928
196/196 - 75s - loss: 17.3349 - MinusLogProbMetric: 17.3349 - val_loss: 17.3787 - val_MinusLogProbMetric: 17.3787 - lr: 1.6667e-04 - 75s/epoch - 383ms/step
Epoch 542/1000
2023-09-27 00:42:15.535 
Epoch 542/1000 
	 loss: 17.3228, MinusLogProbMetric: 17.3228, val_loss: 17.3378, val_MinusLogProbMetric: 17.3378

Epoch 542: val_loss improved from 17.33928 to 17.33783, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 76s - loss: 17.3228 - MinusLogProbMetric: 17.3228 - val_loss: 17.3378 - val_MinusLogProbMetric: 17.3378 - lr: 1.6667e-04 - 76s/epoch - 388ms/step
Epoch 543/1000
2023-09-27 00:43:32.480 
Epoch 543/1000 
	 loss: 17.3545, MinusLogProbMetric: 17.3545, val_loss: 17.3726, val_MinusLogProbMetric: 17.3726

Epoch 543: val_loss did not improve from 17.33783
196/196 - 75s - loss: 17.3545 - MinusLogProbMetric: 17.3545 - val_loss: 17.3726 - val_MinusLogProbMetric: 17.3726 - lr: 1.6667e-04 - 75s/epoch - 383ms/step
Epoch 544/1000
2023-09-27 00:44:47.183 
Epoch 544/1000 
	 loss: 17.3307, MinusLogProbMetric: 17.3307, val_loss: 17.4338, val_MinusLogProbMetric: 17.4338

Epoch 544: val_loss did not improve from 17.33783
196/196 - 75s - loss: 17.3307 - MinusLogProbMetric: 17.3307 - val_loss: 17.4338 - val_MinusLogProbMetric: 17.4338 - lr: 1.6667e-04 - 75s/epoch - 381ms/step
Epoch 545/1000
2023-09-27 00:46:02.286 
Epoch 545/1000 
	 loss: 17.3323, MinusLogProbMetric: 17.3323, val_loss: 17.3916, val_MinusLogProbMetric: 17.3916

Epoch 545: val_loss did not improve from 17.33783
196/196 - 75s - loss: 17.3323 - MinusLogProbMetric: 17.3323 - val_loss: 17.3916 - val_MinusLogProbMetric: 17.3916 - lr: 1.6667e-04 - 75s/epoch - 383ms/step
Epoch 546/1000
2023-09-27 00:47:16.896 
Epoch 546/1000 
	 loss: 17.3306, MinusLogProbMetric: 17.3306, val_loss: 17.3657, val_MinusLogProbMetric: 17.3657

Epoch 546: val_loss did not improve from 17.33783
196/196 - 75s - loss: 17.3306 - MinusLogProbMetric: 17.3306 - val_loss: 17.3657 - val_MinusLogProbMetric: 17.3657 - lr: 1.6667e-04 - 75s/epoch - 381ms/step
Epoch 547/1000
2023-09-27 00:48:31.262 
Epoch 547/1000 
	 loss: 17.3255, MinusLogProbMetric: 17.3255, val_loss: 17.3639, val_MinusLogProbMetric: 17.3639

Epoch 547: val_loss did not improve from 17.33783
196/196 - 74s - loss: 17.3255 - MinusLogProbMetric: 17.3255 - val_loss: 17.3639 - val_MinusLogProbMetric: 17.3639 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 548/1000
2023-09-27 00:49:46.377 
Epoch 548/1000 
	 loss: 17.3128, MinusLogProbMetric: 17.3128, val_loss: 17.3983, val_MinusLogProbMetric: 17.3983

Epoch 548: val_loss did not improve from 17.33783
196/196 - 75s - loss: 17.3128 - MinusLogProbMetric: 17.3128 - val_loss: 17.3983 - val_MinusLogProbMetric: 17.3983 - lr: 1.6667e-04 - 75s/epoch - 383ms/step
Epoch 549/1000
2023-09-27 00:51:01.117 
Epoch 549/1000 
	 loss: 17.3113, MinusLogProbMetric: 17.3113, val_loss: 17.4477, val_MinusLogProbMetric: 17.4477

Epoch 549: val_loss did not improve from 17.33783
196/196 - 75s - loss: 17.3113 - MinusLogProbMetric: 17.3113 - val_loss: 17.4477 - val_MinusLogProbMetric: 17.4477 - lr: 1.6667e-04 - 75s/epoch - 381ms/step
Epoch 550/1000
2023-09-27 00:52:15.498 
Epoch 550/1000 
	 loss: 17.3098, MinusLogProbMetric: 17.3098, val_loss: 17.3366, val_MinusLogProbMetric: 17.3366

Epoch 550: val_loss improved from 17.33783 to 17.33661, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 76s - loss: 17.3098 - MinusLogProbMetric: 17.3098 - val_loss: 17.3366 - val_MinusLogProbMetric: 17.3366 - lr: 1.6667e-04 - 76s/epoch - 386ms/step
Epoch 551/1000
2023-09-27 00:53:31.820 
Epoch 551/1000 
	 loss: 17.3209, MinusLogProbMetric: 17.3209, val_loss: 17.4010, val_MinusLogProbMetric: 17.4010

Epoch 551: val_loss did not improve from 17.33661
196/196 - 75s - loss: 17.3209 - MinusLogProbMetric: 17.3209 - val_loss: 17.4010 - val_MinusLogProbMetric: 17.4010 - lr: 1.6667e-04 - 75s/epoch - 382ms/step
Epoch 552/1000
2023-09-27 00:54:46.140 
Epoch 552/1000 
	 loss: 17.3278, MinusLogProbMetric: 17.3278, val_loss: 17.3768, val_MinusLogProbMetric: 17.3768

Epoch 552: val_loss did not improve from 17.33661
196/196 - 74s - loss: 17.3278 - MinusLogProbMetric: 17.3278 - val_loss: 17.3768 - val_MinusLogProbMetric: 17.3768 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 553/1000
2023-09-27 00:56:00.829 
Epoch 553/1000 
	 loss: 17.3353, MinusLogProbMetric: 17.3353, val_loss: 17.4209, val_MinusLogProbMetric: 17.4209

Epoch 553: val_loss did not improve from 17.33661
196/196 - 75s - loss: 17.3353 - MinusLogProbMetric: 17.3353 - val_loss: 17.4209 - val_MinusLogProbMetric: 17.4209 - lr: 1.6667e-04 - 75s/epoch - 381ms/step
Epoch 554/1000
2023-09-27 00:57:14.961 
Epoch 554/1000 
	 loss: 17.3010, MinusLogProbMetric: 17.3010, val_loss: 17.4920, val_MinusLogProbMetric: 17.4920

Epoch 554: val_loss did not improve from 17.33661
196/196 - 74s - loss: 17.3010 - MinusLogProbMetric: 17.3010 - val_loss: 17.4920 - val_MinusLogProbMetric: 17.4920 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 555/1000
2023-09-27 00:58:28.866 
Epoch 555/1000 
	 loss: 17.3248, MinusLogProbMetric: 17.3248, val_loss: 17.4266, val_MinusLogProbMetric: 17.4266

Epoch 555: val_loss did not improve from 17.33661
196/196 - 74s - loss: 17.3248 - MinusLogProbMetric: 17.3248 - val_loss: 17.4266 - val_MinusLogProbMetric: 17.4266 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 556/1000
2023-09-27 00:59:42.824 
Epoch 556/1000 
	 loss: 17.3180, MinusLogProbMetric: 17.3180, val_loss: 17.3710, val_MinusLogProbMetric: 17.3710

Epoch 556: val_loss did not improve from 17.33661
196/196 - 74s - loss: 17.3180 - MinusLogProbMetric: 17.3180 - val_loss: 17.3710 - val_MinusLogProbMetric: 17.3710 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 557/1000
2023-09-27 01:00:56.560 
Epoch 557/1000 
	 loss: 17.2995, MinusLogProbMetric: 17.2995, val_loss: 17.4000, val_MinusLogProbMetric: 17.4000

Epoch 557: val_loss did not improve from 17.33661
196/196 - 74s - loss: 17.2995 - MinusLogProbMetric: 17.2995 - val_loss: 17.4000 - val_MinusLogProbMetric: 17.4000 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 558/1000
2023-09-27 01:02:10.189 
Epoch 558/1000 
	 loss: 17.3061, MinusLogProbMetric: 17.3061, val_loss: 17.4729, val_MinusLogProbMetric: 17.4729

Epoch 558: val_loss did not improve from 17.33661
196/196 - 74s - loss: 17.3061 - MinusLogProbMetric: 17.3061 - val_loss: 17.4729 - val_MinusLogProbMetric: 17.4729 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 559/1000
2023-09-27 01:03:24.588 
Epoch 559/1000 
	 loss: 17.3159, MinusLogProbMetric: 17.3159, val_loss: 17.4035, val_MinusLogProbMetric: 17.4035

Epoch 559: val_loss did not improve from 17.33661
196/196 - 74s - loss: 17.3159 - MinusLogProbMetric: 17.3159 - val_loss: 17.4035 - val_MinusLogProbMetric: 17.4035 - lr: 1.6667e-04 - 74s/epoch - 380ms/step
Epoch 560/1000
2023-09-27 01:04:38.555 
Epoch 560/1000 
	 loss: 17.3221, MinusLogProbMetric: 17.3221, val_loss: 17.3886, val_MinusLogProbMetric: 17.3886

Epoch 560: val_loss did not improve from 17.33661
196/196 - 74s - loss: 17.3221 - MinusLogProbMetric: 17.3221 - val_loss: 17.3886 - val_MinusLogProbMetric: 17.3886 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 561/1000
2023-09-27 01:05:53.237 
Epoch 561/1000 
	 loss: 17.3023, MinusLogProbMetric: 17.3023, val_loss: 17.4763, val_MinusLogProbMetric: 17.4763

Epoch 561: val_loss did not improve from 17.33661
196/196 - 75s - loss: 17.3023 - MinusLogProbMetric: 17.3023 - val_loss: 17.4763 - val_MinusLogProbMetric: 17.4763 - lr: 1.6667e-04 - 75s/epoch - 381ms/step
Epoch 562/1000
2023-09-27 01:07:07.485 
Epoch 562/1000 
	 loss: 17.3100, MinusLogProbMetric: 17.3100, val_loss: 17.4734, val_MinusLogProbMetric: 17.4734

Epoch 562: val_loss did not improve from 17.33661
196/196 - 74s - loss: 17.3100 - MinusLogProbMetric: 17.3100 - val_loss: 17.4734 - val_MinusLogProbMetric: 17.4734 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 563/1000
2023-09-27 01:08:21.718 
Epoch 563/1000 
	 loss: 17.3252, MinusLogProbMetric: 17.3252, val_loss: 17.3055, val_MinusLogProbMetric: 17.3055

Epoch 563: val_loss improved from 17.33661 to 17.30546, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 76s - loss: 17.3252 - MinusLogProbMetric: 17.3252 - val_loss: 17.3055 - val_MinusLogProbMetric: 17.3055 - lr: 1.6667e-04 - 76s/epoch - 387ms/step
Epoch 564/1000
2023-09-27 01:09:37.188 
Epoch 564/1000 
	 loss: 17.3208, MinusLogProbMetric: 17.3208, val_loss: 17.3875, val_MinusLogProbMetric: 17.3875

Epoch 564: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.3208 - MinusLogProbMetric: 17.3208 - val_loss: 17.3875 - val_MinusLogProbMetric: 17.3875 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 565/1000
2023-09-27 01:10:50.938 
Epoch 565/1000 
	 loss: 17.3307, MinusLogProbMetric: 17.3307, val_loss: 17.3966, val_MinusLogProbMetric: 17.3966

Epoch 565: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.3307 - MinusLogProbMetric: 17.3307 - val_loss: 17.3966 - val_MinusLogProbMetric: 17.3966 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 566/1000
2023-09-27 01:12:05.650 
Epoch 566/1000 
	 loss: 17.3122, MinusLogProbMetric: 17.3122, val_loss: 17.4270, val_MinusLogProbMetric: 17.4270

Epoch 566: val_loss did not improve from 17.30546
196/196 - 75s - loss: 17.3122 - MinusLogProbMetric: 17.3122 - val_loss: 17.4270 - val_MinusLogProbMetric: 17.4270 - lr: 1.6667e-04 - 75s/epoch - 381ms/step
Epoch 567/1000
2023-09-27 01:13:19.932 
Epoch 567/1000 
	 loss: 17.3159, MinusLogProbMetric: 17.3159, val_loss: 17.4247, val_MinusLogProbMetric: 17.4247

Epoch 567: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.3159 - MinusLogProbMetric: 17.3159 - val_loss: 17.4247 - val_MinusLogProbMetric: 17.4247 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 568/1000
2023-09-27 01:14:33.793 
Epoch 568/1000 
	 loss: 17.3051, MinusLogProbMetric: 17.3051, val_loss: 17.4742, val_MinusLogProbMetric: 17.4742

Epoch 568: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.3051 - MinusLogProbMetric: 17.3051 - val_loss: 17.4742 - val_MinusLogProbMetric: 17.4742 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 569/1000
2023-09-27 01:15:48.008 
Epoch 569/1000 
	 loss: 17.3069, MinusLogProbMetric: 17.3069, val_loss: 17.3152, val_MinusLogProbMetric: 17.3152

Epoch 569: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.3069 - MinusLogProbMetric: 17.3069 - val_loss: 17.3152 - val_MinusLogProbMetric: 17.3152 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 570/1000
2023-09-27 01:17:02.290 
Epoch 570/1000 
	 loss: 17.3178, MinusLogProbMetric: 17.3178, val_loss: 17.4127, val_MinusLogProbMetric: 17.4127

Epoch 570: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.3178 - MinusLogProbMetric: 17.3178 - val_loss: 17.4127 - val_MinusLogProbMetric: 17.4127 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 571/1000
2023-09-27 01:18:15.865 
Epoch 571/1000 
	 loss: 17.3130, MinusLogProbMetric: 17.3130, val_loss: 17.3722, val_MinusLogProbMetric: 17.3722

Epoch 571: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.3130 - MinusLogProbMetric: 17.3130 - val_loss: 17.3722 - val_MinusLogProbMetric: 17.3722 - lr: 1.6667e-04 - 74s/epoch - 375ms/step
Epoch 572/1000
2023-09-27 01:19:30.112 
Epoch 572/1000 
	 loss: 17.3062, MinusLogProbMetric: 17.3062, val_loss: 17.3848, val_MinusLogProbMetric: 17.3848

Epoch 572: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.3062 - MinusLogProbMetric: 17.3062 - val_loss: 17.3848 - val_MinusLogProbMetric: 17.3848 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 573/1000
2023-09-27 01:20:44.420 
Epoch 573/1000 
	 loss: 17.2906, MinusLogProbMetric: 17.2906, val_loss: 17.3744, val_MinusLogProbMetric: 17.3744

Epoch 573: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.2906 - MinusLogProbMetric: 17.2906 - val_loss: 17.3744 - val_MinusLogProbMetric: 17.3744 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 574/1000
2023-09-27 01:21:58.685 
Epoch 574/1000 
	 loss: 17.3020, MinusLogProbMetric: 17.3020, val_loss: 17.3569, val_MinusLogProbMetric: 17.3569

Epoch 574: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.3020 - MinusLogProbMetric: 17.3020 - val_loss: 17.3569 - val_MinusLogProbMetric: 17.3569 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 575/1000
2023-09-27 01:23:12.884 
Epoch 575/1000 
	 loss: 17.3127, MinusLogProbMetric: 17.3127, val_loss: 17.3788, val_MinusLogProbMetric: 17.3788

Epoch 575: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.3127 - MinusLogProbMetric: 17.3127 - val_loss: 17.3788 - val_MinusLogProbMetric: 17.3788 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 576/1000
2023-09-27 01:24:27.437 
Epoch 576/1000 
	 loss: 17.3075, MinusLogProbMetric: 17.3075, val_loss: 17.3837, val_MinusLogProbMetric: 17.3837

Epoch 576: val_loss did not improve from 17.30546
196/196 - 75s - loss: 17.3075 - MinusLogProbMetric: 17.3075 - val_loss: 17.3837 - val_MinusLogProbMetric: 17.3837 - lr: 1.6667e-04 - 75s/epoch - 380ms/step
Epoch 577/1000
2023-09-27 01:25:40.866 
Epoch 577/1000 
	 loss: 17.3011, MinusLogProbMetric: 17.3011, val_loss: 17.3451, val_MinusLogProbMetric: 17.3451

Epoch 577: val_loss did not improve from 17.30546
196/196 - 73s - loss: 17.3011 - MinusLogProbMetric: 17.3011 - val_loss: 17.3451 - val_MinusLogProbMetric: 17.3451 - lr: 1.6667e-04 - 73s/epoch - 375ms/step
Epoch 578/1000
2023-09-27 01:26:54.986 
Epoch 578/1000 
	 loss: 17.2893, MinusLogProbMetric: 17.2893, val_loss: 17.3232, val_MinusLogProbMetric: 17.3232

Epoch 578: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.2893 - MinusLogProbMetric: 17.2893 - val_loss: 17.3232 - val_MinusLogProbMetric: 17.3232 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 579/1000
2023-09-27 01:28:09.047 
Epoch 579/1000 
	 loss: 17.3059, MinusLogProbMetric: 17.3059, val_loss: 17.4311, val_MinusLogProbMetric: 17.4311

Epoch 579: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.3059 - MinusLogProbMetric: 17.3059 - val_loss: 17.4311 - val_MinusLogProbMetric: 17.4311 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 580/1000
2023-09-27 01:29:23.213 
Epoch 580/1000 
	 loss: 17.3009, MinusLogProbMetric: 17.3009, val_loss: 17.3159, val_MinusLogProbMetric: 17.3159

Epoch 580: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.3009 - MinusLogProbMetric: 17.3009 - val_loss: 17.3159 - val_MinusLogProbMetric: 17.3159 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 581/1000
2023-09-27 01:30:37.341 
Epoch 581/1000 
	 loss: 17.3022, MinusLogProbMetric: 17.3022, val_loss: 17.3689, val_MinusLogProbMetric: 17.3689

Epoch 581: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.3022 - MinusLogProbMetric: 17.3022 - val_loss: 17.3689 - val_MinusLogProbMetric: 17.3689 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 582/1000
2023-09-27 01:31:51.331 
Epoch 582/1000 
	 loss: 17.2812, MinusLogProbMetric: 17.2812, val_loss: 17.3763, val_MinusLogProbMetric: 17.3763

Epoch 582: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.2812 - MinusLogProbMetric: 17.2812 - val_loss: 17.3763 - val_MinusLogProbMetric: 17.3763 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 583/1000
2023-09-27 01:33:06.095 
Epoch 583/1000 
	 loss: 17.3178, MinusLogProbMetric: 17.3178, val_loss: 17.3840, val_MinusLogProbMetric: 17.3840

Epoch 583: val_loss did not improve from 17.30546
196/196 - 75s - loss: 17.3178 - MinusLogProbMetric: 17.3178 - val_loss: 17.3840 - val_MinusLogProbMetric: 17.3840 - lr: 1.6667e-04 - 75s/epoch - 381ms/step
Epoch 584/1000
2023-09-27 01:34:20.700 
Epoch 584/1000 
	 loss: 17.2950, MinusLogProbMetric: 17.2950, val_loss: 17.3729, val_MinusLogProbMetric: 17.3729

Epoch 584: val_loss did not improve from 17.30546
196/196 - 75s - loss: 17.2950 - MinusLogProbMetric: 17.2950 - val_loss: 17.3729 - val_MinusLogProbMetric: 17.3729 - lr: 1.6667e-04 - 75s/epoch - 381ms/step
Epoch 585/1000
2023-09-27 01:35:34.882 
Epoch 585/1000 
	 loss: 17.2991, MinusLogProbMetric: 17.2991, val_loss: 17.3769, val_MinusLogProbMetric: 17.3769

Epoch 585: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.2991 - MinusLogProbMetric: 17.2991 - val_loss: 17.3769 - val_MinusLogProbMetric: 17.3769 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 586/1000
2023-09-27 01:36:49.409 
Epoch 586/1000 
	 loss: 17.3047, MinusLogProbMetric: 17.3047, val_loss: 17.4461, val_MinusLogProbMetric: 17.4461

Epoch 586: val_loss did not improve from 17.30546
196/196 - 75s - loss: 17.3047 - MinusLogProbMetric: 17.3047 - val_loss: 17.4461 - val_MinusLogProbMetric: 17.4461 - lr: 1.6667e-04 - 75s/epoch - 380ms/step
Epoch 587/1000
2023-09-27 01:38:03.515 
Epoch 587/1000 
	 loss: 17.3212, MinusLogProbMetric: 17.3212, val_loss: 17.3842, val_MinusLogProbMetric: 17.3842

Epoch 587: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.3212 - MinusLogProbMetric: 17.3212 - val_loss: 17.3842 - val_MinusLogProbMetric: 17.3842 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 588/1000
2023-09-27 01:39:16.800 
Epoch 588/1000 
	 loss: 17.3085, MinusLogProbMetric: 17.3085, val_loss: 17.3765, val_MinusLogProbMetric: 17.3765

Epoch 588: val_loss did not improve from 17.30546
196/196 - 73s - loss: 17.3085 - MinusLogProbMetric: 17.3085 - val_loss: 17.3765 - val_MinusLogProbMetric: 17.3765 - lr: 1.6667e-04 - 73s/epoch - 374ms/step
Epoch 589/1000
2023-09-27 01:40:31.376 
Epoch 589/1000 
	 loss: 17.3000, MinusLogProbMetric: 17.3000, val_loss: 17.4196, val_MinusLogProbMetric: 17.4196

Epoch 589: val_loss did not improve from 17.30546
196/196 - 75s - loss: 17.3000 - MinusLogProbMetric: 17.3000 - val_loss: 17.4196 - val_MinusLogProbMetric: 17.4196 - lr: 1.6667e-04 - 75s/epoch - 380ms/step
Epoch 590/1000
2023-09-27 01:41:45.093 
Epoch 590/1000 
	 loss: 17.3154, MinusLogProbMetric: 17.3154, val_loss: 17.4280, val_MinusLogProbMetric: 17.4280

Epoch 590: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.3154 - MinusLogProbMetric: 17.3154 - val_loss: 17.4280 - val_MinusLogProbMetric: 17.4280 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 591/1000
2023-09-27 01:42:59.463 
Epoch 591/1000 
	 loss: 17.3093, MinusLogProbMetric: 17.3093, val_loss: 17.6553, val_MinusLogProbMetric: 17.6553

Epoch 591: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.3093 - MinusLogProbMetric: 17.3093 - val_loss: 17.6553 - val_MinusLogProbMetric: 17.6553 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 592/1000
2023-09-27 01:44:13.429 
Epoch 592/1000 
	 loss: 17.2984, MinusLogProbMetric: 17.2984, val_loss: 17.3284, val_MinusLogProbMetric: 17.3284

Epoch 592: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.2984 - MinusLogProbMetric: 17.2984 - val_loss: 17.3284 - val_MinusLogProbMetric: 17.3284 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 593/1000
2023-09-27 01:45:27.567 
Epoch 593/1000 
	 loss: 17.2918, MinusLogProbMetric: 17.2918, val_loss: 17.3113, val_MinusLogProbMetric: 17.3113

Epoch 593: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.2918 - MinusLogProbMetric: 17.2918 - val_loss: 17.3113 - val_MinusLogProbMetric: 17.3113 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 594/1000
2023-09-27 01:46:41.519 
Epoch 594/1000 
	 loss: 17.3328, MinusLogProbMetric: 17.3328, val_loss: 17.3478, val_MinusLogProbMetric: 17.3478

Epoch 594: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.3328 - MinusLogProbMetric: 17.3328 - val_loss: 17.3478 - val_MinusLogProbMetric: 17.3478 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 595/1000
2023-09-27 01:47:54.976 
Epoch 595/1000 
	 loss: 17.2891, MinusLogProbMetric: 17.2891, val_loss: 17.3210, val_MinusLogProbMetric: 17.3210

Epoch 595: val_loss did not improve from 17.30546
196/196 - 73s - loss: 17.2891 - MinusLogProbMetric: 17.2891 - val_loss: 17.3210 - val_MinusLogProbMetric: 17.3210 - lr: 1.6667e-04 - 73s/epoch - 375ms/step
Epoch 596/1000
2023-09-27 01:49:08.847 
Epoch 596/1000 
	 loss: 17.2781, MinusLogProbMetric: 17.2781, val_loss: 17.4154, val_MinusLogProbMetric: 17.4154

Epoch 596: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.2781 - MinusLogProbMetric: 17.2781 - val_loss: 17.4154 - val_MinusLogProbMetric: 17.4154 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 597/1000
2023-09-27 01:50:22.763 
Epoch 597/1000 
	 loss: 17.2932, MinusLogProbMetric: 17.2932, val_loss: 17.3458, val_MinusLogProbMetric: 17.3458

Epoch 597: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.2932 - MinusLogProbMetric: 17.2932 - val_loss: 17.3458 - val_MinusLogProbMetric: 17.3458 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 598/1000
2023-09-27 01:51:37.437 
Epoch 598/1000 
	 loss: 17.2759, MinusLogProbMetric: 17.2759, val_loss: 17.3967, val_MinusLogProbMetric: 17.3967

Epoch 598: val_loss did not improve from 17.30546
196/196 - 75s - loss: 17.2759 - MinusLogProbMetric: 17.2759 - val_loss: 17.3967 - val_MinusLogProbMetric: 17.3967 - lr: 1.6667e-04 - 75s/epoch - 381ms/step
Epoch 599/1000
2023-09-27 01:52:51.326 
Epoch 599/1000 
	 loss: 17.2795, MinusLogProbMetric: 17.2795, val_loss: 17.4617, val_MinusLogProbMetric: 17.4617

Epoch 599: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.2795 - MinusLogProbMetric: 17.2795 - val_loss: 17.4617 - val_MinusLogProbMetric: 17.4617 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 600/1000
2023-09-27 01:54:05.675 
Epoch 600/1000 
	 loss: 17.3042, MinusLogProbMetric: 17.3042, val_loss: 17.3997, val_MinusLogProbMetric: 17.3997

Epoch 600: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.3042 - MinusLogProbMetric: 17.3042 - val_loss: 17.3997 - val_MinusLogProbMetric: 17.3997 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 601/1000
2023-09-27 01:55:20.199 
Epoch 601/1000 
	 loss: 17.2659, MinusLogProbMetric: 17.2659, val_loss: 17.3323, val_MinusLogProbMetric: 17.3323

Epoch 601: val_loss did not improve from 17.30546
196/196 - 75s - loss: 17.2659 - MinusLogProbMetric: 17.2659 - val_loss: 17.3323 - val_MinusLogProbMetric: 17.3323 - lr: 1.6667e-04 - 75s/epoch - 380ms/step
Epoch 602/1000
2023-09-27 01:56:34.477 
Epoch 602/1000 
	 loss: 17.2633, MinusLogProbMetric: 17.2633, val_loss: 17.3429, val_MinusLogProbMetric: 17.3429

Epoch 602: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.2633 - MinusLogProbMetric: 17.2633 - val_loss: 17.3429 - val_MinusLogProbMetric: 17.3429 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 603/1000
2023-09-27 01:57:48.367 
Epoch 603/1000 
	 loss: 17.2824, MinusLogProbMetric: 17.2824, val_loss: 17.4539, val_MinusLogProbMetric: 17.4539

Epoch 603: val_loss did not improve from 17.30546
196/196 - 74s - loss: 17.2824 - MinusLogProbMetric: 17.2824 - val_loss: 17.4539 - val_MinusLogProbMetric: 17.4539 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 604/1000
2023-09-27 01:59:02.790 
Epoch 604/1000 
	 loss: 17.2813, MinusLogProbMetric: 17.2813, val_loss: 17.2958, val_MinusLogProbMetric: 17.2958

Epoch 604: val_loss improved from 17.30546 to 17.29580, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 76s - loss: 17.2813 - MinusLogProbMetric: 17.2813 - val_loss: 17.2958 - val_MinusLogProbMetric: 17.2958 - lr: 1.6667e-04 - 76s/epoch - 386ms/step
Epoch 605/1000
2023-09-27 02:00:18.772 
Epoch 605/1000 
	 loss: 17.2805, MinusLogProbMetric: 17.2805, val_loss: 17.3459, val_MinusLogProbMetric: 17.3459

Epoch 605: val_loss did not improve from 17.29580
196/196 - 75s - loss: 17.2805 - MinusLogProbMetric: 17.2805 - val_loss: 17.3459 - val_MinusLogProbMetric: 17.3459 - lr: 1.6667e-04 - 75s/epoch - 381ms/step
Epoch 606/1000
2023-09-27 02:01:32.960 
Epoch 606/1000 
	 loss: 17.2976, MinusLogProbMetric: 17.2976, val_loss: 17.3131, val_MinusLogProbMetric: 17.3131

Epoch 606: val_loss did not improve from 17.29580
196/196 - 74s - loss: 17.2976 - MinusLogProbMetric: 17.2976 - val_loss: 17.3131 - val_MinusLogProbMetric: 17.3131 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 607/1000
2023-09-27 02:02:46.768 
Epoch 607/1000 
	 loss: 17.2616, MinusLogProbMetric: 17.2616, val_loss: 17.3236, val_MinusLogProbMetric: 17.3236

Epoch 607: val_loss did not improve from 17.29580
196/196 - 74s - loss: 17.2616 - MinusLogProbMetric: 17.2616 - val_loss: 17.3236 - val_MinusLogProbMetric: 17.3236 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 608/1000
2023-09-27 02:04:01.083 
Epoch 608/1000 
	 loss: 17.2718, MinusLogProbMetric: 17.2718, val_loss: 17.5366, val_MinusLogProbMetric: 17.5366

Epoch 608: val_loss did not improve from 17.29580
196/196 - 74s - loss: 17.2718 - MinusLogProbMetric: 17.2718 - val_loss: 17.5366 - val_MinusLogProbMetric: 17.5366 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 609/1000
2023-09-27 02:05:15.360 
Epoch 609/1000 
	 loss: 17.2868, MinusLogProbMetric: 17.2868, val_loss: 17.4777, val_MinusLogProbMetric: 17.4777

Epoch 609: val_loss did not improve from 17.29580
196/196 - 74s - loss: 17.2868 - MinusLogProbMetric: 17.2868 - val_loss: 17.4777 - val_MinusLogProbMetric: 17.4777 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 610/1000
2023-09-27 02:06:29.050 
Epoch 610/1000 
	 loss: 17.3007, MinusLogProbMetric: 17.3007, val_loss: 17.3393, val_MinusLogProbMetric: 17.3393

Epoch 610: val_loss did not improve from 17.29580
196/196 - 74s - loss: 17.3007 - MinusLogProbMetric: 17.3007 - val_loss: 17.3393 - val_MinusLogProbMetric: 17.3393 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 611/1000
2023-09-27 02:07:42.835 
Epoch 611/1000 
	 loss: 17.2719, MinusLogProbMetric: 17.2719, val_loss: 17.3000, val_MinusLogProbMetric: 17.3000

Epoch 611: val_loss did not improve from 17.29580
196/196 - 74s - loss: 17.2719 - MinusLogProbMetric: 17.2719 - val_loss: 17.3000 - val_MinusLogProbMetric: 17.3000 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 612/1000
2023-09-27 02:08:57.080 
Epoch 612/1000 
	 loss: 17.2763, MinusLogProbMetric: 17.2763, val_loss: 17.3501, val_MinusLogProbMetric: 17.3501

Epoch 612: val_loss did not improve from 17.29580
196/196 - 74s - loss: 17.2763 - MinusLogProbMetric: 17.2763 - val_loss: 17.3501 - val_MinusLogProbMetric: 17.3501 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 613/1000
2023-09-27 02:10:11.473 
Epoch 613/1000 
	 loss: 17.3025, MinusLogProbMetric: 17.3025, val_loss: 17.4010, val_MinusLogProbMetric: 17.4010

Epoch 613: val_loss did not improve from 17.29580
196/196 - 74s - loss: 17.3025 - MinusLogProbMetric: 17.3025 - val_loss: 17.4010 - val_MinusLogProbMetric: 17.4010 - lr: 1.6667e-04 - 74s/epoch - 380ms/step
Epoch 614/1000
2023-09-27 02:11:26.265 
Epoch 614/1000 
	 loss: 17.2923, MinusLogProbMetric: 17.2923, val_loss: 17.3541, val_MinusLogProbMetric: 17.3541

Epoch 614: val_loss did not improve from 17.29580
196/196 - 75s - loss: 17.2923 - MinusLogProbMetric: 17.2923 - val_loss: 17.3541 - val_MinusLogProbMetric: 17.3541 - lr: 1.6667e-04 - 75s/epoch - 382ms/step
Epoch 615/1000
2023-09-27 02:12:39.962 
Epoch 615/1000 
	 loss: 17.2871, MinusLogProbMetric: 17.2871, val_loss: 17.3196, val_MinusLogProbMetric: 17.3196

Epoch 615: val_loss did not improve from 17.29580
196/196 - 74s - loss: 17.2871 - MinusLogProbMetric: 17.2871 - val_loss: 17.3196 - val_MinusLogProbMetric: 17.3196 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 616/1000
2023-09-27 02:13:54.397 
Epoch 616/1000 
	 loss: 17.2586, MinusLogProbMetric: 17.2586, val_loss: 17.3131, val_MinusLogProbMetric: 17.3131

Epoch 616: val_loss did not improve from 17.29580
196/196 - 74s - loss: 17.2586 - MinusLogProbMetric: 17.2586 - val_loss: 17.3131 - val_MinusLogProbMetric: 17.3131 - lr: 1.6667e-04 - 74s/epoch - 380ms/step
Epoch 617/1000
2023-09-27 02:15:08.385 
Epoch 617/1000 
	 loss: 17.2578, MinusLogProbMetric: 17.2578, val_loss: 17.4651, val_MinusLogProbMetric: 17.4651

Epoch 617: val_loss did not improve from 17.29580
196/196 - 74s - loss: 17.2578 - MinusLogProbMetric: 17.2578 - val_loss: 17.4651 - val_MinusLogProbMetric: 17.4651 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 618/1000
2023-09-27 02:16:22.698 
Epoch 618/1000 
	 loss: 17.2876, MinusLogProbMetric: 17.2876, val_loss: 17.5238, val_MinusLogProbMetric: 17.5238

Epoch 618: val_loss did not improve from 17.29580
196/196 - 74s - loss: 17.2876 - MinusLogProbMetric: 17.2876 - val_loss: 17.5238 - val_MinusLogProbMetric: 17.5238 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 619/1000
2023-09-27 02:17:36.749 
Epoch 619/1000 
	 loss: 17.2833, MinusLogProbMetric: 17.2833, val_loss: 17.4378, val_MinusLogProbMetric: 17.4378

Epoch 619: val_loss did not improve from 17.29580
196/196 - 74s - loss: 17.2833 - MinusLogProbMetric: 17.2833 - val_loss: 17.4378 - val_MinusLogProbMetric: 17.4378 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 620/1000
2023-09-27 02:18:51.115 
Epoch 620/1000 
	 loss: 17.3017, MinusLogProbMetric: 17.3017, val_loss: 17.4349, val_MinusLogProbMetric: 17.4349

Epoch 620: val_loss did not improve from 17.29580
196/196 - 74s - loss: 17.3017 - MinusLogProbMetric: 17.3017 - val_loss: 17.4349 - val_MinusLogProbMetric: 17.4349 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 621/1000
2023-09-27 02:20:05.127 
Epoch 621/1000 
	 loss: 17.2712, MinusLogProbMetric: 17.2712, val_loss: 17.4846, val_MinusLogProbMetric: 17.4846

Epoch 621: val_loss did not improve from 17.29580
196/196 - 74s - loss: 17.2712 - MinusLogProbMetric: 17.2712 - val_loss: 17.4846 - val_MinusLogProbMetric: 17.4846 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 622/1000
2023-09-27 02:21:18.872 
Epoch 622/1000 
	 loss: 17.2695, MinusLogProbMetric: 17.2695, val_loss: 17.4293, val_MinusLogProbMetric: 17.4293

Epoch 622: val_loss did not improve from 17.29580
196/196 - 74s - loss: 17.2695 - MinusLogProbMetric: 17.2695 - val_loss: 17.4293 - val_MinusLogProbMetric: 17.4293 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 623/1000
2023-09-27 02:22:32.790 
Epoch 623/1000 
	 loss: 17.2860, MinusLogProbMetric: 17.2860, val_loss: 17.4046, val_MinusLogProbMetric: 17.4046

Epoch 623: val_loss did not improve from 17.29580
196/196 - 74s - loss: 17.2860 - MinusLogProbMetric: 17.2860 - val_loss: 17.4046 - val_MinusLogProbMetric: 17.4046 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 624/1000
2023-09-27 02:23:46.398 
Epoch 624/1000 
	 loss: 17.3099, MinusLogProbMetric: 17.3099, val_loss: 17.3851, val_MinusLogProbMetric: 17.3851

Epoch 624: val_loss did not improve from 17.29580
196/196 - 74s - loss: 17.3099 - MinusLogProbMetric: 17.3099 - val_loss: 17.3851 - val_MinusLogProbMetric: 17.3851 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 625/1000
2023-09-27 02:25:01.406 
Epoch 625/1000 
	 loss: 17.2947, MinusLogProbMetric: 17.2947, val_loss: 17.5201, val_MinusLogProbMetric: 17.5201

Epoch 625: val_loss did not improve from 17.29580
196/196 - 75s - loss: 17.2947 - MinusLogProbMetric: 17.2947 - val_loss: 17.5201 - val_MinusLogProbMetric: 17.5201 - lr: 1.6667e-04 - 75s/epoch - 383ms/step
Epoch 626/1000
2023-09-27 02:26:17.993 
Epoch 626/1000 
	 loss: 17.2807, MinusLogProbMetric: 17.2807, val_loss: 17.4750, val_MinusLogProbMetric: 17.4750

Epoch 626: val_loss did not improve from 17.29580
196/196 - 77s - loss: 17.2807 - MinusLogProbMetric: 17.2807 - val_loss: 17.4750 - val_MinusLogProbMetric: 17.4750 - lr: 1.6667e-04 - 77s/epoch - 391ms/step
Epoch 627/1000
2023-09-27 02:27:35.712 
Epoch 627/1000 
	 loss: 17.2526, MinusLogProbMetric: 17.2526, val_loss: 17.3271, val_MinusLogProbMetric: 17.3271

Epoch 627: val_loss did not improve from 17.29580
196/196 - 78s - loss: 17.2526 - MinusLogProbMetric: 17.2526 - val_loss: 17.3271 - val_MinusLogProbMetric: 17.3271 - lr: 1.6667e-04 - 78s/epoch - 397ms/step
Epoch 628/1000
2023-09-27 02:28:51.244 
Epoch 628/1000 
	 loss: 17.2825, MinusLogProbMetric: 17.2825, val_loss: 17.3460, val_MinusLogProbMetric: 17.3460

Epoch 628: val_loss did not improve from 17.29580
196/196 - 76s - loss: 17.2825 - MinusLogProbMetric: 17.2825 - val_loss: 17.3460 - val_MinusLogProbMetric: 17.3460 - lr: 1.6667e-04 - 76s/epoch - 385ms/step
Epoch 629/1000
2023-09-27 02:30:07.951 
Epoch 629/1000 
	 loss: 17.2766, MinusLogProbMetric: 17.2766, val_loss: 17.4884, val_MinusLogProbMetric: 17.4884

Epoch 629: val_loss did not improve from 17.29580
196/196 - 77s - loss: 17.2766 - MinusLogProbMetric: 17.2766 - val_loss: 17.4884 - val_MinusLogProbMetric: 17.4884 - lr: 1.6667e-04 - 77s/epoch - 391ms/step
Epoch 630/1000
2023-09-27 02:31:25.406 
Epoch 630/1000 
	 loss: 17.2921, MinusLogProbMetric: 17.2921, val_loss: 17.4079, val_MinusLogProbMetric: 17.4079

Epoch 630: val_loss did not improve from 17.29580
196/196 - 77s - loss: 17.2921 - MinusLogProbMetric: 17.2921 - val_loss: 17.4079 - val_MinusLogProbMetric: 17.4079 - lr: 1.6667e-04 - 77s/epoch - 395ms/step
Epoch 631/1000
2023-09-27 02:32:38.207 
Epoch 631/1000 
	 loss: 17.2634, MinusLogProbMetric: 17.2634, val_loss: 17.5405, val_MinusLogProbMetric: 17.5405

Epoch 631: val_loss did not improve from 17.29580
196/196 - 73s - loss: 17.2634 - MinusLogProbMetric: 17.2634 - val_loss: 17.5405 - val_MinusLogProbMetric: 17.5405 - lr: 1.6667e-04 - 73s/epoch - 371ms/step
Epoch 632/1000
2023-09-27 02:33:59.744 
Epoch 632/1000 
	 loss: 17.2542, MinusLogProbMetric: 17.2542, val_loss: 17.4738, val_MinusLogProbMetric: 17.4738

Epoch 632: val_loss did not improve from 17.29580
196/196 - 82s - loss: 17.2542 - MinusLogProbMetric: 17.2542 - val_loss: 17.4738 - val_MinusLogProbMetric: 17.4738 - lr: 1.6667e-04 - 82s/epoch - 416ms/step
Epoch 633/1000
2023-09-27 02:35:22.146 
Epoch 633/1000 
	 loss: 17.2775, MinusLogProbMetric: 17.2775, val_loss: 17.4137, val_MinusLogProbMetric: 17.4137

Epoch 633: val_loss did not improve from 17.29580
196/196 - 82s - loss: 17.2775 - MinusLogProbMetric: 17.2775 - val_loss: 17.4137 - val_MinusLogProbMetric: 17.4137 - lr: 1.6667e-04 - 82s/epoch - 420ms/step
Epoch 634/1000
2023-09-27 02:36:41.233 
Epoch 634/1000 
	 loss: 17.2872, MinusLogProbMetric: 17.2872, val_loss: 17.3477, val_MinusLogProbMetric: 17.3477

Epoch 634: val_loss did not improve from 17.29580
196/196 - 79s - loss: 17.2872 - MinusLogProbMetric: 17.2872 - val_loss: 17.3477 - val_MinusLogProbMetric: 17.3477 - lr: 1.6667e-04 - 79s/epoch - 403ms/step
Epoch 635/1000
2023-09-27 02:37:58.532 
Epoch 635/1000 
	 loss: 17.2732, MinusLogProbMetric: 17.2732, val_loss: 17.6648, val_MinusLogProbMetric: 17.6648

Epoch 635: val_loss did not improve from 17.29580
196/196 - 77s - loss: 17.2732 - MinusLogProbMetric: 17.2732 - val_loss: 17.6648 - val_MinusLogProbMetric: 17.6648 - lr: 1.6667e-04 - 77s/epoch - 394ms/step
Epoch 636/1000
2023-09-27 02:39:15.507 
Epoch 636/1000 
	 loss: 17.2926, MinusLogProbMetric: 17.2926, val_loss: 17.4267, val_MinusLogProbMetric: 17.4267

Epoch 636: val_loss did not improve from 17.29580
196/196 - 77s - loss: 17.2926 - MinusLogProbMetric: 17.2926 - val_loss: 17.4267 - val_MinusLogProbMetric: 17.4267 - lr: 1.6667e-04 - 77s/epoch - 393ms/step
Epoch 637/1000
2023-09-27 02:40:32.911 
Epoch 637/1000 
	 loss: 17.2601, MinusLogProbMetric: 17.2601, val_loss: 17.2404, val_MinusLogProbMetric: 17.2404

Epoch 637: val_loss improved from 17.29580 to 17.24045, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 79s - loss: 17.2601 - MinusLogProbMetric: 17.2601 - val_loss: 17.2404 - val_MinusLogProbMetric: 17.2404 - lr: 1.6667e-04 - 79s/epoch - 401ms/step
Epoch 638/1000
2023-09-27 02:41:50.556 
Epoch 638/1000 
	 loss: 17.2610, MinusLogProbMetric: 17.2610, val_loss: 17.3653, val_MinusLogProbMetric: 17.3653

Epoch 638: val_loss did not improve from 17.24045
196/196 - 76s - loss: 17.2610 - MinusLogProbMetric: 17.2610 - val_loss: 17.3653 - val_MinusLogProbMetric: 17.3653 - lr: 1.6667e-04 - 76s/epoch - 390ms/step
Epoch 639/1000
2023-09-27 02:43:06.810 
Epoch 639/1000 
	 loss: 17.2520, MinusLogProbMetric: 17.2520, val_loss: 17.3268, val_MinusLogProbMetric: 17.3268

Epoch 639: val_loss did not improve from 17.24045
196/196 - 76s - loss: 17.2520 - MinusLogProbMetric: 17.2520 - val_loss: 17.3268 - val_MinusLogProbMetric: 17.3268 - lr: 1.6667e-04 - 76s/epoch - 389ms/step
Epoch 640/1000
2023-09-27 02:44:23.715 
Epoch 640/1000 
	 loss: 17.2728, MinusLogProbMetric: 17.2728, val_loss: 17.3203, val_MinusLogProbMetric: 17.3203

Epoch 640: val_loss did not improve from 17.24045
196/196 - 77s - loss: 17.2728 - MinusLogProbMetric: 17.2728 - val_loss: 17.3203 - val_MinusLogProbMetric: 17.3203 - lr: 1.6667e-04 - 77s/epoch - 392ms/step
Epoch 641/1000
2023-09-27 02:45:40.215 
Epoch 641/1000 
	 loss: 17.2920, MinusLogProbMetric: 17.2920, val_loss: 17.2479, val_MinusLogProbMetric: 17.2479

Epoch 641: val_loss did not improve from 17.24045
196/196 - 76s - loss: 17.2920 - MinusLogProbMetric: 17.2920 - val_loss: 17.2479 - val_MinusLogProbMetric: 17.2479 - lr: 1.6667e-04 - 76s/epoch - 390ms/step
Epoch 642/1000
2023-09-27 02:46:56.088 
Epoch 642/1000 
	 loss: 17.2761, MinusLogProbMetric: 17.2761, val_loss: 17.4432, val_MinusLogProbMetric: 17.4432

Epoch 642: val_loss did not improve from 17.24045
196/196 - 76s - loss: 17.2761 - MinusLogProbMetric: 17.2761 - val_loss: 17.4432 - val_MinusLogProbMetric: 17.4432 - lr: 1.6667e-04 - 76s/epoch - 387ms/step
Epoch 643/1000
2023-09-27 02:48:10.405 
Epoch 643/1000 
	 loss: 17.2806, MinusLogProbMetric: 17.2806, val_loss: 17.3062, val_MinusLogProbMetric: 17.3062

Epoch 643: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2806 - MinusLogProbMetric: 17.2806 - val_loss: 17.3062 - val_MinusLogProbMetric: 17.3062 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 644/1000
2023-09-27 02:49:24.870 
Epoch 644/1000 
	 loss: 17.2669, MinusLogProbMetric: 17.2669, val_loss: 17.4394, val_MinusLogProbMetric: 17.4394

Epoch 644: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2669 - MinusLogProbMetric: 17.2669 - val_loss: 17.4394 - val_MinusLogProbMetric: 17.4394 - lr: 1.6667e-04 - 74s/epoch - 380ms/step
Epoch 645/1000
2023-09-27 02:50:38.939 
Epoch 645/1000 
	 loss: 17.2487, MinusLogProbMetric: 17.2487, val_loss: 17.4408, val_MinusLogProbMetric: 17.4408

Epoch 645: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2487 - MinusLogProbMetric: 17.2487 - val_loss: 17.4408 - val_MinusLogProbMetric: 17.4408 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 646/1000
2023-09-27 02:51:53.132 
Epoch 646/1000 
	 loss: 17.2622, MinusLogProbMetric: 17.2622, val_loss: 17.2914, val_MinusLogProbMetric: 17.2914

Epoch 646: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2622 - MinusLogProbMetric: 17.2622 - val_loss: 17.2914 - val_MinusLogProbMetric: 17.2914 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 647/1000
2023-09-27 02:53:07.426 
Epoch 647/1000 
	 loss: 17.3043, MinusLogProbMetric: 17.3043, val_loss: 17.3034, val_MinusLogProbMetric: 17.3034

Epoch 647: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.3043 - MinusLogProbMetric: 17.3043 - val_loss: 17.3034 - val_MinusLogProbMetric: 17.3034 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 648/1000
2023-09-27 02:54:21.383 
Epoch 648/1000 
	 loss: 17.2586, MinusLogProbMetric: 17.2586, val_loss: 17.3706, val_MinusLogProbMetric: 17.3706

Epoch 648: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2586 - MinusLogProbMetric: 17.2586 - val_loss: 17.3706 - val_MinusLogProbMetric: 17.3706 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 649/1000
2023-09-27 02:55:36.272 
Epoch 649/1000 
	 loss: 17.2686, MinusLogProbMetric: 17.2686, val_loss: 17.3355, val_MinusLogProbMetric: 17.3355

Epoch 649: val_loss did not improve from 17.24045
196/196 - 75s - loss: 17.2686 - MinusLogProbMetric: 17.2686 - val_loss: 17.3355 - val_MinusLogProbMetric: 17.3355 - lr: 1.6667e-04 - 75s/epoch - 382ms/step
Epoch 650/1000
2023-09-27 02:56:50.136 
Epoch 650/1000 
	 loss: 17.2555, MinusLogProbMetric: 17.2555, val_loss: 17.4599, val_MinusLogProbMetric: 17.4599

Epoch 650: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2555 - MinusLogProbMetric: 17.2555 - val_loss: 17.4599 - val_MinusLogProbMetric: 17.4599 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 651/1000
2023-09-27 02:58:04.337 
Epoch 651/1000 
	 loss: 17.2618, MinusLogProbMetric: 17.2618, val_loss: 17.3681, val_MinusLogProbMetric: 17.3681

Epoch 651: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2618 - MinusLogProbMetric: 17.2618 - val_loss: 17.3681 - val_MinusLogProbMetric: 17.3681 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 652/1000
2023-09-27 02:59:18.271 
Epoch 652/1000 
	 loss: 17.2577, MinusLogProbMetric: 17.2577, val_loss: 17.3453, val_MinusLogProbMetric: 17.3453

Epoch 652: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2577 - MinusLogProbMetric: 17.2577 - val_loss: 17.3453 - val_MinusLogProbMetric: 17.3453 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 653/1000
2023-09-27 03:00:31.989 
Epoch 653/1000 
	 loss: 17.2778, MinusLogProbMetric: 17.2778, val_loss: 17.4615, val_MinusLogProbMetric: 17.4615

Epoch 653: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2778 - MinusLogProbMetric: 17.2778 - val_loss: 17.4615 - val_MinusLogProbMetric: 17.4615 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 654/1000
2023-09-27 03:01:46.150 
Epoch 654/1000 
	 loss: 17.2894, MinusLogProbMetric: 17.2894, val_loss: 17.3414, val_MinusLogProbMetric: 17.3414

Epoch 654: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2894 - MinusLogProbMetric: 17.2894 - val_loss: 17.3414 - val_MinusLogProbMetric: 17.3414 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 655/1000
2023-09-27 03:02:59.980 
Epoch 655/1000 
	 loss: 17.2730, MinusLogProbMetric: 17.2730, val_loss: 17.4335, val_MinusLogProbMetric: 17.4335

Epoch 655: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2730 - MinusLogProbMetric: 17.2730 - val_loss: 17.4335 - val_MinusLogProbMetric: 17.4335 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 656/1000
2023-09-27 03:04:14.394 
Epoch 656/1000 
	 loss: 17.2718, MinusLogProbMetric: 17.2718, val_loss: 17.3078, val_MinusLogProbMetric: 17.3078

Epoch 656: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2718 - MinusLogProbMetric: 17.2718 - val_loss: 17.3078 - val_MinusLogProbMetric: 17.3078 - lr: 1.6667e-04 - 74s/epoch - 380ms/step
Epoch 657/1000
2023-09-27 03:05:27.818 
Epoch 657/1000 
	 loss: 17.2541, MinusLogProbMetric: 17.2541, val_loss: 17.2733, val_MinusLogProbMetric: 17.2733

Epoch 657: val_loss did not improve from 17.24045
196/196 - 73s - loss: 17.2541 - MinusLogProbMetric: 17.2541 - val_loss: 17.2733 - val_MinusLogProbMetric: 17.2733 - lr: 1.6667e-04 - 73s/epoch - 375ms/step
Epoch 658/1000
2023-09-27 03:06:41.922 
Epoch 658/1000 
	 loss: 17.2472, MinusLogProbMetric: 17.2472, val_loss: 17.3459, val_MinusLogProbMetric: 17.3459

Epoch 658: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2472 - MinusLogProbMetric: 17.2472 - val_loss: 17.3459 - val_MinusLogProbMetric: 17.3459 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 659/1000
2023-09-27 03:07:55.995 
Epoch 659/1000 
	 loss: 17.2521, MinusLogProbMetric: 17.2521, val_loss: 17.2665, val_MinusLogProbMetric: 17.2665

Epoch 659: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2521 - MinusLogProbMetric: 17.2521 - val_loss: 17.2665 - val_MinusLogProbMetric: 17.2665 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 660/1000
2023-09-27 03:09:10.581 
Epoch 660/1000 
	 loss: 17.2456, MinusLogProbMetric: 17.2456, val_loss: 17.3548, val_MinusLogProbMetric: 17.3548

Epoch 660: val_loss did not improve from 17.24045
196/196 - 75s - loss: 17.2456 - MinusLogProbMetric: 17.2456 - val_loss: 17.3548 - val_MinusLogProbMetric: 17.3548 - lr: 1.6667e-04 - 75s/epoch - 381ms/step
Epoch 661/1000
2023-09-27 03:10:25.431 
Epoch 661/1000 
	 loss: 17.2589, MinusLogProbMetric: 17.2589, val_loss: 17.3200, val_MinusLogProbMetric: 17.3200

Epoch 661: val_loss did not improve from 17.24045
196/196 - 75s - loss: 17.2589 - MinusLogProbMetric: 17.2589 - val_loss: 17.3200 - val_MinusLogProbMetric: 17.3200 - lr: 1.6667e-04 - 75s/epoch - 382ms/step
Epoch 662/1000
2023-09-27 03:11:38.955 
Epoch 662/1000 
	 loss: 17.2540, MinusLogProbMetric: 17.2540, val_loss: 17.3243, val_MinusLogProbMetric: 17.3243

Epoch 662: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2540 - MinusLogProbMetric: 17.2540 - val_loss: 17.3243 - val_MinusLogProbMetric: 17.3243 - lr: 1.6667e-04 - 74s/epoch - 375ms/step
Epoch 663/1000
2023-09-27 03:12:53.075 
Epoch 663/1000 
	 loss: 17.2348, MinusLogProbMetric: 17.2348, val_loss: 17.5475, val_MinusLogProbMetric: 17.5475

Epoch 663: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2348 - MinusLogProbMetric: 17.2348 - val_loss: 17.5475 - val_MinusLogProbMetric: 17.5475 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 664/1000
2023-09-27 03:14:07.214 
Epoch 664/1000 
	 loss: 17.2455, MinusLogProbMetric: 17.2455, val_loss: 17.2806, val_MinusLogProbMetric: 17.2806

Epoch 664: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2455 - MinusLogProbMetric: 17.2455 - val_loss: 17.2806 - val_MinusLogProbMetric: 17.2806 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 665/1000
2023-09-27 03:15:20.883 
Epoch 665/1000 
	 loss: 17.2349, MinusLogProbMetric: 17.2349, val_loss: 17.3193, val_MinusLogProbMetric: 17.3193

Epoch 665: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2349 - MinusLogProbMetric: 17.2349 - val_loss: 17.3193 - val_MinusLogProbMetric: 17.3193 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 666/1000
2023-09-27 03:16:35.292 
Epoch 666/1000 
	 loss: 17.2724, MinusLogProbMetric: 17.2724, val_loss: 17.4000, val_MinusLogProbMetric: 17.4000

Epoch 666: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2724 - MinusLogProbMetric: 17.2724 - val_loss: 17.4000 - val_MinusLogProbMetric: 17.4000 - lr: 1.6667e-04 - 74s/epoch - 380ms/step
Epoch 667/1000
2023-09-27 03:17:48.907 
Epoch 667/1000 
	 loss: 17.2503, MinusLogProbMetric: 17.2503, val_loss: 17.3829, val_MinusLogProbMetric: 17.3829

Epoch 667: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2503 - MinusLogProbMetric: 17.2503 - val_loss: 17.3829 - val_MinusLogProbMetric: 17.3829 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 668/1000
2023-09-27 03:19:03.160 
Epoch 668/1000 
	 loss: 17.2443, MinusLogProbMetric: 17.2443, val_loss: 17.2955, val_MinusLogProbMetric: 17.2955

Epoch 668: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2443 - MinusLogProbMetric: 17.2443 - val_loss: 17.2955 - val_MinusLogProbMetric: 17.2955 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 669/1000
2023-09-27 03:20:16.884 
Epoch 669/1000 
	 loss: 17.2487, MinusLogProbMetric: 17.2487, val_loss: 17.3161, val_MinusLogProbMetric: 17.3161

Epoch 669: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2487 - MinusLogProbMetric: 17.2487 - val_loss: 17.3161 - val_MinusLogProbMetric: 17.3161 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 670/1000
2023-09-27 03:21:30.986 
Epoch 670/1000 
	 loss: 17.2422, MinusLogProbMetric: 17.2422, val_loss: 17.3030, val_MinusLogProbMetric: 17.3030

Epoch 670: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2422 - MinusLogProbMetric: 17.2422 - val_loss: 17.3030 - val_MinusLogProbMetric: 17.3030 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 671/1000
2023-09-27 03:22:45.460 
Epoch 671/1000 
	 loss: 17.2724, MinusLogProbMetric: 17.2724, val_loss: 17.3477, val_MinusLogProbMetric: 17.3477

Epoch 671: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2724 - MinusLogProbMetric: 17.2724 - val_loss: 17.3477 - val_MinusLogProbMetric: 17.3477 - lr: 1.6667e-04 - 74s/epoch - 380ms/step
Epoch 672/1000
2023-09-27 03:23:59.908 
Epoch 672/1000 
	 loss: 17.2629, MinusLogProbMetric: 17.2629, val_loss: 17.4783, val_MinusLogProbMetric: 17.4783

Epoch 672: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2629 - MinusLogProbMetric: 17.2629 - val_loss: 17.4783 - val_MinusLogProbMetric: 17.4783 - lr: 1.6667e-04 - 74s/epoch - 380ms/step
Epoch 673/1000
2023-09-27 03:25:14.205 
Epoch 673/1000 
	 loss: 17.2544, MinusLogProbMetric: 17.2544, val_loss: 17.3570, val_MinusLogProbMetric: 17.3570

Epoch 673: val_loss did not improve from 17.24045
196/196 - 74s - loss: 17.2544 - MinusLogProbMetric: 17.2544 - val_loss: 17.3570 - val_MinusLogProbMetric: 17.3570 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 674/1000
2023-09-27 03:26:27.380 
Epoch 674/1000 
	 loss: 17.2520, MinusLogProbMetric: 17.2520, val_loss: 17.3608, val_MinusLogProbMetric: 17.3608

Epoch 674: val_loss did not improve from 17.24045
196/196 - 73s - loss: 17.2520 - MinusLogProbMetric: 17.2520 - val_loss: 17.3608 - val_MinusLogProbMetric: 17.3608 - lr: 1.6667e-04 - 73s/epoch - 373ms/step
Epoch 675/1000
2023-09-27 03:27:42.044 
Epoch 675/1000 
	 loss: 17.2497, MinusLogProbMetric: 17.2497, val_loss: 17.3386, val_MinusLogProbMetric: 17.3386

Epoch 675: val_loss did not improve from 17.24045
196/196 - 75s - loss: 17.2497 - MinusLogProbMetric: 17.2497 - val_loss: 17.3386 - val_MinusLogProbMetric: 17.3386 - lr: 1.6667e-04 - 75s/epoch - 381ms/step
Epoch 676/1000
2023-09-27 03:28:55.586 
Epoch 676/1000 
	 loss: 17.2604, MinusLogProbMetric: 17.2604, val_loss: 17.2020, val_MinusLogProbMetric: 17.2020

Epoch 676: val_loss improved from 17.24045 to 17.20202, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 17.2604 - MinusLogProbMetric: 17.2604 - val_loss: 17.2020 - val_MinusLogProbMetric: 17.2020 - lr: 1.6667e-04 - 75s/epoch - 383ms/step
Epoch 677/1000
2023-09-27 03:30:11.590 
Epoch 677/1000 
	 loss: 17.2633, MinusLogProbMetric: 17.2633, val_loss: 17.3776, val_MinusLogProbMetric: 17.3776

Epoch 677: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2633 - MinusLogProbMetric: 17.2633 - val_loss: 17.3776 - val_MinusLogProbMetric: 17.3776 - lr: 1.6667e-04 - 74s/epoch - 380ms/step
Epoch 678/1000
2023-09-27 03:31:25.371 
Epoch 678/1000 
	 loss: 17.2681, MinusLogProbMetric: 17.2681, val_loss: 17.2709, val_MinusLogProbMetric: 17.2709

Epoch 678: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2681 - MinusLogProbMetric: 17.2681 - val_loss: 17.2709 - val_MinusLogProbMetric: 17.2709 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 679/1000
2023-09-27 03:32:39.017 
Epoch 679/1000 
	 loss: 17.2426, MinusLogProbMetric: 17.2426, val_loss: 17.2637, val_MinusLogProbMetric: 17.2637

Epoch 679: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2426 - MinusLogProbMetric: 17.2426 - val_loss: 17.2637 - val_MinusLogProbMetric: 17.2637 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 680/1000
2023-09-27 03:33:53.247 
Epoch 680/1000 
	 loss: 17.2367, MinusLogProbMetric: 17.2367, val_loss: 17.3531, val_MinusLogProbMetric: 17.3531

Epoch 680: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2367 - MinusLogProbMetric: 17.2367 - val_loss: 17.3531 - val_MinusLogProbMetric: 17.3531 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 681/1000
2023-09-27 03:35:07.424 
Epoch 681/1000 
	 loss: 17.2608, MinusLogProbMetric: 17.2608, val_loss: 17.3422, val_MinusLogProbMetric: 17.3422

Epoch 681: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2608 - MinusLogProbMetric: 17.2608 - val_loss: 17.3422 - val_MinusLogProbMetric: 17.3422 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 682/1000
2023-09-27 03:36:21.356 
Epoch 682/1000 
	 loss: 17.2409, MinusLogProbMetric: 17.2409, val_loss: 17.2722, val_MinusLogProbMetric: 17.2722

Epoch 682: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2409 - MinusLogProbMetric: 17.2409 - val_loss: 17.2722 - val_MinusLogProbMetric: 17.2722 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 683/1000
2023-09-27 03:37:35.433 
Epoch 683/1000 
	 loss: 17.2434, MinusLogProbMetric: 17.2434, val_loss: 17.3220, val_MinusLogProbMetric: 17.3220

Epoch 683: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2434 - MinusLogProbMetric: 17.2434 - val_loss: 17.3220 - val_MinusLogProbMetric: 17.3220 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 684/1000
2023-09-27 03:38:49.718 
Epoch 684/1000 
	 loss: 17.2311, MinusLogProbMetric: 17.2311, val_loss: 17.2774, val_MinusLogProbMetric: 17.2774

Epoch 684: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2311 - MinusLogProbMetric: 17.2311 - val_loss: 17.2774 - val_MinusLogProbMetric: 17.2774 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 685/1000
2023-09-27 03:40:03.124 
Epoch 685/1000 
	 loss: 17.2494, MinusLogProbMetric: 17.2494, val_loss: 17.3842, val_MinusLogProbMetric: 17.3842

Epoch 685: val_loss did not improve from 17.20202
196/196 - 73s - loss: 17.2494 - MinusLogProbMetric: 17.2494 - val_loss: 17.3842 - val_MinusLogProbMetric: 17.3842 - lr: 1.6667e-04 - 73s/epoch - 374ms/step
Epoch 686/1000
2023-09-27 03:41:17.157 
Epoch 686/1000 
	 loss: 17.2520, MinusLogProbMetric: 17.2520, val_loss: 17.3889, val_MinusLogProbMetric: 17.3889

Epoch 686: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2520 - MinusLogProbMetric: 17.2520 - val_loss: 17.3889 - val_MinusLogProbMetric: 17.3889 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 687/1000
2023-09-27 03:42:31.495 
Epoch 687/1000 
	 loss: 17.2401, MinusLogProbMetric: 17.2401, val_loss: 17.2927, val_MinusLogProbMetric: 17.2927

Epoch 687: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2401 - MinusLogProbMetric: 17.2401 - val_loss: 17.2927 - val_MinusLogProbMetric: 17.2927 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 688/1000
2023-09-27 03:43:44.883 
Epoch 688/1000 
	 loss: 17.2420, MinusLogProbMetric: 17.2420, val_loss: 17.3694, val_MinusLogProbMetric: 17.3694

Epoch 688: val_loss did not improve from 17.20202
196/196 - 73s - loss: 17.2420 - MinusLogProbMetric: 17.2420 - val_loss: 17.3694 - val_MinusLogProbMetric: 17.3694 - lr: 1.6667e-04 - 73s/epoch - 374ms/step
Epoch 689/1000
2023-09-27 03:44:58.469 
Epoch 689/1000 
	 loss: 17.2287, MinusLogProbMetric: 17.2287, val_loss: 17.4263, val_MinusLogProbMetric: 17.4263

Epoch 689: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2287 - MinusLogProbMetric: 17.2287 - val_loss: 17.4263 - val_MinusLogProbMetric: 17.4263 - lr: 1.6667e-04 - 74s/epoch - 375ms/step
Epoch 690/1000
2023-09-27 03:46:11.958 
Epoch 690/1000 
	 loss: 17.2345, MinusLogProbMetric: 17.2345, val_loss: 17.4391, val_MinusLogProbMetric: 17.4391

Epoch 690: val_loss did not improve from 17.20202
196/196 - 73s - loss: 17.2345 - MinusLogProbMetric: 17.2345 - val_loss: 17.4391 - val_MinusLogProbMetric: 17.4391 - lr: 1.6667e-04 - 73s/epoch - 375ms/step
Epoch 691/1000
2023-09-27 03:47:25.356 
Epoch 691/1000 
	 loss: 17.2483, MinusLogProbMetric: 17.2483, val_loss: 17.3315, val_MinusLogProbMetric: 17.3315

Epoch 691: val_loss did not improve from 17.20202
196/196 - 73s - loss: 17.2483 - MinusLogProbMetric: 17.2483 - val_loss: 17.3315 - val_MinusLogProbMetric: 17.3315 - lr: 1.6667e-04 - 73s/epoch - 374ms/step
Epoch 692/1000
2023-09-27 03:48:39.466 
Epoch 692/1000 
	 loss: 17.2438, MinusLogProbMetric: 17.2438, val_loss: 17.3234, val_MinusLogProbMetric: 17.3234

Epoch 692: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2438 - MinusLogProbMetric: 17.2438 - val_loss: 17.3234 - val_MinusLogProbMetric: 17.3234 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 693/1000
2023-09-27 03:49:53.474 
Epoch 693/1000 
	 loss: 17.2309, MinusLogProbMetric: 17.2309, val_loss: 17.2642, val_MinusLogProbMetric: 17.2642

Epoch 693: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2309 - MinusLogProbMetric: 17.2309 - val_loss: 17.2642 - val_MinusLogProbMetric: 17.2642 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 694/1000
2023-09-27 03:51:07.199 
Epoch 694/1000 
	 loss: 17.2551, MinusLogProbMetric: 17.2551, val_loss: 17.2911, val_MinusLogProbMetric: 17.2911

Epoch 694: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2551 - MinusLogProbMetric: 17.2551 - val_loss: 17.2911 - val_MinusLogProbMetric: 17.2911 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 695/1000
2023-09-27 03:52:21.244 
Epoch 695/1000 
	 loss: 17.2487, MinusLogProbMetric: 17.2487, val_loss: 17.3113, val_MinusLogProbMetric: 17.3113

Epoch 695: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2487 - MinusLogProbMetric: 17.2487 - val_loss: 17.3113 - val_MinusLogProbMetric: 17.3113 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 696/1000
2023-09-27 03:53:35.340 
Epoch 696/1000 
	 loss: 17.2286, MinusLogProbMetric: 17.2286, val_loss: 17.2842, val_MinusLogProbMetric: 17.2842

Epoch 696: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2286 - MinusLogProbMetric: 17.2286 - val_loss: 17.2842 - val_MinusLogProbMetric: 17.2842 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 697/1000
2023-09-27 03:54:49.515 
Epoch 697/1000 
	 loss: 17.2272, MinusLogProbMetric: 17.2272, val_loss: 17.2505, val_MinusLogProbMetric: 17.2505

Epoch 697: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2272 - MinusLogProbMetric: 17.2272 - val_loss: 17.2505 - val_MinusLogProbMetric: 17.2505 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 698/1000
2023-09-27 03:56:03.705 
Epoch 698/1000 
	 loss: 17.2274, MinusLogProbMetric: 17.2274, val_loss: 17.3311, val_MinusLogProbMetric: 17.3311

Epoch 698: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2274 - MinusLogProbMetric: 17.2274 - val_loss: 17.3311 - val_MinusLogProbMetric: 17.3311 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 699/1000
2023-09-27 03:57:17.845 
Epoch 699/1000 
	 loss: 17.2893, MinusLogProbMetric: 17.2893, val_loss: 17.5428, val_MinusLogProbMetric: 17.5428

Epoch 699: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2893 - MinusLogProbMetric: 17.2893 - val_loss: 17.5428 - val_MinusLogProbMetric: 17.5428 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 700/1000
2023-09-27 03:58:32.102 
Epoch 700/1000 
	 loss: 17.2365, MinusLogProbMetric: 17.2365, val_loss: 17.2446, val_MinusLogProbMetric: 17.2446

Epoch 700: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2365 - MinusLogProbMetric: 17.2365 - val_loss: 17.2446 - val_MinusLogProbMetric: 17.2446 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 701/1000
2023-09-27 03:59:46.025 
Epoch 701/1000 
	 loss: 17.2555, MinusLogProbMetric: 17.2555, val_loss: 17.3333, val_MinusLogProbMetric: 17.3333

Epoch 701: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2555 - MinusLogProbMetric: 17.2555 - val_loss: 17.3333 - val_MinusLogProbMetric: 17.3333 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 702/1000
2023-09-27 04:00:59.681 
Epoch 702/1000 
	 loss: 17.2477, MinusLogProbMetric: 17.2477, val_loss: 17.2863, val_MinusLogProbMetric: 17.2863

Epoch 702: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2477 - MinusLogProbMetric: 17.2477 - val_loss: 17.2863 - val_MinusLogProbMetric: 17.2863 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 703/1000
2023-09-27 04:02:13.468 
Epoch 703/1000 
	 loss: 17.2245, MinusLogProbMetric: 17.2245, val_loss: 17.3712, val_MinusLogProbMetric: 17.3712

Epoch 703: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2245 - MinusLogProbMetric: 17.2245 - val_loss: 17.3712 - val_MinusLogProbMetric: 17.3712 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 704/1000
2023-09-27 04:03:27.671 
Epoch 704/1000 
	 loss: 17.2426, MinusLogProbMetric: 17.2426, val_loss: 17.4100, val_MinusLogProbMetric: 17.4100

Epoch 704: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2426 - MinusLogProbMetric: 17.2426 - val_loss: 17.4100 - val_MinusLogProbMetric: 17.4100 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 705/1000
2023-09-27 04:04:41.168 
Epoch 705/1000 
	 loss: 17.2093, MinusLogProbMetric: 17.2093, val_loss: 17.2744, val_MinusLogProbMetric: 17.2744

Epoch 705: val_loss did not improve from 17.20202
196/196 - 73s - loss: 17.2093 - MinusLogProbMetric: 17.2093 - val_loss: 17.2744 - val_MinusLogProbMetric: 17.2744 - lr: 1.6667e-04 - 73s/epoch - 375ms/step
Epoch 706/1000
2023-09-27 04:05:55.787 
Epoch 706/1000 
	 loss: 17.2543, MinusLogProbMetric: 17.2543, val_loss: 17.2527, val_MinusLogProbMetric: 17.2527

Epoch 706: val_loss did not improve from 17.20202
196/196 - 75s - loss: 17.2543 - MinusLogProbMetric: 17.2543 - val_loss: 17.2527 - val_MinusLogProbMetric: 17.2527 - lr: 1.6667e-04 - 75s/epoch - 381ms/step
Epoch 707/1000
2023-09-27 04:07:09.802 
Epoch 707/1000 
	 loss: 17.2200, MinusLogProbMetric: 17.2200, val_loss: 17.2711, val_MinusLogProbMetric: 17.2711

Epoch 707: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2200 - MinusLogProbMetric: 17.2200 - val_loss: 17.2711 - val_MinusLogProbMetric: 17.2711 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 708/1000
2023-09-27 04:08:23.522 
Epoch 708/1000 
	 loss: 17.2330, MinusLogProbMetric: 17.2330, val_loss: 17.3077, val_MinusLogProbMetric: 17.3077

Epoch 708: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2330 - MinusLogProbMetric: 17.2330 - val_loss: 17.3077 - val_MinusLogProbMetric: 17.3077 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 709/1000
2023-09-27 04:09:38.008 
Epoch 709/1000 
	 loss: 17.2091, MinusLogProbMetric: 17.2091, val_loss: 17.3220, val_MinusLogProbMetric: 17.3220

Epoch 709: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2091 - MinusLogProbMetric: 17.2091 - val_loss: 17.3220 - val_MinusLogProbMetric: 17.3220 - lr: 1.6667e-04 - 74s/epoch - 380ms/step
Epoch 710/1000
2023-09-27 04:10:52.078 
Epoch 710/1000 
	 loss: 17.2367, MinusLogProbMetric: 17.2367, val_loss: 17.2843, val_MinusLogProbMetric: 17.2843

Epoch 710: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2367 - MinusLogProbMetric: 17.2367 - val_loss: 17.2843 - val_MinusLogProbMetric: 17.2843 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 711/1000
2023-09-27 04:12:05.942 
Epoch 711/1000 
	 loss: 17.2589, MinusLogProbMetric: 17.2589, val_loss: 17.3416, val_MinusLogProbMetric: 17.3416

Epoch 711: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2589 - MinusLogProbMetric: 17.2589 - val_loss: 17.3416 - val_MinusLogProbMetric: 17.3416 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 712/1000
2023-09-27 04:13:19.633 
Epoch 712/1000 
	 loss: 17.2232, MinusLogProbMetric: 17.2232, val_loss: 17.5075, val_MinusLogProbMetric: 17.5075

Epoch 712: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2232 - MinusLogProbMetric: 17.2232 - val_loss: 17.5075 - val_MinusLogProbMetric: 17.5075 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 713/1000
2023-09-27 04:14:32.868 
Epoch 713/1000 
	 loss: 17.2331, MinusLogProbMetric: 17.2331, val_loss: 17.2898, val_MinusLogProbMetric: 17.2898

Epoch 713: val_loss did not improve from 17.20202
196/196 - 73s - loss: 17.2331 - MinusLogProbMetric: 17.2331 - val_loss: 17.2898 - val_MinusLogProbMetric: 17.2898 - lr: 1.6667e-04 - 73s/epoch - 374ms/step
Epoch 714/1000
2023-09-27 04:15:46.686 
Epoch 714/1000 
	 loss: 17.1900, MinusLogProbMetric: 17.1900, val_loss: 17.2573, val_MinusLogProbMetric: 17.2573

Epoch 714: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.1900 - MinusLogProbMetric: 17.1900 - val_loss: 17.2573 - val_MinusLogProbMetric: 17.2573 - lr: 1.6667e-04 - 74s/epoch - 377ms/step
Epoch 715/1000
2023-09-27 04:17:00.981 
Epoch 715/1000 
	 loss: 17.2290, MinusLogProbMetric: 17.2290, val_loss: 17.3993, val_MinusLogProbMetric: 17.3993

Epoch 715: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2290 - MinusLogProbMetric: 17.2290 - val_loss: 17.3993 - val_MinusLogProbMetric: 17.3993 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 716/1000
2023-09-27 04:18:14.725 
Epoch 716/1000 
	 loss: 17.2363, MinusLogProbMetric: 17.2363, val_loss: 17.3797, val_MinusLogProbMetric: 17.3797

Epoch 716: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2363 - MinusLogProbMetric: 17.2363 - val_loss: 17.3797 - val_MinusLogProbMetric: 17.3797 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 717/1000
2023-09-27 04:19:29.349 
Epoch 717/1000 
	 loss: 17.2348, MinusLogProbMetric: 17.2348, val_loss: 17.3292, val_MinusLogProbMetric: 17.3292

Epoch 717: val_loss did not improve from 17.20202
196/196 - 75s - loss: 17.2348 - MinusLogProbMetric: 17.2348 - val_loss: 17.3292 - val_MinusLogProbMetric: 17.3292 - lr: 1.6667e-04 - 75s/epoch - 381ms/step
Epoch 718/1000
2023-09-27 04:20:43.120 
Epoch 718/1000 
	 loss: 17.2364, MinusLogProbMetric: 17.2364, val_loss: 17.2837, val_MinusLogProbMetric: 17.2837

Epoch 718: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2364 - MinusLogProbMetric: 17.2364 - val_loss: 17.2837 - val_MinusLogProbMetric: 17.2837 - lr: 1.6667e-04 - 74s/epoch - 376ms/step
Epoch 719/1000
2023-09-27 04:21:57.313 
Epoch 719/1000 
	 loss: 17.2331, MinusLogProbMetric: 17.2331, val_loss: 17.2563, val_MinusLogProbMetric: 17.2563

Epoch 719: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2331 - MinusLogProbMetric: 17.2331 - val_loss: 17.2563 - val_MinusLogProbMetric: 17.2563 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 720/1000
2023-09-27 04:23:11.632 
Epoch 720/1000 
	 loss: 17.2428, MinusLogProbMetric: 17.2428, val_loss: 17.2986, val_MinusLogProbMetric: 17.2986

Epoch 720: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2428 - MinusLogProbMetric: 17.2428 - val_loss: 17.2986 - val_MinusLogProbMetric: 17.2986 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 721/1000
2023-09-27 04:24:26.291 
Epoch 721/1000 
	 loss: 17.2336, MinusLogProbMetric: 17.2336, val_loss: 17.2884, val_MinusLogProbMetric: 17.2884

Epoch 721: val_loss did not improve from 17.20202
196/196 - 75s - loss: 17.2336 - MinusLogProbMetric: 17.2336 - val_loss: 17.2884 - val_MinusLogProbMetric: 17.2884 - lr: 1.6667e-04 - 75s/epoch - 381ms/step
Epoch 722/1000
2023-09-27 04:25:41.134 
Epoch 722/1000 
	 loss: 17.2032, MinusLogProbMetric: 17.2032, val_loss: 17.3675, val_MinusLogProbMetric: 17.3675

Epoch 722: val_loss did not improve from 17.20202
196/196 - 75s - loss: 17.2032 - MinusLogProbMetric: 17.2032 - val_loss: 17.3675 - val_MinusLogProbMetric: 17.3675 - lr: 1.6667e-04 - 75s/epoch - 382ms/step
Epoch 723/1000
2023-09-27 04:26:55.815 
Epoch 723/1000 
	 loss: 17.1968, MinusLogProbMetric: 17.1968, val_loss: 17.2575, val_MinusLogProbMetric: 17.2575

Epoch 723: val_loss did not improve from 17.20202
196/196 - 75s - loss: 17.1968 - MinusLogProbMetric: 17.1968 - val_loss: 17.2575 - val_MinusLogProbMetric: 17.2575 - lr: 1.6667e-04 - 75s/epoch - 381ms/step
Epoch 724/1000
2023-09-27 04:28:10.123 
Epoch 724/1000 
	 loss: 17.2131, MinusLogProbMetric: 17.2131, val_loss: 17.2388, val_MinusLogProbMetric: 17.2388

Epoch 724: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2131 - MinusLogProbMetric: 17.2131 - val_loss: 17.2388 - val_MinusLogProbMetric: 17.2388 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 725/1000
2023-09-27 04:29:24.433 
Epoch 725/1000 
	 loss: 17.2254, MinusLogProbMetric: 17.2254, val_loss: 17.2496, val_MinusLogProbMetric: 17.2496

Epoch 725: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2254 - MinusLogProbMetric: 17.2254 - val_loss: 17.2496 - val_MinusLogProbMetric: 17.2496 - lr: 1.6667e-04 - 74s/epoch - 379ms/step
Epoch 726/1000
2023-09-27 04:30:38.479 
Epoch 726/1000 
	 loss: 17.2455, MinusLogProbMetric: 17.2455, val_loss: 17.7182, val_MinusLogProbMetric: 17.7182

Epoch 726: val_loss did not improve from 17.20202
196/196 - 74s - loss: 17.2455 - MinusLogProbMetric: 17.2455 - val_loss: 17.7182 - val_MinusLogProbMetric: 17.7182 - lr: 1.6667e-04 - 74s/epoch - 378ms/step
Epoch 727/1000
2023-09-27 04:31:53.184 
Epoch 727/1000 
	 loss: 17.1284, MinusLogProbMetric: 17.1284, val_loss: 17.2317, val_MinusLogProbMetric: 17.2317

Epoch 727: val_loss did not improve from 17.20202
196/196 - 75s - loss: 17.1284 - MinusLogProbMetric: 17.1284 - val_loss: 17.2317 - val_MinusLogProbMetric: 17.2317 - lr: 8.3333e-05 - 75s/epoch - 381ms/step
Epoch 728/1000
2023-09-27 04:33:06.551 
Epoch 728/1000 
	 loss: 17.1135, MinusLogProbMetric: 17.1135, val_loss: 17.1879, val_MinusLogProbMetric: 17.1879

Epoch 728: val_loss improved from 17.20202 to 17.18785, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 17.1135 - MinusLogProbMetric: 17.1135 - val_loss: 17.1879 - val_MinusLogProbMetric: 17.1879 - lr: 8.3333e-05 - 75s/epoch - 381ms/step
Epoch 729/1000
2023-09-27 04:34:21.682 
Epoch 729/1000 
	 loss: 17.1085, MinusLogProbMetric: 17.1085, val_loss: 17.1899, val_MinusLogProbMetric: 17.1899

Epoch 729: val_loss did not improve from 17.18785
196/196 - 74s - loss: 17.1085 - MinusLogProbMetric: 17.1085 - val_loss: 17.1899 - val_MinusLogProbMetric: 17.1899 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 730/1000
2023-09-27 04:35:35.245 
Epoch 730/1000 
	 loss: 17.1133, MinusLogProbMetric: 17.1133, val_loss: 17.1762, val_MinusLogProbMetric: 17.1762

Epoch 730: val_loss improved from 17.18785 to 17.17617, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 17.1133 - MinusLogProbMetric: 17.1133 - val_loss: 17.1762 - val_MinusLogProbMetric: 17.1762 - lr: 8.3333e-05 - 75s/epoch - 383ms/step
Epoch 731/1000
2023-09-27 04:36:50.892 
Epoch 731/1000 
	 loss: 17.1087, MinusLogProbMetric: 17.1087, val_loss: 17.1992, val_MinusLogProbMetric: 17.1992

Epoch 731: val_loss did not improve from 17.17617
196/196 - 74s - loss: 17.1087 - MinusLogProbMetric: 17.1087 - val_loss: 17.1992 - val_MinusLogProbMetric: 17.1992 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 732/1000
2023-09-27 04:38:05.864 
Epoch 732/1000 
	 loss: 17.1099, MinusLogProbMetric: 17.1099, val_loss: 17.2477, val_MinusLogProbMetric: 17.2477

Epoch 732: val_loss did not improve from 17.17617
196/196 - 75s - loss: 17.1099 - MinusLogProbMetric: 17.1099 - val_loss: 17.2477 - val_MinusLogProbMetric: 17.2477 - lr: 8.3333e-05 - 75s/epoch - 382ms/step
Epoch 733/1000
2023-09-27 04:39:20.142 
Epoch 733/1000 
	 loss: 17.1099, MinusLogProbMetric: 17.1099, val_loss: 17.1568, val_MinusLogProbMetric: 17.1568

Epoch 733: val_loss improved from 17.17617 to 17.15682, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 17.1099 - MinusLogProbMetric: 17.1099 - val_loss: 17.1568 - val_MinusLogProbMetric: 17.1568 - lr: 8.3333e-05 - 75s/epoch - 385ms/step
Epoch 734/1000
2023-09-27 04:40:35.817 
Epoch 734/1000 
	 loss: 17.1167, MinusLogProbMetric: 17.1167, val_loss: 17.1591, val_MinusLogProbMetric: 17.1591

Epoch 734: val_loss did not improve from 17.15682
196/196 - 74s - loss: 17.1167 - MinusLogProbMetric: 17.1167 - val_loss: 17.1591 - val_MinusLogProbMetric: 17.1591 - lr: 8.3333e-05 - 74s/epoch - 380ms/step
Epoch 735/1000
2023-09-27 04:41:50.239 
Epoch 735/1000 
	 loss: 17.1053, MinusLogProbMetric: 17.1053, val_loss: 17.1907, val_MinusLogProbMetric: 17.1907

Epoch 735: val_loss did not improve from 17.15682
196/196 - 74s - loss: 17.1053 - MinusLogProbMetric: 17.1053 - val_loss: 17.1907 - val_MinusLogProbMetric: 17.1907 - lr: 8.3333e-05 - 74s/epoch - 380ms/step
Epoch 736/1000
2023-09-27 04:43:04.597 
Epoch 736/1000 
	 loss: 17.1104, MinusLogProbMetric: 17.1104, val_loss: 17.1754, val_MinusLogProbMetric: 17.1754

Epoch 736: val_loss did not improve from 17.15682
196/196 - 74s - loss: 17.1104 - MinusLogProbMetric: 17.1104 - val_loss: 17.1754 - val_MinusLogProbMetric: 17.1754 - lr: 8.3333e-05 - 74s/epoch - 379ms/step
Epoch 737/1000
2023-09-27 04:44:18.645 
Epoch 737/1000 
	 loss: 17.1150, MinusLogProbMetric: 17.1150, val_loss: 17.1971, val_MinusLogProbMetric: 17.1971

Epoch 737: val_loss did not improve from 17.15682
196/196 - 74s - loss: 17.1150 - MinusLogProbMetric: 17.1150 - val_loss: 17.1971 - val_MinusLogProbMetric: 17.1971 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 738/1000
2023-09-27 04:45:32.324 
Epoch 738/1000 
	 loss: 17.1188, MinusLogProbMetric: 17.1188, val_loss: 17.1936, val_MinusLogProbMetric: 17.1936

Epoch 738: val_loss did not improve from 17.15682
196/196 - 74s - loss: 17.1188 - MinusLogProbMetric: 17.1188 - val_loss: 17.1936 - val_MinusLogProbMetric: 17.1936 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 739/1000
2023-09-27 04:46:46.953 
Epoch 739/1000 
	 loss: 17.1084, MinusLogProbMetric: 17.1084, val_loss: 17.2161, val_MinusLogProbMetric: 17.2161

Epoch 739: val_loss did not improve from 17.15682
196/196 - 75s - loss: 17.1084 - MinusLogProbMetric: 17.1084 - val_loss: 17.2161 - val_MinusLogProbMetric: 17.2161 - lr: 8.3333e-05 - 75s/epoch - 381ms/step
Epoch 740/1000
2023-09-27 04:48:01.509 
Epoch 740/1000 
	 loss: 17.1184, MinusLogProbMetric: 17.1184, val_loss: 17.1820, val_MinusLogProbMetric: 17.1820

Epoch 740: val_loss did not improve from 17.15682
196/196 - 75s - loss: 17.1184 - MinusLogProbMetric: 17.1184 - val_loss: 17.1820 - val_MinusLogProbMetric: 17.1820 - lr: 8.3333e-05 - 75s/epoch - 380ms/step
Epoch 741/1000
2023-09-27 04:49:15.343 
Epoch 741/1000 
	 loss: 17.1100, MinusLogProbMetric: 17.1100, val_loss: 17.1912, val_MinusLogProbMetric: 17.1912

Epoch 741: val_loss did not improve from 17.15682
196/196 - 74s - loss: 17.1100 - MinusLogProbMetric: 17.1100 - val_loss: 17.1912 - val_MinusLogProbMetric: 17.1912 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 742/1000
2023-09-27 04:50:29.677 
Epoch 742/1000 
	 loss: 17.1090, MinusLogProbMetric: 17.1090, val_loss: 17.1595, val_MinusLogProbMetric: 17.1595

Epoch 742: val_loss did not improve from 17.15682
196/196 - 74s - loss: 17.1090 - MinusLogProbMetric: 17.1090 - val_loss: 17.1595 - val_MinusLogProbMetric: 17.1595 - lr: 8.3333e-05 - 74s/epoch - 379ms/step
Epoch 743/1000
2023-09-27 04:51:44.020 
Epoch 743/1000 
	 loss: 17.1173, MinusLogProbMetric: 17.1173, val_loss: 17.1668, val_MinusLogProbMetric: 17.1668

Epoch 743: val_loss did not improve from 17.15682
196/196 - 74s - loss: 17.1173 - MinusLogProbMetric: 17.1173 - val_loss: 17.1668 - val_MinusLogProbMetric: 17.1668 - lr: 8.3333e-05 - 74s/epoch - 379ms/step
Epoch 744/1000
2023-09-27 04:52:58.165 
Epoch 744/1000 
	 loss: 17.1100, MinusLogProbMetric: 17.1100, val_loss: 17.1870, val_MinusLogProbMetric: 17.1870

Epoch 744: val_loss did not improve from 17.15682
196/196 - 74s - loss: 17.1100 - MinusLogProbMetric: 17.1100 - val_loss: 17.1870 - val_MinusLogProbMetric: 17.1870 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 745/1000
2023-09-27 04:54:12.124 
Epoch 745/1000 
	 loss: 17.1054, MinusLogProbMetric: 17.1054, val_loss: 17.1541, val_MinusLogProbMetric: 17.1541

Epoch 745: val_loss improved from 17.15682 to 17.15409, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 17.1054 - MinusLogProbMetric: 17.1054 - val_loss: 17.1541 - val_MinusLogProbMetric: 17.1541 - lr: 8.3333e-05 - 75s/epoch - 384ms/step
Epoch 746/1000
2023-09-27 04:55:27.090 
Epoch 746/1000 
	 loss: 17.1111, MinusLogProbMetric: 17.1111, val_loss: 17.1983, val_MinusLogProbMetric: 17.1983

Epoch 746: val_loss did not improve from 17.15409
196/196 - 74s - loss: 17.1111 - MinusLogProbMetric: 17.1111 - val_loss: 17.1983 - val_MinusLogProbMetric: 17.1983 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 747/1000
2023-09-27 04:56:40.710 
Epoch 747/1000 
	 loss: 17.1047, MinusLogProbMetric: 17.1047, val_loss: 17.1929, val_MinusLogProbMetric: 17.1929

Epoch 747: val_loss did not improve from 17.15409
196/196 - 74s - loss: 17.1047 - MinusLogProbMetric: 17.1047 - val_loss: 17.1929 - val_MinusLogProbMetric: 17.1929 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 748/1000
2023-09-27 04:57:51.333 
Epoch 748/1000 
	 loss: 17.1098, MinusLogProbMetric: 17.1098, val_loss: 17.1691, val_MinusLogProbMetric: 17.1691

Epoch 748: val_loss did not improve from 17.15409
196/196 - 71s - loss: 17.1098 - MinusLogProbMetric: 17.1098 - val_loss: 17.1691 - val_MinusLogProbMetric: 17.1691 - lr: 8.3333e-05 - 71s/epoch - 360ms/step
Epoch 749/1000
2023-09-27 04:58:58.405 
Epoch 749/1000 
	 loss: 17.1069, MinusLogProbMetric: 17.1069, val_loss: 17.1596, val_MinusLogProbMetric: 17.1596

Epoch 749: val_loss did not improve from 17.15409
196/196 - 67s - loss: 17.1069 - MinusLogProbMetric: 17.1069 - val_loss: 17.1596 - val_MinusLogProbMetric: 17.1596 - lr: 8.3333e-05 - 67s/epoch - 342ms/step
Epoch 750/1000
2023-09-27 05:00:11.405 
Epoch 750/1000 
	 loss: 17.1055, MinusLogProbMetric: 17.1055, val_loss: 17.1593, val_MinusLogProbMetric: 17.1593

Epoch 750: val_loss did not improve from 17.15409
196/196 - 73s - loss: 17.1055 - MinusLogProbMetric: 17.1055 - val_loss: 17.1593 - val_MinusLogProbMetric: 17.1593 - lr: 8.3333e-05 - 73s/epoch - 372ms/step
Epoch 751/1000
2023-09-27 05:01:25.121 
Epoch 751/1000 
	 loss: 17.1074, MinusLogProbMetric: 17.1074, val_loss: 17.1750, val_MinusLogProbMetric: 17.1750

Epoch 751: val_loss did not improve from 17.15409
196/196 - 74s - loss: 17.1074 - MinusLogProbMetric: 17.1074 - val_loss: 17.1750 - val_MinusLogProbMetric: 17.1750 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 752/1000
2023-09-27 05:02:38.258 
Epoch 752/1000 
	 loss: 17.1009, MinusLogProbMetric: 17.1009, val_loss: 17.1887, val_MinusLogProbMetric: 17.1887

Epoch 752: val_loss did not improve from 17.15409
196/196 - 73s - loss: 17.1009 - MinusLogProbMetric: 17.1009 - val_loss: 17.1887 - val_MinusLogProbMetric: 17.1887 - lr: 8.3333e-05 - 73s/epoch - 373ms/step
Epoch 753/1000
2023-09-27 05:03:51.592 
Epoch 753/1000 
	 loss: 17.1060, MinusLogProbMetric: 17.1060, val_loss: 17.1486, val_MinusLogProbMetric: 17.1486

Epoch 753: val_loss improved from 17.15409 to 17.14862, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 17.1060 - MinusLogProbMetric: 17.1060 - val_loss: 17.1486 - val_MinusLogProbMetric: 17.1486 - lr: 8.3333e-05 - 75s/epoch - 382ms/step
Epoch 754/1000
2023-09-27 05:05:07.090 
Epoch 754/1000 
	 loss: 17.1137, MinusLogProbMetric: 17.1137, val_loss: 17.1625, val_MinusLogProbMetric: 17.1625

Epoch 754: val_loss did not improve from 17.14862
196/196 - 74s - loss: 17.1137 - MinusLogProbMetric: 17.1137 - val_loss: 17.1625 - val_MinusLogProbMetric: 17.1625 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 755/1000
2023-09-27 05:06:20.824 
Epoch 755/1000 
	 loss: 17.1123, MinusLogProbMetric: 17.1123, val_loss: 17.2402, val_MinusLogProbMetric: 17.2402

Epoch 755: val_loss did not improve from 17.14862
196/196 - 74s - loss: 17.1123 - MinusLogProbMetric: 17.1123 - val_loss: 17.2402 - val_MinusLogProbMetric: 17.2402 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 756/1000
2023-09-27 05:07:33.681 
Epoch 756/1000 
	 loss: 17.1046, MinusLogProbMetric: 17.1046, val_loss: 17.2900, val_MinusLogProbMetric: 17.2900

Epoch 756: val_loss did not improve from 17.14862
196/196 - 73s - loss: 17.1046 - MinusLogProbMetric: 17.1046 - val_loss: 17.2900 - val_MinusLogProbMetric: 17.2900 - lr: 8.3333e-05 - 73s/epoch - 372ms/step
Epoch 757/1000
2023-09-27 05:08:47.415 
Epoch 757/1000 
	 loss: 17.1079, MinusLogProbMetric: 17.1079, val_loss: 17.1901, val_MinusLogProbMetric: 17.1901

Epoch 757: val_loss did not improve from 17.14862
196/196 - 74s - loss: 17.1079 - MinusLogProbMetric: 17.1079 - val_loss: 17.1901 - val_MinusLogProbMetric: 17.1901 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 758/1000
2023-09-27 05:10:01.346 
Epoch 758/1000 
	 loss: 17.1096, MinusLogProbMetric: 17.1096, val_loss: 17.1410, val_MinusLogProbMetric: 17.1410

Epoch 758: val_loss improved from 17.14862 to 17.14097, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 17.1096 - MinusLogProbMetric: 17.1096 - val_loss: 17.1410 - val_MinusLogProbMetric: 17.1410 - lr: 8.3333e-05 - 75s/epoch - 383ms/step
Epoch 759/1000
2023-09-27 05:11:16.147 
Epoch 759/1000 
	 loss: 17.1105, MinusLogProbMetric: 17.1105, val_loss: 17.1451, val_MinusLogProbMetric: 17.1451

Epoch 759: val_loss did not improve from 17.14097
196/196 - 74s - loss: 17.1105 - MinusLogProbMetric: 17.1105 - val_loss: 17.1451 - val_MinusLogProbMetric: 17.1451 - lr: 8.3333e-05 - 74s/epoch - 375ms/step
Epoch 760/1000
2023-09-27 05:12:30.240 
Epoch 760/1000 
	 loss: 17.1142, MinusLogProbMetric: 17.1142, val_loss: 17.1746, val_MinusLogProbMetric: 17.1746

Epoch 760: val_loss did not improve from 17.14097
196/196 - 74s - loss: 17.1142 - MinusLogProbMetric: 17.1142 - val_loss: 17.1746 - val_MinusLogProbMetric: 17.1746 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 761/1000
2023-09-27 05:13:43.718 
Epoch 761/1000 
	 loss: 17.1016, MinusLogProbMetric: 17.1016, val_loss: 17.2263, val_MinusLogProbMetric: 17.2263

Epoch 761: val_loss did not improve from 17.14097
196/196 - 73s - loss: 17.1016 - MinusLogProbMetric: 17.1016 - val_loss: 17.2263 - val_MinusLogProbMetric: 17.2263 - lr: 8.3333e-05 - 73s/epoch - 375ms/step
Epoch 762/1000
2023-09-27 05:14:57.346 
Epoch 762/1000 
	 loss: 17.0983, MinusLogProbMetric: 17.0983, val_loss: 17.1603, val_MinusLogProbMetric: 17.1603

Epoch 762: val_loss did not improve from 17.14097
196/196 - 74s - loss: 17.0983 - MinusLogProbMetric: 17.0983 - val_loss: 17.1603 - val_MinusLogProbMetric: 17.1603 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 763/1000
2023-09-27 05:16:11.032 
Epoch 763/1000 
	 loss: 17.0959, MinusLogProbMetric: 17.0959, val_loss: 17.1887, val_MinusLogProbMetric: 17.1887

Epoch 763: val_loss did not improve from 17.14097
196/196 - 74s - loss: 17.0959 - MinusLogProbMetric: 17.0959 - val_loss: 17.1887 - val_MinusLogProbMetric: 17.1887 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 764/1000
2023-09-27 05:17:24.257 
Epoch 764/1000 
	 loss: 17.1048, MinusLogProbMetric: 17.1048, val_loss: 17.1421, val_MinusLogProbMetric: 17.1421

Epoch 764: val_loss did not improve from 17.14097
196/196 - 73s - loss: 17.1048 - MinusLogProbMetric: 17.1048 - val_loss: 17.1421 - val_MinusLogProbMetric: 17.1421 - lr: 8.3333e-05 - 73s/epoch - 374ms/step
Epoch 765/1000
2023-09-27 05:18:37.694 
Epoch 765/1000 
	 loss: 17.1035, MinusLogProbMetric: 17.1035, val_loss: 17.1805, val_MinusLogProbMetric: 17.1805

Epoch 765: val_loss did not improve from 17.14097
196/196 - 73s - loss: 17.1035 - MinusLogProbMetric: 17.1035 - val_loss: 17.1805 - val_MinusLogProbMetric: 17.1805 - lr: 8.3333e-05 - 73s/epoch - 375ms/step
Epoch 766/1000
2023-09-27 05:19:51.208 
Epoch 766/1000 
	 loss: 17.0971, MinusLogProbMetric: 17.0971, val_loss: 17.1512, val_MinusLogProbMetric: 17.1512

Epoch 766: val_loss did not improve from 17.14097
196/196 - 74s - loss: 17.0971 - MinusLogProbMetric: 17.0971 - val_loss: 17.1512 - val_MinusLogProbMetric: 17.1512 - lr: 8.3333e-05 - 74s/epoch - 375ms/step
Epoch 767/1000
2023-09-27 05:21:04.845 
Epoch 767/1000 
	 loss: 17.0980, MinusLogProbMetric: 17.0980, val_loss: 17.1750, val_MinusLogProbMetric: 17.1750

Epoch 767: val_loss did not improve from 17.14097
196/196 - 74s - loss: 17.0980 - MinusLogProbMetric: 17.0980 - val_loss: 17.1750 - val_MinusLogProbMetric: 17.1750 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 768/1000
2023-09-27 05:22:16.092 
Epoch 768/1000 
	 loss: 17.1069, MinusLogProbMetric: 17.1069, val_loss: 17.1568, val_MinusLogProbMetric: 17.1568

Epoch 768: val_loss did not improve from 17.14097
196/196 - 71s - loss: 17.1069 - MinusLogProbMetric: 17.1069 - val_loss: 17.1568 - val_MinusLogProbMetric: 17.1568 - lr: 8.3333e-05 - 71s/epoch - 363ms/step
Epoch 769/1000
2023-09-27 05:23:24.628 
Epoch 769/1000 
	 loss: 17.1086, MinusLogProbMetric: 17.1086, val_loss: 17.1907, val_MinusLogProbMetric: 17.1907

Epoch 769: val_loss did not improve from 17.14097
196/196 - 69s - loss: 17.1086 - MinusLogProbMetric: 17.1086 - val_loss: 17.1907 - val_MinusLogProbMetric: 17.1907 - lr: 8.3333e-05 - 69s/epoch - 350ms/step
Epoch 770/1000
2023-09-27 05:24:37.476 
Epoch 770/1000 
	 loss: 17.0991, MinusLogProbMetric: 17.0991, val_loss: 17.2817, val_MinusLogProbMetric: 17.2817

Epoch 770: val_loss did not improve from 17.14097
196/196 - 73s - loss: 17.0991 - MinusLogProbMetric: 17.0991 - val_loss: 17.2817 - val_MinusLogProbMetric: 17.2817 - lr: 8.3333e-05 - 73s/epoch - 372ms/step
Epoch 771/1000
2023-09-27 05:25:50.301 
Epoch 771/1000 
	 loss: 17.1013, MinusLogProbMetric: 17.1013, val_loss: 17.1449, val_MinusLogProbMetric: 17.1449

Epoch 771: val_loss did not improve from 17.14097
196/196 - 73s - loss: 17.1013 - MinusLogProbMetric: 17.1013 - val_loss: 17.1449 - val_MinusLogProbMetric: 17.1449 - lr: 8.3333e-05 - 73s/epoch - 372ms/step
Epoch 772/1000
2023-09-27 05:27:03.181 
Epoch 772/1000 
	 loss: 17.1036, MinusLogProbMetric: 17.1036, val_loss: 17.1588, val_MinusLogProbMetric: 17.1588

Epoch 772: val_loss did not improve from 17.14097
196/196 - 73s - loss: 17.1036 - MinusLogProbMetric: 17.1036 - val_loss: 17.1588 - val_MinusLogProbMetric: 17.1588 - lr: 8.3333e-05 - 73s/epoch - 372ms/step
Epoch 773/1000
2023-09-27 05:28:16.038 
Epoch 773/1000 
	 loss: 17.1052, MinusLogProbMetric: 17.1052, val_loss: 17.1845, val_MinusLogProbMetric: 17.1845

Epoch 773: val_loss did not improve from 17.14097
196/196 - 73s - loss: 17.1052 - MinusLogProbMetric: 17.1052 - val_loss: 17.1845 - val_MinusLogProbMetric: 17.1845 - lr: 8.3333e-05 - 73s/epoch - 372ms/step
Epoch 774/1000
2023-09-27 05:29:29.003 
Epoch 774/1000 
	 loss: 17.0885, MinusLogProbMetric: 17.0885, val_loss: 17.1424, val_MinusLogProbMetric: 17.1424

Epoch 774: val_loss did not improve from 17.14097
196/196 - 73s - loss: 17.0885 - MinusLogProbMetric: 17.0885 - val_loss: 17.1424 - val_MinusLogProbMetric: 17.1424 - lr: 8.3333e-05 - 73s/epoch - 372ms/step
Epoch 775/1000
2023-09-27 05:30:41.902 
Epoch 775/1000 
	 loss: 17.1057, MinusLogProbMetric: 17.1057, val_loss: 17.2930, val_MinusLogProbMetric: 17.2930

Epoch 775: val_loss did not improve from 17.14097
196/196 - 73s - loss: 17.1057 - MinusLogProbMetric: 17.1057 - val_loss: 17.2930 - val_MinusLogProbMetric: 17.2930 - lr: 8.3333e-05 - 73s/epoch - 372ms/step
Epoch 776/1000
2023-09-27 05:31:54.649 
Epoch 776/1000 
	 loss: 17.1097, MinusLogProbMetric: 17.1097, val_loss: 17.1819, val_MinusLogProbMetric: 17.1819

Epoch 776: val_loss did not improve from 17.14097
196/196 - 73s - loss: 17.1097 - MinusLogProbMetric: 17.1097 - val_loss: 17.1819 - val_MinusLogProbMetric: 17.1819 - lr: 8.3333e-05 - 73s/epoch - 371ms/step
Epoch 777/1000
2023-09-27 05:33:04.165 
Epoch 777/1000 
	 loss: 17.0967, MinusLogProbMetric: 17.0967, val_loss: 17.1734, val_MinusLogProbMetric: 17.1734

Epoch 777: val_loss did not improve from 17.14097
196/196 - 70s - loss: 17.0967 - MinusLogProbMetric: 17.0967 - val_loss: 17.1734 - val_MinusLogProbMetric: 17.1734 - lr: 8.3333e-05 - 70s/epoch - 355ms/step
Epoch 778/1000
2023-09-27 05:34:11.168 
Epoch 778/1000 
	 loss: 17.0942, MinusLogProbMetric: 17.0942, val_loss: 17.1626, val_MinusLogProbMetric: 17.1626

Epoch 778: val_loss did not improve from 17.14097
196/196 - 67s - loss: 17.0942 - MinusLogProbMetric: 17.0942 - val_loss: 17.1626 - val_MinusLogProbMetric: 17.1626 - lr: 8.3333e-05 - 67s/epoch - 342ms/step
Epoch 779/1000
2023-09-27 05:35:22.977 
Epoch 779/1000 
	 loss: 17.0902, MinusLogProbMetric: 17.0902, val_loss: 17.1618, val_MinusLogProbMetric: 17.1618

Epoch 779: val_loss did not improve from 17.14097
196/196 - 72s - loss: 17.0902 - MinusLogProbMetric: 17.0902 - val_loss: 17.1618 - val_MinusLogProbMetric: 17.1618 - lr: 8.3333e-05 - 72s/epoch - 366ms/step
Epoch 780/1000
2023-09-27 05:36:28.619 
Epoch 780/1000 
	 loss: 17.1006, MinusLogProbMetric: 17.1006, val_loss: 17.1240, val_MinusLogProbMetric: 17.1240

Epoch 780: val_loss improved from 17.14097 to 17.12396, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 67s - loss: 17.1006 - MinusLogProbMetric: 17.1006 - val_loss: 17.1240 - val_MinusLogProbMetric: 17.1240 - lr: 8.3333e-05 - 67s/epoch - 342ms/step
Epoch 781/1000
2023-09-27 05:37:44.166 
Epoch 781/1000 
	 loss: 17.0957, MinusLogProbMetric: 17.0957, val_loss: 17.2001, val_MinusLogProbMetric: 17.2001

Epoch 781: val_loss did not improve from 17.12396
196/196 - 74s - loss: 17.0957 - MinusLogProbMetric: 17.0957 - val_loss: 17.2001 - val_MinusLogProbMetric: 17.2001 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 782/1000
2023-09-27 05:38:57.870 
Epoch 782/1000 
	 loss: 17.1091, MinusLogProbMetric: 17.1091, val_loss: 17.1406, val_MinusLogProbMetric: 17.1406

Epoch 782: val_loss did not improve from 17.12396
196/196 - 74s - loss: 17.1091 - MinusLogProbMetric: 17.1091 - val_loss: 17.1406 - val_MinusLogProbMetric: 17.1406 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 783/1000
2023-09-27 05:40:12.146 
Epoch 783/1000 
	 loss: 17.0995, MinusLogProbMetric: 17.0995, val_loss: 17.1546, val_MinusLogProbMetric: 17.1546

Epoch 783: val_loss did not improve from 17.12396
196/196 - 74s - loss: 17.0995 - MinusLogProbMetric: 17.0995 - val_loss: 17.1546 - val_MinusLogProbMetric: 17.1546 - lr: 8.3333e-05 - 74s/epoch - 379ms/step
Epoch 784/1000
2023-09-27 05:41:25.034 
Epoch 784/1000 
	 loss: 17.0945, MinusLogProbMetric: 17.0945, val_loss: 17.1962, val_MinusLogProbMetric: 17.1962

Epoch 784: val_loss did not improve from 17.12396
196/196 - 73s - loss: 17.0945 - MinusLogProbMetric: 17.0945 - val_loss: 17.1962 - val_MinusLogProbMetric: 17.1962 - lr: 8.3333e-05 - 73s/epoch - 372ms/step
Epoch 785/1000
2023-09-27 05:42:38.118 
Epoch 785/1000 
	 loss: 17.0976, MinusLogProbMetric: 17.0976, val_loss: 17.1462, val_MinusLogProbMetric: 17.1462

Epoch 785: val_loss did not improve from 17.12396
196/196 - 73s - loss: 17.0976 - MinusLogProbMetric: 17.0976 - val_loss: 17.1462 - val_MinusLogProbMetric: 17.1462 - lr: 8.3333e-05 - 73s/epoch - 373ms/step
Epoch 786/1000
2023-09-27 05:43:51.719 
Epoch 786/1000 
	 loss: 17.0961, MinusLogProbMetric: 17.0961, val_loss: 17.1927, val_MinusLogProbMetric: 17.1927

Epoch 786: val_loss did not improve from 17.12396
196/196 - 74s - loss: 17.0961 - MinusLogProbMetric: 17.0961 - val_loss: 17.1927 - val_MinusLogProbMetric: 17.1927 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 787/1000
2023-09-27 05:45:04.506 
Epoch 787/1000 
	 loss: 17.0871, MinusLogProbMetric: 17.0871, val_loss: 17.1357, val_MinusLogProbMetric: 17.1357

Epoch 787: val_loss did not improve from 17.12396
196/196 - 73s - loss: 17.0871 - MinusLogProbMetric: 17.0871 - val_loss: 17.1357 - val_MinusLogProbMetric: 17.1357 - lr: 8.3333e-05 - 73s/epoch - 371ms/step
Epoch 788/1000
2023-09-27 05:46:18.253 
Epoch 788/1000 
	 loss: 17.0929, MinusLogProbMetric: 17.0929, val_loss: 17.2611, val_MinusLogProbMetric: 17.2611

Epoch 788: val_loss did not improve from 17.12396
196/196 - 74s - loss: 17.0929 - MinusLogProbMetric: 17.0929 - val_loss: 17.2611 - val_MinusLogProbMetric: 17.2611 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 789/1000
2023-09-27 05:47:32.169 
Epoch 789/1000 
	 loss: 17.0914, MinusLogProbMetric: 17.0914, val_loss: 17.2325, val_MinusLogProbMetric: 17.2325

Epoch 789: val_loss did not improve from 17.12396
196/196 - 74s - loss: 17.0914 - MinusLogProbMetric: 17.0914 - val_loss: 17.2325 - val_MinusLogProbMetric: 17.2325 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 790/1000
2023-09-27 05:48:45.723 
Epoch 790/1000 
	 loss: 17.1018, MinusLogProbMetric: 17.1018, val_loss: 17.1466, val_MinusLogProbMetric: 17.1466

Epoch 790: val_loss did not improve from 17.12396
196/196 - 74s - loss: 17.1018 - MinusLogProbMetric: 17.1018 - val_loss: 17.1466 - val_MinusLogProbMetric: 17.1466 - lr: 8.3333e-05 - 74s/epoch - 375ms/step
Epoch 791/1000
2023-09-27 05:49:58.972 
Epoch 791/1000 
	 loss: 17.0982, MinusLogProbMetric: 17.0982, val_loss: 17.1335, val_MinusLogProbMetric: 17.1335

Epoch 791: val_loss did not improve from 17.12396
196/196 - 73s - loss: 17.0982 - MinusLogProbMetric: 17.0982 - val_loss: 17.1335 - val_MinusLogProbMetric: 17.1335 - lr: 8.3333e-05 - 73s/epoch - 374ms/step
Epoch 792/1000
2023-09-27 05:51:12.527 
Epoch 792/1000 
	 loss: 17.0976, MinusLogProbMetric: 17.0976, val_loss: 17.1994, val_MinusLogProbMetric: 17.1994

Epoch 792: val_loss did not improve from 17.12396
196/196 - 74s - loss: 17.0976 - MinusLogProbMetric: 17.0976 - val_loss: 17.1994 - val_MinusLogProbMetric: 17.1994 - lr: 8.3333e-05 - 74s/epoch - 375ms/step
Epoch 793/1000
2023-09-27 05:52:26.767 
Epoch 793/1000 
	 loss: 17.0862, MinusLogProbMetric: 17.0862, val_loss: 17.1233, val_MinusLogProbMetric: 17.1233

Epoch 793: val_loss improved from 17.12396 to 17.12325, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 76s - loss: 17.0862 - MinusLogProbMetric: 17.0862 - val_loss: 17.1233 - val_MinusLogProbMetric: 17.1233 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 794/1000
2023-09-27 05:53:42.233 
Epoch 794/1000 
	 loss: 17.0898, MinusLogProbMetric: 17.0898, val_loss: 17.1797, val_MinusLogProbMetric: 17.1797

Epoch 794: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0898 - MinusLogProbMetric: 17.0898 - val_loss: 17.1797 - val_MinusLogProbMetric: 17.1797 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 795/1000
2023-09-27 05:54:56.090 
Epoch 795/1000 
	 loss: 17.0989, MinusLogProbMetric: 17.0989, val_loss: 17.1663, val_MinusLogProbMetric: 17.1663

Epoch 795: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0989 - MinusLogProbMetric: 17.0989 - val_loss: 17.1663 - val_MinusLogProbMetric: 17.1663 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 796/1000
2023-09-27 05:56:09.597 
Epoch 796/1000 
	 loss: 17.1000, MinusLogProbMetric: 17.1000, val_loss: 17.1437, val_MinusLogProbMetric: 17.1437

Epoch 796: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.1000 - MinusLogProbMetric: 17.1000 - val_loss: 17.1437 - val_MinusLogProbMetric: 17.1437 - lr: 8.3333e-05 - 74s/epoch - 375ms/step
Epoch 797/1000
2023-09-27 05:57:23.665 
Epoch 797/1000 
	 loss: 17.0952, MinusLogProbMetric: 17.0952, val_loss: 17.2094, val_MinusLogProbMetric: 17.2094

Epoch 797: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0952 - MinusLogProbMetric: 17.0952 - val_loss: 17.2094 - val_MinusLogProbMetric: 17.2094 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 798/1000
2023-09-27 05:58:37.299 
Epoch 798/1000 
	 loss: 17.0955, MinusLogProbMetric: 17.0955, val_loss: 17.1562, val_MinusLogProbMetric: 17.1562

Epoch 798: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0955 - MinusLogProbMetric: 17.0955 - val_loss: 17.1562 - val_MinusLogProbMetric: 17.1562 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 799/1000
2023-09-27 05:59:50.456 
Epoch 799/1000 
	 loss: 17.0860, MinusLogProbMetric: 17.0860, val_loss: 17.1710, val_MinusLogProbMetric: 17.1710

Epoch 799: val_loss did not improve from 17.12325
196/196 - 73s - loss: 17.0860 - MinusLogProbMetric: 17.0860 - val_loss: 17.1710 - val_MinusLogProbMetric: 17.1710 - lr: 8.3333e-05 - 73s/epoch - 373ms/step
Epoch 800/1000
2023-09-27 06:01:03.986 
Epoch 800/1000 
	 loss: 17.0845, MinusLogProbMetric: 17.0845, val_loss: 17.1728, val_MinusLogProbMetric: 17.1728

Epoch 800: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0845 - MinusLogProbMetric: 17.0845 - val_loss: 17.1728 - val_MinusLogProbMetric: 17.1728 - lr: 8.3333e-05 - 74s/epoch - 375ms/step
Epoch 801/1000
2023-09-27 06:02:18.063 
Epoch 801/1000 
	 loss: 17.0924, MinusLogProbMetric: 17.0924, val_loss: 17.1689, val_MinusLogProbMetric: 17.1689

Epoch 801: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0924 - MinusLogProbMetric: 17.0924 - val_loss: 17.1689 - val_MinusLogProbMetric: 17.1689 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 802/1000
2023-09-27 06:03:31.325 
Epoch 802/1000 
	 loss: 17.0876, MinusLogProbMetric: 17.0876, val_loss: 17.1929, val_MinusLogProbMetric: 17.1929

Epoch 802: val_loss did not improve from 17.12325
196/196 - 73s - loss: 17.0876 - MinusLogProbMetric: 17.0876 - val_loss: 17.1929 - val_MinusLogProbMetric: 17.1929 - lr: 8.3333e-05 - 73s/epoch - 374ms/step
Epoch 803/1000
2023-09-27 06:04:45.587 
Epoch 803/1000 
	 loss: 17.0879, MinusLogProbMetric: 17.0879, val_loss: 17.1758, val_MinusLogProbMetric: 17.1758

Epoch 803: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0879 - MinusLogProbMetric: 17.0879 - val_loss: 17.1758 - val_MinusLogProbMetric: 17.1758 - lr: 8.3333e-05 - 74s/epoch - 379ms/step
Epoch 804/1000
2023-09-27 06:05:59.378 
Epoch 804/1000 
	 loss: 17.0905, MinusLogProbMetric: 17.0905, val_loss: 17.1354, val_MinusLogProbMetric: 17.1354

Epoch 804: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0905 - MinusLogProbMetric: 17.0905 - val_loss: 17.1354 - val_MinusLogProbMetric: 17.1354 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 805/1000
2023-09-27 06:07:13.108 
Epoch 805/1000 
	 loss: 17.0950, MinusLogProbMetric: 17.0950, val_loss: 17.1809, val_MinusLogProbMetric: 17.1809

Epoch 805: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0950 - MinusLogProbMetric: 17.0950 - val_loss: 17.1809 - val_MinusLogProbMetric: 17.1809 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 806/1000
2023-09-27 06:08:26.887 
Epoch 806/1000 
	 loss: 17.0902, MinusLogProbMetric: 17.0902, val_loss: 17.1879, val_MinusLogProbMetric: 17.1879

Epoch 806: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0902 - MinusLogProbMetric: 17.0902 - val_loss: 17.1879 - val_MinusLogProbMetric: 17.1879 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 807/1000
2023-09-27 06:09:40.655 
Epoch 807/1000 
	 loss: 17.0890, MinusLogProbMetric: 17.0890, val_loss: 17.1418, val_MinusLogProbMetric: 17.1418

Epoch 807: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0890 - MinusLogProbMetric: 17.0890 - val_loss: 17.1418 - val_MinusLogProbMetric: 17.1418 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 808/1000
2023-09-27 06:10:54.772 
Epoch 808/1000 
	 loss: 17.0854, MinusLogProbMetric: 17.0854, val_loss: 17.1963, val_MinusLogProbMetric: 17.1963

Epoch 808: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0854 - MinusLogProbMetric: 17.0854 - val_loss: 17.1963 - val_MinusLogProbMetric: 17.1963 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 809/1000
2023-09-27 06:12:07.866 
Epoch 809/1000 
	 loss: 17.0911, MinusLogProbMetric: 17.0911, val_loss: 17.1334, val_MinusLogProbMetric: 17.1334

Epoch 809: val_loss did not improve from 17.12325
196/196 - 73s - loss: 17.0911 - MinusLogProbMetric: 17.0911 - val_loss: 17.1334 - val_MinusLogProbMetric: 17.1334 - lr: 8.3333e-05 - 73s/epoch - 373ms/step
Epoch 810/1000
2023-09-27 06:13:21.078 
Epoch 810/1000 
	 loss: 17.0968, MinusLogProbMetric: 17.0968, val_loss: 17.1543, val_MinusLogProbMetric: 17.1543

Epoch 810: val_loss did not improve from 17.12325
196/196 - 73s - loss: 17.0968 - MinusLogProbMetric: 17.0968 - val_loss: 17.1543 - val_MinusLogProbMetric: 17.1543 - lr: 8.3333e-05 - 73s/epoch - 374ms/step
Epoch 811/1000
2023-09-27 06:14:34.589 
Epoch 811/1000 
	 loss: 17.0947, MinusLogProbMetric: 17.0947, val_loss: 17.1439, val_MinusLogProbMetric: 17.1439

Epoch 811: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0947 - MinusLogProbMetric: 17.0947 - val_loss: 17.1439 - val_MinusLogProbMetric: 17.1439 - lr: 8.3333e-05 - 74s/epoch - 375ms/step
Epoch 812/1000
2023-09-27 06:15:48.286 
Epoch 812/1000 
	 loss: 17.0999, MinusLogProbMetric: 17.0999, val_loss: 17.1457, val_MinusLogProbMetric: 17.1457

Epoch 812: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0999 - MinusLogProbMetric: 17.0999 - val_loss: 17.1457 - val_MinusLogProbMetric: 17.1457 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 813/1000
2023-09-27 06:17:02.407 
Epoch 813/1000 
	 loss: 17.0895, MinusLogProbMetric: 17.0895, val_loss: 17.1329, val_MinusLogProbMetric: 17.1329

Epoch 813: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0895 - MinusLogProbMetric: 17.0895 - val_loss: 17.1329 - val_MinusLogProbMetric: 17.1329 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 814/1000
2023-09-27 06:18:17.005 
Epoch 814/1000 
	 loss: 17.0932, MinusLogProbMetric: 17.0932, val_loss: 17.2018, val_MinusLogProbMetric: 17.2018

Epoch 814: val_loss did not improve from 17.12325
196/196 - 75s - loss: 17.0932 - MinusLogProbMetric: 17.0932 - val_loss: 17.2018 - val_MinusLogProbMetric: 17.2018 - lr: 8.3333e-05 - 75s/epoch - 381ms/step
Epoch 815/1000
2023-09-27 06:19:30.727 
Epoch 815/1000 
	 loss: 17.0963, MinusLogProbMetric: 17.0963, val_loss: 17.1797, val_MinusLogProbMetric: 17.1797

Epoch 815: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0963 - MinusLogProbMetric: 17.0963 - val_loss: 17.1797 - val_MinusLogProbMetric: 17.1797 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 816/1000
2023-09-27 06:20:44.140 
Epoch 816/1000 
	 loss: 17.0876, MinusLogProbMetric: 17.0876, val_loss: 17.1397, val_MinusLogProbMetric: 17.1397

Epoch 816: val_loss did not improve from 17.12325
196/196 - 73s - loss: 17.0876 - MinusLogProbMetric: 17.0876 - val_loss: 17.1397 - val_MinusLogProbMetric: 17.1397 - lr: 8.3333e-05 - 73s/epoch - 375ms/step
Epoch 817/1000
2023-09-27 06:21:58.347 
Epoch 817/1000 
	 loss: 17.0920, MinusLogProbMetric: 17.0920, val_loss: 17.1349, val_MinusLogProbMetric: 17.1349

Epoch 817: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0920 - MinusLogProbMetric: 17.0920 - val_loss: 17.1349 - val_MinusLogProbMetric: 17.1349 - lr: 8.3333e-05 - 74s/epoch - 379ms/step
Epoch 818/1000
2023-09-27 06:23:11.637 
Epoch 818/1000 
	 loss: 17.0847, MinusLogProbMetric: 17.0847, val_loss: 17.1807, val_MinusLogProbMetric: 17.1807

Epoch 818: val_loss did not improve from 17.12325
196/196 - 73s - loss: 17.0847 - MinusLogProbMetric: 17.0847 - val_loss: 17.1807 - val_MinusLogProbMetric: 17.1807 - lr: 8.3333e-05 - 73s/epoch - 374ms/step
Epoch 819/1000
2023-09-27 06:24:25.513 
Epoch 819/1000 
	 loss: 17.0856, MinusLogProbMetric: 17.0856, val_loss: 17.1436, val_MinusLogProbMetric: 17.1436

Epoch 819: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0856 - MinusLogProbMetric: 17.0856 - val_loss: 17.1436 - val_MinusLogProbMetric: 17.1436 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 820/1000
2023-09-27 06:25:39.220 
Epoch 820/1000 
	 loss: 17.0944, MinusLogProbMetric: 17.0944, val_loss: 17.1693, val_MinusLogProbMetric: 17.1693

Epoch 820: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0944 - MinusLogProbMetric: 17.0944 - val_loss: 17.1693 - val_MinusLogProbMetric: 17.1693 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 821/1000
2023-09-27 06:26:53.877 
Epoch 821/1000 
	 loss: 17.0804, MinusLogProbMetric: 17.0804, val_loss: 17.1937, val_MinusLogProbMetric: 17.1937

Epoch 821: val_loss did not improve from 17.12325
196/196 - 75s - loss: 17.0804 - MinusLogProbMetric: 17.0804 - val_loss: 17.1937 - val_MinusLogProbMetric: 17.1937 - lr: 8.3333e-05 - 75s/epoch - 381ms/step
Epoch 822/1000
2023-09-27 06:28:07.506 
Epoch 822/1000 
	 loss: 17.0832, MinusLogProbMetric: 17.0832, val_loss: 17.1746, val_MinusLogProbMetric: 17.1746

Epoch 822: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0832 - MinusLogProbMetric: 17.0832 - val_loss: 17.1746 - val_MinusLogProbMetric: 17.1746 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 823/1000
2023-09-27 06:29:21.223 
Epoch 823/1000 
	 loss: 17.0951, MinusLogProbMetric: 17.0951, val_loss: 17.1600, val_MinusLogProbMetric: 17.1600

Epoch 823: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0951 - MinusLogProbMetric: 17.0951 - val_loss: 17.1600 - val_MinusLogProbMetric: 17.1600 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 824/1000
2023-09-27 06:30:35.289 
Epoch 824/1000 
	 loss: 17.0866, MinusLogProbMetric: 17.0866, val_loss: 17.2559, val_MinusLogProbMetric: 17.2559

Epoch 824: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0866 - MinusLogProbMetric: 17.0866 - val_loss: 17.2559 - val_MinusLogProbMetric: 17.2559 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 825/1000
2023-09-27 06:31:49.125 
Epoch 825/1000 
	 loss: 17.0861, MinusLogProbMetric: 17.0861, val_loss: 17.1723, val_MinusLogProbMetric: 17.1723

Epoch 825: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0861 - MinusLogProbMetric: 17.0861 - val_loss: 17.1723 - val_MinusLogProbMetric: 17.1723 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 826/1000
2023-09-27 06:33:02.773 
Epoch 826/1000 
	 loss: 17.0865, MinusLogProbMetric: 17.0865, val_loss: 17.1513, val_MinusLogProbMetric: 17.1513

Epoch 826: val_loss did not improve from 17.12325
196/196 - 74s - loss: 17.0865 - MinusLogProbMetric: 17.0865 - val_loss: 17.1513 - val_MinusLogProbMetric: 17.1513 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 827/1000
2023-09-27 06:34:16.664 
Epoch 827/1000 
	 loss: 17.0801, MinusLogProbMetric: 17.0801, val_loss: 17.1136, val_MinusLogProbMetric: 17.1136

Epoch 827: val_loss improved from 17.12325 to 17.11364, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 17.0801 - MinusLogProbMetric: 17.0801 - val_loss: 17.1136 - val_MinusLogProbMetric: 17.1136 - lr: 8.3333e-05 - 75s/epoch - 384ms/step
Epoch 828/1000
2023-09-27 06:35:32.143 
Epoch 828/1000 
	 loss: 17.0774, MinusLogProbMetric: 17.0774, val_loss: 17.2130, val_MinusLogProbMetric: 17.2130

Epoch 828: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0774 - MinusLogProbMetric: 17.0774 - val_loss: 17.2130 - val_MinusLogProbMetric: 17.2130 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 829/1000
2023-09-27 06:36:45.848 
Epoch 829/1000 
	 loss: 17.0922, MinusLogProbMetric: 17.0922, val_loss: 17.2021, val_MinusLogProbMetric: 17.2021

Epoch 829: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0922 - MinusLogProbMetric: 17.0922 - val_loss: 17.2021 - val_MinusLogProbMetric: 17.2021 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 830/1000
2023-09-27 06:37:59.349 
Epoch 830/1000 
	 loss: 17.0989, MinusLogProbMetric: 17.0989, val_loss: 17.1754, val_MinusLogProbMetric: 17.1754

Epoch 830: val_loss did not improve from 17.11364
196/196 - 73s - loss: 17.0989 - MinusLogProbMetric: 17.0989 - val_loss: 17.1754 - val_MinusLogProbMetric: 17.1754 - lr: 8.3333e-05 - 73s/epoch - 375ms/step
Epoch 831/1000
2023-09-27 06:39:12.654 
Epoch 831/1000 
	 loss: 17.0841, MinusLogProbMetric: 17.0841, val_loss: 17.1340, val_MinusLogProbMetric: 17.1340

Epoch 831: val_loss did not improve from 17.11364
196/196 - 73s - loss: 17.0841 - MinusLogProbMetric: 17.0841 - val_loss: 17.1340 - val_MinusLogProbMetric: 17.1340 - lr: 8.3333e-05 - 73s/epoch - 374ms/step
Epoch 832/1000
2023-09-27 06:40:26.520 
Epoch 832/1000 
	 loss: 17.0845, MinusLogProbMetric: 17.0845, val_loss: 17.1550, val_MinusLogProbMetric: 17.1550

Epoch 832: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0845 - MinusLogProbMetric: 17.0845 - val_loss: 17.1550 - val_MinusLogProbMetric: 17.1550 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 833/1000
2023-09-27 06:41:40.400 
Epoch 833/1000 
	 loss: 17.0863, MinusLogProbMetric: 17.0863, val_loss: 17.2099, val_MinusLogProbMetric: 17.2099

Epoch 833: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0863 - MinusLogProbMetric: 17.0863 - val_loss: 17.2099 - val_MinusLogProbMetric: 17.2099 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 834/1000
2023-09-27 06:42:54.543 
Epoch 834/1000 
	 loss: 17.0847, MinusLogProbMetric: 17.0847, val_loss: 17.1380, val_MinusLogProbMetric: 17.1380

Epoch 834: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0847 - MinusLogProbMetric: 17.0847 - val_loss: 17.1380 - val_MinusLogProbMetric: 17.1380 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 835/1000
2023-09-27 06:44:08.865 
Epoch 835/1000 
	 loss: 17.0858, MinusLogProbMetric: 17.0858, val_loss: 17.1258, val_MinusLogProbMetric: 17.1258

Epoch 835: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0858 - MinusLogProbMetric: 17.0858 - val_loss: 17.1258 - val_MinusLogProbMetric: 17.1258 - lr: 8.3333e-05 - 74s/epoch - 379ms/step
Epoch 836/1000
2023-09-27 06:45:22.711 
Epoch 836/1000 
	 loss: 17.0887, MinusLogProbMetric: 17.0887, val_loss: 17.1392, val_MinusLogProbMetric: 17.1392

Epoch 836: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0887 - MinusLogProbMetric: 17.0887 - val_loss: 17.1392 - val_MinusLogProbMetric: 17.1392 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 837/1000
2023-09-27 06:46:37.192 
Epoch 837/1000 
	 loss: 17.0855, MinusLogProbMetric: 17.0855, val_loss: 17.1618, val_MinusLogProbMetric: 17.1618

Epoch 837: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0855 - MinusLogProbMetric: 17.0855 - val_loss: 17.1618 - val_MinusLogProbMetric: 17.1618 - lr: 8.3333e-05 - 74s/epoch - 380ms/step
Epoch 838/1000
2023-09-27 06:47:51.193 
Epoch 838/1000 
	 loss: 17.0844, MinusLogProbMetric: 17.0844, val_loss: 17.1530, val_MinusLogProbMetric: 17.1530

Epoch 838: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0844 - MinusLogProbMetric: 17.0844 - val_loss: 17.1530 - val_MinusLogProbMetric: 17.1530 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 839/1000
2023-09-27 06:49:05.001 
Epoch 839/1000 
	 loss: 17.0873, MinusLogProbMetric: 17.0873, val_loss: 17.1673, val_MinusLogProbMetric: 17.1673

Epoch 839: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0873 - MinusLogProbMetric: 17.0873 - val_loss: 17.1673 - val_MinusLogProbMetric: 17.1673 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 840/1000
2023-09-27 06:50:18.692 
Epoch 840/1000 
	 loss: 17.0866, MinusLogProbMetric: 17.0866, val_loss: 17.1397, val_MinusLogProbMetric: 17.1397

Epoch 840: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0866 - MinusLogProbMetric: 17.0866 - val_loss: 17.1397 - val_MinusLogProbMetric: 17.1397 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 841/1000
2023-09-27 06:51:31.865 
Epoch 841/1000 
	 loss: 17.0801, MinusLogProbMetric: 17.0801, val_loss: 17.1962, val_MinusLogProbMetric: 17.1962

Epoch 841: val_loss did not improve from 17.11364
196/196 - 73s - loss: 17.0801 - MinusLogProbMetric: 17.0801 - val_loss: 17.1962 - val_MinusLogProbMetric: 17.1962 - lr: 8.3333e-05 - 73s/epoch - 373ms/step
Epoch 842/1000
2023-09-27 06:52:45.359 
Epoch 842/1000 
	 loss: 17.0907, MinusLogProbMetric: 17.0907, val_loss: 17.1606, val_MinusLogProbMetric: 17.1606

Epoch 842: val_loss did not improve from 17.11364
196/196 - 73s - loss: 17.0907 - MinusLogProbMetric: 17.0907 - val_loss: 17.1606 - val_MinusLogProbMetric: 17.1606 - lr: 8.3333e-05 - 73s/epoch - 375ms/step
Epoch 843/1000
2023-09-27 06:53:58.675 
Epoch 843/1000 
	 loss: 17.0847, MinusLogProbMetric: 17.0847, val_loss: 17.1221, val_MinusLogProbMetric: 17.1221

Epoch 843: val_loss did not improve from 17.11364
196/196 - 73s - loss: 17.0847 - MinusLogProbMetric: 17.0847 - val_loss: 17.1221 - val_MinusLogProbMetric: 17.1221 - lr: 8.3333e-05 - 73s/epoch - 374ms/step
Epoch 844/1000
2023-09-27 06:55:12.118 
Epoch 844/1000 
	 loss: 17.0808, MinusLogProbMetric: 17.0808, val_loss: 17.1376, val_MinusLogProbMetric: 17.1376

Epoch 844: val_loss did not improve from 17.11364
196/196 - 73s - loss: 17.0808 - MinusLogProbMetric: 17.0808 - val_loss: 17.1376 - val_MinusLogProbMetric: 17.1376 - lr: 8.3333e-05 - 73s/epoch - 375ms/step
Epoch 845/1000
2023-09-27 06:56:25.552 
Epoch 845/1000 
	 loss: 17.0764, MinusLogProbMetric: 17.0764, val_loss: 17.1617, val_MinusLogProbMetric: 17.1617

Epoch 845: val_loss did not improve from 17.11364
196/196 - 73s - loss: 17.0764 - MinusLogProbMetric: 17.0764 - val_loss: 17.1617 - val_MinusLogProbMetric: 17.1617 - lr: 8.3333e-05 - 73s/epoch - 375ms/step
Epoch 846/1000
2023-09-27 06:57:39.662 
Epoch 846/1000 
	 loss: 17.0765, MinusLogProbMetric: 17.0765, val_loss: 17.1307, val_MinusLogProbMetric: 17.1307

Epoch 846: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0765 - MinusLogProbMetric: 17.0765 - val_loss: 17.1307 - val_MinusLogProbMetric: 17.1307 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 847/1000
2023-09-27 06:58:53.844 
Epoch 847/1000 
	 loss: 17.0802, MinusLogProbMetric: 17.0802, val_loss: 17.2455, val_MinusLogProbMetric: 17.2455

Epoch 847: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0802 - MinusLogProbMetric: 17.0802 - val_loss: 17.2455 - val_MinusLogProbMetric: 17.2455 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 848/1000
2023-09-27 07:00:07.230 
Epoch 848/1000 
	 loss: 17.0836, MinusLogProbMetric: 17.0836, val_loss: 17.2009, val_MinusLogProbMetric: 17.2009

Epoch 848: val_loss did not improve from 17.11364
196/196 - 73s - loss: 17.0836 - MinusLogProbMetric: 17.0836 - val_loss: 17.2009 - val_MinusLogProbMetric: 17.2009 - lr: 8.3333e-05 - 73s/epoch - 374ms/step
Epoch 849/1000
2023-09-27 07:01:20.845 
Epoch 849/1000 
	 loss: 17.0862, MinusLogProbMetric: 17.0862, val_loss: 17.1239, val_MinusLogProbMetric: 17.1239

Epoch 849: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0862 - MinusLogProbMetric: 17.0862 - val_loss: 17.1239 - val_MinusLogProbMetric: 17.1239 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 850/1000
2023-09-27 07:02:34.347 
Epoch 850/1000 
	 loss: 17.0957, MinusLogProbMetric: 17.0957, val_loss: 17.2022, val_MinusLogProbMetric: 17.2022

Epoch 850: val_loss did not improve from 17.11364
196/196 - 73s - loss: 17.0957 - MinusLogProbMetric: 17.0957 - val_loss: 17.2022 - val_MinusLogProbMetric: 17.2022 - lr: 8.3333e-05 - 73s/epoch - 375ms/step
Epoch 851/1000
2023-09-27 07:03:47.697 
Epoch 851/1000 
	 loss: 17.0831, MinusLogProbMetric: 17.0831, val_loss: 17.2326, val_MinusLogProbMetric: 17.2326

Epoch 851: val_loss did not improve from 17.11364
196/196 - 73s - loss: 17.0831 - MinusLogProbMetric: 17.0831 - val_loss: 17.2326 - val_MinusLogProbMetric: 17.2326 - lr: 8.3333e-05 - 73s/epoch - 374ms/step
Epoch 852/1000
2023-09-27 07:05:01.201 
Epoch 852/1000 
	 loss: 17.0816, MinusLogProbMetric: 17.0816, val_loss: 17.1225, val_MinusLogProbMetric: 17.1225

Epoch 852: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0816 - MinusLogProbMetric: 17.0816 - val_loss: 17.1225 - val_MinusLogProbMetric: 17.1225 - lr: 8.3333e-05 - 74s/epoch - 375ms/step
Epoch 853/1000
2023-09-27 07:06:15.255 
Epoch 853/1000 
	 loss: 17.0960, MinusLogProbMetric: 17.0960, val_loss: 17.1348, val_MinusLogProbMetric: 17.1348

Epoch 853: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0960 - MinusLogProbMetric: 17.0960 - val_loss: 17.1348 - val_MinusLogProbMetric: 17.1348 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 854/1000
2023-09-27 07:07:28.491 
Epoch 854/1000 
	 loss: 17.0926, MinusLogProbMetric: 17.0926, val_loss: 17.2121, val_MinusLogProbMetric: 17.2121

Epoch 854: val_loss did not improve from 17.11364
196/196 - 73s - loss: 17.0926 - MinusLogProbMetric: 17.0926 - val_loss: 17.2121 - val_MinusLogProbMetric: 17.2121 - lr: 8.3333e-05 - 73s/epoch - 374ms/step
Epoch 855/1000
2023-09-27 07:08:42.517 
Epoch 855/1000 
	 loss: 17.0898, MinusLogProbMetric: 17.0898, val_loss: 17.1250, val_MinusLogProbMetric: 17.1250

Epoch 855: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0898 - MinusLogProbMetric: 17.0898 - val_loss: 17.1250 - val_MinusLogProbMetric: 17.1250 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 856/1000
2023-09-27 07:09:56.235 
Epoch 856/1000 
	 loss: 17.0765, MinusLogProbMetric: 17.0765, val_loss: 17.1268, val_MinusLogProbMetric: 17.1268

Epoch 856: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0765 - MinusLogProbMetric: 17.0765 - val_loss: 17.1268 - val_MinusLogProbMetric: 17.1268 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 857/1000
2023-09-27 07:11:10.293 
Epoch 857/1000 
	 loss: 17.0897, MinusLogProbMetric: 17.0897, val_loss: 17.1360, val_MinusLogProbMetric: 17.1360

Epoch 857: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0897 - MinusLogProbMetric: 17.0897 - val_loss: 17.1360 - val_MinusLogProbMetric: 17.1360 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 858/1000
2023-09-27 07:12:24.173 
Epoch 858/1000 
	 loss: 17.0703, MinusLogProbMetric: 17.0703, val_loss: 17.1510, val_MinusLogProbMetric: 17.1510

Epoch 858: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0703 - MinusLogProbMetric: 17.0703 - val_loss: 17.1510 - val_MinusLogProbMetric: 17.1510 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 859/1000
2023-09-27 07:13:37.775 
Epoch 859/1000 
	 loss: 17.0971, MinusLogProbMetric: 17.0971, val_loss: 17.1189, val_MinusLogProbMetric: 17.1189

Epoch 859: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0971 - MinusLogProbMetric: 17.0971 - val_loss: 17.1189 - val_MinusLogProbMetric: 17.1189 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 860/1000
2023-09-27 07:14:51.186 
Epoch 860/1000 
	 loss: 17.0722, MinusLogProbMetric: 17.0722, val_loss: 17.1675, val_MinusLogProbMetric: 17.1675

Epoch 860: val_loss did not improve from 17.11364
196/196 - 73s - loss: 17.0722 - MinusLogProbMetric: 17.0722 - val_loss: 17.1675 - val_MinusLogProbMetric: 17.1675 - lr: 8.3333e-05 - 73s/epoch - 375ms/step
Epoch 861/1000
2023-09-27 07:16:05.484 
Epoch 861/1000 
	 loss: 17.0864, MinusLogProbMetric: 17.0864, val_loss: 17.1424, val_MinusLogProbMetric: 17.1424

Epoch 861: val_loss did not improve from 17.11364
196/196 - 74s - loss: 17.0864 - MinusLogProbMetric: 17.0864 - val_loss: 17.1424 - val_MinusLogProbMetric: 17.1424 - lr: 8.3333e-05 - 74s/epoch - 379ms/step
Epoch 862/1000
2023-09-27 07:17:18.797 
Epoch 862/1000 
	 loss: 17.0775, MinusLogProbMetric: 17.0775, val_loss: 17.1382, val_MinusLogProbMetric: 17.1382

Epoch 862: val_loss did not improve from 17.11364
196/196 - 73s - loss: 17.0775 - MinusLogProbMetric: 17.0775 - val_loss: 17.1382 - val_MinusLogProbMetric: 17.1382 - lr: 8.3333e-05 - 73s/epoch - 374ms/step
Epoch 863/1000
2023-09-27 07:18:27.371 
Epoch 863/1000 
	 loss: 17.0772, MinusLogProbMetric: 17.0772, val_loss: 17.0948, val_MinusLogProbMetric: 17.0948

Epoch 863: val_loss improved from 17.11364 to 17.09478, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 70s - loss: 17.0772 - MinusLogProbMetric: 17.0772 - val_loss: 17.0948 - val_MinusLogProbMetric: 17.0948 - lr: 8.3333e-05 - 70s/epoch - 358ms/step
Epoch 864/1000
2023-09-27 07:19:37.177 
Epoch 864/1000 
	 loss: 17.0704, MinusLogProbMetric: 17.0704, val_loss: 17.1407, val_MinusLogProbMetric: 17.1407

Epoch 864: val_loss did not improve from 17.09478
196/196 - 68s - loss: 17.0704 - MinusLogProbMetric: 17.0704 - val_loss: 17.1407 - val_MinusLogProbMetric: 17.1407 - lr: 8.3333e-05 - 68s/epoch - 348ms/step
Epoch 865/1000
2023-09-27 07:20:48.594 
Epoch 865/1000 
	 loss: 17.0819, MinusLogProbMetric: 17.0819, val_loss: 17.1452, val_MinusLogProbMetric: 17.1452

Epoch 865: val_loss did not improve from 17.09478
196/196 - 71s - loss: 17.0819 - MinusLogProbMetric: 17.0819 - val_loss: 17.1452 - val_MinusLogProbMetric: 17.1452 - lr: 8.3333e-05 - 71s/epoch - 364ms/step
Epoch 866/1000
2023-09-27 07:21:54.363 
Epoch 866/1000 
	 loss: 17.0713, MinusLogProbMetric: 17.0713, val_loss: 17.1682, val_MinusLogProbMetric: 17.1682

Epoch 866: val_loss did not improve from 17.09478
196/196 - 66s - loss: 17.0713 - MinusLogProbMetric: 17.0713 - val_loss: 17.1682 - val_MinusLogProbMetric: 17.1682 - lr: 8.3333e-05 - 66s/epoch - 336ms/step
Epoch 867/1000
2023-09-27 07:23:07.717 
Epoch 867/1000 
	 loss: 17.0830, MinusLogProbMetric: 17.0830, val_loss: 17.1536, val_MinusLogProbMetric: 17.1536

Epoch 867: val_loss did not improve from 17.09478
196/196 - 73s - loss: 17.0830 - MinusLogProbMetric: 17.0830 - val_loss: 17.1536 - val_MinusLogProbMetric: 17.1536 - lr: 8.3333e-05 - 73s/epoch - 374ms/step
Epoch 868/1000
2023-09-27 07:24:20.188 
Epoch 868/1000 
	 loss: 17.0807, MinusLogProbMetric: 17.0807, val_loss: 17.1600, val_MinusLogProbMetric: 17.1600

Epoch 868: val_loss did not improve from 17.09478
196/196 - 72s - loss: 17.0807 - MinusLogProbMetric: 17.0807 - val_loss: 17.1600 - val_MinusLogProbMetric: 17.1600 - lr: 8.3333e-05 - 72s/epoch - 370ms/step
Epoch 869/1000
2023-09-27 07:25:32.855 
Epoch 869/1000 
	 loss: 17.0818, MinusLogProbMetric: 17.0818, val_loss: 17.1644, val_MinusLogProbMetric: 17.1644

Epoch 869: val_loss did not improve from 17.09478
196/196 - 73s - loss: 17.0818 - MinusLogProbMetric: 17.0818 - val_loss: 17.1644 - val_MinusLogProbMetric: 17.1644 - lr: 8.3333e-05 - 73s/epoch - 371ms/step
Epoch 870/1000
2023-09-27 07:26:45.314 
Epoch 870/1000 
	 loss: 17.0767, MinusLogProbMetric: 17.0767, val_loss: 17.1118, val_MinusLogProbMetric: 17.1118

Epoch 870: val_loss did not improve from 17.09478
196/196 - 72s - loss: 17.0767 - MinusLogProbMetric: 17.0767 - val_loss: 17.1118 - val_MinusLogProbMetric: 17.1118 - lr: 8.3333e-05 - 72s/epoch - 370ms/step
Epoch 871/1000
2023-09-27 07:27:58.676 
Epoch 871/1000 
	 loss: 17.0761, MinusLogProbMetric: 17.0761, val_loss: 17.1451, val_MinusLogProbMetric: 17.1451

Epoch 871: val_loss did not improve from 17.09478
196/196 - 73s - loss: 17.0761 - MinusLogProbMetric: 17.0761 - val_loss: 17.1451 - val_MinusLogProbMetric: 17.1451 - lr: 8.3333e-05 - 73s/epoch - 374ms/step
Epoch 872/1000
2023-09-27 07:29:12.290 
Epoch 872/1000 
	 loss: 17.0764, MinusLogProbMetric: 17.0764, val_loss: 17.1304, val_MinusLogProbMetric: 17.1304

Epoch 872: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0764 - MinusLogProbMetric: 17.0764 - val_loss: 17.1304 - val_MinusLogProbMetric: 17.1304 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 873/1000
2023-09-27 07:30:25.886 
Epoch 873/1000 
	 loss: 17.0653, MinusLogProbMetric: 17.0653, val_loss: 17.1425, val_MinusLogProbMetric: 17.1425

Epoch 873: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0653 - MinusLogProbMetric: 17.0653 - val_loss: 17.1425 - val_MinusLogProbMetric: 17.1425 - lr: 8.3333e-05 - 74s/epoch - 375ms/step
Epoch 874/1000
2023-09-27 07:31:39.602 
Epoch 874/1000 
	 loss: 17.0753, MinusLogProbMetric: 17.0753, val_loss: 17.1087, val_MinusLogProbMetric: 17.1087

Epoch 874: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0753 - MinusLogProbMetric: 17.0753 - val_loss: 17.1087 - val_MinusLogProbMetric: 17.1087 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 875/1000
2023-09-27 07:32:53.853 
Epoch 875/1000 
	 loss: 17.0725, MinusLogProbMetric: 17.0725, val_loss: 17.1658, val_MinusLogProbMetric: 17.1658

Epoch 875: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0725 - MinusLogProbMetric: 17.0725 - val_loss: 17.1658 - val_MinusLogProbMetric: 17.1658 - lr: 8.3333e-05 - 74s/epoch - 379ms/step
Epoch 876/1000
2023-09-27 07:34:07.642 
Epoch 876/1000 
	 loss: 17.0739, MinusLogProbMetric: 17.0739, val_loss: 17.1309, val_MinusLogProbMetric: 17.1309

Epoch 876: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0739 - MinusLogProbMetric: 17.0739 - val_loss: 17.1309 - val_MinusLogProbMetric: 17.1309 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 877/1000
2023-09-27 07:35:21.246 
Epoch 877/1000 
	 loss: 17.0749, MinusLogProbMetric: 17.0749, val_loss: 17.1444, val_MinusLogProbMetric: 17.1444

Epoch 877: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0749 - MinusLogProbMetric: 17.0749 - val_loss: 17.1444 - val_MinusLogProbMetric: 17.1444 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 878/1000
2023-09-27 07:36:34.831 
Epoch 878/1000 
	 loss: 17.0772, MinusLogProbMetric: 17.0772, val_loss: 17.1138, val_MinusLogProbMetric: 17.1138

Epoch 878: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0772 - MinusLogProbMetric: 17.0772 - val_loss: 17.1138 - val_MinusLogProbMetric: 17.1138 - lr: 8.3333e-05 - 74s/epoch - 375ms/step
Epoch 879/1000
2023-09-27 07:37:48.376 
Epoch 879/1000 
	 loss: 17.0805, MinusLogProbMetric: 17.0805, val_loss: 17.1144, val_MinusLogProbMetric: 17.1144

Epoch 879: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0805 - MinusLogProbMetric: 17.0805 - val_loss: 17.1144 - val_MinusLogProbMetric: 17.1144 - lr: 8.3333e-05 - 74s/epoch - 375ms/step
Epoch 880/1000
2023-09-27 07:39:02.583 
Epoch 880/1000 
	 loss: 17.0754, MinusLogProbMetric: 17.0754, val_loss: 17.1857, val_MinusLogProbMetric: 17.1857

Epoch 880: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0754 - MinusLogProbMetric: 17.0754 - val_loss: 17.1857 - val_MinusLogProbMetric: 17.1857 - lr: 8.3333e-05 - 74s/epoch - 379ms/step
Epoch 881/1000
2023-09-27 07:40:16.213 
Epoch 881/1000 
	 loss: 17.0710, MinusLogProbMetric: 17.0710, val_loss: 17.1187, val_MinusLogProbMetric: 17.1187

Epoch 881: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0710 - MinusLogProbMetric: 17.0710 - val_loss: 17.1187 - val_MinusLogProbMetric: 17.1187 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 882/1000
2023-09-27 07:41:30.162 
Epoch 882/1000 
	 loss: 17.0846, MinusLogProbMetric: 17.0846, val_loss: 17.1296, val_MinusLogProbMetric: 17.1296

Epoch 882: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0846 - MinusLogProbMetric: 17.0846 - val_loss: 17.1296 - val_MinusLogProbMetric: 17.1296 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 883/1000
2023-09-27 07:42:44.074 
Epoch 883/1000 
	 loss: 17.0808, MinusLogProbMetric: 17.0808, val_loss: 17.1538, val_MinusLogProbMetric: 17.1538

Epoch 883: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0808 - MinusLogProbMetric: 17.0808 - val_loss: 17.1538 - val_MinusLogProbMetric: 17.1538 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 884/1000
2023-09-27 07:43:58.901 
Epoch 884/1000 
	 loss: 17.0681, MinusLogProbMetric: 17.0681, val_loss: 17.1494, val_MinusLogProbMetric: 17.1494

Epoch 884: val_loss did not improve from 17.09478
196/196 - 75s - loss: 17.0681 - MinusLogProbMetric: 17.0681 - val_loss: 17.1494 - val_MinusLogProbMetric: 17.1494 - lr: 8.3333e-05 - 75s/epoch - 382ms/step
Epoch 885/1000
2023-09-27 07:45:13.562 
Epoch 885/1000 
	 loss: 17.0648, MinusLogProbMetric: 17.0648, val_loss: 17.1509, val_MinusLogProbMetric: 17.1509

Epoch 885: val_loss did not improve from 17.09478
196/196 - 75s - loss: 17.0648 - MinusLogProbMetric: 17.0648 - val_loss: 17.1509 - val_MinusLogProbMetric: 17.1509 - lr: 8.3333e-05 - 75s/epoch - 381ms/step
Epoch 886/1000
2023-09-27 07:46:27.524 
Epoch 886/1000 
	 loss: 17.0900, MinusLogProbMetric: 17.0900, val_loss: 17.1709, val_MinusLogProbMetric: 17.1709

Epoch 886: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0900 - MinusLogProbMetric: 17.0900 - val_loss: 17.1709 - val_MinusLogProbMetric: 17.1709 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 887/1000
2023-09-27 07:47:41.594 
Epoch 887/1000 
	 loss: 17.0768, MinusLogProbMetric: 17.0768, val_loss: 17.1870, val_MinusLogProbMetric: 17.1870

Epoch 887: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0768 - MinusLogProbMetric: 17.0768 - val_loss: 17.1870 - val_MinusLogProbMetric: 17.1870 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 888/1000
2023-09-27 07:48:57.134 
Epoch 888/1000 
	 loss: 17.0787, MinusLogProbMetric: 17.0787, val_loss: 17.1493, val_MinusLogProbMetric: 17.1493

Epoch 888: val_loss did not improve from 17.09478
196/196 - 76s - loss: 17.0787 - MinusLogProbMetric: 17.0787 - val_loss: 17.1493 - val_MinusLogProbMetric: 17.1493 - lr: 8.3333e-05 - 76s/epoch - 385ms/step
Epoch 889/1000
2023-09-27 07:50:11.492 
Epoch 889/1000 
	 loss: 17.0775, MinusLogProbMetric: 17.0775, val_loss: 17.1883, val_MinusLogProbMetric: 17.1883

Epoch 889: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0775 - MinusLogProbMetric: 17.0775 - val_loss: 17.1883 - val_MinusLogProbMetric: 17.1883 - lr: 8.3333e-05 - 74s/epoch - 379ms/step
Epoch 890/1000
2023-09-27 07:51:24.946 
Epoch 890/1000 
	 loss: 17.0695, MinusLogProbMetric: 17.0695, val_loss: 17.1536, val_MinusLogProbMetric: 17.1536

Epoch 890: val_loss did not improve from 17.09478
196/196 - 73s - loss: 17.0695 - MinusLogProbMetric: 17.0695 - val_loss: 17.1536 - val_MinusLogProbMetric: 17.1536 - lr: 8.3333e-05 - 73s/epoch - 375ms/step
Epoch 891/1000
2023-09-27 07:52:38.641 
Epoch 891/1000 
	 loss: 17.0661, MinusLogProbMetric: 17.0661, val_loss: 17.1405, val_MinusLogProbMetric: 17.1405

Epoch 891: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0661 - MinusLogProbMetric: 17.0661 - val_loss: 17.1405 - val_MinusLogProbMetric: 17.1405 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 892/1000
2023-09-27 07:53:52.466 
Epoch 892/1000 
	 loss: 17.0763, MinusLogProbMetric: 17.0763, val_loss: 17.1254, val_MinusLogProbMetric: 17.1254

Epoch 892: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0763 - MinusLogProbMetric: 17.0763 - val_loss: 17.1254 - val_MinusLogProbMetric: 17.1254 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 893/1000
2023-09-27 07:55:06.610 
Epoch 893/1000 
	 loss: 17.0779, MinusLogProbMetric: 17.0779, val_loss: 17.1275, val_MinusLogProbMetric: 17.1275

Epoch 893: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0779 - MinusLogProbMetric: 17.0779 - val_loss: 17.1275 - val_MinusLogProbMetric: 17.1275 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 894/1000
2023-09-27 07:56:20.545 
Epoch 894/1000 
	 loss: 17.0687, MinusLogProbMetric: 17.0687, val_loss: 17.1137, val_MinusLogProbMetric: 17.1137

Epoch 894: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0687 - MinusLogProbMetric: 17.0687 - val_loss: 17.1137 - val_MinusLogProbMetric: 17.1137 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 895/1000
2023-09-27 07:57:34.343 
Epoch 895/1000 
	 loss: 17.0695, MinusLogProbMetric: 17.0695, val_loss: 17.1340, val_MinusLogProbMetric: 17.1340

Epoch 895: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0695 - MinusLogProbMetric: 17.0695 - val_loss: 17.1340 - val_MinusLogProbMetric: 17.1340 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 896/1000
2023-09-27 07:58:48.306 
Epoch 896/1000 
	 loss: 17.0663, MinusLogProbMetric: 17.0663, val_loss: 17.1406, val_MinusLogProbMetric: 17.1406

Epoch 896: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0663 - MinusLogProbMetric: 17.0663 - val_loss: 17.1406 - val_MinusLogProbMetric: 17.1406 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 897/1000
2023-09-27 08:00:02.556 
Epoch 897/1000 
	 loss: 17.0694, MinusLogProbMetric: 17.0694, val_loss: 17.1255, val_MinusLogProbMetric: 17.1255

Epoch 897: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0694 - MinusLogProbMetric: 17.0694 - val_loss: 17.1255 - val_MinusLogProbMetric: 17.1255 - lr: 8.3333e-05 - 74s/epoch - 379ms/step
Epoch 898/1000
2023-09-27 08:01:16.960 
Epoch 898/1000 
	 loss: 17.0728, MinusLogProbMetric: 17.0728, val_loss: 17.1702, val_MinusLogProbMetric: 17.1702

Epoch 898: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0728 - MinusLogProbMetric: 17.0728 - val_loss: 17.1702 - val_MinusLogProbMetric: 17.1702 - lr: 8.3333e-05 - 74s/epoch - 380ms/step
Epoch 899/1000
2023-09-27 08:02:33.164 
Epoch 899/1000 
	 loss: 17.0705, MinusLogProbMetric: 17.0705, val_loss: 17.1402, val_MinusLogProbMetric: 17.1402

Epoch 899: val_loss did not improve from 17.09478
196/196 - 76s - loss: 17.0705 - MinusLogProbMetric: 17.0705 - val_loss: 17.1402 - val_MinusLogProbMetric: 17.1402 - lr: 8.3333e-05 - 76s/epoch - 389ms/step
Epoch 900/1000
2023-09-27 08:03:50.302 
Epoch 900/1000 
	 loss: 17.0784, MinusLogProbMetric: 17.0784, val_loss: 17.1995, val_MinusLogProbMetric: 17.1995

Epoch 900: val_loss did not improve from 17.09478
196/196 - 77s - loss: 17.0784 - MinusLogProbMetric: 17.0784 - val_loss: 17.1995 - val_MinusLogProbMetric: 17.1995 - lr: 8.3333e-05 - 77s/epoch - 394ms/step
Epoch 901/1000
2023-09-27 08:05:05.875 
Epoch 901/1000 
	 loss: 17.0764, MinusLogProbMetric: 17.0764, val_loss: 17.0979, val_MinusLogProbMetric: 17.0979

Epoch 901: val_loss did not improve from 17.09478
196/196 - 76s - loss: 17.0764 - MinusLogProbMetric: 17.0764 - val_loss: 17.0979 - val_MinusLogProbMetric: 17.0979 - lr: 8.3333e-05 - 76s/epoch - 386ms/step
Epoch 902/1000
2023-09-27 08:06:20.639 
Epoch 902/1000 
	 loss: 17.0649, MinusLogProbMetric: 17.0649, val_loss: 17.1483, val_MinusLogProbMetric: 17.1483

Epoch 902: val_loss did not improve from 17.09478
196/196 - 75s - loss: 17.0649 - MinusLogProbMetric: 17.0649 - val_loss: 17.1483 - val_MinusLogProbMetric: 17.1483 - lr: 8.3333e-05 - 75s/epoch - 381ms/step
Epoch 903/1000
2023-09-27 08:07:35.142 
Epoch 903/1000 
	 loss: 17.0768, MinusLogProbMetric: 17.0768, val_loss: 17.1078, val_MinusLogProbMetric: 17.1078

Epoch 903: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0768 - MinusLogProbMetric: 17.0768 - val_loss: 17.1078 - val_MinusLogProbMetric: 17.1078 - lr: 8.3333e-05 - 74s/epoch - 380ms/step
Epoch 904/1000
2023-09-27 08:08:50.115 
Epoch 904/1000 
	 loss: 17.0602, MinusLogProbMetric: 17.0602, val_loss: 17.1728, val_MinusLogProbMetric: 17.1728

Epoch 904: val_loss did not improve from 17.09478
196/196 - 75s - loss: 17.0602 - MinusLogProbMetric: 17.0602 - val_loss: 17.1728 - val_MinusLogProbMetric: 17.1728 - lr: 8.3333e-05 - 75s/epoch - 383ms/step
Epoch 905/1000
2023-09-27 08:10:04.751 
Epoch 905/1000 
	 loss: 17.0655, MinusLogProbMetric: 17.0655, val_loss: 17.1283, val_MinusLogProbMetric: 17.1283

Epoch 905: val_loss did not improve from 17.09478
196/196 - 75s - loss: 17.0655 - MinusLogProbMetric: 17.0655 - val_loss: 17.1283 - val_MinusLogProbMetric: 17.1283 - lr: 8.3333e-05 - 75s/epoch - 381ms/step
Epoch 906/1000
2023-09-27 08:11:19.469 
Epoch 906/1000 
	 loss: 17.0787, MinusLogProbMetric: 17.0787, val_loss: 17.1108, val_MinusLogProbMetric: 17.1108

Epoch 906: val_loss did not improve from 17.09478
196/196 - 75s - loss: 17.0787 - MinusLogProbMetric: 17.0787 - val_loss: 17.1108 - val_MinusLogProbMetric: 17.1108 - lr: 8.3333e-05 - 75s/epoch - 381ms/step
Epoch 907/1000
2023-09-27 08:12:33.255 
Epoch 907/1000 
	 loss: 17.0694, MinusLogProbMetric: 17.0694, val_loss: 17.1325, val_MinusLogProbMetric: 17.1325

Epoch 907: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0694 - MinusLogProbMetric: 17.0694 - val_loss: 17.1325 - val_MinusLogProbMetric: 17.1325 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 908/1000
2023-09-27 08:13:46.874 
Epoch 908/1000 
	 loss: 17.0692, MinusLogProbMetric: 17.0692, val_loss: 17.2027, val_MinusLogProbMetric: 17.2027

Epoch 908: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0692 - MinusLogProbMetric: 17.0692 - val_loss: 17.2027 - val_MinusLogProbMetric: 17.2027 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 909/1000
2023-09-27 08:15:00.762 
Epoch 909/1000 
	 loss: 17.0673, MinusLogProbMetric: 17.0673, val_loss: 17.1677, val_MinusLogProbMetric: 17.1677

Epoch 909: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0673 - MinusLogProbMetric: 17.0673 - val_loss: 17.1677 - val_MinusLogProbMetric: 17.1677 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 910/1000
2023-09-27 08:16:14.906 
Epoch 910/1000 
	 loss: 17.0806, MinusLogProbMetric: 17.0806, val_loss: 17.1609, val_MinusLogProbMetric: 17.1609

Epoch 910: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0806 - MinusLogProbMetric: 17.0806 - val_loss: 17.1609 - val_MinusLogProbMetric: 17.1609 - lr: 8.3333e-05 - 74s/epoch - 378ms/step
Epoch 911/1000
2023-09-27 08:17:28.739 
Epoch 911/1000 
	 loss: 17.0684, MinusLogProbMetric: 17.0684, val_loss: 17.1173, val_MinusLogProbMetric: 17.1173

Epoch 911: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0684 - MinusLogProbMetric: 17.0684 - val_loss: 17.1173 - val_MinusLogProbMetric: 17.1173 - lr: 8.3333e-05 - 74s/epoch - 377ms/step
Epoch 912/1000
2023-09-27 08:18:43.353 
Epoch 912/1000 
	 loss: 17.0783, MinusLogProbMetric: 17.0783, val_loss: 17.1279, val_MinusLogProbMetric: 17.1279

Epoch 912: val_loss did not improve from 17.09478
196/196 - 75s - loss: 17.0783 - MinusLogProbMetric: 17.0783 - val_loss: 17.1279 - val_MinusLogProbMetric: 17.1279 - lr: 8.3333e-05 - 75s/epoch - 381ms/step
Epoch 913/1000
2023-09-27 08:19:57.074 
Epoch 913/1000 
	 loss: 17.0617, MinusLogProbMetric: 17.0617, val_loss: 17.1560, val_MinusLogProbMetric: 17.1560

Epoch 913: val_loss did not improve from 17.09478
196/196 - 74s - loss: 17.0617 - MinusLogProbMetric: 17.0617 - val_loss: 17.1560 - val_MinusLogProbMetric: 17.1560 - lr: 8.3333e-05 - 74s/epoch - 376ms/step
Epoch 914/1000
2023-09-27 08:21:10.227 
Epoch 914/1000 
	 loss: 17.0237, MinusLogProbMetric: 17.0237, val_loss: 17.0960, val_MinusLogProbMetric: 17.0960

Epoch 914: val_loss did not improve from 17.09478
196/196 - 73s - loss: 17.0237 - MinusLogProbMetric: 17.0237 - val_loss: 17.0960 - val_MinusLogProbMetric: 17.0960 - lr: 4.1667e-05 - 73s/epoch - 373ms/step
Epoch 915/1000
2023-09-27 08:22:23.574 
Epoch 915/1000 
	 loss: 17.0218, MinusLogProbMetric: 17.0218, val_loss: 17.1005, val_MinusLogProbMetric: 17.1005

Epoch 915: val_loss did not improve from 17.09478
196/196 - 73s - loss: 17.0218 - MinusLogProbMetric: 17.0218 - val_loss: 17.1005 - val_MinusLogProbMetric: 17.1005 - lr: 4.1667e-05 - 73s/epoch - 374ms/step
Epoch 916/1000
2023-09-27 08:23:37.276 
Epoch 916/1000 
	 loss: 17.0199, MinusLogProbMetric: 17.0199, val_loss: 17.0828, val_MinusLogProbMetric: 17.0828

Epoch 916: val_loss improved from 17.09478 to 17.08279, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 17.0199 - MinusLogProbMetric: 17.0199 - val_loss: 17.0828 - val_MinusLogProbMetric: 17.0828 - lr: 4.1667e-05 - 75s/epoch - 384ms/step
Epoch 917/1000
2023-09-27 08:24:52.846 
Epoch 917/1000 
	 loss: 17.0157, MinusLogProbMetric: 17.0157, val_loss: 17.0762, val_MinusLogProbMetric: 17.0762

Epoch 917: val_loss improved from 17.08279 to 17.07617, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 17.0157 - MinusLogProbMetric: 17.0157 - val_loss: 17.0762 - val_MinusLogProbMetric: 17.0762 - lr: 4.1667e-05 - 75s/epoch - 384ms/step
Epoch 918/1000
2023-09-27 08:26:09.162 
Epoch 918/1000 
	 loss: 17.0215, MinusLogProbMetric: 17.0215, val_loss: 17.1015, val_MinusLogProbMetric: 17.1015

Epoch 918: val_loss did not improve from 17.07617
196/196 - 75s - loss: 17.0215 - MinusLogProbMetric: 17.0215 - val_loss: 17.1015 - val_MinusLogProbMetric: 17.1015 - lr: 4.1667e-05 - 75s/epoch - 383ms/step
Epoch 919/1000
2023-09-27 08:27:24.449 
Epoch 919/1000 
	 loss: 17.0226, MinusLogProbMetric: 17.0226, val_loss: 17.0781, val_MinusLogProbMetric: 17.0781

Epoch 919: val_loss did not improve from 17.07617
196/196 - 75s - loss: 17.0226 - MinusLogProbMetric: 17.0226 - val_loss: 17.0781 - val_MinusLogProbMetric: 17.0781 - lr: 4.1667e-05 - 75s/epoch - 384ms/step
Epoch 920/1000
2023-09-27 08:28:42.454 
Epoch 920/1000 
	 loss: 17.0184, MinusLogProbMetric: 17.0184, val_loss: 17.0807, val_MinusLogProbMetric: 17.0807

Epoch 920: val_loss did not improve from 17.07617
196/196 - 78s - loss: 17.0184 - MinusLogProbMetric: 17.0184 - val_loss: 17.0807 - val_MinusLogProbMetric: 17.0807 - lr: 4.1667e-05 - 78s/epoch - 398ms/step
Epoch 921/1000
2023-09-27 08:29:59.795 
Epoch 921/1000 
	 loss: 17.0203, MinusLogProbMetric: 17.0203, val_loss: 17.0997, val_MinusLogProbMetric: 17.0997

Epoch 921: val_loss did not improve from 17.07617
196/196 - 77s - loss: 17.0203 - MinusLogProbMetric: 17.0203 - val_loss: 17.0997 - val_MinusLogProbMetric: 17.0997 - lr: 4.1667e-05 - 77s/epoch - 395ms/step
Epoch 922/1000
2023-09-27 08:31:16.750 
Epoch 922/1000 
	 loss: 17.0190, MinusLogProbMetric: 17.0190, val_loss: 17.0879, val_MinusLogProbMetric: 17.0879

Epoch 922: val_loss did not improve from 17.07617
196/196 - 77s - loss: 17.0190 - MinusLogProbMetric: 17.0190 - val_loss: 17.0879 - val_MinusLogProbMetric: 17.0879 - lr: 4.1667e-05 - 77s/epoch - 393ms/step
Epoch 923/1000
2023-09-27 08:32:33.988 
Epoch 923/1000 
	 loss: 17.0233, MinusLogProbMetric: 17.0233, val_loss: 17.0777, val_MinusLogProbMetric: 17.0777

Epoch 923: val_loss did not improve from 17.07617
196/196 - 77s - loss: 17.0233 - MinusLogProbMetric: 17.0233 - val_loss: 17.0777 - val_MinusLogProbMetric: 17.0777 - lr: 4.1667e-05 - 77s/epoch - 394ms/step
Epoch 924/1000
2023-09-27 08:33:51.686 
Epoch 924/1000 
	 loss: 17.0196, MinusLogProbMetric: 17.0196, val_loss: 17.0806, val_MinusLogProbMetric: 17.0806

Epoch 924: val_loss did not improve from 17.07617
196/196 - 78s - loss: 17.0196 - MinusLogProbMetric: 17.0196 - val_loss: 17.0806 - val_MinusLogProbMetric: 17.0806 - lr: 4.1667e-05 - 78s/epoch - 396ms/step
Epoch 925/1000
2023-09-27 08:35:05.024 
Epoch 925/1000 
	 loss: 17.0193, MinusLogProbMetric: 17.0193, val_loss: 17.0726, val_MinusLogProbMetric: 17.0726

Epoch 925: val_loss improved from 17.07617 to 17.07257, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 17.0193 - MinusLogProbMetric: 17.0193 - val_loss: 17.0726 - val_MinusLogProbMetric: 17.0726 - lr: 4.1667e-05 - 75s/epoch - 382ms/step
Epoch 926/1000
2023-09-27 08:36:23.071 
Epoch 926/1000 
	 loss: 17.0260, MinusLogProbMetric: 17.0260, val_loss: 17.0898, val_MinusLogProbMetric: 17.0898

Epoch 926: val_loss did not improve from 17.07257
196/196 - 77s - loss: 17.0260 - MinusLogProbMetric: 17.0260 - val_loss: 17.0898 - val_MinusLogProbMetric: 17.0898 - lr: 4.1667e-05 - 77s/epoch - 390ms/step
Epoch 927/1000
2023-09-27 08:37:39.244 
Epoch 927/1000 
	 loss: 17.0195, MinusLogProbMetric: 17.0195, val_loss: 17.0776, val_MinusLogProbMetric: 17.0776

Epoch 927: val_loss did not improve from 17.07257
196/196 - 76s - loss: 17.0195 - MinusLogProbMetric: 17.0195 - val_loss: 17.0776 - val_MinusLogProbMetric: 17.0776 - lr: 4.1667e-05 - 76s/epoch - 389ms/step
Epoch 928/1000
2023-09-27 08:38:54.416 
Epoch 928/1000 
	 loss: 17.0144, MinusLogProbMetric: 17.0144, val_loss: 17.0764, val_MinusLogProbMetric: 17.0764

Epoch 928: val_loss did not improve from 17.07257
196/196 - 75s - loss: 17.0144 - MinusLogProbMetric: 17.0144 - val_loss: 17.0764 - val_MinusLogProbMetric: 17.0764 - lr: 4.1667e-05 - 75s/epoch - 384ms/step
Epoch 929/1000
2023-09-27 08:40:08.520 
Epoch 929/1000 
	 loss: 17.0219, MinusLogProbMetric: 17.0219, val_loss: 17.0904, val_MinusLogProbMetric: 17.0904

Epoch 929: val_loss did not improve from 17.07257
196/196 - 74s - loss: 17.0219 - MinusLogProbMetric: 17.0219 - val_loss: 17.0904 - val_MinusLogProbMetric: 17.0904 - lr: 4.1667e-05 - 74s/epoch - 378ms/step
Epoch 930/1000
2023-09-27 08:41:22.492 
Epoch 930/1000 
	 loss: 17.0213, MinusLogProbMetric: 17.0213, val_loss: 17.1016, val_MinusLogProbMetric: 17.1016

Epoch 930: val_loss did not improve from 17.07257
196/196 - 74s - loss: 17.0213 - MinusLogProbMetric: 17.0213 - val_loss: 17.1016 - val_MinusLogProbMetric: 17.1016 - lr: 4.1667e-05 - 74s/epoch - 377ms/step
Epoch 931/1000
2023-09-27 08:42:36.008 
Epoch 931/1000 
	 loss: 17.0201, MinusLogProbMetric: 17.0201, val_loss: 17.1260, val_MinusLogProbMetric: 17.1260

Epoch 931: val_loss did not improve from 17.07257
196/196 - 74s - loss: 17.0201 - MinusLogProbMetric: 17.0201 - val_loss: 17.1260 - val_MinusLogProbMetric: 17.1260 - lr: 4.1667e-05 - 74s/epoch - 375ms/step
Epoch 932/1000
2023-09-27 08:43:49.785 
Epoch 932/1000 
	 loss: 17.0197, MinusLogProbMetric: 17.0197, val_loss: 17.0763, val_MinusLogProbMetric: 17.0763

Epoch 932: val_loss did not improve from 17.07257
196/196 - 74s - loss: 17.0197 - MinusLogProbMetric: 17.0197 - val_loss: 17.0763 - val_MinusLogProbMetric: 17.0763 - lr: 4.1667e-05 - 74s/epoch - 376ms/step
Epoch 933/1000
2023-09-27 08:45:03.613 
Epoch 933/1000 
	 loss: 17.0218, MinusLogProbMetric: 17.0218, val_loss: 17.0897, val_MinusLogProbMetric: 17.0897

Epoch 933: val_loss did not improve from 17.07257
196/196 - 74s - loss: 17.0218 - MinusLogProbMetric: 17.0218 - val_loss: 17.0897 - val_MinusLogProbMetric: 17.0897 - lr: 4.1667e-05 - 74s/epoch - 377ms/step
Epoch 934/1000
2023-09-27 08:46:17.547 
Epoch 934/1000 
	 loss: 17.0170, MinusLogProbMetric: 17.0170, val_loss: 17.0670, val_MinusLogProbMetric: 17.0670

Epoch 934: val_loss improved from 17.07257 to 17.06698, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 76s - loss: 17.0170 - MinusLogProbMetric: 17.0170 - val_loss: 17.0670 - val_MinusLogProbMetric: 17.0670 - lr: 4.1667e-05 - 76s/epoch - 386ms/step
Epoch 935/1000
2023-09-27 08:47:33.338 
Epoch 935/1000 
	 loss: 17.0155, MinusLogProbMetric: 17.0155, val_loss: 17.0686, val_MinusLogProbMetric: 17.0686

Epoch 935: val_loss did not improve from 17.06698
196/196 - 74s - loss: 17.0155 - MinusLogProbMetric: 17.0155 - val_loss: 17.0686 - val_MinusLogProbMetric: 17.0686 - lr: 4.1667e-05 - 74s/epoch - 378ms/step
Epoch 936/1000
2023-09-27 08:48:48.085 
Epoch 936/1000 
	 loss: 17.0159, MinusLogProbMetric: 17.0159, val_loss: 17.0665, val_MinusLogProbMetric: 17.0665

Epoch 936: val_loss improved from 17.06698 to 17.06647, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 76s - loss: 17.0159 - MinusLogProbMetric: 17.0159 - val_loss: 17.0665 - val_MinusLogProbMetric: 17.0665 - lr: 4.1667e-05 - 76s/epoch - 388ms/step
Epoch 937/1000
2023-09-27 08:50:04.425 
Epoch 937/1000 
	 loss: 17.0181, MinusLogProbMetric: 17.0181, val_loss: 17.0844, val_MinusLogProbMetric: 17.0844

Epoch 937: val_loss did not improve from 17.06647
196/196 - 75s - loss: 17.0181 - MinusLogProbMetric: 17.0181 - val_loss: 17.0844 - val_MinusLogProbMetric: 17.0844 - lr: 4.1667e-05 - 75s/epoch - 383ms/step
Epoch 938/1000
2023-09-27 08:51:17.833 
Epoch 938/1000 
	 loss: 17.0144, MinusLogProbMetric: 17.0144, val_loss: 17.0832, val_MinusLogProbMetric: 17.0832

Epoch 938: val_loss did not improve from 17.06647
196/196 - 73s - loss: 17.0144 - MinusLogProbMetric: 17.0144 - val_loss: 17.0832 - val_MinusLogProbMetric: 17.0832 - lr: 4.1667e-05 - 73s/epoch - 375ms/step
Epoch 939/1000
2023-09-27 08:52:30.667 
Epoch 939/1000 
	 loss: 17.0151, MinusLogProbMetric: 17.0151, val_loss: 17.0783, val_MinusLogProbMetric: 17.0783

Epoch 939: val_loss did not improve from 17.06647
196/196 - 73s - loss: 17.0151 - MinusLogProbMetric: 17.0151 - val_loss: 17.0783 - val_MinusLogProbMetric: 17.0783 - lr: 4.1667e-05 - 73s/epoch - 372ms/step
Epoch 940/1000
2023-09-27 08:53:44.211 
Epoch 940/1000 
	 loss: 17.0172, MinusLogProbMetric: 17.0172, val_loss: 17.0797, val_MinusLogProbMetric: 17.0797

Epoch 940: val_loss did not improve from 17.06647
196/196 - 74s - loss: 17.0172 - MinusLogProbMetric: 17.0172 - val_loss: 17.0797 - val_MinusLogProbMetric: 17.0797 - lr: 4.1667e-05 - 74s/epoch - 375ms/step
Epoch 941/1000
2023-09-27 08:54:57.056 
Epoch 941/1000 
	 loss: 17.0199, MinusLogProbMetric: 17.0199, val_loss: 17.0912, val_MinusLogProbMetric: 17.0912

Epoch 941: val_loss did not improve from 17.06647
196/196 - 73s - loss: 17.0199 - MinusLogProbMetric: 17.0199 - val_loss: 17.0912 - val_MinusLogProbMetric: 17.0912 - lr: 4.1667e-05 - 73s/epoch - 372ms/step
Epoch 942/1000
2023-09-27 08:56:09.811 
Epoch 942/1000 
	 loss: 17.0150, MinusLogProbMetric: 17.0150, val_loss: 17.0950, val_MinusLogProbMetric: 17.0950

Epoch 942: val_loss did not improve from 17.06647
196/196 - 73s - loss: 17.0150 - MinusLogProbMetric: 17.0150 - val_loss: 17.0950 - val_MinusLogProbMetric: 17.0950 - lr: 4.1667e-05 - 73s/epoch - 371ms/step
Epoch 943/1000
2023-09-27 08:57:19.739 
Epoch 943/1000 
	 loss: 17.0157, MinusLogProbMetric: 17.0157, val_loss: 17.0847, val_MinusLogProbMetric: 17.0847

Epoch 943: val_loss did not improve from 17.06647
196/196 - 70s - loss: 17.0157 - MinusLogProbMetric: 17.0157 - val_loss: 17.0847 - val_MinusLogProbMetric: 17.0847 - lr: 4.1667e-05 - 70s/epoch - 357ms/step
Epoch 944/1000
2023-09-27 08:58:31.591 
Epoch 944/1000 
	 loss: 17.0159, MinusLogProbMetric: 17.0159, val_loss: 17.0953, val_MinusLogProbMetric: 17.0953

Epoch 944: val_loss did not improve from 17.06647
196/196 - 72s - loss: 17.0159 - MinusLogProbMetric: 17.0159 - val_loss: 17.0953 - val_MinusLogProbMetric: 17.0953 - lr: 4.1667e-05 - 72s/epoch - 367ms/step
Epoch 945/1000
2023-09-27 08:59:45.061 
Epoch 945/1000 
	 loss: 17.0236, MinusLogProbMetric: 17.0236, val_loss: 17.0779, val_MinusLogProbMetric: 17.0779

Epoch 945: val_loss did not improve from 17.06647
196/196 - 73s - loss: 17.0236 - MinusLogProbMetric: 17.0236 - val_loss: 17.0779 - val_MinusLogProbMetric: 17.0779 - lr: 4.1667e-05 - 73s/epoch - 375ms/step
Epoch 946/1000
2023-09-27 09:00:56.291 
Epoch 946/1000 
	 loss: 17.0230, MinusLogProbMetric: 17.0230, val_loss: 17.0702, val_MinusLogProbMetric: 17.0702

Epoch 946: val_loss did not improve from 17.06647
196/196 - 71s - loss: 17.0230 - MinusLogProbMetric: 17.0230 - val_loss: 17.0702 - val_MinusLogProbMetric: 17.0702 - lr: 4.1667e-05 - 71s/epoch - 363ms/step
Epoch 947/1000
2023-09-27 09:02:06.390 
Epoch 947/1000 
	 loss: 17.0153, MinusLogProbMetric: 17.0153, val_loss: 17.1019, val_MinusLogProbMetric: 17.1019

Epoch 947: val_loss did not improve from 17.06647
196/196 - 70s - loss: 17.0153 - MinusLogProbMetric: 17.0153 - val_loss: 17.1019 - val_MinusLogProbMetric: 17.1019 - lr: 4.1667e-05 - 70s/epoch - 358ms/step
Epoch 948/1000
2023-09-27 09:03:20.914 
Epoch 948/1000 
	 loss: 17.0148, MinusLogProbMetric: 17.0148, val_loss: 17.0785, val_MinusLogProbMetric: 17.0785

Epoch 948: val_loss did not improve from 17.06647
196/196 - 75s - loss: 17.0148 - MinusLogProbMetric: 17.0148 - val_loss: 17.0785 - val_MinusLogProbMetric: 17.0785 - lr: 4.1667e-05 - 75s/epoch - 380ms/step
Epoch 949/1000
2023-09-27 09:04:35.708 
Epoch 949/1000 
	 loss: 17.0100, MinusLogProbMetric: 17.0100, val_loss: 17.0701, val_MinusLogProbMetric: 17.0701

Epoch 949: val_loss did not improve from 17.06647
196/196 - 75s - loss: 17.0100 - MinusLogProbMetric: 17.0100 - val_loss: 17.0701 - val_MinusLogProbMetric: 17.0701 - lr: 4.1667e-05 - 75s/epoch - 382ms/step
Epoch 950/1000
2023-09-27 09:05:49.159 
Epoch 950/1000 
	 loss: 17.0162, MinusLogProbMetric: 17.0162, val_loss: 17.0747, val_MinusLogProbMetric: 17.0747

Epoch 950: val_loss did not improve from 17.06647
196/196 - 73s - loss: 17.0162 - MinusLogProbMetric: 17.0162 - val_loss: 17.0747 - val_MinusLogProbMetric: 17.0747 - lr: 4.1667e-05 - 73s/epoch - 375ms/step
Epoch 951/1000
2023-09-27 09:07:03.034 
Epoch 951/1000 
	 loss: 17.0152, MinusLogProbMetric: 17.0152, val_loss: 17.0794, val_MinusLogProbMetric: 17.0794

Epoch 951: val_loss did not improve from 17.06647
196/196 - 74s - loss: 17.0152 - MinusLogProbMetric: 17.0152 - val_loss: 17.0794 - val_MinusLogProbMetric: 17.0794 - lr: 4.1667e-05 - 74s/epoch - 377ms/step
Epoch 952/1000
2023-09-27 09:08:16.354 
Epoch 952/1000 
	 loss: 17.0166, MinusLogProbMetric: 17.0166, val_loss: 17.1209, val_MinusLogProbMetric: 17.1209

Epoch 952: val_loss did not improve from 17.06647
196/196 - 73s - loss: 17.0166 - MinusLogProbMetric: 17.0166 - val_loss: 17.1209 - val_MinusLogProbMetric: 17.1209 - lr: 4.1667e-05 - 73s/epoch - 374ms/step
Epoch 953/1000
2023-09-27 09:09:29.798 
Epoch 953/1000 
	 loss: 17.0162, MinusLogProbMetric: 17.0162, val_loss: 17.0759, val_MinusLogProbMetric: 17.0759

Epoch 953: val_loss did not improve from 17.06647
196/196 - 73s - loss: 17.0162 - MinusLogProbMetric: 17.0162 - val_loss: 17.0759 - val_MinusLogProbMetric: 17.0759 - lr: 4.1667e-05 - 73s/epoch - 375ms/step
Epoch 954/1000
2023-09-27 09:10:43.420 
Epoch 954/1000 
	 loss: 17.0186, MinusLogProbMetric: 17.0186, val_loss: 17.0868, val_MinusLogProbMetric: 17.0868

Epoch 954: val_loss did not improve from 17.06647
196/196 - 74s - loss: 17.0186 - MinusLogProbMetric: 17.0186 - val_loss: 17.0868 - val_MinusLogProbMetric: 17.0868 - lr: 4.1667e-05 - 74s/epoch - 376ms/step
Epoch 955/1000
2023-09-27 09:11:56.175 
Epoch 955/1000 
	 loss: 17.0193, MinusLogProbMetric: 17.0193, val_loss: 17.1036, val_MinusLogProbMetric: 17.1036

Epoch 955: val_loss did not improve from 17.06647
196/196 - 73s - loss: 17.0193 - MinusLogProbMetric: 17.0193 - val_loss: 17.1036 - val_MinusLogProbMetric: 17.1036 - lr: 4.1667e-05 - 73s/epoch - 371ms/step
Epoch 956/1000
2023-09-27 09:13:09.638 
Epoch 956/1000 
	 loss: 17.0175, MinusLogProbMetric: 17.0175, val_loss: 17.0597, val_MinusLogProbMetric: 17.0597

Epoch 956: val_loss improved from 17.06647 to 17.05968, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 17.0175 - MinusLogProbMetric: 17.0175 - val_loss: 17.0597 - val_MinusLogProbMetric: 17.0597 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 957/1000
2023-09-27 09:14:24.026 
Epoch 957/1000 
	 loss: 17.0152, MinusLogProbMetric: 17.0152, val_loss: 17.0967, val_MinusLogProbMetric: 17.0967

Epoch 957: val_loss did not improve from 17.05968
196/196 - 73s - loss: 17.0152 - MinusLogProbMetric: 17.0152 - val_loss: 17.0967 - val_MinusLogProbMetric: 17.0967 - lr: 4.1667e-05 - 73s/epoch - 373ms/step
Epoch 958/1000
2023-09-27 09:15:36.963 
Epoch 958/1000 
	 loss: 17.0160, MinusLogProbMetric: 17.0160, val_loss: 17.0807, val_MinusLogProbMetric: 17.0807

Epoch 958: val_loss did not improve from 17.05968
196/196 - 73s - loss: 17.0160 - MinusLogProbMetric: 17.0160 - val_loss: 17.0807 - val_MinusLogProbMetric: 17.0807 - lr: 4.1667e-05 - 73s/epoch - 372ms/step
Epoch 959/1000
2023-09-27 09:16:49.491 
Epoch 959/1000 
	 loss: 17.0166, MinusLogProbMetric: 17.0166, val_loss: 17.1005, val_MinusLogProbMetric: 17.1005

Epoch 959: val_loss did not improve from 17.05968
196/196 - 73s - loss: 17.0166 - MinusLogProbMetric: 17.0166 - val_loss: 17.1005 - val_MinusLogProbMetric: 17.1005 - lr: 4.1667e-05 - 73s/epoch - 370ms/step
Epoch 960/1000
2023-09-27 09:18:01.166 
Epoch 960/1000 
	 loss: 17.0162, MinusLogProbMetric: 17.0162, val_loss: 17.1005, val_MinusLogProbMetric: 17.1005

Epoch 960: val_loss did not improve from 17.05968
196/196 - 72s - loss: 17.0162 - MinusLogProbMetric: 17.0162 - val_loss: 17.1005 - val_MinusLogProbMetric: 17.1005 - lr: 4.1667e-05 - 72s/epoch - 366ms/step
Epoch 961/1000
2023-09-27 09:19:11.770 
Epoch 961/1000 
	 loss: 17.0162, MinusLogProbMetric: 17.0162, val_loss: 17.0591, val_MinusLogProbMetric: 17.0591

Epoch 961: val_loss improved from 17.05968 to 17.05909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 72s - loss: 17.0162 - MinusLogProbMetric: 17.0162 - val_loss: 17.0591 - val_MinusLogProbMetric: 17.0591 - lr: 4.1667e-05 - 72s/epoch - 368ms/step
Epoch 962/1000
2023-09-27 09:20:26.920 
Epoch 962/1000 
	 loss: 17.0168, MinusLogProbMetric: 17.0168, val_loss: 17.0735, val_MinusLogProbMetric: 17.0735

Epoch 962: val_loss did not improve from 17.05909
196/196 - 74s - loss: 17.0168 - MinusLogProbMetric: 17.0168 - val_loss: 17.0735 - val_MinusLogProbMetric: 17.0735 - lr: 4.1667e-05 - 74s/epoch - 376ms/step
Epoch 963/1000
2023-09-27 09:21:39.868 
Epoch 963/1000 
	 loss: 17.0168, MinusLogProbMetric: 17.0168, val_loss: 17.0695, val_MinusLogProbMetric: 17.0695

Epoch 963: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0168 - MinusLogProbMetric: 17.0168 - val_loss: 17.0695 - val_MinusLogProbMetric: 17.0695 - lr: 4.1667e-05 - 73s/epoch - 372ms/step
Epoch 964/1000
2023-09-27 09:22:45.290 
Epoch 964/1000 
	 loss: 17.0164, MinusLogProbMetric: 17.0164, val_loss: 17.0838, val_MinusLogProbMetric: 17.0838

Epoch 964: val_loss did not improve from 17.05909
196/196 - 65s - loss: 17.0164 - MinusLogProbMetric: 17.0164 - val_loss: 17.0838 - val_MinusLogProbMetric: 17.0838 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 965/1000
2023-09-27 09:23:47.800 
Epoch 965/1000 
	 loss: 17.0143, MinusLogProbMetric: 17.0143, val_loss: 17.0864, val_MinusLogProbMetric: 17.0864

Epoch 965: val_loss did not improve from 17.05909
196/196 - 63s - loss: 17.0143 - MinusLogProbMetric: 17.0143 - val_loss: 17.0864 - val_MinusLogProbMetric: 17.0864 - lr: 4.1667e-05 - 63s/epoch - 319ms/step
Epoch 966/1000
2023-09-27 09:24:50.743 
Epoch 966/1000 
	 loss: 17.0189, MinusLogProbMetric: 17.0189, val_loss: 17.0907, val_MinusLogProbMetric: 17.0907

Epoch 966: val_loss did not improve from 17.05909
196/196 - 63s - loss: 17.0189 - MinusLogProbMetric: 17.0189 - val_loss: 17.0907 - val_MinusLogProbMetric: 17.0907 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 967/1000
2023-09-27 09:25:59.181 
Epoch 967/1000 
	 loss: 17.0172, MinusLogProbMetric: 17.0172, val_loss: 17.0766, val_MinusLogProbMetric: 17.0766

Epoch 967: val_loss did not improve from 17.05909
196/196 - 68s - loss: 17.0172 - MinusLogProbMetric: 17.0172 - val_loss: 17.0766 - val_MinusLogProbMetric: 17.0766 - lr: 4.1667e-05 - 68s/epoch - 349ms/step
Epoch 968/1000
2023-09-27 09:27:01.271 
Epoch 968/1000 
	 loss: 17.0152, MinusLogProbMetric: 17.0152, val_loss: 17.0879, val_MinusLogProbMetric: 17.0879

Epoch 968: val_loss did not improve from 17.05909
196/196 - 62s - loss: 17.0152 - MinusLogProbMetric: 17.0152 - val_loss: 17.0879 - val_MinusLogProbMetric: 17.0879 - lr: 4.1667e-05 - 62s/epoch - 317ms/step
Epoch 969/1000
2023-09-27 09:28:06.839 
Epoch 969/1000 
	 loss: 17.0133, MinusLogProbMetric: 17.0133, val_loss: 17.1294, val_MinusLogProbMetric: 17.1294

Epoch 969: val_loss did not improve from 17.05909
196/196 - 66s - loss: 17.0133 - MinusLogProbMetric: 17.0133 - val_loss: 17.1294 - val_MinusLogProbMetric: 17.1294 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 970/1000
2023-09-27 09:29:20.031 
Epoch 970/1000 
	 loss: 17.0169, MinusLogProbMetric: 17.0169, val_loss: 17.0634, val_MinusLogProbMetric: 17.0634

Epoch 970: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0169 - MinusLogProbMetric: 17.0169 - val_loss: 17.0634 - val_MinusLogProbMetric: 17.0634 - lr: 4.1667e-05 - 73s/epoch - 373ms/step
Epoch 971/1000
2023-09-27 09:30:31.555 
Epoch 971/1000 
	 loss: 17.0092, MinusLogProbMetric: 17.0092, val_loss: 17.0624, val_MinusLogProbMetric: 17.0624

Epoch 971: val_loss did not improve from 17.05909
196/196 - 72s - loss: 17.0092 - MinusLogProbMetric: 17.0092 - val_loss: 17.0624 - val_MinusLogProbMetric: 17.0624 - lr: 4.1667e-05 - 72s/epoch - 365ms/step
Epoch 972/1000
2023-09-27 09:31:44.889 
Epoch 972/1000 
	 loss: 17.0138, MinusLogProbMetric: 17.0138, val_loss: 17.0741, val_MinusLogProbMetric: 17.0741

Epoch 972: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0138 - MinusLogProbMetric: 17.0138 - val_loss: 17.0741 - val_MinusLogProbMetric: 17.0741 - lr: 4.1667e-05 - 73s/epoch - 374ms/step
Epoch 973/1000
2023-09-27 09:32:58.124 
Epoch 973/1000 
	 loss: 17.0114, MinusLogProbMetric: 17.0114, val_loss: 17.0845, val_MinusLogProbMetric: 17.0845

Epoch 973: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0114 - MinusLogProbMetric: 17.0114 - val_loss: 17.0845 - val_MinusLogProbMetric: 17.0845 - lr: 4.1667e-05 - 73s/epoch - 374ms/step
Epoch 974/1000
2023-09-27 09:34:11.184 
Epoch 974/1000 
	 loss: 17.0095, MinusLogProbMetric: 17.0095, val_loss: 17.0703, val_MinusLogProbMetric: 17.0703

Epoch 974: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0095 - MinusLogProbMetric: 17.0095 - val_loss: 17.0703 - val_MinusLogProbMetric: 17.0703 - lr: 4.1667e-05 - 73s/epoch - 373ms/step
Epoch 975/1000
2023-09-27 09:35:24.145 
Epoch 975/1000 
	 loss: 17.0114, MinusLogProbMetric: 17.0114, val_loss: 17.1519, val_MinusLogProbMetric: 17.1519

Epoch 975: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0114 - MinusLogProbMetric: 17.0114 - val_loss: 17.1519 - val_MinusLogProbMetric: 17.1519 - lr: 4.1667e-05 - 73s/epoch - 372ms/step
Epoch 976/1000
2023-09-27 09:36:38.741 
Epoch 976/1000 
	 loss: 17.0188, MinusLogProbMetric: 17.0188, val_loss: 17.0698, val_MinusLogProbMetric: 17.0698

Epoch 976: val_loss did not improve from 17.05909
196/196 - 75s - loss: 17.0188 - MinusLogProbMetric: 17.0188 - val_loss: 17.0698 - val_MinusLogProbMetric: 17.0698 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 977/1000
2023-09-27 09:37:52.567 
Epoch 977/1000 
	 loss: 17.0163, MinusLogProbMetric: 17.0163, val_loss: 17.0890, val_MinusLogProbMetric: 17.0890

Epoch 977: val_loss did not improve from 17.05909
196/196 - 74s - loss: 17.0163 - MinusLogProbMetric: 17.0163 - val_loss: 17.0890 - val_MinusLogProbMetric: 17.0890 - lr: 4.1667e-05 - 74s/epoch - 377ms/step
Epoch 978/1000
2023-09-27 09:39:06.064 
Epoch 978/1000 
	 loss: 17.0103, MinusLogProbMetric: 17.0103, val_loss: 17.0665, val_MinusLogProbMetric: 17.0665

Epoch 978: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0103 - MinusLogProbMetric: 17.0103 - val_loss: 17.0665 - val_MinusLogProbMetric: 17.0665 - lr: 4.1667e-05 - 73s/epoch - 375ms/step
Epoch 979/1000
2023-09-27 09:40:19.372 
Epoch 979/1000 
	 loss: 17.0136, MinusLogProbMetric: 17.0136, val_loss: 17.0743, val_MinusLogProbMetric: 17.0743

Epoch 979: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0136 - MinusLogProbMetric: 17.0136 - val_loss: 17.0743 - val_MinusLogProbMetric: 17.0743 - lr: 4.1667e-05 - 73s/epoch - 374ms/step
Epoch 980/1000
2023-09-27 09:41:32.591 
Epoch 980/1000 
	 loss: 17.0141, MinusLogProbMetric: 17.0141, val_loss: 17.0873, val_MinusLogProbMetric: 17.0873

Epoch 980: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0141 - MinusLogProbMetric: 17.0141 - val_loss: 17.0873 - val_MinusLogProbMetric: 17.0873 - lr: 4.1667e-05 - 73s/epoch - 374ms/step
Epoch 981/1000
2023-09-27 09:42:46.694 
Epoch 981/1000 
	 loss: 17.0229, MinusLogProbMetric: 17.0229, val_loss: 17.0803, val_MinusLogProbMetric: 17.0803

Epoch 981: val_loss did not improve from 17.05909
196/196 - 74s - loss: 17.0229 - MinusLogProbMetric: 17.0229 - val_loss: 17.0803 - val_MinusLogProbMetric: 17.0803 - lr: 4.1667e-05 - 74s/epoch - 378ms/step
Epoch 982/1000
2023-09-27 09:43:59.891 
Epoch 982/1000 
	 loss: 17.0070, MinusLogProbMetric: 17.0070, val_loss: 17.0618, val_MinusLogProbMetric: 17.0618

Epoch 982: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0070 - MinusLogProbMetric: 17.0070 - val_loss: 17.0618 - val_MinusLogProbMetric: 17.0618 - lr: 4.1667e-05 - 73s/epoch - 373ms/step
Epoch 983/1000
2023-09-27 09:45:13.043 
Epoch 983/1000 
	 loss: 17.0091, MinusLogProbMetric: 17.0091, val_loss: 17.0744, val_MinusLogProbMetric: 17.0744

Epoch 983: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0091 - MinusLogProbMetric: 17.0091 - val_loss: 17.0744 - val_MinusLogProbMetric: 17.0744 - lr: 4.1667e-05 - 73s/epoch - 373ms/step
Epoch 984/1000
2023-09-27 09:46:26.039 
Epoch 984/1000 
	 loss: 17.0183, MinusLogProbMetric: 17.0183, val_loss: 17.0771, val_MinusLogProbMetric: 17.0771

Epoch 984: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0183 - MinusLogProbMetric: 17.0183 - val_loss: 17.0771 - val_MinusLogProbMetric: 17.0771 - lr: 4.1667e-05 - 73s/epoch - 372ms/step
Epoch 985/1000
2023-09-27 09:47:38.788 
Epoch 985/1000 
	 loss: 17.0117, MinusLogProbMetric: 17.0117, val_loss: 17.0658, val_MinusLogProbMetric: 17.0658

Epoch 985: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0117 - MinusLogProbMetric: 17.0117 - val_loss: 17.0658 - val_MinusLogProbMetric: 17.0658 - lr: 4.1667e-05 - 73s/epoch - 371ms/step
Epoch 986/1000
2023-09-27 09:48:52.040 
Epoch 986/1000 
	 loss: 17.0120, MinusLogProbMetric: 17.0120, val_loss: 17.0656, val_MinusLogProbMetric: 17.0656

Epoch 986: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0120 - MinusLogProbMetric: 17.0120 - val_loss: 17.0656 - val_MinusLogProbMetric: 17.0656 - lr: 4.1667e-05 - 73s/epoch - 374ms/step
Epoch 987/1000
2023-09-27 09:50:05.074 
Epoch 987/1000 
	 loss: 17.0107, MinusLogProbMetric: 17.0107, val_loss: 17.0714, val_MinusLogProbMetric: 17.0714

Epoch 987: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0107 - MinusLogProbMetric: 17.0107 - val_loss: 17.0714 - val_MinusLogProbMetric: 17.0714 - lr: 4.1667e-05 - 73s/epoch - 373ms/step
Epoch 988/1000
2023-09-27 09:51:18.355 
Epoch 988/1000 
	 loss: 17.0078, MinusLogProbMetric: 17.0078, val_loss: 17.0726, val_MinusLogProbMetric: 17.0726

Epoch 988: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0078 - MinusLogProbMetric: 17.0078 - val_loss: 17.0726 - val_MinusLogProbMetric: 17.0726 - lr: 4.1667e-05 - 73s/epoch - 374ms/step
Epoch 989/1000
2023-09-27 09:52:31.800 
Epoch 989/1000 
	 loss: 17.0096, MinusLogProbMetric: 17.0096, val_loss: 17.0892, val_MinusLogProbMetric: 17.0892

Epoch 989: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0096 - MinusLogProbMetric: 17.0096 - val_loss: 17.0892 - val_MinusLogProbMetric: 17.0892 - lr: 4.1667e-05 - 73s/epoch - 375ms/step
Epoch 990/1000
2023-09-27 09:53:45.351 
Epoch 990/1000 
	 loss: 17.0191, MinusLogProbMetric: 17.0191, val_loss: 17.0904, val_MinusLogProbMetric: 17.0904

Epoch 990: val_loss did not improve from 17.05909
196/196 - 74s - loss: 17.0191 - MinusLogProbMetric: 17.0191 - val_loss: 17.0904 - val_MinusLogProbMetric: 17.0904 - lr: 4.1667e-05 - 74s/epoch - 375ms/step
Epoch 991/1000
2023-09-27 09:54:59.005 
Epoch 991/1000 
	 loss: 17.0177, MinusLogProbMetric: 17.0177, val_loss: 17.0759, val_MinusLogProbMetric: 17.0759

Epoch 991: val_loss did not improve from 17.05909
196/196 - 74s - loss: 17.0177 - MinusLogProbMetric: 17.0177 - val_loss: 17.0759 - val_MinusLogProbMetric: 17.0759 - lr: 4.1667e-05 - 74s/epoch - 376ms/step
Epoch 992/1000
2023-09-27 09:56:12.141 
Epoch 992/1000 
	 loss: 17.0199, MinusLogProbMetric: 17.0199, val_loss: 17.0749, val_MinusLogProbMetric: 17.0749

Epoch 992: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0199 - MinusLogProbMetric: 17.0199 - val_loss: 17.0749 - val_MinusLogProbMetric: 17.0749 - lr: 4.1667e-05 - 73s/epoch - 373ms/step
Epoch 993/1000
2023-09-27 09:57:24.985 
Epoch 993/1000 
	 loss: 17.0123, MinusLogProbMetric: 17.0123, val_loss: 17.0893, val_MinusLogProbMetric: 17.0893

Epoch 993: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0123 - MinusLogProbMetric: 17.0123 - val_loss: 17.0893 - val_MinusLogProbMetric: 17.0893 - lr: 4.1667e-05 - 73s/epoch - 372ms/step
Epoch 994/1000
2023-09-27 09:58:37.878 
Epoch 994/1000 
	 loss: 17.0090, MinusLogProbMetric: 17.0090, val_loss: 17.0666, val_MinusLogProbMetric: 17.0666

Epoch 994: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0090 - MinusLogProbMetric: 17.0090 - val_loss: 17.0666 - val_MinusLogProbMetric: 17.0666 - lr: 4.1667e-05 - 73s/epoch - 372ms/step
Epoch 995/1000
2023-09-27 09:59:50.395 
Epoch 995/1000 
	 loss: 17.0102, MinusLogProbMetric: 17.0102, val_loss: 17.0809, val_MinusLogProbMetric: 17.0809

Epoch 995: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0102 - MinusLogProbMetric: 17.0102 - val_loss: 17.0809 - val_MinusLogProbMetric: 17.0809 - lr: 4.1667e-05 - 73s/epoch - 370ms/step
Epoch 996/1000
2023-09-27 10:01:03.845 
Epoch 996/1000 
	 loss: 17.0164, MinusLogProbMetric: 17.0164, val_loss: 17.0709, val_MinusLogProbMetric: 17.0709

Epoch 996: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0164 - MinusLogProbMetric: 17.0164 - val_loss: 17.0709 - val_MinusLogProbMetric: 17.0709 - lr: 4.1667e-05 - 73s/epoch - 375ms/step
Epoch 997/1000
2023-09-27 10:02:16.644 
Epoch 997/1000 
	 loss: 17.0098, MinusLogProbMetric: 17.0098, val_loss: 17.0596, val_MinusLogProbMetric: 17.0596

Epoch 997: val_loss did not improve from 17.05909
196/196 - 73s - loss: 17.0098 - MinusLogProbMetric: 17.0098 - val_loss: 17.0596 - val_MinusLogProbMetric: 17.0596 - lr: 4.1667e-05 - 73s/epoch - 371ms/step
Epoch 998/1000
2023-09-27 10:03:29.680 
Epoch 998/1000 
	 loss: 17.0096, MinusLogProbMetric: 17.0096, val_loss: 17.0548, val_MinusLogProbMetric: 17.0548

Epoch 998: val_loss improved from 17.05909 to 17.05475, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_287/weights/best_weights.h5
196/196 - 75s - loss: 17.0096 - MinusLogProbMetric: 17.0096 - val_loss: 17.0548 - val_MinusLogProbMetric: 17.0548 - lr: 4.1667e-05 - 75s/epoch - 381ms/step
Epoch 999/1000
2023-09-27 10:04:44.829 
Epoch 999/1000 
	 loss: 17.0105, MinusLogProbMetric: 17.0105, val_loss: 17.0688, val_MinusLogProbMetric: 17.0688

Epoch 999: val_loss did not improve from 17.05475
196/196 - 74s - loss: 17.0105 - MinusLogProbMetric: 17.0105 - val_loss: 17.0688 - val_MinusLogProbMetric: 17.0688 - lr: 4.1667e-05 - 74s/epoch - 375ms/step
Epoch 1000/1000
2023-09-27 10:05:58.260 
Epoch 1000/1000 
	 loss: 17.0115, MinusLogProbMetric: 17.0115, val_loss: 17.0923, val_MinusLogProbMetric: 17.0923

Epoch 1000: val_loss did not improve from 17.05475
196/196 - 73s - loss: 17.0115 - MinusLogProbMetric: 17.0115 - val_loss: 17.0923 - val_MinusLogProbMetric: 17.0923 - lr: 4.1667e-05 - 73s/epoch - 375ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 27.514385549002327 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 16.430208106001373 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 12.624107342970092 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 13.215354370011482 seconds.
Training succeeded with seed 541.
Model trained in 74370.58 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 71.99 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 72.54 s.
===========
Run 287/720 done in 74628.72 s.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

===========
Generating train data for run 305.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_305/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_305/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_305/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_305
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_27"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_28 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  413360    
 yer)                                                            
                                                                 
=================================================================
Total params: 413,360
Trainable params: 413,360
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7ff5f88381c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff0fc3a7700>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff0fc3a7700>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff5f8838370>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fefb83b9a80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fefb83b9ff0>, <keras.callbacks.ModelCheckpoint object at 0x7fefb83ba0b0>, <keras.callbacks.EarlyStopping object at 0x7fefb83ba320>, <keras.callbacks.ReduceLROnPlateau object at 0x7fefb83ba350>, <keras.callbacks.TerminateOnNaN object at 0x7fefb83b9f90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_305/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 305/720 with hyperparameters:
timestamp = 2023-09-27 10:07:15.387963
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 413360
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
2023-09-27 10:08:49.840 
Epoch 1/1000 
	 loss: 83.4374, MinusLogProbMetric: 83.4374, val_loss: 32.4421, val_MinusLogProbMetric: 32.4421

Epoch 1: val_loss improved from inf to 32.44208, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 95s - loss: 83.4374 - MinusLogProbMetric: 83.4374 - val_loss: 32.4421 - val_MinusLogProbMetric: 32.4421 - lr: 0.0010 - 95s/epoch - 484ms/step
Epoch 2/1000
2023-09-27 10:09:22.795 
Epoch 2/1000 
	 loss: 28.6038, MinusLogProbMetric: 28.6038, val_loss: 27.3399, val_MinusLogProbMetric: 27.3399

Epoch 2: val_loss improved from 32.44208 to 27.33987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 28.6038 - MinusLogProbMetric: 28.6038 - val_loss: 27.3399 - val_MinusLogProbMetric: 27.3399 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 3/1000
2023-09-27 10:09:55.455 
Epoch 3/1000 
	 loss: 24.9455, MinusLogProbMetric: 24.9455, val_loss: 23.8625, val_MinusLogProbMetric: 23.8625

Epoch 3: val_loss improved from 27.33987 to 23.86246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 24.9455 - MinusLogProbMetric: 24.9455 - val_loss: 23.8625 - val_MinusLogProbMetric: 23.8625 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 4/1000
2023-09-27 10:10:28.073 
Epoch 4/1000 
	 loss: 23.4233, MinusLogProbMetric: 23.4233, val_loss: 22.4275, val_MinusLogProbMetric: 22.4275

Epoch 4: val_loss improved from 23.86246 to 22.42754, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 23.4233 - MinusLogProbMetric: 23.4233 - val_loss: 22.4275 - val_MinusLogProbMetric: 22.4275 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 5/1000
2023-09-27 10:11:00.800 
Epoch 5/1000 
	 loss: 22.2891, MinusLogProbMetric: 22.2891, val_loss: 21.6157, val_MinusLogProbMetric: 21.6157

Epoch 5: val_loss improved from 22.42754 to 21.61566, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 32s - loss: 22.2891 - MinusLogProbMetric: 22.2891 - val_loss: 21.6157 - val_MinusLogProbMetric: 21.6157 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 6/1000
2023-09-27 10:11:33.114 
Epoch 6/1000 
	 loss: 21.6261, MinusLogProbMetric: 21.6261, val_loss: 21.4420, val_MinusLogProbMetric: 21.4420

Epoch 6: val_loss improved from 21.61566 to 21.44197, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 21.6261 - MinusLogProbMetric: 21.6261 - val_loss: 21.4420 - val_MinusLogProbMetric: 21.4420 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 7/1000
2023-09-27 10:12:05.802 
Epoch 7/1000 
	 loss: 21.2093, MinusLogProbMetric: 21.2093, val_loss: 22.0387, val_MinusLogProbMetric: 22.0387

Epoch 7: val_loss did not improve from 21.44197
196/196 - 32s - loss: 21.2093 - MinusLogProbMetric: 21.2093 - val_loss: 22.0387 - val_MinusLogProbMetric: 22.0387 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 8/1000
2023-09-27 10:12:38.001 
Epoch 8/1000 
	 loss: 20.7404, MinusLogProbMetric: 20.7404, val_loss: 21.0663, val_MinusLogProbMetric: 21.0663

Epoch 8: val_loss improved from 21.44197 to 21.06633, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 20.7404 - MinusLogProbMetric: 20.7404 - val_loss: 21.0663 - val_MinusLogProbMetric: 21.0663 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 9/1000
2023-09-27 10:13:10.823 
Epoch 9/1000 
	 loss: 20.5786, MinusLogProbMetric: 20.5786, val_loss: 20.3442, val_MinusLogProbMetric: 20.3442

Epoch 9: val_loss improved from 21.06633 to 20.34415, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 20.5786 - MinusLogProbMetric: 20.5786 - val_loss: 20.3442 - val_MinusLogProbMetric: 20.3442 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 10/1000
2023-09-27 10:13:44.094 
Epoch 10/1000 
	 loss: 20.2432, MinusLogProbMetric: 20.2432, val_loss: 20.5671, val_MinusLogProbMetric: 20.5671

Epoch 10: val_loss did not improve from 20.34415
196/196 - 33s - loss: 20.2432 - MinusLogProbMetric: 20.2432 - val_loss: 20.5671 - val_MinusLogProbMetric: 20.5671 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 11/1000
2023-09-27 10:14:16.461 
Epoch 11/1000 
	 loss: 20.2418, MinusLogProbMetric: 20.2418, val_loss: 19.8404, val_MinusLogProbMetric: 19.8404

Epoch 11: val_loss improved from 20.34415 to 19.84037, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 20.2418 - MinusLogProbMetric: 20.2418 - val_loss: 19.8404 - val_MinusLogProbMetric: 19.8404 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 12/1000
2023-09-27 10:14:49.121 
Epoch 12/1000 
	 loss: 19.9520, MinusLogProbMetric: 19.9520, val_loss: 19.4412, val_MinusLogProbMetric: 19.4412

Epoch 12: val_loss improved from 19.84037 to 19.44116, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 19.9520 - MinusLogProbMetric: 19.9520 - val_loss: 19.4412 - val_MinusLogProbMetric: 19.4412 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 13/1000
2023-09-27 10:15:22.162 
Epoch 13/1000 
	 loss: 19.7356, MinusLogProbMetric: 19.7356, val_loss: 19.7732, val_MinusLogProbMetric: 19.7732

Epoch 13: val_loss did not improve from 19.44116
196/196 - 32s - loss: 19.7356 - MinusLogProbMetric: 19.7356 - val_loss: 19.7732 - val_MinusLogProbMetric: 19.7732 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 14/1000
2023-09-27 10:15:54.385 
Epoch 14/1000 
	 loss: 19.6409, MinusLogProbMetric: 19.6409, val_loss: 19.7671, val_MinusLogProbMetric: 19.7671

Epoch 14: val_loss did not improve from 19.44116
196/196 - 32s - loss: 19.6409 - MinusLogProbMetric: 19.6409 - val_loss: 19.7671 - val_MinusLogProbMetric: 19.7671 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 15/1000
2023-09-27 10:16:26.502 
Epoch 15/1000 
	 loss: 19.3849, MinusLogProbMetric: 19.3849, val_loss: 19.2589, val_MinusLogProbMetric: 19.2589

Epoch 15: val_loss improved from 19.44116 to 19.25891, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 19.3849 - MinusLogProbMetric: 19.3849 - val_loss: 19.2589 - val_MinusLogProbMetric: 19.2589 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 16/1000
2023-09-27 10:16:59.247 
Epoch 16/1000 
	 loss: 19.2582, MinusLogProbMetric: 19.2582, val_loss: 19.4063, val_MinusLogProbMetric: 19.4063

Epoch 16: val_loss did not improve from 19.25891
196/196 - 32s - loss: 19.2582 - MinusLogProbMetric: 19.2582 - val_loss: 19.4063 - val_MinusLogProbMetric: 19.4063 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 17/1000
2023-09-27 10:17:31.395 
Epoch 17/1000 
	 loss: 19.2165, MinusLogProbMetric: 19.2165, val_loss: 20.1708, val_MinusLogProbMetric: 20.1708

Epoch 17: val_loss did not improve from 19.25891
196/196 - 32s - loss: 19.2165 - MinusLogProbMetric: 19.2165 - val_loss: 20.1708 - val_MinusLogProbMetric: 20.1708 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 18/1000
2023-09-27 10:18:03.358 
Epoch 18/1000 
	 loss: 19.1100, MinusLogProbMetric: 19.1100, val_loss: 20.5925, val_MinusLogProbMetric: 20.5925

Epoch 18: val_loss did not improve from 19.25891
196/196 - 32s - loss: 19.1100 - MinusLogProbMetric: 19.1100 - val_loss: 20.5925 - val_MinusLogProbMetric: 20.5925 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 19/1000
2023-09-27 10:18:35.571 
Epoch 19/1000 
	 loss: 19.0922, MinusLogProbMetric: 19.0922, val_loss: 18.7545, val_MinusLogProbMetric: 18.7545

Epoch 19: val_loss improved from 19.25891 to 18.75445, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 19.0922 - MinusLogProbMetric: 19.0922 - val_loss: 18.7545 - val_MinusLogProbMetric: 18.7545 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 20/1000
2023-09-27 10:19:08.275 
Epoch 20/1000 
	 loss: 18.9288, MinusLogProbMetric: 18.9288, val_loss: 18.6088, val_MinusLogProbMetric: 18.6088

Epoch 20: val_loss improved from 18.75445 to 18.60877, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 18.9288 - MinusLogProbMetric: 18.9288 - val_loss: 18.6088 - val_MinusLogProbMetric: 18.6088 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 21/1000
2023-09-27 10:19:41.273 
Epoch 21/1000 
	 loss: 18.7369, MinusLogProbMetric: 18.7369, val_loss: 18.8777, val_MinusLogProbMetric: 18.8777

Epoch 21: val_loss did not improve from 18.60877
196/196 - 32s - loss: 18.7369 - MinusLogProbMetric: 18.7369 - val_loss: 18.8777 - val_MinusLogProbMetric: 18.8777 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 22/1000
2023-09-27 10:20:13.297 
Epoch 22/1000 
	 loss: 18.5933, MinusLogProbMetric: 18.5933, val_loss: 18.9726, val_MinusLogProbMetric: 18.9726

Epoch 22: val_loss did not improve from 18.60877
196/196 - 32s - loss: 18.5933 - MinusLogProbMetric: 18.5933 - val_loss: 18.9726 - val_MinusLogProbMetric: 18.9726 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 23/1000
2023-09-27 10:20:45.795 
Epoch 23/1000 
	 loss: 18.5421, MinusLogProbMetric: 18.5421, val_loss: 21.1248, val_MinusLogProbMetric: 21.1248

Epoch 23: val_loss did not improve from 18.60877
196/196 - 32s - loss: 18.5421 - MinusLogProbMetric: 18.5421 - val_loss: 21.1248 - val_MinusLogProbMetric: 21.1248 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 24/1000
2023-09-27 10:21:18.046 
Epoch 24/1000 
	 loss: 18.6989, MinusLogProbMetric: 18.6989, val_loss: 18.5772, val_MinusLogProbMetric: 18.5772

Epoch 24: val_loss improved from 18.60877 to 18.57718, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 18.6989 - MinusLogProbMetric: 18.6989 - val_loss: 18.5772 - val_MinusLogProbMetric: 18.5772 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 25/1000
2023-09-27 10:21:50.982 
Epoch 25/1000 
	 loss: 18.4811, MinusLogProbMetric: 18.4811, val_loss: 19.0554, val_MinusLogProbMetric: 19.0554

Epoch 25: val_loss did not improve from 18.57718
196/196 - 32s - loss: 18.4811 - MinusLogProbMetric: 18.4811 - val_loss: 19.0554 - val_MinusLogProbMetric: 19.0554 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 26/1000
2023-09-27 10:22:23.422 
Epoch 26/1000 
	 loss: 18.3991, MinusLogProbMetric: 18.3991, val_loss: 18.4951, val_MinusLogProbMetric: 18.4951

Epoch 26: val_loss improved from 18.57718 to 18.49507, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 18.3991 - MinusLogProbMetric: 18.3991 - val_loss: 18.4951 - val_MinusLogProbMetric: 18.4951 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 27/1000
2023-09-27 10:22:56.609 
Epoch 27/1000 
	 loss: 18.4123, MinusLogProbMetric: 18.4123, val_loss: 18.9379, val_MinusLogProbMetric: 18.9379

Epoch 27: val_loss did not improve from 18.49507
196/196 - 33s - loss: 18.4123 - MinusLogProbMetric: 18.4123 - val_loss: 18.9379 - val_MinusLogProbMetric: 18.9379 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 28/1000
2023-09-27 10:23:28.897 
Epoch 28/1000 
	 loss: 18.2682, MinusLogProbMetric: 18.2682, val_loss: 18.3106, val_MinusLogProbMetric: 18.3106

Epoch 28: val_loss improved from 18.49507 to 18.31064, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 18.2682 - MinusLogProbMetric: 18.2682 - val_loss: 18.3106 - val_MinusLogProbMetric: 18.3106 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 29/1000
2023-09-27 10:24:01.933 
Epoch 29/1000 
	 loss: 18.3126, MinusLogProbMetric: 18.3126, val_loss: 18.6356, val_MinusLogProbMetric: 18.6356

Epoch 29: val_loss did not improve from 18.31064
196/196 - 32s - loss: 18.3126 - MinusLogProbMetric: 18.3126 - val_loss: 18.6356 - val_MinusLogProbMetric: 18.6356 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 30/1000
2023-09-27 10:24:34.090 
Epoch 30/1000 
	 loss: 18.2601, MinusLogProbMetric: 18.2601, val_loss: 18.0514, val_MinusLogProbMetric: 18.0514

Epoch 30: val_loss improved from 18.31064 to 18.05141, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 18.2601 - MinusLogProbMetric: 18.2601 - val_loss: 18.0514 - val_MinusLogProbMetric: 18.0514 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 31/1000
2023-09-27 10:25:07.001 
Epoch 31/1000 
	 loss: 18.1817, MinusLogProbMetric: 18.1817, val_loss: 18.2275, val_MinusLogProbMetric: 18.2275

Epoch 31: val_loss did not improve from 18.05141
196/196 - 32s - loss: 18.1817 - MinusLogProbMetric: 18.1817 - val_loss: 18.2275 - val_MinusLogProbMetric: 18.2275 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 32/1000
2023-09-27 10:25:39.110 
Epoch 32/1000 
	 loss: 18.0757, MinusLogProbMetric: 18.0757, val_loss: 18.0320, val_MinusLogProbMetric: 18.0320

Epoch 32: val_loss improved from 18.05141 to 18.03196, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 18.0757 - MinusLogProbMetric: 18.0757 - val_loss: 18.0320 - val_MinusLogProbMetric: 18.0320 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 33/1000
2023-09-27 10:26:11.854 
Epoch 33/1000 
	 loss: 18.1577, MinusLogProbMetric: 18.1577, val_loss: 17.9310, val_MinusLogProbMetric: 17.9310

Epoch 33: val_loss improved from 18.03196 to 17.93102, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 18.1577 - MinusLogProbMetric: 18.1577 - val_loss: 17.9310 - val_MinusLogProbMetric: 17.9310 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 34/1000
2023-09-27 10:26:44.567 
Epoch 34/1000 
	 loss: 18.0690, MinusLogProbMetric: 18.0690, val_loss: 18.5860, val_MinusLogProbMetric: 18.5860

Epoch 34: val_loss did not improve from 17.93102
196/196 - 32s - loss: 18.0690 - MinusLogProbMetric: 18.0690 - val_loss: 18.5860 - val_MinusLogProbMetric: 18.5860 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 35/1000
2023-09-27 10:27:16.638 
Epoch 35/1000 
	 loss: 17.9794, MinusLogProbMetric: 17.9794, val_loss: 18.1544, val_MinusLogProbMetric: 18.1544

Epoch 35: val_loss did not improve from 17.93102
196/196 - 32s - loss: 17.9794 - MinusLogProbMetric: 17.9794 - val_loss: 18.1544 - val_MinusLogProbMetric: 18.1544 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 36/1000
2023-09-27 10:27:48.837 
Epoch 36/1000 
	 loss: 18.0285, MinusLogProbMetric: 18.0285, val_loss: 18.0874, val_MinusLogProbMetric: 18.0874

Epoch 36: val_loss did not improve from 17.93102
196/196 - 32s - loss: 18.0285 - MinusLogProbMetric: 18.0285 - val_loss: 18.0874 - val_MinusLogProbMetric: 18.0874 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 37/1000
2023-09-27 10:28:21.085 
Epoch 37/1000 
	 loss: 17.9558, MinusLogProbMetric: 17.9558, val_loss: 18.6806, val_MinusLogProbMetric: 18.6806

Epoch 37: val_loss did not improve from 17.93102
196/196 - 32s - loss: 17.9558 - MinusLogProbMetric: 17.9558 - val_loss: 18.6806 - val_MinusLogProbMetric: 18.6806 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 38/1000
2023-09-27 10:28:53.497 
Epoch 38/1000 
	 loss: 17.9793, MinusLogProbMetric: 17.9793, val_loss: 18.0113, val_MinusLogProbMetric: 18.0113

Epoch 38: val_loss did not improve from 17.93102
196/196 - 32s - loss: 17.9793 - MinusLogProbMetric: 17.9793 - val_loss: 18.0113 - val_MinusLogProbMetric: 18.0113 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 39/1000
2023-09-27 10:29:25.634 
Epoch 39/1000 
	 loss: 17.8621, MinusLogProbMetric: 17.8621, val_loss: 17.8428, val_MinusLogProbMetric: 17.8428

Epoch 39: val_loss improved from 17.93102 to 17.84282, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 17.8621 - MinusLogProbMetric: 17.8621 - val_loss: 17.8428 - val_MinusLogProbMetric: 17.8428 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 40/1000
2023-09-27 10:29:58.605 
Epoch 40/1000 
	 loss: 17.8790, MinusLogProbMetric: 17.8790, val_loss: 17.7643, val_MinusLogProbMetric: 17.7643

Epoch 40: val_loss improved from 17.84282 to 17.76434, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 17.8790 - MinusLogProbMetric: 17.8790 - val_loss: 17.7643 - val_MinusLogProbMetric: 17.7643 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 41/1000
2023-09-27 10:30:31.469 
Epoch 41/1000 
	 loss: 17.8582, MinusLogProbMetric: 17.8582, val_loss: 17.8932, val_MinusLogProbMetric: 17.8932

Epoch 41: val_loss did not improve from 17.76434
196/196 - 32s - loss: 17.8582 - MinusLogProbMetric: 17.8582 - val_loss: 17.8932 - val_MinusLogProbMetric: 17.8932 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 42/1000
2023-09-27 10:31:03.182 
Epoch 42/1000 
	 loss: 17.8309, MinusLogProbMetric: 17.8309, val_loss: 17.7659, val_MinusLogProbMetric: 17.7659

Epoch 42: val_loss did not improve from 17.76434
196/196 - 32s - loss: 17.8309 - MinusLogProbMetric: 17.8309 - val_loss: 17.7659 - val_MinusLogProbMetric: 17.7659 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 43/1000
2023-09-27 10:31:35.598 
Epoch 43/1000 
	 loss: 17.7642, MinusLogProbMetric: 17.7642, val_loss: 17.6680, val_MinusLogProbMetric: 17.6680

Epoch 43: val_loss improved from 17.76434 to 17.66801, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 17.7642 - MinusLogProbMetric: 17.7642 - val_loss: 17.6680 - val_MinusLogProbMetric: 17.6680 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 44/1000
2023-09-27 10:32:08.393 
Epoch 44/1000 
	 loss: 17.7569, MinusLogProbMetric: 17.7569, val_loss: 18.1057, val_MinusLogProbMetric: 18.1057

Epoch 44: val_loss did not improve from 17.66801
196/196 - 32s - loss: 17.7569 - MinusLogProbMetric: 17.7569 - val_loss: 18.1057 - val_MinusLogProbMetric: 18.1057 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 45/1000
2023-09-27 10:32:40.642 
Epoch 45/1000 
	 loss: 17.7247, MinusLogProbMetric: 17.7247, val_loss: 18.0551, val_MinusLogProbMetric: 18.0551

Epoch 45: val_loss did not improve from 17.66801
196/196 - 32s - loss: 17.7247 - MinusLogProbMetric: 17.7247 - val_loss: 18.0551 - val_MinusLogProbMetric: 18.0551 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 46/1000
2023-09-27 10:33:12.867 
Epoch 46/1000 
	 loss: 17.7387, MinusLogProbMetric: 17.7387, val_loss: 17.9831, val_MinusLogProbMetric: 17.9831

Epoch 46: val_loss did not improve from 17.66801
196/196 - 32s - loss: 17.7387 - MinusLogProbMetric: 17.7387 - val_loss: 17.9831 - val_MinusLogProbMetric: 17.9831 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 47/1000
2023-09-27 10:33:44.928 
Epoch 47/1000 
	 loss: 17.7612, MinusLogProbMetric: 17.7612, val_loss: 17.8294, val_MinusLogProbMetric: 17.8294

Epoch 47: val_loss did not improve from 17.66801
196/196 - 32s - loss: 17.7612 - MinusLogProbMetric: 17.7612 - val_loss: 17.8294 - val_MinusLogProbMetric: 17.8294 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 48/1000
2023-09-27 10:34:17.334 
Epoch 48/1000 
	 loss: 17.6620, MinusLogProbMetric: 17.6620, val_loss: 17.6136, val_MinusLogProbMetric: 17.6136

Epoch 48: val_loss improved from 17.66801 to 17.61363, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 17.6620 - MinusLogProbMetric: 17.6620 - val_loss: 17.6136 - val_MinusLogProbMetric: 17.6136 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 49/1000
2023-09-27 10:34:50.032 
Epoch 49/1000 
	 loss: 17.6804, MinusLogProbMetric: 17.6804, val_loss: 18.3456, val_MinusLogProbMetric: 18.3456

Epoch 49: val_loss did not improve from 17.61363
196/196 - 32s - loss: 17.6804 - MinusLogProbMetric: 17.6804 - val_loss: 18.3456 - val_MinusLogProbMetric: 18.3456 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 50/1000
2023-09-27 10:35:21.688 
Epoch 50/1000 
	 loss: 17.6920, MinusLogProbMetric: 17.6920, val_loss: 18.1277, val_MinusLogProbMetric: 18.1277

Epoch 50: val_loss did not improve from 17.61363
196/196 - 32s - loss: 17.6920 - MinusLogProbMetric: 17.6920 - val_loss: 18.1277 - val_MinusLogProbMetric: 18.1277 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 51/1000
2023-09-27 10:35:54.132 
Epoch 51/1000 
	 loss: 17.6498, MinusLogProbMetric: 17.6498, val_loss: 17.7683, val_MinusLogProbMetric: 17.7683

Epoch 51: val_loss did not improve from 17.61363
196/196 - 32s - loss: 17.6498 - MinusLogProbMetric: 17.6498 - val_loss: 17.7683 - val_MinusLogProbMetric: 17.7683 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 52/1000
2023-09-27 10:36:26.785 
Epoch 52/1000 
	 loss: 17.6749, MinusLogProbMetric: 17.6749, val_loss: 17.9815, val_MinusLogProbMetric: 17.9815

Epoch 52: val_loss did not improve from 17.61363
196/196 - 33s - loss: 17.6749 - MinusLogProbMetric: 17.6749 - val_loss: 17.9815 - val_MinusLogProbMetric: 17.9815 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 53/1000
2023-09-27 10:36:58.914 
Epoch 53/1000 
	 loss: 17.6071, MinusLogProbMetric: 17.6071, val_loss: 17.7213, val_MinusLogProbMetric: 17.7213

Epoch 53: val_loss did not improve from 17.61363
196/196 - 32s - loss: 17.6071 - MinusLogProbMetric: 17.6071 - val_loss: 17.7213 - val_MinusLogProbMetric: 17.7213 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 54/1000
2023-09-27 10:37:31.058 
Epoch 54/1000 
	 loss: 17.5950, MinusLogProbMetric: 17.5950, val_loss: 17.8049, val_MinusLogProbMetric: 17.8049

Epoch 54: val_loss did not improve from 17.61363
196/196 - 32s - loss: 17.5950 - MinusLogProbMetric: 17.5950 - val_loss: 17.8049 - val_MinusLogProbMetric: 17.8049 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 55/1000
2023-09-27 10:38:03.359 
Epoch 55/1000 
	 loss: 17.5671, MinusLogProbMetric: 17.5671, val_loss: 17.7343, val_MinusLogProbMetric: 17.7343

Epoch 55: val_loss did not improve from 17.61363
196/196 - 32s - loss: 17.5671 - MinusLogProbMetric: 17.5671 - val_loss: 17.7343 - val_MinusLogProbMetric: 17.7343 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 56/1000
2023-09-27 10:38:35.500 
Epoch 56/1000 
	 loss: 17.6302, MinusLogProbMetric: 17.6302, val_loss: 17.5647, val_MinusLogProbMetric: 17.5647

Epoch 56: val_loss improved from 17.61363 to 17.56465, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 17.6302 - MinusLogProbMetric: 17.6302 - val_loss: 17.5647 - val_MinusLogProbMetric: 17.5647 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 57/1000
2023-09-27 10:39:08.212 
Epoch 57/1000 
	 loss: 17.5639, MinusLogProbMetric: 17.5639, val_loss: 17.9902, val_MinusLogProbMetric: 17.9902

Epoch 57: val_loss did not improve from 17.56465
196/196 - 32s - loss: 17.5639 - MinusLogProbMetric: 17.5639 - val_loss: 17.9902 - val_MinusLogProbMetric: 17.9902 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 58/1000
2023-09-27 10:39:40.383 
Epoch 58/1000 
	 loss: 17.6014, MinusLogProbMetric: 17.6014, val_loss: 17.8839, val_MinusLogProbMetric: 17.8839

Epoch 58: val_loss did not improve from 17.56465
196/196 - 32s - loss: 17.6014 - MinusLogProbMetric: 17.6014 - val_loss: 17.8839 - val_MinusLogProbMetric: 17.8839 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 59/1000
2023-09-27 10:40:12.672 
Epoch 59/1000 
	 loss: 17.5309, MinusLogProbMetric: 17.5309, val_loss: 17.8571, val_MinusLogProbMetric: 17.8571

Epoch 59: val_loss did not improve from 17.56465
196/196 - 32s - loss: 17.5309 - MinusLogProbMetric: 17.5309 - val_loss: 17.8571 - val_MinusLogProbMetric: 17.8571 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 60/1000
2023-09-27 10:40:44.683 
Epoch 60/1000 
	 loss: 17.5486, MinusLogProbMetric: 17.5486, val_loss: 18.5379, val_MinusLogProbMetric: 18.5379

Epoch 60: val_loss did not improve from 17.56465
196/196 - 32s - loss: 17.5486 - MinusLogProbMetric: 17.5486 - val_loss: 18.5379 - val_MinusLogProbMetric: 18.5379 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 61/1000
2023-09-27 10:41:16.845 
Epoch 61/1000 
	 loss: 17.5204, MinusLogProbMetric: 17.5204, val_loss: 17.5226, val_MinusLogProbMetric: 17.5226

Epoch 61: val_loss improved from 17.56465 to 17.52265, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 17.5204 - MinusLogProbMetric: 17.5204 - val_loss: 17.5226 - val_MinusLogProbMetric: 17.5226 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 62/1000
2023-09-27 10:41:49.438 
Epoch 62/1000 
	 loss: 17.5175, MinusLogProbMetric: 17.5175, val_loss: 17.5620, val_MinusLogProbMetric: 17.5620

Epoch 62: val_loss did not improve from 17.52265
196/196 - 32s - loss: 17.5175 - MinusLogProbMetric: 17.5175 - val_loss: 17.5620 - val_MinusLogProbMetric: 17.5620 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 63/1000
2023-09-27 10:42:21.504 
Epoch 63/1000 
	 loss: 17.5534, MinusLogProbMetric: 17.5534, val_loss: 17.4915, val_MinusLogProbMetric: 17.4915

Epoch 63: val_loss improved from 17.52265 to 17.49151, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 17.5534 - MinusLogProbMetric: 17.5534 - val_loss: 17.4915 - val_MinusLogProbMetric: 17.4915 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 64/1000
2023-09-27 10:42:54.262 
Epoch 64/1000 
	 loss: 17.4593, MinusLogProbMetric: 17.4593, val_loss: 17.8278, val_MinusLogProbMetric: 17.8278

Epoch 64: val_loss did not improve from 17.49151
196/196 - 32s - loss: 17.4593 - MinusLogProbMetric: 17.4593 - val_loss: 17.8278 - val_MinusLogProbMetric: 17.8278 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 65/1000
2023-09-27 10:43:26.428 
Epoch 65/1000 
	 loss: 17.4918, MinusLogProbMetric: 17.4918, val_loss: 17.6530, val_MinusLogProbMetric: 17.6530

Epoch 65: val_loss did not improve from 17.49151
196/196 - 32s - loss: 17.4918 - MinusLogProbMetric: 17.4918 - val_loss: 17.6530 - val_MinusLogProbMetric: 17.6530 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 66/1000
2023-09-27 10:43:58.522 
Epoch 66/1000 
	 loss: 17.4556, MinusLogProbMetric: 17.4556, val_loss: 18.0131, val_MinusLogProbMetric: 18.0131

Epoch 66: val_loss did not improve from 17.49151
196/196 - 32s - loss: 17.4556 - MinusLogProbMetric: 17.4556 - val_loss: 18.0131 - val_MinusLogProbMetric: 18.0131 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 67/1000
2023-09-27 10:44:30.893 
Epoch 67/1000 
	 loss: 17.4632, MinusLogProbMetric: 17.4632, val_loss: 17.4984, val_MinusLogProbMetric: 17.4984

Epoch 67: val_loss did not improve from 17.49151
196/196 - 32s - loss: 17.4632 - MinusLogProbMetric: 17.4632 - val_loss: 17.4984 - val_MinusLogProbMetric: 17.4984 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 68/1000
2023-09-27 10:45:02.930 
Epoch 68/1000 
	 loss: 17.4218, MinusLogProbMetric: 17.4218, val_loss: 17.8412, val_MinusLogProbMetric: 17.8412

Epoch 68: val_loss did not improve from 17.49151
196/196 - 32s - loss: 17.4218 - MinusLogProbMetric: 17.4218 - val_loss: 17.8412 - val_MinusLogProbMetric: 17.8412 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 69/1000
2023-09-27 10:45:35.286 
Epoch 69/1000 
	 loss: 17.4773, MinusLogProbMetric: 17.4773, val_loss: 17.3826, val_MinusLogProbMetric: 17.3826

Epoch 69: val_loss improved from 17.49151 to 17.38262, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 17.4773 - MinusLogProbMetric: 17.4773 - val_loss: 17.3826 - val_MinusLogProbMetric: 17.3826 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 70/1000
2023-09-27 10:46:08.026 
Epoch 70/1000 
	 loss: 17.4212, MinusLogProbMetric: 17.4212, val_loss: 17.8069, val_MinusLogProbMetric: 17.8069

Epoch 70: val_loss did not improve from 17.38262
196/196 - 32s - loss: 17.4212 - MinusLogProbMetric: 17.4212 - val_loss: 17.8069 - val_MinusLogProbMetric: 17.8069 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 71/1000
2023-09-27 10:46:40.183 
Epoch 71/1000 
	 loss: 17.4427, MinusLogProbMetric: 17.4427, val_loss: 17.4169, val_MinusLogProbMetric: 17.4169

Epoch 71: val_loss did not improve from 17.38262
196/196 - 32s - loss: 17.4427 - MinusLogProbMetric: 17.4427 - val_loss: 17.4169 - val_MinusLogProbMetric: 17.4169 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 72/1000
2023-09-27 10:47:12.401 
Epoch 72/1000 
	 loss: 17.4600, MinusLogProbMetric: 17.4600, val_loss: 17.2792, val_MinusLogProbMetric: 17.2792

Epoch 72: val_loss improved from 17.38262 to 17.27917, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 17.4600 - MinusLogProbMetric: 17.4600 - val_loss: 17.2792 - val_MinusLogProbMetric: 17.2792 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 73/1000
2023-09-27 10:47:44.795 
Epoch 73/1000 
	 loss: 17.4006, MinusLogProbMetric: 17.4006, val_loss: 17.5951, val_MinusLogProbMetric: 17.5951

Epoch 73: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.4006 - MinusLogProbMetric: 17.4006 - val_loss: 17.5951 - val_MinusLogProbMetric: 17.5951 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 74/1000
2023-09-27 10:48:16.675 
Epoch 74/1000 
	 loss: 17.3875, MinusLogProbMetric: 17.3875, val_loss: 17.2870, val_MinusLogProbMetric: 17.2870

Epoch 74: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.3875 - MinusLogProbMetric: 17.3875 - val_loss: 17.2870 - val_MinusLogProbMetric: 17.2870 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 75/1000
2023-09-27 10:48:49.114 
Epoch 75/1000 
	 loss: 17.4005, MinusLogProbMetric: 17.4005, val_loss: 17.3311, val_MinusLogProbMetric: 17.3311

Epoch 75: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.4005 - MinusLogProbMetric: 17.4005 - val_loss: 17.3311 - val_MinusLogProbMetric: 17.3311 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 76/1000
2023-09-27 10:49:20.651 
Epoch 76/1000 
	 loss: 17.3644, MinusLogProbMetric: 17.3644, val_loss: 17.3574, val_MinusLogProbMetric: 17.3574

Epoch 76: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.3644 - MinusLogProbMetric: 17.3644 - val_loss: 17.3574 - val_MinusLogProbMetric: 17.3574 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 77/1000
2023-09-27 10:49:52.798 
Epoch 77/1000 
	 loss: 17.4069, MinusLogProbMetric: 17.4069, val_loss: 17.6050, val_MinusLogProbMetric: 17.6050

Epoch 77: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.4069 - MinusLogProbMetric: 17.4069 - val_loss: 17.6050 - val_MinusLogProbMetric: 17.6050 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 78/1000
2023-09-27 10:50:24.645 
Epoch 78/1000 
	 loss: 17.3132, MinusLogProbMetric: 17.3132, val_loss: 17.8541, val_MinusLogProbMetric: 17.8541

Epoch 78: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.3132 - MinusLogProbMetric: 17.3132 - val_loss: 17.8541 - val_MinusLogProbMetric: 17.8541 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 79/1000
2023-09-27 10:50:56.735 
Epoch 79/1000 
	 loss: 17.3733, MinusLogProbMetric: 17.3733, val_loss: 17.8933, val_MinusLogProbMetric: 17.8933

Epoch 79: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.3733 - MinusLogProbMetric: 17.3733 - val_loss: 17.8933 - val_MinusLogProbMetric: 17.8933 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 80/1000
2023-09-27 10:51:28.595 
Epoch 80/1000 
	 loss: 17.3666, MinusLogProbMetric: 17.3666, val_loss: 17.3552, val_MinusLogProbMetric: 17.3552

Epoch 80: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.3666 - MinusLogProbMetric: 17.3666 - val_loss: 17.3552 - val_MinusLogProbMetric: 17.3552 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 81/1000
2023-09-27 10:52:00.485 
Epoch 81/1000 
	 loss: 17.3475, MinusLogProbMetric: 17.3475, val_loss: 17.2968, val_MinusLogProbMetric: 17.2968

Epoch 81: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.3475 - MinusLogProbMetric: 17.3475 - val_loss: 17.2968 - val_MinusLogProbMetric: 17.2968 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 82/1000
2023-09-27 10:52:33.068 
Epoch 82/1000 
	 loss: 17.3707, MinusLogProbMetric: 17.3707, val_loss: 17.7331, val_MinusLogProbMetric: 17.7331

Epoch 82: val_loss did not improve from 17.27917
196/196 - 33s - loss: 17.3707 - MinusLogProbMetric: 17.3707 - val_loss: 17.7331 - val_MinusLogProbMetric: 17.7331 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 83/1000
2023-09-27 10:53:05.149 
Epoch 83/1000 
	 loss: 17.3148, MinusLogProbMetric: 17.3148, val_loss: 17.9258, val_MinusLogProbMetric: 17.9258

Epoch 83: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.3148 - MinusLogProbMetric: 17.3148 - val_loss: 17.9258 - val_MinusLogProbMetric: 17.9258 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 84/1000
2023-09-27 10:53:37.164 
Epoch 84/1000 
	 loss: 17.3185, MinusLogProbMetric: 17.3185, val_loss: 17.3822, val_MinusLogProbMetric: 17.3822

Epoch 84: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.3185 - MinusLogProbMetric: 17.3185 - val_loss: 17.3822 - val_MinusLogProbMetric: 17.3822 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 85/1000
2023-09-27 10:54:09.051 
Epoch 85/1000 
	 loss: 17.3163, MinusLogProbMetric: 17.3163, val_loss: 17.3294, val_MinusLogProbMetric: 17.3294

Epoch 85: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.3163 - MinusLogProbMetric: 17.3163 - val_loss: 17.3294 - val_MinusLogProbMetric: 17.3294 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 86/1000
2023-09-27 10:54:41.813 
Epoch 86/1000 
	 loss: 17.3079, MinusLogProbMetric: 17.3079, val_loss: 17.3041, val_MinusLogProbMetric: 17.3041

Epoch 86: val_loss did not improve from 17.27917
196/196 - 33s - loss: 17.3079 - MinusLogProbMetric: 17.3079 - val_loss: 17.3041 - val_MinusLogProbMetric: 17.3041 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 87/1000
2023-09-27 10:55:13.993 
Epoch 87/1000 
	 loss: 17.3134, MinusLogProbMetric: 17.3134, val_loss: 17.2856, val_MinusLogProbMetric: 17.2856

Epoch 87: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.3134 - MinusLogProbMetric: 17.3134 - val_loss: 17.2856 - val_MinusLogProbMetric: 17.2856 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 88/1000
2023-09-27 10:55:45.838 
Epoch 88/1000 
	 loss: 17.3486, MinusLogProbMetric: 17.3486, val_loss: 17.4546, val_MinusLogProbMetric: 17.4546

Epoch 88: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.3486 - MinusLogProbMetric: 17.3486 - val_loss: 17.4546 - val_MinusLogProbMetric: 17.4546 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 89/1000
2023-09-27 10:56:17.963 
Epoch 89/1000 
	 loss: 17.3025, MinusLogProbMetric: 17.3025, val_loss: 17.5421, val_MinusLogProbMetric: 17.5421

Epoch 89: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.3025 - MinusLogProbMetric: 17.3025 - val_loss: 17.5421 - val_MinusLogProbMetric: 17.5421 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 90/1000
2023-09-27 10:56:49.979 
Epoch 90/1000 
	 loss: 17.2280, MinusLogProbMetric: 17.2280, val_loss: 18.0794, val_MinusLogProbMetric: 18.0794

Epoch 90: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.2280 - MinusLogProbMetric: 17.2280 - val_loss: 18.0794 - val_MinusLogProbMetric: 18.0794 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 91/1000
2023-09-27 10:57:22.246 
Epoch 91/1000 
	 loss: 17.3010, MinusLogProbMetric: 17.3010, val_loss: 17.6585, val_MinusLogProbMetric: 17.6585

Epoch 91: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.3010 - MinusLogProbMetric: 17.3010 - val_loss: 17.6585 - val_MinusLogProbMetric: 17.6585 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 92/1000
2023-09-27 10:57:54.357 
Epoch 92/1000 
	 loss: 17.3048, MinusLogProbMetric: 17.3048, val_loss: 17.3288, val_MinusLogProbMetric: 17.3288

Epoch 92: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.3048 - MinusLogProbMetric: 17.3048 - val_loss: 17.3288 - val_MinusLogProbMetric: 17.3288 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 93/1000
2023-09-27 10:58:26.768 
Epoch 93/1000 
	 loss: 17.2602, MinusLogProbMetric: 17.2602, val_loss: 17.7272, val_MinusLogProbMetric: 17.7272

Epoch 93: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.2602 - MinusLogProbMetric: 17.2602 - val_loss: 17.7272 - val_MinusLogProbMetric: 17.7272 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 94/1000
2023-09-27 10:58:58.919 
Epoch 94/1000 
	 loss: 17.2785, MinusLogProbMetric: 17.2785, val_loss: 17.6129, val_MinusLogProbMetric: 17.6129

Epoch 94: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.2785 - MinusLogProbMetric: 17.2785 - val_loss: 17.6129 - val_MinusLogProbMetric: 17.6129 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 95/1000
2023-09-27 10:59:31.029 
Epoch 95/1000 
	 loss: 17.1790, MinusLogProbMetric: 17.1790, val_loss: 17.3010, val_MinusLogProbMetric: 17.3010

Epoch 95: val_loss did not improve from 17.27917
196/196 - 32s - loss: 17.1790 - MinusLogProbMetric: 17.1790 - val_loss: 17.3010 - val_MinusLogProbMetric: 17.3010 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 96/1000
2023-09-27 11:00:02.929 
Epoch 96/1000 
	 loss: 17.2422, MinusLogProbMetric: 17.2422, val_loss: 17.1828, val_MinusLogProbMetric: 17.1828

Epoch 96: val_loss improved from 17.27917 to 17.18285, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 32s - loss: 17.2422 - MinusLogProbMetric: 17.2422 - val_loss: 17.1828 - val_MinusLogProbMetric: 17.1828 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 97/1000
2023-09-27 11:00:35.422 
Epoch 97/1000 
	 loss: 17.2574, MinusLogProbMetric: 17.2574, val_loss: 17.2490, val_MinusLogProbMetric: 17.2490

Epoch 97: val_loss did not improve from 17.18285
196/196 - 32s - loss: 17.2574 - MinusLogProbMetric: 17.2574 - val_loss: 17.2490 - val_MinusLogProbMetric: 17.2490 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 98/1000
2023-09-27 11:01:07.503 
Epoch 98/1000 
	 loss: 17.2119, MinusLogProbMetric: 17.2119, val_loss: 17.5002, val_MinusLogProbMetric: 17.5002

Epoch 98: val_loss did not improve from 17.18285
196/196 - 32s - loss: 17.2119 - MinusLogProbMetric: 17.2119 - val_loss: 17.5002 - val_MinusLogProbMetric: 17.5002 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 99/1000
2023-09-27 11:01:39.486 
Epoch 99/1000 
	 loss: 17.2102, MinusLogProbMetric: 17.2102, val_loss: 17.6330, val_MinusLogProbMetric: 17.6330

Epoch 99: val_loss did not improve from 17.18285
196/196 - 32s - loss: 17.2102 - MinusLogProbMetric: 17.2102 - val_loss: 17.6330 - val_MinusLogProbMetric: 17.6330 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 100/1000
2023-09-27 11:02:11.643 
Epoch 100/1000 
	 loss: 17.2146, MinusLogProbMetric: 17.2146, val_loss: 17.2985, val_MinusLogProbMetric: 17.2985

Epoch 100: val_loss did not improve from 17.18285
196/196 - 32s - loss: 17.2146 - MinusLogProbMetric: 17.2146 - val_loss: 17.2985 - val_MinusLogProbMetric: 17.2985 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 101/1000
2023-09-27 11:02:44.035 
Epoch 101/1000 
	 loss: 17.2290, MinusLogProbMetric: 17.2290, val_loss: 17.8965, val_MinusLogProbMetric: 17.8965

Epoch 101: val_loss did not improve from 17.18285
196/196 - 32s - loss: 17.2290 - MinusLogProbMetric: 17.2290 - val_loss: 17.8965 - val_MinusLogProbMetric: 17.8965 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 102/1000
2023-09-27 11:03:16.497 
Epoch 102/1000 
	 loss: 17.1719, MinusLogProbMetric: 17.1719, val_loss: 17.2668, val_MinusLogProbMetric: 17.2668

Epoch 102: val_loss did not improve from 17.18285
196/196 - 32s - loss: 17.1719 - MinusLogProbMetric: 17.1719 - val_loss: 17.2668 - val_MinusLogProbMetric: 17.2668 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 103/1000
2023-09-27 11:03:48.437 
Epoch 103/1000 
	 loss: 17.2232, MinusLogProbMetric: 17.2232, val_loss: 17.5927, val_MinusLogProbMetric: 17.5927

Epoch 103: val_loss did not improve from 17.18285
196/196 - 32s - loss: 17.2232 - MinusLogProbMetric: 17.2232 - val_loss: 17.5927 - val_MinusLogProbMetric: 17.5927 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 104/1000
2023-09-27 11:04:20.336 
Epoch 104/1000 
	 loss: 17.1809, MinusLogProbMetric: 17.1809, val_loss: 17.6071, val_MinusLogProbMetric: 17.6071

Epoch 104: val_loss did not improve from 17.18285
196/196 - 32s - loss: 17.1809 - MinusLogProbMetric: 17.1809 - val_loss: 17.6071 - val_MinusLogProbMetric: 17.6071 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 105/1000
2023-09-27 11:04:52.735 
Epoch 105/1000 
	 loss: 17.1724, MinusLogProbMetric: 17.1724, val_loss: 17.1513, val_MinusLogProbMetric: 17.1513

Epoch 105: val_loss improved from 17.18285 to 17.15126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 17.1724 - MinusLogProbMetric: 17.1724 - val_loss: 17.1513 - val_MinusLogProbMetric: 17.1513 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 106/1000
2023-09-27 11:05:25.546 
Epoch 106/1000 
	 loss: 17.1705, MinusLogProbMetric: 17.1705, val_loss: 17.1932, val_MinusLogProbMetric: 17.1932

Epoch 106: val_loss did not improve from 17.15126
196/196 - 32s - loss: 17.1705 - MinusLogProbMetric: 17.1705 - val_loss: 17.1932 - val_MinusLogProbMetric: 17.1932 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 107/1000
2023-09-27 11:05:57.721 
Epoch 107/1000 
	 loss: 17.2003, MinusLogProbMetric: 17.2003, val_loss: 17.1983, val_MinusLogProbMetric: 17.1983

Epoch 107: val_loss did not improve from 17.15126
196/196 - 32s - loss: 17.2003 - MinusLogProbMetric: 17.2003 - val_loss: 17.1983 - val_MinusLogProbMetric: 17.1983 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 108/1000
2023-09-27 11:06:30.200 
Epoch 108/1000 
	 loss: 17.1474, MinusLogProbMetric: 17.1474, val_loss: 17.7303, val_MinusLogProbMetric: 17.7303

Epoch 108: val_loss did not improve from 17.15126
196/196 - 32s - loss: 17.1474 - MinusLogProbMetric: 17.1474 - val_loss: 17.7303 - val_MinusLogProbMetric: 17.7303 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 109/1000
2023-09-27 11:07:01.807 
Epoch 109/1000 
	 loss: 17.1595, MinusLogProbMetric: 17.1595, val_loss: 17.6577, val_MinusLogProbMetric: 17.6577

Epoch 109: val_loss did not improve from 17.15126
196/196 - 32s - loss: 17.1595 - MinusLogProbMetric: 17.1595 - val_loss: 17.6577 - val_MinusLogProbMetric: 17.6577 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 110/1000
2023-09-27 11:07:33.854 
Epoch 110/1000 
	 loss: 17.1542, MinusLogProbMetric: 17.1542, val_loss: 17.5397, val_MinusLogProbMetric: 17.5397

Epoch 110: val_loss did not improve from 17.15126
196/196 - 32s - loss: 17.1542 - MinusLogProbMetric: 17.1542 - val_loss: 17.5397 - val_MinusLogProbMetric: 17.5397 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 111/1000
2023-09-27 11:08:05.805 
Epoch 111/1000 
	 loss: 17.1385, MinusLogProbMetric: 17.1385, val_loss: 17.3609, val_MinusLogProbMetric: 17.3609

Epoch 111: val_loss did not improve from 17.15126
196/196 - 32s - loss: 17.1385 - MinusLogProbMetric: 17.1385 - val_loss: 17.3609 - val_MinusLogProbMetric: 17.3609 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 112/1000
2023-09-27 11:08:37.809 
Epoch 112/1000 
	 loss: 17.1693, MinusLogProbMetric: 17.1693, val_loss: 17.2537, val_MinusLogProbMetric: 17.2537

Epoch 112: val_loss did not improve from 17.15126
196/196 - 32s - loss: 17.1693 - MinusLogProbMetric: 17.1693 - val_loss: 17.2537 - val_MinusLogProbMetric: 17.2537 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 113/1000
2023-09-27 11:09:09.800 
Epoch 113/1000 
	 loss: 17.1185, MinusLogProbMetric: 17.1185, val_loss: 17.1225, val_MinusLogProbMetric: 17.1225

Epoch 113: val_loss improved from 17.15126 to 17.12251, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 32s - loss: 17.1185 - MinusLogProbMetric: 17.1185 - val_loss: 17.1225 - val_MinusLogProbMetric: 17.1225 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 114/1000
2023-09-27 11:09:41.929 
Epoch 114/1000 
	 loss: 17.1121, MinusLogProbMetric: 17.1121, val_loss: 17.3754, val_MinusLogProbMetric: 17.3754

Epoch 114: val_loss did not improve from 17.12251
196/196 - 32s - loss: 17.1121 - MinusLogProbMetric: 17.1121 - val_loss: 17.3754 - val_MinusLogProbMetric: 17.3754 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 115/1000
2023-09-27 11:10:14.189 
Epoch 115/1000 
	 loss: 17.1543, MinusLogProbMetric: 17.1543, val_loss: 17.4579, val_MinusLogProbMetric: 17.4579

Epoch 115: val_loss did not improve from 17.12251
196/196 - 32s - loss: 17.1543 - MinusLogProbMetric: 17.1543 - val_loss: 17.4579 - val_MinusLogProbMetric: 17.4579 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 116/1000
2023-09-27 11:10:45.758 
Epoch 116/1000 
	 loss: 17.1026, MinusLogProbMetric: 17.1026, val_loss: 17.4077, val_MinusLogProbMetric: 17.4077

Epoch 116: val_loss did not improve from 17.12251
196/196 - 32s - loss: 17.1026 - MinusLogProbMetric: 17.1026 - val_loss: 17.4077 - val_MinusLogProbMetric: 17.4077 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 117/1000
2023-09-27 11:11:16.845 
Epoch 117/1000 
	 loss: 17.1342, MinusLogProbMetric: 17.1342, val_loss: 17.4880, val_MinusLogProbMetric: 17.4880

Epoch 117: val_loss did not improve from 17.12251
196/196 - 31s - loss: 17.1342 - MinusLogProbMetric: 17.1342 - val_loss: 17.4880 - val_MinusLogProbMetric: 17.4880 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 118/1000
2023-09-27 11:11:45.758 
Epoch 118/1000 
	 loss: 17.1212, MinusLogProbMetric: 17.1212, val_loss: 17.1909, val_MinusLogProbMetric: 17.1909

Epoch 118: val_loss did not improve from 17.12251
196/196 - 29s - loss: 17.1212 - MinusLogProbMetric: 17.1212 - val_loss: 17.1909 - val_MinusLogProbMetric: 17.1909 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 119/1000
2023-09-27 11:12:17.839 
Epoch 119/1000 
	 loss: 17.1094, MinusLogProbMetric: 17.1094, val_loss: 17.9483, val_MinusLogProbMetric: 17.9483

Epoch 119: val_loss did not improve from 17.12251
196/196 - 32s - loss: 17.1094 - MinusLogProbMetric: 17.1094 - val_loss: 17.9483 - val_MinusLogProbMetric: 17.9483 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 120/1000
2023-09-27 11:12:50.496 
Epoch 120/1000 
	 loss: 17.0970, MinusLogProbMetric: 17.0970, val_loss: 17.4905, val_MinusLogProbMetric: 17.4905

Epoch 120: val_loss did not improve from 17.12251
196/196 - 33s - loss: 17.0970 - MinusLogProbMetric: 17.0970 - val_loss: 17.4905 - val_MinusLogProbMetric: 17.4905 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 121/1000
2023-09-27 11:13:22.590 
Epoch 121/1000 
	 loss: 17.1357, MinusLogProbMetric: 17.1357, val_loss: 17.4725, val_MinusLogProbMetric: 17.4725

Epoch 121: val_loss did not improve from 17.12251
196/196 - 32s - loss: 17.1357 - MinusLogProbMetric: 17.1357 - val_loss: 17.4725 - val_MinusLogProbMetric: 17.4725 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 122/1000
2023-09-27 11:13:55.008 
Epoch 122/1000 
	 loss: 17.0978, MinusLogProbMetric: 17.0978, val_loss: 17.1806, val_MinusLogProbMetric: 17.1806

Epoch 122: val_loss did not improve from 17.12251
196/196 - 32s - loss: 17.0978 - MinusLogProbMetric: 17.0978 - val_loss: 17.1806 - val_MinusLogProbMetric: 17.1806 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 123/1000
2023-09-27 11:14:27.382 
Epoch 123/1000 
	 loss: 17.0988, MinusLogProbMetric: 17.0988, val_loss: 17.3824, val_MinusLogProbMetric: 17.3824

Epoch 123: val_loss did not improve from 17.12251
196/196 - 32s - loss: 17.0988 - MinusLogProbMetric: 17.0988 - val_loss: 17.3824 - val_MinusLogProbMetric: 17.3824 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 124/1000
2023-09-27 11:14:59.797 
Epoch 124/1000 
	 loss: 17.0926, MinusLogProbMetric: 17.0926, val_loss: 17.5691, val_MinusLogProbMetric: 17.5691

Epoch 124: val_loss did not improve from 17.12251
196/196 - 32s - loss: 17.0926 - MinusLogProbMetric: 17.0926 - val_loss: 17.5691 - val_MinusLogProbMetric: 17.5691 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 125/1000
2023-09-27 11:15:32.336 
Epoch 125/1000 
	 loss: 17.1237, MinusLogProbMetric: 17.1237, val_loss: 17.1713, val_MinusLogProbMetric: 17.1713

Epoch 125: val_loss did not improve from 17.12251
196/196 - 33s - loss: 17.1237 - MinusLogProbMetric: 17.1237 - val_loss: 17.1713 - val_MinusLogProbMetric: 17.1713 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 126/1000
2023-09-27 11:16:04.078 
Epoch 126/1000 
	 loss: 17.0821, MinusLogProbMetric: 17.0821, val_loss: 17.0631, val_MinusLogProbMetric: 17.0631

Epoch 126: val_loss improved from 17.12251 to 17.06313, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 32s - loss: 17.0821 - MinusLogProbMetric: 17.0821 - val_loss: 17.0631 - val_MinusLogProbMetric: 17.0631 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 127/1000
2023-09-27 11:16:36.813 
Epoch 127/1000 
	 loss: 17.0744, MinusLogProbMetric: 17.0744, val_loss: 17.2725, val_MinusLogProbMetric: 17.2725

Epoch 127: val_loss did not improve from 17.06313
196/196 - 32s - loss: 17.0744 - MinusLogProbMetric: 17.0744 - val_loss: 17.2725 - val_MinusLogProbMetric: 17.2725 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 128/1000
2023-09-27 11:17:08.808 
Epoch 128/1000 
	 loss: 17.0249, MinusLogProbMetric: 17.0249, val_loss: 17.1566, val_MinusLogProbMetric: 17.1566

Epoch 128: val_loss did not improve from 17.06313
196/196 - 32s - loss: 17.0249 - MinusLogProbMetric: 17.0249 - val_loss: 17.1566 - val_MinusLogProbMetric: 17.1566 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 129/1000
2023-09-27 11:17:40.897 
Epoch 129/1000 
	 loss: 17.0626, MinusLogProbMetric: 17.0626, val_loss: 17.5116, val_MinusLogProbMetric: 17.5116

Epoch 129: val_loss did not improve from 17.06313
196/196 - 32s - loss: 17.0626 - MinusLogProbMetric: 17.0626 - val_loss: 17.5116 - val_MinusLogProbMetric: 17.5116 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 130/1000
2023-09-27 11:18:12.849 
Epoch 130/1000 
	 loss: 17.0814, MinusLogProbMetric: 17.0814, val_loss: 17.3032, val_MinusLogProbMetric: 17.3032

Epoch 130: val_loss did not improve from 17.06313
196/196 - 32s - loss: 17.0814 - MinusLogProbMetric: 17.0814 - val_loss: 17.3032 - val_MinusLogProbMetric: 17.3032 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 131/1000
2023-09-27 11:18:45.340 
Epoch 131/1000 
	 loss: 17.0950, MinusLogProbMetric: 17.0950, val_loss: 17.2329, val_MinusLogProbMetric: 17.2329

Epoch 131: val_loss did not improve from 17.06313
196/196 - 32s - loss: 17.0950 - MinusLogProbMetric: 17.0950 - val_loss: 17.2329 - val_MinusLogProbMetric: 17.2329 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 132/1000
2023-09-27 11:19:17.433 
Epoch 132/1000 
	 loss: 17.0397, MinusLogProbMetric: 17.0397, val_loss: 17.1138, val_MinusLogProbMetric: 17.1138

Epoch 132: val_loss did not improve from 17.06313
196/196 - 32s - loss: 17.0397 - MinusLogProbMetric: 17.0397 - val_loss: 17.1138 - val_MinusLogProbMetric: 17.1138 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 133/1000
2023-09-27 11:19:49.557 
Epoch 133/1000 
	 loss: 17.0156, MinusLogProbMetric: 17.0156, val_loss: 17.5617, val_MinusLogProbMetric: 17.5617

Epoch 133: val_loss did not improve from 17.06313
196/196 - 32s - loss: 17.0156 - MinusLogProbMetric: 17.0156 - val_loss: 17.5617 - val_MinusLogProbMetric: 17.5617 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 134/1000
2023-09-27 11:20:22.017 
Epoch 134/1000 
	 loss: 17.0565, MinusLogProbMetric: 17.0565, val_loss: 17.0954, val_MinusLogProbMetric: 17.0954

Epoch 134: val_loss did not improve from 17.06313
196/196 - 32s - loss: 17.0565 - MinusLogProbMetric: 17.0565 - val_loss: 17.0954 - val_MinusLogProbMetric: 17.0954 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 135/1000
2023-09-27 11:20:54.028 
Epoch 135/1000 
	 loss: 17.0391, MinusLogProbMetric: 17.0391, val_loss: 17.0784, val_MinusLogProbMetric: 17.0784

Epoch 135: val_loss did not improve from 17.06313
196/196 - 32s - loss: 17.0391 - MinusLogProbMetric: 17.0391 - val_loss: 17.0784 - val_MinusLogProbMetric: 17.0784 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 136/1000
2023-09-27 11:21:26.138 
Epoch 136/1000 
	 loss: 17.0303, MinusLogProbMetric: 17.0303, val_loss: 17.3558, val_MinusLogProbMetric: 17.3558

Epoch 136: val_loss did not improve from 17.06313
196/196 - 32s - loss: 17.0303 - MinusLogProbMetric: 17.0303 - val_loss: 17.3558 - val_MinusLogProbMetric: 17.3558 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 137/1000
2023-09-27 11:21:58.324 
Epoch 137/1000 
	 loss: 17.0290, MinusLogProbMetric: 17.0290, val_loss: 17.1881, val_MinusLogProbMetric: 17.1881

Epoch 137: val_loss did not improve from 17.06313
196/196 - 32s - loss: 17.0290 - MinusLogProbMetric: 17.0290 - val_loss: 17.1881 - val_MinusLogProbMetric: 17.1881 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 138/1000
2023-09-27 11:22:30.385 
Epoch 138/1000 
	 loss: 17.0630, MinusLogProbMetric: 17.0630, val_loss: 17.3080, val_MinusLogProbMetric: 17.3080

Epoch 138: val_loss did not improve from 17.06313
196/196 - 32s - loss: 17.0630 - MinusLogProbMetric: 17.0630 - val_loss: 17.3080 - val_MinusLogProbMetric: 17.3080 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 139/1000
2023-09-27 11:23:02.495 
Epoch 139/1000 
	 loss: 17.0265, MinusLogProbMetric: 17.0265, val_loss: 17.0599, val_MinusLogProbMetric: 17.0599

Epoch 139: val_loss improved from 17.06313 to 17.05987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 17.0265 - MinusLogProbMetric: 17.0265 - val_loss: 17.0599 - val_MinusLogProbMetric: 17.0599 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 140/1000
2023-09-27 11:23:35.344 
Epoch 140/1000 
	 loss: 17.0241, MinusLogProbMetric: 17.0241, val_loss: 17.2353, val_MinusLogProbMetric: 17.2353

Epoch 140: val_loss did not improve from 17.05987
196/196 - 32s - loss: 17.0241 - MinusLogProbMetric: 17.0241 - val_loss: 17.2353 - val_MinusLogProbMetric: 17.2353 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 141/1000
2023-09-27 11:24:08.015 
Epoch 141/1000 
	 loss: 16.9932, MinusLogProbMetric: 16.9932, val_loss: 17.2316, val_MinusLogProbMetric: 17.2316

Epoch 141: val_loss did not improve from 17.05987
196/196 - 33s - loss: 16.9932 - MinusLogProbMetric: 16.9932 - val_loss: 17.2316 - val_MinusLogProbMetric: 17.2316 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 142/1000
2023-09-27 11:24:40.252 
Epoch 142/1000 
	 loss: 17.0271, MinusLogProbMetric: 17.0271, val_loss: 17.3301, val_MinusLogProbMetric: 17.3301

Epoch 142: val_loss did not improve from 17.05987
196/196 - 32s - loss: 17.0271 - MinusLogProbMetric: 17.0271 - val_loss: 17.3301 - val_MinusLogProbMetric: 17.3301 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 143/1000
2023-09-27 11:25:12.481 
Epoch 143/1000 
	 loss: 16.9804, MinusLogProbMetric: 16.9804, val_loss: 17.1413, val_MinusLogProbMetric: 17.1413

Epoch 143: val_loss did not improve from 17.05987
196/196 - 32s - loss: 16.9804 - MinusLogProbMetric: 16.9804 - val_loss: 17.1413 - val_MinusLogProbMetric: 17.1413 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 144/1000
2023-09-27 11:25:44.573 
Epoch 144/1000 
	 loss: 16.9888, MinusLogProbMetric: 16.9888, val_loss: 17.1058, val_MinusLogProbMetric: 17.1058

Epoch 144: val_loss did not improve from 17.05987
196/196 - 32s - loss: 16.9888 - MinusLogProbMetric: 16.9888 - val_loss: 17.1058 - val_MinusLogProbMetric: 17.1058 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 145/1000
2023-09-27 11:26:17.134 
Epoch 145/1000 
	 loss: 16.9871, MinusLogProbMetric: 16.9871, val_loss: 17.0815, val_MinusLogProbMetric: 17.0815

Epoch 145: val_loss did not improve from 17.05987
196/196 - 33s - loss: 16.9871 - MinusLogProbMetric: 16.9871 - val_loss: 17.0815 - val_MinusLogProbMetric: 17.0815 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 146/1000
2023-09-27 11:26:49.504 
Epoch 146/1000 
	 loss: 16.9869, MinusLogProbMetric: 16.9869, val_loss: 17.4656, val_MinusLogProbMetric: 17.4656

Epoch 146: val_loss did not improve from 17.05987
196/196 - 32s - loss: 16.9869 - MinusLogProbMetric: 16.9869 - val_loss: 17.4656 - val_MinusLogProbMetric: 17.4656 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 147/1000
2023-09-27 11:27:21.839 
Epoch 147/1000 
	 loss: 16.9835, MinusLogProbMetric: 16.9835, val_loss: 17.5520, val_MinusLogProbMetric: 17.5520

Epoch 147: val_loss did not improve from 17.05987
196/196 - 32s - loss: 16.9835 - MinusLogProbMetric: 16.9835 - val_loss: 17.5520 - val_MinusLogProbMetric: 17.5520 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 148/1000
2023-09-27 11:27:54.218 
Epoch 148/1000 
	 loss: 16.9952, MinusLogProbMetric: 16.9952, val_loss: 17.1399, val_MinusLogProbMetric: 17.1399

Epoch 148: val_loss did not improve from 17.05987
196/196 - 32s - loss: 16.9952 - MinusLogProbMetric: 16.9952 - val_loss: 17.1399 - val_MinusLogProbMetric: 17.1399 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 149/1000
2023-09-27 11:28:26.613 
Epoch 149/1000 
	 loss: 16.9917, MinusLogProbMetric: 16.9917, val_loss: 17.1054, val_MinusLogProbMetric: 17.1054

Epoch 149: val_loss did not improve from 17.05987
196/196 - 32s - loss: 16.9917 - MinusLogProbMetric: 16.9917 - val_loss: 17.1054 - val_MinusLogProbMetric: 17.1054 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 150/1000
2023-09-27 11:28:58.893 
Epoch 150/1000 
	 loss: 16.9955, MinusLogProbMetric: 16.9955, val_loss: 17.2519, val_MinusLogProbMetric: 17.2519

Epoch 150: val_loss did not improve from 17.05987
196/196 - 32s - loss: 16.9955 - MinusLogProbMetric: 16.9955 - val_loss: 17.2519 - val_MinusLogProbMetric: 17.2519 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 151/1000
2023-09-27 11:29:31.101 
Epoch 151/1000 
	 loss: 16.9987, MinusLogProbMetric: 16.9987, val_loss: 17.1104, val_MinusLogProbMetric: 17.1104

Epoch 151: val_loss did not improve from 17.05987
196/196 - 32s - loss: 16.9987 - MinusLogProbMetric: 16.9987 - val_loss: 17.1104 - val_MinusLogProbMetric: 17.1104 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 152/1000
2023-09-27 11:30:03.836 
Epoch 152/1000 
	 loss: 16.9529, MinusLogProbMetric: 16.9529, val_loss: 17.0310, val_MinusLogProbMetric: 17.0310

Epoch 152: val_loss improved from 17.05987 to 17.03102, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 16.9529 - MinusLogProbMetric: 16.9529 - val_loss: 17.0310 - val_MinusLogProbMetric: 17.0310 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 153/1000
2023-09-27 11:30:37.980 
Epoch 153/1000 
	 loss: 16.9543, MinusLogProbMetric: 16.9543, val_loss: 17.1592, val_MinusLogProbMetric: 17.1592

Epoch 153: val_loss did not improve from 17.03102
196/196 - 34s - loss: 16.9543 - MinusLogProbMetric: 16.9543 - val_loss: 17.1592 - val_MinusLogProbMetric: 17.1592 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 154/1000
2023-09-27 11:31:10.340 
Epoch 154/1000 
	 loss: 16.9605, MinusLogProbMetric: 16.9605, val_loss: 18.0054, val_MinusLogProbMetric: 18.0054

Epoch 154: val_loss did not improve from 17.03102
196/196 - 32s - loss: 16.9605 - MinusLogProbMetric: 16.9605 - val_loss: 18.0054 - val_MinusLogProbMetric: 18.0054 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 155/1000
2023-09-27 11:31:42.319 
Epoch 155/1000 
	 loss: 17.0431, MinusLogProbMetric: 17.0431, val_loss: 17.1078, val_MinusLogProbMetric: 17.1078

Epoch 155: val_loss did not improve from 17.03102
196/196 - 32s - loss: 17.0431 - MinusLogProbMetric: 17.0431 - val_loss: 17.1078 - val_MinusLogProbMetric: 17.1078 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 156/1000
2023-09-27 11:32:14.908 
Epoch 156/1000 
	 loss: 16.9362, MinusLogProbMetric: 16.9362, val_loss: 17.1837, val_MinusLogProbMetric: 17.1837

Epoch 156: val_loss did not improve from 17.03102
196/196 - 33s - loss: 16.9362 - MinusLogProbMetric: 16.9362 - val_loss: 17.1837 - val_MinusLogProbMetric: 17.1837 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 157/1000
2023-09-27 11:32:47.300 
Epoch 157/1000 
	 loss: 16.9713, MinusLogProbMetric: 16.9713, val_loss: 17.2682, val_MinusLogProbMetric: 17.2682

Epoch 157: val_loss did not improve from 17.03102
196/196 - 32s - loss: 16.9713 - MinusLogProbMetric: 16.9713 - val_loss: 17.2682 - val_MinusLogProbMetric: 17.2682 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 158/1000
2023-09-27 11:33:19.798 
Epoch 158/1000 
	 loss: 16.9475, MinusLogProbMetric: 16.9475, val_loss: 17.0890, val_MinusLogProbMetric: 17.0890

Epoch 158: val_loss did not improve from 17.03102
196/196 - 32s - loss: 16.9475 - MinusLogProbMetric: 16.9475 - val_loss: 17.0890 - val_MinusLogProbMetric: 17.0890 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 159/1000
2023-09-27 11:33:52.278 
Epoch 159/1000 
	 loss: 16.9385, MinusLogProbMetric: 16.9385, val_loss: 17.0086, val_MinusLogProbMetric: 17.0086

Epoch 159: val_loss improved from 17.03102 to 17.00858, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 16.9385 - MinusLogProbMetric: 16.9385 - val_loss: 17.0086 - val_MinusLogProbMetric: 17.0086 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 160/1000
2023-09-27 11:34:25.197 
Epoch 160/1000 
	 loss: 16.9505, MinusLogProbMetric: 16.9505, val_loss: 17.0828, val_MinusLogProbMetric: 17.0828

Epoch 160: val_loss did not improve from 17.00858
196/196 - 32s - loss: 16.9505 - MinusLogProbMetric: 16.9505 - val_loss: 17.0828 - val_MinusLogProbMetric: 17.0828 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 161/1000
2023-09-27 11:34:57.369 
Epoch 161/1000 
	 loss: 16.9661, MinusLogProbMetric: 16.9661, val_loss: 17.0203, val_MinusLogProbMetric: 17.0203

Epoch 161: val_loss did not improve from 17.00858
196/196 - 32s - loss: 16.9661 - MinusLogProbMetric: 16.9661 - val_loss: 17.0203 - val_MinusLogProbMetric: 17.0203 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 162/1000
2023-09-27 11:35:30.132 
Epoch 162/1000 
	 loss: 16.9444, MinusLogProbMetric: 16.9444, val_loss: 17.2171, val_MinusLogProbMetric: 17.2171

Epoch 162: val_loss did not improve from 17.00858
196/196 - 33s - loss: 16.9444 - MinusLogProbMetric: 16.9444 - val_loss: 17.2171 - val_MinusLogProbMetric: 17.2171 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 163/1000
2023-09-27 11:36:02.320 
Epoch 163/1000 
	 loss: 16.9336, MinusLogProbMetric: 16.9336, val_loss: 17.0813, val_MinusLogProbMetric: 17.0813

Epoch 163: val_loss did not improve from 17.00858
196/196 - 32s - loss: 16.9336 - MinusLogProbMetric: 16.9336 - val_loss: 17.0813 - val_MinusLogProbMetric: 17.0813 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 164/1000
2023-09-27 11:36:34.822 
Epoch 164/1000 
	 loss: 16.9078, MinusLogProbMetric: 16.9078, val_loss: 17.1239, val_MinusLogProbMetric: 17.1239

Epoch 164: val_loss did not improve from 17.00858
196/196 - 32s - loss: 16.9078 - MinusLogProbMetric: 16.9078 - val_loss: 17.1239 - val_MinusLogProbMetric: 17.1239 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 165/1000
2023-09-27 11:37:06.880 
Epoch 165/1000 
	 loss: 16.9098, MinusLogProbMetric: 16.9098, val_loss: 16.9981, val_MinusLogProbMetric: 16.9981

Epoch 165: val_loss improved from 17.00858 to 16.99813, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 16.9098 - MinusLogProbMetric: 16.9098 - val_loss: 16.9981 - val_MinusLogProbMetric: 16.9981 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 166/1000
2023-09-27 11:37:39.526 
Epoch 166/1000 
	 loss: 16.9397, MinusLogProbMetric: 16.9397, val_loss: 17.1498, val_MinusLogProbMetric: 17.1498

Epoch 166: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.9397 - MinusLogProbMetric: 16.9397 - val_loss: 17.1498 - val_MinusLogProbMetric: 17.1498 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 167/1000
2023-09-27 11:38:11.818 
Epoch 167/1000 
	 loss: 16.9116, MinusLogProbMetric: 16.9116, val_loss: 17.6337, val_MinusLogProbMetric: 17.6337

Epoch 167: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.9116 - MinusLogProbMetric: 16.9116 - val_loss: 17.6337 - val_MinusLogProbMetric: 17.6337 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 168/1000
2023-09-27 11:38:44.591 
Epoch 168/1000 
	 loss: 16.9189, MinusLogProbMetric: 16.9189, val_loss: 17.0291, val_MinusLogProbMetric: 17.0291

Epoch 168: val_loss did not improve from 16.99813
196/196 - 33s - loss: 16.9189 - MinusLogProbMetric: 16.9189 - val_loss: 17.0291 - val_MinusLogProbMetric: 17.0291 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 169/1000
2023-09-27 11:39:16.883 
Epoch 169/1000 
	 loss: 16.8845, MinusLogProbMetric: 16.8845, val_loss: 17.1987, val_MinusLogProbMetric: 17.1987

Epoch 169: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.8845 - MinusLogProbMetric: 16.8845 - val_loss: 17.1987 - val_MinusLogProbMetric: 17.1987 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 170/1000
2023-09-27 11:39:49.218 
Epoch 170/1000 
	 loss: 16.9336, MinusLogProbMetric: 16.9336, val_loss: 17.0973, val_MinusLogProbMetric: 17.0973

Epoch 170: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.9336 - MinusLogProbMetric: 16.9336 - val_loss: 17.0973 - val_MinusLogProbMetric: 17.0973 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 171/1000
2023-09-27 11:40:21.359 
Epoch 171/1000 
	 loss: 16.8834, MinusLogProbMetric: 16.8834, val_loss: 17.1452, val_MinusLogProbMetric: 17.1452

Epoch 171: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.8834 - MinusLogProbMetric: 16.8834 - val_loss: 17.1452 - val_MinusLogProbMetric: 17.1452 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 172/1000
2023-09-27 11:40:53.592 
Epoch 172/1000 
	 loss: 16.8986, MinusLogProbMetric: 16.8986, val_loss: 17.1331, val_MinusLogProbMetric: 17.1331

Epoch 172: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.8986 - MinusLogProbMetric: 16.8986 - val_loss: 17.1331 - val_MinusLogProbMetric: 17.1331 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 173/1000
2023-09-27 11:41:26.028 
Epoch 173/1000 
	 loss: 16.8630, MinusLogProbMetric: 16.8630, val_loss: 17.1197, val_MinusLogProbMetric: 17.1197

Epoch 173: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.8630 - MinusLogProbMetric: 16.8630 - val_loss: 17.1197 - val_MinusLogProbMetric: 17.1197 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 174/1000
2023-09-27 11:41:58.336 
Epoch 174/1000 
	 loss: 16.8724, MinusLogProbMetric: 16.8724, val_loss: 17.0335, val_MinusLogProbMetric: 17.0335

Epoch 174: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.8724 - MinusLogProbMetric: 16.8724 - val_loss: 17.0335 - val_MinusLogProbMetric: 17.0335 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 175/1000
2023-09-27 11:42:30.257 
Epoch 175/1000 
	 loss: 16.8891, MinusLogProbMetric: 16.8891, val_loss: 17.1492, val_MinusLogProbMetric: 17.1492

Epoch 175: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.8891 - MinusLogProbMetric: 16.8891 - val_loss: 17.1492 - val_MinusLogProbMetric: 17.1492 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 176/1000
2023-09-27 11:43:02.486 
Epoch 176/1000 
	 loss: 16.8855, MinusLogProbMetric: 16.8855, val_loss: 17.1480, val_MinusLogProbMetric: 17.1480

Epoch 176: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.8855 - MinusLogProbMetric: 16.8855 - val_loss: 17.1480 - val_MinusLogProbMetric: 17.1480 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 177/1000
2023-09-27 11:43:34.859 
Epoch 177/1000 
	 loss: 16.9056, MinusLogProbMetric: 16.9056, val_loss: 17.6848, val_MinusLogProbMetric: 17.6848

Epoch 177: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.9056 - MinusLogProbMetric: 16.9056 - val_loss: 17.6848 - val_MinusLogProbMetric: 17.6848 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 178/1000
2023-09-27 11:44:06.918 
Epoch 178/1000 
	 loss: 16.9188, MinusLogProbMetric: 16.9188, val_loss: 17.3904, val_MinusLogProbMetric: 17.3904

Epoch 178: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.9188 - MinusLogProbMetric: 16.9188 - val_loss: 17.3904 - val_MinusLogProbMetric: 17.3904 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 179/1000
2023-09-27 11:44:38.934 
Epoch 179/1000 
	 loss: 16.8694, MinusLogProbMetric: 16.8694, val_loss: 17.3672, val_MinusLogProbMetric: 17.3672

Epoch 179: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.8694 - MinusLogProbMetric: 16.8694 - val_loss: 17.3672 - val_MinusLogProbMetric: 17.3672 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 180/1000
2023-09-27 11:45:11.073 
Epoch 180/1000 
	 loss: 16.8791, MinusLogProbMetric: 16.8791, val_loss: 17.0694, val_MinusLogProbMetric: 17.0694

Epoch 180: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.8791 - MinusLogProbMetric: 16.8791 - val_loss: 17.0694 - val_MinusLogProbMetric: 17.0694 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 181/1000
2023-09-27 11:45:43.165 
Epoch 181/1000 
	 loss: 16.8784, MinusLogProbMetric: 16.8784, val_loss: 17.1216, val_MinusLogProbMetric: 17.1216

Epoch 181: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.8784 - MinusLogProbMetric: 16.8784 - val_loss: 17.1216 - val_MinusLogProbMetric: 17.1216 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 182/1000
2023-09-27 11:46:15.413 
Epoch 182/1000 
	 loss: 16.8623, MinusLogProbMetric: 16.8623, val_loss: 17.1290, val_MinusLogProbMetric: 17.1290

Epoch 182: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.8623 - MinusLogProbMetric: 16.8623 - val_loss: 17.1290 - val_MinusLogProbMetric: 17.1290 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 183/1000
2023-09-27 11:46:47.481 
Epoch 183/1000 
	 loss: 16.8355, MinusLogProbMetric: 16.8355, val_loss: 17.0684, val_MinusLogProbMetric: 17.0684

Epoch 183: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.8355 - MinusLogProbMetric: 16.8355 - val_loss: 17.0684 - val_MinusLogProbMetric: 17.0684 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 184/1000
2023-09-27 11:47:19.566 
Epoch 184/1000 
	 loss: 16.8557, MinusLogProbMetric: 16.8557, val_loss: 17.3278, val_MinusLogProbMetric: 17.3278

Epoch 184: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.8557 - MinusLogProbMetric: 16.8557 - val_loss: 17.3278 - val_MinusLogProbMetric: 17.3278 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 185/1000
2023-09-27 11:47:51.672 
Epoch 185/1000 
	 loss: 16.8863, MinusLogProbMetric: 16.8863, val_loss: 17.1993, val_MinusLogProbMetric: 17.1993

Epoch 185: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.8863 - MinusLogProbMetric: 16.8863 - val_loss: 17.1993 - val_MinusLogProbMetric: 17.1993 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 186/1000
2023-09-27 11:48:24.046 
Epoch 186/1000 
	 loss: 16.8243, MinusLogProbMetric: 16.8243, val_loss: 17.1888, val_MinusLogProbMetric: 17.1888

Epoch 186: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.8243 - MinusLogProbMetric: 16.8243 - val_loss: 17.1888 - val_MinusLogProbMetric: 17.1888 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 187/1000
2023-09-27 11:48:56.400 
Epoch 187/1000 
	 loss: 16.8338, MinusLogProbMetric: 16.8338, val_loss: 17.3178, val_MinusLogProbMetric: 17.3178

Epoch 187: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.8338 - MinusLogProbMetric: 16.8338 - val_loss: 17.3178 - val_MinusLogProbMetric: 17.3178 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 188/1000
2023-09-27 11:49:28.703 
Epoch 188/1000 
	 loss: 16.8558, MinusLogProbMetric: 16.8558, val_loss: 17.2693, val_MinusLogProbMetric: 17.2693

Epoch 188: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.8558 - MinusLogProbMetric: 16.8558 - val_loss: 17.2693 - val_MinusLogProbMetric: 17.2693 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 189/1000
2023-09-27 11:50:01.141 
Epoch 189/1000 
	 loss: 16.8375, MinusLogProbMetric: 16.8375, val_loss: 17.1272, val_MinusLogProbMetric: 17.1272

Epoch 189: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.8375 - MinusLogProbMetric: 16.8375 - val_loss: 17.1272 - val_MinusLogProbMetric: 17.1272 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 190/1000
2023-09-27 11:50:33.790 
Epoch 190/1000 
	 loss: 16.8162, MinusLogProbMetric: 16.8162, val_loss: 17.2492, val_MinusLogProbMetric: 17.2492

Epoch 190: val_loss did not improve from 16.99813
196/196 - 33s - loss: 16.8162 - MinusLogProbMetric: 16.8162 - val_loss: 17.2492 - val_MinusLogProbMetric: 17.2492 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 191/1000
2023-09-27 11:51:06.262 
Epoch 191/1000 
	 loss: 16.9128, MinusLogProbMetric: 16.9128, val_loss: 17.0473, val_MinusLogProbMetric: 17.0473

Epoch 191: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.9128 - MinusLogProbMetric: 16.9128 - val_loss: 17.0473 - val_MinusLogProbMetric: 17.0473 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 192/1000
2023-09-27 11:51:38.628 
Epoch 192/1000 
	 loss: 16.8480, MinusLogProbMetric: 16.8480, val_loss: 16.9983, val_MinusLogProbMetric: 16.9983

Epoch 192: val_loss did not improve from 16.99813
196/196 - 32s - loss: 16.8480 - MinusLogProbMetric: 16.8480 - val_loss: 16.9983 - val_MinusLogProbMetric: 16.9983 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 193/1000
2023-09-27 11:52:10.808 
Epoch 193/1000 
	 loss: 16.8523, MinusLogProbMetric: 16.8523, val_loss: 16.9635, val_MinusLogProbMetric: 16.9635

Epoch 193: val_loss improved from 16.99813 to 16.96348, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 16.8523 - MinusLogProbMetric: 16.8523 - val_loss: 16.9635 - val_MinusLogProbMetric: 16.9635 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 194/1000
2023-09-27 11:52:43.773 
Epoch 194/1000 
	 loss: 16.8543, MinusLogProbMetric: 16.8543, val_loss: 17.3506, val_MinusLogProbMetric: 17.3506

Epoch 194: val_loss did not improve from 16.96348
196/196 - 32s - loss: 16.8543 - MinusLogProbMetric: 16.8543 - val_loss: 17.3506 - val_MinusLogProbMetric: 17.3506 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 195/1000
2023-09-27 11:53:16.195 
Epoch 195/1000 
	 loss: 16.8126, MinusLogProbMetric: 16.8126, val_loss: 17.2756, val_MinusLogProbMetric: 17.2756

Epoch 195: val_loss did not improve from 16.96348
196/196 - 32s - loss: 16.8126 - MinusLogProbMetric: 16.8126 - val_loss: 17.2756 - val_MinusLogProbMetric: 17.2756 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 196/1000
2023-09-27 11:53:48.103 
Epoch 196/1000 
	 loss: 16.8355, MinusLogProbMetric: 16.8355, val_loss: 17.1246, val_MinusLogProbMetric: 17.1246

Epoch 196: val_loss did not improve from 16.96348
196/196 - 32s - loss: 16.8355 - MinusLogProbMetric: 16.8355 - val_loss: 17.1246 - val_MinusLogProbMetric: 17.1246 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 197/1000
2023-09-27 11:54:20.628 
Epoch 197/1000 
	 loss: 16.8038, MinusLogProbMetric: 16.8038, val_loss: 16.9355, val_MinusLogProbMetric: 16.9355

Epoch 197: val_loss improved from 16.96348 to 16.93548, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 16.8038 - MinusLogProbMetric: 16.8038 - val_loss: 16.9355 - val_MinusLogProbMetric: 16.9355 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 198/1000
2023-09-27 11:54:53.385 
Epoch 198/1000 
	 loss: 16.8672, MinusLogProbMetric: 16.8672, val_loss: 17.0244, val_MinusLogProbMetric: 17.0244

Epoch 198: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.8672 - MinusLogProbMetric: 16.8672 - val_loss: 17.0244 - val_MinusLogProbMetric: 17.0244 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 199/1000
2023-09-27 11:55:25.279 
Epoch 199/1000 
	 loss: 16.7810, MinusLogProbMetric: 16.7810, val_loss: 17.0351, val_MinusLogProbMetric: 17.0351

Epoch 199: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7810 - MinusLogProbMetric: 16.7810 - val_loss: 17.0351 - val_MinusLogProbMetric: 17.0351 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 200/1000
2023-09-27 11:55:58.031 
Epoch 200/1000 
	 loss: 16.8171, MinusLogProbMetric: 16.8171, val_loss: 17.1460, val_MinusLogProbMetric: 17.1460

Epoch 200: val_loss did not improve from 16.93548
196/196 - 33s - loss: 16.8171 - MinusLogProbMetric: 16.8171 - val_loss: 17.1460 - val_MinusLogProbMetric: 17.1460 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 201/1000
2023-09-27 11:56:30.344 
Epoch 201/1000 
	 loss: 16.8178, MinusLogProbMetric: 16.8178, val_loss: 17.2403, val_MinusLogProbMetric: 17.2403

Epoch 201: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.8178 - MinusLogProbMetric: 16.8178 - val_loss: 17.2403 - val_MinusLogProbMetric: 17.2403 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 202/1000
2023-09-27 11:57:02.640 
Epoch 202/1000 
	 loss: 16.8184, MinusLogProbMetric: 16.8184, val_loss: 17.0090, val_MinusLogProbMetric: 17.0090

Epoch 202: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.8184 - MinusLogProbMetric: 16.8184 - val_loss: 17.0090 - val_MinusLogProbMetric: 17.0090 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 203/1000
2023-09-27 11:57:35.164 
Epoch 203/1000 
	 loss: 16.7880, MinusLogProbMetric: 16.7880, val_loss: 17.1836, val_MinusLogProbMetric: 17.1836

Epoch 203: val_loss did not improve from 16.93548
196/196 - 33s - loss: 16.7880 - MinusLogProbMetric: 16.7880 - val_loss: 17.1836 - val_MinusLogProbMetric: 17.1836 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 204/1000
2023-09-27 11:58:07.468 
Epoch 204/1000 
	 loss: 16.8007, MinusLogProbMetric: 16.8007, val_loss: 17.5902, val_MinusLogProbMetric: 17.5902

Epoch 204: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.8007 - MinusLogProbMetric: 16.8007 - val_loss: 17.5902 - val_MinusLogProbMetric: 17.5902 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 205/1000
2023-09-27 11:58:40.015 
Epoch 205/1000 
	 loss: 16.8216, MinusLogProbMetric: 16.8216, val_loss: 17.0123, val_MinusLogProbMetric: 17.0123

Epoch 205: val_loss did not improve from 16.93548
196/196 - 33s - loss: 16.8216 - MinusLogProbMetric: 16.8216 - val_loss: 17.0123 - val_MinusLogProbMetric: 17.0123 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 206/1000
2023-09-27 11:59:12.134 
Epoch 206/1000 
	 loss: 16.7772, MinusLogProbMetric: 16.7772, val_loss: 17.0301, val_MinusLogProbMetric: 17.0301

Epoch 206: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7772 - MinusLogProbMetric: 16.7772 - val_loss: 17.0301 - val_MinusLogProbMetric: 17.0301 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 207/1000
2023-09-27 11:59:44.642 
Epoch 207/1000 
	 loss: 16.8029, MinusLogProbMetric: 16.8029, val_loss: 17.0090, val_MinusLogProbMetric: 17.0090

Epoch 207: val_loss did not improve from 16.93548
196/196 - 33s - loss: 16.8029 - MinusLogProbMetric: 16.8029 - val_loss: 17.0090 - val_MinusLogProbMetric: 17.0090 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 208/1000
2023-09-27 12:00:16.839 
Epoch 208/1000 
	 loss: 16.8020, MinusLogProbMetric: 16.8020, val_loss: 17.0607, val_MinusLogProbMetric: 17.0607

Epoch 208: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.8020 - MinusLogProbMetric: 16.8020 - val_loss: 17.0607 - val_MinusLogProbMetric: 17.0607 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 209/1000
2023-09-27 12:00:48.945 
Epoch 209/1000 
	 loss: 16.7727, MinusLogProbMetric: 16.7727, val_loss: 17.1144, val_MinusLogProbMetric: 17.1144

Epoch 209: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7727 - MinusLogProbMetric: 16.7727 - val_loss: 17.1144 - val_MinusLogProbMetric: 17.1144 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 210/1000
2023-09-27 12:01:21.121 
Epoch 210/1000 
	 loss: 16.7652, MinusLogProbMetric: 16.7652, val_loss: 17.1508, val_MinusLogProbMetric: 17.1508

Epoch 210: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7652 - MinusLogProbMetric: 16.7652 - val_loss: 17.1508 - val_MinusLogProbMetric: 17.1508 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 211/1000
2023-09-27 12:01:53.320 
Epoch 211/1000 
	 loss: 16.8162, MinusLogProbMetric: 16.8162, val_loss: 17.0000, val_MinusLogProbMetric: 17.0000

Epoch 211: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.8162 - MinusLogProbMetric: 16.8162 - val_loss: 17.0000 - val_MinusLogProbMetric: 17.0000 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 212/1000
2023-09-27 12:02:25.454 
Epoch 212/1000 
	 loss: 16.7579, MinusLogProbMetric: 16.7579, val_loss: 17.2447, val_MinusLogProbMetric: 17.2447

Epoch 212: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7579 - MinusLogProbMetric: 16.7579 - val_loss: 17.2447 - val_MinusLogProbMetric: 17.2447 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 213/1000
2023-09-27 12:02:57.927 
Epoch 213/1000 
	 loss: 16.7826, MinusLogProbMetric: 16.7826, val_loss: 17.0555, val_MinusLogProbMetric: 17.0555

Epoch 213: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7826 - MinusLogProbMetric: 16.7826 - val_loss: 17.0555 - val_MinusLogProbMetric: 17.0555 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 214/1000
2023-09-27 12:03:30.090 
Epoch 214/1000 
	 loss: 16.7559, MinusLogProbMetric: 16.7559, val_loss: 16.9677, val_MinusLogProbMetric: 16.9677

Epoch 214: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7559 - MinusLogProbMetric: 16.7559 - val_loss: 16.9677 - val_MinusLogProbMetric: 16.9677 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 215/1000
2023-09-27 12:04:02.361 
Epoch 215/1000 
	 loss: 16.7515, MinusLogProbMetric: 16.7515, val_loss: 17.1559, val_MinusLogProbMetric: 17.1559

Epoch 215: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7515 - MinusLogProbMetric: 16.7515 - val_loss: 17.1559 - val_MinusLogProbMetric: 17.1559 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 216/1000
2023-09-27 12:04:34.630 
Epoch 216/1000 
	 loss: 16.7725, MinusLogProbMetric: 16.7725, val_loss: 16.9758, val_MinusLogProbMetric: 16.9758

Epoch 216: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7725 - MinusLogProbMetric: 16.7725 - val_loss: 16.9758 - val_MinusLogProbMetric: 16.9758 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 217/1000
2023-09-27 12:05:06.855 
Epoch 217/1000 
	 loss: 16.7685, MinusLogProbMetric: 16.7685, val_loss: 17.2226, val_MinusLogProbMetric: 17.2226

Epoch 217: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7685 - MinusLogProbMetric: 16.7685 - val_loss: 17.2226 - val_MinusLogProbMetric: 17.2226 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 218/1000
2023-09-27 12:05:39.365 
Epoch 218/1000 
	 loss: 16.7624, MinusLogProbMetric: 16.7624, val_loss: 17.0413, val_MinusLogProbMetric: 17.0413

Epoch 218: val_loss did not improve from 16.93548
196/196 - 33s - loss: 16.7624 - MinusLogProbMetric: 16.7624 - val_loss: 17.0413 - val_MinusLogProbMetric: 17.0413 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 219/1000
2023-09-27 12:06:11.436 
Epoch 219/1000 
	 loss: 16.7403, MinusLogProbMetric: 16.7403, val_loss: 17.1768, val_MinusLogProbMetric: 17.1768

Epoch 219: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7403 - MinusLogProbMetric: 16.7403 - val_loss: 17.1768 - val_MinusLogProbMetric: 17.1768 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 220/1000
2023-09-27 12:06:43.637 
Epoch 220/1000 
	 loss: 16.7818, MinusLogProbMetric: 16.7818, val_loss: 16.9452, val_MinusLogProbMetric: 16.9452

Epoch 220: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7818 - MinusLogProbMetric: 16.7818 - val_loss: 16.9452 - val_MinusLogProbMetric: 16.9452 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 221/1000
2023-09-27 12:07:15.757 
Epoch 221/1000 
	 loss: 16.7232, MinusLogProbMetric: 16.7232, val_loss: 16.9622, val_MinusLogProbMetric: 16.9622

Epoch 221: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7232 - MinusLogProbMetric: 16.7232 - val_loss: 16.9622 - val_MinusLogProbMetric: 16.9622 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 222/1000
2023-09-27 12:07:48.354 
Epoch 222/1000 
	 loss: 16.7442, MinusLogProbMetric: 16.7442, val_loss: 17.5187, val_MinusLogProbMetric: 17.5187

Epoch 222: val_loss did not improve from 16.93548
196/196 - 33s - loss: 16.7442 - MinusLogProbMetric: 16.7442 - val_loss: 17.5187 - val_MinusLogProbMetric: 17.5187 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 223/1000
2023-09-27 12:08:20.699 
Epoch 223/1000 
	 loss: 16.7350, MinusLogProbMetric: 16.7350, val_loss: 17.1495, val_MinusLogProbMetric: 17.1495

Epoch 223: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7350 - MinusLogProbMetric: 16.7350 - val_loss: 17.1495 - val_MinusLogProbMetric: 17.1495 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 224/1000
2023-09-27 12:08:53.204 
Epoch 224/1000 
	 loss: 16.7530, MinusLogProbMetric: 16.7530, val_loss: 17.1770, val_MinusLogProbMetric: 17.1770

Epoch 224: val_loss did not improve from 16.93548
196/196 - 33s - loss: 16.7530 - MinusLogProbMetric: 16.7530 - val_loss: 17.1770 - val_MinusLogProbMetric: 17.1770 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 225/1000
2023-09-27 12:09:25.777 
Epoch 225/1000 
	 loss: 16.7615, MinusLogProbMetric: 16.7615, val_loss: 16.9838, val_MinusLogProbMetric: 16.9838

Epoch 225: val_loss did not improve from 16.93548
196/196 - 33s - loss: 16.7615 - MinusLogProbMetric: 16.7615 - val_loss: 16.9838 - val_MinusLogProbMetric: 16.9838 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 226/1000
2023-09-27 12:09:58.034 
Epoch 226/1000 
	 loss: 16.7545, MinusLogProbMetric: 16.7545, val_loss: 17.3503, val_MinusLogProbMetric: 17.3503

Epoch 226: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7545 - MinusLogProbMetric: 16.7545 - val_loss: 17.3503 - val_MinusLogProbMetric: 17.3503 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 227/1000
2023-09-27 12:10:30.285 
Epoch 227/1000 
	 loss: 16.7894, MinusLogProbMetric: 16.7894, val_loss: 17.0165, val_MinusLogProbMetric: 17.0165

Epoch 227: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7894 - MinusLogProbMetric: 16.7894 - val_loss: 17.0165 - val_MinusLogProbMetric: 17.0165 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 228/1000
2023-09-27 12:11:02.624 
Epoch 228/1000 
	 loss: 16.7366, MinusLogProbMetric: 16.7366, val_loss: 17.0590, val_MinusLogProbMetric: 17.0590

Epoch 228: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7366 - MinusLogProbMetric: 16.7366 - val_loss: 17.0590 - val_MinusLogProbMetric: 17.0590 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 229/1000
2023-09-27 12:11:35.168 
Epoch 229/1000 
	 loss: 16.7287, MinusLogProbMetric: 16.7287, val_loss: 16.9978, val_MinusLogProbMetric: 16.9978

Epoch 229: val_loss did not improve from 16.93548
196/196 - 33s - loss: 16.7287 - MinusLogProbMetric: 16.7287 - val_loss: 16.9978 - val_MinusLogProbMetric: 16.9978 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 230/1000
2023-09-27 12:12:07.129 
Epoch 230/1000 
	 loss: 16.7700, MinusLogProbMetric: 16.7700, val_loss: 17.0258, val_MinusLogProbMetric: 17.0258

Epoch 230: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7700 - MinusLogProbMetric: 16.7700 - val_loss: 17.0258 - val_MinusLogProbMetric: 17.0258 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 231/1000
2023-09-27 12:12:39.581 
Epoch 231/1000 
	 loss: 16.7378, MinusLogProbMetric: 16.7378, val_loss: 17.1098, val_MinusLogProbMetric: 17.1098

Epoch 231: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7378 - MinusLogProbMetric: 16.7378 - val_loss: 17.1098 - val_MinusLogProbMetric: 17.1098 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 232/1000
2023-09-27 12:13:11.829 
Epoch 232/1000 
	 loss: 16.7206, MinusLogProbMetric: 16.7206, val_loss: 16.9549, val_MinusLogProbMetric: 16.9549

Epoch 232: val_loss did not improve from 16.93548
196/196 - 32s - loss: 16.7206 - MinusLogProbMetric: 16.7206 - val_loss: 16.9549 - val_MinusLogProbMetric: 16.9549 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 233/1000
2023-09-27 12:13:44.081 
Epoch 233/1000 
	 loss: 16.7189, MinusLogProbMetric: 16.7189, val_loss: 16.9006, val_MinusLogProbMetric: 16.9006

Epoch 233: val_loss improved from 16.93548 to 16.90059, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 16.7189 - MinusLogProbMetric: 16.7189 - val_loss: 16.9006 - val_MinusLogProbMetric: 16.9006 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 234/1000
2023-09-27 12:14:17.099 
Epoch 234/1000 
	 loss: 16.7320, MinusLogProbMetric: 16.7320, val_loss: 17.2822, val_MinusLogProbMetric: 17.2822

Epoch 234: val_loss did not improve from 16.90059
196/196 - 33s - loss: 16.7320 - MinusLogProbMetric: 16.7320 - val_loss: 17.2822 - val_MinusLogProbMetric: 17.2822 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 235/1000
2023-09-27 12:14:49.176 
Epoch 235/1000 
	 loss: 16.7678, MinusLogProbMetric: 16.7678, val_loss: 17.2428, val_MinusLogProbMetric: 17.2428

Epoch 235: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.7678 - MinusLogProbMetric: 16.7678 - val_loss: 17.2428 - val_MinusLogProbMetric: 17.2428 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 236/1000
2023-09-27 12:15:21.623 
Epoch 236/1000 
	 loss: 16.7052, MinusLogProbMetric: 16.7052, val_loss: 17.1785, val_MinusLogProbMetric: 17.1785

Epoch 236: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.7052 - MinusLogProbMetric: 16.7052 - val_loss: 17.1785 - val_MinusLogProbMetric: 17.1785 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 237/1000
2023-09-27 12:15:54.288 
Epoch 237/1000 
	 loss: 16.7082, MinusLogProbMetric: 16.7082, val_loss: 17.1878, val_MinusLogProbMetric: 17.1878

Epoch 237: val_loss did not improve from 16.90059
196/196 - 33s - loss: 16.7082 - MinusLogProbMetric: 16.7082 - val_loss: 17.1878 - val_MinusLogProbMetric: 17.1878 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 238/1000
2023-09-27 12:16:26.413 
Epoch 238/1000 
	 loss: 16.7258, MinusLogProbMetric: 16.7258, val_loss: 16.9128, val_MinusLogProbMetric: 16.9128

Epoch 238: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.7258 - MinusLogProbMetric: 16.7258 - val_loss: 16.9128 - val_MinusLogProbMetric: 16.9128 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 239/1000
2023-09-27 12:16:58.861 
Epoch 239/1000 
	 loss: 16.7033, MinusLogProbMetric: 16.7033, val_loss: 16.9604, val_MinusLogProbMetric: 16.9604

Epoch 239: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.7033 - MinusLogProbMetric: 16.7033 - val_loss: 16.9604 - val_MinusLogProbMetric: 16.9604 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 240/1000
2023-09-27 12:17:31.307 
Epoch 240/1000 
	 loss: 16.7277, MinusLogProbMetric: 16.7277, val_loss: 17.0466, val_MinusLogProbMetric: 17.0466

Epoch 240: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.7277 - MinusLogProbMetric: 16.7277 - val_loss: 17.0466 - val_MinusLogProbMetric: 17.0466 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 241/1000
2023-09-27 12:18:03.826 
Epoch 241/1000 
	 loss: 16.7189, MinusLogProbMetric: 16.7189, val_loss: 17.0704, val_MinusLogProbMetric: 17.0704

Epoch 241: val_loss did not improve from 16.90059
196/196 - 33s - loss: 16.7189 - MinusLogProbMetric: 16.7189 - val_loss: 17.0704 - val_MinusLogProbMetric: 17.0704 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 242/1000
2023-09-27 12:18:36.175 
Epoch 242/1000 
	 loss: 16.7346, MinusLogProbMetric: 16.7346, val_loss: 17.0848, val_MinusLogProbMetric: 17.0848

Epoch 242: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.7346 - MinusLogProbMetric: 16.7346 - val_loss: 17.0848 - val_MinusLogProbMetric: 17.0848 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 243/1000
2023-09-27 12:19:08.479 
Epoch 243/1000 
	 loss: 16.7152, MinusLogProbMetric: 16.7152, val_loss: 17.1522, val_MinusLogProbMetric: 17.1522

Epoch 243: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.7152 - MinusLogProbMetric: 16.7152 - val_loss: 17.1522 - val_MinusLogProbMetric: 17.1522 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 244/1000
2023-09-27 12:19:40.630 
Epoch 244/1000 
	 loss: 16.7026, MinusLogProbMetric: 16.7026, val_loss: 17.0476, val_MinusLogProbMetric: 17.0476

Epoch 244: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.7026 - MinusLogProbMetric: 16.7026 - val_loss: 17.0476 - val_MinusLogProbMetric: 17.0476 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 245/1000
2023-09-27 12:20:12.624 
Epoch 245/1000 
	 loss: 16.7138, MinusLogProbMetric: 16.7138, val_loss: 16.9678, val_MinusLogProbMetric: 16.9678

Epoch 245: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.7138 - MinusLogProbMetric: 16.7138 - val_loss: 16.9678 - val_MinusLogProbMetric: 16.9678 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 246/1000
2023-09-27 12:20:44.897 
Epoch 246/1000 
	 loss: 16.6747, MinusLogProbMetric: 16.6747, val_loss: 16.9915, val_MinusLogProbMetric: 16.9915

Epoch 246: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6747 - MinusLogProbMetric: 16.6747 - val_loss: 16.9915 - val_MinusLogProbMetric: 16.9915 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 247/1000
2023-09-27 12:21:16.993 
Epoch 247/1000 
	 loss: 16.6947, MinusLogProbMetric: 16.6947, val_loss: 16.9876, val_MinusLogProbMetric: 16.9876

Epoch 247: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6947 - MinusLogProbMetric: 16.6947 - val_loss: 16.9876 - val_MinusLogProbMetric: 16.9876 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 248/1000
2023-09-27 12:21:49.166 
Epoch 248/1000 
	 loss: 16.6625, MinusLogProbMetric: 16.6625, val_loss: 17.0850, val_MinusLogProbMetric: 17.0850

Epoch 248: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6625 - MinusLogProbMetric: 16.6625 - val_loss: 17.0850 - val_MinusLogProbMetric: 17.0850 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 249/1000
2023-09-27 12:22:21.485 
Epoch 249/1000 
	 loss: 16.7011, MinusLogProbMetric: 16.7011, val_loss: 17.6115, val_MinusLogProbMetric: 17.6115

Epoch 249: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.7011 - MinusLogProbMetric: 16.7011 - val_loss: 17.6115 - val_MinusLogProbMetric: 17.6115 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 250/1000
2023-09-27 12:22:53.482 
Epoch 250/1000 
	 loss: 16.7080, MinusLogProbMetric: 16.7080, val_loss: 17.1012, val_MinusLogProbMetric: 17.1012

Epoch 250: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.7080 - MinusLogProbMetric: 16.7080 - val_loss: 17.1012 - val_MinusLogProbMetric: 17.1012 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 251/1000
2023-09-27 12:23:25.414 
Epoch 251/1000 
	 loss: 16.6776, MinusLogProbMetric: 16.6776, val_loss: 16.9919, val_MinusLogProbMetric: 16.9919

Epoch 251: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6776 - MinusLogProbMetric: 16.6776 - val_loss: 16.9919 - val_MinusLogProbMetric: 16.9919 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 252/1000
2023-09-27 12:23:57.082 
Epoch 252/1000 
	 loss: 16.6819, MinusLogProbMetric: 16.6819, val_loss: 16.9395, val_MinusLogProbMetric: 16.9395

Epoch 252: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6819 - MinusLogProbMetric: 16.6819 - val_loss: 16.9395 - val_MinusLogProbMetric: 16.9395 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 253/1000
2023-09-27 12:24:28.983 
Epoch 253/1000 
	 loss: 16.6807, MinusLogProbMetric: 16.6807, val_loss: 16.9930, val_MinusLogProbMetric: 16.9930

Epoch 253: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6807 - MinusLogProbMetric: 16.6807 - val_loss: 16.9930 - val_MinusLogProbMetric: 16.9930 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 254/1000
2023-09-27 12:25:00.851 
Epoch 254/1000 
	 loss: 16.6627, MinusLogProbMetric: 16.6627, val_loss: 17.0924, val_MinusLogProbMetric: 17.0924

Epoch 254: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6627 - MinusLogProbMetric: 16.6627 - val_loss: 17.0924 - val_MinusLogProbMetric: 17.0924 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 255/1000
2023-09-27 12:25:32.438 
Epoch 255/1000 
	 loss: 16.6909, MinusLogProbMetric: 16.6909, val_loss: 16.9422, val_MinusLogProbMetric: 16.9422

Epoch 255: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6909 - MinusLogProbMetric: 16.6909 - val_loss: 16.9422 - val_MinusLogProbMetric: 16.9422 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 256/1000
2023-09-27 12:26:04.927 
Epoch 256/1000 
	 loss: 16.6582, MinusLogProbMetric: 16.6582, val_loss: 17.0723, val_MinusLogProbMetric: 17.0723

Epoch 256: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6582 - MinusLogProbMetric: 16.6582 - val_loss: 17.0723 - val_MinusLogProbMetric: 17.0723 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 257/1000
2023-09-27 12:26:36.599 
Epoch 257/1000 
	 loss: 16.6506, MinusLogProbMetric: 16.6506, val_loss: 16.9254, val_MinusLogProbMetric: 16.9254

Epoch 257: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6506 - MinusLogProbMetric: 16.6506 - val_loss: 16.9254 - val_MinusLogProbMetric: 16.9254 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 258/1000
2023-09-27 12:27:08.376 
Epoch 258/1000 
	 loss: 16.6680, MinusLogProbMetric: 16.6680, val_loss: 17.0126, val_MinusLogProbMetric: 17.0126

Epoch 258: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6680 - MinusLogProbMetric: 16.6680 - val_loss: 17.0126 - val_MinusLogProbMetric: 17.0126 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 259/1000
2023-09-27 12:27:40.404 
Epoch 259/1000 
	 loss: 16.6608, MinusLogProbMetric: 16.6608, val_loss: 17.3078, val_MinusLogProbMetric: 17.3078

Epoch 259: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6608 - MinusLogProbMetric: 16.6608 - val_loss: 17.3078 - val_MinusLogProbMetric: 17.3078 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 260/1000
2023-09-27 12:28:12.387 
Epoch 260/1000 
	 loss: 16.7232, MinusLogProbMetric: 16.7232, val_loss: 17.0808, val_MinusLogProbMetric: 17.0808

Epoch 260: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.7232 - MinusLogProbMetric: 16.7232 - val_loss: 17.0808 - val_MinusLogProbMetric: 17.0808 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 261/1000
2023-09-27 12:28:44.365 
Epoch 261/1000 
	 loss: 16.6591, MinusLogProbMetric: 16.6591, val_loss: 16.9519, val_MinusLogProbMetric: 16.9519

Epoch 261: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6591 - MinusLogProbMetric: 16.6591 - val_loss: 16.9519 - val_MinusLogProbMetric: 16.9519 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 262/1000
2023-09-27 12:29:16.438 
Epoch 262/1000 
	 loss: 16.6680, MinusLogProbMetric: 16.6680, val_loss: 17.0909, val_MinusLogProbMetric: 17.0909

Epoch 262: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6680 - MinusLogProbMetric: 16.6680 - val_loss: 17.0909 - val_MinusLogProbMetric: 17.0909 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 263/1000
2023-09-27 12:29:48.335 
Epoch 263/1000 
	 loss: 16.6935, MinusLogProbMetric: 16.6935, val_loss: 16.9762, val_MinusLogProbMetric: 16.9762

Epoch 263: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6935 - MinusLogProbMetric: 16.6935 - val_loss: 16.9762 - val_MinusLogProbMetric: 16.9762 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 264/1000
2023-09-27 12:30:20.155 
Epoch 264/1000 
	 loss: 16.6405, MinusLogProbMetric: 16.6405, val_loss: 16.9685, val_MinusLogProbMetric: 16.9685

Epoch 264: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6405 - MinusLogProbMetric: 16.6405 - val_loss: 16.9685 - val_MinusLogProbMetric: 16.9685 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 265/1000
2023-09-27 12:30:52.230 
Epoch 265/1000 
	 loss: 16.6707, MinusLogProbMetric: 16.6707, val_loss: 17.2170, val_MinusLogProbMetric: 17.2170

Epoch 265: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6707 - MinusLogProbMetric: 16.6707 - val_loss: 17.2170 - val_MinusLogProbMetric: 17.2170 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 266/1000
2023-09-27 12:31:24.158 
Epoch 266/1000 
	 loss: 16.6450, MinusLogProbMetric: 16.6450, val_loss: 17.2028, val_MinusLogProbMetric: 17.2028

Epoch 266: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6450 - MinusLogProbMetric: 16.6450 - val_loss: 17.2028 - val_MinusLogProbMetric: 17.2028 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 267/1000
2023-09-27 12:31:55.991 
Epoch 267/1000 
	 loss: 16.6372, MinusLogProbMetric: 16.6372, val_loss: 17.2940, val_MinusLogProbMetric: 17.2940

Epoch 267: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6372 - MinusLogProbMetric: 16.6372 - val_loss: 17.2940 - val_MinusLogProbMetric: 17.2940 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 268/1000
2023-09-27 12:32:28.125 
Epoch 268/1000 
	 loss: 16.6427, MinusLogProbMetric: 16.6427, val_loss: 17.1560, val_MinusLogProbMetric: 17.1560

Epoch 268: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6427 - MinusLogProbMetric: 16.6427 - val_loss: 17.1560 - val_MinusLogProbMetric: 17.1560 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 269/1000
2023-09-27 12:33:00.009 
Epoch 269/1000 
	 loss: 16.6331, MinusLogProbMetric: 16.6331, val_loss: 17.1478, val_MinusLogProbMetric: 17.1478

Epoch 269: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6331 - MinusLogProbMetric: 16.6331 - val_loss: 17.1478 - val_MinusLogProbMetric: 17.1478 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 270/1000
2023-09-27 12:33:32.236 
Epoch 270/1000 
	 loss: 16.6006, MinusLogProbMetric: 16.6006, val_loss: 16.9493, val_MinusLogProbMetric: 16.9493

Epoch 270: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6006 - MinusLogProbMetric: 16.6006 - val_loss: 16.9493 - val_MinusLogProbMetric: 16.9493 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 271/1000
2023-09-27 12:34:03.922 
Epoch 271/1000 
	 loss: 16.6514, MinusLogProbMetric: 16.6514, val_loss: 16.9581, val_MinusLogProbMetric: 16.9581

Epoch 271: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6514 - MinusLogProbMetric: 16.6514 - val_loss: 16.9581 - val_MinusLogProbMetric: 16.9581 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 272/1000
2023-09-27 12:34:35.936 
Epoch 272/1000 
	 loss: 16.6666, MinusLogProbMetric: 16.6666, val_loss: 16.9709, val_MinusLogProbMetric: 16.9709

Epoch 272: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6666 - MinusLogProbMetric: 16.6666 - val_loss: 16.9709 - val_MinusLogProbMetric: 16.9709 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 273/1000
2023-09-27 12:35:07.939 
Epoch 273/1000 
	 loss: 16.6268, MinusLogProbMetric: 16.6268, val_loss: 17.0975, val_MinusLogProbMetric: 17.0975

Epoch 273: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6268 - MinusLogProbMetric: 16.6268 - val_loss: 17.0975 - val_MinusLogProbMetric: 17.0975 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 274/1000
2023-09-27 12:35:39.470 
Epoch 274/1000 
	 loss: 16.6268, MinusLogProbMetric: 16.6268, val_loss: 17.0611, val_MinusLogProbMetric: 17.0611

Epoch 274: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6268 - MinusLogProbMetric: 16.6268 - val_loss: 17.0611 - val_MinusLogProbMetric: 17.0611 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 275/1000
2023-09-27 12:36:11.332 
Epoch 275/1000 
	 loss: 16.6403, MinusLogProbMetric: 16.6403, val_loss: 16.9845, val_MinusLogProbMetric: 16.9845

Epoch 275: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6403 - MinusLogProbMetric: 16.6403 - val_loss: 16.9845 - val_MinusLogProbMetric: 16.9845 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 276/1000
2023-09-27 12:36:43.416 
Epoch 276/1000 
	 loss: 16.6500, MinusLogProbMetric: 16.6500, val_loss: 17.0031, val_MinusLogProbMetric: 17.0031

Epoch 276: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6500 - MinusLogProbMetric: 16.6500 - val_loss: 17.0031 - val_MinusLogProbMetric: 17.0031 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 277/1000
2023-09-27 12:37:15.411 
Epoch 277/1000 
	 loss: 16.6399, MinusLogProbMetric: 16.6399, val_loss: 17.2889, val_MinusLogProbMetric: 17.2889

Epoch 277: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6399 - MinusLogProbMetric: 16.6399 - val_loss: 17.2889 - val_MinusLogProbMetric: 17.2889 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 278/1000
2023-09-27 12:37:47.340 
Epoch 278/1000 
	 loss: 16.6088, MinusLogProbMetric: 16.6088, val_loss: 16.9248, val_MinusLogProbMetric: 16.9248

Epoch 278: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6088 - MinusLogProbMetric: 16.6088 - val_loss: 16.9248 - val_MinusLogProbMetric: 16.9248 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 279/1000
2023-09-27 12:38:19.145 
Epoch 279/1000 
	 loss: 16.7018, MinusLogProbMetric: 16.7018, val_loss: 17.2322, val_MinusLogProbMetric: 17.2322

Epoch 279: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.7018 - MinusLogProbMetric: 16.7018 - val_loss: 17.2322 - val_MinusLogProbMetric: 17.2322 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 280/1000
2023-09-27 12:38:51.354 
Epoch 280/1000 
	 loss: 16.6447, MinusLogProbMetric: 16.6447, val_loss: 16.9892, val_MinusLogProbMetric: 16.9892

Epoch 280: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6447 - MinusLogProbMetric: 16.6447 - val_loss: 16.9892 - val_MinusLogProbMetric: 16.9892 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 281/1000
2023-09-27 12:39:23.004 
Epoch 281/1000 
	 loss: 16.6011, MinusLogProbMetric: 16.6011, val_loss: 16.9220, val_MinusLogProbMetric: 16.9220

Epoch 281: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6011 - MinusLogProbMetric: 16.6011 - val_loss: 16.9220 - val_MinusLogProbMetric: 16.9220 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 282/1000
2023-09-27 12:39:55.141 
Epoch 282/1000 
	 loss: 16.6411, MinusLogProbMetric: 16.6411, val_loss: 16.9248, val_MinusLogProbMetric: 16.9248

Epoch 282: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.6411 - MinusLogProbMetric: 16.6411 - val_loss: 16.9248 - val_MinusLogProbMetric: 16.9248 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 283/1000
2023-09-27 12:40:26.905 
Epoch 283/1000 
	 loss: 16.5890, MinusLogProbMetric: 16.5890, val_loss: 17.0155, val_MinusLogProbMetric: 17.0155

Epoch 283: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.5890 - MinusLogProbMetric: 16.5890 - val_loss: 17.0155 - val_MinusLogProbMetric: 17.0155 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 284/1000
2023-09-27 12:40:58.726 
Epoch 284/1000 
	 loss: 16.4574, MinusLogProbMetric: 16.4574, val_loss: 17.1036, val_MinusLogProbMetric: 17.1036

Epoch 284: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.4574 - MinusLogProbMetric: 16.4574 - val_loss: 17.1036 - val_MinusLogProbMetric: 17.1036 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 285/1000
2023-09-27 12:41:30.843 
Epoch 285/1000 
	 loss: 16.4711, MinusLogProbMetric: 16.4711, val_loss: 16.9293, val_MinusLogProbMetric: 16.9293

Epoch 285: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.4711 - MinusLogProbMetric: 16.4711 - val_loss: 16.9293 - val_MinusLogProbMetric: 16.9293 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 286/1000
2023-09-27 12:42:02.638 
Epoch 286/1000 
	 loss: 16.4631, MinusLogProbMetric: 16.4631, val_loss: 16.9121, val_MinusLogProbMetric: 16.9121

Epoch 286: val_loss did not improve from 16.90059
196/196 - 32s - loss: 16.4631 - MinusLogProbMetric: 16.4631 - val_loss: 16.9121 - val_MinusLogProbMetric: 16.9121 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 287/1000
2023-09-27 12:42:34.339 
Epoch 287/1000 
	 loss: 16.4614, MinusLogProbMetric: 16.4614, val_loss: 16.8677, val_MinusLogProbMetric: 16.8677

Epoch 287: val_loss improved from 16.90059 to 16.86766, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 32s - loss: 16.4614 - MinusLogProbMetric: 16.4614 - val_loss: 16.8677 - val_MinusLogProbMetric: 16.8677 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 288/1000
2023-09-27 12:43:07.134 
Epoch 288/1000 
	 loss: 16.4575, MinusLogProbMetric: 16.4575, val_loss: 16.9944, val_MinusLogProbMetric: 16.9944

Epoch 288: val_loss did not improve from 16.86766
196/196 - 32s - loss: 16.4575 - MinusLogProbMetric: 16.4575 - val_loss: 16.9944 - val_MinusLogProbMetric: 16.9944 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 289/1000
2023-09-27 12:43:38.817 
Epoch 289/1000 
	 loss: 16.4711, MinusLogProbMetric: 16.4711, val_loss: 16.9642, val_MinusLogProbMetric: 16.9642

Epoch 289: val_loss did not improve from 16.86766
196/196 - 32s - loss: 16.4711 - MinusLogProbMetric: 16.4711 - val_loss: 16.9642 - val_MinusLogProbMetric: 16.9642 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 290/1000
2023-09-27 12:44:10.601 
Epoch 290/1000 
	 loss: 16.4656, MinusLogProbMetric: 16.4656, val_loss: 17.0038, val_MinusLogProbMetric: 17.0038

Epoch 290: val_loss did not improve from 16.86766
196/196 - 32s - loss: 16.4656 - MinusLogProbMetric: 16.4656 - val_loss: 17.0038 - val_MinusLogProbMetric: 17.0038 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 291/1000
2023-09-27 12:44:42.411 
Epoch 291/1000 
	 loss: 16.4597, MinusLogProbMetric: 16.4597, val_loss: 16.8735, val_MinusLogProbMetric: 16.8735

Epoch 291: val_loss did not improve from 16.86766
196/196 - 32s - loss: 16.4597 - MinusLogProbMetric: 16.4597 - val_loss: 16.8735 - val_MinusLogProbMetric: 16.8735 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 292/1000
2023-09-27 12:45:14.309 
Epoch 292/1000 
	 loss: 16.4584, MinusLogProbMetric: 16.4584, val_loss: 17.0787, val_MinusLogProbMetric: 17.0787

Epoch 292: val_loss did not improve from 16.86766
196/196 - 32s - loss: 16.4584 - MinusLogProbMetric: 16.4584 - val_loss: 17.0787 - val_MinusLogProbMetric: 17.0787 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 293/1000
2023-09-27 12:45:46.513 
Epoch 293/1000 
	 loss: 16.4537, MinusLogProbMetric: 16.4537, val_loss: 16.9236, val_MinusLogProbMetric: 16.9236

Epoch 293: val_loss did not improve from 16.86766
196/196 - 32s - loss: 16.4537 - MinusLogProbMetric: 16.4537 - val_loss: 16.9236 - val_MinusLogProbMetric: 16.9236 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 294/1000
2023-09-27 12:46:18.433 
Epoch 294/1000 
	 loss: 16.4583, MinusLogProbMetric: 16.4583, val_loss: 16.8807, val_MinusLogProbMetric: 16.8807

Epoch 294: val_loss did not improve from 16.86766
196/196 - 32s - loss: 16.4583 - MinusLogProbMetric: 16.4583 - val_loss: 16.8807 - val_MinusLogProbMetric: 16.8807 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 295/1000
2023-09-27 12:46:50.355 
Epoch 295/1000 
	 loss: 16.4485, MinusLogProbMetric: 16.4485, val_loss: 16.9559, val_MinusLogProbMetric: 16.9559

Epoch 295: val_loss did not improve from 16.86766
196/196 - 32s - loss: 16.4485 - MinusLogProbMetric: 16.4485 - val_loss: 16.9559 - val_MinusLogProbMetric: 16.9559 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 296/1000
2023-09-27 12:47:22.290 
Epoch 296/1000 
	 loss: 16.4533, MinusLogProbMetric: 16.4533, val_loss: 16.9518, val_MinusLogProbMetric: 16.9518

Epoch 296: val_loss did not improve from 16.86766
196/196 - 32s - loss: 16.4533 - MinusLogProbMetric: 16.4533 - val_loss: 16.9518 - val_MinusLogProbMetric: 16.9518 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 297/1000
2023-09-27 12:47:54.213 
Epoch 297/1000 
	 loss: 16.4582, MinusLogProbMetric: 16.4582, val_loss: 16.8558, val_MinusLogProbMetric: 16.8558

Epoch 297: val_loss improved from 16.86766 to 16.85580, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 33s - loss: 16.4582 - MinusLogProbMetric: 16.4582 - val_loss: 16.8558 - val_MinusLogProbMetric: 16.8558 - lr: 5.0000e-04 - 33s/epoch - 166ms/step
Epoch 298/1000
2023-09-27 12:48:26.835 
Epoch 298/1000 
	 loss: 16.4606, MinusLogProbMetric: 16.4606, val_loss: 16.9327, val_MinusLogProbMetric: 16.9327

Epoch 298: val_loss did not improve from 16.85580
196/196 - 32s - loss: 16.4606 - MinusLogProbMetric: 16.4606 - val_loss: 16.9327 - val_MinusLogProbMetric: 16.9327 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 299/1000
2023-09-27 12:48:58.753 
Epoch 299/1000 
	 loss: 16.4594, MinusLogProbMetric: 16.4594, val_loss: 17.0684, val_MinusLogProbMetric: 17.0684

Epoch 299: val_loss did not improve from 16.85580
196/196 - 32s - loss: 16.4594 - MinusLogProbMetric: 16.4594 - val_loss: 17.0684 - val_MinusLogProbMetric: 17.0684 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 300/1000
2023-09-27 12:49:30.988 
Epoch 300/1000 
	 loss: 16.4625, MinusLogProbMetric: 16.4625, val_loss: 17.0695, val_MinusLogProbMetric: 17.0695

Epoch 300: val_loss did not improve from 16.85580
196/196 - 32s - loss: 16.4625 - MinusLogProbMetric: 16.4625 - val_loss: 17.0695 - val_MinusLogProbMetric: 17.0695 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 301/1000
2023-09-27 12:50:02.229 
Epoch 301/1000 
	 loss: 16.4655, MinusLogProbMetric: 16.4655, val_loss: 16.8812, val_MinusLogProbMetric: 16.8812

Epoch 301: val_loss did not improve from 16.85580
196/196 - 31s - loss: 16.4655 - MinusLogProbMetric: 16.4655 - val_loss: 16.8812 - val_MinusLogProbMetric: 16.8812 - lr: 5.0000e-04 - 31s/epoch - 159ms/step
Epoch 302/1000
2023-09-27 12:50:32.831 
Epoch 302/1000 
	 loss: 16.4522, MinusLogProbMetric: 16.4522, val_loss: 16.9057, val_MinusLogProbMetric: 16.9057

Epoch 302: val_loss did not improve from 16.85580
196/196 - 31s - loss: 16.4522 - MinusLogProbMetric: 16.4522 - val_loss: 16.9057 - val_MinusLogProbMetric: 16.9057 - lr: 5.0000e-04 - 31s/epoch - 156ms/step
Epoch 303/1000
2023-09-27 12:51:03.135 
Epoch 303/1000 
	 loss: 16.4557, MinusLogProbMetric: 16.4557, val_loss: 17.0534, val_MinusLogProbMetric: 17.0534

Epoch 303: val_loss did not improve from 16.85580
196/196 - 30s - loss: 16.4557 - MinusLogProbMetric: 16.4557 - val_loss: 17.0534 - val_MinusLogProbMetric: 17.0534 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 304/1000
2023-09-27 12:51:32.912 
Epoch 304/1000 
	 loss: 16.4562, MinusLogProbMetric: 16.4562, val_loss: 16.8400, val_MinusLogProbMetric: 16.8400

Epoch 304: val_loss improved from 16.85580 to 16.83998, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_305/weights/best_weights.h5
196/196 - 30s - loss: 16.4562 - MinusLogProbMetric: 16.4562 - val_loss: 16.8400 - val_MinusLogProbMetric: 16.8400 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 305/1000
2023-09-27 12:52:04.431 
Epoch 305/1000 
	 loss: 16.4453, MinusLogProbMetric: 16.4453, val_loss: 16.8532, val_MinusLogProbMetric: 16.8532

Epoch 305: val_loss did not improve from 16.83998
196/196 - 31s - loss: 16.4453 - MinusLogProbMetric: 16.4453 - val_loss: 16.8532 - val_MinusLogProbMetric: 16.8532 - lr: 5.0000e-04 - 31s/epoch - 158ms/step
Epoch 306/1000
2023-09-27 12:52:36.164 
Epoch 306/1000 
	 loss: 16.4404, MinusLogProbMetric: 16.4404, val_loss: 16.8733, val_MinusLogProbMetric: 16.8733

Epoch 306: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4404 - MinusLogProbMetric: 16.4404 - val_loss: 16.8733 - val_MinusLogProbMetric: 16.8733 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 307/1000
2023-09-27 12:53:08.037 
Epoch 307/1000 
	 loss: 16.4549, MinusLogProbMetric: 16.4549, val_loss: 16.8978, val_MinusLogProbMetric: 16.8978

Epoch 307: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4549 - MinusLogProbMetric: 16.4549 - val_loss: 16.8978 - val_MinusLogProbMetric: 16.8978 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 308/1000
2023-09-27 12:53:39.797 
Epoch 308/1000 
	 loss: 16.4360, MinusLogProbMetric: 16.4360, val_loss: 16.9333, val_MinusLogProbMetric: 16.9333

Epoch 308: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4360 - MinusLogProbMetric: 16.4360 - val_loss: 16.9333 - val_MinusLogProbMetric: 16.9333 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 309/1000
2023-09-27 12:54:11.705 
Epoch 309/1000 
	 loss: 16.4455, MinusLogProbMetric: 16.4455, val_loss: 16.8860, val_MinusLogProbMetric: 16.8860

Epoch 309: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4455 - MinusLogProbMetric: 16.4455 - val_loss: 16.8860 - val_MinusLogProbMetric: 16.8860 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 310/1000
2023-09-27 12:54:43.410 
Epoch 310/1000 
	 loss: 16.4507, MinusLogProbMetric: 16.4507, val_loss: 16.8420, val_MinusLogProbMetric: 16.8420

Epoch 310: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4507 - MinusLogProbMetric: 16.4507 - val_loss: 16.8420 - val_MinusLogProbMetric: 16.8420 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 311/1000
2023-09-27 12:55:15.333 
Epoch 311/1000 
	 loss: 16.4586, MinusLogProbMetric: 16.4586, val_loss: 16.9051, val_MinusLogProbMetric: 16.9051

Epoch 311: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4586 - MinusLogProbMetric: 16.4586 - val_loss: 16.9051 - val_MinusLogProbMetric: 16.9051 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 312/1000
2023-09-27 12:55:47.091 
Epoch 312/1000 
	 loss: 16.4382, MinusLogProbMetric: 16.4382, val_loss: 16.8755, val_MinusLogProbMetric: 16.8755

Epoch 312: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4382 - MinusLogProbMetric: 16.4382 - val_loss: 16.8755 - val_MinusLogProbMetric: 16.8755 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 313/1000
2023-09-27 12:56:19.080 
Epoch 313/1000 
	 loss: 16.4451, MinusLogProbMetric: 16.4451, val_loss: 16.9631, val_MinusLogProbMetric: 16.9631

Epoch 313: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4451 - MinusLogProbMetric: 16.4451 - val_loss: 16.9631 - val_MinusLogProbMetric: 16.9631 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 314/1000
2023-09-27 12:56:50.735 
Epoch 314/1000 
	 loss: 16.4580, MinusLogProbMetric: 16.4580, val_loss: 16.9614, val_MinusLogProbMetric: 16.9614

Epoch 314: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4580 - MinusLogProbMetric: 16.4580 - val_loss: 16.9614 - val_MinusLogProbMetric: 16.9614 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 315/1000
2023-09-27 12:57:22.391 
Epoch 315/1000 
	 loss: 16.4493, MinusLogProbMetric: 16.4493, val_loss: 16.8634, val_MinusLogProbMetric: 16.8634

Epoch 315: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4493 - MinusLogProbMetric: 16.4493 - val_loss: 16.8634 - val_MinusLogProbMetric: 16.8634 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 316/1000
2023-09-27 12:57:54.213 
Epoch 316/1000 
	 loss: 16.4432, MinusLogProbMetric: 16.4432, val_loss: 16.8598, val_MinusLogProbMetric: 16.8598

Epoch 316: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4432 - MinusLogProbMetric: 16.4432 - val_loss: 16.8598 - val_MinusLogProbMetric: 16.8598 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 317/1000
2023-09-27 12:58:25.896 
Epoch 317/1000 
	 loss: 16.4354, MinusLogProbMetric: 16.4354, val_loss: 16.9357, val_MinusLogProbMetric: 16.9357

Epoch 317: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4354 - MinusLogProbMetric: 16.4354 - val_loss: 16.9357 - val_MinusLogProbMetric: 16.9357 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 318/1000
2023-09-27 12:58:57.908 
Epoch 318/1000 
	 loss: 16.4418, MinusLogProbMetric: 16.4418, val_loss: 16.8767, val_MinusLogProbMetric: 16.8767

Epoch 318: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4418 - MinusLogProbMetric: 16.4418 - val_loss: 16.8767 - val_MinusLogProbMetric: 16.8767 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 319/1000
2023-09-27 12:59:29.866 
Epoch 319/1000 
	 loss: 16.4564, MinusLogProbMetric: 16.4564, val_loss: 16.9461, val_MinusLogProbMetric: 16.9461

Epoch 319: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4564 - MinusLogProbMetric: 16.4564 - val_loss: 16.9461 - val_MinusLogProbMetric: 16.9461 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 320/1000
2023-09-27 13:00:01.926 
Epoch 320/1000 
	 loss: 16.4397, MinusLogProbMetric: 16.4397, val_loss: 16.9140, val_MinusLogProbMetric: 16.9140

Epoch 320: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4397 - MinusLogProbMetric: 16.4397 - val_loss: 16.9140 - val_MinusLogProbMetric: 16.9140 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 321/1000
2023-09-27 13:00:33.616 
Epoch 321/1000 
	 loss: 16.4272, MinusLogProbMetric: 16.4272, val_loss: 16.8725, val_MinusLogProbMetric: 16.8725

Epoch 321: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4272 - MinusLogProbMetric: 16.4272 - val_loss: 16.8725 - val_MinusLogProbMetric: 16.8725 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 322/1000
2023-09-27 13:01:05.340 
Epoch 322/1000 
	 loss: 16.4361, MinusLogProbMetric: 16.4361, val_loss: 17.0353, val_MinusLogProbMetric: 17.0353

Epoch 322: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4361 - MinusLogProbMetric: 16.4361 - val_loss: 17.0353 - val_MinusLogProbMetric: 17.0353 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 323/1000
2023-09-27 13:01:36.968 
Epoch 323/1000 
	 loss: 16.4359, MinusLogProbMetric: 16.4359, val_loss: 16.8722, val_MinusLogProbMetric: 16.8722

Epoch 323: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4359 - MinusLogProbMetric: 16.4359 - val_loss: 16.8722 - val_MinusLogProbMetric: 16.8722 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 324/1000
2023-09-27 13:02:08.723 
Epoch 324/1000 
	 loss: 16.4377, MinusLogProbMetric: 16.4377, val_loss: 16.9622, val_MinusLogProbMetric: 16.9622

Epoch 324: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4377 - MinusLogProbMetric: 16.4377 - val_loss: 16.9622 - val_MinusLogProbMetric: 16.9622 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 325/1000
2023-09-27 13:02:40.541 
Epoch 325/1000 
	 loss: 16.4280, MinusLogProbMetric: 16.4280, val_loss: 16.9558, val_MinusLogProbMetric: 16.9558

Epoch 325: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4280 - MinusLogProbMetric: 16.4280 - val_loss: 16.9558 - val_MinusLogProbMetric: 16.9558 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 326/1000
2023-09-27 13:03:12.103 
Epoch 326/1000 
	 loss: 16.4474, MinusLogProbMetric: 16.4474, val_loss: 17.0850, val_MinusLogProbMetric: 17.0850

Epoch 326: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4474 - MinusLogProbMetric: 16.4474 - val_loss: 17.0850 - val_MinusLogProbMetric: 17.0850 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 327/1000
2023-09-27 13:03:43.941 
Epoch 327/1000 
	 loss: 16.4217, MinusLogProbMetric: 16.4217, val_loss: 16.9435, val_MinusLogProbMetric: 16.9435

Epoch 327: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4217 - MinusLogProbMetric: 16.4217 - val_loss: 16.9435 - val_MinusLogProbMetric: 16.9435 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 328/1000
2023-09-27 13:04:15.650 
Epoch 328/1000 
	 loss: 16.4394, MinusLogProbMetric: 16.4394, val_loss: 16.8957, val_MinusLogProbMetric: 16.8957

Epoch 328: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4394 - MinusLogProbMetric: 16.4394 - val_loss: 16.8957 - val_MinusLogProbMetric: 16.8957 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 329/1000
2023-09-27 13:04:47.560 
Epoch 329/1000 
	 loss: 16.4521, MinusLogProbMetric: 16.4521, val_loss: 16.8489, val_MinusLogProbMetric: 16.8489

Epoch 329: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4521 - MinusLogProbMetric: 16.4521 - val_loss: 16.8489 - val_MinusLogProbMetric: 16.8489 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 330/1000
2023-09-27 13:05:19.056 
Epoch 330/1000 
	 loss: 16.4406, MinusLogProbMetric: 16.4406, val_loss: 16.8637, val_MinusLogProbMetric: 16.8637

Epoch 330: val_loss did not improve from 16.83998
196/196 - 31s - loss: 16.4406 - MinusLogProbMetric: 16.4406 - val_loss: 16.8637 - val_MinusLogProbMetric: 16.8637 - lr: 5.0000e-04 - 31s/epoch - 161ms/step
Epoch 331/1000
2023-09-27 13:05:50.647 
Epoch 331/1000 
	 loss: 16.4315, MinusLogProbMetric: 16.4315, val_loss: 16.8847, val_MinusLogProbMetric: 16.8847

Epoch 331: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4315 - MinusLogProbMetric: 16.4315 - val_loss: 16.8847 - val_MinusLogProbMetric: 16.8847 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 332/1000
2023-09-27 13:06:22.613 
Epoch 332/1000 
	 loss: 16.4269, MinusLogProbMetric: 16.4269, val_loss: 16.8650, val_MinusLogProbMetric: 16.8650

Epoch 332: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4269 - MinusLogProbMetric: 16.4269 - val_loss: 16.8650 - val_MinusLogProbMetric: 16.8650 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 333/1000
2023-09-27 13:06:54.188 
Epoch 333/1000 
	 loss: 16.4207, MinusLogProbMetric: 16.4207, val_loss: 16.8663, val_MinusLogProbMetric: 16.8663

Epoch 333: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4207 - MinusLogProbMetric: 16.4207 - val_loss: 16.8663 - val_MinusLogProbMetric: 16.8663 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 334/1000
2023-09-27 13:07:26.291 
Epoch 334/1000 
	 loss: 16.4439, MinusLogProbMetric: 16.4439, val_loss: 16.8927, val_MinusLogProbMetric: 16.8927

Epoch 334: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4439 - MinusLogProbMetric: 16.4439 - val_loss: 16.8927 - val_MinusLogProbMetric: 16.8927 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 335/1000
2023-09-27 13:07:58.124 
Epoch 335/1000 
	 loss: 16.4269, MinusLogProbMetric: 16.4269, val_loss: 16.8883, val_MinusLogProbMetric: 16.8883

Epoch 335: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4269 - MinusLogProbMetric: 16.4269 - val_loss: 16.8883 - val_MinusLogProbMetric: 16.8883 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 336/1000
2023-09-27 13:08:29.871 
Epoch 336/1000 
	 loss: 16.4432, MinusLogProbMetric: 16.4432, val_loss: 16.9001, val_MinusLogProbMetric: 16.9001

Epoch 336: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4432 - MinusLogProbMetric: 16.4432 - val_loss: 16.9001 - val_MinusLogProbMetric: 16.9001 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 337/1000
2023-09-27 13:09:02.138 
Epoch 337/1000 
	 loss: 16.4283, MinusLogProbMetric: 16.4283, val_loss: 17.0155, val_MinusLogProbMetric: 17.0155

Epoch 337: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4283 - MinusLogProbMetric: 16.4283 - val_loss: 17.0155 - val_MinusLogProbMetric: 17.0155 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 338/1000
2023-09-27 13:09:33.653 
Epoch 338/1000 
	 loss: 16.4369, MinusLogProbMetric: 16.4369, val_loss: 16.9314, val_MinusLogProbMetric: 16.9314

Epoch 338: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4369 - MinusLogProbMetric: 16.4369 - val_loss: 16.9314 - val_MinusLogProbMetric: 16.9314 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 339/1000
2023-09-27 13:10:05.633 
Epoch 339/1000 
	 loss: 16.4302, MinusLogProbMetric: 16.4302, val_loss: 16.8929, val_MinusLogProbMetric: 16.8929

Epoch 339: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4302 - MinusLogProbMetric: 16.4302 - val_loss: 16.8929 - val_MinusLogProbMetric: 16.8929 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 340/1000
2023-09-27 13:10:37.393 
Epoch 340/1000 
	 loss: 16.4284, MinusLogProbMetric: 16.4284, val_loss: 16.9779, val_MinusLogProbMetric: 16.9779

Epoch 340: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4284 - MinusLogProbMetric: 16.4284 - val_loss: 16.9779 - val_MinusLogProbMetric: 16.9779 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 341/1000
2023-09-27 13:11:09.165 
Epoch 341/1000 
	 loss: 16.4218, MinusLogProbMetric: 16.4218, val_loss: 16.8953, val_MinusLogProbMetric: 16.8953

Epoch 341: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4218 - MinusLogProbMetric: 16.4218 - val_loss: 16.8953 - val_MinusLogProbMetric: 16.8953 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 342/1000
2023-09-27 13:11:40.956 
Epoch 342/1000 
	 loss: 16.4299, MinusLogProbMetric: 16.4299, val_loss: 16.9130, val_MinusLogProbMetric: 16.9130

Epoch 342: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4299 - MinusLogProbMetric: 16.4299 - val_loss: 16.9130 - val_MinusLogProbMetric: 16.9130 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 343/1000
2023-09-27 13:12:12.665 
Epoch 343/1000 
	 loss: 16.4372, MinusLogProbMetric: 16.4372, val_loss: 16.8851, val_MinusLogProbMetric: 16.8851

Epoch 343: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4372 - MinusLogProbMetric: 16.4372 - val_loss: 16.8851 - val_MinusLogProbMetric: 16.8851 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 344/1000
2023-09-27 13:12:44.373 
Epoch 344/1000 
	 loss: 16.4411, MinusLogProbMetric: 16.4411, val_loss: 16.8725, val_MinusLogProbMetric: 16.8725

Epoch 344: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4411 - MinusLogProbMetric: 16.4411 - val_loss: 16.8725 - val_MinusLogProbMetric: 16.8725 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 345/1000
2023-09-27 13:13:16.307 
Epoch 345/1000 
	 loss: 16.4310, MinusLogProbMetric: 16.4310, val_loss: 16.9941, val_MinusLogProbMetric: 16.9941

Epoch 345: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4310 - MinusLogProbMetric: 16.4310 - val_loss: 16.9941 - val_MinusLogProbMetric: 16.9941 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 346/1000
2023-09-27 13:13:48.110 
Epoch 346/1000 
	 loss: 16.4259, MinusLogProbMetric: 16.4259, val_loss: 16.9242, val_MinusLogProbMetric: 16.9242

Epoch 346: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4259 - MinusLogProbMetric: 16.4259 - val_loss: 16.9242 - val_MinusLogProbMetric: 16.9242 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 347/1000
2023-09-27 13:14:19.902 
Epoch 347/1000 
	 loss: 16.4359, MinusLogProbMetric: 16.4359, val_loss: 16.9260, val_MinusLogProbMetric: 16.9260

Epoch 347: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4359 - MinusLogProbMetric: 16.4359 - val_loss: 16.9260 - val_MinusLogProbMetric: 16.9260 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 348/1000
2023-09-27 13:14:51.858 
Epoch 348/1000 
	 loss: 16.4319, MinusLogProbMetric: 16.4319, val_loss: 16.8916, val_MinusLogProbMetric: 16.8916

Epoch 348: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4319 - MinusLogProbMetric: 16.4319 - val_loss: 16.8916 - val_MinusLogProbMetric: 16.8916 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 349/1000
2023-09-27 13:15:23.733 
Epoch 349/1000 
	 loss: 16.4214, MinusLogProbMetric: 16.4214, val_loss: 16.9264, val_MinusLogProbMetric: 16.9264

Epoch 349: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4214 - MinusLogProbMetric: 16.4214 - val_loss: 16.9264 - val_MinusLogProbMetric: 16.9264 - lr: 5.0000e-04 - 32s/epoch - 163ms/step
Epoch 350/1000
2023-09-27 13:15:55.340 
Epoch 350/1000 
	 loss: 16.4280, MinusLogProbMetric: 16.4280, val_loss: 16.8721, val_MinusLogProbMetric: 16.8721

Epoch 350: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4280 - MinusLogProbMetric: 16.4280 - val_loss: 16.8721 - val_MinusLogProbMetric: 16.8721 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 351/1000
2023-09-27 13:16:27.006 
Epoch 351/1000 
	 loss: 16.4333, MinusLogProbMetric: 16.4333, val_loss: 16.8660, val_MinusLogProbMetric: 16.8660

Epoch 351: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4333 - MinusLogProbMetric: 16.4333 - val_loss: 16.8660 - val_MinusLogProbMetric: 16.8660 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 352/1000
2023-09-27 13:16:58.735 
Epoch 352/1000 
	 loss: 16.4377, MinusLogProbMetric: 16.4377, val_loss: 16.8856, val_MinusLogProbMetric: 16.8856

Epoch 352: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4377 - MinusLogProbMetric: 16.4377 - val_loss: 16.8856 - val_MinusLogProbMetric: 16.8856 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 353/1000
2023-09-27 13:17:30.461 
Epoch 353/1000 
	 loss: 16.4308, MinusLogProbMetric: 16.4308, val_loss: 16.9008, val_MinusLogProbMetric: 16.9008

Epoch 353: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4308 - MinusLogProbMetric: 16.4308 - val_loss: 16.9008 - val_MinusLogProbMetric: 16.9008 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 354/1000
2023-09-27 13:18:02.236 
Epoch 354/1000 
	 loss: 16.4106, MinusLogProbMetric: 16.4106, val_loss: 16.9170, val_MinusLogProbMetric: 16.9170

Epoch 354: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.4106 - MinusLogProbMetric: 16.4106 - val_loss: 16.9170 - val_MinusLogProbMetric: 16.9170 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 355/1000
2023-09-27 13:18:33.725 
Epoch 355/1000 
	 loss: 16.3677, MinusLogProbMetric: 16.3677, val_loss: 16.8653, val_MinusLogProbMetric: 16.8653

Epoch 355: val_loss did not improve from 16.83998
196/196 - 31s - loss: 16.3677 - MinusLogProbMetric: 16.3677 - val_loss: 16.8653 - val_MinusLogProbMetric: 16.8653 - lr: 2.5000e-04 - 31s/epoch - 161ms/step
Epoch 356/1000
2023-09-27 13:19:05.300 
Epoch 356/1000 
	 loss: 16.3576, MinusLogProbMetric: 16.3576, val_loss: 16.9011, val_MinusLogProbMetric: 16.9011

Epoch 356: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3576 - MinusLogProbMetric: 16.3576 - val_loss: 16.9011 - val_MinusLogProbMetric: 16.9011 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 357/1000
2023-09-27 13:19:36.888 
Epoch 357/1000 
	 loss: 16.3630, MinusLogProbMetric: 16.3630, val_loss: 16.8425, val_MinusLogProbMetric: 16.8425

Epoch 357: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3630 - MinusLogProbMetric: 16.3630 - val_loss: 16.8425 - val_MinusLogProbMetric: 16.8425 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 358/1000
2023-09-27 13:20:08.678 
Epoch 358/1000 
	 loss: 16.3546, MinusLogProbMetric: 16.3546, val_loss: 16.9446, val_MinusLogProbMetric: 16.9446

Epoch 358: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3546 - MinusLogProbMetric: 16.3546 - val_loss: 16.9446 - val_MinusLogProbMetric: 16.9446 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 359/1000
2023-09-27 13:20:40.465 
Epoch 359/1000 
	 loss: 16.3572, MinusLogProbMetric: 16.3572, val_loss: 16.8471, val_MinusLogProbMetric: 16.8471

Epoch 359: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3572 - MinusLogProbMetric: 16.3572 - val_loss: 16.8471 - val_MinusLogProbMetric: 16.8471 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 360/1000
2023-09-27 13:21:12.128 
Epoch 360/1000 
	 loss: 16.3536, MinusLogProbMetric: 16.3536, val_loss: 16.8428, val_MinusLogProbMetric: 16.8428

Epoch 360: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3536 - MinusLogProbMetric: 16.3536 - val_loss: 16.8428 - val_MinusLogProbMetric: 16.8428 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 361/1000
2023-09-27 13:21:43.872 
Epoch 361/1000 
	 loss: 16.3587, MinusLogProbMetric: 16.3587, val_loss: 16.9034, val_MinusLogProbMetric: 16.9034

Epoch 361: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3587 - MinusLogProbMetric: 16.3587 - val_loss: 16.9034 - val_MinusLogProbMetric: 16.9034 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 362/1000
2023-09-27 13:22:15.571 
Epoch 362/1000 
	 loss: 16.3621, MinusLogProbMetric: 16.3621, val_loss: 16.8574, val_MinusLogProbMetric: 16.8574

Epoch 362: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3621 - MinusLogProbMetric: 16.3621 - val_loss: 16.8574 - val_MinusLogProbMetric: 16.8574 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 363/1000
2023-09-27 13:22:46.860 
Epoch 363/1000 
	 loss: 16.3566, MinusLogProbMetric: 16.3566, val_loss: 16.8719, val_MinusLogProbMetric: 16.8719

Epoch 363: val_loss did not improve from 16.83998
196/196 - 31s - loss: 16.3566 - MinusLogProbMetric: 16.3566 - val_loss: 16.8719 - val_MinusLogProbMetric: 16.8719 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 364/1000
2023-09-27 13:23:18.282 
Epoch 364/1000 
	 loss: 16.3631, MinusLogProbMetric: 16.3631, val_loss: 16.8442, val_MinusLogProbMetric: 16.8442

Epoch 364: val_loss did not improve from 16.83998
196/196 - 31s - loss: 16.3631 - MinusLogProbMetric: 16.3631 - val_loss: 16.8442 - val_MinusLogProbMetric: 16.8442 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 365/1000
2023-09-27 13:23:50.058 
Epoch 365/1000 
	 loss: 16.3593, MinusLogProbMetric: 16.3593, val_loss: 16.8426, val_MinusLogProbMetric: 16.8426

Epoch 365: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3593 - MinusLogProbMetric: 16.3593 - val_loss: 16.8426 - val_MinusLogProbMetric: 16.8426 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 366/1000
2023-09-27 13:24:21.940 
Epoch 366/1000 
	 loss: 16.3591, MinusLogProbMetric: 16.3591, val_loss: 16.8513, val_MinusLogProbMetric: 16.8513

Epoch 366: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3591 - MinusLogProbMetric: 16.3591 - val_loss: 16.8513 - val_MinusLogProbMetric: 16.8513 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 367/1000
2023-09-27 13:24:53.599 
Epoch 367/1000 
	 loss: 16.3616, MinusLogProbMetric: 16.3616, val_loss: 16.9911, val_MinusLogProbMetric: 16.9911

Epoch 367: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3616 - MinusLogProbMetric: 16.3616 - val_loss: 16.9911 - val_MinusLogProbMetric: 16.9911 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 368/1000
2023-09-27 13:25:25.250 
Epoch 368/1000 
	 loss: 16.3540, MinusLogProbMetric: 16.3540, val_loss: 16.9795, val_MinusLogProbMetric: 16.9795

Epoch 368: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3540 - MinusLogProbMetric: 16.3540 - val_loss: 16.9795 - val_MinusLogProbMetric: 16.9795 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 369/1000
2023-09-27 13:25:57.048 
Epoch 369/1000 
	 loss: 16.3572, MinusLogProbMetric: 16.3572, val_loss: 16.8682, val_MinusLogProbMetric: 16.8682

Epoch 369: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3572 - MinusLogProbMetric: 16.3572 - val_loss: 16.8682 - val_MinusLogProbMetric: 16.8682 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 370/1000
2023-09-27 13:26:28.363 
Epoch 370/1000 
	 loss: 16.3568, MinusLogProbMetric: 16.3568, val_loss: 16.9182, val_MinusLogProbMetric: 16.9182

Epoch 370: val_loss did not improve from 16.83998
196/196 - 31s - loss: 16.3568 - MinusLogProbMetric: 16.3568 - val_loss: 16.9182 - val_MinusLogProbMetric: 16.9182 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 371/1000
2023-09-27 13:26:59.856 
Epoch 371/1000 
	 loss: 16.3581, MinusLogProbMetric: 16.3581, val_loss: 16.9067, val_MinusLogProbMetric: 16.9067

Epoch 371: val_loss did not improve from 16.83998
196/196 - 31s - loss: 16.3581 - MinusLogProbMetric: 16.3581 - val_loss: 16.9067 - val_MinusLogProbMetric: 16.9067 - lr: 2.5000e-04 - 31s/epoch - 161ms/step
Epoch 372/1000
2023-09-27 13:27:31.620 
Epoch 372/1000 
	 loss: 16.3530, MinusLogProbMetric: 16.3530, val_loss: 16.8574, val_MinusLogProbMetric: 16.8574

Epoch 372: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3530 - MinusLogProbMetric: 16.3530 - val_loss: 16.8574 - val_MinusLogProbMetric: 16.8574 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 373/1000
2023-09-27 13:28:02.954 
Epoch 373/1000 
	 loss: 16.3550, MinusLogProbMetric: 16.3550, val_loss: 16.8913, val_MinusLogProbMetric: 16.8913

Epoch 373: val_loss did not improve from 16.83998
196/196 - 31s - loss: 16.3550 - MinusLogProbMetric: 16.3550 - val_loss: 16.8913 - val_MinusLogProbMetric: 16.8913 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 374/1000
2023-09-27 13:28:34.534 
Epoch 374/1000 
	 loss: 16.3481, MinusLogProbMetric: 16.3481, val_loss: 16.8896, val_MinusLogProbMetric: 16.8896

Epoch 374: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3481 - MinusLogProbMetric: 16.3481 - val_loss: 16.8896 - val_MinusLogProbMetric: 16.8896 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 375/1000
2023-09-27 13:29:05.997 
Epoch 375/1000 
	 loss: 16.3569, MinusLogProbMetric: 16.3569, val_loss: 16.8776, val_MinusLogProbMetric: 16.8776

Epoch 375: val_loss did not improve from 16.83998
196/196 - 31s - loss: 16.3569 - MinusLogProbMetric: 16.3569 - val_loss: 16.8776 - val_MinusLogProbMetric: 16.8776 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 376/1000
2023-09-27 13:29:37.876 
Epoch 376/1000 
	 loss: 16.3511, MinusLogProbMetric: 16.3511, val_loss: 16.8898, val_MinusLogProbMetric: 16.8898

Epoch 376: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3511 - MinusLogProbMetric: 16.3511 - val_loss: 16.8898 - val_MinusLogProbMetric: 16.8898 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 377/1000
2023-09-27 13:30:09.681 
Epoch 377/1000 
	 loss: 16.3548, MinusLogProbMetric: 16.3548, val_loss: 16.8556, val_MinusLogProbMetric: 16.8556

Epoch 377: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3548 - MinusLogProbMetric: 16.3548 - val_loss: 16.8556 - val_MinusLogProbMetric: 16.8556 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 378/1000
2023-09-27 13:30:41.493 
Epoch 378/1000 
	 loss: 16.3511, MinusLogProbMetric: 16.3511, val_loss: 16.9595, val_MinusLogProbMetric: 16.9595

Epoch 378: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3511 - MinusLogProbMetric: 16.3511 - val_loss: 16.9595 - val_MinusLogProbMetric: 16.9595 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 379/1000
2023-09-27 13:31:13.115 
Epoch 379/1000 
	 loss: 16.3452, MinusLogProbMetric: 16.3452, val_loss: 16.8793, val_MinusLogProbMetric: 16.8793

Epoch 379: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3452 - MinusLogProbMetric: 16.3452 - val_loss: 16.8793 - val_MinusLogProbMetric: 16.8793 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 380/1000
2023-09-27 13:31:44.458 
Epoch 380/1000 
	 loss: 16.3464, MinusLogProbMetric: 16.3464, val_loss: 16.8672, val_MinusLogProbMetric: 16.8672

Epoch 380: val_loss did not improve from 16.83998
196/196 - 31s - loss: 16.3464 - MinusLogProbMetric: 16.3464 - val_loss: 16.8672 - val_MinusLogProbMetric: 16.8672 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 381/1000
2023-09-27 13:32:15.977 
Epoch 381/1000 
	 loss: 16.3505, MinusLogProbMetric: 16.3505, val_loss: 16.8587, val_MinusLogProbMetric: 16.8587

Epoch 381: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3505 - MinusLogProbMetric: 16.3505 - val_loss: 16.8587 - val_MinusLogProbMetric: 16.8587 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 382/1000
2023-09-27 13:32:47.470 
Epoch 382/1000 
	 loss: 16.3534, MinusLogProbMetric: 16.3534, val_loss: 16.9493, val_MinusLogProbMetric: 16.9493

Epoch 382: val_loss did not improve from 16.83998
196/196 - 31s - loss: 16.3534 - MinusLogProbMetric: 16.3534 - val_loss: 16.9493 - val_MinusLogProbMetric: 16.9493 - lr: 2.5000e-04 - 31s/epoch - 161ms/step
Epoch 383/1000
2023-09-27 13:33:19.131 
Epoch 383/1000 
	 loss: 16.3499, MinusLogProbMetric: 16.3499, val_loss: 16.8654, val_MinusLogProbMetric: 16.8654

Epoch 383: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3499 - MinusLogProbMetric: 16.3499 - val_loss: 16.8654 - val_MinusLogProbMetric: 16.8654 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 384/1000
2023-09-27 13:33:50.683 
Epoch 384/1000 
	 loss: 16.3517, MinusLogProbMetric: 16.3517, val_loss: 16.8951, val_MinusLogProbMetric: 16.8951

Epoch 384: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3517 - MinusLogProbMetric: 16.3517 - val_loss: 16.8951 - val_MinusLogProbMetric: 16.8951 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 385/1000
2023-09-27 13:34:22.242 
Epoch 385/1000 
	 loss: 16.3462, MinusLogProbMetric: 16.3462, val_loss: 16.8762, val_MinusLogProbMetric: 16.8762

Epoch 385: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3462 - MinusLogProbMetric: 16.3462 - val_loss: 16.8762 - val_MinusLogProbMetric: 16.8762 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 386/1000
2023-09-27 13:34:53.890 
Epoch 386/1000 
	 loss: 16.3505, MinusLogProbMetric: 16.3505, val_loss: 16.8661, val_MinusLogProbMetric: 16.8661

Epoch 386: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3505 - MinusLogProbMetric: 16.3505 - val_loss: 16.8661 - val_MinusLogProbMetric: 16.8661 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 387/1000
2023-09-27 13:35:25.452 
Epoch 387/1000 
	 loss: 16.3523, MinusLogProbMetric: 16.3523, val_loss: 16.8713, val_MinusLogProbMetric: 16.8713

Epoch 387: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3523 - MinusLogProbMetric: 16.3523 - val_loss: 16.8713 - val_MinusLogProbMetric: 16.8713 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 388/1000
2023-09-27 13:35:56.977 
Epoch 388/1000 
	 loss: 16.3437, MinusLogProbMetric: 16.3437, val_loss: 16.8572, val_MinusLogProbMetric: 16.8572

Epoch 388: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3437 - MinusLogProbMetric: 16.3437 - val_loss: 16.8572 - val_MinusLogProbMetric: 16.8572 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 389/1000
2023-09-27 13:36:28.501 
Epoch 389/1000 
	 loss: 16.3532, MinusLogProbMetric: 16.3532, val_loss: 16.8621, val_MinusLogProbMetric: 16.8621

Epoch 389: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3532 - MinusLogProbMetric: 16.3532 - val_loss: 16.8621 - val_MinusLogProbMetric: 16.8621 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 390/1000
2023-09-27 13:37:00.115 
Epoch 390/1000 
	 loss: 16.3449, MinusLogProbMetric: 16.3449, val_loss: 16.8656, val_MinusLogProbMetric: 16.8656

Epoch 390: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3449 - MinusLogProbMetric: 16.3449 - val_loss: 16.8656 - val_MinusLogProbMetric: 16.8656 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 391/1000
2023-09-27 13:37:31.878 
Epoch 391/1000 
	 loss: 16.3511, MinusLogProbMetric: 16.3511, val_loss: 16.8853, val_MinusLogProbMetric: 16.8853

Epoch 391: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3511 - MinusLogProbMetric: 16.3511 - val_loss: 16.8853 - val_MinusLogProbMetric: 16.8853 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 392/1000
2023-09-27 13:38:03.726 
Epoch 392/1000 
	 loss: 16.3465, MinusLogProbMetric: 16.3465, val_loss: 16.9384, val_MinusLogProbMetric: 16.9384

Epoch 392: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3465 - MinusLogProbMetric: 16.3465 - val_loss: 16.9384 - val_MinusLogProbMetric: 16.9384 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 393/1000
2023-09-27 13:38:35.343 
Epoch 393/1000 
	 loss: 16.3495, MinusLogProbMetric: 16.3495, val_loss: 16.8537, val_MinusLogProbMetric: 16.8537

Epoch 393: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3495 - MinusLogProbMetric: 16.3495 - val_loss: 16.8537 - val_MinusLogProbMetric: 16.8537 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 394/1000
2023-09-27 13:39:06.988 
Epoch 394/1000 
	 loss: 16.3556, MinusLogProbMetric: 16.3556, val_loss: 16.8577, val_MinusLogProbMetric: 16.8577

Epoch 394: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3556 - MinusLogProbMetric: 16.3556 - val_loss: 16.8577 - val_MinusLogProbMetric: 16.8577 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 395/1000
2023-09-27 13:39:38.604 
Epoch 395/1000 
	 loss: 16.3475, MinusLogProbMetric: 16.3475, val_loss: 16.8828, val_MinusLogProbMetric: 16.8828

Epoch 395: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3475 - MinusLogProbMetric: 16.3475 - val_loss: 16.8828 - val_MinusLogProbMetric: 16.8828 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 396/1000
2023-09-27 13:40:10.653 
Epoch 396/1000 
	 loss: 16.3493, MinusLogProbMetric: 16.3493, val_loss: 16.8622, val_MinusLogProbMetric: 16.8622

Epoch 396: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3493 - MinusLogProbMetric: 16.3493 - val_loss: 16.8622 - val_MinusLogProbMetric: 16.8622 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 397/1000
2023-09-27 13:40:42.195 
Epoch 397/1000 
	 loss: 16.3521, MinusLogProbMetric: 16.3521, val_loss: 16.9056, val_MinusLogProbMetric: 16.9056

Epoch 397: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3521 - MinusLogProbMetric: 16.3521 - val_loss: 16.9056 - val_MinusLogProbMetric: 16.9056 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 398/1000
2023-09-27 13:41:14.023 
Epoch 398/1000 
	 loss: 16.3403, MinusLogProbMetric: 16.3403, val_loss: 16.8709, val_MinusLogProbMetric: 16.8709

Epoch 398: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3403 - MinusLogProbMetric: 16.3403 - val_loss: 16.8709 - val_MinusLogProbMetric: 16.8709 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 399/1000
2023-09-27 13:41:45.646 
Epoch 399/1000 
	 loss: 16.3506, MinusLogProbMetric: 16.3506, val_loss: 16.8796, val_MinusLogProbMetric: 16.8796

Epoch 399: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3506 - MinusLogProbMetric: 16.3506 - val_loss: 16.8796 - val_MinusLogProbMetric: 16.8796 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 400/1000
2023-09-27 13:42:17.144 
Epoch 400/1000 
	 loss: 16.3470, MinusLogProbMetric: 16.3470, val_loss: 16.9242, val_MinusLogProbMetric: 16.9242

Epoch 400: val_loss did not improve from 16.83998
196/196 - 31s - loss: 16.3470 - MinusLogProbMetric: 16.3470 - val_loss: 16.9242 - val_MinusLogProbMetric: 16.9242 - lr: 2.5000e-04 - 31s/epoch - 161ms/step
Epoch 401/1000
2023-09-27 13:42:48.526 
Epoch 401/1000 
	 loss: 16.3446, MinusLogProbMetric: 16.3446, val_loss: 16.9091, val_MinusLogProbMetric: 16.9091

Epoch 401: val_loss did not improve from 16.83998
196/196 - 31s - loss: 16.3446 - MinusLogProbMetric: 16.3446 - val_loss: 16.9091 - val_MinusLogProbMetric: 16.9091 - lr: 2.5000e-04 - 31s/epoch - 160ms/step
Epoch 402/1000
2023-09-27 13:43:20.011 
Epoch 402/1000 
	 loss: 16.3501, MinusLogProbMetric: 16.3501, val_loss: 16.8769, val_MinusLogProbMetric: 16.8769

Epoch 402: val_loss did not improve from 16.83998
196/196 - 31s - loss: 16.3501 - MinusLogProbMetric: 16.3501 - val_loss: 16.8769 - val_MinusLogProbMetric: 16.8769 - lr: 2.5000e-04 - 31s/epoch - 161ms/step
Epoch 403/1000
2023-09-27 13:43:51.683 
Epoch 403/1000 
	 loss: 16.3494, MinusLogProbMetric: 16.3494, val_loss: 16.9201, val_MinusLogProbMetric: 16.9201

Epoch 403: val_loss did not improve from 16.83998
196/196 - 32s - loss: 16.3494 - MinusLogProbMetric: 16.3494 - val_loss: 16.9201 - val_MinusLogProbMetric: 16.9201 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 404/1000
2023-09-27 13:44:23.265 
Epoch 404/1000 
	 loss: 16.3512, MinusLogProbMetric: 16.3512, val_loss: 16.8841, val_MinusLogProbMetric: 16.8841

Epoch 404: val_loss did not improve from 16.83998
Restoring model weights from the end of the best epoch: 304.
196/196 - 32s - loss: 16.3512 - MinusLogProbMetric: 16.3512 - val_loss: 16.8841 - val_MinusLogProbMetric: 16.8841 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 404: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 11.181626247998793 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 7.0714880069717765 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 7.019904310989659 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 6.336009701015428 seconds.
Training succeeded with seed 926.
Model trained in 13028.20 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 32.56 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 32.83 s.
===========
Run 305/720 done in 13065.37 s.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

===========
Generating train data for run 308.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_308/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_308/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_308/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_308
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_33"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_34 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  1399280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,399,280
Trainable params: 1,399,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7ff1bc700490>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff104330880>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff104330880>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff02c6d9ed0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7ff10422b580>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_308/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7ff10422a770>, <keras.callbacks.ModelCheckpoint object at 0x7ff104228a90>, <keras.callbacks.EarlyStopping object at 0x7ff10422aaa0>, <keras.callbacks.ReduceLROnPlateau object at 0x7ff104229a50>, <keras.callbacks.TerminateOnNaN object at 0x7ff104229180>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_308/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 308/720 with hyperparameters:
timestamp = 2023-09-27 13:45:01.700966
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1399280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
2023-09-27 13:46:54.894 
Epoch 1/1000 
	 loss: 54.1829, MinusLogProbMetric: 54.1829, val_loss: 24.9588, val_MinusLogProbMetric: 24.9588

Epoch 1: val_loss improved from inf to 24.95877, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_308/weights/best_weights.h5
196/196 - 114s - loss: 54.1829 - MinusLogProbMetric: 54.1829 - val_loss: 24.9588 - val_MinusLogProbMetric: 24.9588 - lr: 0.0010 - 114s/epoch - 579ms/step
Epoch 2/1000
2023-09-27 13:47:35.124 
Epoch 2/1000 
	 loss: 22.8483, MinusLogProbMetric: 22.8483, val_loss: 21.7957, val_MinusLogProbMetric: 21.7957

Epoch 2: val_loss improved from 24.95877 to 21.79567, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_308/weights/best_weights.h5
196/196 - 40s - loss: 22.8483 - MinusLogProbMetric: 22.8483 - val_loss: 21.7957 - val_MinusLogProbMetric: 21.7957 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 3/1000
2023-09-27 13:48:15.097 
Epoch 3/1000 
	 loss: 21.1477, MinusLogProbMetric: 21.1477, val_loss: 20.4059, val_MinusLogProbMetric: 20.4059

Epoch 3: val_loss improved from 21.79567 to 20.40588, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_308/weights/best_weights.h5
196/196 - 40s - loss: 21.1477 - MinusLogProbMetric: 21.1477 - val_loss: 20.4059 - val_MinusLogProbMetric: 20.4059 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 4/1000
2023-09-27 13:48:55.454 
Epoch 4/1000 
	 loss: 20.3581, MinusLogProbMetric: 20.3581, val_loss: 20.0417, val_MinusLogProbMetric: 20.0417

Epoch 4: val_loss improved from 20.40588 to 20.04166, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_308/weights/best_weights.h5
196/196 - 41s - loss: 20.3581 - MinusLogProbMetric: 20.3581 - val_loss: 20.0417 - val_MinusLogProbMetric: 20.0417 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 5/1000
2023-09-27 13:49:40.385 
Epoch 5/1000 
	 loss: 19.7391, MinusLogProbMetric: 19.7391, val_loss: 19.6482, val_MinusLogProbMetric: 19.6482

Epoch 5: val_loss improved from 20.04166 to 19.64817, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_308/weights/best_weights.h5
196/196 - 45s - loss: 19.7391 - MinusLogProbMetric: 19.7391 - val_loss: 19.6482 - val_MinusLogProbMetric: 19.6482 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 6/1000
2023-09-27 13:50:23.673 
Epoch 6/1000 
	 loss: 19.5907, MinusLogProbMetric: 19.5907, val_loss: 19.4907, val_MinusLogProbMetric: 19.4907

Epoch 6: val_loss improved from 19.64817 to 19.49066, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_308/weights/best_weights.h5
196/196 - 43s - loss: 19.5907 - MinusLogProbMetric: 19.5907 - val_loss: 19.4907 - val_MinusLogProbMetric: 19.4907 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 7/1000
2023-09-27 13:51:07.331 
Epoch 7/1000 
	 loss: 19.1572, MinusLogProbMetric: 19.1572, val_loss: 18.9906, val_MinusLogProbMetric: 18.9906

Epoch 7: val_loss improved from 19.49066 to 18.99057, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_308/weights/best_weights.h5
196/196 - 44s - loss: 19.1572 - MinusLogProbMetric: 19.1572 - val_loss: 18.9906 - val_MinusLogProbMetric: 18.9906 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 8/1000
2023-09-27 13:51:50.912 
Epoch 8/1000 
	 loss: 18.9613, MinusLogProbMetric: 18.9613, val_loss: 19.1965, val_MinusLogProbMetric: 19.1965

Epoch 8: val_loss did not improve from 18.99057
196/196 - 43s - loss: 18.9613 - MinusLogProbMetric: 18.9613 - val_loss: 19.1965 - val_MinusLogProbMetric: 19.1965 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 9/1000
2023-09-27 13:52:34.787 
Epoch 9/1000 
	 loss: 18.7320, MinusLogProbMetric: 18.7320, val_loss: 19.4927, val_MinusLogProbMetric: 19.4927

Epoch 9: val_loss did not improve from 18.99057
196/196 - 44s - loss: 18.7320 - MinusLogProbMetric: 18.7320 - val_loss: 19.4927 - val_MinusLogProbMetric: 19.4927 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 10/1000
2023-09-27 13:53:16.700 
Epoch 10/1000 
	 loss: 18.5785, MinusLogProbMetric: 18.5785, val_loss: 18.2683, val_MinusLogProbMetric: 18.2683

Epoch 10: val_loss improved from 18.99057 to 18.26834, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_308/weights/best_weights.h5
196/196 - 43s - loss: 18.5785 - MinusLogProbMetric: 18.5785 - val_loss: 18.2683 - val_MinusLogProbMetric: 18.2683 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 11/1000
2023-09-27 13:53:59.436 
Epoch 11/1000 
	 loss: 18.2267, MinusLogProbMetric: 18.2267, val_loss: 18.4872, val_MinusLogProbMetric: 18.4872

Epoch 11: val_loss did not improve from 18.26834
196/196 - 42s - loss: 18.2267 - MinusLogProbMetric: 18.2267 - val_loss: 18.4872 - val_MinusLogProbMetric: 18.4872 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 12/1000
2023-09-27 13:54:41.621 
Epoch 12/1000 
	 loss: 18.1053, MinusLogProbMetric: 18.1053, val_loss: 18.8045, val_MinusLogProbMetric: 18.8045

Epoch 12: val_loss did not improve from 18.26834
196/196 - 42s - loss: 18.1053 - MinusLogProbMetric: 18.1053 - val_loss: 18.8045 - val_MinusLogProbMetric: 18.8045 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 13/1000
2023-09-27 13:55:23.544 
Epoch 13/1000 
	 loss: 17.9780, MinusLogProbMetric: 17.9780, val_loss: 17.9911, val_MinusLogProbMetric: 17.9911

Epoch 13: val_loss improved from 18.26834 to 17.99107, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_308/weights/best_weights.h5
196/196 - 43s - loss: 17.9780 - MinusLogProbMetric: 17.9780 - val_loss: 17.9911 - val_MinusLogProbMetric: 17.9911 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 14/1000
2023-09-27 13:56:06.135 
Epoch 14/1000 
	 loss: 17.8783, MinusLogProbMetric: 17.8783, val_loss: 17.8772, val_MinusLogProbMetric: 17.8772

Epoch 14: val_loss improved from 17.99107 to 17.87720, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_308/weights/best_weights.h5
196/196 - 43s - loss: 17.8783 - MinusLogProbMetric: 17.8783 - val_loss: 17.8772 - val_MinusLogProbMetric: 17.8772 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 15/1000
2023-09-27 13:56:49.127 
Epoch 15/1000 
	 loss: 17.7212, MinusLogProbMetric: 17.7212, val_loss: 17.7120, val_MinusLogProbMetric: 17.7120

Epoch 15: val_loss improved from 17.87720 to 17.71205, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_308/weights/best_weights.h5
196/196 - 43s - loss: 17.7212 - MinusLogProbMetric: 17.7212 - val_loss: 17.7120 - val_MinusLogProbMetric: 17.7120 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 16/1000
2023-09-27 13:57:32.497 
Epoch 16/1000 
	 loss: 17.7080, MinusLogProbMetric: 17.7080, val_loss: 18.0857, val_MinusLogProbMetric: 18.0857

Epoch 16: val_loss did not improve from 17.71205
196/196 - 43s - loss: 17.7080 - MinusLogProbMetric: 17.7080 - val_loss: 18.0857 - val_MinusLogProbMetric: 18.0857 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 17/1000
2023-09-27 13:58:14.836 
Epoch 17/1000 
	 loss: 17.6492, MinusLogProbMetric: 17.6492, val_loss: 17.9037, val_MinusLogProbMetric: 17.9037

Epoch 17: val_loss did not improve from 17.71205
196/196 - 42s - loss: 17.6492 - MinusLogProbMetric: 17.6492 - val_loss: 17.9037 - val_MinusLogProbMetric: 17.9037 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 18/1000
2023-09-27 13:58:57.037 
Epoch 18/1000 
	 loss: 17.5430, MinusLogProbMetric: 17.5430, val_loss: 17.8197, val_MinusLogProbMetric: 17.8197

Epoch 18: val_loss did not improve from 17.71205
196/196 - 42s - loss: 17.5430 - MinusLogProbMetric: 17.5430 - val_loss: 17.8197 - val_MinusLogProbMetric: 17.8197 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 19/1000
2023-09-27 13:59:39.094 
Epoch 19/1000 
	 loss: 17.5076, MinusLogProbMetric: 17.5076, val_loss: 17.7507, val_MinusLogProbMetric: 17.7507

Epoch 19: val_loss did not improve from 17.71205
196/196 - 42s - loss: 17.5076 - MinusLogProbMetric: 17.5076 - val_loss: 17.7507 - val_MinusLogProbMetric: 17.7507 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 20/1000
2023-09-27 14:00:21.154 
Epoch 20/1000 
	 loss: 17.4545, MinusLogProbMetric: 17.4545, val_loss: 17.6910, val_MinusLogProbMetric: 17.6910

Epoch 20: val_loss improved from 17.71205 to 17.69099, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_308/weights/best_weights.h5
196/196 - 43s - loss: 17.4545 - MinusLogProbMetric: 17.4545 - val_loss: 17.6910 - val_MinusLogProbMetric: 17.6910 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 21/1000
2023-09-27 14:01:04.505 
Epoch 21/1000 
	 loss: 17.4210, MinusLogProbMetric: 17.4210, val_loss: 17.6032, val_MinusLogProbMetric: 17.6032

Epoch 21: val_loss improved from 17.69099 to 17.60316, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_308/weights/best_weights.h5
196/196 - 43s - loss: 17.4210 - MinusLogProbMetric: 17.4210 - val_loss: 17.6032 - val_MinusLogProbMetric: 17.6032 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 22/1000
2023-09-27 14:01:46.424 
Epoch 22/1000 
	 loss: 17.3549, MinusLogProbMetric: 17.3549, val_loss: 17.6176, val_MinusLogProbMetric: 17.6176

Epoch 22: val_loss did not improve from 17.60316
196/196 - 41s - loss: 17.3549 - MinusLogProbMetric: 17.3549 - val_loss: 17.6176 - val_MinusLogProbMetric: 17.6176 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 23/1000
2023-09-27 14:02:28.030 
Epoch 23/1000 
	 loss: 17.3621, MinusLogProbMetric: 17.3621, val_loss: 18.3183, val_MinusLogProbMetric: 18.3183

Epoch 23: val_loss did not improve from 17.60316
196/196 - 42s - loss: 17.3621 - MinusLogProbMetric: 17.3621 - val_loss: 18.3183 - val_MinusLogProbMetric: 18.3183 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 24/1000
2023-09-27 14:03:09.148 
Epoch 24/1000 
	 loss: 17.2859, MinusLogProbMetric: 17.2859, val_loss: 17.3781, val_MinusLogProbMetric: 17.3781

Epoch 24: val_loss improved from 17.60316 to 17.37807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_308/weights/best_weights.h5
196/196 - 42s - loss: 17.2859 - MinusLogProbMetric: 17.2859 - val_loss: 17.3781 - val_MinusLogProbMetric: 17.3781 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 25/1000
2023-09-27 14:03:51.375 
Epoch 25/1000 
	 loss: 17.3279, MinusLogProbMetric: 17.3279, val_loss: 17.8638, val_MinusLogProbMetric: 17.8638

Epoch 25: val_loss did not improve from 17.37807
196/196 - 42s - loss: 17.3279 - MinusLogProbMetric: 17.3279 - val_loss: 17.8638 - val_MinusLogProbMetric: 17.8638 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 26/1000
2023-09-27 14:04:32.739 
Epoch 26/1000 
	 loss: 17.2582, MinusLogProbMetric: 17.2582, val_loss: 17.9030, val_MinusLogProbMetric: 17.9030

Epoch 26: val_loss did not improve from 17.37807
196/196 - 41s - loss: 17.2582 - MinusLogProbMetric: 17.2582 - val_loss: 17.9030 - val_MinusLogProbMetric: 17.9030 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 27/1000
2023-09-27 14:05:13.958 
Epoch 27/1000 
	 loss: 17.2710, MinusLogProbMetric: 17.2710, val_loss: 17.5664, val_MinusLogProbMetric: 17.5664

Epoch 27: val_loss did not improve from 17.37807
196/196 - 41s - loss: 17.2710 - MinusLogProbMetric: 17.2710 - val_loss: 17.5664 - val_MinusLogProbMetric: 17.5664 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 28/1000
2023-09-27 14:05:53.966 
Epoch 28/1000 
	 loss: 17.2084, MinusLogProbMetric: 17.2084, val_loss: 17.5176, val_MinusLogProbMetric: 17.5176

Epoch 28: val_loss did not improve from 17.37807
196/196 - 40s - loss: 17.2084 - MinusLogProbMetric: 17.2084 - val_loss: 17.5176 - val_MinusLogProbMetric: 17.5176 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 29/1000
2023-09-27 14:06:33.773 
Epoch 29/1000 
	 loss: 17.1749, MinusLogProbMetric: 17.1749, val_loss: 17.4402, val_MinusLogProbMetric: 17.4402

Epoch 29: val_loss did not improve from 17.37807
196/196 - 40s - loss: 17.1749 - MinusLogProbMetric: 17.1749 - val_loss: 17.4402 - val_MinusLogProbMetric: 17.4402 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 30/1000
2023-09-27 14:07:13.550 
Epoch 30/1000 
	 loss: 17.2209, MinusLogProbMetric: 17.2209, val_loss: 17.2726, val_MinusLogProbMetric: 17.2726

Epoch 30: val_loss improved from 17.37807 to 17.27255, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_308/weights/best_weights.h5
196/196 - 40s - loss: 17.2209 - MinusLogProbMetric: 17.2209 - val_loss: 17.2726 - val_MinusLogProbMetric: 17.2726 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 31/1000
2023-09-27 14:07:54.334 
Epoch 31/1000 
	 loss: 17.1373, MinusLogProbMetric: 17.1373, val_loss: 17.4520, val_MinusLogProbMetric: 17.4520

Epoch 31: val_loss did not improve from 17.27255
196/196 - 40s - loss: 17.1373 - MinusLogProbMetric: 17.1373 - val_loss: 17.4520 - val_MinusLogProbMetric: 17.4520 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 32/1000
2023-09-27 14:08:34.255 
Epoch 32/1000 
	 loss: 17.1095, MinusLogProbMetric: 17.1095, val_loss: 17.4197, val_MinusLogProbMetric: 17.4197

Epoch 32: val_loss did not improve from 17.27255
196/196 - 40s - loss: 17.1095 - MinusLogProbMetric: 17.1095 - val_loss: 17.4197 - val_MinusLogProbMetric: 17.4197 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 33/1000
2023-09-27 14:09:13.642 
Epoch 33/1000 
	 loss: 17.0867, MinusLogProbMetric: 17.0867, val_loss: 17.4019, val_MinusLogProbMetric: 17.4019

Epoch 33: val_loss did not improve from 17.27255
196/196 - 39s - loss: 17.0867 - MinusLogProbMetric: 17.0867 - val_loss: 17.4019 - val_MinusLogProbMetric: 17.4019 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 34/1000
2023-09-27 14:09:53.191 
Epoch 34/1000 
	 loss: 17.0423, MinusLogProbMetric: 17.0423, val_loss: 17.5693, val_MinusLogProbMetric: 17.5693

Epoch 34: val_loss did not improve from 17.27255
196/196 - 40s - loss: 17.0423 - MinusLogProbMetric: 17.0423 - val_loss: 17.5693 - val_MinusLogProbMetric: 17.5693 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 35/1000
2023-09-27 14:10:32.807 
Epoch 35/1000 
	 loss: 17.0507, MinusLogProbMetric: 17.0507, val_loss: 17.4126, val_MinusLogProbMetric: 17.4126

Epoch 35: val_loss did not improve from 17.27255
196/196 - 40s - loss: 17.0507 - MinusLogProbMetric: 17.0507 - val_loss: 17.4126 - val_MinusLogProbMetric: 17.4126 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 36/1000
2023-09-27 14:11:12.522 
Epoch 36/1000 
	 loss: 17.0341, MinusLogProbMetric: 17.0341, val_loss: 17.3497, val_MinusLogProbMetric: 17.3497

Epoch 36: val_loss did not improve from 17.27255
196/196 - 40s - loss: 17.0341 - MinusLogProbMetric: 17.0341 - val_loss: 17.3497 - val_MinusLogProbMetric: 17.3497 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 37/1000
2023-09-27 14:11:52.217 
Epoch 37/1000 
	 loss: 17.0086, MinusLogProbMetric: 17.0086, val_loss: 17.2549, val_MinusLogProbMetric: 17.2549

Epoch 37: val_loss improved from 17.27255 to 17.25485, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_308/weights/best_weights.h5
196/196 - 40s - loss: 17.0086 - MinusLogProbMetric: 17.0086 - val_loss: 17.2549 - val_MinusLogProbMetric: 17.2549 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 38/1000
2023-09-27 14:12:32.087 
Epoch 38/1000 
	 loss: 16.9981, MinusLogProbMetric: 16.9981, val_loss: 17.9630, val_MinusLogProbMetric: 17.9630

Epoch 38: val_loss did not improve from 17.25485
196/196 - 39s - loss: 16.9981 - MinusLogProbMetric: 16.9981 - val_loss: 17.9630 - val_MinusLogProbMetric: 17.9630 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 39/1000
2023-09-27 14:13:11.530 
Epoch 39/1000 
	 loss: 17.0055, MinusLogProbMetric: 17.0055, val_loss: 17.5282, val_MinusLogProbMetric: 17.5282

Epoch 39: val_loss did not improve from 17.25485
196/196 - 39s - loss: 17.0055 - MinusLogProbMetric: 17.0055 - val_loss: 17.5282 - val_MinusLogProbMetric: 17.5282 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 40/1000
2023-09-27 14:13:51.046 
Epoch 40/1000 
	 loss: 16.9363, MinusLogProbMetric: 16.9363, val_loss: 17.2585, val_MinusLogProbMetric: 17.2585

Epoch 40: val_loss did not improve from 17.25485
196/196 - 40s - loss: 16.9363 - MinusLogProbMetric: 16.9363 - val_loss: 17.2585 - val_MinusLogProbMetric: 17.2585 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 41/1000
2023-09-27 14:14:30.504 
Epoch 41/1000 
	 loss: 16.9241, MinusLogProbMetric: 16.9241, val_loss: 17.4139, val_MinusLogProbMetric: 17.4139

Epoch 41: val_loss did not improve from 17.25485
196/196 - 39s - loss: 16.9241 - MinusLogProbMetric: 16.9241 - val_loss: 17.4139 - val_MinusLogProbMetric: 17.4139 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 42/1000
2023-09-27 14:15:10.394 
Epoch 42/1000 
	 loss: 16.9180, MinusLogProbMetric: 16.9180, val_loss: 17.3160, val_MinusLogProbMetric: 17.3160

Epoch 42: val_loss did not improve from 17.25485
196/196 - 40s - loss: 16.9180 - MinusLogProbMetric: 16.9180 - val_loss: 17.3160 - val_MinusLogProbMetric: 17.3160 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 43/1000
2023-09-27 14:15:50.091 
Epoch 43/1000 
	 loss: 16.9276, MinusLogProbMetric: 16.9276, val_loss: 17.4123, val_MinusLogProbMetric: 17.4123

Epoch 43: val_loss did not improve from 17.25485
196/196 - 40s - loss: 16.9276 - MinusLogProbMetric: 16.9276 - val_loss: 17.4123 - val_MinusLogProbMetric: 17.4123 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 44/1000
2023-09-27 14:16:29.202 
Epoch 44/1000 
	 loss: 16.8792, MinusLogProbMetric: 16.8792, val_loss: 17.2681, val_MinusLogProbMetric: 17.2681

Epoch 44: val_loss did not improve from 17.25485
196/196 - 39s - loss: 16.8792 - MinusLogProbMetric: 16.8792 - val_loss: 17.2681 - val_MinusLogProbMetric: 17.2681 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 45/1000
2023-09-27 14:17:08.793 
Epoch 45/1000 
	 loss: 16.8652, MinusLogProbMetric: 16.8652, val_loss: 17.5281, val_MinusLogProbMetric: 17.5281

Epoch 45: val_loss did not improve from 17.25485
196/196 - 40s - loss: 16.8652 - MinusLogProbMetric: 16.8652 - val_loss: 17.5281 - val_MinusLogProbMetric: 17.5281 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 46/1000
2023-09-27 14:17:48.115 
Epoch 46/1000 
	 loss: 16.8886, MinusLogProbMetric: 16.8886, val_loss: 17.3640, val_MinusLogProbMetric: 17.3640

Epoch 46: val_loss did not improve from 17.25485
196/196 - 39s - loss: 16.8886 - MinusLogProbMetric: 16.8886 - val_loss: 17.3640 - val_MinusLogProbMetric: 17.3640 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 47/1000
2023-09-27 14:18:27.954 
Epoch 47/1000 
	 loss: 16.8636, MinusLogProbMetric: 16.8636, val_loss: 17.8178, val_MinusLogProbMetric: 17.8178

Epoch 47: val_loss did not improve from 17.25485
196/196 - 40s - loss: 16.8636 - MinusLogProbMetric: 16.8636 - val_loss: 17.8178 - val_MinusLogProbMetric: 17.8178 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 48/1000
2023-09-27 14:19:07.042 
Epoch 48/1000 
	 loss: 16.8737, MinusLogProbMetric: 16.8737, val_loss: 17.2769, val_MinusLogProbMetric: 17.2769

Epoch 48: val_loss did not improve from 17.25485
196/196 - 39s - loss: 16.8737 - MinusLogProbMetric: 16.8737 - val_loss: 17.2769 - val_MinusLogProbMetric: 17.2769 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 49/1000
2023-09-27 14:19:46.754 
Epoch 49/1000 
	 loss: 16.8120, MinusLogProbMetric: 16.8120, val_loss: 17.3029, val_MinusLogProbMetric: 17.3029

Epoch 49: val_loss did not improve from 17.25485
196/196 - 40s - loss: 16.8120 - MinusLogProbMetric: 16.8120 - val_loss: 17.3029 - val_MinusLogProbMetric: 17.3029 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 50/1000
2023-09-27 14:20:25.847 
Epoch 50/1000 
	 loss: 16.8530, MinusLogProbMetric: 16.8530, val_loss: 17.3471, val_MinusLogProbMetric: 17.3471

Epoch 50: val_loss did not improve from 17.25485
196/196 - 39s - loss: 16.8530 - MinusLogProbMetric: 16.8530 - val_loss: 17.3471 - val_MinusLogProbMetric: 17.3471 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 51/1000
2023-09-27 14:21:05.638 
Epoch 51/1000 
	 loss: 16.8103, MinusLogProbMetric: 16.8103, val_loss: 17.8894, val_MinusLogProbMetric: 17.8894

Epoch 51: val_loss did not improve from 17.25485
196/196 - 40s - loss: 16.8103 - MinusLogProbMetric: 16.8103 - val_loss: 17.8894 - val_MinusLogProbMetric: 17.8894 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 52/1000
2023-09-27 14:21:44.707 
Epoch 52/1000 
	 loss: 16.8357, MinusLogProbMetric: 16.8357, val_loss: 17.2833, val_MinusLogProbMetric: 17.2833

Epoch 52: val_loss did not improve from 17.25485
196/196 - 39s - loss: 16.8357 - MinusLogProbMetric: 16.8357 - val_loss: 17.2833 - val_MinusLogProbMetric: 17.2833 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 53/1000
2023-09-27 14:22:24.027 
Epoch 53/1000 
	 loss: 16.7839, MinusLogProbMetric: 16.7839, val_loss: 17.3659, val_MinusLogProbMetric: 17.3659

Epoch 53: val_loss did not improve from 17.25485
196/196 - 39s - loss: 16.7839 - MinusLogProbMetric: 16.7839 - val_loss: 17.3659 - val_MinusLogProbMetric: 17.3659 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 54/1000
2023-09-27 14:23:03.352 
Epoch 54/1000 
	 loss: 16.7954, MinusLogProbMetric: 16.7954, val_loss: 17.3193, val_MinusLogProbMetric: 17.3193

Epoch 54: val_loss did not improve from 17.25485
196/196 - 39s - loss: 16.7954 - MinusLogProbMetric: 16.7954 - val_loss: 17.3193 - val_MinusLogProbMetric: 17.3193 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 55/1000
2023-09-27 14:23:43.077 
Epoch 55/1000 
	 loss: 16.7640, MinusLogProbMetric: 16.7640, val_loss: 17.3565, val_MinusLogProbMetric: 17.3565

Epoch 55: val_loss did not improve from 17.25485
196/196 - 40s - loss: 16.7640 - MinusLogProbMetric: 16.7640 - val_loss: 17.3565 - val_MinusLogProbMetric: 17.3565 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 56/1000
2023-09-27 14:24:22.496 
Epoch 56/1000 
	 loss: 16.7670, MinusLogProbMetric: 16.7670, val_loss: 17.5754, val_MinusLogProbMetric: 17.5754

Epoch 56: val_loss did not improve from 17.25485
196/196 - 39s - loss: 16.7670 - MinusLogProbMetric: 16.7670 - val_loss: 17.5754 - val_MinusLogProbMetric: 17.5754 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 57/1000
2023-09-27 14:25:02.272 
Epoch 57/1000 
	 loss: 16.7154, MinusLogProbMetric: 16.7154, val_loss: 17.3091, val_MinusLogProbMetric: 17.3091

Epoch 57: val_loss did not improve from 17.25485
196/196 - 40s - loss: 16.7154 - MinusLogProbMetric: 16.7154 - val_loss: 17.3091 - val_MinusLogProbMetric: 17.3091 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 58/1000
2023-09-27 14:25:41.575 
Epoch 58/1000 
	 loss: 16.7634, MinusLogProbMetric: 16.7634, val_loss: 17.4230, val_MinusLogProbMetric: 17.4230

Epoch 58: val_loss did not improve from 17.25485
196/196 - 39s - loss: 16.7634 - MinusLogProbMetric: 16.7634 - val_loss: 17.4230 - val_MinusLogProbMetric: 17.4230 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 59/1000
2023-09-27 14:26:21.315 
Epoch 59/1000 
	 loss: 16.6973, MinusLogProbMetric: 16.6973, val_loss: 17.5313, val_MinusLogProbMetric: 17.5313

Epoch 59: val_loss did not improve from 17.25485
196/196 - 40s - loss: 16.6973 - MinusLogProbMetric: 16.6973 - val_loss: 17.5313 - val_MinusLogProbMetric: 17.5313 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 60/1000
2023-09-27 14:27:00.911 
Epoch 60/1000 
	 loss: 16.7249, MinusLogProbMetric: 16.7249, val_loss: 17.3058, val_MinusLogProbMetric: 17.3058

Epoch 60: val_loss did not improve from 17.25485
196/196 - 40s - loss: 16.7249 - MinusLogProbMetric: 16.7249 - val_loss: 17.3058 - val_MinusLogProbMetric: 17.3058 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 61/1000
2023-09-27 14:27:40.229 
Epoch 61/1000 
	 loss: 16.6950, MinusLogProbMetric: 16.6950, val_loss: 17.2310, val_MinusLogProbMetric: 17.2310

Epoch 61: val_loss improved from 17.25485 to 17.23100, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_308/weights/best_weights.h5
196/196 - 40s - loss: 16.6950 - MinusLogProbMetric: 16.6950 - val_loss: 17.2310 - val_MinusLogProbMetric: 17.2310 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 62/1000
2023-09-27 14:28:20.053 
Epoch 62/1000 
	 loss: 16.6445, MinusLogProbMetric: 16.6445, val_loss: 17.3276, val_MinusLogProbMetric: 17.3276

Epoch 62: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.6445 - MinusLogProbMetric: 16.6445 - val_loss: 17.3276 - val_MinusLogProbMetric: 17.3276 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 63/1000
2023-09-27 14:28:59.611 
Epoch 63/1000 
	 loss: 16.6739, MinusLogProbMetric: 16.6739, val_loss: 17.3368, val_MinusLogProbMetric: 17.3368

Epoch 63: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.6739 - MinusLogProbMetric: 16.6739 - val_loss: 17.3368 - val_MinusLogProbMetric: 17.3368 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 64/1000
2023-09-27 14:29:39.274 
Epoch 64/1000 
	 loss: 16.6343, MinusLogProbMetric: 16.6343, val_loss: 17.3359, val_MinusLogProbMetric: 17.3359

Epoch 64: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.6343 - MinusLogProbMetric: 16.6343 - val_loss: 17.3359 - val_MinusLogProbMetric: 17.3359 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 65/1000
2023-09-27 14:30:18.956 
Epoch 65/1000 
	 loss: 16.6526, MinusLogProbMetric: 16.6526, val_loss: 17.7017, val_MinusLogProbMetric: 17.7017

Epoch 65: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.6526 - MinusLogProbMetric: 16.6526 - val_loss: 17.7017 - val_MinusLogProbMetric: 17.7017 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 66/1000
2023-09-27 14:30:58.528 
Epoch 66/1000 
	 loss: 16.6383, MinusLogProbMetric: 16.6383, val_loss: 17.2691, val_MinusLogProbMetric: 17.2691

Epoch 66: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.6383 - MinusLogProbMetric: 16.6383 - val_loss: 17.2691 - val_MinusLogProbMetric: 17.2691 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 67/1000
2023-09-27 14:31:37.855 
Epoch 67/1000 
	 loss: 16.6412, MinusLogProbMetric: 16.6412, val_loss: 17.4966, val_MinusLogProbMetric: 17.4966

Epoch 67: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.6412 - MinusLogProbMetric: 16.6412 - val_loss: 17.4966 - val_MinusLogProbMetric: 17.4966 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 68/1000
2023-09-27 14:32:17.774 
Epoch 68/1000 
	 loss: 16.5840, MinusLogProbMetric: 16.5840, val_loss: 18.6628, val_MinusLogProbMetric: 18.6628

Epoch 68: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.5840 - MinusLogProbMetric: 16.5840 - val_loss: 18.6628 - val_MinusLogProbMetric: 18.6628 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 69/1000
2023-09-27 14:32:57.360 
Epoch 69/1000 
	 loss: 16.6297, MinusLogProbMetric: 16.6297, val_loss: 17.5247, val_MinusLogProbMetric: 17.5247

Epoch 69: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.6297 - MinusLogProbMetric: 16.6297 - val_loss: 17.5247 - val_MinusLogProbMetric: 17.5247 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 70/1000
2023-09-27 14:33:36.931 
Epoch 70/1000 
	 loss: 16.6043, MinusLogProbMetric: 16.6043, val_loss: 17.2669, val_MinusLogProbMetric: 17.2669

Epoch 70: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.6043 - MinusLogProbMetric: 16.6043 - val_loss: 17.2669 - val_MinusLogProbMetric: 17.2669 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 71/1000
2023-09-27 14:34:16.603 
Epoch 71/1000 
	 loss: 16.5485, MinusLogProbMetric: 16.5485, val_loss: 17.3716, val_MinusLogProbMetric: 17.3716

Epoch 71: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.5485 - MinusLogProbMetric: 16.5485 - val_loss: 17.3716 - val_MinusLogProbMetric: 17.3716 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 72/1000
2023-09-27 14:34:56.225 
Epoch 72/1000 
	 loss: 16.5983, MinusLogProbMetric: 16.5983, val_loss: 17.4482, val_MinusLogProbMetric: 17.4482

Epoch 72: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.5983 - MinusLogProbMetric: 16.5983 - val_loss: 17.4482 - val_MinusLogProbMetric: 17.4482 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 73/1000
2023-09-27 14:35:35.868 
Epoch 73/1000 
	 loss: 16.5515, MinusLogProbMetric: 16.5515, val_loss: 17.3956, val_MinusLogProbMetric: 17.3956

Epoch 73: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.5515 - MinusLogProbMetric: 16.5515 - val_loss: 17.3956 - val_MinusLogProbMetric: 17.3956 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 74/1000
2023-09-27 14:36:15.439 
Epoch 74/1000 
	 loss: 16.5301, MinusLogProbMetric: 16.5301, val_loss: 17.7025, val_MinusLogProbMetric: 17.7025

Epoch 74: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.5301 - MinusLogProbMetric: 16.5301 - val_loss: 17.7025 - val_MinusLogProbMetric: 17.7025 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 75/1000
2023-09-27 14:36:55.282 
Epoch 75/1000 
	 loss: 16.5501, MinusLogProbMetric: 16.5501, val_loss: 17.3055, val_MinusLogProbMetric: 17.3055

Epoch 75: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.5501 - MinusLogProbMetric: 16.5501 - val_loss: 17.3055 - val_MinusLogProbMetric: 17.3055 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 76/1000
2023-09-27 14:37:34.827 
Epoch 76/1000 
	 loss: 16.4978, MinusLogProbMetric: 16.4978, val_loss: 17.3342, val_MinusLogProbMetric: 17.3342

Epoch 76: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.4978 - MinusLogProbMetric: 16.4978 - val_loss: 17.3342 - val_MinusLogProbMetric: 17.3342 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 77/1000
2023-09-27 14:38:14.687 
Epoch 77/1000 
	 loss: 16.5130, MinusLogProbMetric: 16.5130, val_loss: 17.3831, val_MinusLogProbMetric: 17.3831

Epoch 77: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.5130 - MinusLogProbMetric: 16.5130 - val_loss: 17.3831 - val_MinusLogProbMetric: 17.3831 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 78/1000
2023-09-27 14:38:54.128 
Epoch 78/1000 
	 loss: 16.4991, MinusLogProbMetric: 16.4991, val_loss: 17.3697, val_MinusLogProbMetric: 17.3697

Epoch 78: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.4991 - MinusLogProbMetric: 16.4991 - val_loss: 17.3697 - val_MinusLogProbMetric: 17.3697 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 79/1000
2023-09-27 14:39:34.273 
Epoch 79/1000 
	 loss: 16.5071, MinusLogProbMetric: 16.5071, val_loss: 17.4967, val_MinusLogProbMetric: 17.4967

Epoch 79: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.5071 - MinusLogProbMetric: 16.5071 - val_loss: 17.4967 - val_MinusLogProbMetric: 17.4967 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 80/1000
2023-09-27 14:40:13.594 
Epoch 80/1000 
	 loss: 16.4657, MinusLogProbMetric: 16.4657, val_loss: 17.2704, val_MinusLogProbMetric: 17.2704

Epoch 80: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.4657 - MinusLogProbMetric: 16.4657 - val_loss: 17.2704 - val_MinusLogProbMetric: 17.2704 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 81/1000
2023-09-27 14:40:53.130 
Epoch 81/1000 
	 loss: 16.4953, MinusLogProbMetric: 16.4953, val_loss: 17.3858, val_MinusLogProbMetric: 17.3858

Epoch 81: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.4953 - MinusLogProbMetric: 16.4953 - val_loss: 17.3858 - val_MinusLogProbMetric: 17.3858 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 82/1000
2023-09-27 14:41:32.631 
Epoch 82/1000 
	 loss: 16.4756, MinusLogProbMetric: 16.4756, val_loss: 17.5657, val_MinusLogProbMetric: 17.5657

Epoch 82: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.4756 - MinusLogProbMetric: 16.4756 - val_loss: 17.5657 - val_MinusLogProbMetric: 17.5657 - lr: 0.0010 - 39s/epoch - 202ms/step
Epoch 83/1000
2023-09-27 14:42:12.131 
Epoch 83/1000 
	 loss: 16.4638, MinusLogProbMetric: 16.4638, val_loss: 17.5745, val_MinusLogProbMetric: 17.5745

Epoch 83: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.4638 - MinusLogProbMetric: 16.4638 - val_loss: 17.5745 - val_MinusLogProbMetric: 17.5745 - lr: 0.0010 - 39s/epoch - 202ms/step
Epoch 84/1000
2023-09-27 14:42:51.995 
Epoch 84/1000 
	 loss: 16.4284, MinusLogProbMetric: 16.4284, val_loss: 17.4064, val_MinusLogProbMetric: 17.4064

Epoch 84: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.4284 - MinusLogProbMetric: 16.4284 - val_loss: 17.4064 - val_MinusLogProbMetric: 17.4064 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 85/1000
2023-09-27 14:43:31.591 
Epoch 85/1000 
	 loss: 16.4456, MinusLogProbMetric: 16.4456, val_loss: 17.4264, val_MinusLogProbMetric: 17.4264

Epoch 85: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.4456 - MinusLogProbMetric: 16.4456 - val_loss: 17.4264 - val_MinusLogProbMetric: 17.4264 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 86/1000
2023-09-27 14:44:11.101 
Epoch 86/1000 
	 loss: 16.4439, MinusLogProbMetric: 16.4439, val_loss: 17.4530, val_MinusLogProbMetric: 17.4530

Epoch 86: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.4439 - MinusLogProbMetric: 16.4439 - val_loss: 17.4530 - val_MinusLogProbMetric: 17.4530 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 87/1000
2023-09-27 14:44:50.674 
Epoch 87/1000 
	 loss: 16.4012, MinusLogProbMetric: 16.4012, val_loss: 17.4250, val_MinusLogProbMetric: 17.4250

Epoch 87: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.4012 - MinusLogProbMetric: 16.4012 - val_loss: 17.4250 - val_MinusLogProbMetric: 17.4250 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 88/1000
2023-09-27 14:45:30.423 
Epoch 88/1000 
	 loss: 16.4465, MinusLogProbMetric: 16.4465, val_loss: 17.5768, val_MinusLogProbMetric: 17.5768

Epoch 88: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.4465 - MinusLogProbMetric: 16.4465 - val_loss: 17.5768 - val_MinusLogProbMetric: 17.5768 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 89/1000
2023-09-27 14:46:09.523 
Epoch 89/1000 
	 loss: 16.3786, MinusLogProbMetric: 16.3786, val_loss: 17.3762, val_MinusLogProbMetric: 17.3762

Epoch 89: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.3786 - MinusLogProbMetric: 16.3786 - val_loss: 17.3762 - val_MinusLogProbMetric: 17.3762 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 90/1000
2023-09-27 14:46:48.790 
Epoch 90/1000 
	 loss: 16.3804, MinusLogProbMetric: 16.3804, val_loss: 17.4966, val_MinusLogProbMetric: 17.4966

Epoch 90: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.3804 - MinusLogProbMetric: 16.3804 - val_loss: 17.4966 - val_MinusLogProbMetric: 17.4966 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 91/1000
2023-09-27 14:47:28.258 
Epoch 91/1000 
	 loss: 16.3951, MinusLogProbMetric: 16.3951, val_loss: 17.4240, val_MinusLogProbMetric: 17.4240

Epoch 91: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.3951 - MinusLogProbMetric: 16.3951 - val_loss: 17.4240 - val_MinusLogProbMetric: 17.4240 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 92/1000
2023-09-27 14:48:07.135 
Epoch 92/1000 
	 loss: 16.3607, MinusLogProbMetric: 16.3607, val_loss: 17.8453, val_MinusLogProbMetric: 17.8453

Epoch 92: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.3607 - MinusLogProbMetric: 16.3607 - val_loss: 17.8453 - val_MinusLogProbMetric: 17.8453 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 93/1000
2023-09-27 14:48:46.364 
Epoch 93/1000 
	 loss: 16.3873, MinusLogProbMetric: 16.3873, val_loss: 17.5454, val_MinusLogProbMetric: 17.5454

Epoch 93: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.3873 - MinusLogProbMetric: 16.3873 - val_loss: 17.5454 - val_MinusLogProbMetric: 17.5454 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 94/1000
2023-09-27 14:49:25.661 
Epoch 94/1000 
	 loss: 16.3491, MinusLogProbMetric: 16.3491, val_loss: 17.5505, val_MinusLogProbMetric: 17.5505

Epoch 94: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.3491 - MinusLogProbMetric: 16.3491 - val_loss: 17.5505 - val_MinusLogProbMetric: 17.5505 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 95/1000
2023-09-27 14:50:05.189 
Epoch 95/1000 
	 loss: 16.3510, MinusLogProbMetric: 16.3510, val_loss: 17.4696, val_MinusLogProbMetric: 17.4696

Epoch 95: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.3510 - MinusLogProbMetric: 16.3510 - val_loss: 17.4696 - val_MinusLogProbMetric: 17.4696 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 96/1000
2023-09-27 14:50:44.585 
Epoch 96/1000 
	 loss: 16.3436, MinusLogProbMetric: 16.3436, val_loss: 17.5875, val_MinusLogProbMetric: 17.5875

Epoch 96: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.3436 - MinusLogProbMetric: 16.3436 - val_loss: 17.5875 - val_MinusLogProbMetric: 17.5875 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 97/1000
2023-09-27 14:51:24.017 
Epoch 97/1000 
	 loss: 16.3276, MinusLogProbMetric: 16.3276, val_loss: 17.5663, val_MinusLogProbMetric: 17.5663

Epoch 97: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.3276 - MinusLogProbMetric: 16.3276 - val_loss: 17.5663 - val_MinusLogProbMetric: 17.5663 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 98/1000
2023-09-27 14:52:03.671 
Epoch 98/1000 
	 loss: 16.3105, MinusLogProbMetric: 16.3105, val_loss: 17.5119, val_MinusLogProbMetric: 17.5119

Epoch 98: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.3105 - MinusLogProbMetric: 16.3105 - val_loss: 17.5119 - val_MinusLogProbMetric: 17.5119 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 99/1000
2023-09-27 14:52:43.190 
Epoch 99/1000 
	 loss: 16.2976, MinusLogProbMetric: 16.2976, val_loss: 17.6550, val_MinusLogProbMetric: 17.6550

Epoch 99: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.2976 - MinusLogProbMetric: 16.2976 - val_loss: 17.6550 - val_MinusLogProbMetric: 17.6550 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 100/1000
2023-09-27 14:53:22.750 
Epoch 100/1000 
	 loss: 16.2973, MinusLogProbMetric: 16.2973, val_loss: 17.5780, val_MinusLogProbMetric: 17.5780

Epoch 100: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.2973 - MinusLogProbMetric: 16.2973 - val_loss: 17.5780 - val_MinusLogProbMetric: 17.5780 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 101/1000
2023-09-27 14:54:02.348 
Epoch 101/1000 
	 loss: 16.2908, MinusLogProbMetric: 16.2908, val_loss: 17.4328, val_MinusLogProbMetric: 17.4328

Epoch 101: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.2908 - MinusLogProbMetric: 16.2908 - val_loss: 17.4328 - val_MinusLogProbMetric: 17.4328 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 102/1000
2023-09-27 14:54:42.034 
Epoch 102/1000 
	 loss: 16.2786, MinusLogProbMetric: 16.2786, val_loss: 17.5235, val_MinusLogProbMetric: 17.5235

Epoch 102: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.2786 - MinusLogProbMetric: 16.2786 - val_loss: 17.5235 - val_MinusLogProbMetric: 17.5235 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 103/1000
2023-09-27 14:55:21.511 
Epoch 103/1000 
	 loss: 16.2882, MinusLogProbMetric: 16.2882, val_loss: 17.5923, val_MinusLogProbMetric: 17.5923

Epoch 103: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.2882 - MinusLogProbMetric: 16.2882 - val_loss: 17.5923 - val_MinusLogProbMetric: 17.5923 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 104/1000
2023-09-27 14:56:00.638 
Epoch 104/1000 
	 loss: 16.2682, MinusLogProbMetric: 16.2682, val_loss: 17.6787, val_MinusLogProbMetric: 17.6787

Epoch 104: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.2682 - MinusLogProbMetric: 16.2682 - val_loss: 17.6787 - val_MinusLogProbMetric: 17.6787 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 105/1000
2023-09-27 14:56:40.164 
Epoch 105/1000 
	 loss: 16.2584, MinusLogProbMetric: 16.2584, val_loss: 17.7916, val_MinusLogProbMetric: 17.7916

Epoch 105: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.2584 - MinusLogProbMetric: 16.2584 - val_loss: 17.7916 - val_MinusLogProbMetric: 17.7916 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 106/1000
2023-09-27 14:57:19.577 
Epoch 106/1000 
	 loss: 16.2373, MinusLogProbMetric: 16.2373, val_loss: 17.5837, val_MinusLogProbMetric: 17.5837

Epoch 106: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.2373 - MinusLogProbMetric: 16.2373 - val_loss: 17.5837 - val_MinusLogProbMetric: 17.5837 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 107/1000
2023-09-27 14:57:59.158 
Epoch 107/1000 
	 loss: 16.2640, MinusLogProbMetric: 16.2640, val_loss: 17.5984, val_MinusLogProbMetric: 17.5984

Epoch 107: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.2640 - MinusLogProbMetric: 16.2640 - val_loss: 17.5984 - val_MinusLogProbMetric: 17.5984 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 108/1000
2023-09-27 14:58:38.480 
Epoch 108/1000 
	 loss: 16.2176, MinusLogProbMetric: 16.2176, val_loss: 17.6957, val_MinusLogProbMetric: 17.6957

Epoch 108: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.2176 - MinusLogProbMetric: 16.2176 - val_loss: 17.6957 - val_MinusLogProbMetric: 17.6957 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 109/1000
2023-09-27 14:59:18.020 
Epoch 109/1000 
	 loss: 16.2297, MinusLogProbMetric: 16.2297, val_loss: 17.6677, val_MinusLogProbMetric: 17.6677

Epoch 109: val_loss did not improve from 17.23100
196/196 - 40s - loss: 16.2297 - MinusLogProbMetric: 16.2297 - val_loss: 17.6677 - val_MinusLogProbMetric: 17.6677 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 110/1000
2023-09-27 14:59:57.496 
Epoch 110/1000 
	 loss: 16.1920, MinusLogProbMetric: 16.1920, val_loss: 17.5642, val_MinusLogProbMetric: 17.5642

Epoch 110: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.1920 - MinusLogProbMetric: 16.1920 - val_loss: 17.5642 - val_MinusLogProbMetric: 17.5642 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 111/1000
2023-09-27 15:00:36.936 
Epoch 111/1000 
	 loss: 16.1784, MinusLogProbMetric: 16.1784, val_loss: 17.6388, val_MinusLogProbMetric: 17.6388

Epoch 111: val_loss did not improve from 17.23100
196/196 - 39s - loss: 16.1784 - MinusLogProbMetric: 16.1784 - val_loss: 17.6388 - val_MinusLogProbMetric: 17.6388 - lr: 0.0010 - 39s/epoch - 201ms/step
Epoch 112/1000
2023-09-27 15:01:15.914 
Epoch 112/1000 
	 loss: 15.9544, MinusLogProbMetric: 15.9544, val_loss: 17.5376, val_MinusLogProbMetric: 17.5376

Epoch 112: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.9544 - MinusLogProbMetric: 15.9544 - val_loss: 17.5376 - val_MinusLogProbMetric: 17.5376 - lr: 5.0000e-04 - 39s/epoch - 199ms/step
Epoch 113/1000
2023-09-27 15:01:55.579 
Epoch 113/1000 
	 loss: 15.9616, MinusLogProbMetric: 15.9616, val_loss: 17.5754, val_MinusLogProbMetric: 17.5754

Epoch 113: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.9616 - MinusLogProbMetric: 15.9616 - val_loss: 17.5754 - val_MinusLogProbMetric: 17.5754 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 114/1000
2023-09-27 15:02:35.022 
Epoch 114/1000 
	 loss: 15.9341, MinusLogProbMetric: 15.9341, val_loss: 17.5768, val_MinusLogProbMetric: 17.5768

Epoch 114: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.9341 - MinusLogProbMetric: 15.9341 - val_loss: 17.5768 - val_MinusLogProbMetric: 17.5768 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 115/1000
2023-09-27 15:03:14.436 
Epoch 115/1000 
	 loss: 15.9240, MinusLogProbMetric: 15.9240, val_loss: 17.6222, val_MinusLogProbMetric: 17.6222

Epoch 115: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.9240 - MinusLogProbMetric: 15.9240 - val_loss: 17.6222 - val_MinusLogProbMetric: 17.6222 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 116/1000
2023-09-27 15:03:53.586 
Epoch 116/1000 
	 loss: 15.9282, MinusLogProbMetric: 15.9282, val_loss: 17.5419, val_MinusLogProbMetric: 17.5419

Epoch 116: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.9282 - MinusLogProbMetric: 15.9282 - val_loss: 17.5419 - val_MinusLogProbMetric: 17.5419 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 117/1000
2023-09-27 15:04:32.672 
Epoch 117/1000 
	 loss: 15.9761, MinusLogProbMetric: 15.9761, val_loss: 17.5927, val_MinusLogProbMetric: 17.5927

Epoch 117: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.9761 - MinusLogProbMetric: 15.9761 - val_loss: 17.5927 - val_MinusLogProbMetric: 17.5927 - lr: 5.0000e-04 - 39s/epoch - 199ms/step
Epoch 118/1000
2023-09-27 15:05:12.373 
Epoch 118/1000 
	 loss: 15.9286, MinusLogProbMetric: 15.9286, val_loss: 17.6562, val_MinusLogProbMetric: 17.6562

Epoch 118: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.9286 - MinusLogProbMetric: 15.9286 - val_loss: 17.6562 - val_MinusLogProbMetric: 17.6562 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 119/1000
2023-09-27 15:05:51.607 
Epoch 119/1000 
	 loss: 15.8778, MinusLogProbMetric: 15.8778, val_loss: 17.8171, val_MinusLogProbMetric: 17.8171

Epoch 119: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.8778 - MinusLogProbMetric: 15.8778 - val_loss: 17.8171 - val_MinusLogProbMetric: 17.8171 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 120/1000
2023-09-27 15:06:30.939 
Epoch 120/1000 
	 loss: 15.8990, MinusLogProbMetric: 15.8990, val_loss: 17.5971, val_MinusLogProbMetric: 17.5971

Epoch 120: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.8990 - MinusLogProbMetric: 15.8990 - val_loss: 17.5971 - val_MinusLogProbMetric: 17.5971 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 121/1000
2023-09-27 15:07:10.364 
Epoch 121/1000 
	 loss: 15.9067, MinusLogProbMetric: 15.9067, val_loss: 17.6630, val_MinusLogProbMetric: 17.6630

Epoch 121: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.9067 - MinusLogProbMetric: 15.9067 - val_loss: 17.6630 - val_MinusLogProbMetric: 17.6630 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 122/1000
2023-09-27 15:07:50.171 
Epoch 122/1000 
	 loss: 15.8747, MinusLogProbMetric: 15.8747, val_loss: 17.6230, val_MinusLogProbMetric: 17.6230

Epoch 122: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.8747 - MinusLogProbMetric: 15.8747 - val_loss: 17.6230 - val_MinusLogProbMetric: 17.6230 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 123/1000
2023-09-27 15:08:29.664 
Epoch 123/1000 
	 loss: 15.8652, MinusLogProbMetric: 15.8652, val_loss: 17.7034, val_MinusLogProbMetric: 17.7034

Epoch 123: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.8652 - MinusLogProbMetric: 15.8652 - val_loss: 17.7034 - val_MinusLogProbMetric: 17.7034 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 124/1000
2023-09-27 15:09:09.262 
Epoch 124/1000 
	 loss: 15.8804, MinusLogProbMetric: 15.8804, val_loss: 17.6370, val_MinusLogProbMetric: 17.6370

Epoch 124: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.8804 - MinusLogProbMetric: 15.8804 - val_loss: 17.6370 - val_MinusLogProbMetric: 17.6370 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 125/1000
2023-09-27 15:09:48.790 
Epoch 125/1000 
	 loss: 15.8847, MinusLogProbMetric: 15.8847, val_loss: 17.9899, val_MinusLogProbMetric: 17.9899

Epoch 125: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.8847 - MinusLogProbMetric: 15.8847 - val_loss: 17.9899 - val_MinusLogProbMetric: 17.9899 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 126/1000
2023-09-27 15:10:28.127 
Epoch 126/1000 
	 loss: 15.8902, MinusLogProbMetric: 15.8902, val_loss: 17.6490, val_MinusLogProbMetric: 17.6490

Epoch 126: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.8902 - MinusLogProbMetric: 15.8902 - val_loss: 17.6490 - val_MinusLogProbMetric: 17.6490 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 127/1000
2023-09-27 15:11:07.516 
Epoch 127/1000 
	 loss: 15.8606, MinusLogProbMetric: 15.8606, val_loss: 17.6014, val_MinusLogProbMetric: 17.6014

Epoch 127: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.8606 - MinusLogProbMetric: 15.8606 - val_loss: 17.6014 - val_MinusLogProbMetric: 17.6014 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 128/1000
2023-09-27 15:11:46.948 
Epoch 128/1000 
	 loss: 15.8358, MinusLogProbMetric: 15.8358, val_loss: 17.7401, val_MinusLogProbMetric: 17.7401

Epoch 128: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.8358 - MinusLogProbMetric: 15.8358 - val_loss: 17.7401 - val_MinusLogProbMetric: 17.7401 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 129/1000
2023-09-27 15:12:26.120 
Epoch 129/1000 
	 loss: 15.8201, MinusLogProbMetric: 15.8201, val_loss: 17.9144, val_MinusLogProbMetric: 17.9144

Epoch 129: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.8201 - MinusLogProbMetric: 15.8201 - val_loss: 17.9144 - val_MinusLogProbMetric: 17.9144 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 130/1000
2023-09-27 15:13:05.652 
Epoch 130/1000 
	 loss: 15.8358, MinusLogProbMetric: 15.8358, val_loss: 17.7438, val_MinusLogProbMetric: 17.7438

Epoch 130: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.8358 - MinusLogProbMetric: 15.8358 - val_loss: 17.7438 - val_MinusLogProbMetric: 17.7438 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 131/1000
2023-09-27 15:13:45.046 
Epoch 131/1000 
	 loss: 15.8224, MinusLogProbMetric: 15.8224, val_loss: 17.8509, val_MinusLogProbMetric: 17.8509

Epoch 131: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.8224 - MinusLogProbMetric: 15.8224 - val_loss: 17.8509 - val_MinusLogProbMetric: 17.8509 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 132/1000
2023-09-27 15:14:24.771 
Epoch 132/1000 
	 loss: 15.8686, MinusLogProbMetric: 15.8686, val_loss: 17.8044, val_MinusLogProbMetric: 17.8044

Epoch 132: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.8686 - MinusLogProbMetric: 15.8686 - val_loss: 17.8044 - val_MinusLogProbMetric: 17.8044 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 133/1000
2023-09-27 15:15:03.928 
Epoch 133/1000 
	 loss: 15.8256, MinusLogProbMetric: 15.8256, val_loss: 17.8645, val_MinusLogProbMetric: 17.8645

Epoch 133: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.8256 - MinusLogProbMetric: 15.8256 - val_loss: 17.8645 - val_MinusLogProbMetric: 17.8645 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 134/1000
2023-09-27 15:15:43.547 
Epoch 134/1000 
	 loss: 15.8288, MinusLogProbMetric: 15.8288, val_loss: 17.7486, val_MinusLogProbMetric: 17.7486

Epoch 134: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.8288 - MinusLogProbMetric: 15.8288 - val_loss: 17.7486 - val_MinusLogProbMetric: 17.7486 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 135/1000
2023-09-27 15:16:22.967 
Epoch 135/1000 
	 loss: 15.7940, MinusLogProbMetric: 15.7940, val_loss: 17.7782, val_MinusLogProbMetric: 17.7782

Epoch 135: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.7940 - MinusLogProbMetric: 15.7940 - val_loss: 17.7782 - val_MinusLogProbMetric: 17.7782 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 136/1000
2023-09-27 15:17:02.229 
Epoch 136/1000 
	 loss: 15.8066, MinusLogProbMetric: 15.8066, val_loss: 17.8697, val_MinusLogProbMetric: 17.8697

Epoch 136: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.8066 - MinusLogProbMetric: 15.8066 - val_loss: 17.8697 - val_MinusLogProbMetric: 17.8697 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 137/1000
2023-09-27 15:17:42.612 
Epoch 137/1000 
	 loss: 15.7837, MinusLogProbMetric: 15.7837, val_loss: 17.9177, val_MinusLogProbMetric: 17.9177

Epoch 137: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.7837 - MinusLogProbMetric: 15.7837 - val_loss: 17.9177 - val_MinusLogProbMetric: 17.9177 - lr: 5.0000e-04 - 40s/epoch - 206ms/step
Epoch 138/1000
2023-09-27 15:18:22.563 
Epoch 138/1000 
	 loss: 15.7983, MinusLogProbMetric: 15.7983, val_loss: 17.9814, val_MinusLogProbMetric: 17.9814

Epoch 138: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.7983 - MinusLogProbMetric: 15.7983 - val_loss: 17.9814 - val_MinusLogProbMetric: 17.9814 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 139/1000
2023-09-27 15:19:02.611 
Epoch 139/1000 
	 loss: 15.7813, MinusLogProbMetric: 15.7813, val_loss: 17.7807, val_MinusLogProbMetric: 17.7807

Epoch 139: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.7813 - MinusLogProbMetric: 15.7813 - val_loss: 17.7807 - val_MinusLogProbMetric: 17.7807 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 140/1000
2023-09-27 15:19:42.462 
Epoch 140/1000 
	 loss: 15.7873, MinusLogProbMetric: 15.7873, val_loss: 17.7927, val_MinusLogProbMetric: 17.7927

Epoch 140: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.7873 - MinusLogProbMetric: 15.7873 - val_loss: 17.7927 - val_MinusLogProbMetric: 17.7927 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 141/1000
2023-09-27 15:20:21.842 
Epoch 141/1000 
	 loss: 15.7759, MinusLogProbMetric: 15.7759, val_loss: 17.8457, val_MinusLogProbMetric: 17.8457

Epoch 141: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.7759 - MinusLogProbMetric: 15.7759 - val_loss: 17.8457 - val_MinusLogProbMetric: 17.8457 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 142/1000
2023-09-27 15:21:03.109 
Epoch 142/1000 
	 loss: 15.7685, MinusLogProbMetric: 15.7685, val_loss: 17.8981, val_MinusLogProbMetric: 17.8981

Epoch 142: val_loss did not improve from 17.23100
196/196 - 41s - loss: 15.7685 - MinusLogProbMetric: 15.7685 - val_loss: 17.8981 - val_MinusLogProbMetric: 17.8981 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 143/1000
2023-09-27 15:21:42.902 
Epoch 143/1000 
	 loss: 15.7703, MinusLogProbMetric: 15.7703, val_loss: 17.8401, val_MinusLogProbMetric: 17.8401

Epoch 143: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.7703 - MinusLogProbMetric: 15.7703 - val_loss: 17.8401 - val_MinusLogProbMetric: 17.8401 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 144/1000
2023-09-27 15:22:22.736 
Epoch 144/1000 
	 loss: 15.7560, MinusLogProbMetric: 15.7560, val_loss: 17.8110, val_MinusLogProbMetric: 17.8110

Epoch 144: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.7560 - MinusLogProbMetric: 15.7560 - val_loss: 17.8110 - val_MinusLogProbMetric: 17.8110 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 145/1000
2023-09-27 15:23:02.529 
Epoch 145/1000 
	 loss: 15.7730, MinusLogProbMetric: 15.7730, val_loss: 17.7811, val_MinusLogProbMetric: 17.7811

Epoch 145: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.7730 - MinusLogProbMetric: 15.7730 - val_loss: 17.7811 - val_MinusLogProbMetric: 17.7811 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 146/1000
2023-09-27 15:23:42.556 
Epoch 146/1000 
	 loss: 15.7650, MinusLogProbMetric: 15.7650, val_loss: 17.8495, val_MinusLogProbMetric: 17.8495

Epoch 146: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.7650 - MinusLogProbMetric: 15.7650 - val_loss: 17.8495 - val_MinusLogProbMetric: 17.8495 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 147/1000
2023-09-27 15:24:21.978 
Epoch 147/1000 
	 loss: 15.7241, MinusLogProbMetric: 15.7241, val_loss: 17.9042, val_MinusLogProbMetric: 17.9042

Epoch 147: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.7241 - MinusLogProbMetric: 15.7241 - val_loss: 17.9042 - val_MinusLogProbMetric: 17.9042 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 148/1000
2023-09-27 15:25:01.856 
Epoch 148/1000 
	 loss: 15.7616, MinusLogProbMetric: 15.7616, val_loss: 17.8399, val_MinusLogProbMetric: 17.8399

Epoch 148: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.7616 - MinusLogProbMetric: 15.7616 - val_loss: 17.8399 - val_MinusLogProbMetric: 17.8399 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 149/1000
2023-09-27 15:25:41.683 
Epoch 149/1000 
	 loss: 15.7350, MinusLogProbMetric: 15.7350, val_loss: 17.9473, val_MinusLogProbMetric: 17.9473

Epoch 149: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.7350 - MinusLogProbMetric: 15.7350 - val_loss: 17.9473 - val_MinusLogProbMetric: 17.9473 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 150/1000
2023-09-27 15:26:21.306 
Epoch 150/1000 
	 loss: 15.7337, MinusLogProbMetric: 15.7337, val_loss: 18.2624, val_MinusLogProbMetric: 18.2624

Epoch 150: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.7337 - MinusLogProbMetric: 15.7337 - val_loss: 18.2624 - val_MinusLogProbMetric: 18.2624 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 151/1000
2023-09-27 15:27:00.795 
Epoch 151/1000 
	 loss: 15.7216, MinusLogProbMetric: 15.7216, val_loss: 17.9962, val_MinusLogProbMetric: 17.9962

Epoch 151: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.7216 - MinusLogProbMetric: 15.7216 - val_loss: 17.9962 - val_MinusLogProbMetric: 17.9962 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 152/1000
2023-09-27 15:27:40.252 
Epoch 152/1000 
	 loss: 15.7206, MinusLogProbMetric: 15.7206, val_loss: 17.9279, val_MinusLogProbMetric: 17.9279

Epoch 152: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.7206 - MinusLogProbMetric: 15.7206 - val_loss: 17.9279 - val_MinusLogProbMetric: 17.9279 - lr: 5.0000e-04 - 39s/epoch - 201ms/step
Epoch 153/1000
2023-09-27 15:28:19.822 
Epoch 153/1000 
	 loss: 15.7027, MinusLogProbMetric: 15.7027, val_loss: 18.0070, val_MinusLogProbMetric: 18.0070

Epoch 153: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.7027 - MinusLogProbMetric: 15.7027 - val_loss: 18.0070 - val_MinusLogProbMetric: 18.0070 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 154/1000
2023-09-27 15:28:58.995 
Epoch 154/1000 
	 loss: 15.7057, MinusLogProbMetric: 15.7057, val_loss: 18.0183, val_MinusLogProbMetric: 18.0183

Epoch 154: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.7057 - MinusLogProbMetric: 15.7057 - val_loss: 18.0183 - val_MinusLogProbMetric: 18.0183 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 155/1000
2023-09-27 15:29:38.650 
Epoch 155/1000 
	 loss: 15.7106, MinusLogProbMetric: 15.7106, val_loss: 18.0008, val_MinusLogProbMetric: 18.0008

Epoch 155: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.7106 - MinusLogProbMetric: 15.7106 - val_loss: 18.0008 - val_MinusLogProbMetric: 18.0008 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 156/1000
2023-09-27 15:30:18.419 
Epoch 156/1000 
	 loss: 15.7005, MinusLogProbMetric: 15.7005, val_loss: 17.9989, val_MinusLogProbMetric: 17.9989

Epoch 156: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.7005 - MinusLogProbMetric: 15.7005 - val_loss: 17.9989 - val_MinusLogProbMetric: 17.9989 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 157/1000
2023-09-27 15:30:58.698 
Epoch 157/1000 
	 loss: 15.6980, MinusLogProbMetric: 15.6980, val_loss: 17.9249, val_MinusLogProbMetric: 17.9249

Epoch 157: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.6980 - MinusLogProbMetric: 15.6980 - val_loss: 17.9249 - val_MinusLogProbMetric: 17.9249 - lr: 5.0000e-04 - 40s/epoch - 205ms/step
Epoch 158/1000
2023-09-27 15:31:37.945 
Epoch 158/1000 
	 loss: 15.6797, MinusLogProbMetric: 15.6797, val_loss: 18.0177, val_MinusLogProbMetric: 18.0177

Epoch 158: val_loss did not improve from 17.23100
196/196 - 39s - loss: 15.6797 - MinusLogProbMetric: 15.6797 - val_loss: 18.0177 - val_MinusLogProbMetric: 18.0177 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 159/1000
2023-09-27 15:32:17.536 
Epoch 159/1000 
	 loss: 15.7001, MinusLogProbMetric: 15.7001, val_loss: 18.0234, val_MinusLogProbMetric: 18.0234

Epoch 159: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.7001 - MinusLogProbMetric: 15.7001 - val_loss: 18.0234 - val_MinusLogProbMetric: 18.0234 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 160/1000
2023-09-27 15:32:57.481 
Epoch 160/1000 
	 loss: 15.6678, MinusLogProbMetric: 15.6678, val_loss: 17.9813, val_MinusLogProbMetric: 17.9813

Epoch 160: val_loss did not improve from 17.23100
196/196 - 40s - loss: 15.6678 - MinusLogProbMetric: 15.6678 - val_loss: 17.9813 - val_MinusLogProbMetric: 17.9813 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 161/1000
2023-09-27 15:33:37.086 
Epoch 161/1000 
	 loss: 15.6891, MinusLogProbMetric: 15.6891, val_loss: 18.1113, val_MinusLogProbMetric: 18.1113

Epoch 161: val_loss did not improve from 17.23100
Restoring model weights from the end of the best epoch: 61.
196/196 - 40s - loss: 15.6891 - MinusLogProbMetric: 15.6891 - val_loss: 18.1113 - val_MinusLogProbMetric: 18.1113 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 161: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 15.422387945000082 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 9.057405158004258 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 6.66811362199951 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 7.781501377990935 seconds.
Training succeeded with seed 926.
Model trained in 6515.90 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 40.21 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 40.55 s.
===========
Run 308/720 done in 6561.75 s.
===========

===========
Generating train data for run 309.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_309
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_44"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_45 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7fef6ce7fb20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fef6cf72b60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fef6cf72b60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff6345348b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fef6ceeeb30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fef6ceef0a0>, <keras.callbacks.ModelCheckpoint object at 0x7fef6ceef160>, <keras.callbacks.EarlyStopping object at 0x7fef6ceef3d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fef6ceef400>, <keras.callbacks.TerminateOnNaN object at 0x7fef6ceef040>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_309/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 309/720 with hyperparameters:
timestamp = 2023-09-27 15:34:26.977856
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 19: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-27 15:36:38.364 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2762.4502, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 131s - loss: nan - MinusLogProbMetric: 2762.4502 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 131s/epoch - 670ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0003333333333333333.
===========
Generating train data for run 309.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_309
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_55"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_56 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7fef465f1120>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fef4603ae60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fef4603ae60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feea058f550>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fef45c32590>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fef45c32b00>, <keras.callbacks.ModelCheckpoint object at 0x7fef45c32bc0>, <keras.callbacks.EarlyStopping object at 0x7fef45c32e30>, <keras.callbacks.ReduceLROnPlateau object at 0x7fef45c32e60>, <keras.callbacks.TerminateOnNaN object at 0x7fef45c32aa0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_309/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 309/720 with hyperparameters:
timestamp = 2023-09-27 15:36:48.670699
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
2023-09-27 15:39:54.981 
Epoch 1/1000 
	 loss: 1135.6724, MinusLogProbMetric: 1135.6724, val_loss: 595.3507, val_MinusLogProbMetric: 595.3507

Epoch 1: val_loss improved from inf to 595.35071, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 187s - loss: 1135.6724 - MinusLogProbMetric: 1135.6724 - val_loss: 595.3507 - val_MinusLogProbMetric: 595.3507 - lr: 3.3333e-04 - 187s/epoch - 953ms/step
Epoch 2/1000
2023-09-27 15:40:57.863 
Epoch 2/1000 
	 loss: 525.2835, MinusLogProbMetric: 525.2835, val_loss: 308.4513, val_MinusLogProbMetric: 308.4513

Epoch 2: val_loss improved from 595.35071 to 308.45126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 63s - loss: 525.2835 - MinusLogProbMetric: 525.2835 - val_loss: 308.4513 - val_MinusLogProbMetric: 308.4513 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 3/1000
2023-09-27 15:41:59.792 
Epoch 3/1000 
	 loss: 351.8255, MinusLogProbMetric: 351.8255, val_loss: 383.1688, val_MinusLogProbMetric: 383.1688

Epoch 3: val_loss did not improve from 308.45126
196/196 - 61s - loss: 351.8255 - MinusLogProbMetric: 351.8255 - val_loss: 383.1688 - val_MinusLogProbMetric: 383.1688 - lr: 3.3333e-04 - 61s/epoch - 311ms/step
Epoch 4/1000
2023-09-27 15:43:01.468 
Epoch 4/1000 
	 loss: 297.3154, MinusLogProbMetric: 297.3154, val_loss: 252.5159, val_MinusLogProbMetric: 252.5159

Epoch 4: val_loss improved from 308.45126 to 252.51591, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 63s - loss: 297.3154 - MinusLogProbMetric: 297.3154 - val_loss: 252.5159 - val_MinusLogProbMetric: 252.5159 - lr: 3.3333e-04 - 63s/epoch - 320ms/step
Epoch 5/1000
2023-09-27 15:44:03.701 
Epoch 5/1000 
	 loss: 214.8384, MinusLogProbMetric: 214.8384, val_loss: 188.5668, val_MinusLogProbMetric: 188.5668

Epoch 5: val_loss improved from 252.51591 to 188.56677, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 62s - loss: 214.8384 - MinusLogProbMetric: 214.8384 - val_loss: 188.5668 - val_MinusLogProbMetric: 188.5668 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 6/1000
2023-09-27 15:45:06.194 
Epoch 6/1000 
	 loss: 174.5563, MinusLogProbMetric: 174.5563, val_loss: 161.8224, val_MinusLogProbMetric: 161.8224

Epoch 6: val_loss improved from 188.56677 to 161.82242, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 62s - loss: 174.5563 - MinusLogProbMetric: 174.5563 - val_loss: 161.8224 - val_MinusLogProbMetric: 161.8224 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 7/1000
2023-09-27 15:46:08.635 
Epoch 7/1000 
	 loss: 153.7086, MinusLogProbMetric: 153.7086, val_loss: 146.2547, val_MinusLogProbMetric: 146.2547

Epoch 7: val_loss improved from 161.82242 to 146.25473, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 62s - loss: 153.7086 - MinusLogProbMetric: 153.7086 - val_loss: 146.2547 - val_MinusLogProbMetric: 146.2547 - lr: 3.3333e-04 - 62s/epoch - 319ms/step
Epoch 8/1000
2023-09-27 15:47:10.661 
Epoch 8/1000 
	 loss: 139.1816, MinusLogProbMetric: 139.1816, val_loss: 134.0582, val_MinusLogProbMetric: 134.0582

Epoch 8: val_loss improved from 146.25473 to 134.05817, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 62s - loss: 139.1816 - MinusLogProbMetric: 139.1816 - val_loss: 134.0582 - val_MinusLogProbMetric: 134.0582 - lr: 3.3333e-04 - 62s/epoch - 317ms/step
Epoch 9/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 104: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-27 15:47:45.971 
Epoch 9/1000 
	 loss: nan, MinusLogProbMetric: 132.2505, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 9: val_loss did not improve from 134.05817
196/196 - 34s - loss: nan - MinusLogProbMetric: 132.2505 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 34s/epoch - 175ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0001111111111111111.
===========
Generating train data for run 309.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_309
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_66"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_67 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7fef46438d30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fee007c3ca0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fee007c3ca0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff634135210>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feee0416410>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feee0416980>, <keras.callbacks.ModelCheckpoint object at 0x7feee0416a40>, <keras.callbacks.EarlyStopping object at 0x7feee0416cb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7feee0416ce0>, <keras.callbacks.TerminateOnNaN object at 0x7feee0416920>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 309/720 with hyperparameters:
timestamp = 2023-09-27 15:47:56.304806
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 63: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-27 15:50:20.900 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 155.7893, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 144s - loss: nan - MinusLogProbMetric: 155.7893 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 144s/epoch - 736ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 3.703703703703703e-05.
===========
Generating train data for run 309.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_309
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_77"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_78 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7feda88b5c60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff6019b0c70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff6019b0c70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fef67d26320>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fef67d25d80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fed9c98d930>, <keras.callbacks.ModelCheckpoint object at 0x7fed9c98cd60>, <keras.callbacks.EarlyStopping object at 0x7fed9c98da80>, <keras.callbacks.ReduceLROnPlateau object at 0x7fed9c98d840>, <keras.callbacks.TerminateOnNaN object at 0x7feda8c4f2e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 309/720 with hyperparameters:
timestamp = 2023-09-27 15:50:31.175179
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
2023-09-27 15:53:38.093 
Epoch 1/1000 
	 loss: 108.8605, MinusLogProbMetric: 108.8605, val_loss: 94.8480, val_MinusLogProbMetric: 94.8480

Epoch 1: val_loss improved from inf to 94.84797, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 188s - loss: 108.8605 - MinusLogProbMetric: 108.8605 - val_loss: 94.8480 - val_MinusLogProbMetric: 94.8480 - lr: 3.7037e-05 - 188s/epoch - 960ms/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-27 15:53:45.106 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 94.1635, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 94.84797
196/196 - 5s - loss: nan - MinusLogProbMetric: 94.1635 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 5s/epoch - 27ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 1.2345679012345677e-05.
===========
Generating train data for run 309.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_309
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_88"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_89 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7fedd86b6860>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7feec0422e30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7feec0422e30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff623197880>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fef45c9a4a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fef45c9aa10>, <keras.callbacks.ModelCheckpoint object at 0x7fef45c9aad0>, <keras.callbacks.EarlyStopping object at 0x7fef45c9ad40>, <keras.callbacks.ReduceLROnPlateau object at 0x7fef45c9ad70>, <keras.callbacks.TerminateOnNaN object at 0x7fef45c9a9b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 309/720 with hyperparameters:
timestamp = 2023-09-27 15:53:56.214210
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
2023-09-27 15:57:37.181 
Epoch 1/1000 
	 loss: 91.1086, MinusLogProbMetric: 91.1086, val_loss: 88.4201, val_MinusLogProbMetric: 88.4201

Epoch 1: val_loss improved from inf to 88.42012, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 222s - loss: 91.1086 - MinusLogProbMetric: 91.1086 - val_loss: 88.4201 - val_MinusLogProbMetric: 88.4201 - lr: 1.2346e-05 - 222s/epoch - 1s/step
Epoch 2/1000
2023-09-27 15:58:43.162 
Epoch 2/1000 
	 loss: 89.8102, MinusLogProbMetric: 89.8102, val_loss: 84.4325, val_MinusLogProbMetric: 84.4325

Epoch 2: val_loss improved from 88.42012 to 84.43253, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 66s - loss: 89.8102 - MinusLogProbMetric: 89.8102 - val_loss: 84.4325 - val_MinusLogProbMetric: 84.4325 - lr: 1.2346e-05 - 66s/epoch - 335ms/step
Epoch 3/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 102: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-27 15:59:20.599 
Epoch 3/1000 
	 loss: nan, MinusLogProbMetric: 82.3698, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 3: val_loss did not improve from 84.43253
196/196 - 36s - loss: nan - MinusLogProbMetric: 82.3698 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 36s/epoch - 186ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 4.115226337448558e-06.
===========
Generating train data for run 309.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_309
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_99"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_100 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7fed8dadfbe0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fed940f89a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fed940f89a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7feea04e6410>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fee606996f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fee60699c60>, <keras.callbacks.ModelCheckpoint object at 0x7fee60699d20>, <keras.callbacks.EarlyStopping object at 0x7fee60699f90>, <keras.callbacks.ReduceLROnPlateau object at 0x7fee60699fc0>, <keras.callbacks.TerminateOnNaN object at 0x7fee60699c00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 309/720 with hyperparameters:
timestamp = 2023-09-27 15:59:31.672309
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
2023-09-27 16:03:14.881 
Epoch 1/1000 
	 loss: 85.2204, MinusLogProbMetric: 85.2204, val_loss: 83.1879, val_MinusLogProbMetric: 83.1879

Epoch 1: val_loss improved from inf to 83.18790, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 224s - loss: 85.2204 - MinusLogProbMetric: 85.2204 - val_loss: 83.1879 - val_MinusLogProbMetric: 83.1879 - lr: 4.1152e-06 - 224s/epoch - 1s/step
Epoch 2/1000
2023-09-27 16:04:19.856 
Epoch 2/1000 
	 loss: 82.7464, MinusLogProbMetric: 82.7464, val_loss: 80.2910, val_MinusLogProbMetric: 80.2910

Epoch 2: val_loss improved from 83.18790 to 80.29102, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 65s - loss: 82.7464 - MinusLogProbMetric: 82.7464 - val_loss: 80.2910 - val_MinusLogProbMetric: 80.2910 - lr: 4.1152e-06 - 65s/epoch - 331ms/step
Epoch 3/1000
2023-09-27 16:05:25.228 
Epoch 3/1000 
	 loss: 79.2000, MinusLogProbMetric: 79.2000, val_loss: 78.1848, val_MinusLogProbMetric: 78.1848

Epoch 3: val_loss improved from 80.29102 to 78.18481, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 65s - loss: 79.2000 - MinusLogProbMetric: 79.2000 - val_loss: 78.1848 - val_MinusLogProbMetric: 78.1848 - lr: 4.1152e-06 - 65s/epoch - 333ms/step
Epoch 4/1000
2023-09-27 16:06:31.229 
Epoch 4/1000 
	 loss: 79.0525, MinusLogProbMetric: 79.0525, val_loss: 77.4945, val_MinusLogProbMetric: 77.4945

Epoch 4: val_loss improved from 78.18481 to 77.49454, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 66s - loss: 79.0525 - MinusLogProbMetric: 79.0525 - val_loss: 77.4945 - val_MinusLogProbMetric: 77.4945 - lr: 4.1152e-06 - 66s/epoch - 337ms/step
Epoch 5/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 27: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-27 16:06:45.168 
Epoch 5/1000 
	 loss: nan, MinusLogProbMetric: 77.1816, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 5: val_loss did not improve from 77.49454
196/196 - 13s - loss: nan - MinusLogProbMetric: 77.1816 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 13s/epoch - 66ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 1.3717421124828526e-06.
===========
Generating train data for run 309.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_309
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_110"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_111 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  826720    
 ayer)                                                           
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7fedb4717760>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fed8d5b7790>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fed8d5b7790>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fed8d4f3280>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7feee024cbb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7feee024d120>, <keras.callbacks.ModelCheckpoint object at 0x7feee024d1e0>, <keras.callbacks.EarlyStopping object at 0x7feee024d450>, <keras.callbacks.ReduceLROnPlateau object at 0x7feee024d480>, <keras.callbacks.TerminateOnNaN object at 0x7feee024d0c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 309/720 with hyperparameters:
timestamp = 2023-09-27 16:06:54.503365
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 10: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-27 16:09:36.904 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 77.8360, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 162s - loss: nan - MinusLogProbMetric: 77.8360 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 162s/epoch - 827ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 4.572473708276175e-07.
===========
Generating train data for run 309.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_309
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_121"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_122 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  826720    
 ayer)                                                           
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7fef6d4bfd90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fed952feda0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fed952feda0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fef6d4503d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fed9cd437f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fed9cd43d60>, <keras.callbacks.ModelCheckpoint object at 0x7fed9cd43e20>, <keras.callbacks.EarlyStopping object at 0x7fed9cd43fd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fed9cd43d30>, <keras.callbacks.TerminateOnNaN object at 0x7fed9cd43f10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 309/720 with hyperparameters:
timestamp = 2023-09-27 16:09:47.717602
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 41: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-27 16:12:53.254 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 77.2460, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 185s - loss: nan - MinusLogProbMetric: 77.2460 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 185s/epoch - 945ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 1.524157902758725e-07.
===========
Generating train data for run 309.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_309
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_132"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_133 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  826720    
 ayer)                                                           
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7fee2019b1c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fedc0756e30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fedc0756e30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fed9c30ab60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fed9c305b10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fed9c306080>, <keras.callbacks.ModelCheckpoint object at 0x7fed9c306140>, <keras.callbacks.EarlyStopping object at 0x7fed9c3063b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fed9c3063e0>, <keras.callbacks.TerminateOnNaN object at 0x7fed9c306020>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 309/720 with hyperparameters:
timestamp = 2023-09-27 16:13:04.389882
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
2023-09-27 16:16:43.009 
Epoch 1/1000 
	 loss: 77.0556, MinusLogProbMetric: 77.0556, val_loss: 77.0065, val_MinusLogProbMetric: 77.0065

Epoch 1: val_loss improved from inf to 77.00652, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 219s - loss: 77.0556 - MinusLogProbMetric: 77.0556 - val_loss: 77.0065 - val_MinusLogProbMetric: 77.0065 - lr: 1.5242e-07 - 219s/epoch - 1s/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 151: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-27 16:17:35.953 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 76.8381, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 77.00652
196/196 - 52s - loss: nan - MinusLogProbMetric: 76.8381 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 52s/epoch - 263ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 5.0805263425290834e-08.
===========
Generating train data for run 309.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_309
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_143"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_144 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  826720    
 ayer)                                                           
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7fed9427dae0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fed9ca55d50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fed9ca55d50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fed947b48b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fed947d04c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fed947d0a30>, <keras.callbacks.ModelCheckpoint object at 0x7fed947d0af0>, <keras.callbacks.EarlyStopping object at 0x7fed947d0d60>, <keras.callbacks.ReduceLROnPlateau object at 0x7fed947d0d90>, <keras.callbacks.TerminateOnNaN object at 0x7fed947d09d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 309/720 with hyperparameters:
timestamp = 2023-09-27 16:17:50.161244
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 26: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-27 16:20:42.267 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 77.0508, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 172s - loss: nan - MinusLogProbMetric: 77.0508 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 172s/epoch - 877ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 1.6935087808430278e-08.
===========
Generating train data for run 309.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_309/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_309
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_154"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_155 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  826720    
 ayer)                                                           
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7fed9cd441c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7ff634534910>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7ff634534910>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fed9cd459c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fef6d1270a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fef6d126ec0>, <keras.callbacks.ModelCheckpoint object at 0x7fef6d127df0>, <keras.callbacks.EarlyStopping object at 0x7fef6d127f40>, <keras.callbacks.ReduceLROnPlateau object at 0x7fef6d126e30>, <keras.callbacks.TerminateOnNaN object at 0x7fef6d126770>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 309/720 with hyperparameters:
timestamp = 2023-09-27 16:20:53.682744
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
2023-09-27 16:24:35.104 
Epoch 1/1000 
	 loss: 76.8963, MinusLogProbMetric: 76.8963, val_loss: 76.9922, val_MinusLogProbMetric: 76.9922

Epoch 1: val_loss improved from inf to 76.99220, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 222s - loss: 76.8963 - MinusLogProbMetric: 76.8963 - val_loss: 76.9922 - val_MinusLogProbMetric: 76.9922 - lr: 1.6935e-08 - 222s/epoch - 1s/step
Epoch 2/1000
2023-09-27 16:25:43.615 
Epoch 2/1000 
	 loss: 76.9073, MinusLogProbMetric: 76.9073, val_loss: 77.0099, val_MinusLogProbMetric: 77.0099

Epoch 2: val_loss did not improve from 76.99220
196/196 - 67s - loss: 76.9073 - MinusLogProbMetric: 76.9073 - val_loss: 77.0099 - val_MinusLogProbMetric: 77.0099 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 3/1000
2023-09-27 16:26:49.717 
Epoch 3/1000 
	 loss: 76.9092, MinusLogProbMetric: 76.9092, val_loss: 77.0050, val_MinusLogProbMetric: 77.0050

Epoch 3: val_loss did not improve from 76.99220
196/196 - 66s - loss: 76.9092 - MinusLogProbMetric: 76.9092 - val_loss: 77.0050 - val_MinusLogProbMetric: 77.0050 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 4/1000
2023-09-27 16:27:56.685 
Epoch 4/1000 
	 loss: 76.9044, MinusLogProbMetric: 76.9044, val_loss: 76.9921, val_MinusLogProbMetric: 76.9921

Epoch 4: val_loss improved from 76.99220 to 76.99209, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 68s - loss: 76.9044 - MinusLogProbMetric: 76.9044 - val_loss: 76.9921 - val_MinusLogProbMetric: 76.9921 - lr: 1.6935e-08 - 68s/epoch - 348ms/step
Epoch 5/1000
2023-09-27 16:29:02.908 
Epoch 5/1000 
	 loss: 76.8962, MinusLogProbMetric: 76.8962, val_loss: 76.9834, val_MinusLogProbMetric: 76.9834

Epoch 5: val_loss improved from 76.99209 to 76.98335, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_309/weights/best_weights.h5
196/196 - 66s - loss: 76.8962 - MinusLogProbMetric: 76.8962 - val_loss: 76.9834 - val_MinusLogProbMetric: 76.9834 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 6/1000
2023-09-27 16:30:08.547 
Epoch 6/1000 
	 loss: 76.8882, MinusLogProbMetric: 76.8882, val_loss: 76.9868, val_MinusLogProbMetric: 76.9868

Epoch 6: val_loss did not improve from 76.98335
196/196 - 65s - loss: 76.8882 - MinusLogProbMetric: 76.8882 - val_loss: 76.9868 - val_MinusLogProbMetric: 76.9868 - lr: 1.6935e-08 - 65s/epoch - 330ms/step
Epoch 7/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 16: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-27 16:30:17.810 
Epoch 7/1000 
	 loss: nan, MinusLogProbMetric: 76.8951, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 7: val_loss did not improve from 76.98335
196/196 - 9s - loss: nan - MinusLogProbMetric: 76.8951 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 9s/epoch - 47ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 5.645029269476759e-09.
===========
Run 309/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 313, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 310.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_310/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_310/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_310/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_310
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_165"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_166 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  2305120   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,305,120
Trainable params: 2,305,120
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7fed9c291330>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fedc10ecf40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fedc10ecf40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7ff5f8e855d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fed74fbfeb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fed74fb4460>, <keras.callbacks.ModelCheckpoint object at 0x7fed74fb4520>, <keras.callbacks.EarlyStopping object at 0x7fed74fb4790>, <keras.callbacks.ReduceLROnPlateau object at 0x7fed74fb47c0>, <keras.callbacks.TerminateOnNaN object at 0x7fed74fb4400>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_310/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 310/720 with hyperparameters:
timestamp = 2023-09-27 16:30:28.032494
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2305120
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 21: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-27 16:32:59.928 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 1829.4255, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 152s - loss: nan - MinusLogProbMetric: 1829.4255 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 152s/epoch - 774ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0003333333333333333.
===========
Generating train data for run 310.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_310/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_310/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_310/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_310
self.data_kwargs: {'seed': 926}
self.x_data: [[1.4696891  3.165189   9.115545   ... 7.2440553  3.018408   2.0407877 ]
 [2.9008398  3.8232424  7.129506   ... 7.411926   2.8866556  1.4983263 ]
 [4.669435   5.457821   0.04349925 ... 0.866001   6.611358   1.4323243 ]
 ...
 [4.261785   5.5317554  0.77893746 ... 0.60558414 5.4824862  1.3011014 ]
 [4.4981847  5.716866   0.72522354 ... 0.9913055  5.7512865  1.5265256 ]
 [5.479814   7.150977   6.3064384  ... 3.1490364  2.6668518  8.143393  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_176"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_177 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  2305120   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,305,120
Trainable params: 2,305,120
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7fef459a8430>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7fef3e984520>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7fef3e984520>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7fef45982dd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7fef3e826650>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7fef3e826bc0>, <keras.callbacks.ModelCheckpoint object at 0x7fef3e826c80>, <keras.callbacks.EarlyStopping object at 0x7fef3e826ef0>, <keras.callbacks.ReduceLROnPlateau object at 0x7fef3e826f20>, <keras.callbacks.TerminateOnNaN object at 0x7fef3e826b60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.5383615 , 4.21851   , 9.147865  , ..., 7.328855  , 2.4535477 ,
        2.0265868 ],
       [1.2026695 , 3.5675488 , 8.490738  , ..., 7.486889  , 2.9732862 ,
        1.6526531 ],
       [5.7808385 , 5.6486273 , 0.85161066, ..., 0.44565296, 6.0336885 ,
        1.4597642 ],
       ...,
       [3.869998  , 5.7079043 , 0.7277752 , ..., 1.4709862 , 7.5385275 ,
        1.2852083 ],
       [2.111317  , 3.3553224 , 7.6223764 , ..., 7.4778986 , 3.6354768 ,
        1.7641349 ],
       [0.83836687, 3.4177158 , 7.4230075 , ..., 7.223582  , 3.3779047 ,
        2.0014815 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_310/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 310/720 with hyperparameters:
timestamp = 2023-09-27 16:33:10.451455
ndims = 32
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2305120
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 1.4696891   3.165189    9.115545    1.2257975   9.89761    -0.04518861
  9.761367    5.046433   10.037439    6.270509    6.6800056   0.37080207
  3.2512977   1.1872      2.5658634   0.9389908   3.4711475   5.1744766
  0.38911742  6.94769     5.6341734   2.616604    4.513363    1.4770697
  4.2721643   9.222334    2.548921    6.22901     1.4673232   7.2440553
  3.018408    2.0407877 ]
Epoch 1/1000
2023-09-27 16:36:57.878 
Epoch 1/1000 
	 loss: 193.7227, MinusLogProbMetric: 193.7227, val_loss: 54.8363, val_MinusLogProbMetric: 54.8363

Epoch 1: val_loss improved from inf to 54.83625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 228s - loss: 193.7227 - MinusLogProbMetric: 193.7227 - val_loss: 54.8363 - val_MinusLogProbMetric: 54.8363 - lr: 3.3333e-04 - 228s/epoch - 1s/step
Epoch 2/1000
2023-09-27 16:38:04.954 
Epoch 2/1000 
	 loss: 44.8502, MinusLogProbMetric: 44.8502, val_loss: 38.5120, val_MinusLogProbMetric: 38.5120

Epoch 2: val_loss improved from 54.83625 to 38.51204, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 67s - loss: 44.8502 - MinusLogProbMetric: 44.8502 - val_loss: 38.5120 - val_MinusLogProbMetric: 38.5120 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 3/1000
2023-09-27 16:39:11.193 
Epoch 3/1000 
	 loss: 35.5988, MinusLogProbMetric: 35.5988, val_loss: 32.8912, val_MinusLogProbMetric: 32.8912

Epoch 3: val_loss improved from 38.51204 to 32.89125, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 66s - loss: 35.5988 - MinusLogProbMetric: 35.5988 - val_loss: 32.8912 - val_MinusLogProbMetric: 32.8912 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 4/1000
2023-09-27 16:40:16.450 
Epoch 4/1000 
	 loss: 30.7002, MinusLogProbMetric: 30.7002, val_loss: 31.2141, val_MinusLogProbMetric: 31.2141

Epoch 4: val_loss improved from 32.89125 to 31.21412, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 65s - loss: 30.7002 - MinusLogProbMetric: 30.7002 - val_loss: 31.2141 - val_MinusLogProbMetric: 31.2141 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 5/1000
2023-09-27 16:41:20.936 
Epoch 5/1000 
	 loss: 28.2803, MinusLogProbMetric: 28.2803, val_loss: 28.1891, val_MinusLogProbMetric: 28.1891

Epoch 5: val_loss improved from 31.21412 to 28.18910, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 64s - loss: 28.2803 - MinusLogProbMetric: 28.2803 - val_loss: 28.1891 - val_MinusLogProbMetric: 28.1891 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 6/1000
2023-09-27 16:42:27.794 
Epoch 6/1000 
	 loss: 26.3517, MinusLogProbMetric: 26.3517, val_loss: 24.6443, val_MinusLogProbMetric: 24.6443

Epoch 6: val_loss improved from 28.18910 to 24.64428, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 67s - loss: 26.3517 - MinusLogProbMetric: 26.3517 - val_loss: 24.6443 - val_MinusLogProbMetric: 24.6443 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 7/1000
2023-09-27 16:43:35.029 
Epoch 7/1000 
	 loss: 25.4060, MinusLogProbMetric: 25.4060, val_loss: 24.5143, val_MinusLogProbMetric: 24.5143

Epoch 7: val_loss improved from 24.64428 to 24.51433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 67s - loss: 25.4060 - MinusLogProbMetric: 25.4060 - val_loss: 24.5143 - val_MinusLogProbMetric: 24.5143 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 8/1000
2023-09-27 16:44:41.569 
Epoch 8/1000 
	 loss: 24.3434, MinusLogProbMetric: 24.3434, val_loss: 26.1745, val_MinusLogProbMetric: 26.1745

Epoch 8: val_loss did not improve from 24.51433
196/196 - 66s - loss: 24.3434 - MinusLogProbMetric: 24.3434 - val_loss: 26.1745 - val_MinusLogProbMetric: 26.1745 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 9/1000
2023-09-27 16:45:47.429 
Epoch 9/1000 
	 loss: 23.9301, MinusLogProbMetric: 23.9301, val_loss: 24.8467, val_MinusLogProbMetric: 24.8467

Epoch 9: val_loss did not improve from 24.51433
196/196 - 66s - loss: 23.9301 - MinusLogProbMetric: 23.9301 - val_loss: 24.8467 - val_MinusLogProbMetric: 24.8467 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 10/1000
2023-09-27 16:46:52.372 
Epoch 10/1000 
	 loss: 23.2778, MinusLogProbMetric: 23.2778, val_loss: 23.2297, val_MinusLogProbMetric: 23.2297

Epoch 10: val_loss improved from 24.51433 to 23.22970, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 66s - loss: 23.2778 - MinusLogProbMetric: 23.2778 - val_loss: 23.2297 - val_MinusLogProbMetric: 23.2297 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 11/1000
2023-09-27 16:47:57.750 
Epoch 11/1000 
	 loss: 22.7720, MinusLogProbMetric: 22.7720, val_loss: 22.7247, val_MinusLogProbMetric: 22.7247

Epoch 11: val_loss improved from 23.22970 to 22.72470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 66s - loss: 22.7720 - MinusLogProbMetric: 22.7720 - val_loss: 22.7247 - val_MinusLogProbMetric: 22.7247 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 12/1000
2023-09-27 16:49:02.230 
Epoch 12/1000 
	 loss: 22.6983, MinusLogProbMetric: 22.6983, val_loss: 22.2421, val_MinusLogProbMetric: 22.2421

Epoch 12: val_loss improved from 22.72470 to 22.24209, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 64s - loss: 22.6983 - MinusLogProbMetric: 22.6983 - val_loss: 22.2421 - val_MinusLogProbMetric: 22.2421 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 13/1000
2023-09-27 16:50:08.125 
Epoch 13/1000 
	 loss: 22.2157, MinusLogProbMetric: 22.2157, val_loss: 23.0308, val_MinusLogProbMetric: 23.0308

Epoch 13: val_loss did not improve from 22.24209
196/196 - 65s - loss: 22.2157 - MinusLogProbMetric: 22.2157 - val_loss: 23.0308 - val_MinusLogProbMetric: 23.0308 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 14/1000
2023-09-27 16:51:12.342 
Epoch 14/1000 
	 loss: 21.9762, MinusLogProbMetric: 21.9762, val_loss: 21.7185, val_MinusLogProbMetric: 21.7185

Epoch 14: val_loss improved from 22.24209 to 21.71853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 65s - loss: 21.9762 - MinusLogProbMetric: 21.9762 - val_loss: 21.7185 - val_MinusLogProbMetric: 21.7185 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 15/1000
2023-09-27 16:52:18.653 
Epoch 15/1000 
	 loss: 21.5644, MinusLogProbMetric: 21.5644, val_loss: 21.4993, val_MinusLogProbMetric: 21.4993

Epoch 15: val_loss improved from 21.71853 to 21.49930, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 66s - loss: 21.5644 - MinusLogProbMetric: 21.5644 - val_loss: 21.4993 - val_MinusLogProbMetric: 21.4993 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 16/1000
2023-09-27 16:53:24.027 
Epoch 16/1000 
	 loss: 21.4657, MinusLogProbMetric: 21.4657, val_loss: 21.6592, val_MinusLogProbMetric: 21.6592

Epoch 16: val_loss did not improve from 21.49930
196/196 - 65s - loss: 21.4657 - MinusLogProbMetric: 21.4657 - val_loss: 21.6592 - val_MinusLogProbMetric: 21.6592 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 17/1000
2023-09-27 16:54:28.536 
Epoch 17/1000 
	 loss: 21.7495, MinusLogProbMetric: 21.7495, val_loss: 22.5105, val_MinusLogProbMetric: 22.5105

Epoch 17: val_loss did not improve from 21.49930
196/196 - 65s - loss: 21.7495 - MinusLogProbMetric: 21.7495 - val_loss: 22.5105 - val_MinusLogProbMetric: 22.5105 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 18/1000
2023-09-27 16:55:34.439 
Epoch 18/1000 
	 loss: 21.3579, MinusLogProbMetric: 21.3579, val_loss: 21.7232, val_MinusLogProbMetric: 21.7232

Epoch 18: val_loss did not improve from 21.49930
196/196 - 66s - loss: 21.3579 - MinusLogProbMetric: 21.3579 - val_loss: 21.7232 - val_MinusLogProbMetric: 21.7232 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 19/1000
2023-09-27 16:56:40.554 
Epoch 19/1000 
	 loss: 21.0824, MinusLogProbMetric: 21.0824, val_loss: 20.7427, val_MinusLogProbMetric: 20.7427

Epoch 19: val_loss improved from 21.49930 to 20.74271, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 67s - loss: 21.0824 - MinusLogProbMetric: 21.0824 - val_loss: 20.7427 - val_MinusLogProbMetric: 20.7427 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 20/1000
2023-09-27 16:57:46.208 
Epoch 20/1000 
	 loss: 20.8991, MinusLogProbMetric: 20.8991, val_loss: 20.8703, val_MinusLogProbMetric: 20.8703

Epoch 20: val_loss did not improve from 20.74271
196/196 - 65s - loss: 20.8991 - MinusLogProbMetric: 20.8991 - val_loss: 20.8703 - val_MinusLogProbMetric: 20.8703 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 21/1000
2023-09-27 16:58:50.776 
Epoch 21/1000 
	 loss: 20.6535, MinusLogProbMetric: 20.6535, val_loss: 22.6171, val_MinusLogProbMetric: 22.6171

Epoch 21: val_loss did not improve from 20.74271
196/196 - 65s - loss: 20.6535 - MinusLogProbMetric: 20.6535 - val_loss: 22.6171 - val_MinusLogProbMetric: 22.6171 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 22/1000
2023-09-27 16:59:55.517 
Epoch 22/1000 
	 loss: 20.8241, MinusLogProbMetric: 20.8241, val_loss: 20.3274, val_MinusLogProbMetric: 20.3274

Epoch 22: val_loss improved from 20.74271 to 20.32735, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 66s - loss: 20.8241 - MinusLogProbMetric: 20.8241 - val_loss: 20.3274 - val_MinusLogProbMetric: 20.3274 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 23/1000
2023-09-27 17:00:59.961 
Epoch 23/1000 
	 loss: 20.4681, MinusLogProbMetric: 20.4681, val_loss: 26.4473, val_MinusLogProbMetric: 26.4473

Epoch 23: val_loss did not improve from 20.32735
196/196 - 63s - loss: 20.4681 - MinusLogProbMetric: 20.4681 - val_loss: 26.4473 - val_MinusLogProbMetric: 26.4473 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 24/1000
2023-09-27 17:02:05.047 
Epoch 24/1000 
	 loss: 20.6182, MinusLogProbMetric: 20.6182, val_loss: 20.9509, val_MinusLogProbMetric: 20.9509

Epoch 24: val_loss did not improve from 20.32735
196/196 - 65s - loss: 20.6182 - MinusLogProbMetric: 20.6182 - val_loss: 20.9509 - val_MinusLogProbMetric: 20.9509 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 25/1000
2023-09-27 17:03:10.000 
Epoch 25/1000 
	 loss: 20.6550, MinusLogProbMetric: 20.6550, val_loss: 20.4543, val_MinusLogProbMetric: 20.4543

Epoch 25: val_loss did not improve from 20.32735
196/196 - 65s - loss: 20.6550 - MinusLogProbMetric: 20.6550 - val_loss: 20.4543 - val_MinusLogProbMetric: 20.4543 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 26/1000
2023-09-27 17:04:14.020 
Epoch 26/1000 
	 loss: 20.2414, MinusLogProbMetric: 20.2414, val_loss: 19.7042, val_MinusLogProbMetric: 19.7042

Epoch 26: val_loss improved from 20.32735 to 19.70424, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 65s - loss: 20.2414 - MinusLogProbMetric: 20.2414 - val_loss: 19.7042 - val_MinusLogProbMetric: 19.7042 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 27/1000
2023-09-27 17:05:19.794 
Epoch 27/1000 
	 loss: 20.1336, MinusLogProbMetric: 20.1336, val_loss: 19.4829, val_MinusLogProbMetric: 19.4829

Epoch 27: val_loss improved from 19.70424 to 19.48291, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 66s - loss: 20.1336 - MinusLogProbMetric: 20.1336 - val_loss: 19.4829 - val_MinusLogProbMetric: 19.4829 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 28/1000
2023-09-27 17:06:17.035 
Epoch 28/1000 
	 loss: 20.1677, MinusLogProbMetric: 20.1677, val_loss: 19.8689, val_MinusLogProbMetric: 19.8689

Epoch 28: val_loss did not improve from 19.48291
196/196 - 56s - loss: 20.1677 - MinusLogProbMetric: 20.1677 - val_loss: 19.8689 - val_MinusLogProbMetric: 19.8689 - lr: 3.3333e-04 - 56s/epoch - 287ms/step
Epoch 29/1000
2023-09-27 17:07:13.059 
Epoch 29/1000 
	 loss: 20.1382, MinusLogProbMetric: 20.1382, val_loss: 20.4200, val_MinusLogProbMetric: 20.4200

Epoch 29: val_loss did not improve from 19.48291
196/196 - 56s - loss: 20.1382 - MinusLogProbMetric: 20.1382 - val_loss: 20.4200 - val_MinusLogProbMetric: 20.4200 - lr: 3.3333e-04 - 56s/epoch - 286ms/step
Epoch 30/1000
2023-09-27 17:08:05.386 
Epoch 30/1000 
	 loss: 19.9904, MinusLogProbMetric: 19.9904, val_loss: 20.5430, val_MinusLogProbMetric: 20.5430

Epoch 30: val_loss did not improve from 19.48291
196/196 - 52s - loss: 19.9904 - MinusLogProbMetric: 19.9904 - val_loss: 20.5430 - val_MinusLogProbMetric: 20.5430 - lr: 3.3333e-04 - 52s/epoch - 267ms/step
Epoch 31/1000
2023-09-27 17:09:00.043 
Epoch 31/1000 
	 loss: 19.8595, MinusLogProbMetric: 19.8595, val_loss: 19.8867, val_MinusLogProbMetric: 19.8867

Epoch 31: val_loss did not improve from 19.48291
196/196 - 55s - loss: 19.8595 - MinusLogProbMetric: 19.8595 - val_loss: 19.8867 - val_MinusLogProbMetric: 19.8867 - lr: 3.3333e-04 - 55s/epoch - 279ms/step
Epoch 32/1000
2023-09-27 17:10:00.153 
Epoch 32/1000 
	 loss: 19.8444, MinusLogProbMetric: 19.8444, val_loss: 19.2692, val_MinusLogProbMetric: 19.2692

Epoch 32: val_loss improved from 19.48291 to 19.26917, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 61s - loss: 19.8444 - MinusLogProbMetric: 19.8444 - val_loss: 19.2692 - val_MinusLogProbMetric: 19.2692 - lr: 3.3333e-04 - 61s/epoch - 311ms/step
Epoch 33/1000
2023-09-27 17:10:52.861 
Epoch 33/1000 
	 loss: 19.7487, MinusLogProbMetric: 19.7487, val_loss: 20.5107, val_MinusLogProbMetric: 20.5107

Epoch 33: val_loss did not improve from 19.26917
196/196 - 52s - loss: 19.7487 - MinusLogProbMetric: 19.7487 - val_loss: 20.5107 - val_MinusLogProbMetric: 20.5107 - lr: 3.3333e-04 - 52s/epoch - 265ms/step
Epoch 34/1000
2023-09-27 17:11:47.113 
Epoch 34/1000 
	 loss: 19.7476, MinusLogProbMetric: 19.7476, val_loss: 19.9627, val_MinusLogProbMetric: 19.9627

Epoch 34: val_loss did not improve from 19.26917
196/196 - 54s - loss: 19.7476 - MinusLogProbMetric: 19.7476 - val_loss: 19.9627 - val_MinusLogProbMetric: 19.9627 - lr: 3.3333e-04 - 54s/epoch - 277ms/step
Epoch 35/1000
2023-09-27 17:12:49.589 
Epoch 35/1000 
	 loss: 19.6834, MinusLogProbMetric: 19.6834, val_loss: 19.9661, val_MinusLogProbMetric: 19.9661

Epoch 35: val_loss did not improve from 19.26917
196/196 - 62s - loss: 19.6834 - MinusLogProbMetric: 19.6834 - val_loss: 19.9661 - val_MinusLogProbMetric: 19.9661 - lr: 3.3333e-04 - 62s/epoch - 319ms/step
Epoch 36/1000
2023-09-27 17:13:52.941 
Epoch 36/1000 
	 loss: 19.7237, MinusLogProbMetric: 19.7237, val_loss: 19.4800, val_MinusLogProbMetric: 19.4800

Epoch 36: val_loss did not improve from 19.26917
196/196 - 63s - loss: 19.7237 - MinusLogProbMetric: 19.7237 - val_loss: 19.4800 - val_MinusLogProbMetric: 19.4800 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 37/1000
2023-09-27 17:14:57.314 
Epoch 37/1000 
	 loss: 19.6044, MinusLogProbMetric: 19.6044, val_loss: 19.3163, val_MinusLogProbMetric: 19.3163

Epoch 37: val_loss did not improve from 19.26917
196/196 - 64s - loss: 19.6044 - MinusLogProbMetric: 19.6044 - val_loss: 19.3163 - val_MinusLogProbMetric: 19.3163 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 38/1000
2023-09-27 17:16:01.646 
Epoch 38/1000 
	 loss: 19.4710, MinusLogProbMetric: 19.4710, val_loss: 19.4875, val_MinusLogProbMetric: 19.4875

Epoch 38: val_loss did not improve from 19.26917
196/196 - 64s - loss: 19.4710 - MinusLogProbMetric: 19.4710 - val_loss: 19.4875 - val_MinusLogProbMetric: 19.4875 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 39/1000
2023-09-27 17:17:07.005 
Epoch 39/1000 
	 loss: 19.4726, MinusLogProbMetric: 19.4726, val_loss: 24.0130, val_MinusLogProbMetric: 24.0130

Epoch 39: val_loss did not improve from 19.26917
196/196 - 65s - loss: 19.4726 - MinusLogProbMetric: 19.4726 - val_loss: 24.0130 - val_MinusLogProbMetric: 24.0130 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 40/1000
2023-09-27 17:18:12.379 
Epoch 40/1000 
	 loss: 19.4656, MinusLogProbMetric: 19.4656, val_loss: 19.4562, val_MinusLogProbMetric: 19.4562

Epoch 40: val_loss did not improve from 19.26917
196/196 - 65s - loss: 19.4656 - MinusLogProbMetric: 19.4656 - val_loss: 19.4562 - val_MinusLogProbMetric: 19.4562 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 41/1000
2023-09-27 17:19:17.411 
Epoch 41/1000 
	 loss: 19.4318, MinusLogProbMetric: 19.4318, val_loss: 19.3632, val_MinusLogProbMetric: 19.3632

Epoch 41: val_loss did not improve from 19.26917
196/196 - 65s - loss: 19.4318 - MinusLogProbMetric: 19.4318 - val_loss: 19.3632 - val_MinusLogProbMetric: 19.3632 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 42/1000
2023-09-27 17:20:23.036 
Epoch 42/1000 
	 loss: 19.4282, MinusLogProbMetric: 19.4282, val_loss: 18.6688, val_MinusLogProbMetric: 18.6688

Epoch 42: val_loss improved from 19.26917 to 18.66882, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 67s - loss: 19.4282 - MinusLogProbMetric: 19.4282 - val_loss: 18.6688 - val_MinusLogProbMetric: 18.6688 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 43/1000
2023-09-27 17:21:28.330 
Epoch 43/1000 
	 loss: 19.3324, MinusLogProbMetric: 19.3324, val_loss: 19.2839, val_MinusLogProbMetric: 19.2839

Epoch 43: val_loss did not improve from 18.66882
196/196 - 64s - loss: 19.3324 - MinusLogProbMetric: 19.3324 - val_loss: 19.2839 - val_MinusLogProbMetric: 19.2839 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 44/1000
2023-09-27 17:22:31.974 
Epoch 44/1000 
	 loss: 19.3232, MinusLogProbMetric: 19.3232, val_loss: 19.2075, val_MinusLogProbMetric: 19.2075

Epoch 44: val_loss did not improve from 18.66882
196/196 - 64s - loss: 19.3232 - MinusLogProbMetric: 19.3232 - val_loss: 19.2075 - val_MinusLogProbMetric: 19.2075 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 45/1000
2023-09-27 17:23:36.351 
Epoch 45/1000 
	 loss: 19.0927, MinusLogProbMetric: 19.0927, val_loss: 19.6488, val_MinusLogProbMetric: 19.6488

Epoch 45: val_loss did not improve from 18.66882
196/196 - 64s - loss: 19.0927 - MinusLogProbMetric: 19.0927 - val_loss: 19.6488 - val_MinusLogProbMetric: 19.6488 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 46/1000
2023-09-27 17:24:41.049 
Epoch 46/1000 
	 loss: 19.2762, MinusLogProbMetric: 19.2762, val_loss: 19.2339, val_MinusLogProbMetric: 19.2339

Epoch 46: val_loss did not improve from 18.66882
196/196 - 65s - loss: 19.2762 - MinusLogProbMetric: 19.2762 - val_loss: 19.2339 - val_MinusLogProbMetric: 19.2339 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 47/1000
2023-09-27 17:25:45.647 
Epoch 47/1000 
	 loss: 19.3005, MinusLogProbMetric: 19.3005, val_loss: 19.7295, val_MinusLogProbMetric: 19.7295

Epoch 47: val_loss did not improve from 18.66882
196/196 - 65s - loss: 19.3005 - MinusLogProbMetric: 19.3005 - val_loss: 19.7295 - val_MinusLogProbMetric: 19.7295 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 48/1000
2023-09-27 17:26:49.887 
Epoch 48/1000 
	 loss: 19.2084, MinusLogProbMetric: 19.2084, val_loss: 19.5445, val_MinusLogProbMetric: 19.5445

Epoch 48: val_loss did not improve from 18.66882
196/196 - 64s - loss: 19.2084 - MinusLogProbMetric: 19.2084 - val_loss: 19.5445 - val_MinusLogProbMetric: 19.5445 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 49/1000
2023-09-27 17:27:55.739 
Epoch 49/1000 
	 loss: 19.0866, MinusLogProbMetric: 19.0866, val_loss: 19.2817, val_MinusLogProbMetric: 19.2817

Epoch 49: val_loss did not improve from 18.66882
196/196 - 66s - loss: 19.0866 - MinusLogProbMetric: 19.0866 - val_loss: 19.2817 - val_MinusLogProbMetric: 19.2817 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 50/1000
2023-09-27 17:29:01.273 
Epoch 50/1000 
	 loss: 19.0313, MinusLogProbMetric: 19.0313, val_loss: 18.9922, val_MinusLogProbMetric: 18.9922

Epoch 50: val_loss did not improve from 18.66882
196/196 - 66s - loss: 19.0313 - MinusLogProbMetric: 19.0313 - val_loss: 18.9922 - val_MinusLogProbMetric: 18.9922 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 51/1000
2023-09-27 17:30:06.172 
Epoch 51/1000 
	 loss: 19.2370, MinusLogProbMetric: 19.2370, val_loss: 20.5326, val_MinusLogProbMetric: 20.5326

Epoch 51: val_loss did not improve from 18.66882
196/196 - 65s - loss: 19.2370 - MinusLogProbMetric: 19.2370 - val_loss: 20.5326 - val_MinusLogProbMetric: 20.5326 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 52/1000
2023-09-27 17:31:10.245 
Epoch 52/1000 
	 loss: 19.0357, MinusLogProbMetric: 19.0357, val_loss: 19.5388, val_MinusLogProbMetric: 19.5388

Epoch 52: val_loss did not improve from 18.66882
196/196 - 64s - loss: 19.0357 - MinusLogProbMetric: 19.0357 - val_loss: 19.5388 - val_MinusLogProbMetric: 19.5388 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 53/1000
2023-09-27 17:32:15.886 
Epoch 53/1000 
	 loss: 18.9340, MinusLogProbMetric: 18.9340, val_loss: 19.6241, val_MinusLogProbMetric: 19.6241

Epoch 53: val_loss did not improve from 18.66882
196/196 - 66s - loss: 18.9340 - MinusLogProbMetric: 18.9340 - val_loss: 19.6241 - val_MinusLogProbMetric: 19.6241 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 54/1000
2023-09-27 17:33:20.007 
Epoch 54/1000 
	 loss: 18.8428, MinusLogProbMetric: 18.8428, val_loss: 18.8015, val_MinusLogProbMetric: 18.8015

Epoch 54: val_loss did not improve from 18.66882
196/196 - 64s - loss: 18.8428 - MinusLogProbMetric: 18.8428 - val_loss: 18.8015 - val_MinusLogProbMetric: 18.8015 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 55/1000
2023-09-27 17:34:24.857 
Epoch 55/1000 
	 loss: 18.9498, MinusLogProbMetric: 18.9498, val_loss: 21.0641, val_MinusLogProbMetric: 21.0641

Epoch 55: val_loss did not improve from 18.66882
196/196 - 65s - loss: 18.9498 - MinusLogProbMetric: 18.9498 - val_loss: 21.0641 - val_MinusLogProbMetric: 21.0641 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 56/1000
2023-09-27 17:35:30.321 
Epoch 56/1000 
	 loss: 18.9522, MinusLogProbMetric: 18.9522, val_loss: 18.9998, val_MinusLogProbMetric: 18.9998

Epoch 56: val_loss did not improve from 18.66882
196/196 - 65s - loss: 18.9522 - MinusLogProbMetric: 18.9522 - val_loss: 18.9998 - val_MinusLogProbMetric: 18.9998 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 57/1000
2023-09-27 17:36:35.150 
Epoch 57/1000 
	 loss: 18.8374, MinusLogProbMetric: 18.8374, val_loss: 18.6562, val_MinusLogProbMetric: 18.6562

Epoch 57: val_loss improved from 18.66882 to 18.65617, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 66s - loss: 18.8374 - MinusLogProbMetric: 18.8374 - val_loss: 18.6562 - val_MinusLogProbMetric: 18.6562 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 58/1000
2023-09-27 17:37:40.616 
Epoch 58/1000 
	 loss: 18.8853, MinusLogProbMetric: 18.8853, val_loss: 18.8087, val_MinusLogProbMetric: 18.8087

Epoch 58: val_loss did not improve from 18.65617
196/196 - 64s - loss: 18.8853 - MinusLogProbMetric: 18.8853 - val_loss: 18.8087 - val_MinusLogProbMetric: 18.8087 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 59/1000
2023-09-27 17:38:45.514 
Epoch 59/1000 
	 loss: 18.7358, MinusLogProbMetric: 18.7358, val_loss: 19.3620, val_MinusLogProbMetric: 19.3620

Epoch 59: val_loss did not improve from 18.65617
196/196 - 65s - loss: 18.7358 - MinusLogProbMetric: 18.7358 - val_loss: 19.3620 - val_MinusLogProbMetric: 19.3620 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 60/1000
2023-09-27 17:39:50.768 
Epoch 60/1000 
	 loss: 18.8134, MinusLogProbMetric: 18.8134, val_loss: 18.8946, val_MinusLogProbMetric: 18.8946

Epoch 60: val_loss did not improve from 18.65617
196/196 - 65s - loss: 18.8134 - MinusLogProbMetric: 18.8134 - val_loss: 18.8946 - val_MinusLogProbMetric: 18.8946 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 61/1000
2023-09-27 17:40:55.268 
Epoch 61/1000 
	 loss: 18.7892, MinusLogProbMetric: 18.7892, val_loss: 19.6390, val_MinusLogProbMetric: 19.6390

Epoch 61: val_loss did not improve from 18.65617
196/196 - 64s - loss: 18.7892 - MinusLogProbMetric: 18.7892 - val_loss: 19.6390 - val_MinusLogProbMetric: 19.6390 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 62/1000
2023-09-27 17:41:58.984 
Epoch 62/1000 
	 loss: 18.7564, MinusLogProbMetric: 18.7564, val_loss: 19.1083, val_MinusLogProbMetric: 19.1083

Epoch 62: val_loss did not improve from 18.65617
196/196 - 64s - loss: 18.7564 - MinusLogProbMetric: 18.7564 - val_loss: 19.1083 - val_MinusLogProbMetric: 19.1083 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 63/1000
2023-09-27 17:43:03.070 
Epoch 63/1000 
	 loss: 18.7506, MinusLogProbMetric: 18.7506, val_loss: 18.3706, val_MinusLogProbMetric: 18.3706

Epoch 63: val_loss improved from 18.65617 to 18.37057, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 65s - loss: 18.7506 - MinusLogProbMetric: 18.7506 - val_loss: 18.3706 - val_MinusLogProbMetric: 18.3706 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 64/1000
2023-09-27 17:44:08.186 
Epoch 64/1000 
	 loss: 18.8690, MinusLogProbMetric: 18.8690, val_loss: 18.8145, val_MinusLogProbMetric: 18.8145

Epoch 64: val_loss did not improve from 18.37057
196/196 - 64s - loss: 18.8690 - MinusLogProbMetric: 18.8690 - val_loss: 18.8145 - val_MinusLogProbMetric: 18.8145 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 65/1000
2023-09-27 17:45:12.921 
Epoch 65/1000 
	 loss: 18.7077, MinusLogProbMetric: 18.7077, val_loss: 19.0042, val_MinusLogProbMetric: 19.0042

Epoch 65: val_loss did not improve from 18.37057
196/196 - 65s - loss: 18.7077 - MinusLogProbMetric: 18.7077 - val_loss: 19.0042 - val_MinusLogProbMetric: 19.0042 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 66/1000
2023-09-27 17:46:19.357 
Epoch 66/1000 
	 loss: 18.7281, MinusLogProbMetric: 18.7281, val_loss: 19.5104, val_MinusLogProbMetric: 19.5104

Epoch 66: val_loss did not improve from 18.37057
196/196 - 66s - loss: 18.7281 - MinusLogProbMetric: 18.7281 - val_loss: 19.5104 - val_MinusLogProbMetric: 19.5104 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 67/1000
2023-09-27 17:47:25.278 
Epoch 67/1000 
	 loss: 18.7306, MinusLogProbMetric: 18.7306, val_loss: 19.2698, val_MinusLogProbMetric: 19.2698

Epoch 67: val_loss did not improve from 18.37057
196/196 - 66s - loss: 18.7306 - MinusLogProbMetric: 18.7306 - val_loss: 19.2698 - val_MinusLogProbMetric: 19.2698 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 68/1000
2023-09-27 17:48:31.770 
Epoch 68/1000 
	 loss: 18.5986, MinusLogProbMetric: 18.5986, val_loss: 19.8308, val_MinusLogProbMetric: 19.8308

Epoch 68: val_loss did not improve from 18.37057
196/196 - 66s - loss: 18.5986 - MinusLogProbMetric: 18.5986 - val_loss: 19.8308 - val_MinusLogProbMetric: 19.8308 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 69/1000
2023-09-27 17:49:35.902 
Epoch 69/1000 
	 loss: 18.6105, MinusLogProbMetric: 18.6105, val_loss: 18.7554, val_MinusLogProbMetric: 18.7554

Epoch 69: val_loss did not improve from 18.37057
196/196 - 64s - loss: 18.6105 - MinusLogProbMetric: 18.6105 - val_loss: 18.7554 - val_MinusLogProbMetric: 18.7554 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 70/1000
2023-09-27 17:50:40.708 
Epoch 70/1000 
	 loss: 18.5401, MinusLogProbMetric: 18.5401, val_loss: 18.5068, val_MinusLogProbMetric: 18.5068

Epoch 70: val_loss did not improve from 18.37057
196/196 - 65s - loss: 18.5401 - MinusLogProbMetric: 18.5401 - val_loss: 18.5068 - val_MinusLogProbMetric: 18.5068 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 71/1000
2023-09-27 17:51:45.883 
Epoch 71/1000 
	 loss: 18.6069, MinusLogProbMetric: 18.6069, val_loss: 19.1322, val_MinusLogProbMetric: 19.1322

Epoch 71: val_loss did not improve from 18.37057
196/196 - 65s - loss: 18.6069 - MinusLogProbMetric: 18.6069 - val_loss: 19.1322 - val_MinusLogProbMetric: 19.1322 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 72/1000
2023-09-27 17:52:50.410 
Epoch 72/1000 
	 loss: 18.5925, MinusLogProbMetric: 18.5925, val_loss: 19.2092, val_MinusLogProbMetric: 19.2092

Epoch 72: val_loss did not improve from 18.37057
196/196 - 65s - loss: 18.5925 - MinusLogProbMetric: 18.5925 - val_loss: 19.2092 - val_MinusLogProbMetric: 19.2092 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 73/1000
2023-09-27 17:53:55.318 
Epoch 73/1000 
	 loss: 18.5279, MinusLogProbMetric: 18.5279, val_loss: 18.7763, val_MinusLogProbMetric: 18.7763

Epoch 73: val_loss did not improve from 18.37057
196/196 - 65s - loss: 18.5279 - MinusLogProbMetric: 18.5279 - val_loss: 18.7763 - val_MinusLogProbMetric: 18.7763 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 74/1000
2023-09-27 17:55:00.282 
Epoch 74/1000 
	 loss: 18.6739, MinusLogProbMetric: 18.6739, val_loss: 18.9685, val_MinusLogProbMetric: 18.9685

Epoch 74: val_loss did not improve from 18.37057
196/196 - 65s - loss: 18.6739 - MinusLogProbMetric: 18.6739 - val_loss: 18.9685 - val_MinusLogProbMetric: 18.9685 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 75/1000
2023-09-27 17:56:05.566 
Epoch 75/1000 
	 loss: 18.5085, MinusLogProbMetric: 18.5085, val_loss: 18.8899, val_MinusLogProbMetric: 18.8899

Epoch 75: val_loss did not improve from 18.37057
196/196 - 65s - loss: 18.5085 - MinusLogProbMetric: 18.5085 - val_loss: 18.8899 - val_MinusLogProbMetric: 18.8899 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 76/1000
2023-09-27 17:57:10.550 
Epoch 76/1000 
	 loss: 18.5664, MinusLogProbMetric: 18.5664, val_loss: 19.1403, val_MinusLogProbMetric: 19.1403

Epoch 76: val_loss did not improve from 18.37057
196/196 - 65s - loss: 18.5664 - MinusLogProbMetric: 18.5664 - val_loss: 19.1403 - val_MinusLogProbMetric: 19.1403 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 77/1000
2023-09-27 17:58:16.134 
Epoch 77/1000 
	 loss: 18.4797, MinusLogProbMetric: 18.4797, val_loss: 19.6239, val_MinusLogProbMetric: 19.6239

Epoch 77: val_loss did not improve from 18.37057
196/196 - 66s - loss: 18.4797 - MinusLogProbMetric: 18.4797 - val_loss: 19.6239 - val_MinusLogProbMetric: 19.6239 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 78/1000
2023-09-27 17:59:20.952 
Epoch 78/1000 
	 loss: 18.5190, MinusLogProbMetric: 18.5190, val_loss: 20.0139, val_MinusLogProbMetric: 20.0139

Epoch 78: val_loss did not improve from 18.37057
196/196 - 65s - loss: 18.5190 - MinusLogProbMetric: 18.5190 - val_loss: 20.0139 - val_MinusLogProbMetric: 20.0139 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 79/1000
2023-09-27 18:00:25.525 
Epoch 79/1000 
	 loss: 18.4332, MinusLogProbMetric: 18.4332, val_loss: 18.6727, val_MinusLogProbMetric: 18.6727

Epoch 79: val_loss did not improve from 18.37057
196/196 - 65s - loss: 18.4332 - MinusLogProbMetric: 18.4332 - val_loss: 18.6727 - val_MinusLogProbMetric: 18.6727 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 80/1000
2023-09-27 18:01:32.304 
Epoch 80/1000 
	 loss: 18.4566, MinusLogProbMetric: 18.4566, val_loss: 19.1430, val_MinusLogProbMetric: 19.1430

Epoch 80: val_loss did not improve from 18.37057
196/196 - 67s - loss: 18.4566 - MinusLogProbMetric: 18.4566 - val_loss: 19.1430 - val_MinusLogProbMetric: 19.1430 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 81/1000
2023-09-27 18:02:38.079 
Epoch 81/1000 
	 loss: 18.4006, MinusLogProbMetric: 18.4006, val_loss: 18.3312, val_MinusLogProbMetric: 18.3312

Epoch 81: val_loss improved from 18.37057 to 18.33120, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 67s - loss: 18.4006 - MinusLogProbMetric: 18.4006 - val_loss: 18.3312 - val_MinusLogProbMetric: 18.3312 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 82/1000
2023-09-27 18:03:43.111 
Epoch 82/1000 
	 loss: 18.4562, MinusLogProbMetric: 18.4562, val_loss: 18.3695, val_MinusLogProbMetric: 18.3695

Epoch 82: val_loss did not improve from 18.33120
196/196 - 64s - loss: 18.4562 - MinusLogProbMetric: 18.4562 - val_loss: 18.3695 - val_MinusLogProbMetric: 18.3695 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 83/1000
2023-09-27 18:04:47.696 
Epoch 83/1000 
	 loss: 18.3443, MinusLogProbMetric: 18.3443, val_loss: 18.9428, val_MinusLogProbMetric: 18.9428

Epoch 83: val_loss did not improve from 18.33120
196/196 - 65s - loss: 18.3443 - MinusLogProbMetric: 18.3443 - val_loss: 18.9428 - val_MinusLogProbMetric: 18.9428 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 84/1000
2023-09-27 18:05:52.507 
Epoch 84/1000 
	 loss: 18.4286, MinusLogProbMetric: 18.4286, val_loss: 18.5093, val_MinusLogProbMetric: 18.5093

Epoch 84: val_loss did not improve from 18.33120
196/196 - 65s - loss: 18.4286 - MinusLogProbMetric: 18.4286 - val_loss: 18.5093 - val_MinusLogProbMetric: 18.5093 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 85/1000
2023-09-27 18:06:57.048 
Epoch 85/1000 
	 loss: 18.4541, MinusLogProbMetric: 18.4541, val_loss: 18.6896, val_MinusLogProbMetric: 18.6896

Epoch 85: val_loss did not improve from 18.33120
196/196 - 65s - loss: 18.4541 - MinusLogProbMetric: 18.4541 - val_loss: 18.6896 - val_MinusLogProbMetric: 18.6896 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 86/1000
2023-09-27 18:08:01.682 
Epoch 86/1000 
	 loss: 18.3821, MinusLogProbMetric: 18.3821, val_loss: 18.1100, val_MinusLogProbMetric: 18.1100

Epoch 86: val_loss improved from 18.33120 to 18.11000, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 66s - loss: 18.3821 - MinusLogProbMetric: 18.3821 - val_loss: 18.1100 - val_MinusLogProbMetric: 18.1100 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 87/1000
2023-09-27 18:09:08.822 
Epoch 87/1000 
	 loss: 18.4542, MinusLogProbMetric: 18.4542, val_loss: 18.8283, val_MinusLogProbMetric: 18.8283

Epoch 87: val_loss did not improve from 18.11000
196/196 - 66s - loss: 18.4542 - MinusLogProbMetric: 18.4542 - val_loss: 18.8283 - val_MinusLogProbMetric: 18.8283 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 88/1000
2023-09-27 18:10:14.319 
Epoch 88/1000 
	 loss: 18.3261, MinusLogProbMetric: 18.3261, val_loss: 18.4070, val_MinusLogProbMetric: 18.4070

Epoch 88: val_loss did not improve from 18.11000
196/196 - 65s - loss: 18.3261 - MinusLogProbMetric: 18.3261 - val_loss: 18.4070 - val_MinusLogProbMetric: 18.4070 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 89/1000
2023-09-27 18:11:19.308 
Epoch 89/1000 
	 loss: 18.3716, MinusLogProbMetric: 18.3716, val_loss: 18.5973, val_MinusLogProbMetric: 18.5973

Epoch 89: val_loss did not improve from 18.11000
196/196 - 65s - loss: 18.3716 - MinusLogProbMetric: 18.3716 - val_loss: 18.5973 - val_MinusLogProbMetric: 18.5973 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 90/1000
2023-09-27 18:12:23.559 
Epoch 90/1000 
	 loss: 18.3311, MinusLogProbMetric: 18.3311, val_loss: 18.3778, val_MinusLogProbMetric: 18.3778

Epoch 90: val_loss did not improve from 18.11000
196/196 - 64s - loss: 18.3311 - MinusLogProbMetric: 18.3311 - val_loss: 18.3778 - val_MinusLogProbMetric: 18.3778 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 91/1000
2023-09-27 18:13:28.803 
Epoch 91/1000 
	 loss: 18.3044, MinusLogProbMetric: 18.3044, val_loss: 18.1753, val_MinusLogProbMetric: 18.1753

Epoch 91: val_loss did not improve from 18.11000
196/196 - 65s - loss: 18.3044 - MinusLogProbMetric: 18.3044 - val_loss: 18.1753 - val_MinusLogProbMetric: 18.1753 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 92/1000
2023-09-27 18:14:33.301 
Epoch 92/1000 
	 loss: 18.3467, MinusLogProbMetric: 18.3467, val_loss: 18.5304, val_MinusLogProbMetric: 18.5304

Epoch 92: val_loss did not improve from 18.11000
196/196 - 64s - loss: 18.3467 - MinusLogProbMetric: 18.3467 - val_loss: 18.5304 - val_MinusLogProbMetric: 18.5304 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 93/1000
2023-09-27 18:15:38.693 
Epoch 93/1000 
	 loss: 18.3433, MinusLogProbMetric: 18.3433, val_loss: 18.4102, val_MinusLogProbMetric: 18.4102

Epoch 93: val_loss did not improve from 18.11000
196/196 - 65s - loss: 18.3433 - MinusLogProbMetric: 18.3433 - val_loss: 18.4102 - val_MinusLogProbMetric: 18.4102 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 94/1000
2023-09-27 18:16:44.021 
Epoch 94/1000 
	 loss: 18.2254, MinusLogProbMetric: 18.2254, val_loss: 18.6638, val_MinusLogProbMetric: 18.6638

Epoch 94: val_loss did not improve from 18.11000
196/196 - 65s - loss: 18.2254 - MinusLogProbMetric: 18.2254 - val_loss: 18.6638 - val_MinusLogProbMetric: 18.6638 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 95/1000
2023-09-27 18:17:49.897 
Epoch 95/1000 
	 loss: 18.2563, MinusLogProbMetric: 18.2563, val_loss: 18.2539, val_MinusLogProbMetric: 18.2539

Epoch 95: val_loss did not improve from 18.11000
196/196 - 66s - loss: 18.2563 - MinusLogProbMetric: 18.2563 - val_loss: 18.2539 - val_MinusLogProbMetric: 18.2539 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 96/1000
2023-09-27 18:18:55.027 
Epoch 96/1000 
	 loss: 18.2434, MinusLogProbMetric: 18.2434, val_loss: 18.0800, val_MinusLogProbMetric: 18.0800

Epoch 96: val_loss improved from 18.11000 to 18.08001, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 66s - loss: 18.2434 - MinusLogProbMetric: 18.2434 - val_loss: 18.0800 - val_MinusLogProbMetric: 18.0800 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 97/1000
2023-09-27 18:20:01.111 
Epoch 97/1000 
	 loss: 18.2568, MinusLogProbMetric: 18.2568, val_loss: 19.2013, val_MinusLogProbMetric: 19.2013

Epoch 97: val_loss did not improve from 18.08001
196/196 - 65s - loss: 18.2568 - MinusLogProbMetric: 18.2568 - val_loss: 19.2013 - val_MinusLogProbMetric: 19.2013 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 98/1000
2023-09-27 18:21:05.795 
Epoch 98/1000 
	 loss: 18.3294, MinusLogProbMetric: 18.3294, val_loss: 18.3640, val_MinusLogProbMetric: 18.3640

Epoch 98: val_loss did not improve from 18.08001
196/196 - 65s - loss: 18.3294 - MinusLogProbMetric: 18.3294 - val_loss: 18.3640 - val_MinusLogProbMetric: 18.3640 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 99/1000
2023-09-27 18:22:10.637 
Epoch 99/1000 
	 loss: 18.2017, MinusLogProbMetric: 18.2017, val_loss: 18.6253, val_MinusLogProbMetric: 18.6253

Epoch 99: val_loss did not improve from 18.08001
196/196 - 65s - loss: 18.2017 - MinusLogProbMetric: 18.2017 - val_loss: 18.6253 - val_MinusLogProbMetric: 18.6253 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 100/1000
2023-09-27 18:23:15.429 
Epoch 100/1000 
	 loss: 18.2927, MinusLogProbMetric: 18.2927, val_loss: 18.1044, val_MinusLogProbMetric: 18.1044

Epoch 100: val_loss did not improve from 18.08001
196/196 - 65s - loss: 18.2927 - MinusLogProbMetric: 18.2927 - val_loss: 18.1044 - val_MinusLogProbMetric: 18.1044 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 101/1000
2023-09-27 18:24:21.106 
Epoch 101/1000 
	 loss: 18.2459, MinusLogProbMetric: 18.2459, val_loss: 18.9448, val_MinusLogProbMetric: 18.9448

Epoch 101: val_loss did not improve from 18.08001
196/196 - 66s - loss: 18.2459 - MinusLogProbMetric: 18.2459 - val_loss: 18.9448 - val_MinusLogProbMetric: 18.9448 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 102/1000
2023-09-27 18:25:24.680 
Epoch 102/1000 
	 loss: 18.2450, MinusLogProbMetric: 18.2450, val_loss: 18.1630, val_MinusLogProbMetric: 18.1630

Epoch 102: val_loss did not improve from 18.08001
196/196 - 64s - loss: 18.2450 - MinusLogProbMetric: 18.2450 - val_loss: 18.1630 - val_MinusLogProbMetric: 18.1630 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 103/1000
2023-09-27 18:26:29.852 
Epoch 103/1000 
	 loss: 18.2800, MinusLogProbMetric: 18.2800, val_loss: 18.2251, val_MinusLogProbMetric: 18.2251

Epoch 103: val_loss did not improve from 18.08001
196/196 - 65s - loss: 18.2800 - MinusLogProbMetric: 18.2800 - val_loss: 18.2251 - val_MinusLogProbMetric: 18.2251 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 104/1000
2023-09-27 18:27:34.858 
Epoch 104/1000 
	 loss: 18.2348, MinusLogProbMetric: 18.2348, val_loss: 18.1488, val_MinusLogProbMetric: 18.1488

Epoch 104: val_loss did not improve from 18.08001
196/196 - 65s - loss: 18.2348 - MinusLogProbMetric: 18.2348 - val_loss: 18.1488 - val_MinusLogProbMetric: 18.1488 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 105/1000
2023-09-27 18:28:39.355 
Epoch 105/1000 
	 loss: 18.0954, MinusLogProbMetric: 18.0954, val_loss: 18.5098, val_MinusLogProbMetric: 18.5098

Epoch 105: val_loss did not improve from 18.08001
196/196 - 64s - loss: 18.0954 - MinusLogProbMetric: 18.0954 - val_loss: 18.5098 - val_MinusLogProbMetric: 18.5098 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 106/1000
2023-09-27 18:29:44.011 
Epoch 106/1000 
	 loss: 18.0768, MinusLogProbMetric: 18.0768, val_loss: 18.0058, val_MinusLogProbMetric: 18.0058

Epoch 106: val_loss improved from 18.08001 to 18.00576, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 66s - loss: 18.0768 - MinusLogProbMetric: 18.0768 - val_loss: 18.0058 - val_MinusLogProbMetric: 18.0058 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 107/1000
2023-09-27 18:30:50.037 
Epoch 107/1000 
	 loss: 18.1916, MinusLogProbMetric: 18.1916, val_loss: 18.2812, val_MinusLogProbMetric: 18.2812

Epoch 107: val_loss did not improve from 18.00576
196/196 - 65s - loss: 18.1916 - MinusLogProbMetric: 18.1916 - val_loss: 18.2812 - val_MinusLogProbMetric: 18.2812 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 108/1000
2023-09-27 18:31:54.328 
Epoch 108/1000 
	 loss: 18.1243, MinusLogProbMetric: 18.1243, val_loss: 18.0383, val_MinusLogProbMetric: 18.0383

Epoch 108: val_loss did not improve from 18.00576
196/196 - 64s - loss: 18.1243 - MinusLogProbMetric: 18.1243 - val_loss: 18.0383 - val_MinusLogProbMetric: 18.0383 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 109/1000
2023-09-27 18:32:59.643 
Epoch 109/1000 
	 loss: 18.0684, MinusLogProbMetric: 18.0684, val_loss: 18.6672, val_MinusLogProbMetric: 18.6672

Epoch 109: val_loss did not improve from 18.00576
196/196 - 65s - loss: 18.0684 - MinusLogProbMetric: 18.0684 - val_loss: 18.6672 - val_MinusLogProbMetric: 18.6672 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 110/1000
2023-09-27 18:34:03.094 
Epoch 110/1000 
	 loss: 18.3844, MinusLogProbMetric: 18.3844, val_loss: 18.6826, val_MinusLogProbMetric: 18.6826

Epoch 110: val_loss did not improve from 18.00576
196/196 - 63s - loss: 18.3844 - MinusLogProbMetric: 18.3844 - val_loss: 18.6826 - val_MinusLogProbMetric: 18.6826 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 111/1000
2023-09-27 18:35:03.406 
Epoch 111/1000 
	 loss: 18.0610, MinusLogProbMetric: 18.0610, val_loss: 18.1927, val_MinusLogProbMetric: 18.1927

Epoch 111: val_loss did not improve from 18.00576
196/196 - 60s - loss: 18.0610 - MinusLogProbMetric: 18.0610 - val_loss: 18.1927 - val_MinusLogProbMetric: 18.1927 - lr: 3.3333e-04 - 60s/epoch - 308ms/step
Epoch 112/1000
2023-09-27 18:36:00.298 
Epoch 112/1000 
	 loss: 18.1601, MinusLogProbMetric: 18.1601, val_loss: 19.3137, val_MinusLogProbMetric: 19.3137

Epoch 112: val_loss did not improve from 18.00576
196/196 - 57s - loss: 18.1601 - MinusLogProbMetric: 18.1601 - val_loss: 19.3137 - val_MinusLogProbMetric: 19.3137 - lr: 3.3333e-04 - 57s/epoch - 290ms/step
Epoch 113/1000
2023-09-27 18:37:05.476 
Epoch 113/1000 
	 loss: 18.1427, MinusLogProbMetric: 18.1427, val_loss: 18.4292, val_MinusLogProbMetric: 18.4292

Epoch 113: val_loss did not improve from 18.00576
196/196 - 65s - loss: 18.1427 - MinusLogProbMetric: 18.1427 - val_loss: 18.4292 - val_MinusLogProbMetric: 18.4292 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 114/1000
2023-09-27 18:38:05.013 
Epoch 114/1000 
	 loss: 18.1718, MinusLogProbMetric: 18.1718, val_loss: 19.1225, val_MinusLogProbMetric: 19.1225

Epoch 114: val_loss did not improve from 18.00576
196/196 - 60s - loss: 18.1718 - MinusLogProbMetric: 18.1718 - val_loss: 19.1225 - val_MinusLogProbMetric: 19.1225 - lr: 3.3333e-04 - 60s/epoch - 304ms/step
Epoch 115/1000
2023-09-27 18:39:01.444 
Epoch 115/1000 
	 loss: 18.1449, MinusLogProbMetric: 18.1449, val_loss: 18.4524, val_MinusLogProbMetric: 18.4524

Epoch 115: val_loss did not improve from 18.00576
196/196 - 56s - loss: 18.1449 - MinusLogProbMetric: 18.1449 - val_loss: 18.4524 - val_MinusLogProbMetric: 18.4524 - lr: 3.3333e-04 - 56s/epoch - 288ms/step
Epoch 116/1000
2023-09-27 18:40:07.019 
Epoch 116/1000 
	 loss: 18.0933, MinusLogProbMetric: 18.0933, val_loss: 18.6000, val_MinusLogProbMetric: 18.6000

Epoch 116: val_loss did not improve from 18.00576
196/196 - 66s - loss: 18.0933 - MinusLogProbMetric: 18.0933 - val_loss: 18.6000 - val_MinusLogProbMetric: 18.6000 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 117/1000
2023-09-27 18:41:15.040 
Epoch 117/1000 
	 loss: 18.0749, MinusLogProbMetric: 18.0749, val_loss: 17.9798, val_MinusLogProbMetric: 17.9798

Epoch 117: val_loss improved from 18.00576 to 17.97983, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 69s - loss: 18.0749 - MinusLogProbMetric: 18.0749 - val_loss: 17.9798 - val_MinusLogProbMetric: 17.9798 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 118/1000
2023-09-27 18:42:24.271 
Epoch 118/1000 
	 loss: 18.2585, MinusLogProbMetric: 18.2585, val_loss: 18.2236, val_MinusLogProbMetric: 18.2236

Epoch 118: val_loss did not improve from 17.97983
196/196 - 68s - loss: 18.2585 - MinusLogProbMetric: 18.2585 - val_loss: 18.2236 - val_MinusLogProbMetric: 18.2236 - lr: 3.3333e-04 - 68s/epoch - 348ms/step
Epoch 119/1000
2023-09-27 18:43:33.815 
Epoch 119/1000 
	 loss: 18.0825, MinusLogProbMetric: 18.0825, val_loss: 18.7456, val_MinusLogProbMetric: 18.7456

Epoch 119: val_loss did not improve from 17.97983
196/196 - 70s - loss: 18.0825 - MinusLogProbMetric: 18.0825 - val_loss: 18.7456 - val_MinusLogProbMetric: 18.7456 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 120/1000
2023-09-27 18:44:42.753 
Epoch 120/1000 
	 loss: 18.0560, MinusLogProbMetric: 18.0560, val_loss: 18.0780, val_MinusLogProbMetric: 18.0780

Epoch 120: val_loss did not improve from 17.97983
196/196 - 69s - loss: 18.0560 - MinusLogProbMetric: 18.0560 - val_loss: 18.0780 - val_MinusLogProbMetric: 18.0780 - lr: 3.3333e-04 - 69s/epoch - 352ms/step
Epoch 121/1000
2023-09-27 18:45:52.114 
Epoch 121/1000 
	 loss: 18.0926, MinusLogProbMetric: 18.0926, val_loss: 20.2839, val_MinusLogProbMetric: 20.2839

Epoch 121: val_loss did not improve from 17.97983
196/196 - 69s - loss: 18.0926 - MinusLogProbMetric: 18.0926 - val_loss: 20.2839 - val_MinusLogProbMetric: 20.2839 - lr: 3.3333e-04 - 69s/epoch - 354ms/step
Epoch 122/1000
2023-09-27 18:47:01.821 
Epoch 122/1000 
	 loss: 17.9768, MinusLogProbMetric: 17.9768, val_loss: 18.6695, val_MinusLogProbMetric: 18.6695

Epoch 122: val_loss did not improve from 17.97983
196/196 - 70s - loss: 17.9768 - MinusLogProbMetric: 17.9768 - val_loss: 18.6695 - val_MinusLogProbMetric: 18.6695 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 123/1000
2023-09-27 18:48:11.143 
Epoch 123/1000 
	 loss: 17.9434, MinusLogProbMetric: 17.9434, val_loss: 18.1126, val_MinusLogProbMetric: 18.1126

Epoch 123: val_loss did not improve from 17.97983
196/196 - 69s - loss: 17.9434 - MinusLogProbMetric: 17.9434 - val_loss: 18.1126 - val_MinusLogProbMetric: 18.1126 - lr: 3.3333e-04 - 69s/epoch - 354ms/step
Epoch 124/1000
2023-09-27 18:49:20.590 
Epoch 124/1000 
	 loss: 18.2043, MinusLogProbMetric: 18.2043, val_loss: 17.9973, val_MinusLogProbMetric: 17.9973

Epoch 124: val_loss did not improve from 17.97983
196/196 - 69s - loss: 18.2043 - MinusLogProbMetric: 18.2043 - val_loss: 17.9973 - val_MinusLogProbMetric: 17.9973 - lr: 3.3333e-04 - 69s/epoch - 354ms/step
Epoch 125/1000
2023-09-27 18:50:29.587 
Epoch 125/1000 
	 loss: 17.9691, MinusLogProbMetric: 17.9691, val_loss: 18.7910, val_MinusLogProbMetric: 18.7910

Epoch 125: val_loss did not improve from 17.97983
196/196 - 69s - loss: 17.9691 - MinusLogProbMetric: 17.9691 - val_loss: 18.7910 - val_MinusLogProbMetric: 18.7910 - lr: 3.3333e-04 - 69s/epoch - 352ms/step
Epoch 126/1000
2023-09-27 18:51:38.975 
Epoch 126/1000 
	 loss: 18.0137, MinusLogProbMetric: 18.0137, val_loss: 19.6577, val_MinusLogProbMetric: 19.6577

Epoch 126: val_loss did not improve from 17.97983
196/196 - 69s - loss: 18.0137 - MinusLogProbMetric: 18.0137 - val_loss: 19.6577 - val_MinusLogProbMetric: 19.6577 - lr: 3.3333e-04 - 69s/epoch - 354ms/step
Epoch 127/1000
2023-09-27 18:52:48.841 
Epoch 127/1000 
	 loss: 17.9164, MinusLogProbMetric: 17.9164, val_loss: 18.1012, val_MinusLogProbMetric: 18.1012

Epoch 127: val_loss did not improve from 17.97983
196/196 - 70s - loss: 17.9164 - MinusLogProbMetric: 17.9164 - val_loss: 18.1012 - val_MinusLogProbMetric: 18.1012 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 128/1000
2023-09-27 18:53:58.046 
Epoch 128/1000 
	 loss: 17.9036, MinusLogProbMetric: 17.9036, val_loss: 18.5768, val_MinusLogProbMetric: 18.5768

Epoch 128: val_loss did not improve from 17.97983
196/196 - 69s - loss: 17.9036 - MinusLogProbMetric: 17.9036 - val_loss: 18.5768 - val_MinusLogProbMetric: 18.5768 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 129/1000
2023-09-27 18:55:07.742 
Epoch 129/1000 
	 loss: 18.0904, MinusLogProbMetric: 18.0904, val_loss: 17.9400, val_MinusLogProbMetric: 17.9400

Epoch 129: val_loss improved from 17.97983 to 17.94003, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 71s - loss: 18.0904 - MinusLogProbMetric: 18.0904 - val_loss: 17.9400 - val_MinusLogProbMetric: 17.9400 - lr: 3.3333e-04 - 71s/epoch - 361ms/step
Epoch 130/1000
2023-09-27 18:56:18.273 
Epoch 130/1000 
	 loss: 18.0330, MinusLogProbMetric: 18.0330, val_loss: 18.1664, val_MinusLogProbMetric: 18.1664

Epoch 130: val_loss did not improve from 17.94003
196/196 - 70s - loss: 18.0330 - MinusLogProbMetric: 18.0330 - val_loss: 18.1664 - val_MinusLogProbMetric: 18.1664 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 131/1000
2023-09-27 18:57:27.882 
Epoch 131/1000 
	 loss: 17.9880, MinusLogProbMetric: 17.9880, val_loss: 19.2451, val_MinusLogProbMetric: 19.2451

Epoch 131: val_loss did not improve from 17.94003
196/196 - 70s - loss: 17.9880 - MinusLogProbMetric: 17.9880 - val_loss: 19.2451 - val_MinusLogProbMetric: 19.2451 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 132/1000
2023-09-27 18:58:36.890 
Epoch 132/1000 
	 loss: 17.8617, MinusLogProbMetric: 17.8617, val_loss: 17.8659, val_MinusLogProbMetric: 17.8659

Epoch 132: val_loss improved from 17.94003 to 17.86593, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 70s - loss: 17.8617 - MinusLogProbMetric: 17.8617 - val_loss: 17.8659 - val_MinusLogProbMetric: 17.8659 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 133/1000
2023-09-27 18:59:47.204 
Epoch 133/1000 
	 loss: 17.9437, MinusLogProbMetric: 17.9437, val_loss: 18.5139, val_MinusLogProbMetric: 18.5139

Epoch 133: val_loss did not improve from 17.86593
196/196 - 69s - loss: 17.9437 - MinusLogProbMetric: 17.9437 - val_loss: 18.5139 - val_MinusLogProbMetric: 18.5139 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 134/1000
2023-09-27 19:00:56.816 
Epoch 134/1000 
	 loss: 17.9320, MinusLogProbMetric: 17.9320, val_loss: 17.8733, val_MinusLogProbMetric: 17.8733

Epoch 134: val_loss did not improve from 17.86593
196/196 - 70s - loss: 17.9320 - MinusLogProbMetric: 17.9320 - val_loss: 17.8733 - val_MinusLogProbMetric: 17.8733 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 135/1000
2023-09-27 19:02:06.024 
Epoch 135/1000 
	 loss: 17.9142, MinusLogProbMetric: 17.9142, val_loss: 18.1031, val_MinusLogProbMetric: 18.1031

Epoch 135: val_loss did not improve from 17.86593
196/196 - 69s - loss: 17.9142 - MinusLogProbMetric: 17.9142 - val_loss: 18.1031 - val_MinusLogProbMetric: 18.1031 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 136/1000
2023-09-27 19:03:15.600 
Epoch 136/1000 
	 loss: 18.0007, MinusLogProbMetric: 18.0007, val_loss: 18.7219, val_MinusLogProbMetric: 18.7219

Epoch 136: val_loss did not improve from 17.86593
196/196 - 70s - loss: 18.0007 - MinusLogProbMetric: 18.0007 - val_loss: 18.7219 - val_MinusLogProbMetric: 18.7219 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 137/1000
2023-09-27 19:04:24.307 
Epoch 137/1000 
	 loss: 17.8367, MinusLogProbMetric: 17.8367, val_loss: 18.6986, val_MinusLogProbMetric: 18.6986

Epoch 137: val_loss did not improve from 17.86593
196/196 - 69s - loss: 17.8367 - MinusLogProbMetric: 17.8367 - val_loss: 18.6986 - val_MinusLogProbMetric: 18.6986 - lr: 3.3333e-04 - 69s/epoch - 351ms/step
Epoch 138/1000
2023-09-27 19:05:33.697 
Epoch 138/1000 
	 loss: 17.8127, MinusLogProbMetric: 17.8127, val_loss: 18.4864, val_MinusLogProbMetric: 18.4864

Epoch 138: val_loss did not improve from 17.86593
196/196 - 69s - loss: 17.8127 - MinusLogProbMetric: 17.8127 - val_loss: 18.4864 - val_MinusLogProbMetric: 18.4864 - lr: 3.3333e-04 - 69s/epoch - 354ms/step
Epoch 139/1000
2023-09-27 19:06:42.488 
Epoch 139/1000 
	 loss: 17.8627, MinusLogProbMetric: 17.8627, val_loss: 18.2604, val_MinusLogProbMetric: 18.2604

Epoch 139: val_loss did not improve from 17.86593
196/196 - 69s - loss: 17.8627 - MinusLogProbMetric: 17.8627 - val_loss: 18.2604 - val_MinusLogProbMetric: 18.2604 - lr: 3.3333e-04 - 69s/epoch - 351ms/step
Epoch 140/1000
2023-09-27 19:07:51.969 
Epoch 140/1000 
	 loss: 17.9805, MinusLogProbMetric: 17.9805, val_loss: 19.1166, val_MinusLogProbMetric: 19.1166

Epoch 140: val_loss did not improve from 17.86593
196/196 - 69s - loss: 17.9805 - MinusLogProbMetric: 17.9805 - val_loss: 19.1166 - val_MinusLogProbMetric: 19.1166 - lr: 3.3333e-04 - 69s/epoch - 354ms/step
Epoch 141/1000
2023-09-27 19:09:01.592 
Epoch 141/1000 
	 loss: 17.8844, MinusLogProbMetric: 17.8844, val_loss: 18.5696, val_MinusLogProbMetric: 18.5696

Epoch 141: val_loss did not improve from 17.86593
196/196 - 70s - loss: 17.8844 - MinusLogProbMetric: 17.8844 - val_loss: 18.5696 - val_MinusLogProbMetric: 18.5696 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 142/1000
2023-09-27 19:10:11.891 
Epoch 142/1000 
	 loss: 17.8796, MinusLogProbMetric: 17.8796, val_loss: 18.0879, val_MinusLogProbMetric: 18.0879

Epoch 142: val_loss did not improve from 17.86593
196/196 - 70s - loss: 17.8796 - MinusLogProbMetric: 17.8796 - val_loss: 18.0879 - val_MinusLogProbMetric: 18.0879 - lr: 3.3333e-04 - 70s/epoch - 359ms/step
Epoch 143/1000
2023-09-27 19:11:21.622 
Epoch 143/1000 
	 loss: 17.8501, MinusLogProbMetric: 17.8501, val_loss: 18.1177, val_MinusLogProbMetric: 18.1177

Epoch 143: val_loss did not improve from 17.86593
196/196 - 70s - loss: 17.8501 - MinusLogProbMetric: 17.8501 - val_loss: 18.1177 - val_MinusLogProbMetric: 18.1177 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 144/1000
2023-09-27 19:12:30.900 
Epoch 144/1000 
	 loss: 17.8242, MinusLogProbMetric: 17.8242, val_loss: 18.2199, val_MinusLogProbMetric: 18.2199

Epoch 144: val_loss did not improve from 17.86593
196/196 - 69s - loss: 17.8242 - MinusLogProbMetric: 17.8242 - val_loss: 18.2199 - val_MinusLogProbMetric: 18.2199 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 145/1000
2023-09-27 19:13:40.256 
Epoch 145/1000 
	 loss: 17.8771, MinusLogProbMetric: 17.8771, val_loss: 18.0058, val_MinusLogProbMetric: 18.0058

Epoch 145: val_loss did not improve from 17.86593
196/196 - 69s - loss: 17.8771 - MinusLogProbMetric: 17.8771 - val_loss: 18.0058 - val_MinusLogProbMetric: 18.0058 - lr: 3.3333e-04 - 69s/epoch - 354ms/step
Epoch 146/1000
2023-09-27 19:14:49.393 
Epoch 146/1000 
	 loss: 17.8813, MinusLogProbMetric: 17.8813, val_loss: 18.1607, val_MinusLogProbMetric: 18.1607

Epoch 146: val_loss did not improve from 17.86593
196/196 - 69s - loss: 17.8813 - MinusLogProbMetric: 17.8813 - val_loss: 18.1607 - val_MinusLogProbMetric: 18.1607 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 147/1000
2023-09-27 19:15:59.133 
Epoch 147/1000 
	 loss: 17.8453, MinusLogProbMetric: 17.8453, val_loss: 17.8770, val_MinusLogProbMetric: 17.8770

Epoch 147: val_loss did not improve from 17.86593
196/196 - 70s - loss: 17.8453 - MinusLogProbMetric: 17.8453 - val_loss: 17.8770 - val_MinusLogProbMetric: 17.8770 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 148/1000
2023-09-27 19:17:09.123 
Epoch 148/1000 
	 loss: 17.9705, MinusLogProbMetric: 17.9705, val_loss: 18.4325, val_MinusLogProbMetric: 18.4325

Epoch 148: val_loss did not improve from 17.86593
196/196 - 70s - loss: 17.9705 - MinusLogProbMetric: 17.9705 - val_loss: 18.4325 - val_MinusLogProbMetric: 18.4325 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 149/1000
2023-09-27 19:18:18.192 
Epoch 149/1000 
	 loss: 17.8255, MinusLogProbMetric: 17.8255, val_loss: 17.9793, val_MinusLogProbMetric: 17.9793

Epoch 149: val_loss did not improve from 17.86593
196/196 - 69s - loss: 17.8255 - MinusLogProbMetric: 17.8255 - val_loss: 17.9793 - val_MinusLogProbMetric: 17.9793 - lr: 3.3333e-04 - 69s/epoch - 352ms/step
Epoch 150/1000
2023-09-27 19:19:27.232 
Epoch 150/1000 
	 loss: 17.8701, MinusLogProbMetric: 17.8701, val_loss: 18.3653, val_MinusLogProbMetric: 18.3653

Epoch 150: val_loss did not improve from 17.86593
196/196 - 69s - loss: 17.8701 - MinusLogProbMetric: 17.8701 - val_loss: 18.3653 - val_MinusLogProbMetric: 18.3653 - lr: 3.3333e-04 - 69s/epoch - 352ms/step
Epoch 151/1000
2023-09-27 19:20:37.300 
Epoch 151/1000 
	 loss: 17.9101, MinusLogProbMetric: 17.9101, val_loss: 18.2774, val_MinusLogProbMetric: 18.2774

Epoch 151: val_loss did not improve from 17.86593
196/196 - 70s - loss: 17.9101 - MinusLogProbMetric: 17.9101 - val_loss: 18.2774 - val_MinusLogProbMetric: 18.2774 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 152/1000
2023-09-27 19:21:47.013 
Epoch 152/1000 
	 loss: 17.7529, MinusLogProbMetric: 17.7529, val_loss: 17.7454, val_MinusLogProbMetric: 17.7454

Epoch 152: val_loss improved from 17.86593 to 17.74541, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 71s - loss: 17.7529 - MinusLogProbMetric: 17.7529 - val_loss: 17.7454 - val_MinusLogProbMetric: 17.7454 - lr: 3.3333e-04 - 71s/epoch - 360ms/step
Epoch 153/1000
2023-09-27 19:22:57.353 
Epoch 153/1000 
	 loss: 17.7968, MinusLogProbMetric: 17.7968, val_loss: 18.6083, val_MinusLogProbMetric: 18.6083

Epoch 153: val_loss did not improve from 17.74541
196/196 - 69s - loss: 17.7968 - MinusLogProbMetric: 17.7968 - val_loss: 18.6083 - val_MinusLogProbMetric: 18.6083 - lr: 3.3333e-04 - 69s/epoch - 354ms/step
Epoch 154/1000
2023-09-27 19:24:06.881 
Epoch 154/1000 
	 loss: 17.7905, MinusLogProbMetric: 17.7905, val_loss: 17.7921, val_MinusLogProbMetric: 17.7921

Epoch 154: val_loss did not improve from 17.74541
196/196 - 70s - loss: 17.7905 - MinusLogProbMetric: 17.7905 - val_loss: 17.7921 - val_MinusLogProbMetric: 17.7921 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 155/1000
2023-09-27 19:25:15.637 
Epoch 155/1000 
	 loss: 17.7008, MinusLogProbMetric: 17.7008, val_loss: 18.1323, val_MinusLogProbMetric: 18.1323

Epoch 155: val_loss did not improve from 17.74541
196/196 - 69s - loss: 17.7008 - MinusLogProbMetric: 17.7008 - val_loss: 18.1323 - val_MinusLogProbMetric: 18.1323 - lr: 3.3333e-04 - 69s/epoch - 351ms/step
Epoch 156/1000
2023-09-27 19:26:22.999 
Epoch 156/1000 
	 loss: 17.7189, MinusLogProbMetric: 17.7189, val_loss: 18.1423, val_MinusLogProbMetric: 18.1423

Epoch 156: val_loss did not improve from 17.74541
196/196 - 67s - loss: 17.7189 - MinusLogProbMetric: 17.7189 - val_loss: 18.1423 - val_MinusLogProbMetric: 18.1423 - lr: 3.3333e-04 - 67s/epoch - 344ms/step
Epoch 157/1000
2023-09-27 19:27:26.739 
Epoch 157/1000 
	 loss: 17.9241, MinusLogProbMetric: 17.9241, val_loss: 17.9936, val_MinusLogProbMetric: 17.9936

Epoch 157: val_loss did not improve from 17.74541
196/196 - 64s - loss: 17.9241 - MinusLogProbMetric: 17.9241 - val_loss: 17.9936 - val_MinusLogProbMetric: 17.9936 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 158/1000
2023-09-27 19:28:32.679 
Epoch 158/1000 
	 loss: 17.7087, MinusLogProbMetric: 17.7087, val_loss: 17.7413, val_MinusLogProbMetric: 17.7413

Epoch 158: val_loss improved from 17.74541 to 17.74130, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 67s - loss: 17.7087 - MinusLogProbMetric: 17.7087 - val_loss: 17.7413 - val_MinusLogProbMetric: 17.7413 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 159/1000
2023-09-27 19:29:42.918 
Epoch 159/1000 
	 loss: 17.7577, MinusLogProbMetric: 17.7577, val_loss: 17.8001, val_MinusLogProbMetric: 17.8001

Epoch 159: val_loss did not improve from 17.74130
196/196 - 69s - loss: 17.7577 - MinusLogProbMetric: 17.7577 - val_loss: 17.8001 - val_MinusLogProbMetric: 17.8001 - lr: 3.3333e-04 - 69s/epoch - 354ms/step
Epoch 160/1000
2023-09-27 19:30:53.421 
Epoch 160/1000 
	 loss: 17.7820, MinusLogProbMetric: 17.7820, val_loss: 18.2802, val_MinusLogProbMetric: 18.2802

Epoch 160: val_loss did not improve from 17.74130
196/196 - 70s - loss: 17.7820 - MinusLogProbMetric: 17.7820 - val_loss: 18.2802 - val_MinusLogProbMetric: 18.2802 - lr: 3.3333e-04 - 70s/epoch - 360ms/step
Epoch 161/1000
2023-09-27 19:32:03.612 
Epoch 161/1000 
	 loss: 17.6520, MinusLogProbMetric: 17.6520, val_loss: 17.5925, val_MinusLogProbMetric: 17.5925

Epoch 161: val_loss improved from 17.74130 to 17.59245, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 71s - loss: 17.6520 - MinusLogProbMetric: 17.6520 - val_loss: 17.5925 - val_MinusLogProbMetric: 17.5925 - lr: 3.3333e-04 - 71s/epoch - 364ms/step
Epoch 162/1000
2023-09-27 19:33:14.348 
Epoch 162/1000 
	 loss: 17.7212, MinusLogProbMetric: 17.7212, val_loss: 18.0899, val_MinusLogProbMetric: 18.0899

Epoch 162: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.7212 - MinusLogProbMetric: 17.7212 - val_loss: 18.0899 - val_MinusLogProbMetric: 18.0899 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 163/1000
2023-09-27 19:34:24.237 
Epoch 163/1000 
	 loss: 17.7717, MinusLogProbMetric: 17.7717, val_loss: 18.4766, val_MinusLogProbMetric: 18.4766

Epoch 163: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.7717 - MinusLogProbMetric: 17.7717 - val_loss: 18.4766 - val_MinusLogProbMetric: 18.4766 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 164/1000
2023-09-27 19:35:34.172 
Epoch 164/1000 
	 loss: 17.7823, MinusLogProbMetric: 17.7823, val_loss: 17.9424, val_MinusLogProbMetric: 17.9424

Epoch 164: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.7823 - MinusLogProbMetric: 17.7823 - val_loss: 17.9424 - val_MinusLogProbMetric: 17.9424 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 165/1000
2023-09-27 19:36:43.900 
Epoch 165/1000 
	 loss: 17.8347, MinusLogProbMetric: 17.8347, val_loss: 18.3269, val_MinusLogProbMetric: 18.3269

Epoch 165: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.8347 - MinusLogProbMetric: 17.8347 - val_loss: 18.3269 - val_MinusLogProbMetric: 18.3269 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 166/1000
2023-09-27 19:37:53.908 
Epoch 166/1000 
	 loss: 17.7012, MinusLogProbMetric: 17.7012, val_loss: 17.9736, val_MinusLogProbMetric: 17.9736

Epoch 166: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.7012 - MinusLogProbMetric: 17.7012 - val_loss: 17.9736 - val_MinusLogProbMetric: 17.9736 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 167/1000
2023-09-27 19:39:03.693 
Epoch 167/1000 
	 loss: 17.7175, MinusLogProbMetric: 17.7175, val_loss: 17.8236, val_MinusLogProbMetric: 17.8236

Epoch 167: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.7175 - MinusLogProbMetric: 17.7175 - val_loss: 17.8236 - val_MinusLogProbMetric: 17.8236 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 168/1000
2023-09-27 19:40:12.915 
Epoch 168/1000 
	 loss: 17.8023, MinusLogProbMetric: 17.8023, val_loss: 17.9124, val_MinusLogProbMetric: 17.9124

Epoch 168: val_loss did not improve from 17.59245
196/196 - 69s - loss: 17.8023 - MinusLogProbMetric: 17.8023 - val_loss: 17.9124 - val_MinusLogProbMetric: 17.9124 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 169/1000
2023-09-27 19:41:22.639 
Epoch 169/1000 
	 loss: 17.6259, MinusLogProbMetric: 17.6259, val_loss: 18.8938, val_MinusLogProbMetric: 18.8938

Epoch 169: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.6259 - MinusLogProbMetric: 17.6259 - val_loss: 18.8938 - val_MinusLogProbMetric: 18.8938 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 170/1000
2023-09-27 19:42:32.250 
Epoch 170/1000 
	 loss: 17.7715, MinusLogProbMetric: 17.7715, val_loss: 19.1510, val_MinusLogProbMetric: 19.1510

Epoch 170: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.7715 - MinusLogProbMetric: 17.7715 - val_loss: 19.1510 - val_MinusLogProbMetric: 19.1510 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 171/1000
2023-09-27 19:43:41.659 
Epoch 171/1000 
	 loss: 17.6470, MinusLogProbMetric: 17.6470, val_loss: 18.2319, val_MinusLogProbMetric: 18.2319

Epoch 171: val_loss did not improve from 17.59245
196/196 - 69s - loss: 17.6470 - MinusLogProbMetric: 17.6470 - val_loss: 18.2319 - val_MinusLogProbMetric: 18.2319 - lr: 3.3333e-04 - 69s/epoch - 354ms/step
Epoch 172/1000
2023-09-27 19:44:51.363 
Epoch 172/1000 
	 loss: 17.7085, MinusLogProbMetric: 17.7085, val_loss: 18.2000, val_MinusLogProbMetric: 18.2000

Epoch 172: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.7085 - MinusLogProbMetric: 17.7085 - val_loss: 18.2000 - val_MinusLogProbMetric: 18.2000 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 173/1000
2023-09-27 19:46:01.078 
Epoch 173/1000 
	 loss: 17.6220, MinusLogProbMetric: 17.6220, val_loss: 17.9080, val_MinusLogProbMetric: 17.9080

Epoch 173: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.6220 - MinusLogProbMetric: 17.6220 - val_loss: 17.9080 - val_MinusLogProbMetric: 17.9080 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 174/1000
2023-09-27 19:47:09.694 
Epoch 174/1000 
	 loss: 17.7381, MinusLogProbMetric: 17.7381, val_loss: 17.9578, val_MinusLogProbMetric: 17.9578

Epoch 174: val_loss did not improve from 17.59245
196/196 - 69s - loss: 17.7381 - MinusLogProbMetric: 17.7381 - val_loss: 17.9578 - val_MinusLogProbMetric: 17.9578 - lr: 3.3333e-04 - 69s/epoch - 350ms/step
Epoch 175/1000
2023-09-27 19:48:19.277 
Epoch 175/1000 
	 loss: 17.6748, MinusLogProbMetric: 17.6748, val_loss: 18.2203, val_MinusLogProbMetric: 18.2203

Epoch 175: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.6748 - MinusLogProbMetric: 17.6748 - val_loss: 18.2203 - val_MinusLogProbMetric: 18.2203 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 176/1000
2023-09-27 19:49:27.885 
Epoch 176/1000 
	 loss: 17.6375, MinusLogProbMetric: 17.6375, val_loss: 17.7469, val_MinusLogProbMetric: 17.7469

Epoch 176: val_loss did not improve from 17.59245
196/196 - 69s - loss: 17.6375 - MinusLogProbMetric: 17.6375 - val_loss: 17.7469 - val_MinusLogProbMetric: 17.7469 - lr: 3.3333e-04 - 69s/epoch - 350ms/step
Epoch 177/1000
2023-09-27 19:50:37.032 
Epoch 177/1000 
	 loss: 17.7220, MinusLogProbMetric: 17.7220, val_loss: 17.7207, val_MinusLogProbMetric: 17.7207

Epoch 177: val_loss did not improve from 17.59245
196/196 - 69s - loss: 17.7220 - MinusLogProbMetric: 17.7220 - val_loss: 17.7207 - val_MinusLogProbMetric: 17.7207 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 178/1000
2023-09-27 19:51:46.892 
Epoch 178/1000 
	 loss: 17.7217, MinusLogProbMetric: 17.7217, val_loss: 18.8462, val_MinusLogProbMetric: 18.8462

Epoch 178: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.7217 - MinusLogProbMetric: 17.7217 - val_loss: 18.8462 - val_MinusLogProbMetric: 18.8462 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 179/1000
2023-09-27 19:52:56.458 
Epoch 179/1000 
	 loss: 17.6544, MinusLogProbMetric: 17.6544, val_loss: 17.9572, val_MinusLogProbMetric: 17.9572

Epoch 179: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.6544 - MinusLogProbMetric: 17.6544 - val_loss: 17.9572 - val_MinusLogProbMetric: 17.9572 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 180/1000
2023-09-27 19:54:06.093 
Epoch 180/1000 
	 loss: 17.6567, MinusLogProbMetric: 17.6567, val_loss: 18.6969, val_MinusLogProbMetric: 18.6969

Epoch 180: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.6567 - MinusLogProbMetric: 17.6567 - val_loss: 18.6969 - val_MinusLogProbMetric: 18.6969 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 181/1000
2023-09-27 19:55:15.604 
Epoch 181/1000 
	 loss: 17.7437, MinusLogProbMetric: 17.7437, val_loss: 17.8404, val_MinusLogProbMetric: 17.8404

Epoch 181: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.7437 - MinusLogProbMetric: 17.7437 - val_loss: 17.8404 - val_MinusLogProbMetric: 17.8404 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 182/1000
2023-09-27 19:56:24.669 
Epoch 182/1000 
	 loss: 17.6077, MinusLogProbMetric: 17.6077, val_loss: 18.2691, val_MinusLogProbMetric: 18.2691

Epoch 182: val_loss did not improve from 17.59245
196/196 - 69s - loss: 17.6077 - MinusLogProbMetric: 17.6077 - val_loss: 18.2691 - val_MinusLogProbMetric: 18.2691 - lr: 3.3333e-04 - 69s/epoch - 352ms/step
Epoch 183/1000
2023-09-27 19:57:34.638 
Epoch 183/1000 
	 loss: 17.6485, MinusLogProbMetric: 17.6485, val_loss: 17.9738, val_MinusLogProbMetric: 17.9738

Epoch 183: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.6485 - MinusLogProbMetric: 17.6485 - val_loss: 17.9738 - val_MinusLogProbMetric: 17.9738 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 184/1000
2023-09-27 19:58:44.567 
Epoch 184/1000 
	 loss: 17.6868, MinusLogProbMetric: 17.6868, val_loss: 18.1383, val_MinusLogProbMetric: 18.1383

Epoch 184: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.6868 - MinusLogProbMetric: 17.6868 - val_loss: 18.1383 - val_MinusLogProbMetric: 18.1383 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 185/1000
2023-09-27 19:59:54.650 
Epoch 185/1000 
	 loss: 17.5466, MinusLogProbMetric: 17.5466, val_loss: 17.7394, val_MinusLogProbMetric: 17.7394

Epoch 185: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.5466 - MinusLogProbMetric: 17.5466 - val_loss: 17.7394 - val_MinusLogProbMetric: 17.7394 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 186/1000
2023-09-27 20:01:04.681 
Epoch 186/1000 
	 loss: 17.6136, MinusLogProbMetric: 17.6136, val_loss: 17.9531, val_MinusLogProbMetric: 17.9531

Epoch 186: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.6136 - MinusLogProbMetric: 17.6136 - val_loss: 17.9531 - val_MinusLogProbMetric: 17.9531 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 187/1000
2023-09-27 20:02:14.669 
Epoch 187/1000 
	 loss: 17.5997, MinusLogProbMetric: 17.5997, val_loss: 19.1070, val_MinusLogProbMetric: 19.1070

Epoch 187: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.5997 - MinusLogProbMetric: 17.5997 - val_loss: 19.1070 - val_MinusLogProbMetric: 19.1070 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 188/1000
2023-09-27 20:03:24.860 
Epoch 188/1000 
	 loss: 17.6433, MinusLogProbMetric: 17.6433, val_loss: 18.0714, val_MinusLogProbMetric: 18.0714

Epoch 188: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.6433 - MinusLogProbMetric: 17.6433 - val_loss: 18.0714 - val_MinusLogProbMetric: 18.0714 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 189/1000
2023-09-27 20:04:34.790 
Epoch 189/1000 
	 loss: 17.6972, MinusLogProbMetric: 17.6972, val_loss: 17.8279, val_MinusLogProbMetric: 17.8279

Epoch 189: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.6972 - MinusLogProbMetric: 17.6972 - val_loss: 17.8279 - val_MinusLogProbMetric: 17.8279 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 190/1000
2023-09-27 20:05:44.212 
Epoch 190/1000 
	 loss: 17.5947, MinusLogProbMetric: 17.5947, val_loss: 17.7672, val_MinusLogProbMetric: 17.7672

Epoch 190: val_loss did not improve from 17.59245
196/196 - 69s - loss: 17.5947 - MinusLogProbMetric: 17.5947 - val_loss: 17.7672 - val_MinusLogProbMetric: 17.7672 - lr: 3.3333e-04 - 69s/epoch - 354ms/step
Epoch 191/1000
2023-09-27 20:06:53.867 
Epoch 191/1000 
	 loss: 17.6015, MinusLogProbMetric: 17.6015, val_loss: 17.8692, val_MinusLogProbMetric: 17.8692

Epoch 191: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.6015 - MinusLogProbMetric: 17.6015 - val_loss: 17.8692 - val_MinusLogProbMetric: 17.8692 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 192/1000
2023-09-27 20:08:03.868 
Epoch 192/1000 
	 loss: 17.6307, MinusLogProbMetric: 17.6307, val_loss: 19.0618, val_MinusLogProbMetric: 19.0618

Epoch 192: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.6307 - MinusLogProbMetric: 17.6307 - val_loss: 19.0618 - val_MinusLogProbMetric: 19.0618 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 193/1000
2023-09-27 20:09:13.842 
Epoch 193/1000 
	 loss: 17.5987, MinusLogProbMetric: 17.5987, val_loss: 18.2051, val_MinusLogProbMetric: 18.2051

Epoch 193: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.5987 - MinusLogProbMetric: 17.5987 - val_loss: 18.2051 - val_MinusLogProbMetric: 18.2051 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 194/1000
2023-09-27 20:10:23.605 
Epoch 194/1000 
	 loss: 17.7209, MinusLogProbMetric: 17.7209, val_loss: 17.7987, val_MinusLogProbMetric: 17.7987

Epoch 194: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.7209 - MinusLogProbMetric: 17.7209 - val_loss: 17.7987 - val_MinusLogProbMetric: 17.7987 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 195/1000
2023-09-27 20:11:33.331 
Epoch 195/1000 
	 loss: 17.5802, MinusLogProbMetric: 17.5802, val_loss: 17.9018, val_MinusLogProbMetric: 17.9018

Epoch 195: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.5802 - MinusLogProbMetric: 17.5802 - val_loss: 17.9018 - val_MinusLogProbMetric: 17.9018 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 196/1000
2023-09-27 20:12:43.422 
Epoch 196/1000 
	 loss: 17.5741, MinusLogProbMetric: 17.5741, val_loss: 18.0022, val_MinusLogProbMetric: 18.0022

Epoch 196: val_loss did not improve from 17.59245
196/196 - 70s - loss: 17.5741 - MinusLogProbMetric: 17.5741 - val_loss: 18.0022 - val_MinusLogProbMetric: 18.0022 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 197/1000
2023-09-27 20:13:53.704 
Epoch 197/1000 
	 loss: 17.6316, MinusLogProbMetric: 17.6316, val_loss: 17.5752, val_MinusLogProbMetric: 17.5752

Epoch 197: val_loss improved from 17.59245 to 17.57515, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 71s - loss: 17.6316 - MinusLogProbMetric: 17.6316 - val_loss: 17.5752 - val_MinusLogProbMetric: 17.5752 - lr: 3.3333e-04 - 71s/epoch - 365ms/step
Epoch 198/1000
2023-09-27 20:15:05.235 
Epoch 198/1000 
	 loss: 17.5575, MinusLogProbMetric: 17.5575, val_loss: 18.1231, val_MinusLogProbMetric: 18.1231

Epoch 198: val_loss did not improve from 17.57515
196/196 - 70s - loss: 17.5575 - MinusLogProbMetric: 17.5575 - val_loss: 18.1231 - val_MinusLogProbMetric: 18.1231 - lr: 3.3333e-04 - 70s/epoch - 359ms/step
Epoch 199/1000
2023-09-27 20:16:15.285 
Epoch 199/1000 
	 loss: 17.5222, MinusLogProbMetric: 17.5222, val_loss: 17.6991, val_MinusLogProbMetric: 17.6991

Epoch 199: val_loss did not improve from 17.57515
196/196 - 70s - loss: 17.5222 - MinusLogProbMetric: 17.5222 - val_loss: 17.6991 - val_MinusLogProbMetric: 17.6991 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 200/1000
2023-09-27 20:17:25.076 
Epoch 200/1000 
	 loss: 17.7554, MinusLogProbMetric: 17.7554, val_loss: 17.7089, val_MinusLogProbMetric: 17.7089

Epoch 200: val_loss did not improve from 17.57515
196/196 - 70s - loss: 17.7554 - MinusLogProbMetric: 17.7554 - val_loss: 17.7089 - val_MinusLogProbMetric: 17.7089 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 201/1000
2023-09-27 20:18:34.619 
Epoch 201/1000 
	 loss: 17.5277, MinusLogProbMetric: 17.5277, val_loss: 18.1778, val_MinusLogProbMetric: 18.1778

Epoch 201: val_loss did not improve from 17.57515
196/196 - 70s - loss: 17.5277 - MinusLogProbMetric: 17.5277 - val_loss: 18.1778 - val_MinusLogProbMetric: 18.1778 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 202/1000
2023-09-27 20:19:44.438 
Epoch 202/1000 
	 loss: 17.5626, MinusLogProbMetric: 17.5626, val_loss: 18.3959, val_MinusLogProbMetric: 18.3959

Epoch 202: val_loss did not improve from 17.57515
196/196 - 70s - loss: 17.5626 - MinusLogProbMetric: 17.5626 - val_loss: 18.3959 - val_MinusLogProbMetric: 18.3959 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 203/1000
2023-09-27 20:20:54.269 
Epoch 203/1000 
	 loss: 17.6173, MinusLogProbMetric: 17.6173, val_loss: 17.9275, val_MinusLogProbMetric: 17.9275

Epoch 203: val_loss did not improve from 17.57515
196/196 - 70s - loss: 17.6173 - MinusLogProbMetric: 17.6173 - val_loss: 17.9275 - val_MinusLogProbMetric: 17.9275 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 204/1000
2023-09-27 20:22:03.730 
Epoch 204/1000 
	 loss: 17.5684, MinusLogProbMetric: 17.5684, val_loss: 18.0588, val_MinusLogProbMetric: 18.0588

Epoch 204: val_loss did not improve from 17.57515
196/196 - 69s - loss: 17.5684 - MinusLogProbMetric: 17.5684 - val_loss: 18.0588 - val_MinusLogProbMetric: 18.0588 - lr: 3.3333e-04 - 69s/epoch - 354ms/step
Epoch 205/1000
2023-09-27 20:23:13.650 
Epoch 205/1000 
	 loss: 17.5275, MinusLogProbMetric: 17.5275, val_loss: 18.1366, val_MinusLogProbMetric: 18.1366

Epoch 205: val_loss did not improve from 17.57515
196/196 - 70s - loss: 17.5275 - MinusLogProbMetric: 17.5275 - val_loss: 18.1366 - val_MinusLogProbMetric: 18.1366 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 206/1000
2023-09-27 20:24:23.861 
Epoch 206/1000 
	 loss: 17.5702, MinusLogProbMetric: 17.5702, val_loss: 18.0577, val_MinusLogProbMetric: 18.0577

Epoch 206: val_loss did not improve from 17.57515
196/196 - 70s - loss: 17.5702 - MinusLogProbMetric: 17.5702 - val_loss: 18.0577 - val_MinusLogProbMetric: 18.0577 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 207/1000
2023-09-27 20:25:33.797 
Epoch 207/1000 
	 loss: 17.4513, MinusLogProbMetric: 17.4513, val_loss: 17.7843, val_MinusLogProbMetric: 17.7843

Epoch 207: val_loss did not improve from 17.57515
196/196 - 70s - loss: 17.4513 - MinusLogProbMetric: 17.4513 - val_loss: 17.7843 - val_MinusLogProbMetric: 17.7843 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 208/1000
2023-09-27 20:26:44.057 
Epoch 208/1000 
	 loss: 17.5350, MinusLogProbMetric: 17.5350, val_loss: 17.9598, val_MinusLogProbMetric: 17.9598

Epoch 208: val_loss did not improve from 17.57515
196/196 - 70s - loss: 17.5350 - MinusLogProbMetric: 17.5350 - val_loss: 17.9598 - val_MinusLogProbMetric: 17.9598 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 209/1000
2023-09-27 20:27:54.196 
Epoch 209/1000 
	 loss: 17.5056, MinusLogProbMetric: 17.5056, val_loss: 17.6873, val_MinusLogProbMetric: 17.6873

Epoch 209: val_loss did not improve from 17.57515
196/196 - 70s - loss: 17.5056 - MinusLogProbMetric: 17.5056 - val_loss: 17.6873 - val_MinusLogProbMetric: 17.6873 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 210/1000
2023-09-27 20:29:04.341 
Epoch 210/1000 
	 loss: 17.4770, MinusLogProbMetric: 17.4770, val_loss: 18.6911, val_MinusLogProbMetric: 18.6911

Epoch 210: val_loss did not improve from 17.57515
196/196 - 70s - loss: 17.4770 - MinusLogProbMetric: 17.4770 - val_loss: 18.6911 - val_MinusLogProbMetric: 18.6911 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 211/1000
2023-09-27 20:30:14.314 
Epoch 211/1000 
	 loss: 17.6389, MinusLogProbMetric: 17.6389, val_loss: 17.8090, val_MinusLogProbMetric: 17.8090

Epoch 211: val_loss did not improve from 17.57515
196/196 - 70s - loss: 17.6389 - MinusLogProbMetric: 17.6389 - val_loss: 17.8090 - val_MinusLogProbMetric: 17.8090 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 212/1000
2023-09-27 20:31:24.009 
Epoch 212/1000 
	 loss: 17.5130, MinusLogProbMetric: 17.5130, val_loss: 17.9855, val_MinusLogProbMetric: 17.9855

Epoch 212: val_loss did not improve from 17.57515
196/196 - 70s - loss: 17.5130 - MinusLogProbMetric: 17.5130 - val_loss: 17.9855 - val_MinusLogProbMetric: 17.9855 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 213/1000
2023-09-27 20:32:33.847 
Epoch 213/1000 
	 loss: 17.5167, MinusLogProbMetric: 17.5167, val_loss: 17.9509, val_MinusLogProbMetric: 17.9509

Epoch 213: val_loss did not improve from 17.57515
196/196 - 70s - loss: 17.5167 - MinusLogProbMetric: 17.5167 - val_loss: 17.9509 - val_MinusLogProbMetric: 17.9509 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 214/1000
2023-09-27 20:33:43.515 
Epoch 214/1000 
	 loss: 17.5059, MinusLogProbMetric: 17.5059, val_loss: 17.6150, val_MinusLogProbMetric: 17.6150

Epoch 214: val_loss did not improve from 17.57515
196/196 - 70s - loss: 17.5059 - MinusLogProbMetric: 17.5059 - val_loss: 17.6150 - val_MinusLogProbMetric: 17.6150 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 215/1000
2023-09-27 20:34:53.337 
Epoch 215/1000 
	 loss: 17.6204, MinusLogProbMetric: 17.6204, val_loss: 17.5659, val_MinusLogProbMetric: 17.5659

Epoch 215: val_loss improved from 17.57515 to 17.56589, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 71s - loss: 17.6204 - MinusLogProbMetric: 17.6204 - val_loss: 17.5659 - val_MinusLogProbMetric: 17.5659 - lr: 3.3333e-04 - 71s/epoch - 361ms/step
Epoch 216/1000
2023-09-27 20:36:03.702 
Epoch 216/1000 
	 loss: 17.5180, MinusLogProbMetric: 17.5180, val_loss: 17.8629, val_MinusLogProbMetric: 17.8629

Epoch 216: val_loss did not improve from 17.56589
196/196 - 69s - loss: 17.5180 - MinusLogProbMetric: 17.5180 - val_loss: 17.8629 - val_MinusLogProbMetric: 17.8629 - lr: 3.3333e-04 - 69s/epoch - 354ms/step
Epoch 217/1000
2023-09-27 20:37:13.457 
Epoch 217/1000 
	 loss: 17.4289, MinusLogProbMetric: 17.4289, val_loss: 17.6889, val_MinusLogProbMetric: 17.6889

Epoch 217: val_loss did not improve from 17.56589
196/196 - 70s - loss: 17.4289 - MinusLogProbMetric: 17.4289 - val_loss: 17.6889 - val_MinusLogProbMetric: 17.6889 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 218/1000
2023-09-27 20:38:23.494 
Epoch 218/1000 
	 loss: 17.4849, MinusLogProbMetric: 17.4849, val_loss: 18.7323, val_MinusLogProbMetric: 18.7323

Epoch 218: val_loss did not improve from 17.56589
196/196 - 70s - loss: 17.4849 - MinusLogProbMetric: 17.4849 - val_loss: 18.7323 - val_MinusLogProbMetric: 18.7323 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 219/1000
2023-09-27 20:39:33.685 
Epoch 219/1000 
	 loss: 17.4681, MinusLogProbMetric: 17.4681, val_loss: 17.6905, val_MinusLogProbMetric: 17.6905

Epoch 219: val_loss did not improve from 17.56589
196/196 - 70s - loss: 17.4681 - MinusLogProbMetric: 17.4681 - val_loss: 17.6905 - val_MinusLogProbMetric: 17.6905 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 220/1000
2023-09-27 20:40:43.514 
Epoch 220/1000 
	 loss: 17.7289, MinusLogProbMetric: 17.7289, val_loss: 18.2491, val_MinusLogProbMetric: 18.2491

Epoch 220: val_loss did not improve from 17.56589
196/196 - 70s - loss: 17.7289 - MinusLogProbMetric: 17.7289 - val_loss: 18.2491 - val_MinusLogProbMetric: 18.2491 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 221/1000
2023-09-27 20:41:53.552 
Epoch 221/1000 
	 loss: 17.4629, MinusLogProbMetric: 17.4629, val_loss: 18.7537, val_MinusLogProbMetric: 18.7537

Epoch 221: val_loss did not improve from 17.56589
196/196 - 70s - loss: 17.4629 - MinusLogProbMetric: 17.4629 - val_loss: 18.7537 - val_MinusLogProbMetric: 18.7537 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 222/1000
2023-09-27 20:42:59.802 
Epoch 222/1000 
	 loss: 17.5188, MinusLogProbMetric: 17.5188, val_loss: 18.5181, val_MinusLogProbMetric: 18.5181

Epoch 222: val_loss did not improve from 17.56589
196/196 - 66s - loss: 17.5188 - MinusLogProbMetric: 17.5188 - val_loss: 18.5181 - val_MinusLogProbMetric: 18.5181 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 223/1000
2023-09-27 20:44:00.134 
Epoch 223/1000 
	 loss: 17.4890, MinusLogProbMetric: 17.4890, val_loss: 17.9164, val_MinusLogProbMetric: 17.9164

Epoch 223: val_loss did not improve from 17.56589
196/196 - 60s - loss: 17.4890 - MinusLogProbMetric: 17.4890 - val_loss: 17.9164 - val_MinusLogProbMetric: 17.9164 - lr: 3.3333e-04 - 60s/epoch - 308ms/step
Epoch 224/1000
2023-09-27 20:45:10.276 
Epoch 224/1000 
	 loss: 17.4885, MinusLogProbMetric: 17.4885, val_loss: 17.7674, val_MinusLogProbMetric: 17.7674

Epoch 224: val_loss did not improve from 17.56589
196/196 - 70s - loss: 17.4885 - MinusLogProbMetric: 17.4885 - val_loss: 17.7674 - val_MinusLogProbMetric: 17.7674 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 225/1000
2023-09-27 20:46:20.682 
Epoch 225/1000 
	 loss: 17.4666, MinusLogProbMetric: 17.4666, val_loss: 18.3047, val_MinusLogProbMetric: 18.3047

Epoch 225: val_loss did not improve from 17.56589
196/196 - 70s - loss: 17.4666 - MinusLogProbMetric: 17.4666 - val_loss: 18.3047 - val_MinusLogProbMetric: 18.3047 - lr: 3.3333e-04 - 70s/epoch - 359ms/step
Epoch 226/1000
2023-09-27 20:47:31.060 
Epoch 226/1000 
	 loss: 17.5433, MinusLogProbMetric: 17.5433, val_loss: 17.7298, val_MinusLogProbMetric: 17.7298

Epoch 226: val_loss did not improve from 17.56589
196/196 - 70s - loss: 17.5433 - MinusLogProbMetric: 17.5433 - val_loss: 17.7298 - val_MinusLogProbMetric: 17.7298 - lr: 3.3333e-04 - 70s/epoch - 359ms/step
Epoch 227/1000
2023-09-27 20:48:41.335 
Epoch 227/1000 
	 loss: 17.4208, MinusLogProbMetric: 17.4208, val_loss: 18.2344, val_MinusLogProbMetric: 18.2344

Epoch 227: val_loss did not improve from 17.56589
196/196 - 70s - loss: 17.4208 - MinusLogProbMetric: 17.4208 - val_loss: 18.2344 - val_MinusLogProbMetric: 18.2344 - lr: 3.3333e-04 - 70s/epoch - 359ms/step
Epoch 228/1000
2023-09-27 20:49:51.053 
Epoch 228/1000 
	 loss: 17.3831, MinusLogProbMetric: 17.3831, val_loss: 17.5613, val_MinusLogProbMetric: 17.5613

Epoch 228: val_loss improved from 17.56589 to 17.56127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 71s - loss: 17.3831 - MinusLogProbMetric: 17.3831 - val_loss: 17.5613 - val_MinusLogProbMetric: 17.5613 - lr: 3.3333e-04 - 71s/epoch - 361ms/step
Epoch 229/1000
2023-09-27 20:51:01.835 
Epoch 229/1000 
	 loss: 17.4816, MinusLogProbMetric: 17.4816, val_loss: 17.9058, val_MinusLogProbMetric: 17.9058

Epoch 229: val_loss did not improve from 17.56127
196/196 - 70s - loss: 17.4816 - MinusLogProbMetric: 17.4816 - val_loss: 17.9058 - val_MinusLogProbMetric: 17.9058 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 230/1000
2023-09-27 20:52:11.197 
Epoch 230/1000 
	 loss: 17.5076, MinusLogProbMetric: 17.5076, val_loss: 17.8090, val_MinusLogProbMetric: 17.8090

Epoch 230: val_loss did not improve from 17.56127
196/196 - 69s - loss: 17.5076 - MinusLogProbMetric: 17.5076 - val_loss: 17.8090 - val_MinusLogProbMetric: 17.8090 - lr: 3.3333e-04 - 69s/epoch - 354ms/step
Epoch 231/1000
2023-09-27 20:53:20.794 
Epoch 231/1000 
	 loss: 17.4046, MinusLogProbMetric: 17.4046, val_loss: 18.0044, val_MinusLogProbMetric: 18.0044

Epoch 231: val_loss did not improve from 17.56127
196/196 - 70s - loss: 17.4046 - MinusLogProbMetric: 17.4046 - val_loss: 18.0044 - val_MinusLogProbMetric: 18.0044 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 232/1000
2023-09-27 20:54:30.566 
Epoch 232/1000 
	 loss: 17.4746, MinusLogProbMetric: 17.4746, val_loss: 18.1966, val_MinusLogProbMetric: 18.1966

Epoch 232: val_loss did not improve from 17.56127
196/196 - 70s - loss: 17.4746 - MinusLogProbMetric: 17.4746 - val_loss: 18.1966 - val_MinusLogProbMetric: 18.1966 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 233/1000
2023-09-27 20:55:40.292 
Epoch 233/1000 
	 loss: 17.4194, MinusLogProbMetric: 17.4194, val_loss: 18.1241, val_MinusLogProbMetric: 18.1241

Epoch 233: val_loss did not improve from 17.56127
196/196 - 70s - loss: 17.4194 - MinusLogProbMetric: 17.4194 - val_loss: 18.1241 - val_MinusLogProbMetric: 18.1241 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 234/1000
2023-09-27 20:56:50.158 
Epoch 234/1000 
	 loss: 17.4262, MinusLogProbMetric: 17.4262, val_loss: 18.1119, val_MinusLogProbMetric: 18.1119

Epoch 234: val_loss did not improve from 17.56127
196/196 - 70s - loss: 17.4262 - MinusLogProbMetric: 17.4262 - val_loss: 18.1119 - val_MinusLogProbMetric: 18.1119 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 235/1000
2023-09-27 20:58:00.256 
Epoch 235/1000 
	 loss: 17.4793, MinusLogProbMetric: 17.4793, val_loss: 17.7499, val_MinusLogProbMetric: 17.7499

Epoch 235: val_loss did not improve from 17.56127
196/196 - 70s - loss: 17.4793 - MinusLogProbMetric: 17.4793 - val_loss: 17.7499 - val_MinusLogProbMetric: 17.7499 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 236/1000
2023-09-27 20:59:10.147 
Epoch 236/1000 
	 loss: 17.4450, MinusLogProbMetric: 17.4450, val_loss: 17.9658, val_MinusLogProbMetric: 17.9658

Epoch 236: val_loss did not improve from 17.56127
196/196 - 70s - loss: 17.4450 - MinusLogProbMetric: 17.4450 - val_loss: 17.9658 - val_MinusLogProbMetric: 17.9658 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 237/1000
2023-09-27 21:00:19.748 
Epoch 237/1000 
	 loss: 17.4045, MinusLogProbMetric: 17.4045, val_loss: 17.8423, val_MinusLogProbMetric: 17.8423

Epoch 237: val_loss did not improve from 17.56127
196/196 - 70s - loss: 17.4045 - MinusLogProbMetric: 17.4045 - val_loss: 17.8423 - val_MinusLogProbMetric: 17.8423 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 238/1000
2023-09-27 21:01:28.837 
Epoch 238/1000 
	 loss: 17.3554, MinusLogProbMetric: 17.3554, val_loss: 17.9328, val_MinusLogProbMetric: 17.9328

Epoch 238: val_loss did not improve from 17.56127
196/196 - 69s - loss: 17.3554 - MinusLogProbMetric: 17.3554 - val_loss: 17.9328 - val_MinusLogProbMetric: 17.9328 - lr: 3.3333e-04 - 69s/epoch - 352ms/step
Epoch 239/1000
2023-09-27 21:02:38.696 
Epoch 239/1000 
	 loss: 17.4453, MinusLogProbMetric: 17.4453, val_loss: 17.8463, val_MinusLogProbMetric: 17.8463

Epoch 239: val_loss did not improve from 17.56127
196/196 - 70s - loss: 17.4453 - MinusLogProbMetric: 17.4453 - val_loss: 17.8463 - val_MinusLogProbMetric: 17.8463 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 240/1000
2023-09-27 21:03:48.306 
Epoch 240/1000 
	 loss: 17.3885, MinusLogProbMetric: 17.3885, val_loss: 17.8347, val_MinusLogProbMetric: 17.8347

Epoch 240: val_loss did not improve from 17.56127
196/196 - 70s - loss: 17.3885 - MinusLogProbMetric: 17.3885 - val_loss: 17.8347 - val_MinusLogProbMetric: 17.8347 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 241/1000
2023-09-27 21:04:58.122 
Epoch 241/1000 
	 loss: 17.3828, MinusLogProbMetric: 17.3828, val_loss: 17.7659, val_MinusLogProbMetric: 17.7659

Epoch 241: val_loss did not improve from 17.56127
196/196 - 70s - loss: 17.3828 - MinusLogProbMetric: 17.3828 - val_loss: 17.7659 - val_MinusLogProbMetric: 17.7659 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 242/1000
2023-09-27 21:06:07.675 
Epoch 242/1000 
	 loss: 17.3422, MinusLogProbMetric: 17.3422, val_loss: 17.6715, val_MinusLogProbMetric: 17.6715

Epoch 242: val_loss did not improve from 17.56127
196/196 - 70s - loss: 17.3422 - MinusLogProbMetric: 17.3422 - val_loss: 17.6715 - val_MinusLogProbMetric: 17.6715 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 243/1000
2023-09-27 21:07:18.027 
Epoch 243/1000 
	 loss: 17.4787, MinusLogProbMetric: 17.4787, val_loss: 17.6773, val_MinusLogProbMetric: 17.6773

Epoch 243: val_loss did not improve from 17.56127
196/196 - 70s - loss: 17.4787 - MinusLogProbMetric: 17.4787 - val_loss: 17.6773 - val_MinusLogProbMetric: 17.6773 - lr: 3.3333e-04 - 70s/epoch - 359ms/step
Epoch 244/1000
2023-09-27 21:08:27.290 
Epoch 244/1000 
	 loss: 17.3692, MinusLogProbMetric: 17.3692, val_loss: 18.8081, val_MinusLogProbMetric: 18.8081

Epoch 244: val_loss did not improve from 17.56127
196/196 - 69s - loss: 17.3692 - MinusLogProbMetric: 17.3692 - val_loss: 18.8081 - val_MinusLogProbMetric: 18.8081 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 245/1000
2023-09-27 21:09:37.170 
Epoch 245/1000 
	 loss: 17.3713, MinusLogProbMetric: 17.3713, val_loss: 17.6806, val_MinusLogProbMetric: 17.6806

Epoch 245: val_loss did not improve from 17.56127
196/196 - 70s - loss: 17.3713 - MinusLogProbMetric: 17.3713 - val_loss: 17.6806 - val_MinusLogProbMetric: 17.6806 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 246/1000
2023-09-27 21:10:47.135 
Epoch 246/1000 
	 loss: 17.3454, MinusLogProbMetric: 17.3454, val_loss: 17.3832, val_MinusLogProbMetric: 17.3832

Epoch 246: val_loss improved from 17.56127 to 17.38323, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 71s - loss: 17.3454 - MinusLogProbMetric: 17.3454 - val_loss: 17.3832 - val_MinusLogProbMetric: 17.3832 - lr: 3.3333e-04 - 71s/epoch - 363ms/step
Epoch 247/1000
2023-09-27 21:11:58.268 
Epoch 247/1000 
	 loss: 17.4093, MinusLogProbMetric: 17.4093, val_loss: 18.1313, val_MinusLogProbMetric: 18.1313

Epoch 247: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.4093 - MinusLogProbMetric: 17.4093 - val_loss: 18.1313 - val_MinusLogProbMetric: 18.1313 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 248/1000
2023-09-27 21:13:08.441 
Epoch 248/1000 
	 loss: 17.3901, MinusLogProbMetric: 17.3901, val_loss: 17.5858, val_MinusLogProbMetric: 17.5858

Epoch 248: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.3901 - MinusLogProbMetric: 17.3901 - val_loss: 17.5858 - val_MinusLogProbMetric: 17.5858 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 249/1000
2023-09-27 21:14:18.301 
Epoch 249/1000 
	 loss: 17.4164, MinusLogProbMetric: 17.4164, val_loss: 17.5872, val_MinusLogProbMetric: 17.5872

Epoch 249: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.4164 - MinusLogProbMetric: 17.4164 - val_loss: 17.5872 - val_MinusLogProbMetric: 17.5872 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 250/1000
2023-09-27 21:15:28.595 
Epoch 250/1000 
	 loss: 17.3680, MinusLogProbMetric: 17.3680, val_loss: 17.9536, val_MinusLogProbMetric: 17.9536

Epoch 250: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.3680 - MinusLogProbMetric: 17.3680 - val_loss: 17.9536 - val_MinusLogProbMetric: 17.9536 - lr: 3.3333e-04 - 70s/epoch - 359ms/step
Epoch 251/1000
2023-09-27 21:16:38.238 
Epoch 251/1000 
	 loss: 17.3631, MinusLogProbMetric: 17.3631, val_loss: 18.1981, val_MinusLogProbMetric: 18.1981

Epoch 251: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.3631 - MinusLogProbMetric: 17.3631 - val_loss: 18.1981 - val_MinusLogProbMetric: 18.1981 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 252/1000
2023-09-27 21:17:47.308 
Epoch 252/1000 
	 loss: 17.4179, MinusLogProbMetric: 17.4179, val_loss: 17.5266, val_MinusLogProbMetric: 17.5266

Epoch 252: val_loss did not improve from 17.38323
196/196 - 69s - loss: 17.4179 - MinusLogProbMetric: 17.4179 - val_loss: 17.5266 - val_MinusLogProbMetric: 17.5266 - lr: 3.3333e-04 - 69s/epoch - 352ms/step
Epoch 253/1000
2023-09-27 21:18:56.564 
Epoch 253/1000 
	 loss: 17.3620, MinusLogProbMetric: 17.3620, val_loss: 17.7635, val_MinusLogProbMetric: 17.7635

Epoch 253: val_loss did not improve from 17.38323
196/196 - 69s - loss: 17.3620 - MinusLogProbMetric: 17.3620 - val_loss: 17.7635 - val_MinusLogProbMetric: 17.7635 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 254/1000
2023-09-27 21:20:05.864 
Epoch 254/1000 
	 loss: 17.4100, MinusLogProbMetric: 17.4100, val_loss: 17.5846, val_MinusLogProbMetric: 17.5846

Epoch 254: val_loss did not improve from 17.38323
196/196 - 69s - loss: 17.4100 - MinusLogProbMetric: 17.4100 - val_loss: 17.5846 - val_MinusLogProbMetric: 17.5846 - lr: 3.3333e-04 - 69s/epoch - 354ms/step
Epoch 255/1000
2023-09-27 21:21:15.797 
Epoch 255/1000 
	 loss: 17.3216, MinusLogProbMetric: 17.3216, val_loss: 17.6234, val_MinusLogProbMetric: 17.6234

Epoch 255: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.3216 - MinusLogProbMetric: 17.3216 - val_loss: 17.6234 - val_MinusLogProbMetric: 17.6234 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 256/1000
2023-09-27 21:22:25.693 
Epoch 256/1000 
	 loss: 17.4834, MinusLogProbMetric: 17.4834, val_loss: 17.5310, val_MinusLogProbMetric: 17.5310

Epoch 256: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.4834 - MinusLogProbMetric: 17.4834 - val_loss: 17.5310 - val_MinusLogProbMetric: 17.5310 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 257/1000
2023-09-27 21:23:35.433 
Epoch 257/1000 
	 loss: 17.3139, MinusLogProbMetric: 17.3139, val_loss: 18.1980, val_MinusLogProbMetric: 18.1980

Epoch 257: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.3139 - MinusLogProbMetric: 17.3139 - val_loss: 18.1980 - val_MinusLogProbMetric: 18.1980 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 258/1000
2023-09-27 21:24:45.935 
Epoch 258/1000 
	 loss: 17.3160, MinusLogProbMetric: 17.3160, val_loss: 17.6247, val_MinusLogProbMetric: 17.6247

Epoch 258: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.3160 - MinusLogProbMetric: 17.3160 - val_loss: 17.6247 - val_MinusLogProbMetric: 17.6247 - lr: 3.3333e-04 - 70s/epoch - 360ms/step
Epoch 259/1000
2023-09-27 21:25:55.894 
Epoch 259/1000 
	 loss: 17.3075, MinusLogProbMetric: 17.3075, val_loss: 17.6331, val_MinusLogProbMetric: 17.6331

Epoch 259: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.3075 - MinusLogProbMetric: 17.3075 - val_loss: 17.6331 - val_MinusLogProbMetric: 17.6331 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 260/1000
2023-09-27 21:27:05.905 
Epoch 260/1000 
	 loss: 17.3990, MinusLogProbMetric: 17.3990, val_loss: 17.6929, val_MinusLogProbMetric: 17.6929

Epoch 260: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.3990 - MinusLogProbMetric: 17.3990 - val_loss: 17.6929 - val_MinusLogProbMetric: 17.6929 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 261/1000
2023-09-27 21:28:15.080 
Epoch 261/1000 
	 loss: 17.3736, MinusLogProbMetric: 17.3736, val_loss: 17.4684, val_MinusLogProbMetric: 17.4684

Epoch 261: val_loss did not improve from 17.38323
196/196 - 69s - loss: 17.3736 - MinusLogProbMetric: 17.3736 - val_loss: 17.4684 - val_MinusLogProbMetric: 17.4684 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 262/1000
2023-09-27 21:29:24.713 
Epoch 262/1000 
	 loss: 17.2877, MinusLogProbMetric: 17.2877, val_loss: 18.2231, val_MinusLogProbMetric: 18.2231

Epoch 262: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.2877 - MinusLogProbMetric: 17.2877 - val_loss: 18.2231 - val_MinusLogProbMetric: 18.2231 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 263/1000
2023-09-27 21:30:35.235 
Epoch 263/1000 
	 loss: 17.2754, MinusLogProbMetric: 17.2754, val_loss: 17.5392, val_MinusLogProbMetric: 17.5392

Epoch 263: val_loss did not improve from 17.38323
196/196 - 71s - loss: 17.2754 - MinusLogProbMetric: 17.2754 - val_loss: 17.5392 - val_MinusLogProbMetric: 17.5392 - lr: 3.3333e-04 - 71s/epoch - 360ms/step
Epoch 264/1000
2023-09-27 21:31:44.968 
Epoch 264/1000 
	 loss: 17.4373, MinusLogProbMetric: 17.4373, val_loss: 17.6602, val_MinusLogProbMetric: 17.6602

Epoch 264: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.4373 - MinusLogProbMetric: 17.4373 - val_loss: 17.6602 - val_MinusLogProbMetric: 17.6602 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 265/1000
2023-09-27 21:32:54.250 
Epoch 265/1000 
	 loss: 17.2730, MinusLogProbMetric: 17.2730, val_loss: 17.9778, val_MinusLogProbMetric: 17.9778

Epoch 265: val_loss did not improve from 17.38323
196/196 - 69s - loss: 17.2730 - MinusLogProbMetric: 17.2730 - val_loss: 17.9778 - val_MinusLogProbMetric: 17.9778 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 266/1000
2023-09-27 21:34:03.842 
Epoch 266/1000 
	 loss: 17.2809, MinusLogProbMetric: 17.2809, val_loss: 18.3170, val_MinusLogProbMetric: 18.3170

Epoch 266: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.2809 - MinusLogProbMetric: 17.2809 - val_loss: 18.3170 - val_MinusLogProbMetric: 18.3170 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 267/1000
2023-09-27 21:35:13.729 
Epoch 267/1000 
	 loss: 17.3870, MinusLogProbMetric: 17.3870, val_loss: 17.9090, val_MinusLogProbMetric: 17.9090

Epoch 267: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.3870 - MinusLogProbMetric: 17.3870 - val_loss: 17.9090 - val_MinusLogProbMetric: 17.9090 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 268/1000
2023-09-27 21:36:22.917 
Epoch 268/1000 
	 loss: 17.3023, MinusLogProbMetric: 17.3023, val_loss: 17.8617, val_MinusLogProbMetric: 17.8617

Epoch 268: val_loss did not improve from 17.38323
196/196 - 69s - loss: 17.3023 - MinusLogProbMetric: 17.3023 - val_loss: 17.8617 - val_MinusLogProbMetric: 17.8617 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 269/1000
2023-09-27 21:37:32.698 
Epoch 269/1000 
	 loss: 17.2983, MinusLogProbMetric: 17.2983, val_loss: 17.8129, val_MinusLogProbMetric: 17.8129

Epoch 269: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.2983 - MinusLogProbMetric: 17.2983 - val_loss: 17.8129 - val_MinusLogProbMetric: 17.8129 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 270/1000
2023-09-27 21:38:42.239 
Epoch 270/1000 
	 loss: 17.3696, MinusLogProbMetric: 17.3696, val_loss: 17.8687, val_MinusLogProbMetric: 17.8687

Epoch 270: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.3696 - MinusLogProbMetric: 17.3696 - val_loss: 17.8687 - val_MinusLogProbMetric: 17.8687 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 271/1000
2023-09-27 21:39:52.298 
Epoch 271/1000 
	 loss: 17.2778, MinusLogProbMetric: 17.2778, val_loss: 18.1452, val_MinusLogProbMetric: 18.1452

Epoch 271: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.2778 - MinusLogProbMetric: 17.2778 - val_loss: 18.1452 - val_MinusLogProbMetric: 18.1452 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 272/1000
2023-09-27 21:41:02.265 
Epoch 272/1000 
	 loss: 17.2691, MinusLogProbMetric: 17.2691, val_loss: 17.8072, val_MinusLogProbMetric: 17.8072

Epoch 272: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.2691 - MinusLogProbMetric: 17.2691 - val_loss: 17.8072 - val_MinusLogProbMetric: 17.8072 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 273/1000
2023-09-27 21:42:12.021 
Epoch 273/1000 
	 loss: 17.3727, MinusLogProbMetric: 17.3727, val_loss: 17.7239, val_MinusLogProbMetric: 17.7239

Epoch 273: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.3727 - MinusLogProbMetric: 17.3727 - val_loss: 17.7239 - val_MinusLogProbMetric: 17.7239 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 274/1000
2023-09-27 21:43:22.037 
Epoch 274/1000 
	 loss: 17.3170, MinusLogProbMetric: 17.3170, val_loss: 17.7976, val_MinusLogProbMetric: 17.7976

Epoch 274: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.3170 - MinusLogProbMetric: 17.3170 - val_loss: 17.7976 - val_MinusLogProbMetric: 17.7976 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 275/1000
2023-09-27 21:44:32.053 
Epoch 275/1000 
	 loss: 17.2921, MinusLogProbMetric: 17.2921, val_loss: 17.5570, val_MinusLogProbMetric: 17.5570

Epoch 275: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.2921 - MinusLogProbMetric: 17.2921 - val_loss: 17.5570 - val_MinusLogProbMetric: 17.5570 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 276/1000
2023-09-27 21:45:42.395 
Epoch 276/1000 
	 loss: 17.3572, MinusLogProbMetric: 17.3572, val_loss: 17.5296, val_MinusLogProbMetric: 17.5296

Epoch 276: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.3572 - MinusLogProbMetric: 17.3572 - val_loss: 17.5296 - val_MinusLogProbMetric: 17.5296 - lr: 3.3333e-04 - 70s/epoch - 359ms/step
Epoch 277/1000
2023-09-27 21:46:51.937 
Epoch 277/1000 
	 loss: 17.2208, MinusLogProbMetric: 17.2208, val_loss: 17.7916, val_MinusLogProbMetric: 17.7916

Epoch 277: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.2208 - MinusLogProbMetric: 17.2208 - val_loss: 17.7916 - val_MinusLogProbMetric: 17.7916 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 278/1000
2023-09-27 21:48:01.838 
Epoch 278/1000 
	 loss: 17.2252, MinusLogProbMetric: 17.2252, val_loss: 17.7469, val_MinusLogProbMetric: 17.7469

Epoch 278: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.2252 - MinusLogProbMetric: 17.2252 - val_loss: 17.7469 - val_MinusLogProbMetric: 17.7469 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 279/1000
2023-09-27 21:49:11.896 
Epoch 279/1000 
	 loss: 17.3063, MinusLogProbMetric: 17.3063, val_loss: 18.1244, val_MinusLogProbMetric: 18.1244

Epoch 279: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.3063 - MinusLogProbMetric: 17.3063 - val_loss: 18.1244 - val_MinusLogProbMetric: 18.1244 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 280/1000
2023-09-27 21:50:21.627 
Epoch 280/1000 
	 loss: 17.2523, MinusLogProbMetric: 17.2523, val_loss: 17.5980, val_MinusLogProbMetric: 17.5980

Epoch 280: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.2523 - MinusLogProbMetric: 17.2523 - val_loss: 17.5980 - val_MinusLogProbMetric: 17.5980 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 281/1000
2023-09-27 21:51:30.421 
Epoch 281/1000 
	 loss: 17.2781, MinusLogProbMetric: 17.2781, val_loss: 17.6831, val_MinusLogProbMetric: 17.6831

Epoch 281: val_loss did not improve from 17.38323
196/196 - 69s - loss: 17.2781 - MinusLogProbMetric: 17.2781 - val_loss: 17.6831 - val_MinusLogProbMetric: 17.6831 - lr: 3.3333e-04 - 69s/epoch - 351ms/step
Epoch 282/1000
2023-09-27 21:52:40.734 
Epoch 282/1000 
	 loss: 17.3310, MinusLogProbMetric: 17.3310, val_loss: 17.9163, val_MinusLogProbMetric: 17.9163

Epoch 282: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.3310 - MinusLogProbMetric: 17.3310 - val_loss: 17.9163 - val_MinusLogProbMetric: 17.9163 - lr: 3.3333e-04 - 70s/epoch - 359ms/step
Epoch 283/1000
2023-09-27 21:53:50.509 
Epoch 283/1000 
	 loss: 17.2533, MinusLogProbMetric: 17.2533, val_loss: 17.4361, val_MinusLogProbMetric: 17.4361

Epoch 283: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.2533 - MinusLogProbMetric: 17.2533 - val_loss: 17.4361 - val_MinusLogProbMetric: 17.4361 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 284/1000
2023-09-27 21:55:00.450 
Epoch 284/1000 
	 loss: 17.2961, MinusLogProbMetric: 17.2961, val_loss: 17.6843, val_MinusLogProbMetric: 17.6843

Epoch 284: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.2961 - MinusLogProbMetric: 17.2961 - val_loss: 17.6843 - val_MinusLogProbMetric: 17.6843 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 285/1000
2023-09-27 21:56:10.394 
Epoch 285/1000 
	 loss: 17.2723, MinusLogProbMetric: 17.2723, val_loss: 17.9227, val_MinusLogProbMetric: 17.9227

Epoch 285: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.2723 - MinusLogProbMetric: 17.2723 - val_loss: 17.9227 - val_MinusLogProbMetric: 17.9227 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 286/1000
2023-09-27 21:57:20.047 
Epoch 286/1000 
	 loss: 17.2641, MinusLogProbMetric: 17.2641, val_loss: 18.2904, val_MinusLogProbMetric: 18.2904

Epoch 286: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.2641 - MinusLogProbMetric: 17.2641 - val_loss: 18.2904 - val_MinusLogProbMetric: 18.2904 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 287/1000
2023-09-27 21:58:29.995 
Epoch 287/1000 
	 loss: 17.2743, MinusLogProbMetric: 17.2743, val_loss: 17.5864, val_MinusLogProbMetric: 17.5864

Epoch 287: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.2743 - MinusLogProbMetric: 17.2743 - val_loss: 17.5864 - val_MinusLogProbMetric: 17.5864 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 288/1000
2023-09-27 21:59:39.463 
Epoch 288/1000 
	 loss: 17.2459, MinusLogProbMetric: 17.2459, val_loss: 17.6628, val_MinusLogProbMetric: 17.6628

Epoch 288: val_loss did not improve from 17.38323
196/196 - 69s - loss: 17.2459 - MinusLogProbMetric: 17.2459 - val_loss: 17.6628 - val_MinusLogProbMetric: 17.6628 - lr: 3.3333e-04 - 69s/epoch - 354ms/step
Epoch 289/1000
2023-09-27 22:00:49.183 
Epoch 289/1000 
	 loss: 17.2292, MinusLogProbMetric: 17.2292, val_loss: 17.8718, val_MinusLogProbMetric: 17.8718

Epoch 289: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.2292 - MinusLogProbMetric: 17.2292 - val_loss: 17.8718 - val_MinusLogProbMetric: 17.8718 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 290/1000
2023-09-27 22:01:59.154 
Epoch 290/1000 
	 loss: 17.1940, MinusLogProbMetric: 17.1940, val_loss: 18.0311, val_MinusLogProbMetric: 18.0311

Epoch 290: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.1940 - MinusLogProbMetric: 17.1940 - val_loss: 18.0311 - val_MinusLogProbMetric: 18.0311 - lr: 3.3333e-04 - 70s/epoch - 357ms/step
Epoch 291/1000
2023-09-27 22:03:09.286 
Epoch 291/1000 
	 loss: 17.2719, MinusLogProbMetric: 17.2719, val_loss: 18.8668, val_MinusLogProbMetric: 18.8668

Epoch 291: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.2719 - MinusLogProbMetric: 17.2719 - val_loss: 18.8668 - val_MinusLogProbMetric: 18.8668 - lr: 3.3333e-04 - 70s/epoch - 358ms/step
Epoch 292/1000
2023-09-27 22:04:18.897 
Epoch 292/1000 
	 loss: 17.2619, MinusLogProbMetric: 17.2619, val_loss: 17.6796, val_MinusLogProbMetric: 17.6796

Epoch 292: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.2619 - MinusLogProbMetric: 17.2619 - val_loss: 17.6796 - val_MinusLogProbMetric: 17.6796 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 293/1000
2023-09-27 22:05:28.460 
Epoch 293/1000 
	 loss: 17.2107, MinusLogProbMetric: 17.2107, val_loss: 17.4858, val_MinusLogProbMetric: 17.4858

Epoch 293: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.2107 - MinusLogProbMetric: 17.2107 - val_loss: 17.4858 - val_MinusLogProbMetric: 17.4858 - lr: 3.3333e-04 - 70s/epoch - 355ms/step
Epoch 294/1000
2023-09-27 22:06:38.187 
Epoch 294/1000 
	 loss: 17.2482, MinusLogProbMetric: 17.2482, val_loss: 17.7064, val_MinusLogProbMetric: 17.7064

Epoch 294: val_loss did not improve from 17.38323
196/196 - 70s - loss: 17.2482 - MinusLogProbMetric: 17.2482 - val_loss: 17.7064 - val_MinusLogProbMetric: 17.7064 - lr: 3.3333e-04 - 70s/epoch - 356ms/step
Epoch 295/1000
2023-09-27 22:07:47.478 
Epoch 295/1000 
	 loss: 17.1771, MinusLogProbMetric: 17.1771, val_loss: 17.8380, val_MinusLogProbMetric: 17.8380

Epoch 295: val_loss did not improve from 17.38323
196/196 - 69s - loss: 17.1771 - MinusLogProbMetric: 17.1771 - val_loss: 17.8380 - val_MinusLogProbMetric: 17.8380 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 296/1000
2023-09-27 22:08:54.633 
Epoch 296/1000 
	 loss: 17.1901, MinusLogProbMetric: 17.1901, val_loss: 17.6409, val_MinusLogProbMetric: 17.6409

Epoch 296: val_loss did not improve from 17.38323
196/196 - 67s - loss: 17.1901 - MinusLogProbMetric: 17.1901 - val_loss: 17.6409 - val_MinusLogProbMetric: 17.6409 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 297/1000
2023-09-27 22:09:54.184 
Epoch 297/1000 
	 loss: 16.7779, MinusLogProbMetric: 16.7779, val_loss: 17.5251, val_MinusLogProbMetric: 17.5251

Epoch 297: val_loss did not improve from 17.38323
196/196 - 60s - loss: 16.7779 - MinusLogProbMetric: 16.7779 - val_loss: 17.5251 - val_MinusLogProbMetric: 17.5251 - lr: 1.6667e-04 - 60s/epoch - 304ms/step
Epoch 298/1000
2023-09-27 22:10:59.097 
Epoch 298/1000 
	 loss: 16.8189, MinusLogProbMetric: 16.8189, val_loss: 17.3214, val_MinusLogProbMetric: 17.3214

Epoch 298: val_loss improved from 17.38323 to 17.32143, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 66s - loss: 16.8189 - MinusLogProbMetric: 16.8189 - val_loss: 17.3214 - val_MinusLogProbMetric: 17.3214 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 299/1000
2023-09-27 22:12:08.978 
Epoch 299/1000 
	 loss: 16.7856, MinusLogProbMetric: 16.7856, val_loss: 17.4037, val_MinusLogProbMetric: 17.4037

Epoch 299: val_loss did not improve from 17.32143
196/196 - 69s - loss: 16.7856 - MinusLogProbMetric: 16.7856 - val_loss: 17.4037 - val_MinusLogProbMetric: 17.4037 - lr: 1.6667e-04 - 69s/epoch - 352ms/step
Epoch 300/1000
2023-09-27 22:13:17.987 
Epoch 300/1000 
	 loss: 16.8144, MinusLogProbMetric: 16.8144, val_loss: 17.2851, val_MinusLogProbMetric: 17.2851

Epoch 300: val_loss improved from 17.32143 to 17.28512, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 70s - loss: 16.8144 - MinusLogProbMetric: 16.8144 - val_loss: 17.2851 - val_MinusLogProbMetric: 17.2851 - lr: 1.6667e-04 - 70s/epoch - 358ms/step
Epoch 301/1000
2023-09-27 22:14:28.842 
Epoch 301/1000 
	 loss: 16.8062, MinusLogProbMetric: 16.8062, val_loss: 17.2968, val_MinusLogProbMetric: 17.2968

Epoch 301: val_loss did not improve from 17.28512
196/196 - 70s - loss: 16.8062 - MinusLogProbMetric: 16.8062 - val_loss: 17.2968 - val_MinusLogProbMetric: 17.2968 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 302/1000
2023-09-27 22:15:38.458 
Epoch 302/1000 
	 loss: 16.7829, MinusLogProbMetric: 16.7829, val_loss: 17.2052, val_MinusLogProbMetric: 17.2052

Epoch 302: val_loss improved from 17.28512 to 17.20515, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 70s - loss: 16.7829 - MinusLogProbMetric: 16.7829 - val_loss: 17.2052 - val_MinusLogProbMetric: 17.2052 - lr: 1.6667e-04 - 70s/epoch - 360ms/step
Epoch 303/1000
2023-09-27 22:16:48.609 
Epoch 303/1000 
	 loss: 16.7970, MinusLogProbMetric: 16.7970, val_loss: 17.1980, val_MinusLogProbMetric: 17.1980

Epoch 303: val_loss improved from 17.20515 to 17.19803, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 70s - loss: 16.7970 - MinusLogProbMetric: 16.7970 - val_loss: 17.1980 - val_MinusLogProbMetric: 17.1980 - lr: 1.6667e-04 - 70s/epoch - 359ms/step
Epoch 304/1000
2023-09-27 22:17:59.528 
Epoch 304/1000 
	 loss: 16.7624, MinusLogProbMetric: 16.7624, val_loss: 17.3452, val_MinusLogProbMetric: 17.3452

Epoch 304: val_loss did not improve from 17.19803
196/196 - 70s - loss: 16.7624 - MinusLogProbMetric: 16.7624 - val_loss: 17.3452 - val_MinusLogProbMetric: 17.3452 - lr: 1.6667e-04 - 70s/epoch - 357ms/step
Epoch 305/1000
2023-09-27 22:19:09.523 
Epoch 305/1000 
	 loss: 16.7804, MinusLogProbMetric: 16.7804, val_loss: 17.3290, val_MinusLogProbMetric: 17.3290

Epoch 305: val_loss did not improve from 17.19803
196/196 - 70s - loss: 16.7804 - MinusLogProbMetric: 16.7804 - val_loss: 17.3290 - val_MinusLogProbMetric: 17.3290 - lr: 1.6667e-04 - 70s/epoch - 357ms/step
Epoch 306/1000
2023-09-27 22:20:19.417 
Epoch 306/1000 
	 loss: 16.7996, MinusLogProbMetric: 16.7996, val_loss: 17.4498, val_MinusLogProbMetric: 17.4498

Epoch 306: val_loss did not improve from 17.19803
196/196 - 70s - loss: 16.7996 - MinusLogProbMetric: 16.7996 - val_loss: 17.4498 - val_MinusLogProbMetric: 17.4498 - lr: 1.6667e-04 - 70s/epoch - 357ms/step
Epoch 307/1000
2023-09-27 22:21:28.808 
Epoch 307/1000 
	 loss: 16.8117, MinusLogProbMetric: 16.8117, val_loss: 17.6155, val_MinusLogProbMetric: 17.6155

Epoch 307: val_loss did not improve from 17.19803
196/196 - 69s - loss: 16.8117 - MinusLogProbMetric: 16.8117 - val_loss: 17.6155 - val_MinusLogProbMetric: 17.6155 - lr: 1.6667e-04 - 69s/epoch - 354ms/step
Epoch 308/1000
2023-09-27 22:22:38.584 
Epoch 308/1000 
	 loss: 16.7803, MinusLogProbMetric: 16.7803, val_loss: 17.3099, val_MinusLogProbMetric: 17.3099

Epoch 308: val_loss did not improve from 17.19803
196/196 - 70s - loss: 16.7803 - MinusLogProbMetric: 16.7803 - val_loss: 17.3099 - val_MinusLogProbMetric: 17.3099 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 309/1000
2023-09-27 22:23:47.796 
Epoch 309/1000 
	 loss: 16.7940, MinusLogProbMetric: 16.7940, val_loss: 17.5908, val_MinusLogProbMetric: 17.5908

Epoch 309: val_loss did not improve from 17.19803
196/196 - 69s - loss: 16.7940 - MinusLogProbMetric: 16.7940 - val_loss: 17.5908 - val_MinusLogProbMetric: 17.5908 - lr: 1.6667e-04 - 69s/epoch - 353ms/step
Epoch 310/1000
2023-09-27 22:24:56.930 
Epoch 310/1000 
	 loss: 16.7695, MinusLogProbMetric: 16.7695, val_loss: 17.2242, val_MinusLogProbMetric: 17.2242

Epoch 310: val_loss did not improve from 17.19803
196/196 - 69s - loss: 16.7695 - MinusLogProbMetric: 16.7695 - val_loss: 17.2242 - val_MinusLogProbMetric: 17.2242 - lr: 1.6667e-04 - 69s/epoch - 353ms/step
Epoch 311/1000
2023-09-27 22:26:06.068 
Epoch 311/1000 
	 loss: 16.8029, MinusLogProbMetric: 16.8029, val_loss: 17.3081, val_MinusLogProbMetric: 17.3081

Epoch 311: val_loss did not improve from 17.19803
196/196 - 69s - loss: 16.8029 - MinusLogProbMetric: 16.8029 - val_loss: 17.3081 - val_MinusLogProbMetric: 17.3081 - lr: 1.6667e-04 - 69s/epoch - 353ms/step
Epoch 312/1000
2023-09-27 22:27:15.075 
Epoch 312/1000 
	 loss: 16.7676, MinusLogProbMetric: 16.7676, val_loss: 17.2720, val_MinusLogProbMetric: 17.2720

Epoch 312: val_loss did not improve from 17.19803
196/196 - 69s - loss: 16.7676 - MinusLogProbMetric: 16.7676 - val_loss: 17.2720 - val_MinusLogProbMetric: 17.2720 - lr: 1.6667e-04 - 69s/epoch - 352ms/step
Epoch 313/1000
2023-09-27 22:28:24.422 
Epoch 313/1000 
	 loss: 16.8141, MinusLogProbMetric: 16.8141, val_loss: 17.2356, val_MinusLogProbMetric: 17.2356

Epoch 313: val_loss did not improve from 17.19803
196/196 - 69s - loss: 16.8141 - MinusLogProbMetric: 16.8141 - val_loss: 17.2356 - val_MinusLogProbMetric: 17.2356 - lr: 1.6667e-04 - 69s/epoch - 354ms/step
Epoch 314/1000
2023-09-27 22:29:33.805 
Epoch 314/1000 
	 loss: 16.7691, MinusLogProbMetric: 16.7691, val_loss: 17.4758, val_MinusLogProbMetric: 17.4758

Epoch 314: val_loss did not improve from 17.19803
196/196 - 69s - loss: 16.7691 - MinusLogProbMetric: 16.7691 - val_loss: 17.4758 - val_MinusLogProbMetric: 17.4758 - lr: 1.6667e-04 - 69s/epoch - 354ms/step
Epoch 315/1000
2023-09-27 22:30:43.617 
Epoch 315/1000 
	 loss: 16.8197, MinusLogProbMetric: 16.8197, val_loss: 17.3053, val_MinusLogProbMetric: 17.3053

Epoch 315: val_loss did not improve from 17.19803
196/196 - 70s - loss: 16.8197 - MinusLogProbMetric: 16.8197 - val_loss: 17.3053 - val_MinusLogProbMetric: 17.3053 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 316/1000
2023-09-27 22:31:53.205 
Epoch 316/1000 
	 loss: 16.7948, MinusLogProbMetric: 16.7948, val_loss: 17.1879, val_MinusLogProbMetric: 17.1879

Epoch 316: val_loss improved from 17.19803 to 17.18788, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 71s - loss: 16.7948 - MinusLogProbMetric: 16.7948 - val_loss: 17.1879 - val_MinusLogProbMetric: 17.1879 - lr: 1.6667e-04 - 71s/epoch - 360ms/step
Epoch 317/1000
2023-09-27 22:33:03.370 
Epoch 317/1000 
	 loss: 16.7852, MinusLogProbMetric: 16.7852, val_loss: 17.4917, val_MinusLogProbMetric: 17.4917

Epoch 317: val_loss did not improve from 17.18788
196/196 - 69s - loss: 16.7852 - MinusLogProbMetric: 16.7852 - val_loss: 17.4917 - val_MinusLogProbMetric: 17.4917 - lr: 1.6667e-04 - 69s/epoch - 353ms/step
Epoch 318/1000
2023-09-27 22:34:12.917 
Epoch 318/1000 
	 loss: 16.7828, MinusLogProbMetric: 16.7828, val_loss: 17.2634, val_MinusLogProbMetric: 17.2634

Epoch 318: val_loss did not improve from 17.18788
196/196 - 70s - loss: 16.7828 - MinusLogProbMetric: 16.7828 - val_loss: 17.2634 - val_MinusLogProbMetric: 17.2634 - lr: 1.6667e-04 - 70s/epoch - 355ms/step
Epoch 319/1000
2023-09-27 22:35:22.602 
Epoch 319/1000 
	 loss: 16.7759, MinusLogProbMetric: 16.7759, val_loss: 17.2387, val_MinusLogProbMetric: 17.2387

Epoch 319: val_loss did not improve from 17.18788
196/196 - 70s - loss: 16.7759 - MinusLogProbMetric: 16.7759 - val_loss: 17.2387 - val_MinusLogProbMetric: 17.2387 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 320/1000
2023-09-27 22:36:32.228 
Epoch 320/1000 
	 loss: 16.7629, MinusLogProbMetric: 16.7629, val_loss: 17.5573, val_MinusLogProbMetric: 17.5573

Epoch 320: val_loss did not improve from 17.18788
196/196 - 70s - loss: 16.7629 - MinusLogProbMetric: 16.7629 - val_loss: 17.5573 - val_MinusLogProbMetric: 17.5573 - lr: 1.6667e-04 - 70s/epoch - 355ms/step
Epoch 321/1000
2023-09-27 22:37:41.857 
Epoch 321/1000 
	 loss: 16.7715, MinusLogProbMetric: 16.7715, val_loss: 17.1878, val_MinusLogProbMetric: 17.1878

Epoch 321: val_loss improved from 17.18788 to 17.18777, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 71s - loss: 16.7715 - MinusLogProbMetric: 16.7715 - val_loss: 17.1878 - val_MinusLogProbMetric: 17.1878 - lr: 1.6667e-04 - 71s/epoch - 361ms/step
Epoch 322/1000
2023-09-27 22:38:53.256 
Epoch 322/1000 
	 loss: 16.8014, MinusLogProbMetric: 16.8014, val_loss: 17.6210, val_MinusLogProbMetric: 17.6210

Epoch 322: val_loss did not improve from 17.18777
196/196 - 70s - loss: 16.8014 - MinusLogProbMetric: 16.8014 - val_loss: 17.6210 - val_MinusLogProbMetric: 17.6210 - lr: 1.6667e-04 - 70s/epoch - 358ms/step
Epoch 323/1000
2023-09-27 22:40:03.035 
Epoch 323/1000 
	 loss: 16.7433, MinusLogProbMetric: 16.7433, val_loss: 17.4341, val_MinusLogProbMetric: 17.4341

Epoch 323: val_loss did not improve from 17.18777
196/196 - 70s - loss: 16.7433 - MinusLogProbMetric: 16.7433 - val_loss: 17.4341 - val_MinusLogProbMetric: 17.4341 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 324/1000
2023-09-27 22:41:12.315 
Epoch 324/1000 
	 loss: 16.7916, MinusLogProbMetric: 16.7916, val_loss: 17.2245, val_MinusLogProbMetric: 17.2245

Epoch 324: val_loss did not improve from 17.18777
196/196 - 69s - loss: 16.7916 - MinusLogProbMetric: 16.7916 - val_loss: 17.2245 - val_MinusLogProbMetric: 17.2245 - lr: 1.6667e-04 - 69s/epoch - 353ms/step
Epoch 325/1000
2023-09-27 22:42:21.412 
Epoch 325/1000 
	 loss: 16.7768, MinusLogProbMetric: 16.7768, val_loss: 17.5254, val_MinusLogProbMetric: 17.5254

Epoch 325: val_loss did not improve from 17.18777
196/196 - 69s - loss: 16.7768 - MinusLogProbMetric: 16.7768 - val_loss: 17.5254 - val_MinusLogProbMetric: 17.5254 - lr: 1.6667e-04 - 69s/epoch - 353ms/step
Epoch 326/1000
2023-09-27 22:43:30.914 
Epoch 326/1000 
	 loss: 16.7617, MinusLogProbMetric: 16.7617, val_loss: 17.2823, val_MinusLogProbMetric: 17.2823

Epoch 326: val_loss did not improve from 17.18777
196/196 - 69s - loss: 16.7617 - MinusLogProbMetric: 16.7617 - val_loss: 17.2823 - val_MinusLogProbMetric: 17.2823 - lr: 1.6667e-04 - 69s/epoch - 355ms/step
Epoch 327/1000
2023-09-27 22:44:39.975 
Epoch 327/1000 
	 loss: 16.7796, MinusLogProbMetric: 16.7796, val_loss: 17.2598, val_MinusLogProbMetric: 17.2598

Epoch 327: val_loss did not improve from 17.18777
196/196 - 69s - loss: 16.7796 - MinusLogProbMetric: 16.7796 - val_loss: 17.2598 - val_MinusLogProbMetric: 17.2598 - lr: 1.6667e-04 - 69s/epoch - 352ms/step
Epoch 328/1000
2023-09-27 22:45:49.735 
Epoch 328/1000 
	 loss: 16.7856, MinusLogProbMetric: 16.7856, val_loss: 17.4072, val_MinusLogProbMetric: 17.4072

Epoch 328: val_loss did not improve from 17.18777
196/196 - 70s - loss: 16.7856 - MinusLogProbMetric: 16.7856 - val_loss: 17.4072 - val_MinusLogProbMetric: 17.4072 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 329/1000
2023-09-27 22:46:57.835 
Epoch 329/1000 
	 loss: 16.7596, MinusLogProbMetric: 16.7596, val_loss: 17.2455, val_MinusLogProbMetric: 17.2455

Epoch 329: val_loss did not improve from 17.18777
196/196 - 68s - loss: 16.7596 - MinusLogProbMetric: 16.7596 - val_loss: 17.2455 - val_MinusLogProbMetric: 17.2455 - lr: 1.6667e-04 - 68s/epoch - 347ms/step
Epoch 330/1000
2023-09-27 22:48:06.832 
Epoch 330/1000 
	 loss: 16.7409, MinusLogProbMetric: 16.7409, val_loss: 17.4833, val_MinusLogProbMetric: 17.4833

Epoch 330: val_loss did not improve from 17.18777
196/196 - 69s - loss: 16.7409 - MinusLogProbMetric: 16.7409 - val_loss: 17.4833 - val_MinusLogProbMetric: 17.4833 - lr: 1.6667e-04 - 69s/epoch - 352ms/step
Epoch 331/1000
2023-09-27 22:49:15.930 
Epoch 331/1000 
	 loss: 16.8000, MinusLogProbMetric: 16.8000, val_loss: 17.2068, val_MinusLogProbMetric: 17.2068

Epoch 331: val_loss did not improve from 17.18777
196/196 - 69s - loss: 16.8000 - MinusLogProbMetric: 16.8000 - val_loss: 17.2068 - val_MinusLogProbMetric: 17.2068 - lr: 1.6667e-04 - 69s/epoch - 353ms/step
Epoch 332/1000
2023-09-27 22:50:26.083 
Epoch 332/1000 
	 loss: 16.7551, MinusLogProbMetric: 16.7551, val_loss: 17.2350, val_MinusLogProbMetric: 17.2350

Epoch 332: val_loss did not improve from 17.18777
196/196 - 70s - loss: 16.7551 - MinusLogProbMetric: 16.7551 - val_loss: 17.2350 - val_MinusLogProbMetric: 17.2350 - lr: 1.6667e-04 - 70s/epoch - 358ms/step
Epoch 333/1000
2023-09-27 22:51:35.865 
Epoch 333/1000 
	 loss: 16.7592, MinusLogProbMetric: 16.7592, val_loss: 17.1777, val_MinusLogProbMetric: 17.1777

Epoch 333: val_loss improved from 17.18777 to 17.17773, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 71s - loss: 16.7592 - MinusLogProbMetric: 16.7592 - val_loss: 17.1777 - val_MinusLogProbMetric: 17.1777 - lr: 1.6667e-04 - 71s/epoch - 362ms/step
Epoch 334/1000
2023-09-27 22:52:46.693 
Epoch 334/1000 
	 loss: 16.8124, MinusLogProbMetric: 16.8124, val_loss: 17.2811, val_MinusLogProbMetric: 17.2811

Epoch 334: val_loss did not improve from 17.17773
196/196 - 70s - loss: 16.8124 - MinusLogProbMetric: 16.8124 - val_loss: 17.2811 - val_MinusLogProbMetric: 17.2811 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 335/1000
2023-09-27 22:53:56.471 
Epoch 335/1000 
	 loss: 16.8087, MinusLogProbMetric: 16.8087, val_loss: 17.5777, val_MinusLogProbMetric: 17.5777

Epoch 335: val_loss did not improve from 17.17773
196/196 - 70s - loss: 16.8087 - MinusLogProbMetric: 16.8087 - val_loss: 17.5777 - val_MinusLogProbMetric: 17.5777 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 336/1000
2023-09-27 22:55:05.601 
Epoch 336/1000 
	 loss: 16.7464, MinusLogProbMetric: 16.7464, val_loss: 17.2095, val_MinusLogProbMetric: 17.2095

Epoch 336: val_loss did not improve from 17.17773
196/196 - 69s - loss: 16.7464 - MinusLogProbMetric: 16.7464 - val_loss: 17.2095 - val_MinusLogProbMetric: 17.2095 - lr: 1.6667e-04 - 69s/epoch - 353ms/step
Epoch 337/1000
2023-09-27 22:56:15.540 
Epoch 337/1000 
	 loss: 16.7784, MinusLogProbMetric: 16.7784, val_loss: 17.1881, val_MinusLogProbMetric: 17.1881

Epoch 337: val_loss did not improve from 17.17773
196/196 - 70s - loss: 16.7784 - MinusLogProbMetric: 16.7784 - val_loss: 17.1881 - val_MinusLogProbMetric: 17.1881 - lr: 1.6667e-04 - 70s/epoch - 357ms/step
Epoch 338/1000
2023-09-27 22:57:24.959 
Epoch 338/1000 
	 loss: 16.8140, MinusLogProbMetric: 16.8140, val_loss: 17.3568, val_MinusLogProbMetric: 17.3568

Epoch 338: val_loss did not improve from 17.17773
196/196 - 69s - loss: 16.8140 - MinusLogProbMetric: 16.8140 - val_loss: 17.3568 - val_MinusLogProbMetric: 17.3568 - lr: 1.6667e-04 - 69s/epoch - 354ms/step
Epoch 339/1000
2023-09-27 22:58:34.069 
Epoch 339/1000 
	 loss: 16.7341, MinusLogProbMetric: 16.7341, val_loss: 17.1240, val_MinusLogProbMetric: 17.1240

Epoch 339: val_loss improved from 17.17773 to 17.12404, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 70s - loss: 16.7341 - MinusLogProbMetric: 16.7341 - val_loss: 17.1240 - val_MinusLogProbMetric: 17.1240 - lr: 1.6667e-04 - 70s/epoch - 357ms/step
Epoch 340/1000
2023-09-27 22:59:44.817 
Epoch 340/1000 
	 loss: 16.7514, MinusLogProbMetric: 16.7514, val_loss: 17.2159, val_MinusLogProbMetric: 17.2159

Epoch 340: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7514 - MinusLogProbMetric: 16.7514 - val_loss: 17.2159 - val_MinusLogProbMetric: 17.2159 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 341/1000
2023-09-27 23:00:54.505 
Epoch 341/1000 
	 loss: 16.7538, MinusLogProbMetric: 16.7538, val_loss: 17.3442, val_MinusLogProbMetric: 17.3442

Epoch 341: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7538 - MinusLogProbMetric: 16.7538 - val_loss: 17.3442 - val_MinusLogProbMetric: 17.3442 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 342/1000
2023-09-27 23:02:04.365 
Epoch 342/1000 
	 loss: 16.7690, MinusLogProbMetric: 16.7690, val_loss: 17.2711, val_MinusLogProbMetric: 17.2711

Epoch 342: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7690 - MinusLogProbMetric: 16.7690 - val_loss: 17.2711 - val_MinusLogProbMetric: 17.2711 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 343/1000
2023-09-27 23:03:14.403 
Epoch 343/1000 
	 loss: 16.7658, MinusLogProbMetric: 16.7658, val_loss: 17.3642, val_MinusLogProbMetric: 17.3642

Epoch 343: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7658 - MinusLogProbMetric: 16.7658 - val_loss: 17.3642 - val_MinusLogProbMetric: 17.3642 - lr: 1.6667e-04 - 70s/epoch - 357ms/step
Epoch 344/1000
2023-09-27 23:04:24.392 
Epoch 344/1000 
	 loss: 16.8153, MinusLogProbMetric: 16.8153, val_loss: 17.2094, val_MinusLogProbMetric: 17.2094

Epoch 344: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.8153 - MinusLogProbMetric: 16.8153 - val_loss: 17.2094 - val_MinusLogProbMetric: 17.2094 - lr: 1.6667e-04 - 70s/epoch - 357ms/step
Epoch 345/1000
2023-09-27 23:05:33.915 
Epoch 345/1000 
	 loss: 16.7415, MinusLogProbMetric: 16.7415, val_loss: 17.2521, val_MinusLogProbMetric: 17.2521

Epoch 345: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7415 - MinusLogProbMetric: 16.7415 - val_loss: 17.2521 - val_MinusLogProbMetric: 17.2521 - lr: 1.6667e-04 - 70s/epoch - 355ms/step
Epoch 346/1000
2023-09-27 23:06:43.657 
Epoch 346/1000 
	 loss: 16.7712, MinusLogProbMetric: 16.7712, val_loss: 17.3840, val_MinusLogProbMetric: 17.3840

Epoch 346: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7712 - MinusLogProbMetric: 16.7712 - val_loss: 17.3840 - val_MinusLogProbMetric: 17.3840 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 347/1000
2023-09-27 23:07:53.385 
Epoch 347/1000 
	 loss: 16.7529, MinusLogProbMetric: 16.7529, val_loss: 17.2798, val_MinusLogProbMetric: 17.2798

Epoch 347: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7529 - MinusLogProbMetric: 16.7529 - val_loss: 17.2798 - val_MinusLogProbMetric: 17.2798 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 348/1000
2023-09-27 23:09:03.028 
Epoch 348/1000 
	 loss: 16.7764, MinusLogProbMetric: 16.7764, val_loss: 17.6949, val_MinusLogProbMetric: 17.6949

Epoch 348: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7764 - MinusLogProbMetric: 16.7764 - val_loss: 17.6949 - val_MinusLogProbMetric: 17.6949 - lr: 1.6667e-04 - 70s/epoch - 355ms/step
Epoch 349/1000
2023-09-27 23:10:12.570 
Epoch 349/1000 
	 loss: 16.7361, MinusLogProbMetric: 16.7361, val_loss: 17.2728, val_MinusLogProbMetric: 17.2728

Epoch 349: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7361 - MinusLogProbMetric: 16.7361 - val_loss: 17.2728 - val_MinusLogProbMetric: 17.2728 - lr: 1.6667e-04 - 70s/epoch - 355ms/step
Epoch 350/1000
2023-09-27 23:11:22.357 
Epoch 350/1000 
	 loss: 16.7951, MinusLogProbMetric: 16.7951, val_loss: 17.4234, val_MinusLogProbMetric: 17.4234

Epoch 350: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7951 - MinusLogProbMetric: 16.7951 - val_loss: 17.4234 - val_MinusLogProbMetric: 17.4234 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 351/1000
2023-09-27 23:12:32.320 
Epoch 351/1000 
	 loss: 16.7751, MinusLogProbMetric: 16.7751, val_loss: 17.2703, val_MinusLogProbMetric: 17.2703

Epoch 351: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7751 - MinusLogProbMetric: 16.7751 - val_loss: 17.2703 - val_MinusLogProbMetric: 17.2703 - lr: 1.6667e-04 - 70s/epoch - 357ms/step
Epoch 352/1000
2023-09-27 23:13:42.687 
Epoch 352/1000 
	 loss: 16.7586, MinusLogProbMetric: 16.7586, val_loss: 17.5890, val_MinusLogProbMetric: 17.5890

Epoch 352: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7586 - MinusLogProbMetric: 16.7586 - val_loss: 17.5890 - val_MinusLogProbMetric: 17.5890 - lr: 1.6667e-04 - 70s/epoch - 359ms/step
Epoch 353/1000
2023-09-27 23:14:52.529 
Epoch 353/1000 
	 loss: 16.7886, MinusLogProbMetric: 16.7886, val_loss: 17.2686, val_MinusLogProbMetric: 17.2686

Epoch 353: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7886 - MinusLogProbMetric: 16.7886 - val_loss: 17.2686 - val_MinusLogProbMetric: 17.2686 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 354/1000
2023-09-27 23:16:02.388 
Epoch 354/1000 
	 loss: 16.7557, MinusLogProbMetric: 16.7557, val_loss: 17.2852, val_MinusLogProbMetric: 17.2852

Epoch 354: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7557 - MinusLogProbMetric: 16.7557 - val_loss: 17.2852 - val_MinusLogProbMetric: 17.2852 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 355/1000
2023-09-27 23:17:11.352 
Epoch 355/1000 
	 loss: 16.7146, MinusLogProbMetric: 16.7146, val_loss: 17.1839, val_MinusLogProbMetric: 17.1839

Epoch 355: val_loss did not improve from 17.12404
196/196 - 69s - loss: 16.7146 - MinusLogProbMetric: 16.7146 - val_loss: 17.1839 - val_MinusLogProbMetric: 17.1839 - lr: 1.6667e-04 - 69s/epoch - 352ms/step
Epoch 356/1000
2023-09-27 23:18:20.936 
Epoch 356/1000 
	 loss: 16.7266, MinusLogProbMetric: 16.7266, val_loss: 17.2092, val_MinusLogProbMetric: 17.2092

Epoch 356: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7266 - MinusLogProbMetric: 16.7266 - val_loss: 17.2092 - val_MinusLogProbMetric: 17.2092 - lr: 1.6667e-04 - 70s/epoch - 355ms/step
Epoch 357/1000
2023-09-27 23:19:30.566 
Epoch 357/1000 
	 loss: 16.7637, MinusLogProbMetric: 16.7637, val_loss: 17.3828, val_MinusLogProbMetric: 17.3828

Epoch 357: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7637 - MinusLogProbMetric: 16.7637 - val_loss: 17.3828 - val_MinusLogProbMetric: 17.3828 - lr: 1.6667e-04 - 70s/epoch - 355ms/step
Epoch 358/1000
2023-09-27 23:20:40.828 
Epoch 358/1000 
	 loss: 16.7377, MinusLogProbMetric: 16.7377, val_loss: 17.2632, val_MinusLogProbMetric: 17.2632

Epoch 358: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7377 - MinusLogProbMetric: 16.7377 - val_loss: 17.2632 - val_MinusLogProbMetric: 17.2632 - lr: 1.6667e-04 - 70s/epoch - 358ms/step
Epoch 359/1000
2023-09-27 23:21:50.567 
Epoch 359/1000 
	 loss: 16.7579, MinusLogProbMetric: 16.7579, val_loss: 17.1766, val_MinusLogProbMetric: 17.1766

Epoch 359: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7579 - MinusLogProbMetric: 16.7579 - val_loss: 17.1766 - val_MinusLogProbMetric: 17.1766 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 360/1000
2023-09-27 23:23:00.524 
Epoch 360/1000 
	 loss: 16.7917, MinusLogProbMetric: 16.7917, val_loss: 17.3973, val_MinusLogProbMetric: 17.3973

Epoch 360: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7917 - MinusLogProbMetric: 16.7917 - val_loss: 17.3973 - val_MinusLogProbMetric: 17.3973 - lr: 1.6667e-04 - 70s/epoch - 357ms/step
Epoch 361/1000
2023-09-27 23:24:09.640 
Epoch 361/1000 
	 loss: 16.7300, MinusLogProbMetric: 16.7300, val_loss: 17.3138, val_MinusLogProbMetric: 17.3138

Epoch 361: val_loss did not improve from 17.12404
196/196 - 69s - loss: 16.7300 - MinusLogProbMetric: 16.7300 - val_loss: 17.3138 - val_MinusLogProbMetric: 17.3138 - lr: 1.6667e-04 - 69s/epoch - 353ms/step
Epoch 362/1000
2023-09-27 23:25:16.991 
Epoch 362/1000 
	 loss: 16.8059, MinusLogProbMetric: 16.8059, val_loss: 17.3022, val_MinusLogProbMetric: 17.3022

Epoch 362: val_loss did not improve from 17.12404
196/196 - 67s - loss: 16.8059 - MinusLogProbMetric: 16.8059 - val_loss: 17.3022 - val_MinusLogProbMetric: 17.3022 - lr: 1.6667e-04 - 67s/epoch - 344ms/step
Epoch 363/1000
2023-09-27 23:26:20.876 
Epoch 363/1000 
	 loss: 16.7363, MinusLogProbMetric: 16.7363, val_loss: 17.5962, val_MinusLogProbMetric: 17.5962

Epoch 363: val_loss did not improve from 17.12404
196/196 - 64s - loss: 16.7363 - MinusLogProbMetric: 16.7363 - val_loss: 17.5962 - val_MinusLogProbMetric: 17.5962 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 364/1000
2023-09-27 23:27:25.058 
Epoch 364/1000 
	 loss: 16.7368, MinusLogProbMetric: 16.7368, val_loss: 17.2009, val_MinusLogProbMetric: 17.2009

Epoch 364: val_loss did not improve from 17.12404
196/196 - 64s - loss: 16.7368 - MinusLogProbMetric: 16.7368 - val_loss: 17.2009 - val_MinusLogProbMetric: 17.2009 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 365/1000
2023-09-27 23:28:34.088 
Epoch 365/1000 
	 loss: 16.7677, MinusLogProbMetric: 16.7677, val_loss: 17.3737, val_MinusLogProbMetric: 17.3737

Epoch 365: val_loss did not improve from 17.12404
196/196 - 69s - loss: 16.7677 - MinusLogProbMetric: 16.7677 - val_loss: 17.3737 - val_MinusLogProbMetric: 17.3737 - lr: 1.6667e-04 - 69s/epoch - 352ms/step
Epoch 366/1000
2023-09-27 23:29:43.907 
Epoch 366/1000 
	 loss: 16.7276, MinusLogProbMetric: 16.7276, val_loss: 17.2651, val_MinusLogProbMetric: 17.2651

Epoch 366: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7276 - MinusLogProbMetric: 16.7276 - val_loss: 17.2651 - val_MinusLogProbMetric: 17.2651 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 367/1000
2023-09-27 23:30:54.114 
Epoch 367/1000 
	 loss: 16.8052, MinusLogProbMetric: 16.8052, val_loss: 17.2431, val_MinusLogProbMetric: 17.2431

Epoch 367: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.8052 - MinusLogProbMetric: 16.8052 - val_loss: 17.2431 - val_MinusLogProbMetric: 17.2431 - lr: 1.6667e-04 - 70s/epoch - 358ms/step
Epoch 368/1000
2023-09-27 23:32:03.911 
Epoch 368/1000 
	 loss: 16.7359, MinusLogProbMetric: 16.7359, val_loss: 17.4944, val_MinusLogProbMetric: 17.4944

Epoch 368: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7359 - MinusLogProbMetric: 16.7359 - val_loss: 17.4944 - val_MinusLogProbMetric: 17.4944 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 369/1000
2023-09-27 23:33:13.931 
Epoch 369/1000 
	 loss: 16.7486, MinusLogProbMetric: 16.7486, val_loss: 17.2959, val_MinusLogProbMetric: 17.2959

Epoch 369: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7486 - MinusLogProbMetric: 16.7486 - val_loss: 17.2959 - val_MinusLogProbMetric: 17.2959 - lr: 1.6667e-04 - 70s/epoch - 357ms/step
Epoch 370/1000
2023-09-27 23:34:24.024 
Epoch 370/1000 
	 loss: 16.7056, MinusLogProbMetric: 16.7056, val_loss: 17.3015, val_MinusLogProbMetric: 17.3015

Epoch 370: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7056 - MinusLogProbMetric: 16.7056 - val_loss: 17.3015 - val_MinusLogProbMetric: 17.3015 - lr: 1.6667e-04 - 70s/epoch - 358ms/step
Epoch 371/1000
2023-09-27 23:35:33.318 
Epoch 371/1000 
	 loss: 16.7493, MinusLogProbMetric: 16.7493, val_loss: 17.4618, val_MinusLogProbMetric: 17.4618

Epoch 371: val_loss did not improve from 17.12404
196/196 - 69s - loss: 16.7493 - MinusLogProbMetric: 16.7493 - val_loss: 17.4618 - val_MinusLogProbMetric: 17.4618 - lr: 1.6667e-04 - 69s/epoch - 354ms/step
Epoch 372/1000
2023-09-27 23:36:43.048 
Epoch 372/1000 
	 loss: 16.7447, MinusLogProbMetric: 16.7447, val_loss: 17.4781, val_MinusLogProbMetric: 17.4781

Epoch 372: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7447 - MinusLogProbMetric: 16.7447 - val_loss: 17.4781 - val_MinusLogProbMetric: 17.4781 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 373/1000
2023-09-27 23:37:53.029 
Epoch 373/1000 
	 loss: 16.7651, MinusLogProbMetric: 16.7651, val_loss: 17.4206, val_MinusLogProbMetric: 17.4206

Epoch 373: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7651 - MinusLogProbMetric: 16.7651 - val_loss: 17.4206 - val_MinusLogProbMetric: 17.4206 - lr: 1.6667e-04 - 70s/epoch - 357ms/step
Epoch 374/1000
2023-09-27 23:39:02.492 
Epoch 374/1000 
	 loss: 16.7112, MinusLogProbMetric: 16.7112, val_loss: 17.3103, val_MinusLogProbMetric: 17.3103

Epoch 374: val_loss did not improve from 17.12404
196/196 - 69s - loss: 16.7112 - MinusLogProbMetric: 16.7112 - val_loss: 17.3103 - val_MinusLogProbMetric: 17.3103 - lr: 1.6667e-04 - 69s/epoch - 354ms/step
Epoch 375/1000
2023-09-27 23:40:12.435 
Epoch 375/1000 
	 loss: 16.7304, MinusLogProbMetric: 16.7304, val_loss: 17.4489, val_MinusLogProbMetric: 17.4489

Epoch 375: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7304 - MinusLogProbMetric: 16.7304 - val_loss: 17.4489 - val_MinusLogProbMetric: 17.4489 - lr: 1.6667e-04 - 70s/epoch - 357ms/step
Epoch 376/1000
2023-09-27 23:41:22.083 
Epoch 376/1000 
	 loss: 16.7668, MinusLogProbMetric: 16.7668, val_loss: 17.3847, val_MinusLogProbMetric: 17.3847

Epoch 376: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7668 - MinusLogProbMetric: 16.7668 - val_loss: 17.3847 - val_MinusLogProbMetric: 17.3847 - lr: 1.6667e-04 - 70s/epoch - 355ms/step
Epoch 377/1000
2023-09-27 23:42:32.382 
Epoch 377/1000 
	 loss: 16.7210, MinusLogProbMetric: 16.7210, val_loss: 17.2222, val_MinusLogProbMetric: 17.2222

Epoch 377: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7210 - MinusLogProbMetric: 16.7210 - val_loss: 17.2222 - val_MinusLogProbMetric: 17.2222 - lr: 1.6667e-04 - 70s/epoch - 359ms/step
Epoch 378/1000
2023-09-27 23:43:42.216 
Epoch 378/1000 
	 loss: 16.6961, MinusLogProbMetric: 16.6961, val_loss: 17.3152, val_MinusLogProbMetric: 17.3152

Epoch 378: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.6961 - MinusLogProbMetric: 16.6961 - val_loss: 17.3152 - val_MinusLogProbMetric: 17.3152 - lr: 1.6667e-04 - 70s/epoch - 356ms/step
Epoch 379/1000
2023-09-27 23:44:51.595 
Epoch 379/1000 
	 loss: 16.7278, MinusLogProbMetric: 16.7278, val_loss: 17.2308, val_MinusLogProbMetric: 17.2308

Epoch 379: val_loss did not improve from 17.12404
196/196 - 69s - loss: 16.7278 - MinusLogProbMetric: 16.7278 - val_loss: 17.2308 - val_MinusLogProbMetric: 17.2308 - lr: 1.6667e-04 - 69s/epoch - 354ms/step
Epoch 380/1000
2023-09-27 23:46:00.420 
Epoch 380/1000 
	 loss: 16.7412, MinusLogProbMetric: 16.7412, val_loss: 17.2340, val_MinusLogProbMetric: 17.2340

Epoch 380: val_loss did not improve from 17.12404
196/196 - 69s - loss: 16.7412 - MinusLogProbMetric: 16.7412 - val_loss: 17.2340 - val_MinusLogProbMetric: 17.2340 - lr: 1.6667e-04 - 69s/epoch - 351ms/step
Epoch 381/1000
2023-09-27 23:47:09.728 
Epoch 381/1000 
	 loss: 16.7296, MinusLogProbMetric: 16.7296, val_loss: 17.4167, val_MinusLogProbMetric: 17.4167

Epoch 381: val_loss did not improve from 17.12404
196/196 - 69s - loss: 16.7296 - MinusLogProbMetric: 16.7296 - val_loss: 17.4167 - val_MinusLogProbMetric: 17.4167 - lr: 1.6667e-04 - 69s/epoch - 354ms/step
Epoch 382/1000
2023-09-27 23:48:19.274 
Epoch 382/1000 
	 loss: 16.7431, MinusLogProbMetric: 16.7431, val_loss: 17.1727, val_MinusLogProbMetric: 17.1727

Epoch 382: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7431 - MinusLogProbMetric: 16.7431 - val_loss: 17.1727 - val_MinusLogProbMetric: 17.1727 - lr: 1.6667e-04 - 70s/epoch - 355ms/step
Epoch 383/1000
2023-09-27 23:49:28.646 
Epoch 383/1000 
	 loss: 16.6946, MinusLogProbMetric: 16.6946, val_loss: 17.2482, val_MinusLogProbMetric: 17.2482

Epoch 383: val_loss did not improve from 17.12404
196/196 - 69s - loss: 16.6946 - MinusLogProbMetric: 16.6946 - val_loss: 17.2482 - val_MinusLogProbMetric: 17.2482 - lr: 1.6667e-04 - 69s/epoch - 354ms/step
Epoch 384/1000
2023-09-27 23:50:37.977 
Epoch 384/1000 
	 loss: 16.7057, MinusLogProbMetric: 16.7057, val_loss: 17.2818, val_MinusLogProbMetric: 17.2818

Epoch 384: val_loss did not improve from 17.12404
196/196 - 69s - loss: 16.7057 - MinusLogProbMetric: 16.7057 - val_loss: 17.2818 - val_MinusLogProbMetric: 17.2818 - lr: 1.6667e-04 - 69s/epoch - 354ms/step
Epoch 385/1000
2023-09-27 23:51:47.863 
Epoch 385/1000 
	 loss: 16.7223, MinusLogProbMetric: 16.7223, val_loss: 17.5510, val_MinusLogProbMetric: 17.5510

Epoch 385: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7223 - MinusLogProbMetric: 16.7223 - val_loss: 17.5510 - val_MinusLogProbMetric: 17.5510 - lr: 1.6667e-04 - 70s/epoch - 357ms/step
Epoch 386/1000
2023-09-27 23:52:57.446 
Epoch 386/1000 
	 loss: 16.7132, MinusLogProbMetric: 16.7132, val_loss: 17.2041, val_MinusLogProbMetric: 17.2041

Epoch 386: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7132 - MinusLogProbMetric: 16.7132 - val_loss: 17.2041 - val_MinusLogProbMetric: 17.2041 - lr: 1.6667e-04 - 70s/epoch - 355ms/step
Epoch 387/1000
2023-09-27 23:54:07.474 
Epoch 387/1000 
	 loss: 16.7479, MinusLogProbMetric: 16.7479, val_loss: 17.2774, val_MinusLogProbMetric: 17.2774

Epoch 387: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7479 - MinusLogProbMetric: 16.7479 - val_loss: 17.2774 - val_MinusLogProbMetric: 17.2774 - lr: 1.6667e-04 - 70s/epoch - 357ms/step
Epoch 388/1000
2023-09-27 23:55:16.917 
Epoch 388/1000 
	 loss: 16.7267, MinusLogProbMetric: 16.7267, val_loss: 17.4019, val_MinusLogProbMetric: 17.4019

Epoch 388: val_loss did not improve from 17.12404
196/196 - 69s - loss: 16.7267 - MinusLogProbMetric: 16.7267 - val_loss: 17.4019 - val_MinusLogProbMetric: 17.4019 - lr: 1.6667e-04 - 69s/epoch - 354ms/step
Epoch 389/1000
2023-09-27 23:56:26.968 
Epoch 389/1000 
	 loss: 16.7249, MinusLogProbMetric: 16.7249, val_loss: 17.1827, val_MinusLogProbMetric: 17.1827

Epoch 389: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.7249 - MinusLogProbMetric: 16.7249 - val_loss: 17.1827 - val_MinusLogProbMetric: 17.1827 - lr: 1.6667e-04 - 70s/epoch - 357ms/step
Epoch 390/1000
2023-09-27 23:57:36.540 
Epoch 390/1000 
	 loss: 16.5145, MinusLogProbMetric: 16.5145, val_loss: 17.1697, val_MinusLogProbMetric: 17.1697

Epoch 390: val_loss did not improve from 17.12404
196/196 - 70s - loss: 16.5145 - MinusLogProbMetric: 16.5145 - val_loss: 17.1697 - val_MinusLogProbMetric: 17.1697 - lr: 8.3333e-05 - 70s/epoch - 355ms/step
Epoch 391/1000
2023-09-27 23:58:45.797 
Epoch 391/1000 
	 loss: 16.5155, MinusLogProbMetric: 16.5155, val_loss: 17.1562, val_MinusLogProbMetric: 17.1562

Epoch 391: val_loss did not improve from 17.12404
196/196 - 69s - loss: 16.5155 - MinusLogProbMetric: 16.5155 - val_loss: 17.1562 - val_MinusLogProbMetric: 17.1562 - lr: 8.3333e-05 - 69s/epoch - 353ms/step
Epoch 392/1000
2023-09-27 23:59:55.369 
Epoch 392/1000 
	 loss: 16.5190, MinusLogProbMetric: 16.5190, val_loss: 17.0506, val_MinusLogProbMetric: 17.0506

Epoch 392: val_loss improved from 17.12404 to 17.05065, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 71s - loss: 16.5190 - MinusLogProbMetric: 16.5190 - val_loss: 17.0506 - val_MinusLogProbMetric: 17.0506 - lr: 8.3333e-05 - 71s/epoch - 360ms/step
Epoch 393/1000
2023-09-28 00:01:05.843 
Epoch 393/1000 
	 loss: 16.5307, MinusLogProbMetric: 16.5307, val_loss: 17.0785, val_MinusLogProbMetric: 17.0785

Epoch 393: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5307 - MinusLogProbMetric: 16.5307 - val_loss: 17.0785 - val_MinusLogProbMetric: 17.0785 - lr: 8.3333e-05 - 69s/epoch - 354ms/step
Epoch 394/1000
2023-09-28 00:02:15.608 
Epoch 394/1000 
	 loss: 16.5499, MinusLogProbMetric: 16.5499, val_loss: 17.1090, val_MinusLogProbMetric: 17.1090

Epoch 394: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.5499 - MinusLogProbMetric: 16.5499 - val_loss: 17.1090 - val_MinusLogProbMetric: 17.1090 - lr: 8.3333e-05 - 70s/epoch - 356ms/step
Epoch 395/1000
2023-09-28 00:03:25.378 
Epoch 395/1000 
	 loss: 16.5168, MinusLogProbMetric: 16.5168, val_loss: 17.0790, val_MinusLogProbMetric: 17.0790

Epoch 395: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.5168 - MinusLogProbMetric: 16.5168 - val_loss: 17.0790 - val_MinusLogProbMetric: 17.0790 - lr: 8.3333e-05 - 70s/epoch - 356ms/step
Epoch 396/1000
2023-09-28 00:04:35.472 
Epoch 396/1000 
	 loss: 16.5461, MinusLogProbMetric: 16.5461, val_loss: 17.1075, val_MinusLogProbMetric: 17.1075

Epoch 396: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.5461 - MinusLogProbMetric: 16.5461 - val_loss: 17.1075 - val_MinusLogProbMetric: 17.1075 - lr: 8.3333e-05 - 70s/epoch - 358ms/step
Epoch 397/1000
2023-09-28 00:05:44.824 
Epoch 397/1000 
	 loss: 16.5050, MinusLogProbMetric: 16.5050, val_loss: 17.1658, val_MinusLogProbMetric: 17.1658

Epoch 397: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5050 - MinusLogProbMetric: 16.5050 - val_loss: 17.1658 - val_MinusLogProbMetric: 17.1658 - lr: 8.3333e-05 - 69s/epoch - 354ms/step
Epoch 398/1000
2023-09-28 00:06:53.989 
Epoch 398/1000 
	 loss: 16.5148, MinusLogProbMetric: 16.5148, val_loss: 17.0792, val_MinusLogProbMetric: 17.0792

Epoch 398: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5148 - MinusLogProbMetric: 16.5148 - val_loss: 17.0792 - val_MinusLogProbMetric: 17.0792 - lr: 8.3333e-05 - 69s/epoch - 353ms/step
Epoch 399/1000
2023-09-28 00:08:03.564 
Epoch 399/1000 
	 loss: 16.5367, MinusLogProbMetric: 16.5367, val_loss: 17.1214, val_MinusLogProbMetric: 17.1214

Epoch 399: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.5367 - MinusLogProbMetric: 16.5367 - val_loss: 17.1214 - val_MinusLogProbMetric: 17.1214 - lr: 8.3333e-05 - 70s/epoch - 355ms/step
Epoch 400/1000
2023-09-28 00:09:13.356 
Epoch 400/1000 
	 loss: 16.5186, MinusLogProbMetric: 16.5186, val_loss: 17.1896, val_MinusLogProbMetric: 17.1896

Epoch 400: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.5186 - MinusLogProbMetric: 16.5186 - val_loss: 17.1896 - val_MinusLogProbMetric: 17.1896 - lr: 8.3333e-05 - 70s/epoch - 356ms/step
Epoch 401/1000
2023-09-28 00:10:23.152 
Epoch 401/1000 
	 loss: 16.5333, MinusLogProbMetric: 16.5333, val_loss: 17.1306, val_MinusLogProbMetric: 17.1306

Epoch 401: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.5333 - MinusLogProbMetric: 16.5333 - val_loss: 17.1306 - val_MinusLogProbMetric: 17.1306 - lr: 8.3333e-05 - 70s/epoch - 356ms/step
Epoch 402/1000
2023-09-28 00:11:32.740 
Epoch 402/1000 
	 loss: 16.5052, MinusLogProbMetric: 16.5052, val_loss: 17.1419, val_MinusLogProbMetric: 17.1419

Epoch 402: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.5052 - MinusLogProbMetric: 16.5052 - val_loss: 17.1419 - val_MinusLogProbMetric: 17.1419 - lr: 8.3333e-05 - 70s/epoch - 355ms/step
Epoch 403/1000
2023-09-28 00:12:41.763 
Epoch 403/1000 
	 loss: 16.5517, MinusLogProbMetric: 16.5517, val_loss: 17.1256, val_MinusLogProbMetric: 17.1256

Epoch 403: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5517 - MinusLogProbMetric: 16.5517 - val_loss: 17.1256 - val_MinusLogProbMetric: 17.1256 - lr: 8.3333e-05 - 69s/epoch - 352ms/step
Epoch 404/1000
2023-09-28 00:13:50.915 
Epoch 404/1000 
	 loss: 16.5276, MinusLogProbMetric: 16.5276, val_loss: 17.0837, val_MinusLogProbMetric: 17.0837

Epoch 404: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5276 - MinusLogProbMetric: 16.5276 - val_loss: 17.0837 - val_MinusLogProbMetric: 17.0837 - lr: 8.3333e-05 - 69s/epoch - 353ms/step
Epoch 405/1000
2023-09-28 00:15:00.276 
Epoch 405/1000 
	 loss: 16.5327, MinusLogProbMetric: 16.5327, val_loss: 17.1195, val_MinusLogProbMetric: 17.1195

Epoch 405: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5327 - MinusLogProbMetric: 16.5327 - val_loss: 17.1195 - val_MinusLogProbMetric: 17.1195 - lr: 8.3333e-05 - 69s/epoch - 354ms/step
Epoch 406/1000
2023-09-28 00:16:09.006 
Epoch 406/1000 
	 loss: 16.5179, MinusLogProbMetric: 16.5179, val_loss: 17.1089, val_MinusLogProbMetric: 17.1089

Epoch 406: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5179 - MinusLogProbMetric: 16.5179 - val_loss: 17.1089 - val_MinusLogProbMetric: 17.1089 - lr: 8.3333e-05 - 69s/epoch - 351ms/step
Epoch 407/1000
2023-09-28 00:17:18.814 
Epoch 407/1000 
	 loss: 16.5291, MinusLogProbMetric: 16.5291, val_loss: 17.1114, val_MinusLogProbMetric: 17.1114

Epoch 407: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.5291 - MinusLogProbMetric: 16.5291 - val_loss: 17.1114 - val_MinusLogProbMetric: 17.1114 - lr: 8.3333e-05 - 70s/epoch - 356ms/step
Epoch 408/1000
2023-09-28 00:18:28.028 
Epoch 408/1000 
	 loss: 16.5344, MinusLogProbMetric: 16.5344, val_loss: 17.2435, val_MinusLogProbMetric: 17.2435

Epoch 408: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5344 - MinusLogProbMetric: 16.5344 - val_loss: 17.2435 - val_MinusLogProbMetric: 17.2435 - lr: 8.3333e-05 - 69s/epoch - 353ms/step
Epoch 409/1000
2023-09-28 00:19:37.661 
Epoch 409/1000 
	 loss: 16.5153, MinusLogProbMetric: 16.5153, val_loss: 17.1019, val_MinusLogProbMetric: 17.1019

Epoch 409: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.5153 - MinusLogProbMetric: 16.5153 - val_loss: 17.1019 - val_MinusLogProbMetric: 17.1019 - lr: 8.3333e-05 - 70s/epoch - 355ms/step
Epoch 410/1000
2023-09-28 00:20:46.990 
Epoch 410/1000 
	 loss: 16.5435, MinusLogProbMetric: 16.5435, val_loss: 17.0993, val_MinusLogProbMetric: 17.0993

Epoch 410: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5435 - MinusLogProbMetric: 16.5435 - val_loss: 17.0993 - val_MinusLogProbMetric: 17.0993 - lr: 8.3333e-05 - 69s/epoch - 354ms/step
Epoch 411/1000
2023-09-28 00:21:56.512 
Epoch 411/1000 
	 loss: 16.5044, MinusLogProbMetric: 16.5044, val_loss: 17.1453, val_MinusLogProbMetric: 17.1453

Epoch 411: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.5044 - MinusLogProbMetric: 16.5044 - val_loss: 17.1453 - val_MinusLogProbMetric: 17.1453 - lr: 8.3333e-05 - 70s/epoch - 355ms/step
Epoch 412/1000
2023-09-28 00:23:06.073 
Epoch 412/1000 
	 loss: 16.5183, MinusLogProbMetric: 16.5183, val_loss: 17.1552, val_MinusLogProbMetric: 17.1552

Epoch 412: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.5183 - MinusLogProbMetric: 16.5183 - val_loss: 17.1552 - val_MinusLogProbMetric: 17.1552 - lr: 8.3333e-05 - 70s/epoch - 355ms/step
Epoch 413/1000
2023-09-28 00:24:14.966 
Epoch 413/1000 
	 loss: 16.5098, MinusLogProbMetric: 16.5098, val_loss: 17.0700, val_MinusLogProbMetric: 17.0700

Epoch 413: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5098 - MinusLogProbMetric: 16.5098 - val_loss: 17.0700 - val_MinusLogProbMetric: 17.0700 - lr: 8.3333e-05 - 69s/epoch - 351ms/step
Epoch 414/1000
2023-09-28 00:25:24.671 
Epoch 414/1000 
	 loss: 16.4904, MinusLogProbMetric: 16.4904, val_loss: 17.0550, val_MinusLogProbMetric: 17.0550

Epoch 414: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.4904 - MinusLogProbMetric: 16.4904 - val_loss: 17.0550 - val_MinusLogProbMetric: 17.0550 - lr: 8.3333e-05 - 70s/epoch - 356ms/step
Epoch 415/1000
2023-09-28 00:26:33.499 
Epoch 415/1000 
	 loss: 16.5164, MinusLogProbMetric: 16.5164, val_loss: 17.3112, val_MinusLogProbMetric: 17.3112

Epoch 415: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5164 - MinusLogProbMetric: 16.5164 - val_loss: 17.3112 - val_MinusLogProbMetric: 17.3112 - lr: 8.3333e-05 - 69s/epoch - 351ms/step
Epoch 416/1000
2023-09-28 00:27:42.838 
Epoch 416/1000 
	 loss: 16.5373, MinusLogProbMetric: 16.5373, val_loss: 17.1629, val_MinusLogProbMetric: 17.1629

Epoch 416: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5373 - MinusLogProbMetric: 16.5373 - val_loss: 17.1629 - val_MinusLogProbMetric: 17.1629 - lr: 8.3333e-05 - 69s/epoch - 354ms/step
Epoch 417/1000
2023-09-28 00:28:52.440 
Epoch 417/1000 
	 loss: 16.5481, MinusLogProbMetric: 16.5481, val_loss: 17.1128, val_MinusLogProbMetric: 17.1128

Epoch 417: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.5481 - MinusLogProbMetric: 16.5481 - val_loss: 17.1128 - val_MinusLogProbMetric: 17.1128 - lr: 8.3333e-05 - 70s/epoch - 355ms/step
Epoch 418/1000
2023-09-28 00:30:02.331 
Epoch 418/1000 
	 loss: 16.5603, MinusLogProbMetric: 16.5603, val_loss: 17.1192, val_MinusLogProbMetric: 17.1192

Epoch 418: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.5603 - MinusLogProbMetric: 16.5603 - val_loss: 17.1192 - val_MinusLogProbMetric: 17.1192 - lr: 8.3333e-05 - 70s/epoch - 357ms/step
Epoch 419/1000
2023-09-28 00:31:12.087 
Epoch 419/1000 
	 loss: 16.5029, MinusLogProbMetric: 16.5029, val_loss: 17.0715, val_MinusLogProbMetric: 17.0715

Epoch 419: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.5029 - MinusLogProbMetric: 16.5029 - val_loss: 17.0715 - val_MinusLogProbMetric: 17.0715 - lr: 8.3333e-05 - 70s/epoch - 356ms/step
Epoch 420/1000
2023-09-28 00:32:22.042 
Epoch 420/1000 
	 loss: 16.5104, MinusLogProbMetric: 16.5104, val_loss: 17.2378, val_MinusLogProbMetric: 17.2378

Epoch 420: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.5104 - MinusLogProbMetric: 16.5104 - val_loss: 17.2378 - val_MinusLogProbMetric: 17.2378 - lr: 8.3333e-05 - 70s/epoch - 357ms/step
Epoch 421/1000
2023-09-28 00:33:31.481 
Epoch 421/1000 
	 loss: 16.5040, MinusLogProbMetric: 16.5040, val_loss: 17.0689, val_MinusLogProbMetric: 17.0689

Epoch 421: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5040 - MinusLogProbMetric: 16.5040 - val_loss: 17.0689 - val_MinusLogProbMetric: 17.0689 - lr: 8.3333e-05 - 69s/epoch - 354ms/step
Epoch 422/1000
2023-09-28 00:34:41.032 
Epoch 422/1000 
	 loss: 16.4908, MinusLogProbMetric: 16.4908, val_loss: 17.0856, val_MinusLogProbMetric: 17.0856

Epoch 422: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.4908 - MinusLogProbMetric: 16.4908 - val_loss: 17.0856 - val_MinusLogProbMetric: 17.0856 - lr: 8.3333e-05 - 70s/epoch - 355ms/step
Epoch 423/1000
2023-09-28 00:35:50.727 
Epoch 423/1000 
	 loss: 16.5003, MinusLogProbMetric: 16.5003, val_loss: 17.0545, val_MinusLogProbMetric: 17.0545

Epoch 423: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.5003 - MinusLogProbMetric: 16.5003 - val_loss: 17.0545 - val_MinusLogProbMetric: 17.0545 - lr: 8.3333e-05 - 70s/epoch - 356ms/step
Epoch 424/1000
2023-09-28 00:36:59.709 
Epoch 424/1000 
	 loss: 16.5623, MinusLogProbMetric: 16.5623, val_loss: 17.0514, val_MinusLogProbMetric: 17.0514

Epoch 424: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5623 - MinusLogProbMetric: 16.5623 - val_loss: 17.0514 - val_MinusLogProbMetric: 17.0514 - lr: 8.3333e-05 - 69s/epoch - 352ms/step
Epoch 425/1000
2023-09-28 00:37:58.738 
Epoch 425/1000 
	 loss: 16.5041, MinusLogProbMetric: 16.5041, val_loss: 17.1263, val_MinusLogProbMetric: 17.1263

Epoch 425: val_loss did not improve from 17.05065
196/196 - 59s - loss: 16.5041 - MinusLogProbMetric: 16.5041 - val_loss: 17.1263 - val_MinusLogProbMetric: 17.1263 - lr: 8.3333e-05 - 59s/epoch - 301ms/step
Epoch 426/1000
2023-09-28 00:39:01.057 
Epoch 426/1000 
	 loss: 16.5423, MinusLogProbMetric: 16.5423, val_loss: 17.1191, val_MinusLogProbMetric: 17.1191

Epoch 426: val_loss did not improve from 17.05065
196/196 - 62s - loss: 16.5423 - MinusLogProbMetric: 16.5423 - val_loss: 17.1191 - val_MinusLogProbMetric: 17.1191 - lr: 8.3333e-05 - 62s/epoch - 318ms/step
Epoch 427/1000
2023-09-28 00:40:01.689 
Epoch 427/1000 
	 loss: 16.5295, MinusLogProbMetric: 16.5295, val_loss: 17.0594, val_MinusLogProbMetric: 17.0594

Epoch 427: val_loss did not improve from 17.05065
196/196 - 61s - loss: 16.5295 - MinusLogProbMetric: 16.5295 - val_loss: 17.0594 - val_MinusLogProbMetric: 17.0594 - lr: 8.3333e-05 - 61s/epoch - 309ms/step
Epoch 428/1000
2023-09-28 00:41:00.601 
Epoch 428/1000 
	 loss: 16.5195, MinusLogProbMetric: 16.5195, val_loss: 17.1245, val_MinusLogProbMetric: 17.1245

Epoch 428: val_loss did not improve from 17.05065
196/196 - 59s - loss: 16.5195 - MinusLogProbMetric: 16.5195 - val_loss: 17.1245 - val_MinusLogProbMetric: 17.1245 - lr: 8.3333e-05 - 59s/epoch - 301ms/step
Epoch 429/1000
2023-09-28 00:42:10.177 
Epoch 429/1000 
	 loss: 16.4949, MinusLogProbMetric: 16.4949, val_loss: 17.2026, val_MinusLogProbMetric: 17.2026

Epoch 429: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.4949 - MinusLogProbMetric: 16.4949 - val_loss: 17.2026 - val_MinusLogProbMetric: 17.2026 - lr: 8.3333e-05 - 70s/epoch - 355ms/step
Epoch 430/1000
2023-09-28 00:43:19.118 
Epoch 430/1000 
	 loss: 16.5163, MinusLogProbMetric: 16.5163, val_loss: 17.1179, val_MinusLogProbMetric: 17.1179

Epoch 430: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5163 - MinusLogProbMetric: 16.5163 - val_loss: 17.1179 - val_MinusLogProbMetric: 17.1179 - lr: 8.3333e-05 - 69s/epoch - 352ms/step
Epoch 431/1000
2023-09-28 00:44:28.530 
Epoch 431/1000 
	 loss: 16.4976, MinusLogProbMetric: 16.4976, val_loss: 17.1095, val_MinusLogProbMetric: 17.1095

Epoch 431: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.4976 - MinusLogProbMetric: 16.4976 - val_loss: 17.1095 - val_MinusLogProbMetric: 17.1095 - lr: 8.3333e-05 - 69s/epoch - 354ms/step
Epoch 432/1000
2023-09-28 00:45:38.436 
Epoch 432/1000 
	 loss: 16.5342, MinusLogProbMetric: 16.5342, val_loss: 17.1235, val_MinusLogProbMetric: 17.1235

Epoch 432: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.5342 - MinusLogProbMetric: 16.5342 - val_loss: 17.1235 - val_MinusLogProbMetric: 17.1235 - lr: 8.3333e-05 - 70s/epoch - 357ms/step
Epoch 433/1000
2023-09-28 00:46:48.626 
Epoch 433/1000 
	 loss: 16.5184, MinusLogProbMetric: 16.5184, val_loss: 17.1017, val_MinusLogProbMetric: 17.1017

Epoch 433: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.5184 - MinusLogProbMetric: 16.5184 - val_loss: 17.1017 - val_MinusLogProbMetric: 17.1017 - lr: 8.3333e-05 - 70s/epoch - 358ms/step
Epoch 434/1000
2023-09-28 00:47:57.971 
Epoch 434/1000 
	 loss: 16.5142, MinusLogProbMetric: 16.5142, val_loss: 17.1099, val_MinusLogProbMetric: 17.1099

Epoch 434: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5142 - MinusLogProbMetric: 16.5142 - val_loss: 17.1099 - val_MinusLogProbMetric: 17.1099 - lr: 8.3333e-05 - 69s/epoch - 354ms/step
Epoch 435/1000
2023-09-28 00:49:07.804 
Epoch 435/1000 
	 loss: 16.5060, MinusLogProbMetric: 16.5060, val_loss: 17.0952, val_MinusLogProbMetric: 17.0952

Epoch 435: val_loss did not improve from 17.05065
196/196 - 70s - loss: 16.5060 - MinusLogProbMetric: 16.5060 - val_loss: 17.0952 - val_MinusLogProbMetric: 17.0952 - lr: 8.3333e-05 - 70s/epoch - 356ms/step
Epoch 436/1000
2023-09-28 00:50:16.923 
Epoch 436/1000 
	 loss: 16.5163, MinusLogProbMetric: 16.5163, val_loss: 17.1767, val_MinusLogProbMetric: 17.1767

Epoch 436: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5163 - MinusLogProbMetric: 16.5163 - val_loss: 17.1767 - val_MinusLogProbMetric: 17.1767 - lr: 8.3333e-05 - 69s/epoch - 353ms/step
Epoch 437/1000
2023-09-28 00:51:26.048 
Epoch 437/1000 
	 loss: 16.5260, MinusLogProbMetric: 16.5260, val_loss: 17.0655, val_MinusLogProbMetric: 17.0655

Epoch 437: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5260 - MinusLogProbMetric: 16.5260 - val_loss: 17.0655 - val_MinusLogProbMetric: 17.0655 - lr: 8.3333e-05 - 69s/epoch - 353ms/step
Epoch 438/1000
2023-09-28 00:52:35.137 
Epoch 438/1000 
	 loss: 16.5074, MinusLogProbMetric: 16.5074, val_loss: 17.0922, val_MinusLogProbMetric: 17.0922

Epoch 438: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5074 - MinusLogProbMetric: 16.5074 - val_loss: 17.0922 - val_MinusLogProbMetric: 17.0922 - lr: 8.3333e-05 - 69s/epoch - 352ms/step
Epoch 439/1000
2023-09-28 00:53:44.439 
Epoch 439/1000 
	 loss: 16.5044, MinusLogProbMetric: 16.5044, val_loss: 17.1957, val_MinusLogProbMetric: 17.1957

Epoch 439: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5044 - MinusLogProbMetric: 16.5044 - val_loss: 17.1957 - val_MinusLogProbMetric: 17.1957 - lr: 8.3333e-05 - 69s/epoch - 354ms/step
Epoch 440/1000
2023-09-28 00:54:53.324 
Epoch 440/1000 
	 loss: 16.4930, MinusLogProbMetric: 16.4930, val_loss: 17.1130, val_MinusLogProbMetric: 17.1130

Epoch 440: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.4930 - MinusLogProbMetric: 16.4930 - val_loss: 17.1130 - val_MinusLogProbMetric: 17.1130 - lr: 8.3333e-05 - 69s/epoch - 351ms/step
Epoch 441/1000
2023-09-28 00:56:02.774 
Epoch 441/1000 
	 loss: 16.5321, MinusLogProbMetric: 16.5321, val_loss: 17.1007, val_MinusLogProbMetric: 17.1007

Epoch 441: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.5321 - MinusLogProbMetric: 16.5321 - val_loss: 17.1007 - val_MinusLogProbMetric: 17.1007 - lr: 8.3333e-05 - 69s/epoch - 354ms/step
Epoch 442/1000
2023-09-28 00:57:12.210 
Epoch 442/1000 
	 loss: 16.4953, MinusLogProbMetric: 16.4953, val_loss: 17.1715, val_MinusLogProbMetric: 17.1715

Epoch 442: val_loss did not improve from 17.05065
196/196 - 69s - loss: 16.4953 - MinusLogProbMetric: 16.4953 - val_loss: 17.1715 - val_MinusLogProbMetric: 17.1715 - lr: 8.3333e-05 - 69s/epoch - 354ms/step
Epoch 443/1000
2023-09-28 00:58:21.208 
Epoch 443/1000 
	 loss: 16.4212, MinusLogProbMetric: 16.4212, val_loss: 17.0469, val_MinusLogProbMetric: 17.0469

Epoch 443: val_loss improved from 17.05065 to 17.04692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 70s - loss: 16.4212 - MinusLogProbMetric: 16.4212 - val_loss: 17.0469 - val_MinusLogProbMetric: 17.0469 - lr: 4.1667e-05 - 70s/epoch - 357ms/step
Epoch 444/1000
2023-09-28 00:59:30.969 
Epoch 444/1000 
	 loss: 16.4116, MinusLogProbMetric: 16.4116, val_loss: 17.0603, val_MinusLogProbMetric: 17.0603

Epoch 444: val_loss did not improve from 17.04692
196/196 - 69s - loss: 16.4116 - MinusLogProbMetric: 16.4116 - val_loss: 17.0603 - val_MinusLogProbMetric: 17.0603 - lr: 4.1667e-05 - 69s/epoch - 351ms/step
Epoch 445/1000
2023-09-28 01:00:40.108 
Epoch 445/1000 
	 loss: 16.4116, MinusLogProbMetric: 16.4116, val_loss: 17.0289, val_MinusLogProbMetric: 17.0289

Epoch 445: val_loss improved from 17.04692 to 17.02892, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 71s - loss: 16.4116 - MinusLogProbMetric: 16.4116 - val_loss: 17.0289 - val_MinusLogProbMetric: 17.0289 - lr: 4.1667e-05 - 71s/epoch - 361ms/step
Epoch 446/1000
2023-09-28 01:01:50.918 
Epoch 446/1000 
	 loss: 16.4160, MinusLogProbMetric: 16.4160, val_loss: 17.1131, val_MinusLogProbMetric: 17.1131

Epoch 446: val_loss did not improve from 17.02892
196/196 - 69s - loss: 16.4160 - MinusLogProbMetric: 16.4160 - val_loss: 17.1131 - val_MinusLogProbMetric: 17.1131 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 447/1000
2023-09-28 01:02:59.873 
Epoch 447/1000 
	 loss: 16.4147, MinusLogProbMetric: 16.4147, val_loss: 17.0470, val_MinusLogProbMetric: 17.0470

Epoch 447: val_loss did not improve from 17.02892
196/196 - 69s - loss: 16.4147 - MinusLogProbMetric: 16.4147 - val_loss: 17.0470 - val_MinusLogProbMetric: 17.0470 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 448/1000
2023-09-28 01:04:08.952 
Epoch 448/1000 
	 loss: 16.4195, MinusLogProbMetric: 16.4195, val_loss: 17.0394, val_MinusLogProbMetric: 17.0394

Epoch 448: val_loss did not improve from 17.02892
196/196 - 69s - loss: 16.4195 - MinusLogProbMetric: 16.4195 - val_loss: 17.0394 - val_MinusLogProbMetric: 17.0394 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 449/1000
2023-09-28 01:05:18.342 
Epoch 449/1000 
	 loss: 16.4107, MinusLogProbMetric: 16.4107, val_loss: 17.0379, val_MinusLogProbMetric: 17.0379

Epoch 449: val_loss did not improve from 17.02892
196/196 - 69s - loss: 16.4107 - MinusLogProbMetric: 16.4107 - val_loss: 17.0379 - val_MinusLogProbMetric: 17.0379 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 450/1000
2023-09-28 01:06:27.874 
Epoch 450/1000 
	 loss: 16.4058, MinusLogProbMetric: 16.4058, val_loss: 17.0180, val_MinusLogProbMetric: 17.0180

Epoch 450: val_loss improved from 17.02892 to 17.01802, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 70s - loss: 16.4058 - MinusLogProbMetric: 16.4058 - val_loss: 17.0180 - val_MinusLogProbMetric: 17.0180 - lr: 4.1667e-05 - 70s/epoch - 360ms/step
Epoch 451/1000
2023-09-28 01:07:38.235 
Epoch 451/1000 
	 loss: 16.4151, MinusLogProbMetric: 16.4151, val_loss: 17.0212, val_MinusLogProbMetric: 17.0212

Epoch 451: val_loss did not improve from 17.01802
196/196 - 69s - loss: 16.4151 - MinusLogProbMetric: 16.4151 - val_loss: 17.0212 - val_MinusLogProbMetric: 17.0212 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 452/1000
2023-09-28 01:08:47.575 
Epoch 452/1000 
	 loss: 16.4045, MinusLogProbMetric: 16.4045, val_loss: 17.0244, val_MinusLogProbMetric: 17.0244

Epoch 452: val_loss did not improve from 17.01802
196/196 - 69s - loss: 16.4045 - MinusLogProbMetric: 16.4045 - val_loss: 17.0244 - val_MinusLogProbMetric: 17.0244 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 453/1000
2023-09-28 01:09:56.648 
Epoch 453/1000 
	 loss: 16.3987, MinusLogProbMetric: 16.3987, val_loss: 17.0383, val_MinusLogProbMetric: 17.0383

Epoch 453: val_loss did not improve from 17.01802
196/196 - 69s - loss: 16.3987 - MinusLogProbMetric: 16.3987 - val_loss: 17.0383 - val_MinusLogProbMetric: 17.0383 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 454/1000
2023-09-28 01:11:06.713 
Epoch 454/1000 
	 loss: 16.4234, MinusLogProbMetric: 16.4234, val_loss: 17.0236, val_MinusLogProbMetric: 17.0236

Epoch 454: val_loss did not improve from 17.01802
196/196 - 70s - loss: 16.4234 - MinusLogProbMetric: 16.4234 - val_loss: 17.0236 - val_MinusLogProbMetric: 17.0236 - lr: 4.1667e-05 - 70s/epoch - 357ms/step
Epoch 455/1000
2023-09-28 01:12:15.942 
Epoch 455/1000 
	 loss: 16.4123, MinusLogProbMetric: 16.4123, val_loss: 17.0348, val_MinusLogProbMetric: 17.0348

Epoch 455: val_loss did not improve from 17.01802
196/196 - 69s - loss: 16.4123 - MinusLogProbMetric: 16.4123 - val_loss: 17.0348 - val_MinusLogProbMetric: 17.0348 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 456/1000
2023-09-28 01:13:25.901 
Epoch 456/1000 
	 loss: 16.4088, MinusLogProbMetric: 16.4088, val_loss: 17.0165, val_MinusLogProbMetric: 17.0165

Epoch 456: val_loss improved from 17.01802 to 17.01651, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 71s - loss: 16.4088 - MinusLogProbMetric: 16.4088 - val_loss: 17.0165 - val_MinusLogProbMetric: 17.0165 - lr: 4.1667e-05 - 71s/epoch - 363ms/step
Epoch 457/1000
2023-09-28 01:14:37.227 
Epoch 457/1000 
	 loss: 16.4268, MinusLogProbMetric: 16.4268, val_loss: 17.0938, val_MinusLogProbMetric: 17.0938

Epoch 457: val_loss did not improve from 17.01651
196/196 - 70s - loss: 16.4268 - MinusLogProbMetric: 16.4268 - val_loss: 17.0938 - val_MinusLogProbMetric: 17.0938 - lr: 4.1667e-05 - 70s/epoch - 358ms/step
Epoch 458/1000
2023-09-28 01:15:46.336 
Epoch 458/1000 
	 loss: 16.3976, MinusLogProbMetric: 16.3976, val_loss: 17.0857, val_MinusLogProbMetric: 17.0857

Epoch 458: val_loss did not improve from 17.01651
196/196 - 69s - loss: 16.3976 - MinusLogProbMetric: 16.3976 - val_loss: 17.0857 - val_MinusLogProbMetric: 17.0857 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 459/1000
2023-09-28 01:16:55.577 
Epoch 459/1000 
	 loss: 16.4080, MinusLogProbMetric: 16.4080, val_loss: 17.0147, val_MinusLogProbMetric: 17.0147

Epoch 459: val_loss improved from 17.01651 to 17.01466, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 70s - loss: 16.4080 - MinusLogProbMetric: 16.4080 - val_loss: 17.0147 - val_MinusLogProbMetric: 17.0147 - lr: 4.1667e-05 - 70s/epoch - 358ms/step
Epoch 460/1000
2023-09-28 01:18:06.113 
Epoch 460/1000 
	 loss: 16.4102, MinusLogProbMetric: 16.4102, val_loss: 17.0307, val_MinusLogProbMetric: 17.0307

Epoch 460: val_loss did not improve from 17.01466
196/196 - 70s - loss: 16.4102 - MinusLogProbMetric: 16.4102 - val_loss: 17.0307 - val_MinusLogProbMetric: 17.0307 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 461/1000
2023-09-28 01:19:15.526 
Epoch 461/1000 
	 loss: 16.4201, MinusLogProbMetric: 16.4201, val_loss: 17.0381, val_MinusLogProbMetric: 17.0381

Epoch 461: val_loss did not improve from 17.01466
196/196 - 69s - loss: 16.4201 - MinusLogProbMetric: 16.4201 - val_loss: 17.0381 - val_MinusLogProbMetric: 17.0381 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 462/1000
2023-09-28 01:20:25.506 
Epoch 462/1000 
	 loss: 16.3973, MinusLogProbMetric: 16.3973, val_loss: 17.0369, val_MinusLogProbMetric: 17.0369

Epoch 462: val_loss did not improve from 17.01466
196/196 - 70s - loss: 16.3973 - MinusLogProbMetric: 16.3973 - val_loss: 17.0369 - val_MinusLogProbMetric: 17.0369 - lr: 4.1667e-05 - 70s/epoch - 357ms/step
Epoch 463/1000
2023-09-28 01:21:34.378 
Epoch 463/1000 
	 loss: 16.4251, MinusLogProbMetric: 16.4251, val_loss: 17.3275, val_MinusLogProbMetric: 17.3275

Epoch 463: val_loss did not improve from 17.01466
196/196 - 69s - loss: 16.4251 - MinusLogProbMetric: 16.4251 - val_loss: 17.3275 - val_MinusLogProbMetric: 17.3275 - lr: 4.1667e-05 - 69s/epoch - 351ms/step
Epoch 464/1000
2023-09-28 01:22:43.641 
Epoch 464/1000 
	 loss: 16.4516, MinusLogProbMetric: 16.4516, val_loss: 17.0985, val_MinusLogProbMetric: 17.0985

Epoch 464: val_loss did not improve from 17.01466
196/196 - 69s - loss: 16.4516 - MinusLogProbMetric: 16.4516 - val_loss: 17.0985 - val_MinusLogProbMetric: 17.0985 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 465/1000
2023-09-28 01:23:52.924 
Epoch 465/1000 
	 loss: 16.4010, MinusLogProbMetric: 16.4010, val_loss: 17.0379, val_MinusLogProbMetric: 17.0379

Epoch 465: val_loss did not improve from 17.01466
196/196 - 69s - loss: 16.4010 - MinusLogProbMetric: 16.4010 - val_loss: 17.0379 - val_MinusLogProbMetric: 17.0379 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 466/1000
2023-09-28 01:25:02.275 
Epoch 466/1000 
	 loss: 16.4248, MinusLogProbMetric: 16.4248, val_loss: 17.0149, val_MinusLogProbMetric: 17.0149

Epoch 466: val_loss did not improve from 17.01466
196/196 - 69s - loss: 16.4248 - MinusLogProbMetric: 16.4248 - val_loss: 17.0149 - val_MinusLogProbMetric: 17.0149 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 467/1000
2023-09-28 01:26:11.904 
Epoch 467/1000 
	 loss: 16.4090, MinusLogProbMetric: 16.4090, val_loss: 17.0651, val_MinusLogProbMetric: 17.0651

Epoch 467: val_loss did not improve from 17.01466
196/196 - 70s - loss: 16.4090 - MinusLogProbMetric: 16.4090 - val_loss: 17.0651 - val_MinusLogProbMetric: 17.0651 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 468/1000
2023-09-28 01:27:21.927 
Epoch 468/1000 
	 loss: 16.4015, MinusLogProbMetric: 16.4015, val_loss: 17.1444, val_MinusLogProbMetric: 17.1444

Epoch 468: val_loss did not improve from 17.01466
196/196 - 70s - loss: 16.4015 - MinusLogProbMetric: 16.4015 - val_loss: 17.1444 - val_MinusLogProbMetric: 17.1444 - lr: 4.1667e-05 - 70s/epoch - 357ms/step
Epoch 469/1000
2023-09-28 01:28:31.893 
Epoch 469/1000 
	 loss: 16.4052, MinusLogProbMetric: 16.4052, val_loss: 17.0363, val_MinusLogProbMetric: 17.0363

Epoch 469: val_loss did not improve from 17.01466
196/196 - 70s - loss: 16.4052 - MinusLogProbMetric: 16.4052 - val_loss: 17.0363 - val_MinusLogProbMetric: 17.0363 - lr: 4.1667e-05 - 70s/epoch - 357ms/step
Epoch 470/1000
2023-09-28 01:29:41.328 
Epoch 470/1000 
	 loss: 16.4024, MinusLogProbMetric: 16.4024, val_loss: 17.0452, val_MinusLogProbMetric: 17.0452

Epoch 470: val_loss did not improve from 17.01466
196/196 - 69s - loss: 16.4024 - MinusLogProbMetric: 16.4024 - val_loss: 17.0452 - val_MinusLogProbMetric: 17.0452 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 471/1000
2023-09-28 01:30:50.475 
Epoch 471/1000 
	 loss: 16.4034, MinusLogProbMetric: 16.4034, val_loss: 17.0437, val_MinusLogProbMetric: 17.0437

Epoch 471: val_loss did not improve from 17.01466
196/196 - 69s - loss: 16.4034 - MinusLogProbMetric: 16.4034 - val_loss: 17.0437 - val_MinusLogProbMetric: 17.0437 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 472/1000
2023-09-28 01:31:59.851 
Epoch 472/1000 
	 loss: 16.4112, MinusLogProbMetric: 16.4112, val_loss: 17.0354, val_MinusLogProbMetric: 17.0354

Epoch 472: val_loss did not improve from 17.01466
196/196 - 69s - loss: 16.4112 - MinusLogProbMetric: 16.4112 - val_loss: 17.0354 - val_MinusLogProbMetric: 17.0354 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 473/1000
2023-09-28 01:33:09.381 
Epoch 473/1000 
	 loss: 16.4305, MinusLogProbMetric: 16.4305, val_loss: 17.0375, val_MinusLogProbMetric: 17.0375

Epoch 473: val_loss did not improve from 17.01466
196/196 - 70s - loss: 16.4305 - MinusLogProbMetric: 16.4305 - val_loss: 17.0375 - val_MinusLogProbMetric: 17.0375 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 474/1000
2023-09-28 01:34:19.396 
Epoch 474/1000 
	 loss: 16.3987, MinusLogProbMetric: 16.3987, val_loss: 17.0260, val_MinusLogProbMetric: 17.0260

Epoch 474: val_loss did not improve from 17.01466
196/196 - 70s - loss: 16.3987 - MinusLogProbMetric: 16.3987 - val_loss: 17.0260 - val_MinusLogProbMetric: 17.0260 - lr: 4.1667e-05 - 70s/epoch - 357ms/step
Epoch 475/1000
2023-09-28 01:35:28.773 
Epoch 475/1000 
	 loss: 16.3944, MinusLogProbMetric: 16.3944, val_loss: 17.0275, val_MinusLogProbMetric: 17.0275

Epoch 475: val_loss did not improve from 17.01466
196/196 - 69s - loss: 16.3944 - MinusLogProbMetric: 16.3944 - val_loss: 17.0275 - val_MinusLogProbMetric: 17.0275 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 476/1000
2023-09-28 01:36:37.929 
Epoch 476/1000 
	 loss: 16.3969, MinusLogProbMetric: 16.3969, val_loss: 17.0109, val_MinusLogProbMetric: 17.0109

Epoch 476: val_loss improved from 17.01466 to 17.01089, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 70s - loss: 16.3969 - MinusLogProbMetric: 16.3969 - val_loss: 17.0109 - val_MinusLogProbMetric: 17.0109 - lr: 4.1667e-05 - 70s/epoch - 358ms/step
Epoch 477/1000
2023-09-28 01:37:48.644 
Epoch 477/1000 
	 loss: 16.4332, MinusLogProbMetric: 16.4332, val_loss: 17.1534, val_MinusLogProbMetric: 17.1534

Epoch 477: val_loss did not improve from 17.01089
196/196 - 70s - loss: 16.4332 - MinusLogProbMetric: 16.4332 - val_loss: 17.1534 - val_MinusLogProbMetric: 17.1534 - lr: 4.1667e-05 - 70s/epoch - 356ms/step
Epoch 478/1000
2023-09-28 01:38:57.747 
Epoch 478/1000 
	 loss: 16.4166, MinusLogProbMetric: 16.4166, val_loss: 17.0354, val_MinusLogProbMetric: 17.0354

Epoch 478: val_loss did not improve from 17.01089
196/196 - 69s - loss: 16.4166 - MinusLogProbMetric: 16.4166 - val_loss: 17.0354 - val_MinusLogProbMetric: 17.0354 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 479/1000
2023-09-28 01:40:04.759 
Epoch 479/1000 
	 loss: 16.4083, MinusLogProbMetric: 16.4083, val_loss: 17.2152, val_MinusLogProbMetric: 17.2152

Epoch 479: val_loss did not improve from 17.01089
196/196 - 67s - loss: 16.4083 - MinusLogProbMetric: 16.4083 - val_loss: 17.2152 - val_MinusLogProbMetric: 17.2152 - lr: 4.1667e-05 - 67s/epoch - 342ms/step
Epoch 480/1000
2023-09-28 01:41:02.707 
Epoch 480/1000 
	 loss: 16.4051, MinusLogProbMetric: 16.4051, val_loss: 17.0372, val_MinusLogProbMetric: 17.0372

Epoch 480: val_loss did not improve from 17.01089
196/196 - 58s - loss: 16.4051 - MinusLogProbMetric: 16.4051 - val_loss: 17.0372 - val_MinusLogProbMetric: 17.0372 - lr: 4.1667e-05 - 58s/epoch - 296ms/step
Epoch 481/1000
2023-09-28 01:42:01.701 
Epoch 481/1000 
	 loss: 16.4086, MinusLogProbMetric: 16.4086, val_loss: 17.0298, val_MinusLogProbMetric: 17.0298

Epoch 481: val_loss did not improve from 17.01089
196/196 - 59s - loss: 16.4086 - MinusLogProbMetric: 16.4086 - val_loss: 17.0298 - val_MinusLogProbMetric: 17.0298 - lr: 4.1667e-05 - 59s/epoch - 301ms/step
Epoch 482/1000
2023-09-28 01:43:04.662 
Epoch 482/1000 
	 loss: 16.4070, MinusLogProbMetric: 16.4070, val_loss: 17.0297, val_MinusLogProbMetric: 17.0297

Epoch 482: val_loss did not improve from 17.01089
196/196 - 63s - loss: 16.4070 - MinusLogProbMetric: 16.4070 - val_loss: 17.0297 - val_MinusLogProbMetric: 17.0297 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 483/1000
2023-09-28 01:44:01.323 
Epoch 483/1000 
	 loss: 16.4131, MinusLogProbMetric: 16.4131, val_loss: 17.0074, val_MinusLogProbMetric: 17.0074

Epoch 483: val_loss improved from 17.01089 to 17.00737, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 57s - loss: 16.4131 - MinusLogProbMetric: 16.4131 - val_loss: 17.0074 - val_MinusLogProbMetric: 17.0074 - lr: 4.1667e-05 - 57s/epoch - 293ms/step
Epoch 484/1000
2023-09-28 01:45:09.515 
Epoch 484/1000 
	 loss: 16.3954, MinusLogProbMetric: 16.3954, val_loss: 17.0297, val_MinusLogProbMetric: 17.0297

Epoch 484: val_loss did not improve from 17.00737
196/196 - 67s - loss: 16.3954 - MinusLogProbMetric: 16.3954 - val_loss: 17.0297 - val_MinusLogProbMetric: 17.0297 - lr: 4.1667e-05 - 67s/epoch - 344ms/step
Epoch 485/1000
2023-09-28 01:46:18.550 
Epoch 485/1000 
	 loss: 16.4009, MinusLogProbMetric: 16.4009, val_loss: 17.0277, val_MinusLogProbMetric: 17.0277

Epoch 485: val_loss did not improve from 17.00737
196/196 - 69s - loss: 16.4009 - MinusLogProbMetric: 16.4009 - val_loss: 17.0277 - val_MinusLogProbMetric: 17.0277 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 486/1000
2023-09-28 01:47:28.452 
Epoch 486/1000 
	 loss: 16.3977, MinusLogProbMetric: 16.3977, val_loss: 17.0117, val_MinusLogProbMetric: 17.0117

Epoch 486: val_loss did not improve from 17.00737
196/196 - 70s - loss: 16.3977 - MinusLogProbMetric: 16.3977 - val_loss: 17.0117 - val_MinusLogProbMetric: 17.0117 - lr: 4.1667e-05 - 70s/epoch - 357ms/step
Epoch 487/1000
2023-09-28 01:48:38.260 
Epoch 487/1000 
	 loss: 16.3941, MinusLogProbMetric: 16.3941, val_loss: 17.1538, val_MinusLogProbMetric: 17.1538

Epoch 487: val_loss did not improve from 17.00737
196/196 - 70s - loss: 16.3941 - MinusLogProbMetric: 16.3941 - val_loss: 17.1538 - val_MinusLogProbMetric: 17.1538 - lr: 4.1667e-05 - 70s/epoch - 356ms/step
Epoch 488/1000
2023-09-28 01:49:47.643 
Epoch 488/1000 
	 loss: 16.4065, MinusLogProbMetric: 16.4065, val_loss: 17.0568, val_MinusLogProbMetric: 17.0568

Epoch 488: val_loss did not improve from 17.00737
196/196 - 69s - loss: 16.4065 - MinusLogProbMetric: 16.4065 - val_loss: 17.0568 - val_MinusLogProbMetric: 17.0568 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 489/1000
2023-09-28 01:50:56.856 
Epoch 489/1000 
	 loss: 16.4116, MinusLogProbMetric: 16.4116, val_loss: 17.0082, val_MinusLogProbMetric: 17.0082

Epoch 489: val_loss did not improve from 17.00737
196/196 - 69s - loss: 16.4116 - MinusLogProbMetric: 16.4116 - val_loss: 17.0082 - val_MinusLogProbMetric: 17.0082 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 490/1000
2023-09-28 01:52:06.044 
Epoch 490/1000 
	 loss: 16.4000, MinusLogProbMetric: 16.4000, val_loss: 17.0349, val_MinusLogProbMetric: 17.0349

Epoch 490: val_loss did not improve from 17.00737
196/196 - 69s - loss: 16.4000 - MinusLogProbMetric: 16.4000 - val_loss: 17.0349 - val_MinusLogProbMetric: 17.0349 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 491/1000
2023-09-28 01:53:14.980 
Epoch 491/1000 
	 loss: 16.4170, MinusLogProbMetric: 16.4170, val_loss: 17.0600, val_MinusLogProbMetric: 17.0600

Epoch 491: val_loss did not improve from 17.00737
196/196 - 69s - loss: 16.4170 - MinusLogProbMetric: 16.4170 - val_loss: 17.0600 - val_MinusLogProbMetric: 17.0600 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 492/1000
2023-09-28 01:54:23.893 
Epoch 492/1000 
	 loss: 16.4052, MinusLogProbMetric: 16.4052, val_loss: 17.0356, val_MinusLogProbMetric: 17.0356

Epoch 492: val_loss did not improve from 17.00737
196/196 - 69s - loss: 16.4052 - MinusLogProbMetric: 16.4052 - val_loss: 17.0356 - val_MinusLogProbMetric: 17.0356 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 493/1000
2023-09-28 01:55:33.815 
Epoch 493/1000 
	 loss: 16.4048, MinusLogProbMetric: 16.4048, val_loss: 17.0462, val_MinusLogProbMetric: 17.0462

Epoch 493: val_loss did not improve from 17.00737
196/196 - 70s - loss: 16.4048 - MinusLogProbMetric: 16.4048 - val_loss: 17.0462 - val_MinusLogProbMetric: 17.0462 - lr: 4.1667e-05 - 70s/epoch - 357ms/step
Epoch 494/1000
2023-09-28 01:56:42.956 
Epoch 494/1000 
	 loss: 16.3970, MinusLogProbMetric: 16.3970, val_loss: 17.1338, val_MinusLogProbMetric: 17.1338

Epoch 494: val_loss did not improve from 17.00737
196/196 - 69s - loss: 16.3970 - MinusLogProbMetric: 16.3970 - val_loss: 17.1338 - val_MinusLogProbMetric: 17.1338 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 495/1000
2023-09-28 01:57:52.206 
Epoch 495/1000 
	 loss: 16.3905, MinusLogProbMetric: 16.3905, val_loss: 17.0454, val_MinusLogProbMetric: 17.0454

Epoch 495: val_loss did not improve from 17.00737
196/196 - 69s - loss: 16.3905 - MinusLogProbMetric: 16.3905 - val_loss: 17.0454 - val_MinusLogProbMetric: 17.0454 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 496/1000
2023-09-28 01:59:01.229 
Epoch 496/1000 
	 loss: 16.4094, MinusLogProbMetric: 16.4094, val_loss: 17.0350, val_MinusLogProbMetric: 17.0350

Epoch 496: val_loss did not improve from 17.00737
196/196 - 69s - loss: 16.4094 - MinusLogProbMetric: 16.4094 - val_loss: 17.0350 - val_MinusLogProbMetric: 17.0350 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 497/1000
2023-09-28 02:00:10.836 
Epoch 497/1000 
	 loss: 16.4094, MinusLogProbMetric: 16.4094, val_loss: 17.0515, val_MinusLogProbMetric: 17.0515

Epoch 497: val_loss did not improve from 17.00737
196/196 - 70s - loss: 16.4094 - MinusLogProbMetric: 16.4094 - val_loss: 17.0515 - val_MinusLogProbMetric: 17.0515 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 498/1000
2023-09-28 02:01:20.404 
Epoch 498/1000 
	 loss: 16.3969, MinusLogProbMetric: 16.3969, val_loss: 17.0105, val_MinusLogProbMetric: 17.0105

Epoch 498: val_loss did not improve from 17.00737
196/196 - 70s - loss: 16.3969 - MinusLogProbMetric: 16.3969 - val_loss: 17.0105 - val_MinusLogProbMetric: 17.0105 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 499/1000
2023-09-28 02:02:29.262 
Epoch 499/1000 
	 loss: 16.4003, MinusLogProbMetric: 16.4003, val_loss: 17.0454, val_MinusLogProbMetric: 17.0454

Epoch 499: val_loss did not improve from 17.00737
196/196 - 69s - loss: 16.4003 - MinusLogProbMetric: 16.4003 - val_loss: 17.0454 - val_MinusLogProbMetric: 17.0454 - lr: 4.1667e-05 - 69s/epoch - 351ms/step
Epoch 500/1000
2023-09-28 02:03:38.232 
Epoch 500/1000 
	 loss: 16.4023, MinusLogProbMetric: 16.4023, val_loss: 17.0606, val_MinusLogProbMetric: 17.0606

Epoch 500: val_loss did not improve from 17.00737
196/196 - 69s - loss: 16.4023 - MinusLogProbMetric: 16.4023 - val_loss: 17.0606 - val_MinusLogProbMetric: 17.0606 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 501/1000
2023-09-28 02:04:47.343 
Epoch 501/1000 
	 loss: 16.4175, MinusLogProbMetric: 16.4175, val_loss: 17.0369, val_MinusLogProbMetric: 17.0369

Epoch 501: val_loss did not improve from 17.00737
196/196 - 69s - loss: 16.4175 - MinusLogProbMetric: 16.4175 - val_loss: 17.0369 - val_MinusLogProbMetric: 17.0369 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 502/1000
2023-09-28 02:05:57.375 
Epoch 502/1000 
	 loss: 16.3866, MinusLogProbMetric: 16.3866, val_loss: 17.0272, val_MinusLogProbMetric: 17.0272

Epoch 502: val_loss did not improve from 17.00737
196/196 - 70s - loss: 16.3866 - MinusLogProbMetric: 16.3866 - val_loss: 17.0272 - val_MinusLogProbMetric: 17.0272 - lr: 4.1667e-05 - 70s/epoch - 357ms/step
Epoch 503/1000
2023-09-28 02:07:06.920 
Epoch 503/1000 
	 loss: 16.4122, MinusLogProbMetric: 16.4122, val_loss: 17.0990, val_MinusLogProbMetric: 17.0990

Epoch 503: val_loss did not improve from 17.00737
196/196 - 70s - loss: 16.4122 - MinusLogProbMetric: 16.4122 - val_loss: 17.0990 - val_MinusLogProbMetric: 17.0990 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 504/1000
2023-09-28 02:08:16.433 
Epoch 504/1000 
	 loss: 16.4046, MinusLogProbMetric: 16.4046, val_loss: 17.0268, val_MinusLogProbMetric: 17.0268

Epoch 504: val_loss did not improve from 17.00737
196/196 - 70s - loss: 16.4046 - MinusLogProbMetric: 16.4046 - val_loss: 17.0268 - val_MinusLogProbMetric: 17.0268 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 505/1000
2023-09-28 02:09:25.428 
Epoch 505/1000 
	 loss: 16.4063, MinusLogProbMetric: 16.4063, val_loss: 17.0362, val_MinusLogProbMetric: 17.0362

Epoch 505: val_loss did not improve from 17.00737
196/196 - 69s - loss: 16.4063 - MinusLogProbMetric: 16.4063 - val_loss: 17.0362 - val_MinusLogProbMetric: 17.0362 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 506/1000
2023-09-28 02:10:34.426 
Epoch 506/1000 
	 loss: 16.3886, MinusLogProbMetric: 16.3886, val_loss: 17.0044, val_MinusLogProbMetric: 17.0044

Epoch 506: val_loss improved from 17.00737 to 17.00438, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 70s - loss: 16.3886 - MinusLogProbMetric: 16.3886 - val_loss: 17.0044 - val_MinusLogProbMetric: 17.0044 - lr: 4.1667e-05 - 70s/epoch - 357ms/step
Epoch 507/1000
2023-09-28 02:11:44.557 
Epoch 507/1000 
	 loss: 16.4230, MinusLogProbMetric: 16.4230, val_loss: 17.0274, val_MinusLogProbMetric: 17.0274

Epoch 507: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.4230 - MinusLogProbMetric: 16.4230 - val_loss: 17.0274 - val_MinusLogProbMetric: 17.0274 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 508/1000
2023-09-28 02:12:53.404 
Epoch 508/1000 
	 loss: 16.3919, MinusLogProbMetric: 16.3919, val_loss: 17.0337, val_MinusLogProbMetric: 17.0337

Epoch 508: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.3919 - MinusLogProbMetric: 16.3919 - val_loss: 17.0337 - val_MinusLogProbMetric: 17.0337 - lr: 4.1667e-05 - 69s/epoch - 351ms/step
Epoch 509/1000
2023-09-28 02:14:02.775 
Epoch 509/1000 
	 loss: 16.4155, MinusLogProbMetric: 16.4155, val_loss: 17.0247, val_MinusLogProbMetric: 17.0247

Epoch 509: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.4155 - MinusLogProbMetric: 16.4155 - val_loss: 17.0247 - val_MinusLogProbMetric: 17.0247 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 510/1000
2023-09-28 02:15:12.463 
Epoch 510/1000 
	 loss: 16.4051, MinusLogProbMetric: 16.4051, val_loss: 17.0683, val_MinusLogProbMetric: 17.0683

Epoch 510: val_loss did not improve from 17.00438
196/196 - 70s - loss: 16.4051 - MinusLogProbMetric: 16.4051 - val_loss: 17.0683 - val_MinusLogProbMetric: 17.0683 - lr: 4.1667e-05 - 70s/epoch - 356ms/step
Epoch 511/1000
2023-09-28 02:16:22.043 
Epoch 511/1000 
	 loss: 16.3978, MinusLogProbMetric: 16.3978, val_loss: 17.0341, val_MinusLogProbMetric: 17.0341

Epoch 511: val_loss did not improve from 17.00438
196/196 - 70s - loss: 16.3978 - MinusLogProbMetric: 16.3978 - val_loss: 17.0341 - val_MinusLogProbMetric: 17.0341 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 512/1000
2023-09-28 02:17:31.579 
Epoch 512/1000 
	 loss: 16.4068, MinusLogProbMetric: 16.4068, val_loss: 17.0160, val_MinusLogProbMetric: 17.0160

Epoch 512: val_loss did not improve from 17.00438
196/196 - 70s - loss: 16.4068 - MinusLogProbMetric: 16.4068 - val_loss: 17.0160 - val_MinusLogProbMetric: 17.0160 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 513/1000
2023-09-28 02:18:41.729 
Epoch 513/1000 
	 loss: 16.3775, MinusLogProbMetric: 16.3775, val_loss: 17.0367, val_MinusLogProbMetric: 17.0367

Epoch 513: val_loss did not improve from 17.00438
196/196 - 70s - loss: 16.3775 - MinusLogProbMetric: 16.3775 - val_loss: 17.0367 - val_MinusLogProbMetric: 17.0367 - lr: 4.1667e-05 - 70s/epoch - 358ms/step
Epoch 514/1000
2023-09-28 02:19:51.155 
Epoch 514/1000 
	 loss: 16.4060, MinusLogProbMetric: 16.4060, val_loss: 17.0467, val_MinusLogProbMetric: 17.0467

Epoch 514: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.4060 - MinusLogProbMetric: 16.4060 - val_loss: 17.0467 - val_MinusLogProbMetric: 17.0467 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 515/1000
2023-09-28 02:21:00.821 
Epoch 515/1000 
	 loss: 16.4059, MinusLogProbMetric: 16.4059, val_loss: 17.0164, val_MinusLogProbMetric: 17.0164

Epoch 515: val_loss did not improve from 17.00438
196/196 - 70s - loss: 16.4059 - MinusLogProbMetric: 16.4059 - val_loss: 17.0164 - val_MinusLogProbMetric: 17.0164 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 516/1000
2023-09-28 02:22:10.226 
Epoch 516/1000 
	 loss: 16.4030, MinusLogProbMetric: 16.4030, val_loss: 17.2360, val_MinusLogProbMetric: 17.2360

Epoch 516: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.4030 - MinusLogProbMetric: 16.4030 - val_loss: 17.2360 - val_MinusLogProbMetric: 17.2360 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 517/1000
2023-09-28 02:23:19.225 
Epoch 517/1000 
	 loss: 16.3879, MinusLogProbMetric: 16.3879, val_loss: 17.0622, val_MinusLogProbMetric: 17.0622

Epoch 517: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.3879 - MinusLogProbMetric: 16.3879 - val_loss: 17.0622 - val_MinusLogProbMetric: 17.0622 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 518/1000
2023-09-28 02:24:28.519 
Epoch 518/1000 
	 loss: 16.3794, MinusLogProbMetric: 16.3794, val_loss: 17.0107, val_MinusLogProbMetric: 17.0107

Epoch 518: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.3794 - MinusLogProbMetric: 16.3794 - val_loss: 17.0107 - val_MinusLogProbMetric: 17.0107 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 519/1000
2023-09-28 02:25:37.620 
Epoch 519/1000 
	 loss: 16.4135, MinusLogProbMetric: 16.4135, val_loss: 17.0125, val_MinusLogProbMetric: 17.0125

Epoch 519: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.4135 - MinusLogProbMetric: 16.4135 - val_loss: 17.0125 - val_MinusLogProbMetric: 17.0125 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 520/1000
2023-09-28 02:26:46.042 
Epoch 520/1000 
	 loss: 16.3998, MinusLogProbMetric: 16.3998, val_loss: 17.0260, val_MinusLogProbMetric: 17.0260

Epoch 520: val_loss did not improve from 17.00438
196/196 - 68s - loss: 16.3998 - MinusLogProbMetric: 16.3998 - val_loss: 17.0260 - val_MinusLogProbMetric: 17.0260 - lr: 4.1667e-05 - 68s/epoch - 349ms/step
Epoch 521/1000
2023-09-28 02:27:55.165 
Epoch 521/1000 
	 loss: 16.3962, MinusLogProbMetric: 16.3962, val_loss: 17.0934, val_MinusLogProbMetric: 17.0934

Epoch 521: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.3962 - MinusLogProbMetric: 16.3962 - val_loss: 17.0934 - val_MinusLogProbMetric: 17.0934 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 522/1000
2023-09-28 02:29:04.567 
Epoch 522/1000 
	 loss: 16.3816, MinusLogProbMetric: 16.3816, val_loss: 17.0624, val_MinusLogProbMetric: 17.0624

Epoch 522: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.3816 - MinusLogProbMetric: 16.3816 - val_loss: 17.0624 - val_MinusLogProbMetric: 17.0624 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 523/1000
2023-09-28 02:30:14.087 
Epoch 523/1000 
	 loss: 16.3957, MinusLogProbMetric: 16.3957, val_loss: 17.0598, val_MinusLogProbMetric: 17.0598

Epoch 523: val_loss did not improve from 17.00438
196/196 - 70s - loss: 16.3957 - MinusLogProbMetric: 16.3957 - val_loss: 17.0598 - val_MinusLogProbMetric: 17.0598 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 524/1000
2023-09-28 02:31:22.989 
Epoch 524/1000 
	 loss: 16.3964, MinusLogProbMetric: 16.3964, val_loss: 17.0370, val_MinusLogProbMetric: 17.0370

Epoch 524: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.3964 - MinusLogProbMetric: 16.3964 - val_loss: 17.0370 - val_MinusLogProbMetric: 17.0370 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 525/1000
2023-09-28 02:32:32.063 
Epoch 525/1000 
	 loss: 16.4017, MinusLogProbMetric: 16.4017, val_loss: 17.0173, val_MinusLogProbMetric: 17.0173

Epoch 525: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.4017 - MinusLogProbMetric: 16.4017 - val_loss: 17.0173 - val_MinusLogProbMetric: 17.0173 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 526/1000
2023-09-28 02:33:40.742 
Epoch 526/1000 
	 loss: 16.3999, MinusLogProbMetric: 16.3999, val_loss: 17.0795, val_MinusLogProbMetric: 17.0795

Epoch 526: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.3999 - MinusLogProbMetric: 16.3999 - val_loss: 17.0795 - val_MinusLogProbMetric: 17.0795 - lr: 4.1667e-05 - 69s/epoch - 350ms/step
Epoch 527/1000
2023-09-28 02:34:49.532 
Epoch 527/1000 
	 loss: 16.3985, MinusLogProbMetric: 16.3985, val_loss: 17.1331, val_MinusLogProbMetric: 17.1331

Epoch 527: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.3985 - MinusLogProbMetric: 16.3985 - val_loss: 17.1331 - val_MinusLogProbMetric: 17.1331 - lr: 4.1667e-05 - 69s/epoch - 351ms/step
Epoch 528/1000
2023-09-28 02:35:58.463 
Epoch 528/1000 
	 loss: 16.3981, MinusLogProbMetric: 16.3981, val_loss: 17.0362, val_MinusLogProbMetric: 17.0362

Epoch 528: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.3981 - MinusLogProbMetric: 16.3981 - val_loss: 17.0362 - val_MinusLogProbMetric: 17.0362 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 529/1000
2023-09-28 02:37:07.019 
Epoch 529/1000 
	 loss: 16.4230, MinusLogProbMetric: 16.4230, val_loss: 17.0634, val_MinusLogProbMetric: 17.0634

Epoch 529: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.4230 - MinusLogProbMetric: 16.4230 - val_loss: 17.0634 - val_MinusLogProbMetric: 17.0634 - lr: 4.1667e-05 - 69s/epoch - 350ms/step
Epoch 530/1000
2023-09-28 02:38:16.611 
Epoch 530/1000 
	 loss: 16.3846, MinusLogProbMetric: 16.3846, val_loss: 17.0804, val_MinusLogProbMetric: 17.0804

Epoch 530: val_loss did not improve from 17.00438
196/196 - 70s - loss: 16.3846 - MinusLogProbMetric: 16.3846 - val_loss: 17.0804 - val_MinusLogProbMetric: 17.0804 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 531/1000
2023-09-28 02:39:25.495 
Epoch 531/1000 
	 loss: 16.3913, MinusLogProbMetric: 16.3913, val_loss: 17.0243, val_MinusLogProbMetric: 17.0243

Epoch 531: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.3913 - MinusLogProbMetric: 16.3913 - val_loss: 17.0243 - val_MinusLogProbMetric: 17.0243 - lr: 4.1667e-05 - 69s/epoch - 351ms/step
Epoch 532/1000
2023-09-28 02:40:35.418 
Epoch 532/1000 
	 loss: 16.4072, MinusLogProbMetric: 16.4072, val_loss: 17.1188, val_MinusLogProbMetric: 17.1188

Epoch 532: val_loss did not improve from 17.00438
196/196 - 70s - loss: 16.4072 - MinusLogProbMetric: 16.4072 - val_loss: 17.1188 - val_MinusLogProbMetric: 17.1188 - lr: 4.1667e-05 - 70s/epoch - 357ms/step
Epoch 533/1000
2023-09-28 02:41:44.970 
Epoch 533/1000 
	 loss: 16.3790, MinusLogProbMetric: 16.3790, val_loss: 17.0089, val_MinusLogProbMetric: 17.0089

Epoch 533: val_loss did not improve from 17.00438
196/196 - 70s - loss: 16.3790 - MinusLogProbMetric: 16.3790 - val_loss: 17.0089 - val_MinusLogProbMetric: 17.0089 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 534/1000
2023-09-28 02:42:54.150 
Epoch 534/1000 
	 loss: 16.3849, MinusLogProbMetric: 16.3849, val_loss: 17.0229, val_MinusLogProbMetric: 17.0229

Epoch 534: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.3849 - MinusLogProbMetric: 16.3849 - val_loss: 17.0229 - val_MinusLogProbMetric: 17.0229 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 535/1000
2023-09-28 02:44:03.906 
Epoch 535/1000 
	 loss: 16.3923, MinusLogProbMetric: 16.3923, val_loss: 17.0113, val_MinusLogProbMetric: 17.0113

Epoch 535: val_loss did not improve from 17.00438
196/196 - 70s - loss: 16.3923 - MinusLogProbMetric: 16.3923 - val_loss: 17.0113 - val_MinusLogProbMetric: 17.0113 - lr: 4.1667e-05 - 70s/epoch - 356ms/step
Epoch 536/1000
2023-09-28 02:45:12.736 
Epoch 536/1000 
	 loss: 16.4174, MinusLogProbMetric: 16.4174, val_loss: 17.0676, val_MinusLogProbMetric: 17.0676

Epoch 536: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.4174 - MinusLogProbMetric: 16.4174 - val_loss: 17.0676 - val_MinusLogProbMetric: 17.0676 - lr: 4.1667e-05 - 69s/epoch - 351ms/step
Epoch 537/1000
2023-09-28 02:46:22.014 
Epoch 537/1000 
	 loss: 16.3947, MinusLogProbMetric: 16.3947, val_loss: 17.1167, val_MinusLogProbMetric: 17.1167

Epoch 537: val_loss did not improve from 17.00438
196/196 - 69s - loss: 16.3947 - MinusLogProbMetric: 16.3947 - val_loss: 17.1167 - val_MinusLogProbMetric: 17.1167 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 538/1000
2023-09-28 02:47:30.382 
Epoch 538/1000 
	 loss: 16.4043, MinusLogProbMetric: 16.4043, val_loss: 17.0160, val_MinusLogProbMetric: 17.0160

Epoch 538: val_loss did not improve from 17.00438
196/196 - 68s - loss: 16.4043 - MinusLogProbMetric: 16.4043 - val_loss: 17.0160 - val_MinusLogProbMetric: 17.0160 - lr: 4.1667e-05 - 68s/epoch - 349ms/step
Epoch 539/1000
2023-09-28 02:48:39.364 
Epoch 539/1000 
	 loss: 16.3873, MinusLogProbMetric: 16.3873, val_loss: 17.0036, val_MinusLogProbMetric: 17.0036

Epoch 539: val_loss improved from 17.00438 to 17.00359, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 70s - loss: 16.3873 - MinusLogProbMetric: 16.3873 - val_loss: 17.0036 - val_MinusLogProbMetric: 17.0036 - lr: 4.1667e-05 - 70s/epoch - 357ms/step
Epoch 540/1000
2023-09-28 02:49:50.216 
Epoch 540/1000 
	 loss: 16.3844, MinusLogProbMetric: 16.3844, val_loss: 17.0443, val_MinusLogProbMetric: 17.0443

Epoch 540: val_loss did not improve from 17.00359
196/196 - 70s - loss: 16.3844 - MinusLogProbMetric: 16.3844 - val_loss: 17.0443 - val_MinusLogProbMetric: 17.0443 - lr: 4.1667e-05 - 70s/epoch - 356ms/step
Epoch 541/1000
2023-09-28 02:50:58.975 
Epoch 541/1000 
	 loss: 16.4020, MinusLogProbMetric: 16.4020, val_loss: 17.0069, val_MinusLogProbMetric: 17.0069

Epoch 541: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.4020 - MinusLogProbMetric: 16.4020 - val_loss: 17.0069 - val_MinusLogProbMetric: 17.0069 - lr: 4.1667e-05 - 69s/epoch - 351ms/step
Epoch 542/1000
2023-09-28 02:52:07.588 
Epoch 542/1000 
	 loss: 16.3797, MinusLogProbMetric: 16.3797, val_loss: 17.0977, val_MinusLogProbMetric: 17.0977

Epoch 542: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3797 - MinusLogProbMetric: 16.3797 - val_loss: 17.0977 - val_MinusLogProbMetric: 17.0977 - lr: 4.1667e-05 - 69s/epoch - 350ms/step
Epoch 543/1000
2023-09-28 02:53:16.521 
Epoch 543/1000 
	 loss: 16.4142, MinusLogProbMetric: 16.4142, val_loss: 17.1562, val_MinusLogProbMetric: 17.1562

Epoch 543: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.4142 - MinusLogProbMetric: 16.4142 - val_loss: 17.1562 - val_MinusLogProbMetric: 17.1562 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 544/1000
2023-09-28 02:54:25.265 
Epoch 544/1000 
	 loss: 16.3887, MinusLogProbMetric: 16.3887, val_loss: 17.0280, val_MinusLogProbMetric: 17.0280

Epoch 544: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3887 - MinusLogProbMetric: 16.3887 - val_loss: 17.0280 - val_MinusLogProbMetric: 17.0280 - lr: 4.1667e-05 - 69s/epoch - 351ms/step
Epoch 545/1000
2023-09-28 02:55:34.083 
Epoch 545/1000 
	 loss: 16.4032, MinusLogProbMetric: 16.4032, val_loss: 17.0291, val_MinusLogProbMetric: 17.0291

Epoch 545: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.4032 - MinusLogProbMetric: 16.4032 - val_loss: 17.0291 - val_MinusLogProbMetric: 17.0291 - lr: 4.1667e-05 - 69s/epoch - 351ms/step
Epoch 546/1000
2023-09-28 02:56:43.147 
Epoch 546/1000 
	 loss: 16.3917, MinusLogProbMetric: 16.3917, val_loss: 17.1556, val_MinusLogProbMetric: 17.1556

Epoch 546: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3917 - MinusLogProbMetric: 16.3917 - val_loss: 17.1556 - val_MinusLogProbMetric: 17.1556 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 547/1000
2023-09-28 02:57:52.472 
Epoch 547/1000 
	 loss: 16.3866, MinusLogProbMetric: 16.3866, val_loss: 17.0212, val_MinusLogProbMetric: 17.0212

Epoch 547: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3866 - MinusLogProbMetric: 16.3866 - val_loss: 17.0212 - val_MinusLogProbMetric: 17.0212 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 548/1000
2023-09-28 02:59:01.220 
Epoch 548/1000 
	 loss: 16.4083, MinusLogProbMetric: 16.4083, val_loss: 17.0134, val_MinusLogProbMetric: 17.0134

Epoch 548: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.4083 - MinusLogProbMetric: 16.4083 - val_loss: 17.0134 - val_MinusLogProbMetric: 17.0134 - lr: 4.1667e-05 - 69s/epoch - 351ms/step
Epoch 549/1000
2023-09-28 03:00:10.752 
Epoch 549/1000 
	 loss: 16.3798, MinusLogProbMetric: 16.3798, val_loss: 17.0531, val_MinusLogProbMetric: 17.0531

Epoch 549: val_loss did not improve from 17.00359
196/196 - 70s - loss: 16.3798 - MinusLogProbMetric: 16.3798 - val_loss: 17.0531 - val_MinusLogProbMetric: 17.0531 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 550/1000
2023-09-28 03:01:19.895 
Epoch 550/1000 
	 loss: 16.3855, MinusLogProbMetric: 16.3855, val_loss: 17.0611, val_MinusLogProbMetric: 17.0611

Epoch 550: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3855 - MinusLogProbMetric: 16.3855 - val_loss: 17.0611 - val_MinusLogProbMetric: 17.0611 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 551/1000
2023-09-28 03:02:29.146 
Epoch 551/1000 
	 loss: 16.4140, MinusLogProbMetric: 16.4140, val_loss: 17.0154, val_MinusLogProbMetric: 17.0154

Epoch 551: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.4140 - MinusLogProbMetric: 16.4140 - val_loss: 17.0154 - val_MinusLogProbMetric: 17.0154 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 552/1000
2023-09-28 03:03:38.480 
Epoch 552/1000 
	 loss: 16.3894, MinusLogProbMetric: 16.3894, val_loss: 17.3508, val_MinusLogProbMetric: 17.3508

Epoch 552: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3894 - MinusLogProbMetric: 16.3894 - val_loss: 17.3508 - val_MinusLogProbMetric: 17.3508 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 553/1000
2023-09-28 03:04:48.052 
Epoch 553/1000 
	 loss: 16.3850, MinusLogProbMetric: 16.3850, val_loss: 17.1275, val_MinusLogProbMetric: 17.1275

Epoch 553: val_loss did not improve from 17.00359
196/196 - 70s - loss: 16.3850 - MinusLogProbMetric: 16.3850 - val_loss: 17.1275 - val_MinusLogProbMetric: 17.1275 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 554/1000
2023-09-28 03:05:57.465 
Epoch 554/1000 
	 loss: 16.3817, MinusLogProbMetric: 16.3817, val_loss: 17.0125, val_MinusLogProbMetric: 17.0125

Epoch 554: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3817 - MinusLogProbMetric: 16.3817 - val_loss: 17.0125 - val_MinusLogProbMetric: 17.0125 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 555/1000
2023-09-28 03:07:06.222 
Epoch 555/1000 
	 loss: 16.3973, MinusLogProbMetric: 16.3973, val_loss: 17.0230, val_MinusLogProbMetric: 17.0230

Epoch 555: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3973 - MinusLogProbMetric: 16.3973 - val_loss: 17.0230 - val_MinusLogProbMetric: 17.0230 - lr: 4.1667e-05 - 69s/epoch - 351ms/step
Epoch 556/1000
2023-09-28 03:08:16.155 
Epoch 556/1000 
	 loss: 16.3885, MinusLogProbMetric: 16.3885, val_loss: 17.0445, val_MinusLogProbMetric: 17.0445

Epoch 556: val_loss did not improve from 17.00359
196/196 - 70s - loss: 16.3885 - MinusLogProbMetric: 16.3885 - val_loss: 17.0445 - val_MinusLogProbMetric: 17.0445 - lr: 4.1667e-05 - 70s/epoch - 357ms/step
Epoch 557/1000
2023-09-28 03:09:25.240 
Epoch 557/1000 
	 loss: 16.3996, MinusLogProbMetric: 16.3996, val_loss: 17.0592, val_MinusLogProbMetric: 17.0592

Epoch 557: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3996 - MinusLogProbMetric: 16.3996 - val_loss: 17.0592 - val_MinusLogProbMetric: 17.0592 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 558/1000
2023-09-28 03:10:34.509 
Epoch 558/1000 
	 loss: 16.3627, MinusLogProbMetric: 16.3627, val_loss: 17.0338, val_MinusLogProbMetric: 17.0338

Epoch 558: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3627 - MinusLogProbMetric: 16.3627 - val_loss: 17.0338 - val_MinusLogProbMetric: 17.0338 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 559/1000
2023-09-28 03:11:43.300 
Epoch 559/1000 
	 loss: 16.3900, MinusLogProbMetric: 16.3900, val_loss: 17.0874, val_MinusLogProbMetric: 17.0874

Epoch 559: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3900 - MinusLogProbMetric: 16.3900 - val_loss: 17.0874 - val_MinusLogProbMetric: 17.0874 - lr: 4.1667e-05 - 69s/epoch - 351ms/step
Epoch 560/1000
2023-09-28 03:12:52.536 
Epoch 560/1000 
	 loss: 16.3873, MinusLogProbMetric: 16.3873, val_loss: 17.0295, val_MinusLogProbMetric: 17.0295

Epoch 560: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3873 - MinusLogProbMetric: 16.3873 - val_loss: 17.0295 - val_MinusLogProbMetric: 17.0295 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 561/1000
2023-09-28 03:14:01.745 
Epoch 561/1000 
	 loss: 16.3693, MinusLogProbMetric: 16.3693, val_loss: 17.0326, val_MinusLogProbMetric: 17.0326

Epoch 561: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3693 - MinusLogProbMetric: 16.3693 - val_loss: 17.0326 - val_MinusLogProbMetric: 17.0326 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 562/1000
2023-09-28 03:15:10.748 
Epoch 562/1000 
	 loss: 16.4250, MinusLogProbMetric: 16.4250, val_loss: 17.0415, val_MinusLogProbMetric: 17.0415

Epoch 562: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.4250 - MinusLogProbMetric: 16.4250 - val_loss: 17.0415 - val_MinusLogProbMetric: 17.0415 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 563/1000
2023-09-28 03:16:19.724 
Epoch 563/1000 
	 loss: 16.3739, MinusLogProbMetric: 16.3739, val_loss: 17.0774, val_MinusLogProbMetric: 17.0774

Epoch 563: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3739 - MinusLogProbMetric: 16.3739 - val_loss: 17.0774 - val_MinusLogProbMetric: 17.0774 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 564/1000
2023-09-28 03:17:28.740 
Epoch 564/1000 
	 loss: 16.3984, MinusLogProbMetric: 16.3984, val_loss: 17.0156, val_MinusLogProbMetric: 17.0156

Epoch 564: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3984 - MinusLogProbMetric: 16.3984 - val_loss: 17.0156 - val_MinusLogProbMetric: 17.0156 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 565/1000
2023-09-28 03:18:37.714 
Epoch 565/1000 
	 loss: 16.3886, MinusLogProbMetric: 16.3886, val_loss: 17.0133, val_MinusLogProbMetric: 17.0133

Epoch 565: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3886 - MinusLogProbMetric: 16.3886 - val_loss: 17.0133 - val_MinusLogProbMetric: 17.0133 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 566/1000
2023-09-28 03:19:46.838 
Epoch 566/1000 
	 loss: 16.4011, MinusLogProbMetric: 16.4011, val_loss: 17.1006, val_MinusLogProbMetric: 17.1006

Epoch 566: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.4011 - MinusLogProbMetric: 16.4011 - val_loss: 17.1006 - val_MinusLogProbMetric: 17.1006 - lr: 4.1667e-05 - 69s/epoch - 353ms/step
Epoch 567/1000
2023-09-28 03:20:56.394 
Epoch 567/1000 
	 loss: 16.3894, MinusLogProbMetric: 16.3894, val_loss: 17.0308, val_MinusLogProbMetric: 17.0308

Epoch 567: val_loss did not improve from 17.00359
196/196 - 70s - loss: 16.3894 - MinusLogProbMetric: 16.3894 - val_loss: 17.0308 - val_MinusLogProbMetric: 17.0308 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 568/1000
2023-09-28 03:22:06.623 
Epoch 568/1000 
	 loss: 16.3947, MinusLogProbMetric: 16.3947, val_loss: 17.0927, val_MinusLogProbMetric: 17.0927

Epoch 568: val_loss did not improve from 17.00359
196/196 - 70s - loss: 16.3947 - MinusLogProbMetric: 16.3947 - val_loss: 17.0927 - val_MinusLogProbMetric: 17.0927 - lr: 4.1667e-05 - 70s/epoch - 358ms/step
Epoch 569/1000
2023-09-28 03:23:16.064 
Epoch 569/1000 
	 loss: 16.3752, MinusLogProbMetric: 16.3752, val_loss: 17.0194, val_MinusLogProbMetric: 17.0194

Epoch 569: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3752 - MinusLogProbMetric: 16.3752 - val_loss: 17.0194 - val_MinusLogProbMetric: 17.0194 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 570/1000
2023-09-28 03:24:25.843 
Epoch 570/1000 
	 loss: 16.3765, MinusLogProbMetric: 16.3765, val_loss: 17.0255, val_MinusLogProbMetric: 17.0255

Epoch 570: val_loss did not improve from 17.00359
196/196 - 70s - loss: 16.3765 - MinusLogProbMetric: 16.3765 - val_loss: 17.0255 - val_MinusLogProbMetric: 17.0255 - lr: 4.1667e-05 - 70s/epoch - 356ms/step
Epoch 571/1000
2023-09-28 03:25:35.952 
Epoch 571/1000 
	 loss: 16.3745, MinusLogProbMetric: 16.3745, val_loss: 17.0606, val_MinusLogProbMetric: 17.0606

Epoch 571: val_loss did not improve from 17.00359
196/196 - 70s - loss: 16.3745 - MinusLogProbMetric: 16.3745 - val_loss: 17.0606 - val_MinusLogProbMetric: 17.0606 - lr: 4.1667e-05 - 70s/epoch - 358ms/step
Epoch 572/1000
2023-09-28 03:26:45.282 
Epoch 572/1000 
	 loss: 16.4024, MinusLogProbMetric: 16.4024, val_loss: 17.0938, val_MinusLogProbMetric: 17.0938

Epoch 572: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.4024 - MinusLogProbMetric: 16.4024 - val_loss: 17.0938 - val_MinusLogProbMetric: 17.0938 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 573/1000
2023-09-28 03:27:55.077 
Epoch 573/1000 
	 loss: 16.3877, MinusLogProbMetric: 16.3877, val_loss: 17.0363, val_MinusLogProbMetric: 17.0363

Epoch 573: val_loss did not improve from 17.00359
196/196 - 70s - loss: 16.3877 - MinusLogProbMetric: 16.3877 - val_loss: 17.0363 - val_MinusLogProbMetric: 17.0363 - lr: 4.1667e-05 - 70s/epoch - 356ms/step
Epoch 574/1000
2023-09-28 03:29:04.638 
Epoch 574/1000 
	 loss: 16.3806, MinusLogProbMetric: 16.3806, val_loss: 17.0686, val_MinusLogProbMetric: 17.0686

Epoch 574: val_loss did not improve from 17.00359
196/196 - 70s - loss: 16.3806 - MinusLogProbMetric: 16.3806 - val_loss: 17.0686 - val_MinusLogProbMetric: 17.0686 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 575/1000
2023-09-28 03:30:14.115 
Epoch 575/1000 
	 loss: 16.4060, MinusLogProbMetric: 16.4060, val_loss: 17.0439, val_MinusLogProbMetric: 17.0439

Epoch 575: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.4060 - MinusLogProbMetric: 16.4060 - val_loss: 17.0439 - val_MinusLogProbMetric: 17.0439 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 576/1000
2023-09-28 03:31:23.641 
Epoch 576/1000 
	 loss: 16.3729, MinusLogProbMetric: 16.3729, val_loss: 17.0602, val_MinusLogProbMetric: 17.0602

Epoch 576: val_loss did not improve from 17.00359
196/196 - 70s - loss: 16.3729 - MinusLogProbMetric: 16.3729 - val_loss: 17.0602 - val_MinusLogProbMetric: 17.0602 - lr: 4.1667e-05 - 70s/epoch - 355ms/step
Epoch 577/1000
2023-09-28 03:32:33.105 
Epoch 577/1000 
	 loss: 16.3872, MinusLogProbMetric: 16.3872, val_loss: 17.0341, val_MinusLogProbMetric: 17.0341

Epoch 577: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3872 - MinusLogProbMetric: 16.3872 - val_loss: 17.0341 - val_MinusLogProbMetric: 17.0341 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 578/1000
2023-09-28 03:33:42.482 
Epoch 578/1000 
	 loss: 16.4319, MinusLogProbMetric: 16.4319, val_loss: 17.0638, val_MinusLogProbMetric: 17.0638

Epoch 578: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.4319 - MinusLogProbMetric: 16.4319 - val_loss: 17.0638 - val_MinusLogProbMetric: 17.0638 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 579/1000
2023-09-28 03:34:51.801 
Epoch 579/1000 
	 loss: 16.3831, MinusLogProbMetric: 16.3831, val_loss: 17.0572, val_MinusLogProbMetric: 17.0572

Epoch 579: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3831 - MinusLogProbMetric: 16.3831 - val_loss: 17.0572 - val_MinusLogProbMetric: 17.0572 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 580/1000
2023-09-28 03:36:00.758 
Epoch 580/1000 
	 loss: 16.3801, MinusLogProbMetric: 16.3801, val_loss: 17.0199, val_MinusLogProbMetric: 17.0199

Epoch 580: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3801 - MinusLogProbMetric: 16.3801 - val_loss: 17.0199 - val_MinusLogProbMetric: 17.0199 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 581/1000
2023-09-28 03:37:10.499 
Epoch 581/1000 
	 loss: 16.3936, MinusLogProbMetric: 16.3936, val_loss: 17.0118, val_MinusLogProbMetric: 17.0118

Epoch 581: val_loss did not improve from 17.00359
196/196 - 70s - loss: 16.3936 - MinusLogProbMetric: 16.3936 - val_loss: 17.0118 - val_MinusLogProbMetric: 17.0118 - lr: 4.1667e-05 - 70s/epoch - 356ms/step
Epoch 582/1000
2023-09-28 03:38:19.534 
Epoch 582/1000 
	 loss: 16.3875, MinusLogProbMetric: 16.3875, val_loss: 17.0689, val_MinusLogProbMetric: 17.0689

Epoch 582: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3875 - MinusLogProbMetric: 16.3875 - val_loss: 17.0689 - val_MinusLogProbMetric: 17.0689 - lr: 4.1667e-05 - 69s/epoch - 352ms/step
Epoch 583/1000
2023-09-28 03:39:29.268 
Epoch 583/1000 
	 loss: 16.3814, MinusLogProbMetric: 16.3814, val_loss: 17.0210, val_MinusLogProbMetric: 17.0210

Epoch 583: val_loss did not improve from 17.00359
196/196 - 70s - loss: 16.3814 - MinusLogProbMetric: 16.3814 - val_loss: 17.0210 - val_MinusLogProbMetric: 17.0210 - lr: 4.1667e-05 - 70s/epoch - 356ms/step
Epoch 584/1000
2023-09-28 03:40:38.707 
Epoch 584/1000 
	 loss: 16.3835, MinusLogProbMetric: 16.3835, val_loss: 17.1977, val_MinusLogProbMetric: 17.1977

Epoch 584: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3835 - MinusLogProbMetric: 16.3835 - val_loss: 17.1977 - val_MinusLogProbMetric: 17.1977 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 585/1000
2023-09-28 03:41:48.888 
Epoch 585/1000 
	 loss: 16.4187, MinusLogProbMetric: 16.4187, val_loss: 17.0463, val_MinusLogProbMetric: 17.0463

Epoch 585: val_loss did not improve from 17.00359
196/196 - 70s - loss: 16.4187 - MinusLogProbMetric: 16.4187 - val_loss: 17.0463 - val_MinusLogProbMetric: 17.0463 - lr: 4.1667e-05 - 70s/epoch - 358ms/step
Epoch 586/1000
2023-09-28 03:42:58.286 
Epoch 586/1000 
	 loss: 16.3856, MinusLogProbMetric: 16.3856, val_loss: 17.0154, val_MinusLogProbMetric: 17.0154

Epoch 586: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3856 - MinusLogProbMetric: 16.3856 - val_loss: 17.0154 - val_MinusLogProbMetric: 17.0154 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 587/1000
2023-09-28 03:44:08.360 
Epoch 587/1000 
	 loss: 16.3770, MinusLogProbMetric: 16.3770, val_loss: 17.0243, val_MinusLogProbMetric: 17.0243

Epoch 587: val_loss did not improve from 17.00359
196/196 - 70s - loss: 16.3770 - MinusLogProbMetric: 16.3770 - val_loss: 17.0243 - val_MinusLogProbMetric: 17.0243 - lr: 4.1667e-05 - 70s/epoch - 358ms/step
Epoch 588/1000
2023-09-28 03:45:18.066 
Epoch 588/1000 
	 loss: 16.3806, MinusLogProbMetric: 16.3806, val_loss: 17.0099, val_MinusLogProbMetric: 17.0099

Epoch 588: val_loss did not improve from 17.00359
196/196 - 70s - loss: 16.3806 - MinusLogProbMetric: 16.3806 - val_loss: 17.0099 - val_MinusLogProbMetric: 17.0099 - lr: 4.1667e-05 - 70s/epoch - 356ms/step
Epoch 589/1000
2023-09-28 03:46:27.536 
Epoch 589/1000 
	 loss: 16.3632, MinusLogProbMetric: 16.3632, val_loss: 17.0126, val_MinusLogProbMetric: 17.0126

Epoch 589: val_loss did not improve from 17.00359
196/196 - 69s - loss: 16.3632 - MinusLogProbMetric: 16.3632 - val_loss: 17.0126 - val_MinusLogProbMetric: 17.0126 - lr: 4.1667e-05 - 69s/epoch - 354ms/step
Epoch 590/1000
2023-09-28 03:47:36.621 
Epoch 590/1000 
	 loss: 16.3247, MinusLogProbMetric: 16.3247, val_loss: 16.9947, val_MinusLogProbMetric: 16.9947

Epoch 590: val_loss improved from 17.00359 to 16.99473, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 70s - loss: 16.3247 - MinusLogProbMetric: 16.3247 - val_loss: 16.9947 - val_MinusLogProbMetric: 16.9947 - lr: 2.0833e-05 - 70s/epoch - 358ms/step
Epoch 591/1000
2023-09-28 03:48:47.151 
Epoch 591/1000 
	 loss: 16.3289, MinusLogProbMetric: 16.3289, val_loss: 17.0045, val_MinusLogProbMetric: 17.0045

Epoch 591: val_loss did not improve from 16.99473
196/196 - 69s - loss: 16.3289 - MinusLogProbMetric: 16.3289 - val_loss: 17.0045 - val_MinusLogProbMetric: 17.0045 - lr: 2.0833e-05 - 69s/epoch - 355ms/step
Epoch 592/1000
2023-09-28 03:49:57.030 
Epoch 592/1000 
	 loss: 16.3245, MinusLogProbMetric: 16.3245, val_loss: 17.0175, val_MinusLogProbMetric: 17.0175

Epoch 592: val_loss did not improve from 16.99473
196/196 - 70s - loss: 16.3245 - MinusLogProbMetric: 16.3245 - val_loss: 17.0175 - val_MinusLogProbMetric: 17.0175 - lr: 2.0833e-05 - 70s/epoch - 357ms/step
Epoch 593/1000
2023-09-28 03:51:05.773 
Epoch 593/1000 
	 loss: 16.3267, MinusLogProbMetric: 16.3267, val_loss: 17.0053, val_MinusLogProbMetric: 17.0053

Epoch 593: val_loss did not improve from 16.99473
196/196 - 69s - loss: 16.3267 - MinusLogProbMetric: 16.3267 - val_loss: 17.0053 - val_MinusLogProbMetric: 17.0053 - lr: 2.0833e-05 - 69s/epoch - 351ms/step
Epoch 594/1000
2023-09-28 03:52:15.354 
Epoch 594/1000 
	 loss: 16.3375, MinusLogProbMetric: 16.3375, val_loss: 17.0043, val_MinusLogProbMetric: 17.0043

Epoch 594: val_loss did not improve from 16.99473
196/196 - 70s - loss: 16.3375 - MinusLogProbMetric: 16.3375 - val_loss: 17.0043 - val_MinusLogProbMetric: 17.0043 - lr: 2.0833e-05 - 70s/epoch - 355ms/step
Epoch 595/1000
2023-09-28 03:53:24.814 
Epoch 595/1000 
	 loss: 16.3275, MinusLogProbMetric: 16.3275, val_loss: 17.0035, val_MinusLogProbMetric: 17.0035

Epoch 595: val_loss did not improve from 16.99473
196/196 - 69s - loss: 16.3275 - MinusLogProbMetric: 16.3275 - val_loss: 17.0035 - val_MinusLogProbMetric: 17.0035 - lr: 2.0833e-05 - 69s/epoch - 354ms/step
Epoch 596/1000
2023-09-28 03:54:34.234 
Epoch 596/1000 
	 loss: 16.3294, MinusLogProbMetric: 16.3294, val_loss: 17.0284, val_MinusLogProbMetric: 17.0284

Epoch 596: val_loss did not improve from 16.99473
196/196 - 69s - loss: 16.3294 - MinusLogProbMetric: 16.3294 - val_loss: 17.0284 - val_MinusLogProbMetric: 17.0284 - lr: 2.0833e-05 - 69s/epoch - 354ms/step
Epoch 597/1000
2023-09-28 03:55:43.547 
Epoch 597/1000 
	 loss: 16.3209, MinusLogProbMetric: 16.3209, val_loss: 17.0051, val_MinusLogProbMetric: 17.0051

Epoch 597: val_loss did not improve from 16.99473
196/196 - 69s - loss: 16.3209 - MinusLogProbMetric: 16.3209 - val_loss: 17.0051 - val_MinusLogProbMetric: 17.0051 - lr: 2.0833e-05 - 69s/epoch - 354ms/step
Epoch 598/1000
2023-09-28 03:56:52.943 
Epoch 598/1000 
	 loss: 16.3289, MinusLogProbMetric: 16.3289, val_loss: 17.0257, val_MinusLogProbMetric: 17.0257

Epoch 598: val_loss did not improve from 16.99473
196/196 - 69s - loss: 16.3289 - MinusLogProbMetric: 16.3289 - val_loss: 17.0257 - val_MinusLogProbMetric: 17.0257 - lr: 2.0833e-05 - 69s/epoch - 354ms/step
Epoch 599/1000
2023-09-28 03:58:02.572 
Epoch 599/1000 
	 loss: 16.3325, MinusLogProbMetric: 16.3325, val_loss: 16.9935, val_MinusLogProbMetric: 16.9935

Epoch 599: val_loss improved from 16.99473 to 16.99354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 71s - loss: 16.3325 - MinusLogProbMetric: 16.3325 - val_loss: 16.9935 - val_MinusLogProbMetric: 16.9935 - lr: 2.0833e-05 - 71s/epoch - 360ms/step
Epoch 600/1000
2023-09-28 03:59:13.395 
Epoch 600/1000 
	 loss: 16.3398, MinusLogProbMetric: 16.3398, val_loss: 16.9965, val_MinusLogProbMetric: 16.9965

Epoch 600: val_loss did not improve from 16.99354
196/196 - 70s - loss: 16.3398 - MinusLogProbMetric: 16.3398 - val_loss: 16.9965 - val_MinusLogProbMetric: 16.9965 - lr: 2.0833e-05 - 70s/epoch - 357ms/step
Epoch 601/1000
2023-09-28 04:00:22.682 
Epoch 601/1000 
	 loss: 16.3242, MinusLogProbMetric: 16.3242, val_loss: 17.0136, val_MinusLogProbMetric: 17.0136

Epoch 601: val_loss did not improve from 16.99354
196/196 - 69s - loss: 16.3242 - MinusLogProbMetric: 16.3242 - val_loss: 17.0136 - val_MinusLogProbMetric: 17.0136 - lr: 2.0833e-05 - 69s/epoch - 353ms/step
Epoch 602/1000
2023-09-28 04:01:32.075 
Epoch 602/1000 
	 loss: 16.3327, MinusLogProbMetric: 16.3327, val_loss: 17.0028, val_MinusLogProbMetric: 17.0028

Epoch 602: val_loss did not improve from 16.99354
196/196 - 69s - loss: 16.3327 - MinusLogProbMetric: 16.3327 - val_loss: 17.0028 - val_MinusLogProbMetric: 17.0028 - lr: 2.0833e-05 - 69s/epoch - 354ms/step
Epoch 603/1000
2023-09-28 04:02:41.203 
Epoch 603/1000 
	 loss: 16.3358, MinusLogProbMetric: 16.3358, val_loss: 17.0177, val_MinusLogProbMetric: 17.0177

Epoch 603: val_loss did not improve from 16.99354
196/196 - 69s - loss: 16.3358 - MinusLogProbMetric: 16.3358 - val_loss: 17.0177 - val_MinusLogProbMetric: 17.0177 - lr: 2.0833e-05 - 69s/epoch - 353ms/step
Epoch 604/1000
2023-09-28 04:03:50.788 
Epoch 604/1000 
	 loss: 16.3377, MinusLogProbMetric: 16.3377, val_loss: 17.0674, val_MinusLogProbMetric: 17.0674

Epoch 604: val_loss did not improve from 16.99354
196/196 - 70s - loss: 16.3377 - MinusLogProbMetric: 16.3377 - val_loss: 17.0674 - val_MinusLogProbMetric: 17.0674 - lr: 2.0833e-05 - 70s/epoch - 355ms/step
Epoch 605/1000
2023-09-28 04:05:00.985 
Epoch 605/1000 
	 loss: 16.3280, MinusLogProbMetric: 16.3280, val_loss: 17.0131, val_MinusLogProbMetric: 17.0131

Epoch 605: val_loss did not improve from 16.99354
196/196 - 70s - loss: 16.3280 - MinusLogProbMetric: 16.3280 - val_loss: 17.0131 - val_MinusLogProbMetric: 17.0131 - lr: 2.0833e-05 - 70s/epoch - 358ms/step
Epoch 606/1000
2023-09-28 04:06:09.885 
Epoch 606/1000 
	 loss: 16.3304, MinusLogProbMetric: 16.3304, val_loss: 17.0068, val_MinusLogProbMetric: 17.0068

Epoch 606: val_loss did not improve from 16.99354
196/196 - 69s - loss: 16.3304 - MinusLogProbMetric: 16.3304 - val_loss: 17.0068 - val_MinusLogProbMetric: 17.0068 - lr: 2.0833e-05 - 69s/epoch - 351ms/step
Epoch 607/1000
2023-09-28 04:07:18.528 
Epoch 607/1000 
	 loss: 16.3258, MinusLogProbMetric: 16.3258, val_loss: 16.9882, val_MinusLogProbMetric: 16.9882

Epoch 607: val_loss improved from 16.99354 to 16.98817, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 70s - loss: 16.3258 - MinusLogProbMetric: 16.3258 - val_loss: 16.9882 - val_MinusLogProbMetric: 16.9882 - lr: 2.0833e-05 - 70s/epoch - 356ms/step
Epoch 608/1000
2023-09-28 04:08:28.538 
Epoch 608/1000 
	 loss: 16.3228, MinusLogProbMetric: 16.3228, val_loss: 17.0198, val_MinusLogProbMetric: 17.0198

Epoch 608: val_loss did not improve from 16.98817
196/196 - 69s - loss: 16.3228 - MinusLogProbMetric: 16.3228 - val_loss: 17.0198 - val_MinusLogProbMetric: 17.0198 - lr: 2.0833e-05 - 69s/epoch - 352ms/step
Epoch 609/1000
2023-09-28 04:09:37.751 
Epoch 609/1000 
	 loss: 16.3280, MinusLogProbMetric: 16.3280, val_loss: 17.0139, val_MinusLogProbMetric: 17.0139

Epoch 609: val_loss did not improve from 16.98817
196/196 - 69s - loss: 16.3280 - MinusLogProbMetric: 16.3280 - val_loss: 17.0139 - val_MinusLogProbMetric: 17.0139 - lr: 2.0833e-05 - 69s/epoch - 353ms/step
Epoch 610/1000
2023-09-28 04:10:47.385 
Epoch 610/1000 
	 loss: 16.3212, MinusLogProbMetric: 16.3212, val_loss: 17.0056, val_MinusLogProbMetric: 17.0056

Epoch 610: val_loss did not improve from 16.98817
196/196 - 70s - loss: 16.3212 - MinusLogProbMetric: 16.3212 - val_loss: 17.0056 - val_MinusLogProbMetric: 17.0056 - lr: 2.0833e-05 - 70s/epoch - 355ms/step
Epoch 611/1000
2023-09-28 04:11:56.490 
Epoch 611/1000 
	 loss: 16.3308, MinusLogProbMetric: 16.3308, val_loss: 17.0233, val_MinusLogProbMetric: 17.0233

Epoch 611: val_loss did not improve from 16.98817
196/196 - 69s - loss: 16.3308 - MinusLogProbMetric: 16.3308 - val_loss: 17.0233 - val_MinusLogProbMetric: 17.0233 - lr: 2.0833e-05 - 69s/epoch - 353ms/step
Epoch 612/1000
2023-09-28 04:13:05.899 
Epoch 612/1000 
	 loss: 16.3276, MinusLogProbMetric: 16.3276, val_loss: 17.0140, val_MinusLogProbMetric: 17.0140

Epoch 612: val_loss did not improve from 16.98817
196/196 - 69s - loss: 16.3276 - MinusLogProbMetric: 16.3276 - val_loss: 17.0140 - val_MinusLogProbMetric: 17.0140 - lr: 2.0833e-05 - 69s/epoch - 354ms/step
Epoch 613/1000
2023-09-28 04:14:15.375 
Epoch 613/1000 
	 loss: 16.3291, MinusLogProbMetric: 16.3291, val_loss: 17.0411, val_MinusLogProbMetric: 17.0411

Epoch 613: val_loss did not improve from 16.98817
196/196 - 69s - loss: 16.3291 - MinusLogProbMetric: 16.3291 - val_loss: 17.0411 - val_MinusLogProbMetric: 17.0411 - lr: 2.0833e-05 - 69s/epoch - 354ms/step
Epoch 614/1000
2023-09-28 04:15:25.061 
Epoch 614/1000 
	 loss: 16.3313, MinusLogProbMetric: 16.3313, val_loss: 16.9824, val_MinusLogProbMetric: 16.9824

Epoch 614: val_loss improved from 16.98817 to 16.98239, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 71s - loss: 16.3313 - MinusLogProbMetric: 16.3313 - val_loss: 16.9824 - val_MinusLogProbMetric: 16.9824 - lr: 2.0833e-05 - 71s/epoch - 360ms/step
Epoch 615/1000
2023-09-28 04:16:35.790 
Epoch 615/1000 
	 loss: 16.3204, MinusLogProbMetric: 16.3204, val_loss: 17.0101, val_MinusLogProbMetric: 17.0101

Epoch 615: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3204 - MinusLogProbMetric: 16.3204 - val_loss: 17.0101 - val_MinusLogProbMetric: 17.0101 - lr: 2.0833e-05 - 70s/epoch - 356ms/step
Epoch 616/1000
2023-09-28 04:17:44.876 
Epoch 616/1000 
	 loss: 16.3266, MinusLogProbMetric: 16.3266, val_loss: 16.9954, val_MinusLogProbMetric: 16.9954

Epoch 616: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3266 - MinusLogProbMetric: 16.3266 - val_loss: 16.9954 - val_MinusLogProbMetric: 16.9954 - lr: 2.0833e-05 - 69s/epoch - 352ms/step
Epoch 617/1000
2023-09-28 04:18:53.929 
Epoch 617/1000 
	 loss: 16.3278, MinusLogProbMetric: 16.3278, val_loss: 16.9973, val_MinusLogProbMetric: 16.9973

Epoch 617: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3278 - MinusLogProbMetric: 16.3278 - val_loss: 16.9973 - val_MinusLogProbMetric: 16.9973 - lr: 2.0833e-05 - 69s/epoch - 352ms/step
Epoch 618/1000
2023-09-28 04:20:03.450 
Epoch 618/1000 
	 loss: 16.3380, MinusLogProbMetric: 16.3380, val_loss: 17.0212, val_MinusLogProbMetric: 17.0212

Epoch 618: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3380 - MinusLogProbMetric: 16.3380 - val_loss: 17.0212 - val_MinusLogProbMetric: 17.0212 - lr: 2.0833e-05 - 70s/epoch - 355ms/step
Epoch 619/1000
2023-09-28 04:21:12.663 
Epoch 619/1000 
	 loss: 16.3290, MinusLogProbMetric: 16.3290, val_loss: 17.0424, val_MinusLogProbMetric: 17.0424

Epoch 619: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3290 - MinusLogProbMetric: 16.3290 - val_loss: 17.0424 - val_MinusLogProbMetric: 17.0424 - lr: 2.0833e-05 - 69s/epoch - 353ms/step
Epoch 620/1000
2023-09-28 04:22:22.009 
Epoch 620/1000 
	 loss: 16.3306, MinusLogProbMetric: 16.3306, val_loss: 17.1848, val_MinusLogProbMetric: 17.1848

Epoch 620: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3306 - MinusLogProbMetric: 16.3306 - val_loss: 17.1848 - val_MinusLogProbMetric: 17.1848 - lr: 2.0833e-05 - 69s/epoch - 354ms/step
Epoch 621/1000
2023-09-28 04:23:31.806 
Epoch 621/1000 
	 loss: 16.3359, MinusLogProbMetric: 16.3359, val_loss: 16.9973, val_MinusLogProbMetric: 16.9973

Epoch 621: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3359 - MinusLogProbMetric: 16.3359 - val_loss: 16.9973 - val_MinusLogProbMetric: 16.9973 - lr: 2.0833e-05 - 70s/epoch - 356ms/step
Epoch 622/1000
2023-09-28 04:24:41.251 
Epoch 622/1000 
	 loss: 16.3281, MinusLogProbMetric: 16.3281, val_loss: 17.0168, val_MinusLogProbMetric: 17.0168

Epoch 622: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3281 - MinusLogProbMetric: 16.3281 - val_loss: 17.0168 - val_MinusLogProbMetric: 17.0168 - lr: 2.0833e-05 - 69s/epoch - 354ms/step
Epoch 623/1000
2023-09-28 04:25:50.893 
Epoch 623/1000 
	 loss: 16.3261, MinusLogProbMetric: 16.3261, val_loss: 17.0526, val_MinusLogProbMetric: 17.0526

Epoch 623: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3261 - MinusLogProbMetric: 16.3261 - val_loss: 17.0526 - val_MinusLogProbMetric: 17.0526 - lr: 2.0833e-05 - 70s/epoch - 355ms/step
Epoch 624/1000
2023-09-28 04:26:59.933 
Epoch 624/1000 
	 loss: 16.3262, MinusLogProbMetric: 16.3262, val_loss: 17.0492, val_MinusLogProbMetric: 17.0492

Epoch 624: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3262 - MinusLogProbMetric: 16.3262 - val_loss: 17.0492 - val_MinusLogProbMetric: 17.0492 - lr: 2.0833e-05 - 69s/epoch - 352ms/step
Epoch 625/1000
2023-09-28 04:28:10.042 
Epoch 625/1000 
	 loss: 16.3358, MinusLogProbMetric: 16.3358, val_loss: 16.9906, val_MinusLogProbMetric: 16.9906

Epoch 625: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3358 - MinusLogProbMetric: 16.3358 - val_loss: 16.9906 - val_MinusLogProbMetric: 16.9906 - lr: 2.0833e-05 - 70s/epoch - 358ms/step
Epoch 626/1000
2023-09-28 04:29:20.251 
Epoch 626/1000 
	 loss: 16.3223, MinusLogProbMetric: 16.3223, val_loss: 17.0022, val_MinusLogProbMetric: 17.0022

Epoch 626: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3223 - MinusLogProbMetric: 16.3223 - val_loss: 17.0022 - val_MinusLogProbMetric: 17.0022 - lr: 2.0833e-05 - 70s/epoch - 358ms/step
Epoch 627/1000
2023-09-28 04:30:29.872 
Epoch 627/1000 
	 loss: 16.3180, MinusLogProbMetric: 16.3180, val_loss: 17.0202, val_MinusLogProbMetric: 17.0202

Epoch 627: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3180 - MinusLogProbMetric: 16.3180 - val_loss: 17.0202 - val_MinusLogProbMetric: 17.0202 - lr: 2.0833e-05 - 70s/epoch - 355ms/step
Epoch 628/1000
2023-09-28 04:31:38.736 
Epoch 628/1000 
	 loss: 16.3289, MinusLogProbMetric: 16.3289, val_loss: 17.1598, val_MinusLogProbMetric: 17.1598

Epoch 628: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3289 - MinusLogProbMetric: 16.3289 - val_loss: 17.1598 - val_MinusLogProbMetric: 17.1598 - lr: 2.0833e-05 - 69s/epoch - 351ms/step
Epoch 629/1000
2023-09-28 04:32:49.189 
Epoch 629/1000 
	 loss: 16.3401, MinusLogProbMetric: 16.3401, val_loss: 17.1001, val_MinusLogProbMetric: 17.1001

Epoch 629: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3401 - MinusLogProbMetric: 16.3401 - val_loss: 17.1001 - val_MinusLogProbMetric: 17.1001 - lr: 2.0833e-05 - 70s/epoch - 359ms/step
Epoch 630/1000
2023-09-28 04:33:59.093 
Epoch 630/1000 
	 loss: 16.3292, MinusLogProbMetric: 16.3292, val_loss: 17.0148, val_MinusLogProbMetric: 17.0148

Epoch 630: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3292 - MinusLogProbMetric: 16.3292 - val_loss: 17.0148 - val_MinusLogProbMetric: 17.0148 - lr: 2.0833e-05 - 70s/epoch - 357ms/step
Epoch 631/1000
2023-09-28 04:35:08.396 
Epoch 631/1000 
	 loss: 16.3182, MinusLogProbMetric: 16.3182, val_loss: 17.0170, val_MinusLogProbMetric: 17.0170

Epoch 631: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3182 - MinusLogProbMetric: 16.3182 - val_loss: 17.0170 - val_MinusLogProbMetric: 17.0170 - lr: 2.0833e-05 - 69s/epoch - 354ms/step
Epoch 632/1000
2023-09-28 04:36:17.999 
Epoch 632/1000 
	 loss: 16.3279, MinusLogProbMetric: 16.3279, val_loss: 17.1188, val_MinusLogProbMetric: 17.1188

Epoch 632: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3279 - MinusLogProbMetric: 16.3279 - val_loss: 17.1188 - val_MinusLogProbMetric: 17.1188 - lr: 2.0833e-05 - 70s/epoch - 355ms/step
Epoch 633/1000
2023-09-28 04:37:27.932 
Epoch 633/1000 
	 loss: 16.3283, MinusLogProbMetric: 16.3283, val_loss: 16.9961, val_MinusLogProbMetric: 16.9961

Epoch 633: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3283 - MinusLogProbMetric: 16.3283 - val_loss: 16.9961 - val_MinusLogProbMetric: 16.9961 - lr: 2.0833e-05 - 70s/epoch - 357ms/step
Epoch 634/1000
2023-09-28 04:38:38.016 
Epoch 634/1000 
	 loss: 16.3316, MinusLogProbMetric: 16.3316, val_loss: 16.9882, val_MinusLogProbMetric: 16.9882

Epoch 634: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3316 - MinusLogProbMetric: 16.3316 - val_loss: 16.9882 - val_MinusLogProbMetric: 16.9882 - lr: 2.0833e-05 - 70s/epoch - 358ms/step
Epoch 635/1000
2023-09-28 04:39:47.783 
Epoch 635/1000 
	 loss: 16.3278, MinusLogProbMetric: 16.3278, val_loss: 17.0374, val_MinusLogProbMetric: 17.0374

Epoch 635: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3278 - MinusLogProbMetric: 16.3278 - val_loss: 17.0374 - val_MinusLogProbMetric: 17.0374 - lr: 2.0833e-05 - 70s/epoch - 356ms/step
Epoch 636/1000
2023-09-28 04:40:58.399 
Epoch 636/1000 
	 loss: 16.3379, MinusLogProbMetric: 16.3379, val_loss: 17.0650, val_MinusLogProbMetric: 17.0650

Epoch 636: val_loss did not improve from 16.98239
196/196 - 71s - loss: 16.3379 - MinusLogProbMetric: 16.3379 - val_loss: 17.0650 - val_MinusLogProbMetric: 17.0650 - lr: 2.0833e-05 - 71s/epoch - 360ms/step
Epoch 637/1000
2023-09-28 04:42:08.647 
Epoch 637/1000 
	 loss: 16.3267, MinusLogProbMetric: 16.3267, val_loss: 17.0038, val_MinusLogProbMetric: 17.0038

Epoch 637: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3267 - MinusLogProbMetric: 16.3267 - val_loss: 17.0038 - val_MinusLogProbMetric: 17.0038 - lr: 2.0833e-05 - 70s/epoch - 358ms/step
Epoch 638/1000
2023-09-28 04:43:17.935 
Epoch 638/1000 
	 loss: 16.3169, MinusLogProbMetric: 16.3169, val_loss: 17.0005, val_MinusLogProbMetric: 17.0005

Epoch 638: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3169 - MinusLogProbMetric: 16.3169 - val_loss: 17.0005 - val_MinusLogProbMetric: 17.0005 - lr: 2.0833e-05 - 69s/epoch - 353ms/step
Epoch 639/1000
2023-09-28 04:44:27.138 
Epoch 639/1000 
	 loss: 16.3205, MinusLogProbMetric: 16.3205, val_loss: 17.0404, val_MinusLogProbMetric: 17.0404

Epoch 639: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3205 - MinusLogProbMetric: 16.3205 - val_loss: 17.0404 - val_MinusLogProbMetric: 17.0404 - lr: 2.0833e-05 - 69s/epoch - 353ms/step
Epoch 640/1000
2023-09-28 04:45:36.126 
Epoch 640/1000 
	 loss: 16.3286, MinusLogProbMetric: 16.3286, val_loss: 17.0456, val_MinusLogProbMetric: 17.0456

Epoch 640: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3286 - MinusLogProbMetric: 16.3286 - val_loss: 17.0456 - val_MinusLogProbMetric: 17.0456 - lr: 2.0833e-05 - 69s/epoch - 352ms/step
Epoch 641/1000
2023-09-28 04:46:45.855 
Epoch 641/1000 
	 loss: 16.3508, MinusLogProbMetric: 16.3508, val_loss: 16.9969, val_MinusLogProbMetric: 16.9969

Epoch 641: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3508 - MinusLogProbMetric: 16.3508 - val_loss: 16.9969 - val_MinusLogProbMetric: 16.9969 - lr: 2.0833e-05 - 70s/epoch - 356ms/step
Epoch 642/1000
2023-09-28 04:47:55.459 
Epoch 642/1000 
	 loss: 16.3341, MinusLogProbMetric: 16.3341, val_loss: 17.0002, val_MinusLogProbMetric: 17.0002

Epoch 642: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3341 - MinusLogProbMetric: 16.3341 - val_loss: 17.0002 - val_MinusLogProbMetric: 17.0002 - lr: 2.0833e-05 - 70s/epoch - 355ms/step
Epoch 643/1000
2023-09-28 04:49:05.088 
Epoch 643/1000 
	 loss: 16.3367, MinusLogProbMetric: 16.3367, val_loss: 17.0237, val_MinusLogProbMetric: 17.0237

Epoch 643: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3367 - MinusLogProbMetric: 16.3367 - val_loss: 17.0237 - val_MinusLogProbMetric: 17.0237 - lr: 2.0833e-05 - 70s/epoch - 355ms/step
Epoch 644/1000
2023-09-28 04:50:14.062 
Epoch 644/1000 
	 loss: 16.3173, MinusLogProbMetric: 16.3173, val_loss: 17.0111, val_MinusLogProbMetric: 17.0111

Epoch 644: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3173 - MinusLogProbMetric: 16.3173 - val_loss: 17.0111 - val_MinusLogProbMetric: 17.0111 - lr: 2.0833e-05 - 69s/epoch - 352ms/step
Epoch 645/1000
2023-09-28 04:51:22.815 
Epoch 645/1000 
	 loss: 16.3221, MinusLogProbMetric: 16.3221, val_loss: 17.0253, val_MinusLogProbMetric: 17.0253

Epoch 645: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3221 - MinusLogProbMetric: 16.3221 - val_loss: 17.0253 - val_MinusLogProbMetric: 17.0253 - lr: 2.0833e-05 - 69s/epoch - 351ms/step
Epoch 646/1000
2023-09-28 04:52:33.230 
Epoch 646/1000 
	 loss: 16.3218, MinusLogProbMetric: 16.3218, val_loss: 17.0308, val_MinusLogProbMetric: 17.0308

Epoch 646: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3218 - MinusLogProbMetric: 16.3218 - val_loss: 17.0308 - val_MinusLogProbMetric: 17.0308 - lr: 2.0833e-05 - 70s/epoch - 359ms/step
Epoch 647/1000
2023-09-28 04:53:42.665 
Epoch 647/1000 
	 loss: 16.3319, MinusLogProbMetric: 16.3319, val_loss: 16.9874, val_MinusLogProbMetric: 16.9874

Epoch 647: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3319 - MinusLogProbMetric: 16.3319 - val_loss: 16.9874 - val_MinusLogProbMetric: 16.9874 - lr: 2.0833e-05 - 69s/epoch - 354ms/step
Epoch 648/1000
2023-09-28 04:54:52.558 
Epoch 648/1000 
	 loss: 16.3165, MinusLogProbMetric: 16.3165, val_loss: 16.9987, val_MinusLogProbMetric: 16.9987

Epoch 648: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3165 - MinusLogProbMetric: 16.3165 - val_loss: 16.9987 - val_MinusLogProbMetric: 16.9987 - lr: 2.0833e-05 - 70s/epoch - 357ms/step
Epoch 649/1000
2023-09-28 04:56:02.348 
Epoch 649/1000 
	 loss: 16.3175, MinusLogProbMetric: 16.3175, val_loss: 16.9946, val_MinusLogProbMetric: 16.9946

Epoch 649: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3175 - MinusLogProbMetric: 16.3175 - val_loss: 16.9946 - val_MinusLogProbMetric: 16.9946 - lr: 2.0833e-05 - 70s/epoch - 356ms/step
Epoch 650/1000
2023-09-28 04:57:11.857 
Epoch 650/1000 
	 loss: 16.3214, MinusLogProbMetric: 16.3214, val_loss: 16.9917, val_MinusLogProbMetric: 16.9917

Epoch 650: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3214 - MinusLogProbMetric: 16.3214 - val_loss: 16.9917 - val_MinusLogProbMetric: 16.9917 - lr: 2.0833e-05 - 70s/epoch - 355ms/step
Epoch 651/1000
2023-09-28 04:58:20.883 
Epoch 651/1000 
	 loss: 16.3183, MinusLogProbMetric: 16.3183, val_loss: 16.9934, val_MinusLogProbMetric: 16.9934

Epoch 651: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3183 - MinusLogProbMetric: 16.3183 - val_loss: 16.9934 - val_MinusLogProbMetric: 16.9934 - lr: 2.0833e-05 - 69s/epoch - 352ms/step
Epoch 652/1000
2023-09-28 04:59:30.275 
Epoch 652/1000 
	 loss: 16.3126, MinusLogProbMetric: 16.3126, val_loss: 16.9874, val_MinusLogProbMetric: 16.9874

Epoch 652: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3126 - MinusLogProbMetric: 16.3126 - val_loss: 16.9874 - val_MinusLogProbMetric: 16.9874 - lr: 2.0833e-05 - 69s/epoch - 354ms/step
Epoch 653/1000
2023-09-28 05:00:40.202 
Epoch 653/1000 
	 loss: 16.3204, MinusLogProbMetric: 16.3204, val_loss: 17.0047, val_MinusLogProbMetric: 17.0047

Epoch 653: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3204 - MinusLogProbMetric: 16.3204 - val_loss: 17.0047 - val_MinusLogProbMetric: 17.0047 - lr: 2.0833e-05 - 70s/epoch - 357ms/step
Epoch 654/1000
2023-09-28 05:01:48.784 
Epoch 654/1000 
	 loss: 16.3317, MinusLogProbMetric: 16.3317, val_loss: 17.0052, val_MinusLogProbMetric: 17.0052

Epoch 654: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3317 - MinusLogProbMetric: 16.3317 - val_loss: 17.0052 - val_MinusLogProbMetric: 17.0052 - lr: 2.0833e-05 - 69s/epoch - 350ms/step
Epoch 655/1000
2023-09-28 05:02:58.199 
Epoch 655/1000 
	 loss: 16.3176, MinusLogProbMetric: 16.3176, val_loss: 17.0527, val_MinusLogProbMetric: 17.0527

Epoch 655: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3176 - MinusLogProbMetric: 16.3176 - val_loss: 17.0527 - val_MinusLogProbMetric: 17.0527 - lr: 2.0833e-05 - 69s/epoch - 354ms/step
Epoch 656/1000
2023-09-28 05:04:08.008 
Epoch 656/1000 
	 loss: 16.3175, MinusLogProbMetric: 16.3175, val_loss: 16.9873, val_MinusLogProbMetric: 16.9873

Epoch 656: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3175 - MinusLogProbMetric: 16.3175 - val_loss: 16.9873 - val_MinusLogProbMetric: 16.9873 - lr: 2.0833e-05 - 70s/epoch - 356ms/step
Epoch 657/1000
2023-09-28 05:05:17.601 
Epoch 657/1000 
	 loss: 16.3186, MinusLogProbMetric: 16.3186, val_loss: 17.0398, val_MinusLogProbMetric: 17.0398

Epoch 657: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3186 - MinusLogProbMetric: 16.3186 - val_loss: 17.0398 - val_MinusLogProbMetric: 17.0398 - lr: 2.0833e-05 - 70s/epoch - 355ms/step
Epoch 658/1000
2023-09-28 05:06:27.431 
Epoch 658/1000 
	 loss: 16.3194, MinusLogProbMetric: 16.3194, val_loss: 17.0126, val_MinusLogProbMetric: 17.0126

Epoch 658: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3194 - MinusLogProbMetric: 16.3194 - val_loss: 17.0126 - val_MinusLogProbMetric: 17.0126 - lr: 2.0833e-05 - 70s/epoch - 356ms/step
Epoch 659/1000
2023-09-28 05:07:36.607 
Epoch 659/1000 
	 loss: 16.3174, MinusLogProbMetric: 16.3174, val_loss: 17.0278, val_MinusLogProbMetric: 17.0278

Epoch 659: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3174 - MinusLogProbMetric: 16.3174 - val_loss: 17.0278 - val_MinusLogProbMetric: 17.0278 - lr: 2.0833e-05 - 69s/epoch - 353ms/step
Epoch 660/1000
2023-09-28 05:08:46.128 
Epoch 660/1000 
	 loss: 16.3145, MinusLogProbMetric: 16.3145, val_loss: 17.0300, val_MinusLogProbMetric: 17.0300

Epoch 660: val_loss did not improve from 16.98239
196/196 - 70s - loss: 16.3145 - MinusLogProbMetric: 16.3145 - val_loss: 17.0300 - val_MinusLogProbMetric: 17.0300 - lr: 2.0833e-05 - 70s/epoch - 355ms/step
Epoch 661/1000
2023-09-28 05:09:55.549 
Epoch 661/1000 
	 loss: 16.3157, MinusLogProbMetric: 16.3157, val_loss: 16.9900, val_MinusLogProbMetric: 16.9900

Epoch 661: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3157 - MinusLogProbMetric: 16.3157 - val_loss: 16.9900 - val_MinusLogProbMetric: 16.9900 - lr: 2.0833e-05 - 69s/epoch - 354ms/step
Epoch 662/1000
2023-09-28 05:11:04.882 
Epoch 662/1000 
	 loss: 16.3169, MinusLogProbMetric: 16.3169, val_loss: 16.9900, val_MinusLogProbMetric: 16.9900

Epoch 662: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3169 - MinusLogProbMetric: 16.3169 - val_loss: 16.9900 - val_MinusLogProbMetric: 16.9900 - lr: 2.0833e-05 - 69s/epoch - 354ms/step
Epoch 663/1000
2023-09-28 05:12:14.075 
Epoch 663/1000 
	 loss: 16.3153, MinusLogProbMetric: 16.3153, val_loss: 17.0000, val_MinusLogProbMetric: 17.0000

Epoch 663: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3153 - MinusLogProbMetric: 16.3153 - val_loss: 17.0000 - val_MinusLogProbMetric: 17.0000 - lr: 2.0833e-05 - 69s/epoch - 353ms/step
Epoch 664/1000
2023-09-28 05:13:23.498 
Epoch 664/1000 
	 loss: 16.3239, MinusLogProbMetric: 16.3239, val_loss: 16.9961, val_MinusLogProbMetric: 16.9961

Epoch 664: val_loss did not improve from 16.98239
196/196 - 69s - loss: 16.3239 - MinusLogProbMetric: 16.3239 - val_loss: 16.9961 - val_MinusLogProbMetric: 16.9961 - lr: 2.0833e-05 - 69s/epoch - 354ms/step
Epoch 665/1000
2023-09-28 05:14:32.587 
Epoch 665/1000 
	 loss: 16.2972, MinusLogProbMetric: 16.2972, val_loss: 16.9813, val_MinusLogProbMetric: 16.9813

Epoch 665: val_loss improved from 16.98239 to 16.98126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 70s - loss: 16.2972 - MinusLogProbMetric: 16.2972 - val_loss: 16.9813 - val_MinusLogProbMetric: 16.9813 - lr: 1.0417e-05 - 70s/epoch - 358ms/step
Epoch 666/1000
2023-09-28 05:15:42.999 
Epoch 666/1000 
	 loss: 16.2940, MinusLogProbMetric: 16.2940, val_loss: 16.9960, val_MinusLogProbMetric: 16.9960

Epoch 666: val_loss did not improve from 16.98126
196/196 - 69s - loss: 16.2940 - MinusLogProbMetric: 16.2940 - val_loss: 16.9960 - val_MinusLogProbMetric: 16.9960 - lr: 1.0417e-05 - 69s/epoch - 354ms/step
Epoch 667/1000
2023-09-28 05:16:52.170 
Epoch 667/1000 
	 loss: 16.2928, MinusLogProbMetric: 16.2928, val_loss: 16.9798, val_MinusLogProbMetric: 16.9798

Epoch 667: val_loss improved from 16.98126 to 16.97979, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 70s - loss: 16.2928 - MinusLogProbMetric: 16.2928 - val_loss: 16.9798 - val_MinusLogProbMetric: 16.9798 - lr: 1.0417e-05 - 70s/epoch - 358ms/step
Epoch 668/1000
2023-09-28 05:18:02.882 
Epoch 668/1000 
	 loss: 16.2944, MinusLogProbMetric: 16.2944, val_loss: 16.9934, val_MinusLogProbMetric: 16.9934

Epoch 668: val_loss did not improve from 16.97979
196/196 - 70s - loss: 16.2944 - MinusLogProbMetric: 16.2944 - val_loss: 16.9934 - val_MinusLogProbMetric: 16.9934 - lr: 1.0417e-05 - 70s/epoch - 356ms/step
Epoch 669/1000
2023-09-28 05:19:12.560 
Epoch 669/1000 
	 loss: 16.2978, MinusLogProbMetric: 16.2978, val_loss: 16.9827, val_MinusLogProbMetric: 16.9827

Epoch 669: val_loss did not improve from 16.97979
196/196 - 70s - loss: 16.2978 - MinusLogProbMetric: 16.2978 - val_loss: 16.9827 - val_MinusLogProbMetric: 16.9827 - lr: 1.0417e-05 - 70s/epoch - 355ms/step
Epoch 670/1000
2023-09-28 05:20:21.841 
Epoch 670/1000 
	 loss: 16.2957, MinusLogProbMetric: 16.2957, val_loss: 16.9851, val_MinusLogProbMetric: 16.9851

Epoch 670: val_loss did not improve from 16.97979
196/196 - 69s - loss: 16.2957 - MinusLogProbMetric: 16.2957 - val_loss: 16.9851 - val_MinusLogProbMetric: 16.9851 - lr: 1.0417e-05 - 69s/epoch - 353ms/step
Epoch 671/1000
2023-09-28 05:21:31.122 
Epoch 671/1000 
	 loss: 16.3010, MinusLogProbMetric: 16.3010, val_loss: 16.9799, val_MinusLogProbMetric: 16.9799

Epoch 671: val_loss did not improve from 16.97979
196/196 - 69s - loss: 16.3010 - MinusLogProbMetric: 16.3010 - val_loss: 16.9799 - val_MinusLogProbMetric: 16.9799 - lr: 1.0417e-05 - 69s/epoch - 353ms/step
Epoch 672/1000
2023-09-28 05:22:40.629 
Epoch 672/1000 
	 loss: 16.2989, MinusLogProbMetric: 16.2989, val_loss: 16.9932, val_MinusLogProbMetric: 16.9932

Epoch 672: val_loss did not improve from 16.97979
196/196 - 70s - loss: 16.2989 - MinusLogProbMetric: 16.2989 - val_loss: 16.9932 - val_MinusLogProbMetric: 16.9932 - lr: 1.0417e-05 - 70s/epoch - 355ms/step
Epoch 673/1000
2023-09-28 05:23:50.298 
Epoch 673/1000 
	 loss: 16.2981, MinusLogProbMetric: 16.2981, val_loss: 16.9953, val_MinusLogProbMetric: 16.9953

Epoch 673: val_loss did not improve from 16.97979
196/196 - 70s - loss: 16.2981 - MinusLogProbMetric: 16.2981 - val_loss: 16.9953 - val_MinusLogProbMetric: 16.9953 - lr: 1.0417e-05 - 70s/epoch - 355ms/step
Epoch 674/1000
2023-09-28 05:24:59.487 
Epoch 674/1000 
	 loss: 16.2916, MinusLogProbMetric: 16.2916, val_loss: 16.9860, val_MinusLogProbMetric: 16.9860

Epoch 674: val_loss did not improve from 16.97979
196/196 - 69s - loss: 16.2916 - MinusLogProbMetric: 16.2916 - val_loss: 16.9860 - val_MinusLogProbMetric: 16.9860 - lr: 1.0417e-05 - 69s/epoch - 353ms/step
Epoch 675/1000
2023-09-28 05:26:08.968 
Epoch 675/1000 
	 loss: 16.2976, MinusLogProbMetric: 16.2976, val_loss: 17.0089, val_MinusLogProbMetric: 17.0089

Epoch 675: val_loss did not improve from 16.97979
196/196 - 69s - loss: 16.2976 - MinusLogProbMetric: 16.2976 - val_loss: 17.0089 - val_MinusLogProbMetric: 17.0089 - lr: 1.0417e-05 - 69s/epoch - 354ms/step
Epoch 676/1000
2023-09-28 05:27:18.083 
Epoch 676/1000 
	 loss: 16.2984, MinusLogProbMetric: 16.2984, val_loss: 16.9952, val_MinusLogProbMetric: 16.9952

Epoch 676: val_loss did not improve from 16.97979
196/196 - 69s - loss: 16.2984 - MinusLogProbMetric: 16.2984 - val_loss: 16.9952 - val_MinusLogProbMetric: 16.9952 - lr: 1.0417e-05 - 69s/epoch - 353ms/step
Epoch 677/1000
2023-09-28 05:28:28.361 
Epoch 677/1000 
	 loss: 16.2970, MinusLogProbMetric: 16.2970, val_loss: 17.0029, val_MinusLogProbMetric: 17.0029

Epoch 677: val_loss did not improve from 16.97979
196/196 - 70s - loss: 16.2970 - MinusLogProbMetric: 16.2970 - val_loss: 17.0029 - val_MinusLogProbMetric: 17.0029 - lr: 1.0417e-05 - 70s/epoch - 359ms/step
Epoch 678/1000
2023-09-28 05:29:38.372 
Epoch 678/1000 
	 loss: 16.2953, MinusLogProbMetric: 16.2953, val_loss: 16.9919, val_MinusLogProbMetric: 16.9919

Epoch 678: val_loss did not improve from 16.97979
196/196 - 70s - loss: 16.2953 - MinusLogProbMetric: 16.2953 - val_loss: 16.9919 - val_MinusLogProbMetric: 16.9919 - lr: 1.0417e-05 - 70s/epoch - 357ms/step
Epoch 679/1000
2023-09-28 05:30:48.697 
Epoch 679/1000 
	 loss: 16.2981, MinusLogProbMetric: 16.2981, val_loss: 16.9874, val_MinusLogProbMetric: 16.9874

Epoch 679: val_loss did not improve from 16.97979
196/196 - 70s - loss: 16.2981 - MinusLogProbMetric: 16.2981 - val_loss: 16.9874 - val_MinusLogProbMetric: 16.9874 - lr: 1.0417e-05 - 70s/epoch - 359ms/step
Epoch 680/1000
2023-09-28 05:31:58.302 
Epoch 680/1000 
	 loss: 16.2928, MinusLogProbMetric: 16.2928, val_loss: 17.0096, val_MinusLogProbMetric: 17.0096

Epoch 680: val_loss did not improve from 16.97979
196/196 - 70s - loss: 16.2928 - MinusLogProbMetric: 16.2928 - val_loss: 17.0096 - val_MinusLogProbMetric: 17.0096 - lr: 1.0417e-05 - 70s/epoch - 355ms/step
Epoch 681/1000
2023-09-28 05:33:07.958 
Epoch 681/1000 
	 loss: 16.2954, MinusLogProbMetric: 16.2954, val_loss: 16.9830, val_MinusLogProbMetric: 16.9830

Epoch 681: val_loss did not improve from 16.97979
196/196 - 70s - loss: 16.2954 - MinusLogProbMetric: 16.2954 - val_loss: 16.9830 - val_MinusLogProbMetric: 16.9830 - lr: 1.0417e-05 - 70s/epoch - 355ms/step
Epoch 682/1000
2023-09-28 05:34:17.737 
Epoch 682/1000 
	 loss: 16.2950, MinusLogProbMetric: 16.2950, val_loss: 16.9863, val_MinusLogProbMetric: 16.9863

Epoch 682: val_loss did not improve from 16.97979
196/196 - 70s - loss: 16.2950 - MinusLogProbMetric: 16.2950 - val_loss: 16.9863 - val_MinusLogProbMetric: 16.9863 - lr: 1.0417e-05 - 70s/epoch - 356ms/step
Epoch 683/1000
2023-09-28 05:35:26.788 
Epoch 683/1000 
	 loss: 16.2960, MinusLogProbMetric: 16.2960, val_loss: 16.9992, val_MinusLogProbMetric: 16.9992

Epoch 683: val_loss did not improve from 16.97979
196/196 - 69s - loss: 16.2960 - MinusLogProbMetric: 16.2960 - val_loss: 16.9992 - val_MinusLogProbMetric: 16.9992 - lr: 1.0417e-05 - 69s/epoch - 352ms/step
Epoch 684/1000
2023-09-28 05:36:36.362 
Epoch 684/1000 
	 loss: 16.2963, MinusLogProbMetric: 16.2963, val_loss: 16.9828, val_MinusLogProbMetric: 16.9828

Epoch 684: val_loss did not improve from 16.97979
196/196 - 70s - loss: 16.2963 - MinusLogProbMetric: 16.2963 - val_loss: 16.9828 - val_MinusLogProbMetric: 16.9828 - lr: 1.0417e-05 - 70s/epoch - 355ms/step
Epoch 685/1000
2023-09-28 05:37:45.206 
Epoch 685/1000 
	 loss: 16.3000, MinusLogProbMetric: 16.3000, val_loss: 17.0001, val_MinusLogProbMetric: 17.0001

Epoch 685: val_loss did not improve from 16.97979
196/196 - 69s - loss: 16.3000 - MinusLogProbMetric: 16.3000 - val_loss: 17.0001 - val_MinusLogProbMetric: 17.0001 - lr: 1.0417e-05 - 69s/epoch - 351ms/step
Epoch 686/1000
2023-09-28 05:38:54.748 
Epoch 686/1000 
	 loss: 16.2975, MinusLogProbMetric: 16.2975, val_loss: 17.0103, val_MinusLogProbMetric: 17.0103

Epoch 686: val_loss did not improve from 16.97979
196/196 - 70s - loss: 16.2975 - MinusLogProbMetric: 16.2975 - val_loss: 17.0103 - val_MinusLogProbMetric: 17.0103 - lr: 1.0417e-05 - 70s/epoch - 355ms/step
Epoch 687/1000
2023-09-28 05:40:03.894 
Epoch 687/1000 
	 loss: 16.3077, MinusLogProbMetric: 16.3077, val_loss: 17.0292, val_MinusLogProbMetric: 17.0292

Epoch 687: val_loss did not improve from 16.97979
196/196 - 69s - loss: 16.3077 - MinusLogProbMetric: 16.3077 - val_loss: 17.0292 - val_MinusLogProbMetric: 17.0292 - lr: 1.0417e-05 - 69s/epoch - 353ms/step
Epoch 688/1000
2023-09-28 05:41:12.664 
Epoch 688/1000 
	 loss: 16.2947, MinusLogProbMetric: 16.2947, val_loss: 16.9835, val_MinusLogProbMetric: 16.9835

Epoch 688: val_loss did not improve from 16.97979
196/196 - 69s - loss: 16.2947 - MinusLogProbMetric: 16.2947 - val_loss: 16.9835 - val_MinusLogProbMetric: 16.9835 - lr: 1.0417e-05 - 69s/epoch - 351ms/step
Epoch 689/1000
2023-09-28 05:42:21.809 
Epoch 689/1000 
	 loss: 16.2920, MinusLogProbMetric: 16.2920, val_loss: 16.9796, val_MinusLogProbMetric: 16.9796

Epoch 689: val_loss improved from 16.97979 to 16.97963, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 70s - loss: 16.2920 - MinusLogProbMetric: 16.2920 - val_loss: 16.9796 - val_MinusLogProbMetric: 16.9796 - lr: 1.0417e-05 - 70s/epoch - 358ms/step
Epoch 690/1000
2023-09-28 05:43:31.661 
Epoch 690/1000 
	 loss: 16.2971, MinusLogProbMetric: 16.2971, val_loss: 16.9875, val_MinusLogProbMetric: 16.9875

Epoch 690: val_loss did not improve from 16.97963
196/196 - 69s - loss: 16.2971 - MinusLogProbMetric: 16.2971 - val_loss: 16.9875 - val_MinusLogProbMetric: 16.9875 - lr: 1.0417e-05 - 69s/epoch - 351ms/step
Epoch 691/1000
2023-09-28 05:44:40.458 
Epoch 691/1000 
	 loss: 16.2919, MinusLogProbMetric: 16.2919, val_loss: 17.0323, val_MinusLogProbMetric: 17.0323

Epoch 691: val_loss did not improve from 16.97963
196/196 - 69s - loss: 16.2919 - MinusLogProbMetric: 16.2919 - val_loss: 17.0323 - val_MinusLogProbMetric: 17.0323 - lr: 1.0417e-05 - 69s/epoch - 351ms/step
Epoch 692/1000
2023-09-28 05:45:49.862 
Epoch 692/1000 
	 loss: 16.2997, MinusLogProbMetric: 16.2997, val_loss: 16.9858, val_MinusLogProbMetric: 16.9858

Epoch 692: val_loss did not improve from 16.97963
196/196 - 69s - loss: 16.2997 - MinusLogProbMetric: 16.2997 - val_loss: 16.9858 - val_MinusLogProbMetric: 16.9858 - lr: 1.0417e-05 - 69s/epoch - 354ms/step
Epoch 693/1000
2023-09-28 05:46:58.697 
Epoch 693/1000 
	 loss: 16.2916, MinusLogProbMetric: 16.2916, val_loss: 17.0052, val_MinusLogProbMetric: 17.0052

Epoch 693: val_loss did not improve from 16.97963
196/196 - 69s - loss: 16.2916 - MinusLogProbMetric: 16.2916 - val_loss: 17.0052 - val_MinusLogProbMetric: 17.0052 - lr: 1.0417e-05 - 69s/epoch - 351ms/step
Epoch 694/1000
2023-09-28 05:48:07.314 
Epoch 694/1000 
	 loss: 16.2962, MinusLogProbMetric: 16.2962, val_loss: 16.9909, val_MinusLogProbMetric: 16.9909

Epoch 694: val_loss did not improve from 16.97963
196/196 - 69s - loss: 16.2962 - MinusLogProbMetric: 16.2962 - val_loss: 16.9909 - val_MinusLogProbMetric: 16.9909 - lr: 1.0417e-05 - 69s/epoch - 350ms/step
Epoch 695/1000
2023-09-28 05:49:17.175 
Epoch 695/1000 
	 loss: 16.2946, MinusLogProbMetric: 16.2946, val_loss: 16.9767, val_MinusLogProbMetric: 16.9767

Epoch 695: val_loss improved from 16.97963 to 16.97671, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 71s - loss: 16.2946 - MinusLogProbMetric: 16.2946 - val_loss: 16.9767 - val_MinusLogProbMetric: 16.9767 - lr: 1.0417e-05 - 71s/epoch - 362ms/step
Epoch 696/1000
2023-09-28 05:50:27.541 
Epoch 696/1000 
	 loss: 16.2926, MinusLogProbMetric: 16.2926, val_loss: 16.9832, val_MinusLogProbMetric: 16.9832

Epoch 696: val_loss did not improve from 16.97671
196/196 - 69s - loss: 16.2926 - MinusLogProbMetric: 16.2926 - val_loss: 16.9832 - val_MinusLogProbMetric: 16.9832 - lr: 1.0417e-05 - 69s/epoch - 353ms/step
Epoch 697/1000
2023-09-28 05:51:36.559 
Epoch 697/1000 
	 loss: 16.3006, MinusLogProbMetric: 16.3006, val_loss: 16.9898, val_MinusLogProbMetric: 16.9898

Epoch 697: val_loss did not improve from 16.97671
196/196 - 69s - loss: 16.3006 - MinusLogProbMetric: 16.3006 - val_loss: 16.9898 - val_MinusLogProbMetric: 16.9898 - lr: 1.0417e-05 - 69s/epoch - 352ms/step
Epoch 698/1000
2023-09-28 05:52:46.507 
Epoch 698/1000 
	 loss: 16.2927, MinusLogProbMetric: 16.2927, val_loss: 17.0027, val_MinusLogProbMetric: 17.0027

Epoch 698: val_loss did not improve from 16.97671
196/196 - 70s - loss: 16.2927 - MinusLogProbMetric: 16.2927 - val_loss: 17.0027 - val_MinusLogProbMetric: 17.0027 - lr: 1.0417e-05 - 70s/epoch - 357ms/step
Epoch 699/1000
2023-09-28 05:53:56.029 
Epoch 699/1000 
	 loss: 16.2932, MinusLogProbMetric: 16.2932, val_loss: 16.9957, val_MinusLogProbMetric: 16.9957

Epoch 699: val_loss did not improve from 16.97671
196/196 - 70s - loss: 16.2932 - MinusLogProbMetric: 16.2932 - val_loss: 16.9957 - val_MinusLogProbMetric: 16.9957 - lr: 1.0417e-05 - 70s/epoch - 355ms/step
Epoch 700/1000
2023-09-28 05:55:05.410 
Epoch 700/1000 
	 loss: 16.2930, MinusLogProbMetric: 16.2930, val_loss: 16.9991, val_MinusLogProbMetric: 16.9991

Epoch 700: val_loss did not improve from 16.97671
196/196 - 69s - loss: 16.2930 - MinusLogProbMetric: 16.2930 - val_loss: 16.9991 - val_MinusLogProbMetric: 16.9991 - lr: 1.0417e-05 - 69s/epoch - 354ms/step
Epoch 701/1000
2023-09-28 05:56:14.434 
Epoch 701/1000 
	 loss: 16.2926, MinusLogProbMetric: 16.2926, val_loss: 17.0034, val_MinusLogProbMetric: 17.0034

Epoch 701: val_loss did not improve from 16.97671
196/196 - 69s - loss: 16.2926 - MinusLogProbMetric: 16.2926 - val_loss: 17.0034 - val_MinusLogProbMetric: 17.0034 - lr: 1.0417e-05 - 69s/epoch - 352ms/step
Epoch 702/1000
2023-09-28 05:57:23.858 
Epoch 702/1000 
	 loss: 16.3030, MinusLogProbMetric: 16.3030, val_loss: 17.0074, val_MinusLogProbMetric: 17.0074

Epoch 702: val_loss did not improve from 16.97671
196/196 - 69s - loss: 16.3030 - MinusLogProbMetric: 16.3030 - val_loss: 17.0074 - val_MinusLogProbMetric: 17.0074 - lr: 1.0417e-05 - 69s/epoch - 354ms/step
Epoch 703/1000
2023-09-28 05:58:33.564 
Epoch 703/1000 
	 loss: 16.2924, MinusLogProbMetric: 16.2924, val_loss: 16.9904, val_MinusLogProbMetric: 16.9904

Epoch 703: val_loss did not improve from 16.97671
196/196 - 70s - loss: 16.2924 - MinusLogProbMetric: 16.2924 - val_loss: 16.9904 - val_MinusLogProbMetric: 16.9904 - lr: 1.0417e-05 - 70s/epoch - 356ms/step
Epoch 704/1000
2023-09-28 05:59:43.020 
Epoch 704/1000 
	 loss: 16.2903, MinusLogProbMetric: 16.2903, val_loss: 16.9910, val_MinusLogProbMetric: 16.9910

Epoch 704: val_loss did not improve from 16.97671
196/196 - 69s - loss: 16.2903 - MinusLogProbMetric: 16.2903 - val_loss: 16.9910 - val_MinusLogProbMetric: 16.9910 - lr: 1.0417e-05 - 69s/epoch - 354ms/step
Epoch 705/1000
2023-09-28 06:00:52.127 
Epoch 705/1000 
	 loss: 16.2993, MinusLogProbMetric: 16.2993, val_loss: 16.9903, val_MinusLogProbMetric: 16.9903

Epoch 705: val_loss did not improve from 16.97671
196/196 - 69s - loss: 16.2993 - MinusLogProbMetric: 16.2993 - val_loss: 16.9903 - val_MinusLogProbMetric: 16.9903 - lr: 1.0417e-05 - 69s/epoch - 353ms/step
Epoch 706/1000
2023-09-28 06:02:01.574 
Epoch 706/1000 
	 loss: 16.2930, MinusLogProbMetric: 16.2930, val_loss: 16.9949, val_MinusLogProbMetric: 16.9949

Epoch 706: val_loss did not improve from 16.97671
196/196 - 69s - loss: 16.2930 - MinusLogProbMetric: 16.2930 - val_loss: 16.9949 - val_MinusLogProbMetric: 16.9949 - lr: 1.0417e-05 - 69s/epoch - 354ms/step
Epoch 707/1000
2023-09-28 06:03:10.779 
Epoch 707/1000 
	 loss: 16.2922, MinusLogProbMetric: 16.2922, val_loss: 16.9936, val_MinusLogProbMetric: 16.9936

Epoch 707: val_loss did not improve from 16.97671
196/196 - 69s - loss: 16.2922 - MinusLogProbMetric: 16.2922 - val_loss: 16.9936 - val_MinusLogProbMetric: 16.9936 - lr: 1.0417e-05 - 69s/epoch - 353ms/step
Epoch 708/1000
2023-09-28 06:04:20.320 
Epoch 708/1000 
	 loss: 16.2924, MinusLogProbMetric: 16.2924, val_loss: 17.0073, val_MinusLogProbMetric: 17.0073

Epoch 708: val_loss did not improve from 16.97671
196/196 - 70s - loss: 16.2924 - MinusLogProbMetric: 16.2924 - val_loss: 17.0073 - val_MinusLogProbMetric: 17.0073 - lr: 1.0417e-05 - 70s/epoch - 355ms/step
Epoch 709/1000
2023-09-28 06:05:30.119 
Epoch 709/1000 
	 loss: 16.2949, MinusLogProbMetric: 16.2949, val_loss: 17.0084, val_MinusLogProbMetric: 17.0084

Epoch 709: val_loss did not improve from 16.97671
196/196 - 70s - loss: 16.2949 - MinusLogProbMetric: 16.2949 - val_loss: 17.0084 - val_MinusLogProbMetric: 17.0084 - lr: 1.0417e-05 - 70s/epoch - 356ms/step
Epoch 710/1000
2023-09-28 06:06:40.078 
Epoch 710/1000 
	 loss: 16.2941, MinusLogProbMetric: 16.2941, val_loss: 17.0001, val_MinusLogProbMetric: 17.0001

Epoch 710: val_loss did not improve from 16.97671
196/196 - 70s - loss: 16.2941 - MinusLogProbMetric: 16.2941 - val_loss: 17.0001 - val_MinusLogProbMetric: 17.0001 - lr: 1.0417e-05 - 70s/epoch - 357ms/step
Epoch 711/1000
2023-09-28 06:07:49.377 
Epoch 711/1000 
	 loss: 16.2992, MinusLogProbMetric: 16.2992, val_loss: 16.9838, val_MinusLogProbMetric: 16.9838

Epoch 711: val_loss did not improve from 16.97671
196/196 - 69s - loss: 16.2992 - MinusLogProbMetric: 16.2992 - val_loss: 16.9838 - val_MinusLogProbMetric: 16.9838 - lr: 1.0417e-05 - 69s/epoch - 354ms/step
Epoch 712/1000
2023-09-28 06:08:59.080 
Epoch 712/1000 
	 loss: 16.2895, MinusLogProbMetric: 16.2895, val_loss: 16.9869, val_MinusLogProbMetric: 16.9869

Epoch 712: val_loss did not improve from 16.97671
196/196 - 70s - loss: 16.2895 - MinusLogProbMetric: 16.2895 - val_loss: 16.9869 - val_MinusLogProbMetric: 16.9869 - lr: 1.0417e-05 - 70s/epoch - 356ms/step
Epoch 713/1000
2023-09-28 06:10:09.253 
Epoch 713/1000 
	 loss: 16.2957, MinusLogProbMetric: 16.2957, val_loss: 16.9879, val_MinusLogProbMetric: 16.9879

Epoch 713: val_loss did not improve from 16.97671
196/196 - 70s - loss: 16.2957 - MinusLogProbMetric: 16.2957 - val_loss: 16.9879 - val_MinusLogProbMetric: 16.9879 - lr: 1.0417e-05 - 70s/epoch - 358ms/step
Epoch 714/1000
2023-09-28 06:11:18.332 
Epoch 714/1000 
	 loss: 16.2954, MinusLogProbMetric: 16.2954, val_loss: 16.9765, val_MinusLogProbMetric: 16.9765

Epoch 714: val_loss improved from 16.97671 to 16.97647, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 70s - loss: 16.2954 - MinusLogProbMetric: 16.2954 - val_loss: 16.9765 - val_MinusLogProbMetric: 16.9765 - lr: 1.0417e-05 - 70s/epoch - 358ms/step
Epoch 715/1000
2023-09-28 06:12:28.409 
Epoch 715/1000 
	 loss: 16.2943, MinusLogProbMetric: 16.2943, val_loss: 17.0194, val_MinusLogProbMetric: 17.0194

Epoch 715: val_loss did not improve from 16.97647
196/196 - 69s - loss: 16.2943 - MinusLogProbMetric: 16.2943 - val_loss: 17.0194 - val_MinusLogProbMetric: 17.0194 - lr: 1.0417e-05 - 69s/epoch - 352ms/step
Epoch 716/1000
2023-09-28 06:13:38.215 
Epoch 716/1000 
	 loss: 16.2945, MinusLogProbMetric: 16.2945, val_loss: 17.0125, val_MinusLogProbMetric: 17.0125

Epoch 716: val_loss did not improve from 16.97647
196/196 - 70s - loss: 16.2945 - MinusLogProbMetric: 16.2945 - val_loss: 17.0125 - val_MinusLogProbMetric: 17.0125 - lr: 1.0417e-05 - 70s/epoch - 356ms/step
Epoch 717/1000
2023-09-28 06:14:47.843 
Epoch 717/1000 
	 loss: 16.2935, MinusLogProbMetric: 16.2935, val_loss: 16.9923, val_MinusLogProbMetric: 16.9923

Epoch 717: val_loss did not improve from 16.97647
196/196 - 70s - loss: 16.2935 - MinusLogProbMetric: 16.2935 - val_loss: 16.9923 - val_MinusLogProbMetric: 16.9923 - lr: 1.0417e-05 - 70s/epoch - 355ms/step
Epoch 718/1000
2023-09-28 06:15:57.239 
Epoch 718/1000 
	 loss: 16.2940, MinusLogProbMetric: 16.2940, val_loss: 16.9789, val_MinusLogProbMetric: 16.9789

Epoch 718: val_loss did not improve from 16.97647
196/196 - 69s - loss: 16.2940 - MinusLogProbMetric: 16.2940 - val_loss: 16.9789 - val_MinusLogProbMetric: 16.9789 - lr: 1.0417e-05 - 69s/epoch - 354ms/step
Epoch 719/1000
2023-09-28 06:17:07.197 
Epoch 719/1000 
	 loss: 16.2942, MinusLogProbMetric: 16.2942, val_loss: 16.9912, val_MinusLogProbMetric: 16.9912

Epoch 719: val_loss did not improve from 16.97647
196/196 - 70s - loss: 16.2942 - MinusLogProbMetric: 16.2942 - val_loss: 16.9912 - val_MinusLogProbMetric: 16.9912 - lr: 1.0417e-05 - 70s/epoch - 357ms/step
Epoch 720/1000
2023-09-28 06:18:16.879 
Epoch 720/1000 
	 loss: 16.2923, MinusLogProbMetric: 16.2923, val_loss: 16.9780, val_MinusLogProbMetric: 16.9780

Epoch 720: val_loss did not improve from 16.97647
196/196 - 70s - loss: 16.2923 - MinusLogProbMetric: 16.2923 - val_loss: 16.9780 - val_MinusLogProbMetric: 16.9780 - lr: 1.0417e-05 - 70s/epoch - 355ms/step
Epoch 721/1000
2023-09-28 06:19:26.327 
Epoch 721/1000 
	 loss: 16.2901, MinusLogProbMetric: 16.2901, val_loss: 16.9930, val_MinusLogProbMetric: 16.9930

Epoch 721: val_loss did not improve from 16.97647
196/196 - 69s - loss: 16.2901 - MinusLogProbMetric: 16.2901 - val_loss: 16.9930 - val_MinusLogProbMetric: 16.9930 - lr: 1.0417e-05 - 69s/epoch - 354ms/step
Epoch 722/1000
2023-09-28 06:20:35.883 
Epoch 722/1000 
	 loss: 16.2893, MinusLogProbMetric: 16.2893, val_loss: 16.9810, val_MinusLogProbMetric: 16.9810

Epoch 722: val_loss did not improve from 16.97647
196/196 - 70s - loss: 16.2893 - MinusLogProbMetric: 16.2893 - val_loss: 16.9810 - val_MinusLogProbMetric: 16.9810 - lr: 1.0417e-05 - 70s/epoch - 355ms/step
Epoch 723/1000
2023-09-28 06:21:44.916 
Epoch 723/1000 
	 loss: 16.2868, MinusLogProbMetric: 16.2868, val_loss: 16.9917, val_MinusLogProbMetric: 16.9917

Epoch 723: val_loss did not improve from 16.97647
196/196 - 69s - loss: 16.2868 - MinusLogProbMetric: 16.2868 - val_loss: 16.9917 - val_MinusLogProbMetric: 16.9917 - lr: 1.0417e-05 - 69s/epoch - 352ms/step
Epoch 724/1000
2023-09-28 06:22:54.472 
Epoch 724/1000 
	 loss: 16.2889, MinusLogProbMetric: 16.2889, val_loss: 16.9856, val_MinusLogProbMetric: 16.9856

Epoch 724: val_loss did not improve from 16.97647
196/196 - 70s - loss: 16.2889 - MinusLogProbMetric: 16.2889 - val_loss: 16.9856 - val_MinusLogProbMetric: 16.9856 - lr: 1.0417e-05 - 70s/epoch - 355ms/step
Epoch 725/1000
2023-09-28 06:24:04.135 
Epoch 725/1000 
	 loss: 16.2919, MinusLogProbMetric: 16.2919, val_loss: 16.9827, val_MinusLogProbMetric: 16.9827

Epoch 725: val_loss did not improve from 16.97647
196/196 - 70s - loss: 16.2919 - MinusLogProbMetric: 16.2919 - val_loss: 16.9827 - val_MinusLogProbMetric: 16.9827 - lr: 1.0417e-05 - 70s/epoch - 355ms/step
Epoch 726/1000
2023-09-28 06:25:13.683 
Epoch 726/1000 
	 loss: 16.2912, MinusLogProbMetric: 16.2912, val_loss: 16.9987, val_MinusLogProbMetric: 16.9987

Epoch 726: val_loss did not improve from 16.97647
196/196 - 70s - loss: 16.2912 - MinusLogProbMetric: 16.2912 - val_loss: 16.9987 - val_MinusLogProbMetric: 16.9987 - lr: 1.0417e-05 - 70s/epoch - 355ms/step
Epoch 727/1000
2023-09-28 06:26:18.768 
Epoch 727/1000 
	 loss: 16.2901, MinusLogProbMetric: 16.2901, val_loss: 16.9854, val_MinusLogProbMetric: 16.9854

Epoch 727: val_loss did not improve from 16.97647
196/196 - 65s - loss: 16.2901 - MinusLogProbMetric: 16.2901 - val_loss: 16.9854 - val_MinusLogProbMetric: 16.9854 - lr: 1.0417e-05 - 65s/epoch - 332ms/step
Epoch 728/1000
2023-09-28 06:27:21.621 
Epoch 728/1000 
	 loss: 16.2927, MinusLogProbMetric: 16.2927, val_loss: 16.9970, val_MinusLogProbMetric: 16.9970

Epoch 728: val_loss did not improve from 16.97647
196/196 - 63s - loss: 16.2927 - MinusLogProbMetric: 16.2927 - val_loss: 16.9970 - val_MinusLogProbMetric: 16.9970 - lr: 1.0417e-05 - 63s/epoch - 321ms/step
Epoch 729/1000
2023-09-28 06:28:15.481 
Epoch 729/1000 
	 loss: 16.2869, MinusLogProbMetric: 16.2869, val_loss: 16.9815, val_MinusLogProbMetric: 16.9815

Epoch 729: val_loss did not improve from 16.97647
196/196 - 54s - loss: 16.2869 - MinusLogProbMetric: 16.2869 - val_loss: 16.9815 - val_MinusLogProbMetric: 16.9815 - lr: 1.0417e-05 - 54s/epoch - 275ms/step
Epoch 730/1000
2023-09-28 06:29:10.978 
Epoch 730/1000 
	 loss: 16.2865, MinusLogProbMetric: 16.2865, val_loss: 16.9765, val_MinusLogProbMetric: 16.9765

Epoch 730: val_loss did not improve from 16.97647
196/196 - 55s - loss: 16.2865 - MinusLogProbMetric: 16.2865 - val_loss: 16.9765 - val_MinusLogProbMetric: 16.9765 - lr: 1.0417e-05 - 55s/epoch - 283ms/step
Epoch 731/1000
2023-09-28 06:30:17.546 
Epoch 731/1000 
	 loss: 16.2903, MinusLogProbMetric: 16.2903, val_loss: 16.9810, val_MinusLogProbMetric: 16.9810

Epoch 731: val_loss did not improve from 16.97647
196/196 - 67s - loss: 16.2903 - MinusLogProbMetric: 16.2903 - val_loss: 16.9810 - val_MinusLogProbMetric: 16.9810 - lr: 1.0417e-05 - 67s/epoch - 340ms/step
Epoch 732/1000
2023-09-28 06:31:14.303 
Epoch 732/1000 
	 loss: 16.2889, MinusLogProbMetric: 16.2889, val_loss: 17.0138, val_MinusLogProbMetric: 17.0138

Epoch 732: val_loss did not improve from 16.97647
196/196 - 57s - loss: 16.2889 - MinusLogProbMetric: 16.2889 - val_loss: 17.0138 - val_MinusLogProbMetric: 17.0138 - lr: 1.0417e-05 - 57s/epoch - 290ms/step
Epoch 733/1000
2023-09-28 06:32:09.271 
Epoch 733/1000 
	 loss: 16.2927, MinusLogProbMetric: 16.2927, val_loss: 17.0135, val_MinusLogProbMetric: 17.0135

Epoch 733: val_loss did not improve from 16.97647
196/196 - 55s - loss: 16.2927 - MinusLogProbMetric: 16.2927 - val_loss: 17.0135 - val_MinusLogProbMetric: 17.0135 - lr: 1.0417e-05 - 55s/epoch - 280ms/step
Epoch 734/1000
2023-09-28 06:33:16.186 
Epoch 734/1000 
	 loss: 16.2878, MinusLogProbMetric: 16.2878, val_loss: 16.9779, val_MinusLogProbMetric: 16.9779

Epoch 734: val_loss did not improve from 16.97647
196/196 - 67s - loss: 16.2878 - MinusLogProbMetric: 16.2878 - val_loss: 16.9779 - val_MinusLogProbMetric: 16.9779 - lr: 1.0417e-05 - 67s/epoch - 341ms/step
Epoch 735/1000
2023-09-28 06:34:15.124 
Epoch 735/1000 
	 loss: 16.2875, MinusLogProbMetric: 16.2875, val_loss: 16.9892, val_MinusLogProbMetric: 16.9892

Epoch 735: val_loss did not improve from 16.97647
196/196 - 59s - loss: 16.2875 - MinusLogProbMetric: 16.2875 - val_loss: 16.9892 - val_MinusLogProbMetric: 16.9892 - lr: 1.0417e-05 - 59s/epoch - 301ms/step
Epoch 736/1000
2023-09-28 06:35:10.408 
Epoch 736/1000 
	 loss: 16.2858, MinusLogProbMetric: 16.2858, val_loss: 16.9799, val_MinusLogProbMetric: 16.9799

Epoch 736: val_loss did not improve from 16.97647
196/196 - 55s - loss: 16.2858 - MinusLogProbMetric: 16.2858 - val_loss: 16.9799 - val_MinusLogProbMetric: 16.9799 - lr: 1.0417e-05 - 55s/epoch - 282ms/step
Epoch 737/1000
2023-09-28 06:36:12.665 
Epoch 737/1000 
	 loss: 16.2928, MinusLogProbMetric: 16.2928, val_loss: 16.9870, val_MinusLogProbMetric: 16.9870

Epoch 737: val_loss did not improve from 16.97647
196/196 - 62s - loss: 16.2928 - MinusLogProbMetric: 16.2928 - val_loss: 16.9870 - val_MinusLogProbMetric: 16.9870 - lr: 1.0417e-05 - 62s/epoch - 318ms/step
Epoch 738/1000
2023-09-28 06:37:13.439 
Epoch 738/1000 
	 loss: 16.2883, MinusLogProbMetric: 16.2883, val_loss: 16.9869, val_MinusLogProbMetric: 16.9869

Epoch 738: val_loss did not improve from 16.97647
196/196 - 61s - loss: 16.2883 - MinusLogProbMetric: 16.2883 - val_loss: 16.9869 - val_MinusLogProbMetric: 16.9869 - lr: 1.0417e-05 - 61s/epoch - 310ms/step
Epoch 739/1000
2023-09-28 06:38:08.382 
Epoch 739/1000 
	 loss: 16.2867, MinusLogProbMetric: 16.2867, val_loss: 16.9911, val_MinusLogProbMetric: 16.9911

Epoch 739: val_loss did not improve from 16.97647
196/196 - 55s - loss: 16.2867 - MinusLogProbMetric: 16.2867 - val_loss: 16.9911 - val_MinusLogProbMetric: 16.9911 - lr: 1.0417e-05 - 55s/epoch - 280ms/step
Epoch 740/1000
2023-09-28 06:39:09.671 
Epoch 740/1000 
	 loss: 16.2885, MinusLogProbMetric: 16.2885, val_loss: 16.9778, val_MinusLogProbMetric: 16.9778

Epoch 740: val_loss did not improve from 16.97647
196/196 - 61s - loss: 16.2885 - MinusLogProbMetric: 16.2885 - val_loss: 16.9778 - val_MinusLogProbMetric: 16.9778 - lr: 1.0417e-05 - 61s/epoch - 313ms/step
Epoch 741/1000
2023-09-28 06:40:10.812 
Epoch 741/1000 
	 loss: 16.2913, MinusLogProbMetric: 16.2913, val_loss: 17.0071, val_MinusLogProbMetric: 17.0071

Epoch 741: val_loss did not improve from 16.97647
196/196 - 61s - loss: 16.2913 - MinusLogProbMetric: 16.2913 - val_loss: 17.0071 - val_MinusLogProbMetric: 17.0071 - lr: 1.0417e-05 - 61s/epoch - 312ms/step
Epoch 742/1000
2023-09-28 06:41:05.163 
Epoch 742/1000 
	 loss: 16.2922, MinusLogProbMetric: 16.2922, val_loss: 17.0096, val_MinusLogProbMetric: 17.0096

Epoch 742: val_loss did not improve from 16.97647
196/196 - 54s - loss: 16.2922 - MinusLogProbMetric: 16.2922 - val_loss: 17.0096 - val_MinusLogProbMetric: 17.0096 - lr: 1.0417e-05 - 54s/epoch - 277ms/step
Epoch 743/1000
2023-09-28 06:42:05.187 
Epoch 743/1000 
	 loss: 16.2916, MinusLogProbMetric: 16.2916, val_loss: 17.0182, val_MinusLogProbMetric: 17.0182

Epoch 743: val_loss did not improve from 16.97647
196/196 - 60s - loss: 16.2916 - MinusLogProbMetric: 16.2916 - val_loss: 17.0182 - val_MinusLogProbMetric: 17.0182 - lr: 1.0417e-05 - 60s/epoch - 306ms/step
Epoch 744/1000
2023-09-28 06:43:07.752 
Epoch 744/1000 
	 loss: 16.2868, MinusLogProbMetric: 16.2868, val_loss: 16.9854, val_MinusLogProbMetric: 16.9854

Epoch 744: val_loss did not improve from 16.97647
196/196 - 63s - loss: 16.2868 - MinusLogProbMetric: 16.2868 - val_loss: 16.9854 - val_MinusLogProbMetric: 16.9854 - lr: 1.0417e-05 - 63s/epoch - 319ms/step
Epoch 745/1000
2023-09-28 06:44:02.190 
Epoch 745/1000 
	 loss: 16.2905, MinusLogProbMetric: 16.2905, val_loss: 17.0520, val_MinusLogProbMetric: 17.0520

Epoch 745: val_loss did not improve from 16.97647
196/196 - 54s - loss: 16.2905 - MinusLogProbMetric: 16.2905 - val_loss: 17.0520 - val_MinusLogProbMetric: 17.0520 - lr: 1.0417e-05 - 54s/epoch - 278ms/step
Epoch 746/1000
2023-09-28 06:45:01.490 
Epoch 746/1000 
	 loss: 16.2947, MinusLogProbMetric: 16.2947, val_loss: 16.9914, val_MinusLogProbMetric: 16.9914

Epoch 746: val_loss did not improve from 16.97647
196/196 - 59s - loss: 16.2947 - MinusLogProbMetric: 16.2947 - val_loss: 16.9914 - val_MinusLogProbMetric: 16.9914 - lr: 1.0417e-05 - 59s/epoch - 303ms/step
Epoch 747/1000
2023-09-28 06:46:08.171 
Epoch 747/1000 
	 loss: 16.2884, MinusLogProbMetric: 16.2884, val_loss: 16.9887, val_MinusLogProbMetric: 16.9887

Epoch 747: val_loss did not improve from 16.97647
196/196 - 67s - loss: 16.2884 - MinusLogProbMetric: 16.2884 - val_loss: 16.9887 - val_MinusLogProbMetric: 16.9887 - lr: 1.0417e-05 - 67s/epoch - 340ms/step
Epoch 748/1000
2023-09-28 06:47:03.364 
Epoch 748/1000 
	 loss: 16.2911, MinusLogProbMetric: 16.2911, val_loss: 16.9760, val_MinusLogProbMetric: 16.9760

Epoch 748: val_loss improved from 16.97647 to 16.97600, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 56s - loss: 16.2911 - MinusLogProbMetric: 16.2911 - val_loss: 16.9760 - val_MinusLogProbMetric: 16.9760 - lr: 1.0417e-05 - 56s/epoch - 286ms/step
Epoch 749/1000
2023-09-28 06:48:00.622 
Epoch 749/1000 
	 loss: 16.2933, MinusLogProbMetric: 16.2933, val_loss: 16.9882, val_MinusLogProbMetric: 16.9882

Epoch 749: val_loss did not improve from 16.97600
196/196 - 56s - loss: 16.2933 - MinusLogProbMetric: 16.2933 - val_loss: 16.9882 - val_MinusLogProbMetric: 16.9882 - lr: 1.0417e-05 - 56s/epoch - 288ms/step
Epoch 750/1000
2023-09-28 06:49:06.619 
Epoch 750/1000 
	 loss: 16.2847, MinusLogProbMetric: 16.2847, val_loss: 16.9845, val_MinusLogProbMetric: 16.9845

Epoch 750: val_loss did not improve from 16.97600
196/196 - 66s - loss: 16.2847 - MinusLogProbMetric: 16.2847 - val_loss: 16.9845 - val_MinusLogProbMetric: 16.9845 - lr: 1.0417e-05 - 66s/epoch - 337ms/step
Epoch 751/1000
2023-09-28 06:50:03.702 
Epoch 751/1000 
	 loss: 16.2872, MinusLogProbMetric: 16.2872, val_loss: 16.9852, val_MinusLogProbMetric: 16.9852

Epoch 751: val_loss did not improve from 16.97600
196/196 - 57s - loss: 16.2872 - MinusLogProbMetric: 16.2872 - val_loss: 16.9852 - val_MinusLogProbMetric: 16.9852 - lr: 1.0417e-05 - 57s/epoch - 291ms/step
Epoch 752/1000
2023-09-28 06:50:58.964 
Epoch 752/1000 
	 loss: 16.2921, MinusLogProbMetric: 16.2921, val_loss: 17.0033, val_MinusLogProbMetric: 17.0033

Epoch 752: val_loss did not improve from 16.97600
196/196 - 55s - loss: 16.2921 - MinusLogProbMetric: 16.2921 - val_loss: 17.0033 - val_MinusLogProbMetric: 17.0033 - lr: 1.0417e-05 - 55s/epoch - 282ms/step
Epoch 753/1000
2023-09-28 06:52:03.804 
Epoch 753/1000 
	 loss: 16.2894, MinusLogProbMetric: 16.2894, val_loss: 16.9956, val_MinusLogProbMetric: 16.9956

Epoch 753: val_loss did not improve from 16.97600
196/196 - 65s - loss: 16.2894 - MinusLogProbMetric: 16.2894 - val_loss: 16.9956 - val_MinusLogProbMetric: 16.9956 - lr: 1.0417e-05 - 65s/epoch - 331ms/step
Epoch 754/1000
2023-09-28 06:53:07.181 
Epoch 754/1000 
	 loss: 16.2956, MinusLogProbMetric: 16.2956, val_loss: 16.9937, val_MinusLogProbMetric: 16.9937

Epoch 754: val_loss did not improve from 16.97600
196/196 - 63s - loss: 16.2956 - MinusLogProbMetric: 16.2956 - val_loss: 16.9937 - val_MinusLogProbMetric: 16.9937 - lr: 1.0417e-05 - 63s/epoch - 323ms/step
Epoch 755/1000
2023-09-28 06:54:01.891 
Epoch 755/1000 
	 loss: 16.2950, MinusLogProbMetric: 16.2950, val_loss: 16.9888, val_MinusLogProbMetric: 16.9888

Epoch 755: val_loss did not improve from 16.97600
196/196 - 55s - loss: 16.2950 - MinusLogProbMetric: 16.2950 - val_loss: 16.9888 - val_MinusLogProbMetric: 16.9888 - lr: 1.0417e-05 - 55s/epoch - 279ms/step
Epoch 756/1000
2023-09-28 06:55:00.495 
Epoch 756/1000 
	 loss: 16.2868, MinusLogProbMetric: 16.2868, val_loss: 16.9975, val_MinusLogProbMetric: 16.9975

Epoch 756: val_loss did not improve from 16.97600
196/196 - 59s - loss: 16.2868 - MinusLogProbMetric: 16.2868 - val_loss: 16.9975 - val_MinusLogProbMetric: 16.9975 - lr: 1.0417e-05 - 59s/epoch - 299ms/step
Epoch 757/1000
2023-09-28 06:56:05.450 
Epoch 757/1000 
	 loss: 16.2880, MinusLogProbMetric: 16.2880, val_loss: 16.9794, val_MinusLogProbMetric: 16.9794

Epoch 757: val_loss did not improve from 16.97600
196/196 - 65s - loss: 16.2880 - MinusLogProbMetric: 16.2880 - val_loss: 16.9794 - val_MinusLogProbMetric: 16.9794 - lr: 1.0417e-05 - 65s/epoch - 331ms/step
Epoch 758/1000
2023-09-28 06:56:59.974 
Epoch 758/1000 
	 loss: 16.2919, MinusLogProbMetric: 16.2919, val_loss: 17.0269, val_MinusLogProbMetric: 17.0269

Epoch 758: val_loss did not improve from 16.97600
196/196 - 55s - loss: 16.2919 - MinusLogProbMetric: 16.2919 - val_loss: 17.0269 - val_MinusLogProbMetric: 17.0269 - lr: 1.0417e-05 - 55s/epoch - 278ms/step
Epoch 759/1000
2023-09-28 06:57:55.771 
Epoch 759/1000 
	 loss: 16.2913, MinusLogProbMetric: 16.2913, val_loss: 17.0046, val_MinusLogProbMetric: 17.0046

Epoch 759: val_loss did not improve from 16.97600
196/196 - 56s - loss: 16.2913 - MinusLogProbMetric: 16.2913 - val_loss: 17.0046 - val_MinusLogProbMetric: 17.0046 - lr: 1.0417e-05 - 56s/epoch - 285ms/step
Epoch 760/1000
2023-09-28 06:59:00.994 
Epoch 760/1000 
	 loss: 16.2904, MinusLogProbMetric: 16.2904, val_loss: 16.9854, val_MinusLogProbMetric: 16.9854

Epoch 760: val_loss did not improve from 16.97600
196/196 - 65s - loss: 16.2904 - MinusLogProbMetric: 16.2904 - val_loss: 16.9854 - val_MinusLogProbMetric: 16.9854 - lr: 1.0417e-05 - 65s/epoch - 333ms/step
Epoch 761/1000
2023-09-28 06:59:58.757 
Epoch 761/1000 
	 loss: 16.2881, MinusLogProbMetric: 16.2881, val_loss: 16.9937, val_MinusLogProbMetric: 16.9937

Epoch 761: val_loss did not improve from 16.97600
196/196 - 58s - loss: 16.2881 - MinusLogProbMetric: 16.2881 - val_loss: 16.9937 - val_MinusLogProbMetric: 16.9937 - lr: 1.0417e-05 - 58s/epoch - 295ms/step
Epoch 762/1000
2023-09-28 07:00:53.093 
Epoch 762/1000 
	 loss: 16.2979, MinusLogProbMetric: 16.2979, val_loss: 17.0246, val_MinusLogProbMetric: 17.0246

Epoch 762: val_loss did not improve from 16.97600
196/196 - 54s - loss: 16.2979 - MinusLogProbMetric: 16.2979 - val_loss: 17.0246 - val_MinusLogProbMetric: 17.0246 - lr: 1.0417e-05 - 54s/epoch - 277ms/step
Epoch 763/1000
2023-09-28 07:01:59.108 
Epoch 763/1000 
	 loss: 16.2879, MinusLogProbMetric: 16.2879, val_loss: 16.9948, val_MinusLogProbMetric: 16.9948

Epoch 763: val_loss did not improve from 16.97600
196/196 - 66s - loss: 16.2879 - MinusLogProbMetric: 16.2879 - val_loss: 16.9948 - val_MinusLogProbMetric: 16.9948 - lr: 1.0417e-05 - 66s/epoch - 337ms/step
Epoch 764/1000
2023-09-28 07:03:06.819 
Epoch 764/1000 
	 loss: 16.2877, MinusLogProbMetric: 16.2877, val_loss: 16.9824, val_MinusLogProbMetric: 16.9824

Epoch 764: val_loss did not improve from 16.97600
196/196 - 68s - loss: 16.2877 - MinusLogProbMetric: 16.2877 - val_loss: 16.9824 - val_MinusLogProbMetric: 16.9824 - lr: 1.0417e-05 - 68s/epoch - 345ms/step
Epoch 765/1000
2023-09-28 07:04:07.904 
Epoch 765/1000 
	 loss: 16.2855, MinusLogProbMetric: 16.2855, val_loss: 16.9949, val_MinusLogProbMetric: 16.9949

Epoch 765: val_loss did not improve from 16.97600
196/196 - 61s - loss: 16.2855 - MinusLogProbMetric: 16.2855 - val_loss: 16.9949 - val_MinusLogProbMetric: 16.9949 - lr: 1.0417e-05 - 61s/epoch - 312ms/step
Epoch 766/1000
2023-09-28 07:05:17.531 
Epoch 766/1000 
	 loss: 16.2858, MinusLogProbMetric: 16.2858, val_loss: 16.9788, val_MinusLogProbMetric: 16.9788

Epoch 766: val_loss did not improve from 16.97600
196/196 - 70s - loss: 16.2858 - MinusLogProbMetric: 16.2858 - val_loss: 16.9788 - val_MinusLogProbMetric: 16.9788 - lr: 1.0417e-05 - 70s/epoch - 355ms/step
Epoch 767/1000
2023-09-28 07:06:27.329 
Epoch 767/1000 
	 loss: 16.2854, MinusLogProbMetric: 16.2854, val_loss: 17.0551, val_MinusLogProbMetric: 17.0551

Epoch 767: val_loss did not improve from 16.97600
196/196 - 70s - loss: 16.2854 - MinusLogProbMetric: 16.2854 - val_loss: 17.0551 - val_MinusLogProbMetric: 17.0551 - lr: 1.0417e-05 - 70s/epoch - 356ms/step
Epoch 768/1000
2023-09-28 07:07:37.228 
Epoch 768/1000 
	 loss: 16.2971, MinusLogProbMetric: 16.2971, val_loss: 16.9891, val_MinusLogProbMetric: 16.9891

Epoch 768: val_loss did not improve from 16.97600
196/196 - 70s - loss: 16.2971 - MinusLogProbMetric: 16.2971 - val_loss: 16.9891 - val_MinusLogProbMetric: 16.9891 - lr: 1.0417e-05 - 70s/epoch - 357ms/step
Epoch 769/1000
2023-09-28 07:08:47.550 
Epoch 769/1000 
	 loss: 16.2882, MinusLogProbMetric: 16.2882, val_loss: 16.9817, val_MinusLogProbMetric: 16.9817

Epoch 769: val_loss did not improve from 16.97600
196/196 - 70s - loss: 16.2882 - MinusLogProbMetric: 16.2882 - val_loss: 16.9817 - val_MinusLogProbMetric: 16.9817 - lr: 1.0417e-05 - 70s/epoch - 359ms/step
Epoch 770/1000
2023-09-28 07:09:50.202 
Epoch 770/1000 
	 loss: 16.2876, MinusLogProbMetric: 16.2876, val_loss: 16.9782, val_MinusLogProbMetric: 16.9782

Epoch 770: val_loss did not improve from 16.97600
196/196 - 63s - loss: 16.2876 - MinusLogProbMetric: 16.2876 - val_loss: 16.9782 - val_MinusLogProbMetric: 16.9782 - lr: 1.0417e-05 - 63s/epoch - 320ms/step
Epoch 771/1000
2023-09-28 07:10:55.785 
Epoch 771/1000 
	 loss: 16.2901, MinusLogProbMetric: 16.2901, val_loss: 17.0168, val_MinusLogProbMetric: 17.0168

Epoch 771: val_loss did not improve from 16.97600
196/196 - 66s - loss: 16.2901 - MinusLogProbMetric: 16.2901 - val_loss: 17.0168 - val_MinusLogProbMetric: 17.0168 - lr: 1.0417e-05 - 66s/epoch - 335ms/step
Epoch 772/1000
2023-09-28 07:12:06.064 
Epoch 772/1000 
	 loss: 16.2917, MinusLogProbMetric: 16.2917, val_loss: 16.9879, val_MinusLogProbMetric: 16.9879

Epoch 772: val_loss did not improve from 16.97600
196/196 - 70s - loss: 16.2917 - MinusLogProbMetric: 16.2917 - val_loss: 16.9879 - val_MinusLogProbMetric: 16.9879 - lr: 1.0417e-05 - 70s/epoch - 359ms/step
Epoch 773/1000
2023-09-28 07:13:16.722 
Epoch 773/1000 
	 loss: 16.2869, MinusLogProbMetric: 16.2869, val_loss: 17.0025, val_MinusLogProbMetric: 17.0025

Epoch 773: val_loss did not improve from 16.97600
196/196 - 71s - loss: 16.2869 - MinusLogProbMetric: 16.2869 - val_loss: 17.0025 - val_MinusLogProbMetric: 17.0025 - lr: 1.0417e-05 - 71s/epoch - 360ms/step
Epoch 774/1000
2023-09-28 07:14:27.183 
Epoch 774/1000 
	 loss: 16.2875, MinusLogProbMetric: 16.2875, val_loss: 17.0531, val_MinusLogProbMetric: 17.0531

Epoch 774: val_loss did not improve from 16.97600
196/196 - 70s - loss: 16.2875 - MinusLogProbMetric: 16.2875 - val_loss: 17.0531 - val_MinusLogProbMetric: 17.0531 - lr: 1.0417e-05 - 70s/epoch - 359ms/step
Epoch 775/1000
2023-09-28 07:15:37.744 
Epoch 775/1000 
	 loss: 16.2891, MinusLogProbMetric: 16.2891, val_loss: 16.9922, val_MinusLogProbMetric: 16.9922

Epoch 775: val_loss did not improve from 16.97600
196/196 - 71s - loss: 16.2891 - MinusLogProbMetric: 16.2891 - val_loss: 16.9922 - val_MinusLogProbMetric: 16.9922 - lr: 1.0417e-05 - 71s/epoch - 360ms/step
Epoch 776/1000
2023-09-28 07:16:48.371 
Epoch 776/1000 
	 loss: 16.2897, MinusLogProbMetric: 16.2897, val_loss: 16.9925, val_MinusLogProbMetric: 16.9925

Epoch 776: val_loss did not improve from 16.97600
196/196 - 71s - loss: 16.2897 - MinusLogProbMetric: 16.2897 - val_loss: 16.9925 - val_MinusLogProbMetric: 16.9925 - lr: 1.0417e-05 - 71s/epoch - 360ms/step
Epoch 777/1000
2023-09-28 07:17:58.611 
Epoch 777/1000 
	 loss: 16.2908, MinusLogProbMetric: 16.2908, val_loss: 16.9784, val_MinusLogProbMetric: 16.9784

Epoch 777: val_loss did not improve from 16.97600
196/196 - 70s - loss: 16.2908 - MinusLogProbMetric: 16.2908 - val_loss: 16.9784 - val_MinusLogProbMetric: 16.9784 - lr: 1.0417e-05 - 70s/epoch - 358ms/step
Epoch 778/1000
2023-09-28 07:19:08.817 
Epoch 778/1000 
	 loss: 16.2891, MinusLogProbMetric: 16.2891, val_loss: 17.0112, val_MinusLogProbMetric: 17.0112

Epoch 778: val_loss did not improve from 16.97600
196/196 - 70s - loss: 16.2891 - MinusLogProbMetric: 16.2891 - val_loss: 17.0112 - val_MinusLogProbMetric: 17.0112 - lr: 1.0417e-05 - 70s/epoch - 358ms/step
Epoch 779/1000
2023-09-28 07:20:19.156 
Epoch 779/1000 
	 loss: 16.2933, MinusLogProbMetric: 16.2933, val_loss: 17.0005, val_MinusLogProbMetric: 17.0005

Epoch 779: val_loss did not improve from 16.97600
196/196 - 70s - loss: 16.2933 - MinusLogProbMetric: 16.2933 - val_loss: 17.0005 - val_MinusLogProbMetric: 17.0005 - lr: 1.0417e-05 - 70s/epoch - 359ms/step
Epoch 780/1000
2023-09-28 07:21:29.551 
Epoch 780/1000 
	 loss: 16.2922, MinusLogProbMetric: 16.2922, val_loss: 17.0001, val_MinusLogProbMetric: 17.0001

Epoch 780: val_loss did not improve from 16.97600
196/196 - 70s - loss: 16.2922 - MinusLogProbMetric: 16.2922 - val_loss: 17.0001 - val_MinusLogProbMetric: 17.0001 - lr: 1.0417e-05 - 70s/epoch - 359ms/step
Epoch 781/1000
2023-09-28 07:22:39.938 
Epoch 781/1000 
	 loss: 16.2904, MinusLogProbMetric: 16.2904, val_loss: 16.9851, val_MinusLogProbMetric: 16.9851

Epoch 781: val_loss did not improve from 16.97600
196/196 - 70s - loss: 16.2904 - MinusLogProbMetric: 16.2904 - val_loss: 16.9851 - val_MinusLogProbMetric: 16.9851 - lr: 1.0417e-05 - 70s/epoch - 359ms/step
Epoch 782/1000
2023-09-28 07:23:50.514 
Epoch 782/1000 
	 loss: 16.2927, MinusLogProbMetric: 16.2927, val_loss: 16.9762, val_MinusLogProbMetric: 16.9762

Epoch 782: val_loss did not improve from 16.97600
196/196 - 71s - loss: 16.2927 - MinusLogProbMetric: 16.2927 - val_loss: 16.9762 - val_MinusLogProbMetric: 16.9762 - lr: 1.0417e-05 - 71s/epoch - 360ms/step
Epoch 783/1000
2023-09-28 07:25:01.486 
Epoch 783/1000 
	 loss: 16.2873, MinusLogProbMetric: 16.2873, val_loss: 16.9811, val_MinusLogProbMetric: 16.9811

Epoch 783: val_loss did not improve from 16.97600
196/196 - 71s - loss: 16.2873 - MinusLogProbMetric: 16.2873 - val_loss: 16.9811 - val_MinusLogProbMetric: 16.9811 - lr: 1.0417e-05 - 71s/epoch - 362ms/step
Epoch 784/1000
2023-09-28 07:26:11.880 
Epoch 784/1000 
	 loss: 16.2852, MinusLogProbMetric: 16.2852, val_loss: 16.9788, val_MinusLogProbMetric: 16.9788

Epoch 784: val_loss did not improve from 16.97600
196/196 - 70s - loss: 16.2852 - MinusLogProbMetric: 16.2852 - val_loss: 16.9788 - val_MinusLogProbMetric: 16.9788 - lr: 1.0417e-05 - 70s/epoch - 359ms/step
Epoch 785/1000
2023-09-28 07:27:22.305 
Epoch 785/1000 
	 loss: 16.2864, MinusLogProbMetric: 16.2864, val_loss: 16.9822, val_MinusLogProbMetric: 16.9822

Epoch 785: val_loss did not improve from 16.97600
196/196 - 70s - loss: 16.2864 - MinusLogProbMetric: 16.2864 - val_loss: 16.9822 - val_MinusLogProbMetric: 16.9822 - lr: 1.0417e-05 - 70s/epoch - 359ms/step
Epoch 786/1000
2023-09-28 07:28:32.891 
Epoch 786/1000 
	 loss: 16.2864, MinusLogProbMetric: 16.2864, val_loss: 16.9880, val_MinusLogProbMetric: 16.9880

Epoch 786: val_loss did not improve from 16.97600
196/196 - 71s - loss: 16.2864 - MinusLogProbMetric: 16.2864 - val_loss: 16.9880 - val_MinusLogProbMetric: 16.9880 - lr: 1.0417e-05 - 71s/epoch - 360ms/step
Epoch 787/1000
2023-09-28 07:29:43.282 
Epoch 787/1000 
	 loss: 16.2828, MinusLogProbMetric: 16.2828, val_loss: 17.0198, val_MinusLogProbMetric: 17.0198

Epoch 787: val_loss did not improve from 16.97600
196/196 - 70s - loss: 16.2828 - MinusLogProbMetric: 16.2828 - val_loss: 17.0198 - val_MinusLogProbMetric: 17.0198 - lr: 1.0417e-05 - 70s/epoch - 359ms/step
Epoch 788/1000
2023-09-28 07:30:54.007 
Epoch 788/1000 
	 loss: 16.2916, MinusLogProbMetric: 16.2916, val_loss: 17.0016, val_MinusLogProbMetric: 17.0016

Epoch 788: val_loss did not improve from 16.97600
196/196 - 71s - loss: 16.2916 - MinusLogProbMetric: 16.2916 - val_loss: 17.0016 - val_MinusLogProbMetric: 17.0016 - lr: 1.0417e-05 - 71s/epoch - 361ms/step
Epoch 789/1000
2023-09-28 07:32:04.374 
Epoch 789/1000 
	 loss: 16.2844, MinusLogProbMetric: 16.2844, val_loss: 17.0434, val_MinusLogProbMetric: 17.0434

Epoch 789: val_loss did not improve from 16.97600
196/196 - 70s - loss: 16.2844 - MinusLogProbMetric: 16.2844 - val_loss: 17.0434 - val_MinusLogProbMetric: 17.0434 - lr: 1.0417e-05 - 70s/epoch - 359ms/step
Epoch 790/1000
2023-09-28 07:33:15.219 
Epoch 790/1000 
	 loss: 16.2854, MinusLogProbMetric: 16.2854, val_loss: 17.0010, val_MinusLogProbMetric: 17.0010

Epoch 790: val_loss did not improve from 16.97600
196/196 - 71s - loss: 16.2854 - MinusLogProbMetric: 16.2854 - val_loss: 17.0010 - val_MinusLogProbMetric: 17.0010 - lr: 1.0417e-05 - 71s/epoch - 361ms/step
Epoch 791/1000
2023-09-28 07:34:25.891 
Epoch 791/1000 
	 loss: 16.2887, MinusLogProbMetric: 16.2887, val_loss: 16.9779, val_MinusLogProbMetric: 16.9779

Epoch 791: val_loss did not improve from 16.97600
196/196 - 71s - loss: 16.2887 - MinusLogProbMetric: 16.2887 - val_loss: 16.9779 - val_MinusLogProbMetric: 16.9779 - lr: 1.0417e-05 - 71s/epoch - 361ms/step
Epoch 792/1000
2023-09-28 07:35:36.279 
Epoch 792/1000 
	 loss: 16.2831, MinusLogProbMetric: 16.2831, val_loss: 16.9820, val_MinusLogProbMetric: 16.9820

Epoch 792: val_loss did not improve from 16.97600
196/196 - 70s - loss: 16.2831 - MinusLogProbMetric: 16.2831 - val_loss: 16.9820 - val_MinusLogProbMetric: 16.9820 - lr: 1.0417e-05 - 70s/epoch - 359ms/step
Epoch 793/1000
2023-09-28 07:36:46.499 
Epoch 793/1000 
	 loss: 16.2864, MinusLogProbMetric: 16.2864, val_loss: 16.9896, val_MinusLogProbMetric: 16.9896

Epoch 793: val_loss did not improve from 16.97600
196/196 - 70s - loss: 16.2864 - MinusLogProbMetric: 16.2864 - val_loss: 16.9896 - val_MinusLogProbMetric: 16.9896 - lr: 1.0417e-05 - 70s/epoch - 358ms/step
Epoch 794/1000
2023-09-28 07:37:57.282 
Epoch 794/1000 
	 loss: 16.2866, MinusLogProbMetric: 16.2866, val_loss: 16.9896, val_MinusLogProbMetric: 16.9896

Epoch 794: val_loss did not improve from 16.97600
196/196 - 71s - loss: 16.2866 - MinusLogProbMetric: 16.2866 - val_loss: 16.9896 - val_MinusLogProbMetric: 16.9896 - lr: 1.0417e-05 - 71s/epoch - 361ms/step
Epoch 795/1000
2023-09-28 07:39:08.363 
Epoch 795/1000 
	 loss: 16.2814, MinusLogProbMetric: 16.2814, val_loss: 16.9792, val_MinusLogProbMetric: 16.9792

Epoch 795: val_loss did not improve from 16.97600
196/196 - 71s - loss: 16.2814 - MinusLogProbMetric: 16.2814 - val_loss: 16.9792 - val_MinusLogProbMetric: 16.9792 - lr: 1.0417e-05 - 71s/epoch - 363ms/step
Epoch 796/1000
2023-09-28 07:40:19.102 
Epoch 796/1000 
	 loss: 16.2859, MinusLogProbMetric: 16.2859, val_loss: 16.9996, val_MinusLogProbMetric: 16.9996

Epoch 796: val_loss did not improve from 16.97600
196/196 - 71s - loss: 16.2859 - MinusLogProbMetric: 16.2859 - val_loss: 16.9996 - val_MinusLogProbMetric: 16.9996 - lr: 1.0417e-05 - 71s/epoch - 361ms/step
Epoch 797/1000
2023-09-28 07:41:30.048 
Epoch 797/1000 
	 loss: 16.2882, MinusLogProbMetric: 16.2882, val_loss: 16.9866, val_MinusLogProbMetric: 16.9866

Epoch 797: val_loss did not improve from 16.97600
196/196 - 71s - loss: 16.2882 - MinusLogProbMetric: 16.2882 - val_loss: 16.9866 - val_MinusLogProbMetric: 16.9866 - lr: 1.0417e-05 - 71s/epoch - 362ms/step
Epoch 798/1000
2023-09-28 07:42:40.866 
Epoch 798/1000 
	 loss: 16.2852, MinusLogProbMetric: 16.2852, val_loss: 17.0226, val_MinusLogProbMetric: 17.0226

Epoch 798: val_loss did not improve from 16.97600
196/196 - 71s - loss: 16.2852 - MinusLogProbMetric: 16.2852 - val_loss: 17.0226 - val_MinusLogProbMetric: 17.0226 - lr: 1.0417e-05 - 71s/epoch - 361ms/step
Epoch 799/1000
2023-09-28 07:43:51.803 
Epoch 799/1000 
	 loss: 16.2759, MinusLogProbMetric: 16.2759, val_loss: 16.9800, val_MinusLogProbMetric: 16.9800

Epoch 799: val_loss did not improve from 16.97600
196/196 - 71s - loss: 16.2759 - MinusLogProbMetric: 16.2759 - val_loss: 16.9800 - val_MinusLogProbMetric: 16.9800 - lr: 5.2083e-06 - 71s/epoch - 362ms/step
Epoch 800/1000
2023-09-28 07:45:02.560 
Epoch 800/1000 
	 loss: 16.2735, MinusLogProbMetric: 16.2735, val_loss: 16.9742, val_MinusLogProbMetric: 16.9742

Epoch 800: val_loss improved from 16.97600 to 16.97420, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 72s - loss: 16.2735 - MinusLogProbMetric: 16.2735 - val_loss: 16.9742 - val_MinusLogProbMetric: 16.9742 - lr: 5.2083e-06 - 72s/epoch - 365ms/step
Epoch 801/1000
2023-09-28 07:46:14.072 
Epoch 801/1000 
	 loss: 16.2736, MinusLogProbMetric: 16.2736, val_loss: 16.9780, val_MinusLogProbMetric: 16.9780

Epoch 801: val_loss did not improve from 16.97420
196/196 - 71s - loss: 16.2736 - MinusLogProbMetric: 16.2736 - val_loss: 16.9780 - val_MinusLogProbMetric: 16.9780 - lr: 5.2083e-06 - 71s/epoch - 360ms/step
Epoch 802/1000
2023-09-28 07:47:24.957 
Epoch 802/1000 
	 loss: 16.2760, MinusLogProbMetric: 16.2760, val_loss: 16.9759, val_MinusLogProbMetric: 16.9759

Epoch 802: val_loss did not improve from 16.97420
196/196 - 71s - loss: 16.2760 - MinusLogProbMetric: 16.2760 - val_loss: 16.9759 - val_MinusLogProbMetric: 16.9759 - lr: 5.2083e-06 - 71s/epoch - 362ms/step
Epoch 803/1000
2023-09-28 07:48:35.821 
Epoch 803/1000 
	 loss: 16.2755, MinusLogProbMetric: 16.2755, val_loss: 16.9900, val_MinusLogProbMetric: 16.9900

Epoch 803: val_loss did not improve from 16.97420
196/196 - 71s - loss: 16.2755 - MinusLogProbMetric: 16.2755 - val_loss: 16.9900 - val_MinusLogProbMetric: 16.9900 - lr: 5.2083e-06 - 71s/epoch - 362ms/step
Epoch 804/1000
2023-09-28 07:49:46.870 
Epoch 804/1000 
	 loss: 16.2737, MinusLogProbMetric: 16.2737, val_loss: 16.9835, val_MinusLogProbMetric: 16.9835

Epoch 804: val_loss did not improve from 16.97420
196/196 - 71s - loss: 16.2737 - MinusLogProbMetric: 16.2737 - val_loss: 16.9835 - val_MinusLogProbMetric: 16.9835 - lr: 5.2083e-06 - 71s/epoch - 362ms/step
Epoch 805/1000
2023-09-28 07:50:58.431 
Epoch 805/1000 
	 loss: 16.2755, MinusLogProbMetric: 16.2755, val_loss: 16.9806, val_MinusLogProbMetric: 16.9806

Epoch 805: val_loss did not improve from 16.97420
196/196 - 72s - loss: 16.2755 - MinusLogProbMetric: 16.2755 - val_loss: 16.9806 - val_MinusLogProbMetric: 16.9806 - lr: 5.2083e-06 - 72s/epoch - 365ms/step
Epoch 806/1000
2023-09-28 07:52:09.317 
Epoch 806/1000 
	 loss: 16.2731, MinusLogProbMetric: 16.2731, val_loss: 16.9773, val_MinusLogProbMetric: 16.9773

Epoch 806: val_loss did not improve from 16.97420
196/196 - 71s - loss: 16.2731 - MinusLogProbMetric: 16.2731 - val_loss: 16.9773 - val_MinusLogProbMetric: 16.9773 - lr: 5.2083e-06 - 71s/epoch - 362ms/step
Epoch 807/1000
2023-09-28 07:53:20.419 
Epoch 807/1000 
	 loss: 16.2763, MinusLogProbMetric: 16.2763, val_loss: 16.9756, val_MinusLogProbMetric: 16.9756

Epoch 807: val_loss did not improve from 16.97420
196/196 - 71s - loss: 16.2763 - MinusLogProbMetric: 16.2763 - val_loss: 16.9756 - val_MinusLogProbMetric: 16.9756 - lr: 5.2083e-06 - 71s/epoch - 363ms/step
Epoch 808/1000
2023-09-28 07:54:31.492 
Epoch 808/1000 
	 loss: 16.2744, MinusLogProbMetric: 16.2744, val_loss: 16.9783, val_MinusLogProbMetric: 16.9783

Epoch 808: val_loss did not improve from 16.97420
196/196 - 71s - loss: 16.2744 - MinusLogProbMetric: 16.2744 - val_loss: 16.9783 - val_MinusLogProbMetric: 16.9783 - lr: 5.2083e-06 - 71s/epoch - 363ms/step
Epoch 809/1000
2023-09-28 07:55:42.258 
Epoch 809/1000 
	 loss: 16.2745, MinusLogProbMetric: 16.2745, val_loss: 16.9758, val_MinusLogProbMetric: 16.9758

Epoch 809: val_loss did not improve from 16.97420
196/196 - 71s - loss: 16.2745 - MinusLogProbMetric: 16.2745 - val_loss: 16.9758 - val_MinusLogProbMetric: 16.9758 - lr: 5.2083e-06 - 71s/epoch - 361ms/step
Epoch 810/1000
2023-09-28 07:56:53.703 
Epoch 810/1000 
	 loss: 16.2737, MinusLogProbMetric: 16.2737, val_loss: 16.9815, val_MinusLogProbMetric: 16.9815

Epoch 810: val_loss did not improve from 16.97420
196/196 - 71s - loss: 16.2737 - MinusLogProbMetric: 16.2737 - val_loss: 16.9815 - val_MinusLogProbMetric: 16.9815 - lr: 5.2083e-06 - 71s/epoch - 364ms/step
Epoch 811/1000
2023-09-28 07:58:03.971 
Epoch 811/1000 
	 loss: 16.2737, MinusLogProbMetric: 16.2737, val_loss: 16.9782, val_MinusLogProbMetric: 16.9782

Epoch 811: val_loss did not improve from 16.97420
196/196 - 70s - loss: 16.2737 - MinusLogProbMetric: 16.2737 - val_loss: 16.9782 - val_MinusLogProbMetric: 16.9782 - lr: 5.2083e-06 - 70s/epoch - 358ms/step
Epoch 812/1000
2023-09-28 07:59:15.348 
Epoch 812/1000 
	 loss: 16.2745, MinusLogProbMetric: 16.2745, val_loss: 16.9766, val_MinusLogProbMetric: 16.9766

Epoch 812: val_loss did not improve from 16.97420
196/196 - 71s - loss: 16.2745 - MinusLogProbMetric: 16.2745 - val_loss: 16.9766 - val_MinusLogProbMetric: 16.9766 - lr: 5.2083e-06 - 71s/epoch - 364ms/step
Epoch 813/1000
2023-09-28 08:00:26.641 
Epoch 813/1000 
	 loss: 16.2721, MinusLogProbMetric: 16.2721, val_loss: 16.9744, val_MinusLogProbMetric: 16.9744

Epoch 813: val_loss did not improve from 16.97420
196/196 - 71s - loss: 16.2721 - MinusLogProbMetric: 16.2721 - val_loss: 16.9744 - val_MinusLogProbMetric: 16.9744 - lr: 5.2083e-06 - 71s/epoch - 364ms/step
Epoch 814/1000
2023-09-28 08:01:37.774 
Epoch 814/1000 
	 loss: 16.2733, MinusLogProbMetric: 16.2733, val_loss: 16.9740, val_MinusLogProbMetric: 16.9740

Epoch 814: val_loss improved from 16.97420 to 16.97399, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 72s - loss: 16.2733 - MinusLogProbMetric: 16.2733 - val_loss: 16.9740 - val_MinusLogProbMetric: 16.9740 - lr: 5.2083e-06 - 72s/epoch - 368ms/step
Epoch 815/1000
2023-09-28 08:02:49.667 
Epoch 815/1000 
	 loss: 16.2737, MinusLogProbMetric: 16.2737, val_loss: 16.9749, val_MinusLogProbMetric: 16.9749

Epoch 815: val_loss did not improve from 16.97399
196/196 - 71s - loss: 16.2737 - MinusLogProbMetric: 16.2737 - val_loss: 16.9749 - val_MinusLogProbMetric: 16.9749 - lr: 5.2083e-06 - 71s/epoch - 362ms/step
Epoch 816/1000
2023-09-28 08:04:00.420 
Epoch 816/1000 
	 loss: 16.2747, MinusLogProbMetric: 16.2747, val_loss: 16.9831, val_MinusLogProbMetric: 16.9831

Epoch 816: val_loss did not improve from 16.97399
196/196 - 71s - loss: 16.2747 - MinusLogProbMetric: 16.2747 - val_loss: 16.9831 - val_MinusLogProbMetric: 16.9831 - lr: 5.2083e-06 - 71s/epoch - 361ms/step
Epoch 817/1000
2023-09-28 08:05:11.550 
Epoch 817/1000 
	 loss: 16.2727, MinusLogProbMetric: 16.2727, val_loss: 16.9899, val_MinusLogProbMetric: 16.9899

Epoch 817: val_loss did not improve from 16.97399
196/196 - 71s - loss: 16.2727 - MinusLogProbMetric: 16.2727 - val_loss: 16.9899 - val_MinusLogProbMetric: 16.9899 - lr: 5.2083e-06 - 71s/epoch - 363ms/step
Epoch 818/1000
2023-09-28 08:06:22.542 
Epoch 818/1000 
	 loss: 16.2735, MinusLogProbMetric: 16.2735, val_loss: 16.9741, val_MinusLogProbMetric: 16.9741

Epoch 818: val_loss did not improve from 16.97399
196/196 - 71s - loss: 16.2735 - MinusLogProbMetric: 16.2735 - val_loss: 16.9741 - val_MinusLogProbMetric: 16.9741 - lr: 5.2083e-06 - 71s/epoch - 362ms/step
Epoch 819/1000
2023-09-28 08:07:33.600 
Epoch 819/1000 
	 loss: 16.2752, MinusLogProbMetric: 16.2752, val_loss: 16.9771, val_MinusLogProbMetric: 16.9771

Epoch 819: val_loss did not improve from 16.97399
196/196 - 71s - loss: 16.2752 - MinusLogProbMetric: 16.2752 - val_loss: 16.9771 - val_MinusLogProbMetric: 16.9771 - lr: 5.2083e-06 - 71s/epoch - 363ms/step
Epoch 820/1000
2023-09-28 08:08:44.377 
Epoch 820/1000 
	 loss: 16.2736, MinusLogProbMetric: 16.2736, val_loss: 16.9781, val_MinusLogProbMetric: 16.9781

Epoch 820: val_loss did not improve from 16.97399
196/196 - 71s - loss: 16.2736 - MinusLogProbMetric: 16.2736 - val_loss: 16.9781 - val_MinusLogProbMetric: 16.9781 - lr: 5.2083e-06 - 71s/epoch - 361ms/step
Epoch 821/1000
2023-09-28 08:09:55.531 
Epoch 821/1000 
	 loss: 16.2719, MinusLogProbMetric: 16.2719, val_loss: 16.9758, val_MinusLogProbMetric: 16.9758

Epoch 821: val_loss did not improve from 16.97399
196/196 - 71s - loss: 16.2719 - MinusLogProbMetric: 16.2719 - val_loss: 16.9758 - val_MinusLogProbMetric: 16.9758 - lr: 5.2083e-06 - 71s/epoch - 363ms/step
Epoch 822/1000
2023-09-28 08:11:05.826 
Epoch 822/1000 
	 loss: 16.2747, MinusLogProbMetric: 16.2747, val_loss: 16.9805, val_MinusLogProbMetric: 16.9805

Epoch 822: val_loss did not improve from 16.97399
196/196 - 70s - loss: 16.2747 - MinusLogProbMetric: 16.2747 - val_loss: 16.9805 - val_MinusLogProbMetric: 16.9805 - lr: 5.2083e-06 - 70s/epoch - 359ms/step
Epoch 823/1000
2023-09-28 08:12:16.338 
Epoch 823/1000 
	 loss: 16.2720, MinusLogProbMetric: 16.2720, val_loss: 16.9812, val_MinusLogProbMetric: 16.9812

Epoch 823: val_loss did not improve from 16.97399
196/196 - 71s - loss: 16.2720 - MinusLogProbMetric: 16.2720 - val_loss: 16.9812 - val_MinusLogProbMetric: 16.9812 - lr: 5.2083e-06 - 71s/epoch - 360ms/step
Epoch 824/1000
2023-09-28 08:13:27.279 
Epoch 824/1000 
	 loss: 16.2750, MinusLogProbMetric: 16.2750, val_loss: 16.9833, val_MinusLogProbMetric: 16.9833

Epoch 824: val_loss did not improve from 16.97399
196/196 - 71s - loss: 16.2750 - MinusLogProbMetric: 16.2750 - val_loss: 16.9833 - val_MinusLogProbMetric: 16.9833 - lr: 5.2083e-06 - 71s/epoch - 362ms/step
Epoch 825/1000
2023-09-28 08:14:37.593 
Epoch 825/1000 
	 loss: 16.2715, MinusLogProbMetric: 16.2715, val_loss: 16.9811, val_MinusLogProbMetric: 16.9811

Epoch 825: val_loss did not improve from 16.97399
196/196 - 70s - loss: 16.2715 - MinusLogProbMetric: 16.2715 - val_loss: 16.9811 - val_MinusLogProbMetric: 16.9811 - lr: 5.2083e-06 - 70s/epoch - 359ms/step
Epoch 826/1000
2023-09-28 08:15:48.289 
Epoch 826/1000 
	 loss: 16.2739, MinusLogProbMetric: 16.2739, val_loss: 16.9756, val_MinusLogProbMetric: 16.9756

Epoch 826: val_loss did not improve from 16.97399
196/196 - 71s - loss: 16.2739 - MinusLogProbMetric: 16.2739 - val_loss: 16.9756 - val_MinusLogProbMetric: 16.9756 - lr: 5.2083e-06 - 71s/epoch - 361ms/step
Epoch 827/1000
2023-09-28 08:16:59.063 
Epoch 827/1000 
	 loss: 16.2746, MinusLogProbMetric: 16.2746, val_loss: 16.9763, val_MinusLogProbMetric: 16.9763

Epoch 827: val_loss did not improve from 16.97399
196/196 - 71s - loss: 16.2746 - MinusLogProbMetric: 16.2746 - val_loss: 16.9763 - val_MinusLogProbMetric: 16.9763 - lr: 5.2083e-06 - 71s/epoch - 361ms/step
Epoch 828/1000
2023-09-28 08:18:10.017 
Epoch 828/1000 
	 loss: 16.2743, MinusLogProbMetric: 16.2743, val_loss: 16.9730, val_MinusLogProbMetric: 16.9730

Epoch 828: val_loss improved from 16.97399 to 16.97301, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_310/weights/best_weights.h5
196/196 - 72s - loss: 16.2743 - MinusLogProbMetric: 16.2743 - val_loss: 16.9730 - val_MinusLogProbMetric: 16.9730 - lr: 5.2083e-06 - 72s/epoch - 367ms/step
Epoch 829/1000
2023-09-28 08:19:21.322 
Epoch 829/1000 
	 loss: 16.2712, MinusLogProbMetric: 16.2712, val_loss: 16.9739, val_MinusLogProbMetric: 16.9739

Epoch 829: val_loss did not improve from 16.97301
196/196 - 70s - loss: 16.2712 - MinusLogProbMetric: 16.2712 - val_loss: 16.9739 - val_MinusLogProbMetric: 16.9739 - lr: 5.2083e-06 - 70s/epoch - 359ms/step
Epoch 830/1000
2023-09-28 08:20:32.282 
Epoch 830/1000 
	 loss: 16.2712, MinusLogProbMetric: 16.2712, val_loss: 16.9790, val_MinusLogProbMetric: 16.9790

Epoch 830: val_loss did not improve from 16.97301
196/196 - 71s - loss: 16.2712 - MinusLogProbMetric: 16.2712 - val_loss: 16.9790 - val_MinusLogProbMetric: 16.9790 - lr: 5.2083e-06 - 71s/epoch - 362ms/step
Epoch 831/1000
2023-09-28 08:21:42.880 
Epoch 831/1000 
	 loss: 16.2748, MinusLogProbMetric: 16.2748, val_loss: 16.9740, val_MinusLogProbMetric: 16.9740

Epoch 831: val_loss did not improve from 16.97301
196/196 - 71s - loss: 16.2748 - MinusLogProbMetric: 16.2748 - val_loss: 16.9740 - val_MinusLogProbMetric: 16.9740 - lr: 5.2083e-06 - 71s/epoch - 360ms/step
Epoch 832/1000
2023-09-28 08:22:52.960 
Epoch 832/1000 
	 loss: 16.2730, MinusLogProbMetric: 16.2730, val_loss: 16.9771, val_MinusLogProbMetric: 16.9771

Epoch 832: val_loss did not improve from 16.97301
196/196 - 70s - loss: 16.2730 - MinusLogProbMetric: 16.2730 - val_loss: 16.9771 - val_MinusLogProbMetric: 16.9771 - lr: 5.2083e-06 - 70s/epoch - 358ms/step
Epoch 833/1000
2023-09-28 08:24:03.400 
Epoch 833/1000 
	 loss: 16.2714, MinusLogProbMetric: 16.2714, val_loss: 16.9782, val_MinusLogProbMetric: 16.9782

Epoch 833: val_loss did not improve from 16.97301
196/196 - 70s - loss: 16.2714 - MinusLogProbMetric: 16.2714 - val_loss: 16.9782 - val_MinusLogProbMetric: 16.9782 - lr: 5.2083e-06 - 70s/epoch - 359ms/step
Epoch 834/1000
2023-09-28 08:25:14.275 
Epoch 834/1000 
	 loss: 16.2727, MinusLogProbMetric: 16.2727, val_loss: 16.9810, val_MinusLogProbMetric: 16.9810

Epoch 834: val_loss did not improve from 16.97301
196/196 - 71s - loss: 16.2727 - MinusLogProbMetric: 16.2727 - val_loss: 16.9810 - val_MinusLogProbMetric: 16.9810 - lr: 5.2083e-06 - 71s/epoch - 362ms/step
Epoch 835/1000
2023-09-28 08:26:24.912 
Epoch 835/1000 
	 loss: 16.2742, MinusLogProbMetric: 16.2742, val_loss: 16.9746, val_MinusLogProbMetric: 16.9746

Epoch 835: val_loss did not improve from 16.97301
196/196 - 71s - loss: 16.2742 - MinusLogProbMetric: 16.2742 - val_loss: 16.9746 - val_MinusLogProbMetric: 16.9746 - lr: 5.2083e-06 - 71s/epoch - 360ms/step
Epoch 836/1000
2023-09-28 08:27:36.111 
Epoch 836/1000 
	 loss: 16.2718, MinusLogProbMetric: 16.2718, val_loss: 16.9750, val_MinusLogProbMetric: 16.9750

Epoch 836: val_loss did not improve from 16.97301
196/196 - 71s - loss: 16.2718 - MinusLogProbMetric: 16.2718 - val_loss: 16.9750 - val_MinusLogProbMetric: 16.9750 - lr: 5.2083e-06 - 71s/epoch - 363ms/step
Epoch 837/1000
2023-09-28 08:28:46.744 
Epoch 837/1000 
	 loss: 16.2715, MinusLogProbMetric: 16.2715, val_loss: 16.9801, val_MinusLogProbMetric: 16.9801

Epoch 837: val_loss did not improve from 16.97301
196/196 - 71s - loss: 16.2715 - MinusLogProbMetric: 16.2715 - val_loss: 16.9801 - val_MinusLogProbMetric: 16.9801 - lr: 5.2083e-06 - 71s/epoch - 360ms/step
Epoch 838/1000
2023-09-28 08:29:57.305 
Epoch 838/1000 
	 loss: 16.2711, MinusLogProbMetric: 16.2711, val_loss: 16.9809, val_MinusLogProbMetric: 16.9809

Epoch 838: val_loss did not improve from 16.97301
196/196 - 71s - loss: 16.2711 - MinusLogProbMetric: 16.2711 - val_loss: 16.9809 - val_MinusLogProbMetric: 16.9809 - lr: 5.2083e-06 - 71s/epoch - 360ms/step
Epoch 839/1000
2023-09-28 08:31:08.153 
Epoch 839/1000 
	 loss: 16.2704, MinusLogProbMetric: 16.2704, val_loss: 16.9755, val_MinusLogProbMetric: 16.9755

Epoch 839: val_loss did not improve from 16.97301
196/196 - 71s - loss: 16.2704 - MinusLogProbMetric: 16.2704 - val_loss: 16.9755 - val_MinusLogProbMetric: 16.9755 - lr: 5.2083e-06 - 71s/epoch - 361ms/step
Epoch 840/1000
2023-09-28 08:32:18.314 
Epoch 840/1000 
	 loss: 16.2724, MinusLogProbMetric: 16.2724, val_loss: 16.9739, val_MinusLogProbMetric: 16.9739

Epoch 840: val_loss did not improve from 16.97301
196/196 - 70s - loss: 16.2724 - MinusLogProbMetric: 16.2724 - val_loss: 16.9739 - val_MinusLogProbMetric: 16.9739 - lr: 5.2083e-06 - 70s/epoch - 358ms/step
Epoch 841/1000
2023-09-28 08:33:29.101 
Epoch 841/1000 
	 loss: 16.2722, MinusLogProbMetric: 16.2722, val_loss: 16.9874, val_MinusLogProbMetric: 16.9874

Epoch 841: val_loss did not improve from 16.97301
196/196 - 71s - loss: 16.2722 - MinusLogProbMetric: 16.2722 - val_loss: 16.9874 - val_MinusLogProbMetric: 16.9874 - lr: 5.2083e-06 - 71s/epoch - 361ms/step
Epoch 842/1000
2023-09-28 08:34:39.530 
Epoch 842/1000 
	 loss: 16.2734, MinusLogProbMetric: 16.2734, val_loss: 16.9805, val_MinusLogProbMetric: 16.9805

Epoch 842: val_loss did not improve from 16.97301
196/196 - 70s - loss: 16.2734 - MinusLogProbMetric: 16.2734 - val_loss: 16.9805 - val_MinusLogProbMetric: 16.9805 - lr: 5.2083e-06 - 70s/epoch - 359ms/step
Epoch 843/1000
2023-09-28 08:35:49.729 
Epoch 843/1000 
	 loss: 16.2731, MinusLogProbMetric: 16.2731, val_loss: 16.9810, val_MinusLogProbMetric: 16.9810

Epoch 843: val_loss did not improve from 16.97301
196/196 - 70s - loss: 16.2731 - MinusLogProbMetric: 16.2731 - val_loss: 16.9810 - val_MinusLogProbMetric: 16.9810 - lr: 5.2083e-06 - 70s/epoch - 358ms/step
Epoch 844/1000
2023-09-28 08:37:00.142 
Epoch 844/1000 
	 loss: 16.2725, MinusLogProbMetric: 16.2725, val_loss: 16.9867, val_MinusLogProbMetric: 16.9867

Epoch 844: val_loss did not improve from 16.97301
196/196 - 70s - loss: 16.2725 - MinusLogProbMetric: 16.2725 - val_loss: 16.9867 - val_MinusLogProbMetric: 16.9867 - lr: 5.2083e-06 - 70s/epoch - 359ms/step
Epoch 845/1000
2023-09-28 08:38:10.801 
Epoch 845/1000 
	 loss: 16.2734, MinusLogProbMetric: 16.2734, val_loss: 16.9762, val_MinusLogProbMetric: 16.9762

Epoch 845: val_loss did not improve from 16.97301
196/196 - 71s - loss: 16.2734 - MinusLogProbMetric: 16.2734 - val_loss: 16.9762 - val_MinusLogProbMetric: 16.9762 - lr: 5.2083e-06 - 71s/epoch - 360ms/step
Epoch 846/1000
2023-09-28 08:39:21.277 
Epoch 846/1000 
	 loss: 16.2714, MinusLogProbMetric: 16.2714, val_loss: 16.9731, val_MinusLogProbMetric: 16.9731

Epoch 846: val_loss did not improve from 16.97301
196/196 - 70s - loss: 16.2714 - MinusLogProbMetric: 16.2714 - val_loss: 16.9731 - val_MinusLogProbMetric: 16.9731 - lr: 5.2083e-06 - 70s/epoch - 360ms/step
Epoch 847/1000
2023-09-28 08:40:31.773 
Epoch 847/1000 
	 loss: 16.2722, MinusLogProbMetric: 16.2722, val_loss: 16.9746, val_MinusLogProbMetric: 16.9746

Epoch 847: val_loss did not improve from 16.97301
196/196 - 70s - loss: 16.2722 - MinusLogProbMetric: 16.2722 - val_loss: 16.9746 - val_MinusLogProbMetric: 16.9746 - lr: 5.2083e-06 - 70s/epoch - 360ms/step
Epoch 848/1000
2023-09-28 08:41:42.335 
Epoch 848/1000 
	 loss: 16.2736, MinusLogProbMetric: 16.2736, val_loss: 16.9761, val_MinusLogProbMetric: 16.9761

Epoch 848: val_loss did not improve from 16.97301
196/196 - 71s - loss: 16.2736 - MinusLogProbMetric: 16.2736 - val_loss: 16.9761 - val_MinusLogProbMetric: 16.9761 - lr: 5.2083e-06 - 71s/epoch - 360ms/step
Epoch 849/1000
2023-09-28 08:42:52.617 
Epoch 849/1000 
	 loss: 16.2733, MinusLogProbMetric: 16.2733, val_loss: 16.9809, val_MinusLogProbMetric: 16.9809

Epoch 849: val_loss did not improve from 16.97301
196/196 - 70s - loss: 16.2733 - MinusLogProbMetric: 16.2733 - val_loss: 16.9809 - val_MinusLogProbMetric: 16.9809 - lr: 5.2083e-06 - 70s/epoch - 359ms/step
Epoch 850/1000
2023-09-28 08:44:03.162 
Epoch 850/1000 
	 loss: 16.2748, MinusLogProbMetric: 16.2748, val_loss: 16.9819, val_MinusLogProbMetric: 16.9819

Epoch 850: val_loss did not improve from 16.97301
196/196 - 71s - loss: 16.2748 - MinusLogProbMetric: 16.2748 - val_loss: 16.9819 - val_MinusLogProbMetric: 16.9819 - lr: 5.2083e-06 - 71s/epoch - 360ms/step
Epoch 851/1000
2023-09-28 08:45:12.344 
Epoch 851/1000 
	 loss: 16.2735, MinusLogProbMetric: 16.2735, val_loss: 16.9797, val_MinusLogProbMetric: 16.9797

Epoch 851: val_loss did not improve from 16.97301
196/196 - 69s - loss: 16.2735 - MinusLogProbMetric: 16.2735 - val_loss: 16.9797 - val_MinusLogProbMetric: 16.9797 - lr: 5.2083e-06 - 69s/epoch - 353ms/step
Epoch 852/1000
2023-09-28 08:46:20.540 
Epoch 852/1000 
	 loss: 16.2737, MinusLogProbMetric: 16.2737, val_loss: 16.9832, val_MinusLogProbMetric: 16.9832

Epoch 852: val_loss did not improve from 16.97301
196/196 - 68s - loss: 16.2737 - MinusLogProbMetric: 16.2737 - val_loss: 16.9832 - val_MinusLogProbMetric: 16.9832 - lr: 5.2083e-06 - 68s/epoch - 348ms/step
Epoch 853/1000
