2023-10-23 12:32:31.424741: Importing os...
2023-10-23 12:32:31.424812: Importing sys...
2023-10-23 12:32:31.424828: Importing and initializing argparse...
Visible devices: [1]
2023-10-23 12:32:31.442213: Importing timer from timeit...
2023-10-23 12:32:31.442786: Setting env variables for tf import (only device [1] will be available)...
2023-10-23 12:32:31.442831: Importing numpy...
2023-10-23 12:32:31.618860: Importing pandas...
2023-10-23 12:32:31.803096: Importing shutil...
2023-10-23 12:32:31.803118: Importing subprocess...
2023-10-23 12:32:31.803125: Importing tensorflow...
Tensorflow version: 2.12.0
2023-10-23 12:32:34.188105: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-10-23 12:32:34.718555: Importing textwrap...
2023-10-23 12:32:34.718599: Importing timeit...
2023-10-23 12:32:34.718611: Importing traceback...
2023-10-23 12:32:34.718620: Importing typing...
2023-10-23 12:32:34.718632: Setting tf configs...
2023-10-23 12:32:34.968836: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-10-23 12:32:36.658437: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

===========
Generating train data for run 245.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_245/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_245/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.9518368 , 3.3973382 , 9.531531  , ..., 7.757203  , 2.6203427 ,
        1.8047177 ],
       [2.0608988 , 4.2499633 , 8.220001  , ..., 7.0034    , 2.2546027 ,
        2.1452668 ],
       [2.843429  , 3.9051921 , 7.4207163 , ..., 7.300193  , 3.1696632 ,
        1.5780848 ],
       ...,
       [5.2732286 , 6.077929  , 1.3790838 , ..., 0.65630984, 7.270858  ,
        1.3828237 ],
       [4.184838  , 7.1681747 , 6.2432003 , ..., 4.410938  , 2.659537  ,
        7.4785604 ],
       [5.6083927 , 7.161964  , 6.0947328 , ..., 4.4707136 , 2.6326852 ,
        7.344711  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_245/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_245
self.data_kwargs: {'seed': 0}
self.x_data: [[2.147613   3.0747468  7.824332   ... 7.103659   2.4434035  1.8856683 ]
 [5.330741   5.782131   0.60243964 ... 0.4385597  6.6599607  1.2875487 ]
 [5.9696746  7.1899557  6.848176   ... 3.815116   2.6560376  7.342258  ]
 ...
 [3.6073484  5.5595417  0.9042071  ... 0.7584033  6.576529   1.36466   ]
 [3.793339   5.232353   1.1439581  ... 1.5372672  6.3050704  1.3603225 ]
 [1.2775126  3.7349403  9.479768   ... 7.3904037  2.7995863  1.7972767 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  826720    
 r)                                                              
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7f1b04082ef0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1ac040ffa0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1ac040ffa0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1b04083e20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1ac043be80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1ac0374430>, <keras.callbacks.ModelCheckpoint object at 0x7f1ac0374580>, <keras.callbacks.EarlyStopping object at 0x7f1ac0374790>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1ac03747c0>, <keras.callbacks.TerminateOnNaN object at 0x7f1ac03744f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.9518368 , 3.3973382 , 9.531531  , ..., 7.757203  , 2.6203427 ,
        1.8047177 ],
       [2.0608988 , 4.2499633 , 8.220001  , ..., 7.0034    , 2.2546027 ,
        2.1452668 ],
       [2.843429  , 3.9051921 , 7.4207163 , ..., 7.300193  , 3.1696632 ,
        1.5780848 ],
       ...,
       [5.2732286 , 6.077929  , 1.3790838 , ..., 0.65630984, 7.270858  ,
        1.3828237 ],
       [4.184838  , 7.1681747 , 6.2432003 , ..., 4.410938  , 2.659537  ,
        7.4785604 ],
       [5.6083927 , 7.161964  , 6.0947328 , ..., 4.4707136 , 2.6326852 ,
        7.344711  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_245/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 245/720 with hyperparameters:
timestamp = 2023-10-23 12:32:42.861210
ndims = 32
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.147613    3.0747468   7.824332    1.8133726   9.163057    0.34882736
  9.789957    4.1820974   9.68468     6.146849    7.005386    0.3680902
  2.6196527   1.2054405   2.7207217   1.3141394   2.9878392   4.696398
 -0.43681622  6.9252343   5.6900735   2.943952    6.358174    1.1145298
  6.7379575   9.321477    3.595148    7.318303    0.9570862   7.103659
  2.4434035   1.8856683 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 52: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-23 12:34:45.134 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2065.0200, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 122s - loss: nan - MinusLogProbMetric: 2065.0200 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 122s/epoch - 623ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0003333333333333333.
===========
Generating train data for run 245.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_245/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_245/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.9518368 , 3.3973382 , 9.531531  , ..., 7.757203  , 2.6203427 ,
        1.8047177 ],
       [2.0608988 , 4.2499633 , 8.220001  , ..., 7.0034    , 2.2546027 ,
        2.1452668 ],
       [2.843429  , 3.9051921 , 7.4207163 , ..., 7.300193  , 3.1696632 ,
        1.5780848 ],
       ...,
       [5.2732286 , 6.077929  , 1.3790838 , ..., 0.65630984, 7.270858  ,
        1.3828237 ],
       [4.184838  , 7.1681747 , 6.2432003 , ..., 4.410938  , 2.659537  ,
        7.4785604 ],
       [5.6083927 , 7.161964  , 6.0947328 , ..., 4.4707136 , 2.6326852 ,
        7.344711  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_245/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_245
self.data_kwargs: {'seed': 0}
self.x_data: [[2.147613   3.0747468  7.824332   ... 7.103659   2.4434035  1.8856683 ]
 [5.330741   5.782131   0.60243964 ... 0.4385597  6.6599607  1.2875487 ]
 [5.9696746  7.1899557  6.848176   ... 3.815116   2.6560376  7.342258  ]
 ...
 [3.6073484  5.5595417  0.9042071  ... 0.7584033  6.576529   1.36466   ]
 [3.793339   5.232353   1.1439581  ... 1.5372672  6.3050704  1.3603225 ]
 [1.2775126  3.7349403  9.479768   ... 7.3904037  2.7995863  1.7972767 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7f1e99f4f9d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1eaa8c16f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1eaa8c16f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1e99a1ff70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1e9997bb80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1e999bc130>, <keras.callbacks.ModelCheckpoint object at 0x7f1e999bc1f0>, <keras.callbacks.EarlyStopping object at 0x7f1e999bc460>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1e999bc490>, <keras.callbacks.TerminateOnNaN object at 0x7f1e999bc0d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.9518368 , 3.3973382 , 9.531531  , ..., 7.757203  , 2.6203427 ,
        1.8047177 ],
       [2.0608988 , 4.2499633 , 8.220001  , ..., 7.0034    , 2.2546027 ,
        2.1452668 ],
       [2.843429  , 3.9051921 , 7.4207163 , ..., 7.300193  , 3.1696632 ,
        1.5780848 ],
       ...,
       [5.2732286 , 6.077929  , 1.3790838 , ..., 0.65630984, 7.270858  ,
        1.3828237 ],
       [4.184838  , 7.1681747 , 6.2432003 , ..., 4.410938  , 2.659537  ,
        7.4785604 ],
       [5.6083927 , 7.161964  , 6.0947328 , ..., 4.4707136 , 2.6326852 ,
        7.344711  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_245/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 245/720 with hyperparameters:
timestamp = 2023-10-23 12:34:52.503133
ndims = 32
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.147613    3.0747468   7.824332    1.8133726   9.163057    0.34882736
  9.789957    4.1820974   9.68468     6.146849    7.005386    0.3680902
  2.6196527   1.2054405   2.7207217   1.3141394   2.9878392   4.696398
 -0.43681622  6.9252343   5.6900735   2.943952    6.358174    1.1145298
  6.7379575   9.321477    3.595148    7.318303    0.9570862   7.103659
  2.4434035   1.8856683 ]
Epoch 1/1000
2023-10-23 12:37:35.326 
Epoch 1/1000 
	 loss: 1089.6072, MinusLogProbMetric: 1089.6072, val_loss: 375.7800, val_MinusLogProbMetric: 375.7800

Epoch 1: val_loss improved from inf to 375.78000, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 163s - loss: 1089.6072 - MinusLogProbMetric: 1089.6072 - val_loss: 375.7800 - val_MinusLogProbMetric: 375.7800 - lr: 3.3333e-04 - 163s/epoch - 833ms/step
Epoch 2/1000
2023-10-23 12:38:31.207 
Epoch 2/1000 
	 loss: 281.6277, MinusLogProbMetric: 281.6277, val_loss: 209.3362, val_MinusLogProbMetric: 209.3362

Epoch 2: val_loss improved from 375.78000 to 209.33621, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 56s - loss: 281.6277 - MinusLogProbMetric: 281.6277 - val_loss: 209.3362 - val_MinusLogProbMetric: 209.3362 - lr: 3.3333e-04 - 56s/epoch - 284ms/step
Epoch 3/1000
2023-10-23 12:39:24.799 
Epoch 3/1000 
	 loss: 196.0177, MinusLogProbMetric: 196.0177, val_loss: 170.6448, val_MinusLogProbMetric: 170.6448

Epoch 3: val_loss improved from 209.33621 to 170.64476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 196.0177 - MinusLogProbMetric: 196.0177 - val_loss: 170.6448 - val_MinusLogProbMetric: 170.6448 - lr: 3.3333e-04 - 54s/epoch - 274ms/step
Epoch 4/1000
2023-10-23 12:40:25.114 
Epoch 4/1000 
	 loss: 143.7654, MinusLogProbMetric: 143.7654, val_loss: 121.4803, val_MinusLogProbMetric: 121.4803

Epoch 4: val_loss improved from 170.64476 to 121.48028, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 143.7654 - MinusLogProbMetric: 143.7654 - val_loss: 121.4803 - val_MinusLogProbMetric: 121.4803 - lr: 3.3333e-04 - 60s/epoch - 308ms/step
Epoch 5/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 140: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-23 12:41:07.067 
Epoch 5/1000 
	 loss: nan, MinusLogProbMetric: 187.5094, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 5: val_loss did not improve from 121.48028
196/196 - 41s - loss: nan - MinusLogProbMetric: 187.5094 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 41s/epoch - 210ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0001111111111111111.
===========
Generating train data for run 245.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_245/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_245/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.9518368 , 3.3973382 , 9.531531  , ..., 7.757203  , 2.6203427 ,
        1.8047177 ],
       [2.0608988 , 4.2499633 , 8.220001  , ..., 7.0034    , 2.2546027 ,
        2.1452668 ],
       [2.843429  , 3.9051921 , 7.4207163 , ..., 7.300193  , 3.1696632 ,
        1.5780848 ],
       ...,
       [5.2732286 , 6.077929  , 1.3790838 , ..., 0.65630984, 7.270858  ,
        1.3828237 ],
       [4.184838  , 7.1681747 , 6.2432003 , ..., 4.410938  , 2.659537  ,
        7.4785604 ],
       [5.6083927 , 7.161964  , 6.0947328 , ..., 4.4707136 , 2.6326852 ,
        7.344711  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_245/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_245
self.data_kwargs: {'seed': 0}
self.x_data: [[2.147613   3.0747468  7.824332   ... 7.103659   2.4434035  1.8856683 ]
 [5.330741   5.782131   0.60243964 ... 0.4385597  6.6599607  1.2875487 ]
 [5.9696746  7.1899557  6.848176   ... 3.815116   2.6560376  7.342258  ]
 ...
 [3.6073484  5.5595417  0.9042071  ... 0.7584033  6.576529   1.36466   ]
 [3.793339   5.232353   1.1439581  ... 1.5372672  6.3050704  1.3603225 ]
 [1.2775126  3.7349403  9.479768   ... 7.3904037  2.7995863  1.7972767 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7f1a4c5f3fd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f181033e710>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f181033e710>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1a4c5f39d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1a28235300>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1a28235870>, <keras.callbacks.ModelCheckpoint object at 0x7f1a28235930>, <keras.callbacks.EarlyStopping object at 0x7f1a28235ba0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1a28235bd0>, <keras.callbacks.TerminateOnNaN object at 0x7f1a28235810>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.9518368 , 3.3973382 , 9.531531  , ..., 7.757203  , 2.6203427 ,
        1.8047177 ],
       [2.0608988 , 4.2499633 , 8.220001  , ..., 7.0034    , 2.2546027 ,
        2.1452668 ],
       [2.843429  , 3.9051921 , 7.4207163 , ..., 7.300193  , 3.1696632 ,
        1.5780848 ],
       ...,
       [5.2732286 , 6.077929  , 1.3790838 , ..., 0.65630984, 7.270858  ,
        1.3828237 ],
       [4.184838  , 7.1681747 , 6.2432003 , ..., 4.410938  , 2.659537  ,
        7.4785604 ],
       [5.6083927 , 7.161964  , 6.0947328 , ..., 4.4707136 , 2.6326852 ,
        7.344711  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 245/720 with hyperparameters:
timestamp = 2023-10-23 12:41:13.585402
ndims = 32
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.147613    3.0747468   7.824332    1.8133726   9.163057    0.34882736
  9.789957    4.1820974   9.68468     6.146849    7.005386    0.3680902
  2.6196527   1.2054405   2.7207217   1.3141394   2.9878392   4.696398
 -0.43681622  6.9252343   5.6900735   2.943952    6.358174    1.1145298
  6.7379575   9.321477    3.595148    7.318303    0.9570862   7.103659
  2.4434035   1.8856683 ]
Epoch 1/1000
2023-10-23 12:43:50.440 
Epoch 1/1000 
	 loss: 144.8622, MinusLogProbMetric: 144.8622, val_loss: 123.1590, val_MinusLogProbMetric: 123.1590

Epoch 1: val_loss improved from inf to 123.15900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 157s - loss: 144.8622 - MinusLogProbMetric: 144.8622 - val_loss: 123.1590 - val_MinusLogProbMetric: 123.1590 - lr: 1.1111e-04 - 157s/epoch - 803ms/step
Epoch 2/1000
2023-10-23 12:44:40.153 
Epoch 2/1000 
	 loss: 158.1351, MinusLogProbMetric: 158.1351, val_loss: 100.3784, val_MinusLogProbMetric: 100.3784

Epoch 2: val_loss improved from 123.15900 to 100.37836, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 49s - loss: 158.1351 - MinusLogProbMetric: 158.1351 - val_loss: 100.3784 - val_MinusLogProbMetric: 100.3784 - lr: 1.1111e-04 - 49s/epoch - 253ms/step
Epoch 3/1000
2023-10-23 12:45:30.297 
Epoch 3/1000 
	 loss: 89.3715, MinusLogProbMetric: 89.3715, val_loss: 81.3400, val_MinusLogProbMetric: 81.3400

Epoch 3: val_loss improved from 100.37836 to 81.33996, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 89.3715 - MinusLogProbMetric: 89.3715 - val_loss: 81.3400 - val_MinusLogProbMetric: 81.3400 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 4/1000
2023-10-23 12:46:26.820 
Epoch 4/1000 
	 loss: 88.0412, MinusLogProbMetric: 88.0412, val_loss: 78.9783, val_MinusLogProbMetric: 78.9783

Epoch 4: val_loss improved from 81.33996 to 78.97832, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 57s - loss: 88.0412 - MinusLogProbMetric: 88.0412 - val_loss: 78.9783 - val_MinusLogProbMetric: 78.9783 - lr: 1.1111e-04 - 57s/epoch - 290ms/step
Epoch 5/1000
2023-10-23 12:47:22.084 
Epoch 5/1000 
	 loss: 71.2670, MinusLogProbMetric: 71.2670, val_loss: 67.0104, val_MinusLogProbMetric: 67.0104

Epoch 5: val_loss improved from 78.97832 to 67.01039, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 71.2670 - MinusLogProbMetric: 71.2670 - val_loss: 67.0104 - val_MinusLogProbMetric: 67.0104 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 6/1000
2023-10-23 12:48:16.046 
Epoch 6/1000 
	 loss: 68.9526, MinusLogProbMetric: 68.9526, val_loss: 74.1052, val_MinusLogProbMetric: 74.1052

Epoch 6: val_loss did not improve from 67.01039
196/196 - 53s - loss: 68.9526 - MinusLogProbMetric: 68.9526 - val_loss: 74.1052 - val_MinusLogProbMetric: 74.1052 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 7/1000
2023-10-23 12:49:07.403 
Epoch 7/1000 
	 loss: 68.4250, MinusLogProbMetric: 68.4250, val_loss: 60.1558, val_MinusLogProbMetric: 60.1558

Epoch 7: val_loss improved from 67.01039 to 60.15584, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 68.4250 - MinusLogProbMetric: 68.4250 - val_loss: 60.1558 - val_MinusLogProbMetric: 60.1558 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 8/1000
2023-10-23 12:50:03.585 
Epoch 8/1000 
	 loss: 57.8880, MinusLogProbMetric: 57.8880, val_loss: 55.5279, val_MinusLogProbMetric: 55.5279

Epoch 8: val_loss improved from 60.15584 to 55.52790, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 56s - loss: 57.8880 - MinusLogProbMetric: 57.8880 - val_loss: 55.5279 - val_MinusLogProbMetric: 55.5279 - lr: 1.1111e-04 - 56s/epoch - 287ms/step
Epoch 9/1000
2023-10-23 12:51:00.266 
Epoch 9/1000 
	 loss: 130.2179, MinusLogProbMetric: 130.2179, val_loss: 91.1654, val_MinusLogProbMetric: 91.1654

Epoch 9: val_loss did not improve from 55.52790
196/196 - 56s - loss: 130.2179 - MinusLogProbMetric: 130.2179 - val_loss: 91.1654 - val_MinusLogProbMetric: 91.1654 - lr: 1.1111e-04 - 56s/epoch - 284ms/step
Epoch 10/1000
2023-10-23 12:51:57.035 
Epoch 10/1000 
	 loss: 81.4081, MinusLogProbMetric: 81.4081, val_loss: 69.6348, val_MinusLogProbMetric: 69.6348

Epoch 10: val_loss did not improve from 55.52790
196/196 - 57s - loss: 81.4081 - MinusLogProbMetric: 81.4081 - val_loss: 69.6348 - val_MinusLogProbMetric: 69.6348 - lr: 1.1111e-04 - 57s/epoch - 290ms/step
Epoch 11/1000
2023-10-23 12:52:51.243 
Epoch 11/1000 
	 loss: 73.6974, MinusLogProbMetric: 73.6974, val_loss: 68.1461, val_MinusLogProbMetric: 68.1461

Epoch 11: val_loss did not improve from 55.52790
196/196 - 54s - loss: 73.6974 - MinusLogProbMetric: 73.6974 - val_loss: 68.1461 - val_MinusLogProbMetric: 68.1461 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 12/1000
2023-10-23 12:53:41.076 
Epoch 12/1000 
	 loss: 62.3287, MinusLogProbMetric: 62.3287, val_loss: 58.2220, val_MinusLogProbMetric: 58.2220

Epoch 12: val_loss did not improve from 55.52790
196/196 - 50s - loss: 62.3287 - MinusLogProbMetric: 62.3287 - val_loss: 58.2220 - val_MinusLogProbMetric: 58.2220 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 13/1000
2023-10-23 12:54:35.414 
Epoch 13/1000 
	 loss: 57.1790, MinusLogProbMetric: 57.1790, val_loss: 210.3249, val_MinusLogProbMetric: 210.3249

Epoch 13: val_loss did not improve from 55.52790
196/196 - 54s - loss: 57.1790 - MinusLogProbMetric: 57.1790 - val_loss: 210.3249 - val_MinusLogProbMetric: 210.3249 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 14/1000
2023-10-23 12:55:30.994 
Epoch 14/1000 
	 loss: 129.4529, MinusLogProbMetric: 129.4529, val_loss: 76.7565, val_MinusLogProbMetric: 76.7565

Epoch 14: val_loss did not improve from 55.52790
196/196 - 56s - loss: 129.4529 - MinusLogProbMetric: 129.4529 - val_loss: 76.7565 - val_MinusLogProbMetric: 76.7565 - lr: 1.1111e-04 - 56s/epoch - 284ms/step
Epoch 15/1000
2023-10-23 12:56:24.407 
Epoch 15/1000 
	 loss: 67.4624, MinusLogProbMetric: 67.4624, val_loss: 62.1731, val_MinusLogProbMetric: 62.1731

Epoch 15: val_loss did not improve from 55.52790
196/196 - 53s - loss: 67.4624 - MinusLogProbMetric: 67.4624 - val_loss: 62.1731 - val_MinusLogProbMetric: 62.1731 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 16/1000
2023-10-23 12:57:21.087 
Epoch 16/1000 
	 loss: 66.2432, MinusLogProbMetric: 66.2432, val_loss: 56.2154, val_MinusLogProbMetric: 56.2154

Epoch 16: val_loss did not improve from 55.52790
196/196 - 57s - loss: 66.2432 - MinusLogProbMetric: 66.2432 - val_loss: 56.2154 - val_MinusLogProbMetric: 56.2154 - lr: 1.1111e-04 - 57s/epoch - 289ms/step
Epoch 17/1000
2023-10-23 12:58:19.310 
Epoch 17/1000 
	 loss: 52.2569, MinusLogProbMetric: 52.2569, val_loss: 49.4819, val_MinusLogProbMetric: 49.4819

Epoch 17: val_loss improved from 55.52790 to 49.48190, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 59s - loss: 52.2569 - MinusLogProbMetric: 52.2569 - val_loss: 49.4819 - val_MinusLogProbMetric: 49.4819 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 18/1000
2023-10-23 12:59:11.038 
Epoch 18/1000 
	 loss: 49.7480, MinusLogProbMetric: 49.7480, val_loss: 46.4666, val_MinusLogProbMetric: 46.4666

Epoch 18: val_loss improved from 49.48190 to 46.46664, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 49.7480 - MinusLogProbMetric: 49.7480 - val_loss: 46.4666 - val_MinusLogProbMetric: 46.4666 - lr: 1.1111e-04 - 52s/epoch - 263ms/step
Epoch 19/1000
2023-10-23 13:00:09.957 
Epoch 19/1000 
	 loss: 45.0093, MinusLogProbMetric: 45.0093, val_loss: 43.3292, val_MinusLogProbMetric: 43.3292

Epoch 19: val_loss improved from 46.46664 to 43.32920, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 59s - loss: 45.0093 - MinusLogProbMetric: 45.0093 - val_loss: 43.3292 - val_MinusLogProbMetric: 43.3292 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 20/1000
2023-10-23 13:01:03.893 
Epoch 20/1000 
	 loss: 42.6771, MinusLogProbMetric: 42.6771, val_loss: 42.4305, val_MinusLogProbMetric: 42.4305

Epoch 20: val_loss improved from 43.32920 to 42.43052, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 42.6771 - MinusLogProbMetric: 42.6771 - val_loss: 42.4305 - val_MinusLogProbMetric: 42.4305 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 21/1000
2023-10-23 13:01:56.556 
Epoch 21/1000 
	 loss: 41.1309, MinusLogProbMetric: 41.1309, val_loss: 39.7385, val_MinusLogProbMetric: 39.7385

Epoch 21: val_loss improved from 42.43052 to 39.73851, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 41.1309 - MinusLogProbMetric: 41.1309 - val_loss: 39.7385 - val_MinusLogProbMetric: 39.7385 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 22/1000
2023-10-23 13:02:55.086 
Epoch 22/1000 
	 loss: 56.2768, MinusLogProbMetric: 56.2768, val_loss: 57.3435, val_MinusLogProbMetric: 57.3435

Epoch 22: val_loss did not improve from 39.73851
196/196 - 58s - loss: 56.2768 - MinusLogProbMetric: 56.2768 - val_loss: 57.3435 - val_MinusLogProbMetric: 57.3435 - lr: 1.1111e-04 - 58s/epoch - 294ms/step
Epoch 23/1000
2023-10-23 13:03:43.905 
Epoch 23/1000 
	 loss: 44.8863, MinusLogProbMetric: 44.8863, val_loss: 40.5881, val_MinusLogProbMetric: 40.5881

Epoch 23: val_loss did not improve from 39.73851
196/196 - 49s - loss: 44.8863 - MinusLogProbMetric: 44.8863 - val_loss: 40.5881 - val_MinusLogProbMetric: 40.5881 - lr: 1.1111e-04 - 49s/epoch - 249ms/step
Epoch 24/1000
2023-10-23 13:04:33.325 
Epoch 24/1000 
	 loss: 39.2331, MinusLogProbMetric: 39.2331, val_loss: 37.9811, val_MinusLogProbMetric: 37.9811

Epoch 24: val_loss improved from 39.73851 to 37.98106, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 39.2331 - MinusLogProbMetric: 39.2331 - val_loss: 37.9811 - val_MinusLogProbMetric: 37.9811 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 25/1000
2023-10-23 13:05:25.233 
Epoch 25/1000 
	 loss: 39.5080, MinusLogProbMetric: 39.5080, val_loss: 37.5959, val_MinusLogProbMetric: 37.5959

Epoch 25: val_loss improved from 37.98106 to 37.59594, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 39.5080 - MinusLogProbMetric: 39.5080 - val_loss: 37.5959 - val_MinusLogProbMetric: 37.5959 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 26/1000
2023-10-23 13:06:15.286 
Epoch 26/1000 
	 loss: 91.5140, MinusLogProbMetric: 91.5140, val_loss: 84.4139, val_MinusLogProbMetric: 84.4139

Epoch 26: val_loss did not improve from 37.59594
196/196 - 49s - loss: 91.5140 - MinusLogProbMetric: 91.5140 - val_loss: 84.4139 - val_MinusLogProbMetric: 84.4139 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 27/1000
2023-10-23 13:07:06.732 
Epoch 27/1000 
	 loss: 66.8033, MinusLogProbMetric: 66.8033, val_loss: 55.8287, val_MinusLogProbMetric: 55.8287

Epoch 27: val_loss did not improve from 37.59594
196/196 - 51s - loss: 66.8033 - MinusLogProbMetric: 66.8033 - val_loss: 55.8287 - val_MinusLogProbMetric: 55.8287 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 28/1000
2023-10-23 13:07:56.641 
Epoch 28/1000 
	 loss: 50.9904, MinusLogProbMetric: 50.9904, val_loss: 46.7120, val_MinusLogProbMetric: 46.7120

Epoch 28: val_loss did not improve from 37.59594
196/196 - 50s - loss: 50.9904 - MinusLogProbMetric: 50.9904 - val_loss: 46.7120 - val_MinusLogProbMetric: 46.7120 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 29/1000
2023-10-23 13:08:44.885 
Epoch 29/1000 
	 loss: 44.6913, MinusLogProbMetric: 44.6913, val_loss: 42.5144, val_MinusLogProbMetric: 42.5144

Epoch 29: val_loss did not improve from 37.59594
196/196 - 48s - loss: 44.6913 - MinusLogProbMetric: 44.6913 - val_loss: 42.5144 - val_MinusLogProbMetric: 42.5144 - lr: 1.1111e-04 - 48s/epoch - 246ms/step
Epoch 30/1000
2023-10-23 13:09:38.115 
Epoch 30/1000 
	 loss: 41.5234, MinusLogProbMetric: 41.5234, val_loss: 40.1488, val_MinusLogProbMetric: 40.1488

Epoch 30: val_loss did not improve from 37.59594
196/196 - 53s - loss: 41.5234 - MinusLogProbMetric: 41.5234 - val_loss: 40.1488 - val_MinusLogProbMetric: 40.1488 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 31/1000
2023-10-23 13:10:33.688 
Epoch 31/1000 
	 loss: 39.6025, MinusLogProbMetric: 39.6025, val_loss: 39.3352, val_MinusLogProbMetric: 39.3352

Epoch 31: val_loss did not improve from 37.59594
196/196 - 56s - loss: 39.6025 - MinusLogProbMetric: 39.6025 - val_loss: 39.3352 - val_MinusLogProbMetric: 39.3352 - lr: 1.1111e-04 - 56s/epoch - 284ms/step
Epoch 32/1000
2023-10-23 13:11:24.003 
Epoch 32/1000 
	 loss: 38.0417, MinusLogProbMetric: 38.0417, val_loss: 37.1230, val_MinusLogProbMetric: 37.1230

Epoch 32: val_loss improved from 37.59594 to 37.12298, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 38.0417 - MinusLogProbMetric: 38.0417 - val_loss: 37.1230 - val_MinusLogProbMetric: 37.1230 - lr: 1.1111e-04 - 51s/epoch - 260ms/step
Epoch 33/1000
2023-10-23 13:12:16.910 
Epoch 33/1000 
	 loss: 37.1384, MinusLogProbMetric: 37.1384, val_loss: 37.3964, val_MinusLogProbMetric: 37.3964

Epoch 33: val_loss did not improve from 37.12298
196/196 - 52s - loss: 37.1384 - MinusLogProbMetric: 37.1384 - val_loss: 37.3964 - val_MinusLogProbMetric: 37.3964 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 34/1000
2023-10-23 13:13:15.509 
Epoch 34/1000 
	 loss: 35.9072, MinusLogProbMetric: 35.9072, val_loss: 35.5096, val_MinusLogProbMetric: 35.5096

Epoch 34: val_loss improved from 37.12298 to 35.50964, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 35.9072 - MinusLogProbMetric: 35.9072 - val_loss: 35.5096 - val_MinusLogProbMetric: 35.5096 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 35/1000
2023-10-23 13:14:16.545 
Epoch 35/1000 
	 loss: 35.0378, MinusLogProbMetric: 35.0378, val_loss: 35.9304, val_MinusLogProbMetric: 35.9304

Epoch 35: val_loss did not improve from 35.50964
196/196 - 60s - loss: 35.0378 - MinusLogProbMetric: 35.0378 - val_loss: 35.9304 - val_MinusLogProbMetric: 35.9304 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 36/1000
2023-10-23 13:15:22.560 
Epoch 36/1000 
	 loss: 34.3634, MinusLogProbMetric: 34.3634, val_loss: 33.6255, val_MinusLogProbMetric: 33.6255

Epoch 36: val_loss improved from 35.50964 to 33.62549, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 67s - loss: 34.3634 - MinusLogProbMetric: 34.3634 - val_loss: 33.6255 - val_MinusLogProbMetric: 33.6255 - lr: 1.1111e-04 - 67s/epoch - 342ms/step
Epoch 37/1000
2023-10-23 13:16:26.621 
Epoch 37/1000 
	 loss: 34.3930, MinusLogProbMetric: 34.3930, val_loss: 33.4462, val_MinusLogProbMetric: 33.4462

Epoch 37: val_loss improved from 33.62549 to 33.44620, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 64s - loss: 34.3930 - MinusLogProbMetric: 34.3930 - val_loss: 33.4462 - val_MinusLogProbMetric: 33.4462 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 38/1000
2023-10-23 13:17:25.443 
Epoch 38/1000 
	 loss: 32.9615, MinusLogProbMetric: 32.9615, val_loss: 33.1731, val_MinusLogProbMetric: 33.1731

Epoch 38: val_loss improved from 33.44620 to 33.17306, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 59s - loss: 32.9615 - MinusLogProbMetric: 32.9615 - val_loss: 33.1731 - val_MinusLogProbMetric: 33.1731 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 39/1000
2023-10-23 13:18:30.312 
Epoch 39/1000 
	 loss: 32.3283, MinusLogProbMetric: 32.3283, val_loss: 32.6928, val_MinusLogProbMetric: 32.6928

Epoch 39: val_loss improved from 33.17306 to 32.69280, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 65s - loss: 32.3283 - MinusLogProbMetric: 32.3283 - val_loss: 32.6928 - val_MinusLogProbMetric: 32.6928 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 40/1000
2023-10-23 13:19:29.853 
Epoch 40/1000 
	 loss: 31.7206, MinusLogProbMetric: 31.7206, val_loss: 31.1827, val_MinusLogProbMetric: 31.1827

Epoch 40: val_loss improved from 32.69280 to 31.18267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 31.7206 - MinusLogProbMetric: 31.7206 - val_loss: 31.1827 - val_MinusLogProbMetric: 31.1827 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 41/1000
2023-10-23 13:20:21.523 
Epoch 41/1000 
	 loss: 31.0996, MinusLogProbMetric: 31.0996, val_loss: 30.7439, val_MinusLogProbMetric: 30.7439

Epoch 41: val_loss improved from 31.18267 to 30.74392, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 31.0996 - MinusLogProbMetric: 31.0996 - val_loss: 30.7439 - val_MinusLogProbMetric: 30.7439 - lr: 1.1111e-04 - 52s/epoch - 263ms/step
Epoch 42/1000
2023-10-23 13:21:14.668 
Epoch 42/1000 
	 loss: 30.4538, MinusLogProbMetric: 30.4538, val_loss: 31.4342, val_MinusLogProbMetric: 31.4342

Epoch 42: val_loss did not improve from 30.74392
196/196 - 52s - loss: 30.4538 - MinusLogProbMetric: 30.4538 - val_loss: 31.4342 - val_MinusLogProbMetric: 31.4342 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 43/1000
2023-10-23 13:22:18.190 
Epoch 43/1000 
	 loss: 30.0917, MinusLogProbMetric: 30.0917, val_loss: 30.1120, val_MinusLogProbMetric: 30.1120

Epoch 43: val_loss improved from 30.74392 to 30.11201, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 64s - loss: 30.0917 - MinusLogProbMetric: 30.0917 - val_loss: 30.1120 - val_MinusLogProbMetric: 30.1120 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 44/1000
2023-10-23 13:23:17.491 
Epoch 44/1000 
	 loss: 29.7483, MinusLogProbMetric: 29.7483, val_loss: 29.5627, val_MinusLogProbMetric: 29.5627

Epoch 44: val_loss improved from 30.11201 to 29.56266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 59s - loss: 29.7483 - MinusLogProbMetric: 29.7483 - val_loss: 29.5627 - val_MinusLogProbMetric: 29.5627 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 45/1000
2023-10-23 13:24:14.947 
Epoch 45/1000 
	 loss: 29.3725, MinusLogProbMetric: 29.3725, val_loss: 29.6544, val_MinusLogProbMetric: 29.6544

Epoch 45: val_loss did not improve from 29.56266
196/196 - 57s - loss: 29.3725 - MinusLogProbMetric: 29.3725 - val_loss: 29.6544 - val_MinusLogProbMetric: 29.6544 - lr: 1.1111e-04 - 57s/epoch - 289ms/step
Epoch 46/1000
2023-10-23 13:25:14.624 
Epoch 46/1000 
	 loss: 29.0305, MinusLogProbMetric: 29.0305, val_loss: 28.9674, val_MinusLogProbMetric: 28.9674

Epoch 46: val_loss improved from 29.56266 to 28.96743, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 29.0305 - MinusLogProbMetric: 29.0305 - val_loss: 28.9674 - val_MinusLogProbMetric: 28.9674 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 47/1000
2023-10-23 13:26:16.326 
Epoch 47/1000 
	 loss: 28.6779, MinusLogProbMetric: 28.6779, val_loss: 28.4482, val_MinusLogProbMetric: 28.4482

Epoch 47: val_loss improved from 28.96743 to 28.44817, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 62s - loss: 28.6779 - MinusLogProbMetric: 28.6779 - val_loss: 28.4482 - val_MinusLogProbMetric: 28.4482 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 48/1000
2023-10-23 13:27:17.108 
Epoch 48/1000 
	 loss: 28.5697, MinusLogProbMetric: 28.5697, val_loss: 28.5085, val_MinusLogProbMetric: 28.5085

Epoch 48: val_loss did not improve from 28.44817
196/196 - 60s - loss: 28.5697 - MinusLogProbMetric: 28.5697 - val_loss: 28.5085 - val_MinusLogProbMetric: 28.5085 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 49/1000
2023-10-23 13:28:15.701 
Epoch 49/1000 
	 loss: 28.1701, MinusLogProbMetric: 28.1701, val_loss: 28.5095, val_MinusLogProbMetric: 28.5095

Epoch 49: val_loss did not improve from 28.44817
196/196 - 59s - loss: 28.1701 - MinusLogProbMetric: 28.1701 - val_loss: 28.5095 - val_MinusLogProbMetric: 28.5095 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 50/1000
2023-10-23 13:29:20.210 
Epoch 50/1000 
	 loss: 27.9973, MinusLogProbMetric: 27.9973, val_loss: 27.8867, val_MinusLogProbMetric: 27.8867

Epoch 50: val_loss improved from 28.44817 to 27.88669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 65s - loss: 27.9973 - MinusLogProbMetric: 27.9973 - val_loss: 27.8867 - val_MinusLogProbMetric: 27.8867 - lr: 1.1111e-04 - 65s/epoch - 334ms/step
Epoch 51/1000
2023-10-23 13:30:20.466 
Epoch 51/1000 
	 loss: 27.6887, MinusLogProbMetric: 27.6887, val_loss: 27.4771, val_MinusLogProbMetric: 27.4771

Epoch 51: val_loss improved from 27.88669 to 27.47705, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 27.6887 - MinusLogProbMetric: 27.6887 - val_loss: 27.4771 - val_MinusLogProbMetric: 27.4771 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 52/1000
2023-10-23 13:31:20.583 
Epoch 52/1000 
	 loss: 27.4475, MinusLogProbMetric: 27.4475, val_loss: 27.3886, val_MinusLogProbMetric: 27.3886

Epoch 52: val_loss improved from 27.47705 to 27.38861, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 27.4475 - MinusLogProbMetric: 27.4475 - val_loss: 27.3886 - val_MinusLogProbMetric: 27.3886 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 53/1000
2023-10-23 13:32:17.603 
Epoch 53/1000 
	 loss: 32.4113, MinusLogProbMetric: 32.4113, val_loss: 28.3735, val_MinusLogProbMetric: 28.3735

Epoch 53: val_loss did not improve from 27.38861
196/196 - 56s - loss: 32.4113 - MinusLogProbMetric: 32.4113 - val_loss: 28.3735 - val_MinusLogProbMetric: 28.3735 - lr: 1.1111e-04 - 56s/epoch - 287ms/step
Epoch 54/1000
2023-10-23 13:33:10.259 
Epoch 54/1000 
	 loss: 27.8186, MinusLogProbMetric: 27.8186, val_loss: 27.7253, val_MinusLogProbMetric: 27.7253

Epoch 54: val_loss did not improve from 27.38861
196/196 - 53s - loss: 27.8186 - MinusLogProbMetric: 27.8186 - val_loss: 27.7253 - val_MinusLogProbMetric: 27.7253 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 55/1000
2023-10-23 13:34:05.326 
Epoch 55/1000 
	 loss: 27.3503, MinusLogProbMetric: 27.3503, val_loss: 27.1658, val_MinusLogProbMetric: 27.1658

Epoch 55: val_loss improved from 27.38861 to 27.16575, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 56s - loss: 27.3503 - MinusLogProbMetric: 27.3503 - val_loss: 27.1658 - val_MinusLogProbMetric: 27.1658 - lr: 1.1111e-04 - 56s/epoch - 285ms/step
Epoch 56/1000
2023-10-23 13:35:01.545 
Epoch 56/1000 
	 loss: 26.8497, MinusLogProbMetric: 26.8497, val_loss: 26.7555, val_MinusLogProbMetric: 26.7555

Epoch 56: val_loss improved from 27.16575 to 26.75547, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 56s - loss: 26.8497 - MinusLogProbMetric: 26.8497 - val_loss: 26.7555 - val_MinusLogProbMetric: 26.7555 - lr: 1.1111e-04 - 56s/epoch - 287ms/step
Epoch 57/1000
2023-10-23 13:36:02.319 
Epoch 57/1000 
	 loss: 26.5443, MinusLogProbMetric: 26.5443, val_loss: 26.6353, val_MinusLogProbMetric: 26.6353

Epoch 57: val_loss improved from 26.75547 to 26.63527, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 61s - loss: 26.5443 - MinusLogProbMetric: 26.5443 - val_loss: 26.6353 - val_MinusLogProbMetric: 26.6353 - lr: 1.1111e-04 - 61s/epoch - 311ms/step
Epoch 58/1000
2023-10-23 13:37:01.365 
Epoch 58/1000 
	 loss: 26.2553, MinusLogProbMetric: 26.2553, val_loss: 26.3655, val_MinusLogProbMetric: 26.3655

Epoch 58: val_loss improved from 26.63527 to 26.36552, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 59s - loss: 26.2553 - MinusLogProbMetric: 26.2553 - val_loss: 26.3655 - val_MinusLogProbMetric: 26.3655 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 59/1000
2023-10-23 13:37:56.346 
Epoch 59/1000 
	 loss: 25.9637, MinusLogProbMetric: 25.9637, val_loss: 25.8235, val_MinusLogProbMetric: 25.8235

Epoch 59: val_loss improved from 26.36552 to 25.82350, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 25.9637 - MinusLogProbMetric: 25.9637 - val_loss: 25.8235 - val_MinusLogProbMetric: 25.8235 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 60/1000
2023-10-23 13:38:49.518 
Epoch 60/1000 
	 loss: 25.8146, MinusLogProbMetric: 25.8146, val_loss: 25.7672, val_MinusLogProbMetric: 25.7672

Epoch 60: val_loss improved from 25.82350 to 25.76721, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 25.8146 - MinusLogProbMetric: 25.8146 - val_loss: 25.7672 - val_MinusLogProbMetric: 25.7672 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 61/1000
2023-10-23 13:39:43.763 
Epoch 61/1000 
	 loss: 25.5388, MinusLogProbMetric: 25.5388, val_loss: 25.3546, val_MinusLogProbMetric: 25.3546

Epoch 61: val_loss improved from 25.76721 to 25.35461, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 25.5388 - MinusLogProbMetric: 25.5388 - val_loss: 25.3546 - val_MinusLogProbMetric: 25.3546 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 62/1000
2023-10-23 13:40:39.356 
Epoch 62/1000 
	 loss: 25.3176, MinusLogProbMetric: 25.3176, val_loss: 25.4837, val_MinusLogProbMetric: 25.4837

Epoch 62: val_loss did not improve from 25.35461
196/196 - 55s - loss: 25.3176 - MinusLogProbMetric: 25.3176 - val_loss: 25.4837 - val_MinusLogProbMetric: 25.4837 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 63/1000
2023-10-23 13:41:37.071 
Epoch 63/1000 
	 loss: 25.1667, MinusLogProbMetric: 25.1667, val_loss: 25.8267, val_MinusLogProbMetric: 25.8267

Epoch 63: val_loss did not improve from 25.35461
196/196 - 58s - loss: 25.1667 - MinusLogProbMetric: 25.1667 - val_loss: 25.8267 - val_MinusLogProbMetric: 25.8267 - lr: 1.1111e-04 - 58s/epoch - 294ms/step
Epoch 64/1000
2023-10-23 13:42:39.524 
Epoch 64/1000 
	 loss: 25.0267, MinusLogProbMetric: 25.0267, val_loss: 25.7369, val_MinusLogProbMetric: 25.7369

Epoch 64: val_loss did not improve from 25.35461
196/196 - 62s - loss: 25.0267 - MinusLogProbMetric: 25.0267 - val_loss: 25.7369 - val_MinusLogProbMetric: 25.7369 - lr: 1.1111e-04 - 62s/epoch - 319ms/step
Epoch 65/1000
2023-10-23 13:43:38.775 
Epoch 65/1000 
	 loss: 24.8026, MinusLogProbMetric: 24.8026, val_loss: 24.8845, val_MinusLogProbMetric: 24.8845

Epoch 65: val_loss improved from 25.35461 to 24.88448, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 24.8026 - MinusLogProbMetric: 24.8026 - val_loss: 24.8845 - val_MinusLogProbMetric: 24.8845 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 66/1000
2023-10-23 13:44:36.890 
Epoch 66/1000 
	 loss: 24.6576, MinusLogProbMetric: 24.6576, val_loss: 24.9272, val_MinusLogProbMetric: 24.9272

Epoch 66: val_loss did not improve from 24.88448
196/196 - 57s - loss: 24.6576 - MinusLogProbMetric: 24.6576 - val_loss: 24.9272 - val_MinusLogProbMetric: 24.9272 - lr: 1.1111e-04 - 57s/epoch - 292ms/step
Epoch 67/1000
2023-10-23 13:45:40.837 
Epoch 67/1000 
	 loss: 24.5022, MinusLogProbMetric: 24.5022, val_loss: 24.8424, val_MinusLogProbMetric: 24.8424

Epoch 67: val_loss improved from 24.88448 to 24.84241, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 65s - loss: 24.5022 - MinusLogProbMetric: 24.5022 - val_loss: 24.8424 - val_MinusLogProbMetric: 24.8424 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 68/1000
2023-10-23 13:46:35.090 
Epoch 68/1000 
	 loss: 24.4234, MinusLogProbMetric: 24.4234, val_loss: 24.4680, val_MinusLogProbMetric: 24.4680

Epoch 68: val_loss improved from 24.84241 to 24.46800, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 24.4234 - MinusLogProbMetric: 24.4234 - val_loss: 24.4680 - val_MinusLogProbMetric: 24.4680 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 69/1000
2023-10-23 13:47:30.341 
Epoch 69/1000 
	 loss: 24.3338, MinusLogProbMetric: 24.3338, val_loss: 24.8321, val_MinusLogProbMetric: 24.8321

Epoch 69: val_loss did not improve from 24.46800
196/196 - 54s - loss: 24.3338 - MinusLogProbMetric: 24.3338 - val_loss: 24.8321 - val_MinusLogProbMetric: 24.8321 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 70/1000
2023-10-23 13:48:28.488 
Epoch 70/1000 
	 loss: 24.1695, MinusLogProbMetric: 24.1695, val_loss: 24.0669, val_MinusLogProbMetric: 24.0669

Epoch 70: val_loss improved from 24.46800 to 24.06690, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 59s - loss: 24.1695 - MinusLogProbMetric: 24.1695 - val_loss: 24.0669 - val_MinusLogProbMetric: 24.0669 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 71/1000
2023-10-23 13:49:30.302 
Epoch 71/1000 
	 loss: 24.0977, MinusLogProbMetric: 24.0977, val_loss: 24.0329, val_MinusLogProbMetric: 24.0329

Epoch 71: val_loss improved from 24.06690 to 24.03288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 62s - loss: 24.0977 - MinusLogProbMetric: 24.0977 - val_loss: 24.0329 - val_MinusLogProbMetric: 24.0329 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 72/1000
2023-10-23 13:50:34.809 
Epoch 72/1000 
	 loss: 23.9993, MinusLogProbMetric: 23.9993, val_loss: 23.8706, val_MinusLogProbMetric: 23.8706

Epoch 72: val_loss improved from 24.03288 to 23.87059, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 64s - loss: 23.9993 - MinusLogProbMetric: 23.9993 - val_loss: 23.8706 - val_MinusLogProbMetric: 23.8706 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 73/1000
2023-10-23 13:51:40.281 
Epoch 73/1000 
	 loss: 23.8244, MinusLogProbMetric: 23.8244, val_loss: 23.9855, val_MinusLogProbMetric: 23.9855

Epoch 73: val_loss did not improve from 23.87059
196/196 - 65s - loss: 23.8244 - MinusLogProbMetric: 23.8244 - val_loss: 23.9855 - val_MinusLogProbMetric: 23.9855 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 74/1000
2023-10-23 13:52:37.456 
Epoch 74/1000 
	 loss: 23.7432, MinusLogProbMetric: 23.7432, val_loss: 23.6050, val_MinusLogProbMetric: 23.6050

Epoch 74: val_loss improved from 23.87059 to 23.60496, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 58s - loss: 23.7432 - MinusLogProbMetric: 23.7432 - val_loss: 23.6050 - val_MinusLogProbMetric: 23.6050 - lr: 1.1111e-04 - 58s/epoch - 295ms/step
Epoch 75/1000
2023-10-23 13:53:38.006 
Epoch 75/1000 
	 loss: 23.5594, MinusLogProbMetric: 23.5594, val_loss: 23.4587, val_MinusLogProbMetric: 23.4587

Epoch 75: val_loss improved from 23.60496 to 23.45875, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 61s - loss: 23.5594 - MinusLogProbMetric: 23.5594 - val_loss: 23.4587 - val_MinusLogProbMetric: 23.4587 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 76/1000
2023-10-23 13:54:38.720 
Epoch 76/1000 
	 loss: 46.6260, MinusLogProbMetric: 46.6260, val_loss: 30.2748, val_MinusLogProbMetric: 30.2748

Epoch 76: val_loss did not improve from 23.45875
196/196 - 60s - loss: 46.6260 - MinusLogProbMetric: 46.6260 - val_loss: 30.2748 - val_MinusLogProbMetric: 30.2748 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 77/1000
2023-10-23 13:55:41.351 
Epoch 77/1000 
	 loss: 28.5186, MinusLogProbMetric: 28.5186, val_loss: 27.4474, val_MinusLogProbMetric: 27.4474

Epoch 77: val_loss did not improve from 23.45875
196/196 - 63s - loss: 28.5186 - MinusLogProbMetric: 28.5186 - val_loss: 27.4474 - val_MinusLogProbMetric: 27.4474 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 78/1000
2023-10-23 13:56:43.597 
Epoch 78/1000 
	 loss: 26.7325, MinusLogProbMetric: 26.7325, val_loss: 26.3995, val_MinusLogProbMetric: 26.3995

Epoch 78: val_loss did not improve from 23.45875
196/196 - 62s - loss: 26.7325 - MinusLogProbMetric: 26.7325 - val_loss: 26.3995 - val_MinusLogProbMetric: 26.3995 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 79/1000
2023-10-23 13:57:46.259 
Epoch 79/1000 
	 loss: 25.7974, MinusLogProbMetric: 25.7974, val_loss: 25.5323, val_MinusLogProbMetric: 25.5323

Epoch 79: val_loss did not improve from 23.45875
196/196 - 63s - loss: 25.7974 - MinusLogProbMetric: 25.7974 - val_loss: 25.5323 - val_MinusLogProbMetric: 25.5323 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 80/1000
2023-10-23 13:58:48.576 
Epoch 80/1000 
	 loss: 25.2044, MinusLogProbMetric: 25.2044, val_loss: 25.1191, val_MinusLogProbMetric: 25.1191

Epoch 80: val_loss did not improve from 23.45875
196/196 - 62s - loss: 25.2044 - MinusLogProbMetric: 25.2044 - val_loss: 25.1191 - val_MinusLogProbMetric: 25.1191 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 81/1000
2023-10-23 13:59:48.422 
Epoch 81/1000 
	 loss: 24.7684, MinusLogProbMetric: 24.7684, val_loss: 24.7483, val_MinusLogProbMetric: 24.7483

Epoch 81: val_loss did not improve from 23.45875
196/196 - 60s - loss: 24.7684 - MinusLogProbMetric: 24.7684 - val_loss: 24.7483 - val_MinusLogProbMetric: 24.7483 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 82/1000
2023-10-23 14:00:49.513 
Epoch 82/1000 
	 loss: 24.4839, MinusLogProbMetric: 24.4839, val_loss: 24.5600, val_MinusLogProbMetric: 24.5600

Epoch 82: val_loss did not improve from 23.45875
196/196 - 61s - loss: 24.4839 - MinusLogProbMetric: 24.4839 - val_loss: 24.5600 - val_MinusLogProbMetric: 24.5600 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 83/1000
2023-10-23 14:01:49.597 
Epoch 83/1000 
	 loss: 24.2143, MinusLogProbMetric: 24.2143, val_loss: 24.4564, val_MinusLogProbMetric: 24.4564

Epoch 83: val_loss did not improve from 23.45875
196/196 - 60s - loss: 24.2143 - MinusLogProbMetric: 24.2143 - val_loss: 24.4564 - val_MinusLogProbMetric: 24.4564 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 84/1000
2023-10-23 14:02:50.681 
Epoch 84/1000 
	 loss: 24.0621, MinusLogProbMetric: 24.0621, val_loss: 23.9633, val_MinusLogProbMetric: 23.9633

Epoch 84: val_loss did not improve from 23.45875
196/196 - 61s - loss: 24.0621 - MinusLogProbMetric: 24.0621 - val_loss: 23.9633 - val_MinusLogProbMetric: 23.9633 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 85/1000
2023-10-23 14:03:47.751 
Epoch 85/1000 
	 loss: 23.8659, MinusLogProbMetric: 23.8659, val_loss: 24.0411, val_MinusLogProbMetric: 24.0411

Epoch 85: val_loss did not improve from 23.45875
196/196 - 57s - loss: 23.8659 - MinusLogProbMetric: 23.8659 - val_loss: 24.0411 - val_MinusLogProbMetric: 24.0411 - lr: 1.1111e-04 - 57s/epoch - 291ms/step
Epoch 86/1000
2023-10-23 14:04:49.077 
Epoch 86/1000 
	 loss: 23.6928, MinusLogProbMetric: 23.6928, val_loss: 23.8787, val_MinusLogProbMetric: 23.8787

Epoch 86: val_loss did not improve from 23.45875
196/196 - 61s - loss: 23.6928 - MinusLogProbMetric: 23.6928 - val_loss: 23.8787 - val_MinusLogProbMetric: 23.8787 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 87/1000
2023-10-23 14:05:50.968 
Epoch 87/1000 
	 loss: 23.5088, MinusLogProbMetric: 23.5088, val_loss: 23.6592, val_MinusLogProbMetric: 23.6592

Epoch 87: val_loss did not improve from 23.45875
196/196 - 62s - loss: 23.5088 - MinusLogProbMetric: 23.5088 - val_loss: 23.6592 - val_MinusLogProbMetric: 23.6592 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 88/1000
2023-10-23 14:06:52.691 
Epoch 88/1000 
	 loss: 23.3869, MinusLogProbMetric: 23.3869, val_loss: 23.6467, val_MinusLogProbMetric: 23.6467

Epoch 88: val_loss did not improve from 23.45875
196/196 - 62s - loss: 23.3869 - MinusLogProbMetric: 23.3869 - val_loss: 23.6467 - val_MinusLogProbMetric: 23.6467 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 89/1000
2023-10-23 14:07:49.246 
Epoch 89/1000 
	 loss: 23.2543, MinusLogProbMetric: 23.2543, val_loss: 23.2084, val_MinusLogProbMetric: 23.2084

Epoch 89: val_loss improved from 23.45875 to 23.20843, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 57s - loss: 23.2543 - MinusLogProbMetric: 23.2543 - val_loss: 23.2084 - val_MinusLogProbMetric: 23.2084 - lr: 1.1111e-04 - 57s/epoch - 293ms/step
Epoch 90/1000
2023-10-23 14:08:49.002 
Epoch 90/1000 
	 loss: 23.0331, MinusLogProbMetric: 23.0331, val_loss: 22.9781, val_MinusLogProbMetric: 22.9781

Epoch 90: val_loss improved from 23.20843 to 22.97812, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 23.0331 - MinusLogProbMetric: 23.0331 - val_loss: 22.9781 - val_MinusLogProbMetric: 22.9781 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 91/1000
2023-10-23 14:09:50.780 
Epoch 91/1000 
	 loss: 22.9675, MinusLogProbMetric: 22.9675, val_loss: 23.3882, val_MinusLogProbMetric: 23.3882

Epoch 91: val_loss did not improve from 22.97812
196/196 - 61s - loss: 22.9675 - MinusLogProbMetric: 22.9675 - val_loss: 23.3882 - val_MinusLogProbMetric: 23.3882 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 92/1000
2023-10-23 14:10:51.584 
Epoch 92/1000 
	 loss: 22.8551, MinusLogProbMetric: 22.8551, val_loss: 22.6697, val_MinusLogProbMetric: 22.6697

Epoch 92: val_loss improved from 22.97812 to 22.66967, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 62s - loss: 22.8551 - MinusLogProbMetric: 22.8551 - val_loss: 22.6697 - val_MinusLogProbMetric: 22.6697 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 93/1000
2023-10-23 14:11:52.867 
Epoch 93/1000 
	 loss: 22.7500, MinusLogProbMetric: 22.7500, val_loss: 22.9723, val_MinusLogProbMetric: 22.9723

Epoch 93: val_loss did not improve from 22.66967
196/196 - 60s - loss: 22.7500 - MinusLogProbMetric: 22.7500 - val_loss: 22.9723 - val_MinusLogProbMetric: 22.9723 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 94/1000
2023-10-23 14:12:55.562 
Epoch 94/1000 
	 loss: 22.6356, MinusLogProbMetric: 22.6356, val_loss: 22.8599, val_MinusLogProbMetric: 22.8599

Epoch 94: val_loss did not improve from 22.66967
196/196 - 63s - loss: 22.6356 - MinusLogProbMetric: 22.6356 - val_loss: 22.8599 - val_MinusLogProbMetric: 22.8599 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 95/1000
2023-10-23 14:13:59.576 
Epoch 95/1000 
	 loss: 22.5088, MinusLogProbMetric: 22.5088, val_loss: 22.7010, val_MinusLogProbMetric: 22.7010

Epoch 95: val_loss did not improve from 22.66967
196/196 - 64s - loss: 22.5088 - MinusLogProbMetric: 22.5088 - val_loss: 22.7010 - val_MinusLogProbMetric: 22.7010 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 96/1000
2023-10-23 14:15:03.587 
Epoch 96/1000 
	 loss: 22.4376, MinusLogProbMetric: 22.4376, val_loss: 22.3023, val_MinusLogProbMetric: 22.3023

Epoch 96: val_loss improved from 22.66967 to 22.30231, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 65s - loss: 22.4376 - MinusLogProbMetric: 22.4376 - val_loss: 22.3023 - val_MinusLogProbMetric: 22.3023 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 97/1000
2023-10-23 14:16:02.968 
Epoch 97/1000 
	 loss: 22.3343, MinusLogProbMetric: 22.3343, val_loss: 22.3440, val_MinusLogProbMetric: 22.3440

Epoch 97: val_loss did not improve from 22.30231
196/196 - 59s - loss: 22.3343 - MinusLogProbMetric: 22.3343 - val_loss: 22.3440 - val_MinusLogProbMetric: 22.3440 - lr: 1.1111e-04 - 59s/epoch - 298ms/step
Epoch 98/1000
2023-10-23 14:16:59.535 
Epoch 98/1000 
	 loss: 22.1805, MinusLogProbMetric: 22.1805, val_loss: 22.3757, val_MinusLogProbMetric: 22.3757

Epoch 98: val_loss did not improve from 22.30231
196/196 - 57s - loss: 22.1805 - MinusLogProbMetric: 22.1805 - val_loss: 22.3757 - val_MinusLogProbMetric: 22.3757 - lr: 1.1111e-04 - 57s/epoch - 289ms/step
Epoch 99/1000
2023-10-23 14:17:55.970 
Epoch 99/1000 
	 loss: 22.1514, MinusLogProbMetric: 22.1514, val_loss: 22.3469, val_MinusLogProbMetric: 22.3469

Epoch 99: val_loss did not improve from 22.30231
196/196 - 56s - loss: 22.1514 - MinusLogProbMetric: 22.1514 - val_loss: 22.3469 - val_MinusLogProbMetric: 22.3469 - lr: 1.1111e-04 - 56s/epoch - 288ms/step
Epoch 100/1000
2023-10-23 14:18:51.869 
Epoch 100/1000 
	 loss: 22.0355, MinusLogProbMetric: 22.0355, val_loss: 21.9536, val_MinusLogProbMetric: 21.9536

Epoch 100: val_loss improved from 22.30231 to 21.95361, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 57s - loss: 22.0355 - MinusLogProbMetric: 22.0355 - val_loss: 21.9536 - val_MinusLogProbMetric: 21.9536 - lr: 1.1111e-04 - 57s/epoch - 289ms/step
Epoch 101/1000
2023-10-23 14:19:48.094 
Epoch 101/1000 
	 loss: 21.9584, MinusLogProbMetric: 21.9584, val_loss: 21.9745, val_MinusLogProbMetric: 21.9745

Epoch 101: val_loss did not improve from 21.95361
196/196 - 55s - loss: 21.9584 - MinusLogProbMetric: 21.9584 - val_loss: 21.9745 - val_MinusLogProbMetric: 21.9745 - lr: 1.1111e-04 - 55s/epoch - 283ms/step
Epoch 102/1000
2023-10-23 14:20:47.073 
Epoch 102/1000 
	 loss: 21.9688, MinusLogProbMetric: 21.9688, val_loss: 22.0455, val_MinusLogProbMetric: 22.0455

Epoch 102: val_loss did not improve from 21.95361
196/196 - 59s - loss: 21.9688 - MinusLogProbMetric: 21.9688 - val_loss: 22.0455 - val_MinusLogProbMetric: 22.0455 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 103/1000
2023-10-23 14:21:46.286 
Epoch 103/1000 
	 loss: 21.8136, MinusLogProbMetric: 21.8136, val_loss: 22.0608, val_MinusLogProbMetric: 22.0608

Epoch 103: val_loss did not improve from 21.95361
196/196 - 59s - loss: 21.8136 - MinusLogProbMetric: 21.8136 - val_loss: 22.0608 - val_MinusLogProbMetric: 22.0608 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 104/1000
2023-10-23 14:22:46.052 
Epoch 104/1000 
	 loss: 21.8005, MinusLogProbMetric: 21.8005, val_loss: 21.7513, val_MinusLogProbMetric: 21.7513

Epoch 104: val_loss improved from 21.95361 to 21.75134, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 21.8005 - MinusLogProbMetric: 21.8005 - val_loss: 21.7513 - val_MinusLogProbMetric: 21.7513 - lr: 1.1111e-04 - 60s/epoch - 309ms/step
Epoch 105/1000
2023-10-23 14:23:43.530 
Epoch 105/1000 
	 loss: 21.6542, MinusLogProbMetric: 21.6542, val_loss: 22.1960, val_MinusLogProbMetric: 22.1960

Epoch 105: val_loss did not improve from 21.75134
196/196 - 57s - loss: 21.6542 - MinusLogProbMetric: 21.6542 - val_loss: 22.1960 - val_MinusLogProbMetric: 22.1960 - lr: 1.1111e-04 - 57s/epoch - 289ms/step
Epoch 106/1000
2023-10-23 14:24:35.400 
Epoch 106/1000 
	 loss: 21.7356, MinusLogProbMetric: 21.7356, val_loss: 21.9427, val_MinusLogProbMetric: 21.9427

Epoch 106: val_loss did not improve from 21.75134
196/196 - 52s - loss: 21.7356 - MinusLogProbMetric: 21.7356 - val_loss: 21.9427 - val_MinusLogProbMetric: 21.9427 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 107/1000
2023-10-23 14:25:29.622 
Epoch 107/1000 
	 loss: 21.5759, MinusLogProbMetric: 21.5759, val_loss: 21.6529, val_MinusLogProbMetric: 21.6529

Epoch 107: val_loss improved from 21.75134 to 21.65295, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 21.5759 - MinusLogProbMetric: 21.5759 - val_loss: 21.6529 - val_MinusLogProbMetric: 21.6529 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 108/1000
2023-10-23 14:26:25.440 
Epoch 108/1000 
	 loss: 21.5306, MinusLogProbMetric: 21.5306, val_loss: 21.4853, val_MinusLogProbMetric: 21.4853

Epoch 108: val_loss improved from 21.65295 to 21.48526, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 56s - loss: 21.5306 - MinusLogProbMetric: 21.5306 - val_loss: 21.4853 - val_MinusLogProbMetric: 21.4853 - lr: 1.1111e-04 - 56s/epoch - 284ms/step
Epoch 109/1000
2023-10-23 14:27:19.598 
Epoch 109/1000 
	 loss: 21.5029, MinusLogProbMetric: 21.5029, val_loss: 21.7104, val_MinusLogProbMetric: 21.7104

Epoch 109: val_loss did not improve from 21.48526
196/196 - 53s - loss: 21.5029 - MinusLogProbMetric: 21.5029 - val_loss: 21.7104 - val_MinusLogProbMetric: 21.7104 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 110/1000
2023-10-23 14:28:18.386 
Epoch 110/1000 
	 loss: 21.4946, MinusLogProbMetric: 21.4946, val_loss: 21.5220, val_MinusLogProbMetric: 21.5220

Epoch 110: val_loss did not improve from 21.48526
196/196 - 59s - loss: 21.4946 - MinusLogProbMetric: 21.4946 - val_loss: 21.5220 - val_MinusLogProbMetric: 21.5220 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 111/1000
2023-10-23 14:29:20.271 
Epoch 111/1000 
	 loss: 21.3992, MinusLogProbMetric: 21.3992, val_loss: 21.3948, val_MinusLogProbMetric: 21.3948

Epoch 111: val_loss improved from 21.48526 to 21.39476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 63s - loss: 21.3992 - MinusLogProbMetric: 21.3992 - val_loss: 21.3948 - val_MinusLogProbMetric: 21.3948 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 112/1000
2023-10-23 14:30:23.303 
Epoch 112/1000 
	 loss: 21.3456, MinusLogProbMetric: 21.3456, val_loss: 21.6430, val_MinusLogProbMetric: 21.6430

Epoch 112: val_loss did not improve from 21.39476
196/196 - 62s - loss: 21.3456 - MinusLogProbMetric: 21.3456 - val_loss: 21.6430 - val_MinusLogProbMetric: 21.6430 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 113/1000
2023-10-23 14:31:21.171 
Epoch 113/1000 
	 loss: 21.2990, MinusLogProbMetric: 21.2990, val_loss: 21.3092, val_MinusLogProbMetric: 21.3092

Epoch 113: val_loss improved from 21.39476 to 21.30917, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 59s - loss: 21.2990 - MinusLogProbMetric: 21.2990 - val_loss: 21.3092 - val_MinusLogProbMetric: 21.3092 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 114/1000
2023-10-23 14:32:18.328 
Epoch 114/1000 
	 loss: 21.2586, MinusLogProbMetric: 21.2586, val_loss: 21.2582, val_MinusLogProbMetric: 21.2582

Epoch 114: val_loss improved from 21.30917 to 21.25818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 57s - loss: 21.2586 - MinusLogProbMetric: 21.2586 - val_loss: 21.2582 - val_MinusLogProbMetric: 21.2582 - lr: 1.1111e-04 - 57s/epoch - 291ms/step
Epoch 115/1000
2023-10-23 14:33:15.554 
Epoch 115/1000 
	 loss: 21.2081, MinusLogProbMetric: 21.2081, val_loss: 21.1898, val_MinusLogProbMetric: 21.1898

Epoch 115: val_loss improved from 21.25818 to 21.18981, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 57s - loss: 21.2081 - MinusLogProbMetric: 21.2081 - val_loss: 21.1898 - val_MinusLogProbMetric: 21.1898 - lr: 1.1111e-04 - 57s/epoch - 292ms/step
Epoch 116/1000
2023-10-23 14:34:14.184 
Epoch 116/1000 
	 loss: 21.0832, MinusLogProbMetric: 21.0832, val_loss: 21.2386, val_MinusLogProbMetric: 21.2386

Epoch 116: val_loss did not improve from 21.18981
196/196 - 58s - loss: 21.0832 - MinusLogProbMetric: 21.0832 - val_loss: 21.2386 - val_MinusLogProbMetric: 21.2386 - lr: 1.1111e-04 - 58s/epoch - 295ms/step
Epoch 117/1000
2023-10-23 14:35:10.347 
Epoch 117/1000 
	 loss: 21.1892, MinusLogProbMetric: 21.1892, val_loss: 21.2674, val_MinusLogProbMetric: 21.2674

Epoch 117: val_loss did not improve from 21.18981
196/196 - 56s - loss: 21.1892 - MinusLogProbMetric: 21.1892 - val_loss: 21.2674 - val_MinusLogProbMetric: 21.2674 - lr: 1.1111e-04 - 56s/epoch - 287ms/step
Epoch 118/1000
2023-10-23 14:36:03.120 
Epoch 118/1000 
	 loss: 21.0340, MinusLogProbMetric: 21.0340, val_loss: 21.4005, val_MinusLogProbMetric: 21.4005

Epoch 118: val_loss did not improve from 21.18981
196/196 - 53s - loss: 21.0340 - MinusLogProbMetric: 21.0340 - val_loss: 21.4005 - val_MinusLogProbMetric: 21.4005 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 119/1000
2023-10-23 14:36:57.910 
Epoch 119/1000 
	 loss: 21.0239, MinusLogProbMetric: 21.0239, val_loss: 21.6740, val_MinusLogProbMetric: 21.6740

Epoch 119: val_loss did not improve from 21.18981
196/196 - 55s - loss: 21.0239 - MinusLogProbMetric: 21.0239 - val_loss: 21.6740 - val_MinusLogProbMetric: 21.6740 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 120/1000
2023-10-23 14:37:57.579 
Epoch 120/1000 
	 loss: 20.8911, MinusLogProbMetric: 20.8911, val_loss: 20.8475, val_MinusLogProbMetric: 20.8475

Epoch 120: val_loss improved from 21.18981 to 20.84746, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 61s - loss: 20.8911 - MinusLogProbMetric: 20.8911 - val_loss: 20.8475 - val_MinusLogProbMetric: 20.8475 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 121/1000
2023-10-23 14:38:59.738 
Epoch 121/1000 
	 loss: 20.8946, MinusLogProbMetric: 20.8946, val_loss: 20.7540, val_MinusLogProbMetric: 20.7540

Epoch 121: val_loss improved from 20.84746 to 20.75401, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 62s - loss: 20.8946 - MinusLogProbMetric: 20.8946 - val_loss: 20.7540 - val_MinusLogProbMetric: 20.7540 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 122/1000
2023-10-23 14:39:58.251 
Epoch 122/1000 
	 loss: 21.0453, MinusLogProbMetric: 21.0453, val_loss: 21.1210, val_MinusLogProbMetric: 21.1210

Epoch 122: val_loss did not improve from 20.75401
196/196 - 58s - loss: 21.0453 - MinusLogProbMetric: 21.0453 - val_loss: 21.1210 - val_MinusLogProbMetric: 21.1210 - lr: 1.1111e-04 - 58s/epoch - 293ms/step
Epoch 123/1000
2023-10-23 14:40:53.015 
Epoch 123/1000 
	 loss: 20.7905, MinusLogProbMetric: 20.7905, val_loss: 20.7944, val_MinusLogProbMetric: 20.7944

Epoch 123: val_loss did not improve from 20.75401
196/196 - 55s - loss: 20.7905 - MinusLogProbMetric: 20.7905 - val_loss: 20.7944 - val_MinusLogProbMetric: 20.7944 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 124/1000
2023-10-23 14:41:57.155 
Epoch 124/1000 
	 loss: 20.7958, MinusLogProbMetric: 20.7958, val_loss: 20.7272, val_MinusLogProbMetric: 20.7272

Epoch 124: val_loss improved from 20.75401 to 20.72723, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 65s - loss: 20.7958 - MinusLogProbMetric: 20.7958 - val_loss: 20.7272 - val_MinusLogProbMetric: 20.7272 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 125/1000
2023-10-23 14:42:56.381 
Epoch 125/1000 
	 loss: 20.7383, MinusLogProbMetric: 20.7383, val_loss: 21.0103, val_MinusLogProbMetric: 21.0103

Epoch 125: val_loss did not improve from 20.72723
196/196 - 58s - loss: 20.7383 - MinusLogProbMetric: 20.7383 - val_loss: 21.0103 - val_MinusLogProbMetric: 21.0103 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 126/1000
2023-10-23 14:43:49.495 
Epoch 126/1000 
	 loss: 20.7490, MinusLogProbMetric: 20.7490, val_loss: 21.0802, val_MinusLogProbMetric: 21.0802

Epoch 126: val_loss did not improve from 20.72723
196/196 - 53s - loss: 20.7490 - MinusLogProbMetric: 20.7490 - val_loss: 21.0802 - val_MinusLogProbMetric: 21.0802 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 127/1000
2023-10-23 14:44:47.066 
Epoch 127/1000 
	 loss: 20.6746, MinusLogProbMetric: 20.6746, val_loss: 20.8948, val_MinusLogProbMetric: 20.8948

Epoch 127: val_loss did not improve from 20.72723
196/196 - 58s - loss: 20.6746 - MinusLogProbMetric: 20.6746 - val_loss: 20.8948 - val_MinusLogProbMetric: 20.8948 - lr: 1.1111e-04 - 58s/epoch - 294ms/step
Epoch 128/1000
2023-10-23 14:45:51.742 
Epoch 128/1000 
	 loss: 20.5901, MinusLogProbMetric: 20.5901, val_loss: 21.3653, val_MinusLogProbMetric: 21.3653

Epoch 128: val_loss did not improve from 20.72723
196/196 - 65s - loss: 20.5901 - MinusLogProbMetric: 20.5901 - val_loss: 21.3653 - val_MinusLogProbMetric: 21.3653 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 129/1000
2023-10-23 14:46:55.792 
Epoch 129/1000 
	 loss: 20.6094, MinusLogProbMetric: 20.6094, val_loss: 20.5984, val_MinusLogProbMetric: 20.5984

Epoch 129: val_loss improved from 20.72723 to 20.59836, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 65s - loss: 20.6094 - MinusLogProbMetric: 20.6094 - val_loss: 20.5984 - val_MinusLogProbMetric: 20.5984 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 130/1000
2023-10-23 14:47:53.130 
Epoch 130/1000 
	 loss: 20.5008, MinusLogProbMetric: 20.5008, val_loss: 20.6776, val_MinusLogProbMetric: 20.6776

Epoch 130: val_loss did not improve from 20.59836
196/196 - 56s - loss: 20.5008 - MinusLogProbMetric: 20.5008 - val_loss: 20.6776 - val_MinusLogProbMetric: 20.6776 - lr: 1.1111e-04 - 56s/epoch - 288ms/step
Epoch 131/1000
2023-10-23 14:48:51.718 
Epoch 131/1000 
	 loss: 20.4783, MinusLogProbMetric: 20.4783, val_loss: 20.9047, val_MinusLogProbMetric: 20.9047

Epoch 131: val_loss did not improve from 20.59836
196/196 - 59s - loss: 20.4783 - MinusLogProbMetric: 20.4783 - val_loss: 20.9047 - val_MinusLogProbMetric: 20.9047 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 132/1000
2023-10-23 14:49:47.923 
Epoch 132/1000 
	 loss: 20.4979, MinusLogProbMetric: 20.4979, val_loss: 20.5089, val_MinusLogProbMetric: 20.5089

Epoch 132: val_loss improved from 20.59836 to 20.50895, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 57s - loss: 20.4979 - MinusLogProbMetric: 20.4979 - val_loss: 20.5089 - val_MinusLogProbMetric: 20.5089 - lr: 1.1111e-04 - 57s/epoch - 291ms/step
Epoch 133/1000
2023-10-23 14:50:42.267 
Epoch 133/1000 
	 loss: 20.4575, MinusLogProbMetric: 20.4575, val_loss: 20.5837, val_MinusLogProbMetric: 20.5837

Epoch 133: val_loss did not improve from 20.50895
196/196 - 54s - loss: 20.4575 - MinusLogProbMetric: 20.4575 - val_loss: 20.5837 - val_MinusLogProbMetric: 20.5837 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 134/1000
2023-10-23 14:51:38.402 
Epoch 134/1000 
	 loss: 20.4143, MinusLogProbMetric: 20.4143, val_loss: 20.5382, val_MinusLogProbMetric: 20.5382

Epoch 134: val_loss did not improve from 20.50895
196/196 - 56s - loss: 20.4143 - MinusLogProbMetric: 20.4143 - val_loss: 20.5382 - val_MinusLogProbMetric: 20.5382 - lr: 1.1111e-04 - 56s/epoch - 286ms/step
Epoch 135/1000
2023-10-23 14:52:35.524 
Epoch 135/1000 
	 loss: 20.3941, MinusLogProbMetric: 20.3941, val_loss: 20.6065, val_MinusLogProbMetric: 20.6065

Epoch 135: val_loss did not improve from 20.50895
196/196 - 57s - loss: 20.3941 - MinusLogProbMetric: 20.3941 - val_loss: 20.6065 - val_MinusLogProbMetric: 20.6065 - lr: 1.1111e-04 - 57s/epoch - 291ms/step
Epoch 136/1000
2023-10-23 14:53:35.247 
Epoch 136/1000 
	 loss: 20.3510, MinusLogProbMetric: 20.3510, val_loss: 20.3539, val_MinusLogProbMetric: 20.3539

Epoch 136: val_loss improved from 20.50895 to 20.35391, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 61s - loss: 20.3510 - MinusLogProbMetric: 20.3510 - val_loss: 20.3539 - val_MinusLogProbMetric: 20.3539 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 137/1000
2023-10-23 14:54:35.077 
Epoch 137/1000 
	 loss: 20.3961, MinusLogProbMetric: 20.3961, val_loss: 20.6021, val_MinusLogProbMetric: 20.6021

Epoch 137: val_loss did not improve from 20.35391
196/196 - 59s - loss: 20.3961 - MinusLogProbMetric: 20.3961 - val_loss: 20.6021 - val_MinusLogProbMetric: 20.6021 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 138/1000
2023-10-23 14:55:32.038 
Epoch 138/1000 
	 loss: 20.3769, MinusLogProbMetric: 20.3769, val_loss: 20.3623, val_MinusLogProbMetric: 20.3623

Epoch 138: val_loss did not improve from 20.35391
196/196 - 57s - loss: 20.3769 - MinusLogProbMetric: 20.3769 - val_loss: 20.3623 - val_MinusLogProbMetric: 20.3623 - lr: 1.1111e-04 - 57s/epoch - 291ms/step
Epoch 139/1000
2023-10-23 14:56:27.215 
Epoch 139/1000 
	 loss: 20.3022, MinusLogProbMetric: 20.3022, val_loss: 20.2900, val_MinusLogProbMetric: 20.2900

Epoch 139: val_loss improved from 20.35391 to 20.29001, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 56s - loss: 20.3022 - MinusLogProbMetric: 20.3022 - val_loss: 20.2900 - val_MinusLogProbMetric: 20.2900 - lr: 1.1111e-04 - 56s/epoch - 286ms/step
Epoch 140/1000
2023-10-23 14:57:26.170 
Epoch 140/1000 
	 loss: 20.3751, MinusLogProbMetric: 20.3751, val_loss: 20.2570, val_MinusLogProbMetric: 20.2570

Epoch 140: val_loss improved from 20.29001 to 20.25695, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 59s - loss: 20.3751 - MinusLogProbMetric: 20.3751 - val_loss: 20.2570 - val_MinusLogProbMetric: 20.2570 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 141/1000
2023-10-23 14:58:21.937 
Epoch 141/1000 
	 loss: 20.1832, MinusLogProbMetric: 20.1832, val_loss: 20.1703, val_MinusLogProbMetric: 20.1703

Epoch 141: val_loss improved from 20.25695 to 20.17029, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 56s - loss: 20.1832 - MinusLogProbMetric: 20.1832 - val_loss: 20.1703 - val_MinusLogProbMetric: 20.1703 - lr: 1.1111e-04 - 56s/epoch - 284ms/step
Epoch 142/1000
2023-10-23 14:59:22.581 
Epoch 142/1000 
	 loss: 20.2903, MinusLogProbMetric: 20.2903, val_loss: 20.6144, val_MinusLogProbMetric: 20.6144

Epoch 142: val_loss did not improve from 20.17029
196/196 - 60s - loss: 20.2903 - MinusLogProbMetric: 20.2903 - val_loss: 20.6144 - val_MinusLogProbMetric: 20.6144 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 143/1000
2023-10-23 15:00:20.247 
Epoch 143/1000 
	 loss: 20.1853, MinusLogProbMetric: 20.1853, val_loss: 21.1141, val_MinusLogProbMetric: 21.1141

Epoch 143: val_loss did not improve from 20.17029
196/196 - 58s - loss: 20.1853 - MinusLogProbMetric: 20.1853 - val_loss: 21.1141 - val_MinusLogProbMetric: 21.1141 - lr: 1.1111e-04 - 58s/epoch - 294ms/step
Epoch 144/1000
2023-10-23 15:01:18.891 
Epoch 144/1000 
	 loss: 20.1943, MinusLogProbMetric: 20.1943, val_loss: 20.3221, val_MinusLogProbMetric: 20.3221

Epoch 144: val_loss did not improve from 20.17029
196/196 - 59s - loss: 20.1943 - MinusLogProbMetric: 20.1943 - val_loss: 20.3221 - val_MinusLogProbMetric: 20.3221 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 145/1000
2023-10-23 15:02:16.489 
Epoch 145/1000 
	 loss: 20.1110, MinusLogProbMetric: 20.1110, val_loss: 20.3629, val_MinusLogProbMetric: 20.3629

Epoch 145: val_loss did not improve from 20.17029
196/196 - 58s - loss: 20.1110 - MinusLogProbMetric: 20.1110 - val_loss: 20.3629 - val_MinusLogProbMetric: 20.3629 - lr: 1.1111e-04 - 58s/epoch - 294ms/step
Epoch 146/1000
2023-10-23 15:03:17.248 
Epoch 146/1000 
	 loss: 20.1335, MinusLogProbMetric: 20.1335, val_loss: 20.0388, val_MinusLogProbMetric: 20.0388

Epoch 146: val_loss improved from 20.17029 to 20.03881, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 61s - loss: 20.1335 - MinusLogProbMetric: 20.1335 - val_loss: 20.0388 - val_MinusLogProbMetric: 20.0388 - lr: 1.1111e-04 - 61s/epoch - 313ms/step
Epoch 147/1000
2023-10-23 15:04:12.559 
Epoch 147/1000 
	 loss: 20.1295, MinusLogProbMetric: 20.1295, val_loss: 20.0251, val_MinusLogProbMetric: 20.0251

Epoch 147: val_loss improved from 20.03881 to 20.02511, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 20.1295 - MinusLogProbMetric: 20.1295 - val_loss: 20.0251 - val_MinusLogProbMetric: 20.0251 - lr: 1.1111e-04 - 55s/epoch - 283ms/step
Epoch 148/1000
2023-10-23 15:05:06.243 
Epoch 148/1000 
	 loss: 20.0193, MinusLogProbMetric: 20.0193, val_loss: 20.1992, val_MinusLogProbMetric: 20.1992

Epoch 148: val_loss did not improve from 20.02511
196/196 - 53s - loss: 20.0193 - MinusLogProbMetric: 20.0193 - val_loss: 20.1992 - val_MinusLogProbMetric: 20.1992 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 149/1000
2023-10-23 15:06:02.346 
Epoch 149/1000 
	 loss: 20.1231, MinusLogProbMetric: 20.1231, val_loss: 20.0378, val_MinusLogProbMetric: 20.0378

Epoch 149: val_loss did not improve from 20.02511
196/196 - 56s - loss: 20.1231 - MinusLogProbMetric: 20.1231 - val_loss: 20.0378 - val_MinusLogProbMetric: 20.0378 - lr: 1.1111e-04 - 56s/epoch - 286ms/step
Epoch 150/1000
2023-10-23 15:06:58.866 
Epoch 150/1000 
	 loss: 19.9904, MinusLogProbMetric: 19.9904, val_loss: 20.0181, val_MinusLogProbMetric: 20.0181

Epoch 150: val_loss improved from 20.02511 to 20.01812, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 57s - loss: 19.9904 - MinusLogProbMetric: 19.9904 - val_loss: 20.0181 - val_MinusLogProbMetric: 20.0181 - lr: 1.1111e-04 - 57s/epoch - 293ms/step
Epoch 151/1000
2023-10-23 15:07:56.202 
Epoch 151/1000 
	 loss: 19.9278, MinusLogProbMetric: 19.9278, val_loss: 19.8895, val_MinusLogProbMetric: 19.8895

Epoch 151: val_loss improved from 20.01812 to 19.88954, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 57s - loss: 19.9278 - MinusLogProbMetric: 19.9278 - val_loss: 19.8895 - val_MinusLogProbMetric: 19.8895 - lr: 1.1111e-04 - 57s/epoch - 292ms/step
Epoch 152/1000
2023-10-23 15:08:52.967 
Epoch 152/1000 
	 loss: 19.9575, MinusLogProbMetric: 19.9575, val_loss: 20.0835, val_MinusLogProbMetric: 20.0835

Epoch 152: val_loss did not improve from 19.88954
196/196 - 56s - loss: 19.9575 - MinusLogProbMetric: 19.9575 - val_loss: 20.0835 - val_MinusLogProbMetric: 20.0835 - lr: 1.1111e-04 - 56s/epoch - 285ms/step
Epoch 153/1000
2023-10-23 15:09:52.221 
Epoch 153/1000 
	 loss: 19.9255, MinusLogProbMetric: 19.9255, val_loss: 19.8407, val_MinusLogProbMetric: 19.8407

Epoch 153: val_loss improved from 19.88954 to 19.84071, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 19.9255 - MinusLogProbMetric: 19.9255 - val_loss: 19.8407 - val_MinusLogProbMetric: 19.8407 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 154/1000
2023-10-23 15:10:51.460 
Epoch 154/1000 
	 loss: 19.9937, MinusLogProbMetric: 19.9937, val_loss: 23.2607, val_MinusLogProbMetric: 23.2607

Epoch 154: val_loss did not improve from 19.84071
196/196 - 58s - loss: 19.9937 - MinusLogProbMetric: 19.9937 - val_loss: 23.2607 - val_MinusLogProbMetric: 23.2607 - lr: 1.1111e-04 - 58s/epoch - 298ms/step
Epoch 155/1000
2023-10-23 15:11:46.885 
Epoch 155/1000 
	 loss: 19.9606, MinusLogProbMetric: 19.9606, val_loss: 20.0496, val_MinusLogProbMetric: 20.0496

Epoch 155: val_loss did not improve from 19.84071
196/196 - 55s - loss: 19.9606 - MinusLogProbMetric: 19.9606 - val_loss: 20.0496 - val_MinusLogProbMetric: 20.0496 - lr: 1.1111e-04 - 55s/epoch - 283ms/step
Epoch 156/1000
2023-10-23 15:12:38.269 
Epoch 156/1000 
	 loss: 19.8485, MinusLogProbMetric: 19.8485, val_loss: 20.1392, val_MinusLogProbMetric: 20.1392

Epoch 156: val_loss did not improve from 19.84071
196/196 - 51s - loss: 19.8485 - MinusLogProbMetric: 19.8485 - val_loss: 20.1392 - val_MinusLogProbMetric: 20.1392 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 157/1000
2023-10-23 15:13:29.657 
Epoch 157/1000 
	 loss: 19.8717, MinusLogProbMetric: 19.8717, val_loss: 19.9053, val_MinusLogProbMetric: 19.9053

Epoch 157: val_loss did not improve from 19.84071
196/196 - 51s - loss: 19.8717 - MinusLogProbMetric: 19.8717 - val_loss: 19.9053 - val_MinusLogProbMetric: 19.9053 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 158/1000
2023-10-23 15:14:28.732 
Epoch 158/1000 
	 loss: 19.8333, MinusLogProbMetric: 19.8333, val_loss: 20.7215, val_MinusLogProbMetric: 20.7215

Epoch 158: val_loss did not improve from 19.84071
196/196 - 59s - loss: 19.8333 - MinusLogProbMetric: 19.8333 - val_loss: 20.7215 - val_MinusLogProbMetric: 20.7215 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 159/1000
2023-10-23 15:15:27.544 
Epoch 159/1000 
	 loss: 19.7994, MinusLogProbMetric: 19.7994, val_loss: 19.7964, val_MinusLogProbMetric: 19.7964

Epoch 159: val_loss improved from 19.84071 to 19.79636, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 19.7994 - MinusLogProbMetric: 19.7994 - val_loss: 19.7964 - val_MinusLogProbMetric: 19.7964 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 160/1000
2023-10-23 15:16:22.491 
Epoch 160/1000 
	 loss: 19.7882, MinusLogProbMetric: 19.7882, val_loss: 19.9340, val_MinusLogProbMetric: 19.9340

Epoch 160: val_loss did not improve from 19.79636
196/196 - 54s - loss: 19.7882 - MinusLogProbMetric: 19.7882 - val_loss: 19.9340 - val_MinusLogProbMetric: 19.9340 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 161/1000
2023-10-23 15:17:17.368 
Epoch 161/1000 
	 loss: 19.8318, MinusLogProbMetric: 19.8318, val_loss: 19.6488, val_MinusLogProbMetric: 19.6488

Epoch 161: val_loss improved from 19.79636 to 19.64881, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 56s - loss: 19.8318 - MinusLogProbMetric: 19.8318 - val_loss: 19.6488 - val_MinusLogProbMetric: 19.6488 - lr: 1.1111e-04 - 56s/epoch - 284ms/step
Epoch 162/1000
2023-10-23 15:18:20.099 
Epoch 162/1000 
	 loss: 19.8462, MinusLogProbMetric: 19.8462, val_loss: 19.8822, val_MinusLogProbMetric: 19.8822

Epoch 162: val_loss did not improve from 19.64881
196/196 - 62s - loss: 19.8462 - MinusLogProbMetric: 19.8462 - val_loss: 19.8822 - val_MinusLogProbMetric: 19.8822 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 163/1000
2023-10-23 15:19:17.384 
Epoch 163/1000 
	 loss: 19.7904, MinusLogProbMetric: 19.7904, val_loss: 19.8470, val_MinusLogProbMetric: 19.8470

Epoch 163: val_loss did not improve from 19.64881
196/196 - 57s - loss: 19.7904 - MinusLogProbMetric: 19.7904 - val_loss: 19.8470 - val_MinusLogProbMetric: 19.8470 - lr: 1.1111e-04 - 57s/epoch - 292ms/step
Epoch 164/1000
2023-10-23 15:20:11.546 
Epoch 164/1000 
	 loss: 19.6487, MinusLogProbMetric: 19.6487, val_loss: 19.7051, val_MinusLogProbMetric: 19.7051

Epoch 164: val_loss did not improve from 19.64881
196/196 - 54s - loss: 19.6487 - MinusLogProbMetric: 19.6487 - val_loss: 19.7051 - val_MinusLogProbMetric: 19.7051 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 165/1000
2023-10-23 15:21:11.475 
Epoch 165/1000 
	 loss: 19.6762, MinusLogProbMetric: 19.6762, val_loss: 20.1491, val_MinusLogProbMetric: 20.1491

Epoch 165: val_loss did not improve from 19.64881
196/196 - 60s - loss: 19.6762 - MinusLogProbMetric: 19.6762 - val_loss: 20.1491 - val_MinusLogProbMetric: 20.1491 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 166/1000
2023-10-23 15:22:08.289 
Epoch 166/1000 
	 loss: 19.7097, MinusLogProbMetric: 19.7097, val_loss: 19.8706, val_MinusLogProbMetric: 19.8706

Epoch 166: val_loss did not improve from 19.64881
196/196 - 57s - loss: 19.7097 - MinusLogProbMetric: 19.7097 - val_loss: 19.8706 - val_MinusLogProbMetric: 19.8706 - lr: 1.1111e-04 - 57s/epoch - 290ms/step
Epoch 167/1000
2023-10-23 15:23:07.658 
Epoch 167/1000 
	 loss: 19.6895, MinusLogProbMetric: 19.6895, val_loss: 19.6774, val_MinusLogProbMetric: 19.6774

Epoch 167: val_loss did not improve from 19.64881
196/196 - 59s - loss: 19.6895 - MinusLogProbMetric: 19.6895 - val_loss: 19.6774 - val_MinusLogProbMetric: 19.6774 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 168/1000
2023-10-23 15:24:08.731 
Epoch 168/1000 
	 loss: 19.5430, MinusLogProbMetric: 19.5430, val_loss: 20.0641, val_MinusLogProbMetric: 20.0641

Epoch 168: val_loss did not improve from 19.64881
196/196 - 61s - loss: 19.5430 - MinusLogProbMetric: 19.5430 - val_loss: 20.0641 - val_MinusLogProbMetric: 20.0641 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 169/1000
2023-10-23 15:25:04.965 
Epoch 169/1000 
	 loss: 19.5915, MinusLogProbMetric: 19.5915, val_loss: 19.6153, val_MinusLogProbMetric: 19.6153

Epoch 169: val_loss improved from 19.64881 to 19.61534, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 57s - loss: 19.5915 - MinusLogProbMetric: 19.5915 - val_loss: 19.6153 - val_MinusLogProbMetric: 19.6153 - lr: 1.1111e-04 - 57s/epoch - 291ms/step
Epoch 170/1000
2023-10-23 15:26:01.907 
Epoch 170/1000 
	 loss: 19.6109, MinusLogProbMetric: 19.6109, val_loss: 19.8031, val_MinusLogProbMetric: 19.8031

Epoch 170: val_loss did not improve from 19.61534
196/196 - 56s - loss: 19.6109 - MinusLogProbMetric: 19.6109 - val_loss: 19.8031 - val_MinusLogProbMetric: 19.8031 - lr: 1.1111e-04 - 56s/epoch - 286ms/step
Epoch 171/1000
2023-10-23 15:26:55.491 
Epoch 171/1000 
	 loss: 19.5767, MinusLogProbMetric: 19.5767, val_loss: 19.5454, val_MinusLogProbMetric: 19.5454

Epoch 171: val_loss improved from 19.61534 to 19.54536, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 19.5767 - MinusLogProbMetric: 19.5767 - val_loss: 19.5454 - val_MinusLogProbMetric: 19.5454 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 172/1000
2023-10-23 15:27:48.004 
Epoch 172/1000 
	 loss: 19.5377, MinusLogProbMetric: 19.5377, val_loss: 19.5804, val_MinusLogProbMetric: 19.5804

Epoch 172: val_loss did not improve from 19.54536
196/196 - 52s - loss: 19.5377 - MinusLogProbMetric: 19.5377 - val_loss: 19.5804 - val_MinusLogProbMetric: 19.5804 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 173/1000
2023-10-23 15:28:47.929 
Epoch 173/1000 
	 loss: 19.6071, MinusLogProbMetric: 19.6071, val_loss: 19.6418, val_MinusLogProbMetric: 19.6418

Epoch 173: val_loss did not improve from 19.54536
196/196 - 60s - loss: 19.6071 - MinusLogProbMetric: 19.6071 - val_loss: 19.6418 - val_MinusLogProbMetric: 19.6418 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 174/1000
2023-10-23 15:29:48.438 
Epoch 174/1000 
	 loss: 19.4882, MinusLogProbMetric: 19.4882, val_loss: 19.7872, val_MinusLogProbMetric: 19.7872

Epoch 174: val_loss did not improve from 19.54536
196/196 - 61s - loss: 19.4882 - MinusLogProbMetric: 19.4882 - val_loss: 19.7872 - val_MinusLogProbMetric: 19.7872 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 175/1000
2023-10-23 15:30:44.030 
Epoch 175/1000 
	 loss: 19.4947, MinusLogProbMetric: 19.4947, val_loss: 19.8128, val_MinusLogProbMetric: 19.8128

Epoch 175: val_loss did not improve from 19.54536
196/196 - 56s - loss: 19.4947 - MinusLogProbMetric: 19.4947 - val_loss: 19.8128 - val_MinusLogProbMetric: 19.8128 - lr: 1.1111e-04 - 56s/epoch - 284ms/step
Epoch 176/1000
2023-10-23 15:31:36.529 
Epoch 176/1000 
	 loss: 19.5156, MinusLogProbMetric: 19.5156, val_loss: 19.5299, val_MinusLogProbMetric: 19.5299

Epoch 176: val_loss improved from 19.54536 to 19.52988, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 19.5156 - MinusLogProbMetric: 19.5156 - val_loss: 19.5299 - val_MinusLogProbMetric: 19.5299 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 177/1000
2023-10-23 15:32:36.258 
Epoch 177/1000 
	 loss: 19.5111, MinusLogProbMetric: 19.5111, val_loss: 19.9294, val_MinusLogProbMetric: 19.9294

Epoch 177: val_loss did not improve from 19.52988
196/196 - 59s - loss: 19.5111 - MinusLogProbMetric: 19.5111 - val_loss: 19.9294 - val_MinusLogProbMetric: 19.9294 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 178/1000
2023-10-23 15:33:35.187 
Epoch 178/1000 
	 loss: 19.4621, MinusLogProbMetric: 19.4621, val_loss: 19.3876, val_MinusLogProbMetric: 19.3876

Epoch 178: val_loss improved from 19.52988 to 19.38760, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 19.4621 - MinusLogProbMetric: 19.4621 - val_loss: 19.3876 - val_MinusLogProbMetric: 19.3876 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 179/1000
2023-10-23 15:34:32.785 
Epoch 179/1000 
	 loss: 19.4547, MinusLogProbMetric: 19.4547, val_loss: 19.4804, val_MinusLogProbMetric: 19.4804

Epoch 179: val_loss did not improve from 19.38760
196/196 - 57s - loss: 19.4547 - MinusLogProbMetric: 19.4547 - val_loss: 19.4804 - val_MinusLogProbMetric: 19.4804 - lr: 1.1111e-04 - 57s/epoch - 290ms/step
Epoch 180/1000
2023-10-23 15:35:29.640 
Epoch 180/1000 
	 loss: 19.4428, MinusLogProbMetric: 19.4428, val_loss: 19.4804, val_MinusLogProbMetric: 19.4804

Epoch 180: val_loss did not improve from 19.38760
196/196 - 57s - loss: 19.4428 - MinusLogProbMetric: 19.4428 - val_loss: 19.4804 - val_MinusLogProbMetric: 19.4804 - lr: 1.1111e-04 - 57s/epoch - 290ms/step
Epoch 181/1000
2023-10-23 15:36:27.738 
Epoch 181/1000 
	 loss: 19.4049, MinusLogProbMetric: 19.4049, val_loss: 19.6936, val_MinusLogProbMetric: 19.6936

Epoch 181: val_loss did not improve from 19.38760
196/196 - 58s - loss: 19.4049 - MinusLogProbMetric: 19.4049 - val_loss: 19.6936 - val_MinusLogProbMetric: 19.6936 - lr: 1.1111e-04 - 58s/epoch - 296ms/step
Epoch 182/1000
2023-10-23 15:37:26.083 
Epoch 182/1000 
	 loss: 19.4104, MinusLogProbMetric: 19.4104, val_loss: 19.3167, val_MinusLogProbMetric: 19.3167

Epoch 182: val_loss improved from 19.38760 to 19.31674, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 59s - loss: 19.4104 - MinusLogProbMetric: 19.4104 - val_loss: 19.3167 - val_MinusLogProbMetric: 19.3167 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 183/1000
2023-10-23 15:38:21.543 
Epoch 183/1000 
	 loss: 19.3626, MinusLogProbMetric: 19.3626, val_loss: 19.4380, val_MinusLogProbMetric: 19.4380

Epoch 183: val_loss did not improve from 19.31674
196/196 - 55s - loss: 19.3626 - MinusLogProbMetric: 19.3626 - val_loss: 19.4380 - val_MinusLogProbMetric: 19.4380 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 184/1000
2023-10-23 15:39:17.345 
Epoch 184/1000 
	 loss: 19.3300, MinusLogProbMetric: 19.3300, val_loss: 19.6363, val_MinusLogProbMetric: 19.6363

Epoch 184: val_loss did not improve from 19.31674
196/196 - 56s - loss: 19.3300 - MinusLogProbMetric: 19.3300 - val_loss: 19.6363 - val_MinusLogProbMetric: 19.6363 - lr: 1.1111e-04 - 56s/epoch - 285ms/step
Epoch 185/1000
2023-10-23 15:40:13.861 
Epoch 185/1000 
	 loss: 19.3532, MinusLogProbMetric: 19.3532, val_loss: 19.4579, val_MinusLogProbMetric: 19.4579

Epoch 185: val_loss did not improve from 19.31674
196/196 - 57s - loss: 19.3532 - MinusLogProbMetric: 19.3532 - val_loss: 19.4579 - val_MinusLogProbMetric: 19.4579 - lr: 1.1111e-04 - 57s/epoch - 288ms/step
Epoch 186/1000
2023-10-23 15:41:10.598 
Epoch 186/1000 
	 loss: 19.3261, MinusLogProbMetric: 19.3261, val_loss: 19.6137, val_MinusLogProbMetric: 19.6137

Epoch 186: val_loss did not improve from 19.31674
196/196 - 57s - loss: 19.3261 - MinusLogProbMetric: 19.3261 - val_loss: 19.6137 - val_MinusLogProbMetric: 19.6137 - lr: 1.1111e-04 - 57s/epoch - 289ms/step
Epoch 187/1000
2023-10-23 15:42:08.619 
Epoch 187/1000 
	 loss: 19.3088, MinusLogProbMetric: 19.3088, val_loss: 19.7477, val_MinusLogProbMetric: 19.7477

Epoch 187: val_loss did not improve from 19.31674
196/196 - 58s - loss: 19.3088 - MinusLogProbMetric: 19.3088 - val_loss: 19.7477 - val_MinusLogProbMetric: 19.7477 - lr: 1.1111e-04 - 58s/epoch - 296ms/step
Epoch 188/1000
2023-10-23 15:43:04.749 
Epoch 188/1000 
	 loss: 19.3719, MinusLogProbMetric: 19.3719, val_loss: 20.3069, val_MinusLogProbMetric: 20.3069

Epoch 188: val_loss did not improve from 19.31674
196/196 - 56s - loss: 19.3719 - MinusLogProbMetric: 19.3719 - val_loss: 20.3069 - val_MinusLogProbMetric: 20.3069 - lr: 1.1111e-04 - 56s/epoch - 286ms/step
Epoch 189/1000
2023-10-23 15:44:05.268 
Epoch 189/1000 
	 loss: 19.2740, MinusLogProbMetric: 19.2740, val_loss: 19.9173, val_MinusLogProbMetric: 19.9173

Epoch 189: val_loss did not improve from 19.31674
196/196 - 61s - loss: 19.2740 - MinusLogProbMetric: 19.2740 - val_loss: 19.9173 - val_MinusLogProbMetric: 19.9173 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 190/1000
2023-10-23 15:45:04.579 
Epoch 190/1000 
	 loss: 19.2569, MinusLogProbMetric: 19.2569, val_loss: 19.6791, val_MinusLogProbMetric: 19.6791

Epoch 190: val_loss did not improve from 19.31674
196/196 - 59s - loss: 19.2569 - MinusLogProbMetric: 19.2569 - val_loss: 19.6791 - val_MinusLogProbMetric: 19.6791 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 191/1000
2023-10-23 15:46:00.823 
Epoch 191/1000 
	 loss: 19.2583, MinusLogProbMetric: 19.2583, val_loss: 19.3211, val_MinusLogProbMetric: 19.3211

Epoch 191: val_loss did not improve from 19.31674
196/196 - 56s - loss: 19.2583 - MinusLogProbMetric: 19.2583 - val_loss: 19.3211 - val_MinusLogProbMetric: 19.3211 - lr: 1.1111e-04 - 56s/epoch - 287ms/step
Epoch 192/1000
2023-10-23 15:46:58.618 
Epoch 192/1000 
	 loss: 19.2404, MinusLogProbMetric: 19.2404, val_loss: 19.6401, val_MinusLogProbMetric: 19.6401

Epoch 192: val_loss did not improve from 19.31674
196/196 - 58s - loss: 19.2404 - MinusLogProbMetric: 19.2404 - val_loss: 19.6401 - val_MinusLogProbMetric: 19.6401 - lr: 1.1111e-04 - 58s/epoch - 295ms/step
Epoch 193/1000
2023-10-23 15:47:58.609 
Epoch 193/1000 
	 loss: 19.2743, MinusLogProbMetric: 19.2743, val_loss: 19.4676, val_MinusLogProbMetric: 19.4676

Epoch 193: val_loss did not improve from 19.31674
196/196 - 60s - loss: 19.2743 - MinusLogProbMetric: 19.2743 - val_loss: 19.4676 - val_MinusLogProbMetric: 19.4676 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 194/1000
2023-10-23 15:48:56.475 
Epoch 194/1000 
	 loss: 19.1901, MinusLogProbMetric: 19.1901, val_loss: 19.1870, val_MinusLogProbMetric: 19.1870

Epoch 194: val_loss improved from 19.31674 to 19.18695, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 59s - loss: 19.1901 - MinusLogProbMetric: 19.1901 - val_loss: 19.1870 - val_MinusLogProbMetric: 19.1870 - lr: 1.1111e-04 - 59s/epoch - 300ms/step
Epoch 195/1000
2023-10-23 15:49:56.650 
Epoch 195/1000 
	 loss: 19.1915, MinusLogProbMetric: 19.1915, val_loss: 19.1950, val_MinusLogProbMetric: 19.1950

Epoch 195: val_loss did not improve from 19.18695
196/196 - 59s - loss: 19.1915 - MinusLogProbMetric: 19.1915 - val_loss: 19.1950 - val_MinusLogProbMetric: 19.1950 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 196/1000
2023-10-23 15:50:53.833 
Epoch 196/1000 
	 loss: 19.1607, MinusLogProbMetric: 19.1607, val_loss: 19.6290, val_MinusLogProbMetric: 19.6290

Epoch 196: val_loss did not improve from 19.18695
196/196 - 57s - loss: 19.1607 - MinusLogProbMetric: 19.1607 - val_loss: 19.6290 - val_MinusLogProbMetric: 19.6290 - lr: 1.1111e-04 - 57s/epoch - 292ms/step
Epoch 197/1000
2023-10-23 15:51:52.538 
Epoch 197/1000 
	 loss: 19.1323, MinusLogProbMetric: 19.1323, val_loss: 19.3589, val_MinusLogProbMetric: 19.3589

Epoch 197: val_loss did not improve from 19.18695
196/196 - 59s - loss: 19.1323 - MinusLogProbMetric: 19.1323 - val_loss: 19.3589 - val_MinusLogProbMetric: 19.3589 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 198/1000
2023-10-23 15:52:50.250 
Epoch 198/1000 
	 loss: 19.1664, MinusLogProbMetric: 19.1664, val_loss: 19.1935, val_MinusLogProbMetric: 19.1935

Epoch 198: val_loss did not improve from 19.18695
196/196 - 58s - loss: 19.1664 - MinusLogProbMetric: 19.1664 - val_loss: 19.1935 - val_MinusLogProbMetric: 19.1935 - lr: 1.1111e-04 - 58s/epoch - 294ms/step
Epoch 199/1000
2023-10-23 15:53:49.688 
Epoch 199/1000 
	 loss: 19.1763, MinusLogProbMetric: 19.1763, val_loss: 19.4444, val_MinusLogProbMetric: 19.4444

Epoch 199: val_loss did not improve from 19.18695
196/196 - 59s - loss: 19.1763 - MinusLogProbMetric: 19.1763 - val_loss: 19.4444 - val_MinusLogProbMetric: 19.4444 - lr: 1.1111e-04 - 59s/epoch - 303ms/step
Epoch 200/1000
2023-10-23 15:54:50.083 
Epoch 200/1000 
	 loss: 19.1705, MinusLogProbMetric: 19.1705, val_loss: 19.2293, val_MinusLogProbMetric: 19.2293

Epoch 200: val_loss did not improve from 19.18695
196/196 - 60s - loss: 19.1705 - MinusLogProbMetric: 19.1705 - val_loss: 19.2293 - val_MinusLogProbMetric: 19.2293 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 201/1000
2023-10-23 15:55:49.913 
Epoch 201/1000 
	 loss: 19.1455, MinusLogProbMetric: 19.1455, val_loss: 19.1530, val_MinusLogProbMetric: 19.1530

Epoch 201: val_loss improved from 19.18695 to 19.15298, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 61s - loss: 19.1455 - MinusLogProbMetric: 19.1455 - val_loss: 19.1530 - val_MinusLogProbMetric: 19.1530 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 202/1000
2023-10-23 15:56:47.638 
Epoch 202/1000 
	 loss: 19.0802, MinusLogProbMetric: 19.0802, val_loss: 19.3162, val_MinusLogProbMetric: 19.3162

Epoch 202: val_loss did not improve from 19.15298
196/196 - 57s - loss: 19.0802 - MinusLogProbMetric: 19.0802 - val_loss: 19.3162 - val_MinusLogProbMetric: 19.3162 - lr: 1.1111e-04 - 57s/epoch - 290ms/step
Epoch 203/1000
2023-10-23 15:57:48.017 
Epoch 203/1000 
	 loss: 19.0421, MinusLogProbMetric: 19.0421, val_loss: 19.1080, val_MinusLogProbMetric: 19.1080

Epoch 203: val_loss improved from 19.15298 to 19.10804, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 61s - loss: 19.0421 - MinusLogProbMetric: 19.0421 - val_loss: 19.1080 - val_MinusLogProbMetric: 19.1080 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 204/1000
2023-10-23 15:58:46.154 
Epoch 204/1000 
	 loss: 19.1172, MinusLogProbMetric: 19.1172, val_loss: 19.1706, val_MinusLogProbMetric: 19.1706

Epoch 204: val_loss did not improve from 19.10804
196/196 - 57s - loss: 19.1172 - MinusLogProbMetric: 19.1172 - val_loss: 19.1706 - val_MinusLogProbMetric: 19.1706 - lr: 1.1111e-04 - 57s/epoch - 293ms/step
Epoch 205/1000
2023-10-23 15:59:47.879 
Epoch 205/1000 
	 loss: 19.1074, MinusLogProbMetric: 19.1074, val_loss: 19.7888, val_MinusLogProbMetric: 19.7888

Epoch 205: val_loss did not improve from 19.10804
196/196 - 62s - loss: 19.1074 - MinusLogProbMetric: 19.1074 - val_loss: 19.7888 - val_MinusLogProbMetric: 19.7888 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 206/1000
2023-10-23 16:00:47.833 
Epoch 206/1000 
	 loss: 19.0735, MinusLogProbMetric: 19.0735, val_loss: 19.5061, val_MinusLogProbMetric: 19.5061

Epoch 206: val_loss did not improve from 19.10804
196/196 - 60s - loss: 19.0735 - MinusLogProbMetric: 19.0735 - val_loss: 19.5061 - val_MinusLogProbMetric: 19.5061 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 207/1000
2023-10-23 16:01:45.424 
Epoch 207/1000 
	 loss: 19.0914, MinusLogProbMetric: 19.0914, val_loss: 19.7410, val_MinusLogProbMetric: 19.7410

Epoch 207: val_loss did not improve from 19.10804
196/196 - 58s - loss: 19.0914 - MinusLogProbMetric: 19.0914 - val_loss: 19.7410 - val_MinusLogProbMetric: 19.7410 - lr: 1.1111e-04 - 58s/epoch - 294ms/step
Epoch 208/1000
2023-10-23 16:02:45.245 
Epoch 208/1000 
	 loss: 19.0618, MinusLogProbMetric: 19.0618, val_loss: 19.3825, val_MinusLogProbMetric: 19.3825

Epoch 208: val_loss did not improve from 19.10804
196/196 - 60s - loss: 19.0618 - MinusLogProbMetric: 19.0618 - val_loss: 19.3825 - val_MinusLogProbMetric: 19.3825 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 209/1000
2023-10-23 16:03:41.011 
Epoch 209/1000 
	 loss: 19.0130, MinusLogProbMetric: 19.0130, val_loss: 19.0907, val_MinusLogProbMetric: 19.0907

Epoch 209: val_loss improved from 19.10804 to 19.09068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 57s - loss: 19.0130 - MinusLogProbMetric: 19.0130 - val_loss: 19.0907 - val_MinusLogProbMetric: 19.0907 - lr: 1.1111e-04 - 57s/epoch - 289ms/step
Epoch 210/1000
2023-10-23 16:04:44.398 
Epoch 210/1000 
	 loss: 19.0194, MinusLogProbMetric: 19.0194, val_loss: 19.1652, val_MinusLogProbMetric: 19.1652

Epoch 210: val_loss did not improve from 19.09068
196/196 - 63s - loss: 19.0194 - MinusLogProbMetric: 19.0194 - val_loss: 19.1652 - val_MinusLogProbMetric: 19.1652 - lr: 1.1111e-04 - 63s/epoch - 319ms/step
Epoch 211/1000
2023-10-23 16:05:40.427 
Epoch 211/1000 
	 loss: 18.9871, MinusLogProbMetric: 18.9871, val_loss: 19.3211, val_MinusLogProbMetric: 19.3211

Epoch 211: val_loss did not improve from 19.09068
196/196 - 56s - loss: 18.9871 - MinusLogProbMetric: 18.9871 - val_loss: 19.3211 - val_MinusLogProbMetric: 19.3211 - lr: 1.1111e-04 - 56s/epoch - 286ms/step
Epoch 212/1000
2023-10-23 16:06:40.779 
Epoch 212/1000 
	 loss: 19.0677, MinusLogProbMetric: 19.0677, val_loss: 19.0269, val_MinusLogProbMetric: 19.0269

Epoch 212: val_loss improved from 19.09068 to 19.02689, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 61s - loss: 19.0677 - MinusLogProbMetric: 19.0677 - val_loss: 19.0269 - val_MinusLogProbMetric: 19.0269 - lr: 1.1111e-04 - 61s/epoch - 312ms/step
Epoch 213/1000
2023-10-23 16:07:40.925 
Epoch 213/1000 
	 loss: 18.9678, MinusLogProbMetric: 18.9678, val_loss: 19.0104, val_MinusLogProbMetric: 19.0104

Epoch 213: val_loss improved from 19.02689 to 19.01036, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 18.9678 - MinusLogProbMetric: 18.9678 - val_loss: 19.0104 - val_MinusLogProbMetric: 19.0104 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 214/1000
2023-10-23 16:08:39.035 
Epoch 214/1000 
	 loss: 19.0034, MinusLogProbMetric: 19.0034, val_loss: 19.0619, val_MinusLogProbMetric: 19.0619

Epoch 214: val_loss did not improve from 19.01036
196/196 - 57s - loss: 19.0034 - MinusLogProbMetric: 19.0034 - val_loss: 19.0619 - val_MinusLogProbMetric: 19.0619 - lr: 1.1111e-04 - 57s/epoch - 293ms/step
Epoch 215/1000
2023-10-23 16:09:33.364 
Epoch 215/1000 
	 loss: 18.9748, MinusLogProbMetric: 18.9748, val_loss: 19.1689, val_MinusLogProbMetric: 19.1689

Epoch 215: val_loss did not improve from 19.01036
196/196 - 54s - loss: 18.9748 - MinusLogProbMetric: 18.9748 - val_loss: 19.1689 - val_MinusLogProbMetric: 19.1689 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 216/1000
2023-10-23 16:10:34.965 
Epoch 216/1000 
	 loss: 18.9534, MinusLogProbMetric: 18.9534, val_loss: 19.1548, val_MinusLogProbMetric: 19.1548

Epoch 216: val_loss did not improve from 19.01036
196/196 - 62s - loss: 18.9534 - MinusLogProbMetric: 18.9534 - val_loss: 19.1548 - val_MinusLogProbMetric: 19.1548 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 217/1000
2023-10-23 16:11:35.071 
Epoch 217/1000 
	 loss: 18.9273, MinusLogProbMetric: 18.9273, val_loss: 19.0338, val_MinusLogProbMetric: 19.0338

Epoch 217: val_loss did not improve from 19.01036
196/196 - 60s - loss: 18.9273 - MinusLogProbMetric: 18.9273 - val_loss: 19.0338 - val_MinusLogProbMetric: 19.0338 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 218/1000
2023-10-23 16:12:34.859 
Epoch 218/1000 
	 loss: 19.0102, MinusLogProbMetric: 19.0102, val_loss: 19.0034, val_MinusLogProbMetric: 19.0034

Epoch 218: val_loss improved from 19.01036 to 19.00344, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 61s - loss: 19.0102 - MinusLogProbMetric: 19.0102 - val_loss: 19.0034 - val_MinusLogProbMetric: 19.0034 - lr: 1.1111e-04 - 61s/epoch - 309ms/step
Epoch 219/1000
2023-10-23 16:13:31.430 
Epoch 219/1000 
	 loss: 18.9517, MinusLogProbMetric: 18.9517, val_loss: 19.2893, val_MinusLogProbMetric: 19.2893

Epoch 219: val_loss did not improve from 19.00344
196/196 - 56s - loss: 18.9517 - MinusLogProbMetric: 18.9517 - val_loss: 19.2893 - val_MinusLogProbMetric: 19.2893 - lr: 1.1111e-04 - 56s/epoch - 285ms/step
Epoch 220/1000
2023-10-23 16:14:31.483 
Epoch 220/1000 
	 loss: 18.9422, MinusLogProbMetric: 18.9422, val_loss: 18.8281, val_MinusLogProbMetric: 18.8281

Epoch 220: val_loss improved from 19.00344 to 18.82809, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 61s - loss: 18.9422 - MinusLogProbMetric: 18.9422 - val_loss: 18.8281 - val_MinusLogProbMetric: 18.8281 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 221/1000
2023-10-23 16:15:30.708 
Epoch 221/1000 
	 loss: 18.9218, MinusLogProbMetric: 18.9218, val_loss: 18.8026, val_MinusLogProbMetric: 18.8026

Epoch 221: val_loss improved from 18.82809 to 18.80258, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 59s - loss: 18.9218 - MinusLogProbMetric: 18.9218 - val_loss: 18.8026 - val_MinusLogProbMetric: 18.8026 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 222/1000
2023-10-23 16:16:34.208 
Epoch 222/1000 
	 loss: 18.9245, MinusLogProbMetric: 18.9245, val_loss: 18.8731, val_MinusLogProbMetric: 18.8731

Epoch 222: val_loss did not improve from 18.80258
196/196 - 63s - loss: 18.9245 - MinusLogProbMetric: 18.9245 - val_loss: 18.8731 - val_MinusLogProbMetric: 18.8731 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 223/1000
2023-10-23 16:17:36.242 
Epoch 223/1000 
	 loss: 18.8690, MinusLogProbMetric: 18.8690, val_loss: 19.0994, val_MinusLogProbMetric: 19.0994

Epoch 223: val_loss did not improve from 18.80258
196/196 - 62s - loss: 18.8690 - MinusLogProbMetric: 18.8690 - val_loss: 19.0994 - val_MinusLogProbMetric: 19.0994 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 224/1000
2023-10-23 16:18:33.731 
Epoch 224/1000 
	 loss: 18.8670, MinusLogProbMetric: 18.8670, val_loss: 19.6280, val_MinusLogProbMetric: 19.6280

Epoch 224: val_loss did not improve from 18.80258
196/196 - 57s - loss: 18.8670 - MinusLogProbMetric: 18.8670 - val_loss: 19.6280 - val_MinusLogProbMetric: 19.6280 - lr: 1.1111e-04 - 57s/epoch - 293ms/step
Epoch 225/1000
2023-10-23 16:19:32.016 
Epoch 225/1000 
	 loss: 18.8559, MinusLogProbMetric: 18.8559, val_loss: 18.8849, val_MinusLogProbMetric: 18.8849

Epoch 225: val_loss did not improve from 18.80258
196/196 - 58s - loss: 18.8559 - MinusLogProbMetric: 18.8559 - val_loss: 18.8849 - val_MinusLogProbMetric: 18.8849 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 226/1000
2023-10-23 16:20:28.359 
Epoch 226/1000 
	 loss: 18.9135, MinusLogProbMetric: 18.9135, val_loss: 19.0266, val_MinusLogProbMetric: 19.0266

Epoch 226: val_loss did not improve from 18.80258
196/196 - 56s - loss: 18.9135 - MinusLogProbMetric: 18.9135 - val_loss: 19.0266 - val_MinusLogProbMetric: 19.0266 - lr: 1.1111e-04 - 56s/epoch - 287ms/step
Epoch 227/1000
2023-10-23 16:21:21.392 
Epoch 227/1000 
	 loss: 18.8156, MinusLogProbMetric: 18.8156, val_loss: 19.4748, val_MinusLogProbMetric: 19.4748

Epoch 227: val_loss did not improve from 18.80258
196/196 - 53s - loss: 18.8156 - MinusLogProbMetric: 18.8156 - val_loss: 19.4748 - val_MinusLogProbMetric: 19.4748 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 228/1000
2023-10-23 16:22:21.655 
Epoch 228/1000 
	 loss: 18.8562, MinusLogProbMetric: 18.8562, val_loss: 18.9262, val_MinusLogProbMetric: 18.9262

Epoch 228: val_loss did not improve from 18.80258
196/196 - 60s - loss: 18.8562 - MinusLogProbMetric: 18.8562 - val_loss: 18.9262 - val_MinusLogProbMetric: 18.9262 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 229/1000
2023-10-23 16:23:15.316 
Epoch 229/1000 
	 loss: 18.8339, MinusLogProbMetric: 18.8339, val_loss: 18.9306, val_MinusLogProbMetric: 18.9306

Epoch 229: val_loss did not improve from 18.80258
196/196 - 54s - loss: 18.8339 - MinusLogProbMetric: 18.8339 - val_loss: 18.9306 - val_MinusLogProbMetric: 18.9306 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 230/1000
2023-10-23 16:24:10.865 
Epoch 230/1000 
	 loss: 18.9016, MinusLogProbMetric: 18.9016, val_loss: 18.8212, val_MinusLogProbMetric: 18.8212

Epoch 230: val_loss did not improve from 18.80258
196/196 - 56s - loss: 18.9016 - MinusLogProbMetric: 18.9016 - val_loss: 18.8212 - val_MinusLogProbMetric: 18.8212 - lr: 1.1111e-04 - 56s/epoch - 283ms/step
Epoch 231/1000
2023-10-23 16:25:07.518 
Epoch 231/1000 
	 loss: 18.7978, MinusLogProbMetric: 18.7978, val_loss: 18.7151, val_MinusLogProbMetric: 18.7151

Epoch 231: val_loss improved from 18.80258 to 18.71515, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 57s - loss: 18.7978 - MinusLogProbMetric: 18.7978 - val_loss: 18.7151 - val_MinusLogProbMetric: 18.7151 - lr: 1.1111e-04 - 57s/epoch - 293ms/step
Epoch 232/1000
2023-10-23 16:26:02.001 
Epoch 232/1000 
	 loss: 18.9503, MinusLogProbMetric: 18.9503, val_loss: 18.7977, val_MinusLogProbMetric: 18.7977

Epoch 232: val_loss did not improve from 18.71515
196/196 - 54s - loss: 18.9503 - MinusLogProbMetric: 18.9503 - val_loss: 18.7977 - val_MinusLogProbMetric: 18.7977 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 233/1000
2023-10-23 16:27:02.837 
Epoch 233/1000 
	 loss: 18.7840, MinusLogProbMetric: 18.7840, val_loss: 20.8259, val_MinusLogProbMetric: 20.8259

Epoch 233: val_loss did not improve from 18.71515
196/196 - 61s - loss: 18.7840 - MinusLogProbMetric: 18.7840 - val_loss: 20.8259 - val_MinusLogProbMetric: 20.8259 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 234/1000
2023-10-23 16:28:04.962 
Epoch 234/1000 
	 loss: 18.7814, MinusLogProbMetric: 18.7814, val_loss: 18.7059, val_MinusLogProbMetric: 18.7059

Epoch 234: val_loss improved from 18.71515 to 18.70587, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 63s - loss: 18.7814 - MinusLogProbMetric: 18.7814 - val_loss: 18.7059 - val_MinusLogProbMetric: 18.7059 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 235/1000
2023-10-23 16:29:02.117 
Epoch 235/1000 
	 loss: 18.8541, MinusLogProbMetric: 18.8541, val_loss: 19.1597, val_MinusLogProbMetric: 19.1597

Epoch 235: val_loss did not improve from 18.70587
196/196 - 56s - loss: 18.8541 - MinusLogProbMetric: 18.8541 - val_loss: 19.1597 - val_MinusLogProbMetric: 19.1597 - lr: 1.1111e-04 - 56s/epoch - 287ms/step
Epoch 236/1000
2023-10-23 16:30:01.274 
Epoch 236/1000 
	 loss: 18.8215, MinusLogProbMetric: 18.8215, val_loss: 19.0104, val_MinusLogProbMetric: 19.0104

Epoch 236: val_loss did not improve from 18.70587
196/196 - 59s - loss: 18.8215 - MinusLogProbMetric: 18.8215 - val_loss: 19.0104 - val_MinusLogProbMetric: 19.0104 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 237/1000
2023-10-23 16:30:57.701 
Epoch 237/1000 
	 loss: 18.8038, MinusLogProbMetric: 18.8038, val_loss: 18.8366, val_MinusLogProbMetric: 18.8366

Epoch 237: val_loss did not improve from 18.70587
196/196 - 56s - loss: 18.8038 - MinusLogProbMetric: 18.8038 - val_loss: 18.8366 - val_MinusLogProbMetric: 18.8366 - lr: 1.1111e-04 - 56s/epoch - 288ms/step
Epoch 238/1000
2023-10-23 16:31:53.212 
Epoch 238/1000 
	 loss: 18.7856, MinusLogProbMetric: 18.7856, val_loss: 19.2608, val_MinusLogProbMetric: 19.2608

Epoch 238: val_loss did not improve from 18.70587
196/196 - 56s - loss: 18.7856 - MinusLogProbMetric: 18.7856 - val_loss: 19.2608 - val_MinusLogProbMetric: 19.2608 - lr: 1.1111e-04 - 56s/epoch - 283ms/step
Epoch 239/1000
2023-10-23 16:32:52.450 
Epoch 239/1000 
	 loss: 18.7108, MinusLogProbMetric: 18.7108, val_loss: 18.7535, val_MinusLogProbMetric: 18.7535

Epoch 239: val_loss did not improve from 18.70587
196/196 - 59s - loss: 18.7108 - MinusLogProbMetric: 18.7108 - val_loss: 18.7535 - val_MinusLogProbMetric: 18.7535 - lr: 1.1111e-04 - 59s/epoch - 302ms/step
Epoch 240/1000
2023-10-23 16:33:49.622 
Epoch 240/1000 
	 loss: 18.7860, MinusLogProbMetric: 18.7860, val_loss: 18.8417, val_MinusLogProbMetric: 18.8417

Epoch 240: val_loss did not improve from 18.70587
196/196 - 57s - loss: 18.7860 - MinusLogProbMetric: 18.7860 - val_loss: 18.8417 - val_MinusLogProbMetric: 18.8417 - lr: 1.1111e-04 - 57s/epoch - 292ms/step
Epoch 241/1000
2023-10-23 16:34:47.393 
Epoch 241/1000 
	 loss: 18.7849, MinusLogProbMetric: 18.7849, val_loss: 18.6204, val_MinusLogProbMetric: 18.6204

Epoch 241: val_loss improved from 18.70587 to 18.62041, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 59s - loss: 18.7849 - MinusLogProbMetric: 18.7849 - val_loss: 18.6204 - val_MinusLogProbMetric: 18.6204 - lr: 1.1111e-04 - 59s/epoch - 299ms/step
Epoch 242/1000
2023-10-23 16:35:47.898 
Epoch 242/1000 
	 loss: 18.6906, MinusLogProbMetric: 18.6906, val_loss: 18.6681, val_MinusLogProbMetric: 18.6681

Epoch 242: val_loss did not improve from 18.62041
196/196 - 60s - loss: 18.6906 - MinusLogProbMetric: 18.6906 - val_loss: 18.6681 - val_MinusLogProbMetric: 18.6681 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 243/1000
2023-10-23 16:36:46.922 
Epoch 243/1000 
	 loss: 18.7381, MinusLogProbMetric: 18.7381, val_loss: 19.0159, val_MinusLogProbMetric: 19.0159

Epoch 243: val_loss did not improve from 18.62041
196/196 - 59s - loss: 18.7381 - MinusLogProbMetric: 18.7381 - val_loss: 19.0159 - val_MinusLogProbMetric: 19.0159 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 244/1000
2023-10-23 16:37:47.128 
Epoch 244/1000 
	 loss: 18.7390, MinusLogProbMetric: 18.7390, val_loss: 19.2051, val_MinusLogProbMetric: 19.2051

Epoch 244: val_loss did not improve from 18.62041
196/196 - 60s - loss: 18.7390 - MinusLogProbMetric: 18.7390 - val_loss: 19.2051 - val_MinusLogProbMetric: 19.2051 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 245/1000
2023-10-23 16:38:46.133 
Epoch 245/1000 
	 loss: 18.6891, MinusLogProbMetric: 18.6891, val_loss: 18.6079, val_MinusLogProbMetric: 18.6079

Epoch 245: val_loss improved from 18.62041 to 18.60795, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 18.6891 - MinusLogProbMetric: 18.6891 - val_loss: 18.6079 - val_MinusLogProbMetric: 18.6079 - lr: 1.1111e-04 - 60s/epoch - 306ms/step
Epoch 246/1000
2023-10-23 16:39:47.345 
Epoch 246/1000 
	 loss: 18.6856, MinusLogProbMetric: 18.6856, val_loss: 18.7551, val_MinusLogProbMetric: 18.7551

Epoch 246: val_loss did not improve from 18.60795
196/196 - 60s - loss: 18.6856 - MinusLogProbMetric: 18.6856 - val_loss: 18.7551 - val_MinusLogProbMetric: 18.7551 - lr: 1.1111e-04 - 60s/epoch - 307ms/step
Epoch 247/1000
2023-10-23 16:40:42.890 
Epoch 247/1000 
	 loss: 18.7163, MinusLogProbMetric: 18.7163, val_loss: 18.8216, val_MinusLogProbMetric: 18.8216

Epoch 247: val_loss did not improve from 18.60795
196/196 - 56s - loss: 18.7163 - MinusLogProbMetric: 18.7163 - val_loss: 18.8216 - val_MinusLogProbMetric: 18.8216 - lr: 1.1111e-04 - 56s/epoch - 283ms/step
Epoch 248/1000
2023-10-23 16:41:37.703 
Epoch 248/1000 
	 loss: 18.7215, MinusLogProbMetric: 18.7215, val_loss: 18.7842, val_MinusLogProbMetric: 18.7842

Epoch 248: val_loss did not improve from 18.60795
196/196 - 55s - loss: 18.7215 - MinusLogProbMetric: 18.7215 - val_loss: 18.7842 - val_MinusLogProbMetric: 18.7842 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 249/1000
2023-10-23 16:42:34.832 
Epoch 249/1000 
	 loss: 18.6597, MinusLogProbMetric: 18.6597, val_loss: 18.7184, val_MinusLogProbMetric: 18.7184

Epoch 249: val_loss did not improve from 18.60795
196/196 - 57s - loss: 18.6597 - MinusLogProbMetric: 18.6597 - val_loss: 18.7184 - val_MinusLogProbMetric: 18.7184 - lr: 1.1111e-04 - 57s/epoch - 291ms/step
Epoch 250/1000
2023-10-23 16:43:37.855 
Epoch 250/1000 
	 loss: 18.6580, MinusLogProbMetric: 18.6580, val_loss: 19.7826, val_MinusLogProbMetric: 19.7826

Epoch 250: val_loss did not improve from 18.60795
196/196 - 63s - loss: 18.6580 - MinusLogProbMetric: 18.6580 - val_loss: 19.7826 - val_MinusLogProbMetric: 19.7826 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 251/1000
2023-10-23 16:44:31.847 
Epoch 251/1000 
	 loss: 18.6934, MinusLogProbMetric: 18.6934, val_loss: 18.8588, val_MinusLogProbMetric: 18.8588

Epoch 251: val_loss did not improve from 18.60795
196/196 - 54s - loss: 18.6934 - MinusLogProbMetric: 18.6934 - val_loss: 18.8588 - val_MinusLogProbMetric: 18.8588 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 252/1000
2023-10-23 16:45:29.133 
Epoch 252/1000 
	 loss: 18.7028, MinusLogProbMetric: 18.7028, val_loss: 18.5269, val_MinusLogProbMetric: 18.5269

Epoch 252: val_loss improved from 18.60795 to 18.52692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 58s - loss: 18.7028 - MinusLogProbMetric: 18.7028 - val_loss: 18.5269 - val_MinusLogProbMetric: 18.5269 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 253/1000
2023-10-23 16:46:28.110 
Epoch 253/1000 
	 loss: 18.6461, MinusLogProbMetric: 18.6461, val_loss: 18.5879, val_MinusLogProbMetric: 18.5879

Epoch 253: val_loss did not improve from 18.52692
196/196 - 58s - loss: 18.6461 - MinusLogProbMetric: 18.6461 - val_loss: 18.5879 - val_MinusLogProbMetric: 18.5879 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 254/1000
2023-10-23 16:47:25.929 
Epoch 254/1000 
	 loss: 18.6439, MinusLogProbMetric: 18.6439, val_loss: 18.6519, val_MinusLogProbMetric: 18.6519

Epoch 254: val_loss did not improve from 18.52692
196/196 - 58s - loss: 18.6439 - MinusLogProbMetric: 18.6439 - val_loss: 18.6519 - val_MinusLogProbMetric: 18.6519 - lr: 1.1111e-04 - 58s/epoch - 295ms/step
Epoch 255/1000
2023-10-23 16:48:15.680 
Epoch 255/1000 
	 loss: 18.7250, MinusLogProbMetric: 18.7250, val_loss: 18.9647, val_MinusLogProbMetric: 18.9647

Epoch 255: val_loss did not improve from 18.52692
196/196 - 50s - loss: 18.7250 - MinusLogProbMetric: 18.7250 - val_loss: 18.9647 - val_MinusLogProbMetric: 18.9647 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 256/1000
2023-10-23 16:49:05.955 
Epoch 256/1000 
	 loss: 18.6440, MinusLogProbMetric: 18.6440, val_loss: 19.0618, val_MinusLogProbMetric: 19.0618

Epoch 256: val_loss did not improve from 18.52692
196/196 - 50s - loss: 18.6440 - MinusLogProbMetric: 18.6440 - val_loss: 19.0618 - val_MinusLogProbMetric: 19.0618 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 257/1000
2023-10-23 16:49:55.220 
Epoch 257/1000 
	 loss: 18.6796, MinusLogProbMetric: 18.6796, val_loss: 18.6374, val_MinusLogProbMetric: 18.6374

Epoch 257: val_loss did not improve from 18.52692
196/196 - 49s - loss: 18.6796 - MinusLogProbMetric: 18.6796 - val_loss: 18.6374 - val_MinusLogProbMetric: 18.6374 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 258/1000
2023-10-23 16:50:44.963 
Epoch 258/1000 
	 loss: 18.6095, MinusLogProbMetric: 18.6095, val_loss: 18.9668, val_MinusLogProbMetric: 18.9668

Epoch 258: val_loss did not improve from 18.52692
196/196 - 50s - loss: 18.6095 - MinusLogProbMetric: 18.6095 - val_loss: 18.9668 - val_MinusLogProbMetric: 18.9668 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 259/1000
2023-10-23 16:51:34.860 
Epoch 259/1000 
	 loss: 18.6842, MinusLogProbMetric: 18.6842, val_loss: 18.7117, val_MinusLogProbMetric: 18.7117

Epoch 259: val_loss did not improve from 18.52692
196/196 - 50s - loss: 18.6842 - MinusLogProbMetric: 18.6842 - val_loss: 18.7117 - val_MinusLogProbMetric: 18.7117 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 260/1000
2023-10-23 16:52:24.770 
Epoch 260/1000 
	 loss: 18.5437, MinusLogProbMetric: 18.5437, val_loss: 18.7215, val_MinusLogProbMetric: 18.7215

Epoch 260: val_loss did not improve from 18.52692
196/196 - 50s - loss: 18.5437 - MinusLogProbMetric: 18.5437 - val_loss: 18.7215 - val_MinusLogProbMetric: 18.7215 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 261/1000
2023-10-23 16:53:14.873 
Epoch 261/1000 
	 loss: 18.6953, MinusLogProbMetric: 18.6953, val_loss: 18.5449, val_MinusLogProbMetric: 18.5449

Epoch 261: val_loss did not improve from 18.52692
196/196 - 50s - loss: 18.6953 - MinusLogProbMetric: 18.6953 - val_loss: 18.5449 - val_MinusLogProbMetric: 18.5449 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 262/1000
2023-10-23 16:54:04.064 
Epoch 262/1000 
	 loss: 18.6155, MinusLogProbMetric: 18.6155, val_loss: 18.8464, val_MinusLogProbMetric: 18.8464

Epoch 262: val_loss did not improve from 18.52692
196/196 - 49s - loss: 18.6155 - MinusLogProbMetric: 18.6155 - val_loss: 18.8464 - val_MinusLogProbMetric: 18.8464 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 263/1000
2023-10-23 16:54:52.798 
Epoch 263/1000 
	 loss: 18.6202, MinusLogProbMetric: 18.6202, val_loss: 18.5943, val_MinusLogProbMetric: 18.5943

Epoch 263: val_loss did not improve from 18.52692
196/196 - 49s - loss: 18.6202 - MinusLogProbMetric: 18.6202 - val_loss: 18.5943 - val_MinusLogProbMetric: 18.5943 - lr: 1.1111e-04 - 49s/epoch - 249ms/step
Epoch 264/1000
2023-10-23 16:55:43.591 
Epoch 264/1000 
	 loss: 18.5726, MinusLogProbMetric: 18.5726, val_loss: 18.5502, val_MinusLogProbMetric: 18.5502

Epoch 264: val_loss did not improve from 18.52692
196/196 - 51s - loss: 18.5726 - MinusLogProbMetric: 18.5726 - val_loss: 18.5502 - val_MinusLogProbMetric: 18.5502 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 265/1000
2023-10-23 16:56:33.179 
Epoch 265/1000 
	 loss: 18.5839, MinusLogProbMetric: 18.5839, val_loss: 18.6928, val_MinusLogProbMetric: 18.6928

Epoch 265: val_loss did not improve from 18.52692
196/196 - 50s - loss: 18.5839 - MinusLogProbMetric: 18.5839 - val_loss: 18.6928 - val_MinusLogProbMetric: 18.6928 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 266/1000
2023-10-23 16:57:22.377 
Epoch 266/1000 
	 loss: 18.6862, MinusLogProbMetric: 18.6862, val_loss: 18.6494, val_MinusLogProbMetric: 18.6494

Epoch 266: val_loss did not improve from 18.52692
196/196 - 49s - loss: 18.6862 - MinusLogProbMetric: 18.6862 - val_loss: 18.6494 - val_MinusLogProbMetric: 18.6494 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 267/1000
2023-10-23 16:58:13.314 
Epoch 267/1000 
	 loss: 18.5921, MinusLogProbMetric: 18.5921, val_loss: 18.7501, val_MinusLogProbMetric: 18.7501

Epoch 267: val_loss did not improve from 18.52692
196/196 - 51s - loss: 18.5921 - MinusLogProbMetric: 18.5921 - val_loss: 18.7501 - val_MinusLogProbMetric: 18.7501 - lr: 1.1111e-04 - 51s/epoch - 260ms/step
Epoch 268/1000
2023-10-23 16:59:02.540 
Epoch 268/1000 
	 loss: 18.5712, MinusLogProbMetric: 18.5712, val_loss: 18.6546, val_MinusLogProbMetric: 18.6546

Epoch 268: val_loss did not improve from 18.52692
196/196 - 49s - loss: 18.5712 - MinusLogProbMetric: 18.5712 - val_loss: 18.6546 - val_MinusLogProbMetric: 18.6546 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 269/1000
2023-10-23 16:59:53.308 
Epoch 269/1000 
	 loss: 18.5206, MinusLogProbMetric: 18.5206, val_loss: 18.5760, val_MinusLogProbMetric: 18.5760

Epoch 269: val_loss did not improve from 18.52692
196/196 - 51s - loss: 18.5206 - MinusLogProbMetric: 18.5206 - val_loss: 18.5760 - val_MinusLogProbMetric: 18.5760 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 270/1000
2023-10-23 17:00:42.444 
Epoch 270/1000 
	 loss: 18.5623, MinusLogProbMetric: 18.5623, val_loss: 18.6088, val_MinusLogProbMetric: 18.6088

Epoch 270: val_loss did not improve from 18.52692
196/196 - 49s - loss: 18.5623 - MinusLogProbMetric: 18.5623 - val_loss: 18.6088 - val_MinusLogProbMetric: 18.6088 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 271/1000
2023-10-23 17:01:33.499 
Epoch 271/1000 
	 loss: 18.5046, MinusLogProbMetric: 18.5046, val_loss: 18.6118, val_MinusLogProbMetric: 18.6118

Epoch 271: val_loss did not improve from 18.52692
196/196 - 51s - loss: 18.5046 - MinusLogProbMetric: 18.5046 - val_loss: 18.6118 - val_MinusLogProbMetric: 18.6118 - lr: 1.1111e-04 - 51s/epoch - 260ms/step
Epoch 272/1000
2023-10-23 17:02:22.111 
Epoch 272/1000 
	 loss: 18.5201, MinusLogProbMetric: 18.5201, val_loss: 18.7238, val_MinusLogProbMetric: 18.7238

Epoch 272: val_loss did not improve from 18.52692
196/196 - 49s - loss: 18.5201 - MinusLogProbMetric: 18.5201 - val_loss: 18.7238 - val_MinusLogProbMetric: 18.7238 - lr: 1.1111e-04 - 49s/epoch - 248ms/step
Epoch 273/1000
2023-10-23 17:03:12.016 
Epoch 273/1000 
	 loss: 18.6245, MinusLogProbMetric: 18.6245, val_loss: 18.9962, val_MinusLogProbMetric: 18.9962

Epoch 273: val_loss did not improve from 18.52692
196/196 - 50s - loss: 18.6245 - MinusLogProbMetric: 18.6245 - val_loss: 18.9962 - val_MinusLogProbMetric: 18.9962 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 274/1000
2023-10-23 17:04:01.137 
Epoch 274/1000 
	 loss: 18.6133, MinusLogProbMetric: 18.6133, val_loss: 18.5010, val_MinusLogProbMetric: 18.5010

Epoch 274: val_loss improved from 18.52692 to 18.50104, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 18.6133 - MinusLogProbMetric: 18.6133 - val_loss: 18.5010 - val_MinusLogProbMetric: 18.5010 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 275/1000
2023-10-23 17:04:50.521 
Epoch 275/1000 
	 loss: 18.5418, MinusLogProbMetric: 18.5418, val_loss: 18.7165, val_MinusLogProbMetric: 18.7165

Epoch 275: val_loss did not improve from 18.50104
196/196 - 49s - loss: 18.5418 - MinusLogProbMetric: 18.5418 - val_loss: 18.7165 - val_MinusLogProbMetric: 18.7165 - lr: 1.1111e-04 - 49s/epoch - 248ms/step
Epoch 276/1000
2023-10-23 17:05:40.589 
Epoch 276/1000 
	 loss: 18.4831, MinusLogProbMetric: 18.4831, val_loss: 18.4122, val_MinusLogProbMetric: 18.4122

Epoch 276: val_loss improved from 18.50104 to 18.41218, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 18.4831 - MinusLogProbMetric: 18.4831 - val_loss: 18.4122 - val_MinusLogProbMetric: 18.4122 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 277/1000
2023-10-23 17:06:30.250 
Epoch 277/1000 
	 loss: 18.4931, MinusLogProbMetric: 18.4931, val_loss: 19.0576, val_MinusLogProbMetric: 19.0576

Epoch 277: val_loss did not improve from 18.41218
196/196 - 49s - loss: 18.4931 - MinusLogProbMetric: 18.4931 - val_loss: 19.0576 - val_MinusLogProbMetric: 19.0576 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 278/1000
2023-10-23 17:07:19.630 
Epoch 278/1000 
	 loss: 18.5783, MinusLogProbMetric: 18.5783, val_loss: 18.7378, val_MinusLogProbMetric: 18.7378

Epoch 278: val_loss did not improve from 18.41218
196/196 - 49s - loss: 18.5783 - MinusLogProbMetric: 18.5783 - val_loss: 18.7378 - val_MinusLogProbMetric: 18.7378 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 279/1000
2023-10-23 17:08:09.948 
Epoch 279/1000 
	 loss: 18.5083, MinusLogProbMetric: 18.5083, val_loss: 18.6304, val_MinusLogProbMetric: 18.6304

Epoch 279: val_loss did not improve from 18.41218
196/196 - 50s - loss: 18.5083 - MinusLogProbMetric: 18.5083 - val_loss: 18.6304 - val_MinusLogProbMetric: 18.6304 - lr: 1.1111e-04 - 50s/epoch - 257ms/step
Epoch 280/1000
2023-10-23 17:08:59.365 
Epoch 280/1000 
	 loss: 18.4560, MinusLogProbMetric: 18.4560, val_loss: 19.5279, val_MinusLogProbMetric: 19.5279

Epoch 280: val_loss did not improve from 18.41218
196/196 - 49s - loss: 18.4560 - MinusLogProbMetric: 18.4560 - val_loss: 19.5279 - val_MinusLogProbMetric: 19.5279 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 281/1000
2023-10-23 17:09:48.669 
Epoch 281/1000 
	 loss: 18.5374, MinusLogProbMetric: 18.5374, val_loss: 19.0231, val_MinusLogProbMetric: 19.0231

Epoch 281: val_loss did not improve from 18.41218
196/196 - 49s - loss: 18.5374 - MinusLogProbMetric: 18.5374 - val_loss: 19.0231 - val_MinusLogProbMetric: 19.0231 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 282/1000
2023-10-23 17:10:43.978 
Epoch 282/1000 
	 loss: 18.5269, MinusLogProbMetric: 18.5269, val_loss: 18.5347, val_MinusLogProbMetric: 18.5347

Epoch 282: val_loss did not improve from 18.41218
196/196 - 55s - loss: 18.5269 - MinusLogProbMetric: 18.5269 - val_loss: 18.5347 - val_MinusLogProbMetric: 18.5347 - lr: 1.1111e-04 - 55s/epoch - 282ms/step
Epoch 283/1000
2023-10-23 17:11:32.678 
Epoch 283/1000 
	 loss: 18.4339, MinusLogProbMetric: 18.4339, val_loss: 18.8442, val_MinusLogProbMetric: 18.8442

Epoch 283: val_loss did not improve from 18.41218
196/196 - 49s - loss: 18.4339 - MinusLogProbMetric: 18.4339 - val_loss: 18.8442 - val_MinusLogProbMetric: 18.8442 - lr: 1.1111e-04 - 49s/epoch - 248ms/step
Epoch 284/1000
2023-10-23 17:12:21.924 
Epoch 284/1000 
	 loss: 18.4627, MinusLogProbMetric: 18.4627, val_loss: 18.7626, val_MinusLogProbMetric: 18.7626

Epoch 284: val_loss did not improve from 18.41218
196/196 - 49s - loss: 18.4627 - MinusLogProbMetric: 18.4627 - val_loss: 18.7626 - val_MinusLogProbMetric: 18.7626 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 285/1000
2023-10-23 17:13:12.062 
Epoch 285/1000 
	 loss: 18.4706, MinusLogProbMetric: 18.4706, val_loss: 18.3806, val_MinusLogProbMetric: 18.3806

Epoch 285: val_loss improved from 18.41218 to 18.38060, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 18.4706 - MinusLogProbMetric: 18.4706 - val_loss: 18.3806 - val_MinusLogProbMetric: 18.3806 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 286/1000
2023-10-23 17:14:01.544 
Epoch 286/1000 
	 loss: 18.4419, MinusLogProbMetric: 18.4419, val_loss: 18.9600, val_MinusLogProbMetric: 18.9600

Epoch 286: val_loss did not improve from 18.38060
196/196 - 49s - loss: 18.4419 - MinusLogProbMetric: 18.4419 - val_loss: 18.9600 - val_MinusLogProbMetric: 18.9600 - lr: 1.1111e-04 - 49s/epoch - 249ms/step
Epoch 287/1000
2023-10-23 17:14:50.319 
Epoch 287/1000 
	 loss: 18.4999, MinusLogProbMetric: 18.4999, val_loss: 18.4073, val_MinusLogProbMetric: 18.4073

Epoch 287: val_loss did not improve from 18.38060
196/196 - 49s - loss: 18.4999 - MinusLogProbMetric: 18.4999 - val_loss: 18.4073 - val_MinusLogProbMetric: 18.4073 - lr: 1.1111e-04 - 49s/epoch - 249ms/step
Epoch 288/1000
2023-10-23 17:15:43.849 
Epoch 288/1000 
	 loss: 18.4804, MinusLogProbMetric: 18.4804, val_loss: 18.4300, val_MinusLogProbMetric: 18.4300

Epoch 288: val_loss did not improve from 18.38060
196/196 - 54s - loss: 18.4804 - MinusLogProbMetric: 18.4804 - val_loss: 18.4300 - val_MinusLogProbMetric: 18.4300 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 289/1000
2023-10-23 17:16:33.660 
Epoch 289/1000 
	 loss: 18.4290, MinusLogProbMetric: 18.4290, val_loss: 18.6719, val_MinusLogProbMetric: 18.6719

Epoch 289: val_loss did not improve from 18.38060
196/196 - 50s - loss: 18.4290 - MinusLogProbMetric: 18.4290 - val_loss: 18.6719 - val_MinusLogProbMetric: 18.6719 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 290/1000
2023-10-23 17:17:23.217 
Epoch 290/1000 
	 loss: 18.4615, MinusLogProbMetric: 18.4615, val_loss: 18.4398, val_MinusLogProbMetric: 18.4398

Epoch 290: val_loss did not improve from 18.38060
196/196 - 50s - loss: 18.4615 - MinusLogProbMetric: 18.4615 - val_loss: 18.4398 - val_MinusLogProbMetric: 18.4398 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 291/1000
2023-10-23 17:18:12.822 
Epoch 291/1000 
	 loss: 18.4567, MinusLogProbMetric: 18.4567, val_loss: 18.5710, val_MinusLogProbMetric: 18.5710

Epoch 291: val_loss did not improve from 18.38060
196/196 - 50s - loss: 18.4567 - MinusLogProbMetric: 18.4567 - val_loss: 18.5710 - val_MinusLogProbMetric: 18.5710 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 292/1000
2023-10-23 17:19:02.580 
Epoch 292/1000 
	 loss: 18.4482, MinusLogProbMetric: 18.4482, val_loss: 18.9029, val_MinusLogProbMetric: 18.9029

Epoch 292: val_loss did not improve from 18.38060
196/196 - 50s - loss: 18.4482 - MinusLogProbMetric: 18.4482 - val_loss: 18.9029 - val_MinusLogProbMetric: 18.9029 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 293/1000
2023-10-23 17:19:51.379 
Epoch 293/1000 
	 loss: 18.4667, MinusLogProbMetric: 18.4667, val_loss: 18.8193, val_MinusLogProbMetric: 18.8193

Epoch 293: val_loss did not improve from 18.38060
196/196 - 49s - loss: 18.4667 - MinusLogProbMetric: 18.4667 - val_loss: 18.8193 - val_MinusLogProbMetric: 18.8193 - lr: 1.1111e-04 - 49s/epoch - 249ms/step
Epoch 294/1000
2023-10-23 17:20:41.639 
Epoch 294/1000 
	 loss: 18.4542, MinusLogProbMetric: 18.4542, val_loss: 18.8067, val_MinusLogProbMetric: 18.8067

Epoch 294: val_loss did not improve from 18.38060
196/196 - 50s - loss: 18.4542 - MinusLogProbMetric: 18.4542 - val_loss: 18.8067 - val_MinusLogProbMetric: 18.8067 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 295/1000
2023-10-23 17:21:31.699 
Epoch 295/1000 
	 loss: 18.4647, MinusLogProbMetric: 18.4647, val_loss: 18.6006, val_MinusLogProbMetric: 18.6006

Epoch 295: val_loss did not improve from 18.38060
196/196 - 50s - loss: 18.4647 - MinusLogProbMetric: 18.4647 - val_loss: 18.6006 - val_MinusLogProbMetric: 18.6006 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 296/1000
2023-10-23 17:22:21.336 
Epoch 296/1000 
	 loss: 18.4404, MinusLogProbMetric: 18.4404, val_loss: 18.4262, val_MinusLogProbMetric: 18.4262

Epoch 296: val_loss did not improve from 18.38060
196/196 - 50s - loss: 18.4404 - MinusLogProbMetric: 18.4404 - val_loss: 18.4262 - val_MinusLogProbMetric: 18.4262 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 297/1000
2023-10-23 17:23:10.651 
Epoch 297/1000 
	 loss: 18.3484, MinusLogProbMetric: 18.3484, val_loss: 18.5878, val_MinusLogProbMetric: 18.5878

Epoch 297: val_loss did not improve from 18.38060
196/196 - 49s - loss: 18.3484 - MinusLogProbMetric: 18.3484 - val_loss: 18.5878 - val_MinusLogProbMetric: 18.5878 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 298/1000
2023-10-23 17:24:03.129 
Epoch 298/1000 
	 loss: 18.4068, MinusLogProbMetric: 18.4068, val_loss: 18.5259, val_MinusLogProbMetric: 18.5259

Epoch 298: val_loss did not improve from 18.38060
196/196 - 52s - loss: 18.4068 - MinusLogProbMetric: 18.4068 - val_loss: 18.5259 - val_MinusLogProbMetric: 18.5259 - lr: 1.1111e-04 - 52s/epoch - 268ms/step
Epoch 299/1000
2023-10-23 17:24:52.292 
Epoch 299/1000 
	 loss: 18.3606, MinusLogProbMetric: 18.3606, val_loss: 18.5954, val_MinusLogProbMetric: 18.5954

Epoch 299: val_loss did not improve from 18.38060
196/196 - 49s - loss: 18.3606 - MinusLogProbMetric: 18.3606 - val_loss: 18.5954 - val_MinusLogProbMetric: 18.5954 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 300/1000
2023-10-23 17:25:41.597 
Epoch 300/1000 
	 loss: 18.3572, MinusLogProbMetric: 18.3572, val_loss: 18.9772, val_MinusLogProbMetric: 18.9772

Epoch 300: val_loss did not improve from 18.38060
196/196 - 49s - loss: 18.3572 - MinusLogProbMetric: 18.3572 - val_loss: 18.9772 - val_MinusLogProbMetric: 18.9772 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 301/1000
2023-10-23 17:26:32.456 
Epoch 301/1000 
	 loss: 18.5634, MinusLogProbMetric: 18.5634, val_loss: 18.7514, val_MinusLogProbMetric: 18.7514

Epoch 301: val_loss did not improve from 18.38060
196/196 - 51s - loss: 18.5634 - MinusLogProbMetric: 18.5634 - val_loss: 18.7514 - val_MinusLogProbMetric: 18.7514 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 302/1000
2023-10-23 17:27:21.208 
Epoch 302/1000 
	 loss: 18.3563, MinusLogProbMetric: 18.3563, val_loss: 18.5550, val_MinusLogProbMetric: 18.5550

Epoch 302: val_loss did not improve from 18.38060
196/196 - 49s - loss: 18.3563 - MinusLogProbMetric: 18.3563 - val_loss: 18.5550 - val_MinusLogProbMetric: 18.5550 - lr: 1.1111e-04 - 49s/epoch - 249ms/step
Epoch 303/1000
2023-10-23 17:28:10.542 
Epoch 303/1000 
	 loss: 18.3869, MinusLogProbMetric: 18.3869, val_loss: 18.5140, val_MinusLogProbMetric: 18.5140

Epoch 303: val_loss did not improve from 18.38060
196/196 - 49s - loss: 18.3869 - MinusLogProbMetric: 18.3869 - val_loss: 18.5140 - val_MinusLogProbMetric: 18.5140 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 304/1000
2023-10-23 17:29:00.019 
Epoch 304/1000 
	 loss: 18.3548, MinusLogProbMetric: 18.3548, val_loss: 19.0952, val_MinusLogProbMetric: 19.0952

Epoch 304: val_loss did not improve from 18.38060
196/196 - 49s - loss: 18.3548 - MinusLogProbMetric: 18.3548 - val_loss: 19.0952 - val_MinusLogProbMetric: 19.0952 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 305/1000
2023-10-23 17:29:50.293 
Epoch 305/1000 
	 loss: 18.3466, MinusLogProbMetric: 18.3466, val_loss: 18.6323, val_MinusLogProbMetric: 18.6323

Epoch 305: val_loss did not improve from 18.38060
196/196 - 50s - loss: 18.3466 - MinusLogProbMetric: 18.3466 - val_loss: 18.6323 - val_MinusLogProbMetric: 18.6323 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 306/1000
2023-10-23 17:30:39.353 
Epoch 306/1000 
	 loss: 18.3981, MinusLogProbMetric: 18.3981, val_loss: 18.3304, val_MinusLogProbMetric: 18.3304

Epoch 306: val_loss improved from 18.38060 to 18.33042, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 18.3981 - MinusLogProbMetric: 18.3981 - val_loss: 18.3304 - val_MinusLogProbMetric: 18.3304 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 307/1000
2023-10-23 17:31:29.678 
Epoch 307/1000 
	 loss: 18.5410, MinusLogProbMetric: 18.5410, val_loss: 19.8967, val_MinusLogProbMetric: 19.8967

Epoch 307: val_loss did not improve from 18.33042
196/196 - 50s - loss: 18.5410 - MinusLogProbMetric: 18.5410 - val_loss: 19.8967 - val_MinusLogProbMetric: 19.8967 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 308/1000
2023-10-23 17:32:19.589 
Epoch 308/1000 
	 loss: 18.4133, MinusLogProbMetric: 18.4133, val_loss: 18.3756, val_MinusLogProbMetric: 18.3756

Epoch 308: val_loss did not improve from 18.33042
196/196 - 50s - loss: 18.4133 - MinusLogProbMetric: 18.4133 - val_loss: 18.3756 - val_MinusLogProbMetric: 18.3756 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 309/1000
2023-10-23 17:33:08.730 
Epoch 309/1000 
	 loss: 18.3514, MinusLogProbMetric: 18.3514, val_loss: 19.1300, val_MinusLogProbMetric: 19.1300

Epoch 309: val_loss did not improve from 18.33042
196/196 - 49s - loss: 18.3514 - MinusLogProbMetric: 18.3514 - val_loss: 19.1300 - val_MinusLogProbMetric: 19.1300 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 310/1000
2023-10-23 17:33:58.463 
Epoch 310/1000 
	 loss: 18.5277, MinusLogProbMetric: 18.5277, val_loss: 18.6757, val_MinusLogProbMetric: 18.6757

Epoch 310: val_loss did not improve from 18.33042
196/196 - 50s - loss: 18.5277 - MinusLogProbMetric: 18.5277 - val_loss: 18.6757 - val_MinusLogProbMetric: 18.6757 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 311/1000
2023-10-23 17:34:49.985 
Epoch 311/1000 
	 loss: 18.3041, MinusLogProbMetric: 18.3041, val_loss: 18.7734, val_MinusLogProbMetric: 18.7734

Epoch 311: val_loss did not improve from 18.33042
196/196 - 52s - loss: 18.3041 - MinusLogProbMetric: 18.3041 - val_loss: 18.7734 - val_MinusLogProbMetric: 18.7734 - lr: 1.1111e-04 - 52s/epoch - 263ms/step
Epoch 312/1000
2023-10-23 17:35:39.171 
Epoch 312/1000 
	 loss: 18.4423, MinusLogProbMetric: 18.4423, val_loss: 18.3175, val_MinusLogProbMetric: 18.3175

Epoch 312: val_loss improved from 18.33042 to 18.31749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 18.4423 - MinusLogProbMetric: 18.4423 - val_loss: 18.3175 - val_MinusLogProbMetric: 18.3175 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 313/1000
2023-10-23 17:36:29.079 
Epoch 313/1000 
	 loss: 18.3184, MinusLogProbMetric: 18.3184, val_loss: 19.8683, val_MinusLogProbMetric: 19.8683

Epoch 313: val_loss did not improve from 18.31749
196/196 - 49s - loss: 18.3184 - MinusLogProbMetric: 18.3184 - val_loss: 19.8683 - val_MinusLogProbMetric: 19.8683 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 314/1000
2023-10-23 17:37:19.928 
Epoch 314/1000 
	 loss: 18.3103, MinusLogProbMetric: 18.3103, val_loss: 18.6047, val_MinusLogProbMetric: 18.6047

Epoch 314: val_loss did not improve from 18.31749
196/196 - 51s - loss: 18.3103 - MinusLogProbMetric: 18.3103 - val_loss: 18.6047 - val_MinusLogProbMetric: 18.6047 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 315/1000
2023-10-23 17:38:09.775 
Epoch 315/1000 
	 loss: 18.3367, MinusLogProbMetric: 18.3367, val_loss: 18.5819, val_MinusLogProbMetric: 18.5819

Epoch 315: val_loss did not improve from 18.31749
196/196 - 50s - loss: 18.3367 - MinusLogProbMetric: 18.3367 - val_loss: 18.5819 - val_MinusLogProbMetric: 18.5819 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 316/1000
2023-10-23 17:38:59.283 
Epoch 316/1000 
	 loss: 18.3606, MinusLogProbMetric: 18.3606, val_loss: 18.5765, val_MinusLogProbMetric: 18.5765

Epoch 316: val_loss did not improve from 18.31749
196/196 - 50s - loss: 18.3606 - MinusLogProbMetric: 18.3606 - val_loss: 18.5765 - val_MinusLogProbMetric: 18.5765 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 317/1000
2023-10-23 17:39:48.719 
Epoch 317/1000 
	 loss: 18.3358, MinusLogProbMetric: 18.3358, val_loss: 18.4610, val_MinusLogProbMetric: 18.4610

Epoch 317: val_loss did not improve from 18.31749
196/196 - 49s - loss: 18.3358 - MinusLogProbMetric: 18.3358 - val_loss: 18.4610 - val_MinusLogProbMetric: 18.4610 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 318/1000
2023-10-23 17:40:39.481 
Epoch 318/1000 
	 loss: 18.3560, MinusLogProbMetric: 18.3560, val_loss: 18.2326, val_MinusLogProbMetric: 18.2326

Epoch 318: val_loss improved from 18.31749 to 18.23256, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 18.3560 - MinusLogProbMetric: 18.3560 - val_loss: 18.2326 - val_MinusLogProbMetric: 18.2326 - lr: 1.1111e-04 - 51s/epoch - 263ms/step
Epoch 319/1000
2023-10-23 17:41:29.395 
Epoch 319/1000 
	 loss: 18.3370, MinusLogProbMetric: 18.3370, val_loss: 18.9352, val_MinusLogProbMetric: 18.9352

Epoch 319: val_loss did not improve from 18.23256
196/196 - 49s - loss: 18.3370 - MinusLogProbMetric: 18.3370 - val_loss: 18.9352 - val_MinusLogProbMetric: 18.9352 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 320/1000
2023-10-23 17:42:20.849 
Epoch 320/1000 
	 loss: 18.3233, MinusLogProbMetric: 18.3233, val_loss: 18.2809, val_MinusLogProbMetric: 18.2809

Epoch 320: val_loss did not improve from 18.23256
196/196 - 51s - loss: 18.3233 - MinusLogProbMetric: 18.3233 - val_loss: 18.2809 - val_MinusLogProbMetric: 18.2809 - lr: 1.1111e-04 - 51s/epoch - 263ms/step
Epoch 321/1000
2023-10-23 17:43:10.152 
Epoch 321/1000 
	 loss: 18.3043, MinusLogProbMetric: 18.3043, val_loss: 18.3007, val_MinusLogProbMetric: 18.3007

Epoch 321: val_loss did not improve from 18.23256
196/196 - 49s - loss: 18.3043 - MinusLogProbMetric: 18.3043 - val_loss: 18.3007 - val_MinusLogProbMetric: 18.3007 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 322/1000
2023-10-23 17:43:59.619 
Epoch 322/1000 
	 loss: 18.2494, MinusLogProbMetric: 18.2494, val_loss: 18.2104, val_MinusLogProbMetric: 18.2104

Epoch 322: val_loss improved from 18.23256 to 18.21039, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 18.2494 - MinusLogProbMetric: 18.2494 - val_loss: 18.2104 - val_MinusLogProbMetric: 18.2104 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 323/1000
2023-10-23 17:44:52.088 
Epoch 323/1000 
	 loss: 18.3070, MinusLogProbMetric: 18.3070, val_loss: 18.1981, val_MinusLogProbMetric: 18.1981

Epoch 323: val_loss improved from 18.21039 to 18.19807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 18.3070 - MinusLogProbMetric: 18.3070 - val_loss: 18.1981 - val_MinusLogProbMetric: 18.1981 - lr: 1.1111e-04 - 52s/epoch - 268ms/step
Epoch 324/1000
2023-10-23 17:45:42.060 
Epoch 324/1000 
	 loss: 18.2665, MinusLogProbMetric: 18.2665, val_loss: 18.4024, val_MinusLogProbMetric: 18.4024

Epoch 324: val_loss did not improve from 18.19807
196/196 - 49s - loss: 18.2665 - MinusLogProbMetric: 18.2665 - val_loss: 18.4024 - val_MinusLogProbMetric: 18.4024 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 325/1000
2023-10-23 17:46:32.146 
Epoch 325/1000 
	 loss: 18.3334, MinusLogProbMetric: 18.3334, val_loss: 18.5066, val_MinusLogProbMetric: 18.5066

Epoch 325: val_loss did not improve from 18.19807
196/196 - 50s - loss: 18.3334 - MinusLogProbMetric: 18.3334 - val_loss: 18.5066 - val_MinusLogProbMetric: 18.5066 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 326/1000
2023-10-23 17:47:21.876 
Epoch 326/1000 
	 loss: 18.3452, MinusLogProbMetric: 18.3452, val_loss: 18.4596, val_MinusLogProbMetric: 18.4596

Epoch 326: val_loss did not improve from 18.19807
196/196 - 50s - loss: 18.3452 - MinusLogProbMetric: 18.3452 - val_loss: 18.4596 - val_MinusLogProbMetric: 18.4596 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 327/1000
2023-10-23 17:48:10.770 
Epoch 327/1000 
	 loss: 18.2478, MinusLogProbMetric: 18.2478, val_loss: 18.7656, val_MinusLogProbMetric: 18.7656

Epoch 327: val_loss did not improve from 18.19807
196/196 - 49s - loss: 18.2478 - MinusLogProbMetric: 18.2478 - val_loss: 18.7656 - val_MinusLogProbMetric: 18.7656 - lr: 1.1111e-04 - 49s/epoch - 249ms/step
Epoch 328/1000
2023-10-23 17:49:02.306 
Epoch 328/1000 
	 loss: 18.3310, MinusLogProbMetric: 18.3310, val_loss: 18.6933, val_MinusLogProbMetric: 18.6933

Epoch 328: val_loss did not improve from 18.19807
196/196 - 52s - loss: 18.3310 - MinusLogProbMetric: 18.3310 - val_loss: 18.6933 - val_MinusLogProbMetric: 18.6933 - lr: 1.1111e-04 - 52s/epoch - 263ms/step
Epoch 329/1000
2023-10-23 17:49:53.221 
Epoch 329/1000 
	 loss: 18.2978, MinusLogProbMetric: 18.2978, val_loss: 18.2422, val_MinusLogProbMetric: 18.2422

Epoch 329: val_loss did not improve from 18.19807
196/196 - 51s - loss: 18.2978 - MinusLogProbMetric: 18.2978 - val_loss: 18.2422 - val_MinusLogProbMetric: 18.2422 - lr: 1.1111e-04 - 51s/epoch - 260ms/step
Epoch 330/1000
2023-10-23 17:50:42.518 
Epoch 330/1000 
	 loss: 18.3069, MinusLogProbMetric: 18.3069, val_loss: 18.6852, val_MinusLogProbMetric: 18.6852

Epoch 330: val_loss did not improve from 18.19807
196/196 - 49s - loss: 18.3069 - MinusLogProbMetric: 18.3069 - val_loss: 18.6852 - val_MinusLogProbMetric: 18.6852 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 331/1000
2023-10-23 17:51:35.800 
Epoch 331/1000 
	 loss: 18.3834, MinusLogProbMetric: 18.3834, val_loss: 19.3698, val_MinusLogProbMetric: 19.3698

Epoch 331: val_loss did not improve from 18.19807
196/196 - 53s - loss: 18.3834 - MinusLogProbMetric: 18.3834 - val_loss: 19.3698 - val_MinusLogProbMetric: 19.3698 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 332/1000
2023-10-23 17:52:24.897 
Epoch 332/1000 
	 loss: 18.3588, MinusLogProbMetric: 18.3588, val_loss: 18.4610, val_MinusLogProbMetric: 18.4610

Epoch 332: val_loss did not improve from 18.19807
196/196 - 49s - loss: 18.3588 - MinusLogProbMetric: 18.3588 - val_loss: 18.4610 - val_MinusLogProbMetric: 18.4610 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 333/1000
2023-10-23 17:53:17.217 
Epoch 333/1000 
	 loss: 18.2550, MinusLogProbMetric: 18.2550, val_loss: 18.3275, val_MinusLogProbMetric: 18.3275

Epoch 333: val_loss did not improve from 18.19807
196/196 - 52s - loss: 18.2550 - MinusLogProbMetric: 18.2550 - val_loss: 18.3275 - val_MinusLogProbMetric: 18.3275 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 334/1000
2023-10-23 17:54:07.254 
Epoch 334/1000 
	 loss: 18.3158, MinusLogProbMetric: 18.3158, val_loss: 18.5420, val_MinusLogProbMetric: 18.5420

Epoch 334: val_loss did not improve from 18.19807
196/196 - 50s - loss: 18.3158 - MinusLogProbMetric: 18.3158 - val_loss: 18.5420 - val_MinusLogProbMetric: 18.5420 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 335/1000
2023-10-23 17:54:57.231 
Epoch 335/1000 
	 loss: 18.2411, MinusLogProbMetric: 18.2411, val_loss: 18.4564, val_MinusLogProbMetric: 18.4564

Epoch 335: val_loss did not improve from 18.19807
196/196 - 50s - loss: 18.2411 - MinusLogProbMetric: 18.2411 - val_loss: 18.4564 - val_MinusLogProbMetric: 18.4564 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 336/1000
2023-10-23 17:55:49.151 
Epoch 336/1000 
	 loss: 18.2844, MinusLogProbMetric: 18.2844, val_loss: 18.6635, val_MinusLogProbMetric: 18.6635

Epoch 336: val_loss did not improve from 18.19807
196/196 - 52s - loss: 18.2844 - MinusLogProbMetric: 18.2844 - val_loss: 18.6635 - val_MinusLogProbMetric: 18.6635 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 337/1000
2023-10-23 17:56:38.572 
Epoch 337/1000 
	 loss: 18.3261, MinusLogProbMetric: 18.3261, val_loss: 18.1939, val_MinusLogProbMetric: 18.1939

Epoch 337: val_loss improved from 18.19807 to 18.19391, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 18.3261 - MinusLogProbMetric: 18.3261 - val_loss: 18.1939 - val_MinusLogProbMetric: 18.1939 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 338/1000
2023-10-23 17:57:28.443 
Epoch 338/1000 
	 loss: 18.2318, MinusLogProbMetric: 18.2318, val_loss: 18.4705, val_MinusLogProbMetric: 18.4705

Epoch 338: val_loss did not improve from 18.19391
196/196 - 49s - loss: 18.2318 - MinusLogProbMetric: 18.2318 - val_loss: 18.4705 - val_MinusLogProbMetric: 18.4705 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 339/1000
2023-10-23 17:58:16.995 
Epoch 339/1000 
	 loss: 18.2797, MinusLogProbMetric: 18.2797, val_loss: 18.5015, val_MinusLogProbMetric: 18.5015

Epoch 339: val_loss did not improve from 18.19391
196/196 - 49s - loss: 18.2797 - MinusLogProbMetric: 18.2797 - val_loss: 18.5015 - val_MinusLogProbMetric: 18.5015 - lr: 1.1111e-04 - 49s/epoch - 248ms/step
Epoch 340/1000
2023-10-23 17:59:06.006 
Epoch 340/1000 
	 loss: 18.2517, MinusLogProbMetric: 18.2517, val_loss: 18.4561, val_MinusLogProbMetric: 18.4561

Epoch 340: val_loss did not improve from 18.19391
196/196 - 49s - loss: 18.2517 - MinusLogProbMetric: 18.2517 - val_loss: 18.4561 - val_MinusLogProbMetric: 18.4561 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 341/1000
2023-10-23 17:59:57.933 
Epoch 341/1000 
	 loss: 18.2132, MinusLogProbMetric: 18.2132, val_loss: 18.2584, val_MinusLogProbMetric: 18.2584

Epoch 341: val_loss did not improve from 18.19391
196/196 - 52s - loss: 18.2132 - MinusLogProbMetric: 18.2132 - val_loss: 18.2584 - val_MinusLogProbMetric: 18.2584 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 342/1000
2023-10-23 18:00:48.407 
Epoch 342/1000 
	 loss: 18.2202, MinusLogProbMetric: 18.2202, val_loss: 18.3380, val_MinusLogProbMetric: 18.3380

Epoch 342: val_loss did not improve from 18.19391
196/196 - 50s - loss: 18.2202 - MinusLogProbMetric: 18.2202 - val_loss: 18.3380 - val_MinusLogProbMetric: 18.3380 - lr: 1.1111e-04 - 50s/epoch - 258ms/step
Epoch 343/1000
2023-10-23 18:01:37.942 
Epoch 343/1000 
	 loss: 18.2964, MinusLogProbMetric: 18.2964, val_loss: 18.1058, val_MinusLogProbMetric: 18.1058

Epoch 343: val_loss improved from 18.19391 to 18.10583, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 18.2964 - MinusLogProbMetric: 18.2964 - val_loss: 18.1058 - val_MinusLogProbMetric: 18.1058 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 344/1000
2023-10-23 18:02:34.876 
Epoch 344/1000 
	 loss: 18.2158, MinusLogProbMetric: 18.2158, val_loss: 18.1884, val_MinusLogProbMetric: 18.1884

Epoch 344: val_loss did not improve from 18.10583
196/196 - 56s - loss: 18.2158 - MinusLogProbMetric: 18.2158 - val_loss: 18.1884 - val_MinusLogProbMetric: 18.1884 - lr: 1.1111e-04 - 56s/epoch - 287ms/step
Epoch 345/1000
2023-10-23 18:03:24.160 
Epoch 345/1000 
	 loss: 18.1856, MinusLogProbMetric: 18.1856, val_loss: 18.2060, val_MinusLogProbMetric: 18.2060

Epoch 345: val_loss did not improve from 18.10583
196/196 - 49s - loss: 18.1856 - MinusLogProbMetric: 18.1856 - val_loss: 18.2060 - val_MinusLogProbMetric: 18.2060 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 346/1000
2023-10-23 18:04:12.905 
Epoch 346/1000 
	 loss: 18.2730, MinusLogProbMetric: 18.2730, val_loss: 18.7443, val_MinusLogProbMetric: 18.7443

Epoch 346: val_loss did not improve from 18.10583
196/196 - 49s - loss: 18.2730 - MinusLogProbMetric: 18.2730 - val_loss: 18.7443 - val_MinusLogProbMetric: 18.7443 - lr: 1.1111e-04 - 49s/epoch - 249ms/step
Epoch 347/1000
2023-10-23 18:05:02.804 
Epoch 347/1000 
	 loss: 18.1718, MinusLogProbMetric: 18.1718, val_loss: 18.3633, val_MinusLogProbMetric: 18.3633

Epoch 347: val_loss did not improve from 18.10583
196/196 - 50s - loss: 18.1718 - MinusLogProbMetric: 18.1718 - val_loss: 18.3633 - val_MinusLogProbMetric: 18.3633 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 348/1000
2023-10-23 18:05:52.150 
Epoch 348/1000 
	 loss: 18.2101, MinusLogProbMetric: 18.2101, val_loss: 18.4227, val_MinusLogProbMetric: 18.4227

Epoch 348: val_loss did not improve from 18.10583
196/196 - 49s - loss: 18.2101 - MinusLogProbMetric: 18.2101 - val_loss: 18.4227 - val_MinusLogProbMetric: 18.4227 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 349/1000
2023-10-23 18:06:42.066 
Epoch 349/1000 
	 loss: 18.3152, MinusLogProbMetric: 18.3152, val_loss: 18.3067, val_MinusLogProbMetric: 18.3067

Epoch 349: val_loss did not improve from 18.10583
196/196 - 50s - loss: 18.3152 - MinusLogProbMetric: 18.3152 - val_loss: 18.3067 - val_MinusLogProbMetric: 18.3067 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 350/1000
2023-10-23 18:07:32.793 
Epoch 350/1000 
	 loss: 18.2129, MinusLogProbMetric: 18.2129, val_loss: 18.0916, val_MinusLogProbMetric: 18.0916

Epoch 350: val_loss improved from 18.10583 to 18.09162, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 18.2129 - MinusLogProbMetric: 18.2129 - val_loss: 18.0916 - val_MinusLogProbMetric: 18.0916 - lr: 1.1111e-04 - 51s/epoch - 263ms/step
Epoch 351/1000
2023-10-23 18:08:22.493 
Epoch 351/1000 
	 loss: 18.1637, MinusLogProbMetric: 18.1637, val_loss: 18.3493, val_MinusLogProbMetric: 18.3493

Epoch 351: val_loss did not improve from 18.09162
196/196 - 49s - loss: 18.1637 - MinusLogProbMetric: 18.1637 - val_loss: 18.3493 - val_MinusLogProbMetric: 18.3493 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 352/1000
2023-10-23 18:09:11.577 
Epoch 352/1000 
	 loss: 18.2880, MinusLogProbMetric: 18.2880, val_loss: 18.1948, val_MinusLogProbMetric: 18.1948

Epoch 352: val_loss did not improve from 18.09162
196/196 - 49s - loss: 18.2880 - MinusLogProbMetric: 18.2880 - val_loss: 18.1948 - val_MinusLogProbMetric: 18.1948 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 353/1000
2023-10-23 18:10:02.754 
Epoch 353/1000 
	 loss: 18.1495, MinusLogProbMetric: 18.1495, val_loss: 18.5148, val_MinusLogProbMetric: 18.5148

Epoch 353: val_loss did not improve from 18.09162
196/196 - 51s - loss: 18.1495 - MinusLogProbMetric: 18.1495 - val_loss: 18.5148 - val_MinusLogProbMetric: 18.5148 - lr: 1.1111e-04 - 51s/epoch - 261ms/step
Epoch 354/1000
2023-10-23 18:10:51.161 
Epoch 354/1000 
	 loss: 18.2386, MinusLogProbMetric: 18.2386, val_loss: 18.2382, val_MinusLogProbMetric: 18.2382

Epoch 354: val_loss did not improve from 18.09162
196/196 - 48s - loss: 18.2386 - MinusLogProbMetric: 18.2386 - val_loss: 18.2382 - val_MinusLogProbMetric: 18.2382 - lr: 1.1111e-04 - 48s/epoch - 247ms/step
Epoch 355/1000
2023-10-23 18:11:40.196 
Epoch 355/1000 
	 loss: 18.1803, MinusLogProbMetric: 18.1803, val_loss: 18.6315, val_MinusLogProbMetric: 18.6315

Epoch 355: val_loss did not improve from 18.09162
196/196 - 49s - loss: 18.1803 - MinusLogProbMetric: 18.1803 - val_loss: 18.6315 - val_MinusLogProbMetric: 18.6315 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 356/1000
2023-10-23 18:12:30.048 
Epoch 356/1000 
	 loss: 18.1952, MinusLogProbMetric: 18.1952, val_loss: 18.2623, val_MinusLogProbMetric: 18.2623

Epoch 356: val_loss did not improve from 18.09162
196/196 - 50s - loss: 18.1952 - MinusLogProbMetric: 18.1952 - val_loss: 18.2623 - val_MinusLogProbMetric: 18.2623 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 357/1000
2023-10-23 18:13:20.970 
Epoch 357/1000 
	 loss: 18.2168, MinusLogProbMetric: 18.2168, val_loss: 18.4399, val_MinusLogProbMetric: 18.4399

Epoch 357: val_loss did not improve from 18.09162
196/196 - 51s - loss: 18.2168 - MinusLogProbMetric: 18.2168 - val_loss: 18.4399 - val_MinusLogProbMetric: 18.4399 - lr: 1.1111e-04 - 51s/epoch - 260ms/step
Epoch 358/1000
2023-10-23 18:14:10.148 
Epoch 358/1000 
	 loss: 18.1907, MinusLogProbMetric: 18.1907, val_loss: 18.4681, val_MinusLogProbMetric: 18.4681

Epoch 358: val_loss did not improve from 18.09162
196/196 - 49s - loss: 18.1907 - MinusLogProbMetric: 18.1907 - val_loss: 18.4681 - val_MinusLogProbMetric: 18.4681 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 359/1000
2023-10-23 18:14:59.331 
Epoch 359/1000 
	 loss: 18.2989, MinusLogProbMetric: 18.2989, val_loss: 18.1367, val_MinusLogProbMetric: 18.1367

Epoch 359: val_loss did not improve from 18.09162
196/196 - 49s - loss: 18.2989 - MinusLogProbMetric: 18.2989 - val_loss: 18.1367 - val_MinusLogProbMetric: 18.1367 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 360/1000
2023-10-23 18:15:48.607 
Epoch 360/1000 
	 loss: 18.1137, MinusLogProbMetric: 18.1137, val_loss: 18.3335, val_MinusLogProbMetric: 18.3335

Epoch 360: val_loss did not improve from 18.09162
196/196 - 49s - loss: 18.1137 - MinusLogProbMetric: 18.1137 - val_loss: 18.3335 - val_MinusLogProbMetric: 18.3335 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 361/1000
2023-10-23 18:16:38.330 
Epoch 361/1000 
	 loss: 18.1122, MinusLogProbMetric: 18.1122, val_loss: 18.0951, val_MinusLogProbMetric: 18.0951

Epoch 361: val_loss did not improve from 18.09162
196/196 - 50s - loss: 18.1122 - MinusLogProbMetric: 18.1122 - val_loss: 18.0951 - val_MinusLogProbMetric: 18.0951 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 362/1000
2023-10-23 18:17:27.760 
Epoch 362/1000 
	 loss: 18.1530, MinusLogProbMetric: 18.1530, val_loss: 18.3645, val_MinusLogProbMetric: 18.3645

Epoch 362: val_loss did not improve from 18.09162
196/196 - 49s - loss: 18.1530 - MinusLogProbMetric: 18.1530 - val_loss: 18.3645 - val_MinusLogProbMetric: 18.3645 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 363/1000
2023-10-23 18:18:18.734 
Epoch 363/1000 
	 loss: 18.1037, MinusLogProbMetric: 18.1037, val_loss: 18.5454, val_MinusLogProbMetric: 18.5454

Epoch 363: val_loss did not improve from 18.09162
196/196 - 51s - loss: 18.1037 - MinusLogProbMetric: 18.1037 - val_loss: 18.5454 - val_MinusLogProbMetric: 18.5454 - lr: 1.1111e-04 - 51s/epoch - 260ms/step
Epoch 364/1000
2023-10-23 18:19:07.792 
Epoch 364/1000 
	 loss: 18.2553, MinusLogProbMetric: 18.2553, val_loss: 18.1639, val_MinusLogProbMetric: 18.1639

Epoch 364: val_loss did not improve from 18.09162
196/196 - 49s - loss: 18.2553 - MinusLogProbMetric: 18.2553 - val_loss: 18.1639 - val_MinusLogProbMetric: 18.1639 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 365/1000
2023-10-23 18:19:56.262 
Epoch 365/1000 
	 loss: 18.0695, MinusLogProbMetric: 18.0695, val_loss: 18.1272, val_MinusLogProbMetric: 18.1272

Epoch 365: val_loss did not improve from 18.09162
196/196 - 48s - loss: 18.0695 - MinusLogProbMetric: 18.0695 - val_loss: 18.1272 - val_MinusLogProbMetric: 18.1272 - lr: 1.1111e-04 - 48s/epoch - 247ms/step
Epoch 366/1000
2023-10-23 18:20:45.239 
Epoch 366/1000 
	 loss: 18.1834, MinusLogProbMetric: 18.1834, val_loss: 18.2389, val_MinusLogProbMetric: 18.2389

Epoch 366: val_loss did not improve from 18.09162
196/196 - 49s - loss: 18.1834 - MinusLogProbMetric: 18.1834 - val_loss: 18.2389 - val_MinusLogProbMetric: 18.2389 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 367/1000
2023-10-23 18:21:36.221 
Epoch 367/1000 
	 loss: 18.1682, MinusLogProbMetric: 18.1682, val_loss: 18.9958, val_MinusLogProbMetric: 18.9958

Epoch 367: val_loss did not improve from 18.09162
196/196 - 51s - loss: 18.1682 - MinusLogProbMetric: 18.1682 - val_loss: 18.9958 - val_MinusLogProbMetric: 18.9958 - lr: 1.1111e-04 - 51s/epoch - 260ms/step
Epoch 368/1000
2023-10-23 18:22:25.309 
Epoch 368/1000 
	 loss: 18.1987, MinusLogProbMetric: 18.1987, val_loss: 18.5940, val_MinusLogProbMetric: 18.5940

Epoch 368: val_loss did not improve from 18.09162
196/196 - 49s - loss: 18.1987 - MinusLogProbMetric: 18.1987 - val_loss: 18.5940 - val_MinusLogProbMetric: 18.5940 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 369/1000
2023-10-23 18:23:14.196 
Epoch 369/1000 
	 loss: 18.2253, MinusLogProbMetric: 18.2253, val_loss: 18.1579, val_MinusLogProbMetric: 18.1579

Epoch 369: val_loss did not improve from 18.09162
196/196 - 49s - loss: 18.2253 - MinusLogProbMetric: 18.2253 - val_loss: 18.1579 - val_MinusLogProbMetric: 18.1579 - lr: 1.1111e-04 - 49s/epoch - 249ms/step
Epoch 370/1000
2023-10-23 18:24:05.546 
Epoch 370/1000 
	 loss: 18.1490, MinusLogProbMetric: 18.1490, val_loss: 18.3931, val_MinusLogProbMetric: 18.3931

Epoch 370: val_loss did not improve from 18.09162
196/196 - 51s - loss: 18.1490 - MinusLogProbMetric: 18.1490 - val_loss: 18.3931 - val_MinusLogProbMetric: 18.3931 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 371/1000
2023-10-23 18:24:56.145 
Epoch 371/1000 
	 loss: 18.0454, MinusLogProbMetric: 18.0454, val_loss: 18.1661, val_MinusLogProbMetric: 18.1661

Epoch 371: val_loss did not improve from 18.09162
196/196 - 51s - loss: 18.0454 - MinusLogProbMetric: 18.0454 - val_loss: 18.1661 - val_MinusLogProbMetric: 18.1661 - lr: 1.1111e-04 - 51s/epoch - 258ms/step
Epoch 372/1000
2023-10-23 18:25:45.115 
Epoch 372/1000 
	 loss: 18.0896, MinusLogProbMetric: 18.0896, val_loss: 18.2216, val_MinusLogProbMetric: 18.2216

Epoch 372: val_loss did not improve from 18.09162
196/196 - 49s - loss: 18.0896 - MinusLogProbMetric: 18.0896 - val_loss: 18.2216 - val_MinusLogProbMetric: 18.2216 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 373/1000
2023-10-23 18:26:34.262 
Epoch 373/1000 
	 loss: 18.1416, MinusLogProbMetric: 18.1416, val_loss: 18.4358, val_MinusLogProbMetric: 18.4358

Epoch 373: val_loss did not improve from 18.09162
196/196 - 49s - loss: 18.1416 - MinusLogProbMetric: 18.1416 - val_loss: 18.4358 - val_MinusLogProbMetric: 18.4358 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 374/1000
2023-10-23 18:27:23.907 
Epoch 374/1000 
	 loss: 18.1168, MinusLogProbMetric: 18.1168, val_loss: 18.3358, val_MinusLogProbMetric: 18.3358

Epoch 374: val_loss did not improve from 18.09162
196/196 - 50s - loss: 18.1168 - MinusLogProbMetric: 18.1168 - val_loss: 18.3358 - val_MinusLogProbMetric: 18.3358 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 375/1000
2023-10-23 18:28:12.158 
Epoch 375/1000 
	 loss: 18.0997, MinusLogProbMetric: 18.0997, val_loss: 19.0920, val_MinusLogProbMetric: 19.0920

Epoch 375: val_loss did not improve from 18.09162
196/196 - 48s - loss: 18.0997 - MinusLogProbMetric: 18.0997 - val_loss: 19.0920 - val_MinusLogProbMetric: 19.0920 - lr: 1.1111e-04 - 48s/epoch - 246ms/step
Epoch 376/1000
2023-10-23 18:29:00.691 
Epoch 376/1000 
	 loss: 18.1293, MinusLogProbMetric: 18.1293, val_loss: 18.6186, val_MinusLogProbMetric: 18.6186

Epoch 376: val_loss did not improve from 18.09162
196/196 - 49s - loss: 18.1293 - MinusLogProbMetric: 18.1293 - val_loss: 18.6186 - val_MinusLogProbMetric: 18.6186 - lr: 1.1111e-04 - 49s/epoch - 248ms/step
Epoch 377/1000
2023-10-23 18:29:49.717 
Epoch 377/1000 
	 loss: 18.2609, MinusLogProbMetric: 18.2609, val_loss: 18.0828, val_MinusLogProbMetric: 18.0828

Epoch 377: val_loss improved from 18.09162 to 18.08281, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 18.2609 - MinusLogProbMetric: 18.2609 - val_loss: 18.0828 - val_MinusLogProbMetric: 18.0828 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 378/1000
2023-10-23 18:30:39.639 
Epoch 378/1000 
	 loss: 18.0710, MinusLogProbMetric: 18.0710, val_loss: 18.1393, val_MinusLogProbMetric: 18.1393

Epoch 378: val_loss did not improve from 18.08281
196/196 - 49s - loss: 18.0710 - MinusLogProbMetric: 18.0710 - val_loss: 18.1393 - val_MinusLogProbMetric: 18.1393 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 379/1000
2023-10-23 18:31:27.839 
Epoch 379/1000 
	 loss: 18.0985, MinusLogProbMetric: 18.0985, val_loss: 18.1408, val_MinusLogProbMetric: 18.1408

Epoch 379: val_loss did not improve from 18.08281
196/196 - 48s - loss: 18.0985 - MinusLogProbMetric: 18.0985 - val_loss: 18.1408 - val_MinusLogProbMetric: 18.1408 - lr: 1.1111e-04 - 48s/epoch - 246ms/step
Epoch 380/1000
2023-10-23 18:32:17.685 
Epoch 380/1000 
	 loss: 18.0531, MinusLogProbMetric: 18.0531, val_loss: 18.0991, val_MinusLogProbMetric: 18.0991

Epoch 380: val_loss did not improve from 18.08281
196/196 - 50s - loss: 18.0531 - MinusLogProbMetric: 18.0531 - val_loss: 18.0991 - val_MinusLogProbMetric: 18.0991 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 381/1000
2023-10-23 18:33:10.891 
Epoch 381/1000 
	 loss: 18.1298, MinusLogProbMetric: 18.1298, val_loss: 18.1621, val_MinusLogProbMetric: 18.1621

Epoch 381: val_loss did not improve from 18.08281
196/196 - 53s - loss: 18.1298 - MinusLogProbMetric: 18.1298 - val_loss: 18.1621 - val_MinusLogProbMetric: 18.1621 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 382/1000
2023-10-23 18:33:59.857 
Epoch 382/1000 
	 loss: 18.0358, MinusLogProbMetric: 18.0358, val_loss: 18.1598, val_MinusLogProbMetric: 18.1598

Epoch 382: val_loss did not improve from 18.08281
196/196 - 49s - loss: 18.0358 - MinusLogProbMetric: 18.0358 - val_loss: 18.1598 - val_MinusLogProbMetric: 18.1598 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 383/1000
2023-10-23 18:34:48.787 
Epoch 383/1000 
	 loss: 18.1585, MinusLogProbMetric: 18.1585, val_loss: 18.1782, val_MinusLogProbMetric: 18.1782

Epoch 383: val_loss did not improve from 18.08281
196/196 - 49s - loss: 18.1585 - MinusLogProbMetric: 18.1585 - val_loss: 18.1782 - val_MinusLogProbMetric: 18.1782 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 384/1000
2023-10-23 18:35:38.895 
Epoch 384/1000 
	 loss: 18.0795, MinusLogProbMetric: 18.0795, val_loss: 18.3229, val_MinusLogProbMetric: 18.3229

Epoch 384: val_loss did not improve from 18.08281
196/196 - 50s - loss: 18.0795 - MinusLogProbMetric: 18.0795 - val_loss: 18.3229 - val_MinusLogProbMetric: 18.3229 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 385/1000
2023-10-23 18:36:29.029 
Epoch 385/1000 
	 loss: 18.0615, MinusLogProbMetric: 18.0615, val_loss: 18.0442, val_MinusLogProbMetric: 18.0442

Epoch 385: val_loss improved from 18.08281 to 18.04419, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 18.0615 - MinusLogProbMetric: 18.0615 - val_loss: 18.0442 - val_MinusLogProbMetric: 18.0442 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 386/1000
2023-10-23 18:37:18.872 
Epoch 386/1000 
	 loss: 18.0783, MinusLogProbMetric: 18.0783, val_loss: 18.1955, val_MinusLogProbMetric: 18.1955

Epoch 386: val_loss did not improve from 18.04419
196/196 - 49s - loss: 18.0783 - MinusLogProbMetric: 18.0783 - val_loss: 18.1955 - val_MinusLogProbMetric: 18.1955 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 387/1000
2023-10-23 18:38:07.782 
Epoch 387/1000 
	 loss: 18.0728, MinusLogProbMetric: 18.0728, val_loss: 18.4919, val_MinusLogProbMetric: 18.4919

Epoch 387: val_loss did not improve from 18.04419
196/196 - 49s - loss: 18.0728 - MinusLogProbMetric: 18.0728 - val_loss: 18.4919 - val_MinusLogProbMetric: 18.4919 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 388/1000
2023-10-23 18:38:56.395 
Epoch 388/1000 
	 loss: 18.0391, MinusLogProbMetric: 18.0391, val_loss: 18.1061, val_MinusLogProbMetric: 18.1061

Epoch 388: val_loss did not improve from 18.04419
196/196 - 49s - loss: 18.0391 - MinusLogProbMetric: 18.0391 - val_loss: 18.1061 - val_MinusLogProbMetric: 18.1061 - lr: 1.1111e-04 - 49s/epoch - 248ms/step
Epoch 389/1000
2023-10-23 18:39:45.339 
Epoch 389/1000 
	 loss: 18.0519, MinusLogProbMetric: 18.0519, val_loss: 18.0141, val_MinusLogProbMetric: 18.0141

Epoch 389: val_loss improved from 18.04419 to 18.01412, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 18.0519 - MinusLogProbMetric: 18.0519 - val_loss: 18.0141 - val_MinusLogProbMetric: 18.0141 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 390/1000
2023-10-23 18:40:34.371 
Epoch 390/1000 
	 loss: 18.1313, MinusLogProbMetric: 18.1313, val_loss: 18.1838, val_MinusLogProbMetric: 18.1838

Epoch 390: val_loss did not improve from 18.01412
196/196 - 48s - loss: 18.1313 - MinusLogProbMetric: 18.1313 - val_loss: 18.1838 - val_MinusLogProbMetric: 18.1838 - lr: 1.1111e-04 - 48s/epoch - 246ms/step
Epoch 391/1000
2023-10-23 18:41:23.920 
Epoch 391/1000 
	 loss: 18.0601, MinusLogProbMetric: 18.0601, val_loss: 18.1915, val_MinusLogProbMetric: 18.1915

Epoch 391: val_loss did not improve from 18.01412
196/196 - 50s - loss: 18.0601 - MinusLogProbMetric: 18.0601 - val_loss: 18.1915 - val_MinusLogProbMetric: 18.1915 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 392/1000
2023-10-23 18:42:14.724 
Epoch 392/1000 
	 loss: 17.9998, MinusLogProbMetric: 17.9998, val_loss: 18.1383, val_MinusLogProbMetric: 18.1383

Epoch 392: val_loss did not improve from 18.01412
196/196 - 51s - loss: 17.9998 - MinusLogProbMetric: 17.9998 - val_loss: 18.1383 - val_MinusLogProbMetric: 18.1383 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 393/1000
2023-10-23 18:43:03.064 
Epoch 393/1000 
	 loss: 18.1993, MinusLogProbMetric: 18.1993, val_loss: 18.2919, val_MinusLogProbMetric: 18.2919

Epoch 393: val_loss did not improve from 18.01412
196/196 - 48s - loss: 18.1993 - MinusLogProbMetric: 18.1993 - val_loss: 18.2919 - val_MinusLogProbMetric: 18.2919 - lr: 1.1111e-04 - 48s/epoch - 247ms/step
Epoch 394/1000
2023-10-23 18:43:52.411 
Epoch 394/1000 
	 loss: 18.1260, MinusLogProbMetric: 18.1260, val_loss: 18.0648, val_MinusLogProbMetric: 18.0648

Epoch 394: val_loss did not improve from 18.01412
196/196 - 49s - loss: 18.1260 - MinusLogProbMetric: 18.1260 - val_loss: 18.0648 - val_MinusLogProbMetric: 18.0648 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 395/1000
2023-10-23 18:44:41.617 
Epoch 395/1000 
	 loss: 18.0542, MinusLogProbMetric: 18.0542, val_loss: 18.1973, val_MinusLogProbMetric: 18.1973

Epoch 395: val_loss did not improve from 18.01412
196/196 - 49s - loss: 18.0542 - MinusLogProbMetric: 18.0542 - val_loss: 18.1973 - val_MinusLogProbMetric: 18.1973 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 396/1000
2023-10-23 18:45:31.500 
Epoch 396/1000 
	 loss: 18.0279, MinusLogProbMetric: 18.0279, val_loss: 17.9389, val_MinusLogProbMetric: 17.9389

Epoch 396: val_loss improved from 18.01412 to 17.93886, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 18.0279 - MinusLogProbMetric: 18.0279 - val_loss: 17.9389 - val_MinusLogProbMetric: 17.9389 - lr: 1.1111e-04 - 51s/epoch - 258ms/step
Epoch 397/1000
2023-10-23 18:46:21.413 
Epoch 397/1000 
	 loss: 18.0759, MinusLogProbMetric: 18.0759, val_loss: 18.6884, val_MinusLogProbMetric: 18.6884

Epoch 397: val_loss did not improve from 17.93886
196/196 - 49s - loss: 18.0759 - MinusLogProbMetric: 18.0759 - val_loss: 18.6884 - val_MinusLogProbMetric: 18.6884 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 398/1000
2023-10-23 18:47:11.145 
Epoch 398/1000 
	 loss: 18.0848, MinusLogProbMetric: 18.0848, val_loss: 18.0628, val_MinusLogProbMetric: 18.0628

Epoch 398: val_loss did not improve from 17.93886
196/196 - 50s - loss: 18.0848 - MinusLogProbMetric: 18.0848 - val_loss: 18.0628 - val_MinusLogProbMetric: 18.0628 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 399/1000
2023-10-23 18:48:05.610 
Epoch 399/1000 
	 loss: 18.0402, MinusLogProbMetric: 18.0402, val_loss: 18.0520, val_MinusLogProbMetric: 18.0520

Epoch 399: val_loss did not improve from 17.93886
196/196 - 54s - loss: 18.0402 - MinusLogProbMetric: 18.0402 - val_loss: 18.0520 - val_MinusLogProbMetric: 18.0520 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 400/1000
2023-10-23 18:49:01.343 
Epoch 400/1000 
	 loss: 18.0603, MinusLogProbMetric: 18.0603, val_loss: 19.0837, val_MinusLogProbMetric: 19.0837

Epoch 400: val_loss did not improve from 17.93886
196/196 - 56s - loss: 18.0603 - MinusLogProbMetric: 18.0603 - val_loss: 19.0837 - val_MinusLogProbMetric: 19.0837 - lr: 1.1111e-04 - 56s/epoch - 284ms/step
Epoch 401/1000
2023-10-23 18:49:55.158 
Epoch 401/1000 
	 loss: 18.0263, MinusLogProbMetric: 18.0263, val_loss: 18.0068, val_MinusLogProbMetric: 18.0068

Epoch 401: val_loss did not improve from 17.93886
196/196 - 54s - loss: 18.0263 - MinusLogProbMetric: 18.0263 - val_loss: 18.0068 - val_MinusLogProbMetric: 18.0068 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 402/1000
2023-10-23 18:50:51.421 
Epoch 402/1000 
	 loss: 18.1414, MinusLogProbMetric: 18.1414, val_loss: 18.2758, val_MinusLogProbMetric: 18.2758

Epoch 402: val_loss did not improve from 17.93886
196/196 - 56s - loss: 18.1414 - MinusLogProbMetric: 18.1414 - val_loss: 18.2758 - val_MinusLogProbMetric: 18.2758 - lr: 1.1111e-04 - 56s/epoch - 287ms/step
Epoch 403/1000
2023-10-23 18:51:49.178 
Epoch 403/1000 
	 loss: 18.1238, MinusLogProbMetric: 18.1238, val_loss: 18.3080, val_MinusLogProbMetric: 18.3080

Epoch 403: val_loss did not improve from 17.93886
196/196 - 58s - loss: 18.1238 - MinusLogProbMetric: 18.1238 - val_loss: 18.3080 - val_MinusLogProbMetric: 18.3080 - lr: 1.1111e-04 - 58s/epoch - 295ms/step
Epoch 404/1000
2023-10-23 18:52:46.204 
Epoch 404/1000 
	 loss: 18.0777, MinusLogProbMetric: 18.0777, val_loss: 18.0358, val_MinusLogProbMetric: 18.0358

Epoch 404: val_loss did not improve from 17.93886
196/196 - 57s - loss: 18.0777 - MinusLogProbMetric: 18.0777 - val_loss: 18.0358 - val_MinusLogProbMetric: 18.0358 - lr: 1.1111e-04 - 57s/epoch - 291ms/step
Epoch 405/1000
2023-10-23 18:53:37.873 
Epoch 405/1000 
	 loss: 18.0991, MinusLogProbMetric: 18.0991, val_loss: 18.0418, val_MinusLogProbMetric: 18.0418

Epoch 405: val_loss did not improve from 17.93886
196/196 - 52s - loss: 18.0991 - MinusLogProbMetric: 18.0991 - val_loss: 18.0418 - val_MinusLogProbMetric: 18.0418 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 406/1000
2023-10-23 18:54:29.662 
Epoch 406/1000 
	 loss: 18.1195, MinusLogProbMetric: 18.1195, val_loss: 18.5301, val_MinusLogProbMetric: 18.5301

Epoch 406: val_loss did not improve from 17.93886
196/196 - 52s - loss: 18.1195 - MinusLogProbMetric: 18.1195 - val_loss: 18.5301 - val_MinusLogProbMetric: 18.5301 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 407/1000
2023-10-23 18:55:22.393 
Epoch 407/1000 
	 loss: 18.0730, MinusLogProbMetric: 18.0730, val_loss: 17.9686, val_MinusLogProbMetric: 17.9686

Epoch 407: val_loss did not improve from 17.93886
196/196 - 53s - loss: 18.0730 - MinusLogProbMetric: 18.0730 - val_loss: 17.9686 - val_MinusLogProbMetric: 17.9686 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 408/1000
2023-10-23 18:56:14.655 
Epoch 408/1000 
	 loss: 17.9390, MinusLogProbMetric: 17.9390, val_loss: 17.9361, val_MinusLogProbMetric: 17.9361

Epoch 408: val_loss improved from 17.93886 to 17.93615, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 17.9390 - MinusLogProbMetric: 17.9390 - val_loss: 17.9361 - val_MinusLogProbMetric: 17.9361 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 409/1000
2023-10-23 18:57:05.142 
Epoch 409/1000 
	 loss: 18.0386, MinusLogProbMetric: 18.0386, val_loss: 18.9288, val_MinusLogProbMetric: 18.9288

Epoch 409: val_loss did not improve from 17.93615
196/196 - 50s - loss: 18.0386 - MinusLogProbMetric: 18.0386 - val_loss: 18.9288 - val_MinusLogProbMetric: 18.9288 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 410/1000
2023-10-23 18:57:54.723 
Epoch 410/1000 
	 loss: 18.0765, MinusLogProbMetric: 18.0765, val_loss: 18.1469, val_MinusLogProbMetric: 18.1469

Epoch 410: val_loss did not improve from 17.93615
196/196 - 50s - loss: 18.0765 - MinusLogProbMetric: 18.0765 - val_loss: 18.1469 - val_MinusLogProbMetric: 18.1469 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 411/1000
2023-10-23 18:58:44.275 
Epoch 411/1000 
	 loss: 17.9373, MinusLogProbMetric: 17.9373, val_loss: 18.1083, val_MinusLogProbMetric: 18.1083

Epoch 411: val_loss did not improve from 17.93615
196/196 - 50s - loss: 17.9373 - MinusLogProbMetric: 17.9373 - val_loss: 18.1083 - val_MinusLogProbMetric: 18.1083 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 412/1000
2023-10-23 18:59:34.332 
Epoch 412/1000 
	 loss: 18.0598, MinusLogProbMetric: 18.0598, val_loss: 18.0149, val_MinusLogProbMetric: 18.0149

Epoch 412: val_loss did not improve from 17.93615
196/196 - 50s - loss: 18.0598 - MinusLogProbMetric: 18.0598 - val_loss: 18.0149 - val_MinusLogProbMetric: 18.0149 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 413/1000
2023-10-23 19:00:23.608 
Epoch 413/1000 
	 loss: 18.0192, MinusLogProbMetric: 18.0192, val_loss: 18.1448, val_MinusLogProbMetric: 18.1448

Epoch 413: val_loss did not improve from 17.93615
196/196 - 49s - loss: 18.0192 - MinusLogProbMetric: 18.0192 - val_loss: 18.1448 - val_MinusLogProbMetric: 18.1448 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 414/1000
2023-10-23 19:01:13.121 
Epoch 414/1000 
	 loss: 17.9521, MinusLogProbMetric: 17.9521, val_loss: 18.3796, val_MinusLogProbMetric: 18.3796

Epoch 414: val_loss did not improve from 17.93615
196/196 - 50s - loss: 17.9521 - MinusLogProbMetric: 17.9521 - val_loss: 18.3796 - val_MinusLogProbMetric: 18.3796 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 415/1000
2023-10-23 19:02:03.135 
Epoch 415/1000 
	 loss: 17.9375, MinusLogProbMetric: 17.9375, val_loss: 17.9605, val_MinusLogProbMetric: 17.9605

Epoch 415: val_loss did not improve from 17.93615
196/196 - 50s - loss: 17.9375 - MinusLogProbMetric: 17.9375 - val_loss: 17.9605 - val_MinusLogProbMetric: 17.9605 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 416/1000
2023-10-23 19:02:52.736 
Epoch 416/1000 
	 loss: 18.2272, MinusLogProbMetric: 18.2272, val_loss: 18.0337, val_MinusLogProbMetric: 18.0337

Epoch 416: val_loss did not improve from 17.93615
196/196 - 50s - loss: 18.2272 - MinusLogProbMetric: 18.2272 - val_loss: 18.0337 - val_MinusLogProbMetric: 18.0337 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 417/1000
2023-10-23 19:03:42.476 
Epoch 417/1000 
	 loss: 18.0441, MinusLogProbMetric: 18.0441, val_loss: 18.3830, val_MinusLogProbMetric: 18.3830

Epoch 417: val_loss did not improve from 17.93615
196/196 - 50s - loss: 18.0441 - MinusLogProbMetric: 18.0441 - val_loss: 18.3830 - val_MinusLogProbMetric: 18.3830 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 418/1000
2023-10-23 19:04:32.462 
Epoch 418/1000 
	 loss: 18.0004, MinusLogProbMetric: 18.0004, val_loss: 18.1692, val_MinusLogProbMetric: 18.1692

Epoch 418: val_loss did not improve from 17.93615
196/196 - 50s - loss: 18.0004 - MinusLogProbMetric: 18.0004 - val_loss: 18.1692 - val_MinusLogProbMetric: 18.1692 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 419/1000
2023-10-23 19:05:22.011 
Epoch 419/1000 
	 loss: 18.0484, MinusLogProbMetric: 18.0484, val_loss: 19.2468, val_MinusLogProbMetric: 19.2468

Epoch 419: val_loss did not improve from 17.93615
196/196 - 50s - loss: 18.0484 - MinusLogProbMetric: 18.0484 - val_loss: 19.2468 - val_MinusLogProbMetric: 19.2468 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 420/1000
2023-10-23 19:06:11.640 
Epoch 420/1000 
	 loss: 18.0185, MinusLogProbMetric: 18.0185, val_loss: 18.0108, val_MinusLogProbMetric: 18.0108

Epoch 420: val_loss did not improve from 17.93615
196/196 - 50s - loss: 18.0185 - MinusLogProbMetric: 18.0185 - val_loss: 18.0108 - val_MinusLogProbMetric: 18.0108 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 421/1000
2023-10-23 19:07:01.190 
Epoch 421/1000 
	 loss: 18.0129, MinusLogProbMetric: 18.0129, val_loss: 18.1098, val_MinusLogProbMetric: 18.1098

Epoch 421: val_loss did not improve from 17.93615
196/196 - 50s - loss: 18.0129 - MinusLogProbMetric: 18.0129 - val_loss: 18.1098 - val_MinusLogProbMetric: 18.1098 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 422/1000
2023-10-23 19:07:50.084 
Epoch 422/1000 
	 loss: 17.9365, MinusLogProbMetric: 17.9365, val_loss: 20.8465, val_MinusLogProbMetric: 20.8465

Epoch 422: val_loss did not improve from 17.93615
196/196 - 49s - loss: 17.9365 - MinusLogProbMetric: 17.9365 - val_loss: 20.8465 - val_MinusLogProbMetric: 20.8465 - lr: 1.1111e-04 - 49s/epoch - 249ms/step
Epoch 423/1000
2023-10-23 19:08:39.826 
Epoch 423/1000 
	 loss: 18.0550, MinusLogProbMetric: 18.0550, val_loss: 17.8578, val_MinusLogProbMetric: 17.8578

Epoch 423: val_loss improved from 17.93615 to 17.85782, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 18.0550 - MinusLogProbMetric: 18.0550 - val_loss: 17.8578 - val_MinusLogProbMetric: 17.8578 - lr: 1.1111e-04 - 50s/epoch - 257ms/step
Epoch 424/1000
2023-10-23 19:09:29.847 
Epoch 424/1000 
	 loss: 17.9981, MinusLogProbMetric: 17.9981, val_loss: 18.0380, val_MinusLogProbMetric: 18.0380

Epoch 424: val_loss did not improve from 17.85782
196/196 - 49s - loss: 17.9981 - MinusLogProbMetric: 17.9981 - val_loss: 18.0380 - val_MinusLogProbMetric: 18.0380 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 425/1000
2023-10-23 19:10:20.362 
Epoch 425/1000 
	 loss: 18.0518, MinusLogProbMetric: 18.0518, val_loss: 18.7165, val_MinusLogProbMetric: 18.7165

Epoch 425: val_loss did not improve from 17.85782
196/196 - 51s - loss: 18.0518 - MinusLogProbMetric: 18.0518 - val_loss: 18.7165 - val_MinusLogProbMetric: 18.7165 - lr: 1.1111e-04 - 51s/epoch - 258ms/step
Epoch 426/1000
2023-10-23 19:11:11.007 
Epoch 426/1000 
	 loss: 18.1741, MinusLogProbMetric: 18.1741, val_loss: 18.0838, val_MinusLogProbMetric: 18.0838

Epoch 426: val_loss did not improve from 17.85782
196/196 - 51s - loss: 18.1741 - MinusLogProbMetric: 18.1741 - val_loss: 18.0838 - val_MinusLogProbMetric: 18.0838 - lr: 1.1111e-04 - 51s/epoch - 258ms/step
Epoch 427/1000
2023-10-23 19:12:01.534 
Epoch 427/1000 
	 loss: 17.9288, MinusLogProbMetric: 17.9288, val_loss: 18.1253, val_MinusLogProbMetric: 18.1253

Epoch 427: val_loss did not improve from 17.85782
196/196 - 51s - loss: 17.9288 - MinusLogProbMetric: 17.9288 - val_loss: 18.1253 - val_MinusLogProbMetric: 18.1253 - lr: 1.1111e-04 - 51s/epoch - 258ms/step
Epoch 428/1000
2023-10-23 19:12:51.730 
Epoch 428/1000 
	 loss: 17.9086, MinusLogProbMetric: 17.9086, val_loss: 18.1561, val_MinusLogProbMetric: 18.1561

Epoch 428: val_loss did not improve from 17.85782
196/196 - 50s - loss: 17.9086 - MinusLogProbMetric: 17.9086 - val_loss: 18.1561 - val_MinusLogProbMetric: 18.1561 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 429/1000
2023-10-23 19:13:41.565 
Epoch 429/1000 
	 loss: 18.0051, MinusLogProbMetric: 18.0051, val_loss: 18.0009, val_MinusLogProbMetric: 18.0009

Epoch 429: val_loss did not improve from 17.85782
196/196 - 50s - loss: 18.0051 - MinusLogProbMetric: 18.0051 - val_loss: 18.0009 - val_MinusLogProbMetric: 18.0009 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 430/1000
2023-10-23 19:14:32.462 
Epoch 430/1000 
	 loss: 17.9231, MinusLogProbMetric: 17.9231, val_loss: 17.9439, val_MinusLogProbMetric: 17.9439

Epoch 430: val_loss did not improve from 17.85782
196/196 - 51s - loss: 17.9231 - MinusLogProbMetric: 17.9231 - val_loss: 17.9439 - val_MinusLogProbMetric: 17.9439 - lr: 1.1111e-04 - 51s/epoch - 260ms/step
Epoch 431/1000
2023-10-23 19:15:21.408 
Epoch 431/1000 
	 loss: 18.5293, MinusLogProbMetric: 18.5293, val_loss: 18.8426, val_MinusLogProbMetric: 18.8426

Epoch 431: val_loss did not improve from 17.85782
196/196 - 49s - loss: 18.5293 - MinusLogProbMetric: 18.5293 - val_loss: 18.8426 - val_MinusLogProbMetric: 18.8426 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 432/1000
2023-10-23 19:16:10.782 
Epoch 432/1000 
	 loss: 18.0692, MinusLogProbMetric: 18.0692, val_loss: 17.9653, val_MinusLogProbMetric: 17.9653

Epoch 432: val_loss did not improve from 17.85782
196/196 - 49s - loss: 18.0692 - MinusLogProbMetric: 18.0692 - val_loss: 17.9653 - val_MinusLogProbMetric: 17.9653 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 433/1000
2023-10-23 19:17:00.310 
Epoch 433/1000 
	 loss: 17.9356, MinusLogProbMetric: 17.9356, val_loss: 18.2684, val_MinusLogProbMetric: 18.2684

Epoch 433: val_loss did not improve from 17.85782
196/196 - 50s - loss: 17.9356 - MinusLogProbMetric: 17.9356 - val_loss: 18.2684 - val_MinusLogProbMetric: 18.2684 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 434/1000
2023-10-23 19:17:50.165 
Epoch 434/1000 
	 loss: 17.9606, MinusLogProbMetric: 17.9606, val_loss: 18.6628, val_MinusLogProbMetric: 18.6628

Epoch 434: val_loss did not improve from 17.85782
196/196 - 50s - loss: 17.9606 - MinusLogProbMetric: 17.9606 - val_loss: 18.6628 - val_MinusLogProbMetric: 18.6628 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 435/1000
2023-10-23 19:18:39.612 
Epoch 435/1000 
	 loss: 17.9328, MinusLogProbMetric: 17.9328, val_loss: 17.8641, val_MinusLogProbMetric: 17.8641

Epoch 435: val_loss did not improve from 17.85782
196/196 - 49s - loss: 17.9328 - MinusLogProbMetric: 17.9328 - val_loss: 17.8641 - val_MinusLogProbMetric: 17.8641 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 436/1000
2023-10-23 19:19:29.790 
Epoch 436/1000 
	 loss: 18.0726, MinusLogProbMetric: 18.0726, val_loss: 18.0303, val_MinusLogProbMetric: 18.0303

Epoch 436: val_loss did not improve from 17.85782
196/196 - 50s - loss: 18.0726 - MinusLogProbMetric: 18.0726 - val_loss: 18.0303 - val_MinusLogProbMetric: 18.0303 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 437/1000
2023-10-23 19:20:19.294 
Epoch 437/1000 
	 loss: 18.0409, MinusLogProbMetric: 18.0409, val_loss: 18.0708, val_MinusLogProbMetric: 18.0708

Epoch 437: val_loss did not improve from 17.85782
196/196 - 50s - loss: 18.0409 - MinusLogProbMetric: 18.0409 - val_loss: 18.0708 - val_MinusLogProbMetric: 18.0708 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 438/1000
2023-10-23 19:21:10.716 
Epoch 438/1000 
	 loss: 17.8692, MinusLogProbMetric: 17.8692, val_loss: 17.9289, val_MinusLogProbMetric: 17.9289

Epoch 438: val_loss did not improve from 17.85782
196/196 - 51s - loss: 17.8692 - MinusLogProbMetric: 17.8692 - val_loss: 17.9289 - val_MinusLogProbMetric: 17.9289 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 439/1000
2023-10-23 19:22:00.522 
Epoch 439/1000 
	 loss: 18.0169, MinusLogProbMetric: 18.0169, val_loss: 18.4728, val_MinusLogProbMetric: 18.4728

Epoch 439: val_loss did not improve from 17.85782
196/196 - 50s - loss: 18.0169 - MinusLogProbMetric: 18.0169 - val_loss: 18.4728 - val_MinusLogProbMetric: 18.4728 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 440/1000
2023-10-23 19:22:50.680 
Epoch 440/1000 
	 loss: 17.8990, MinusLogProbMetric: 17.8990, val_loss: 17.8832, val_MinusLogProbMetric: 17.8832

Epoch 440: val_loss did not improve from 17.85782
196/196 - 50s - loss: 17.8990 - MinusLogProbMetric: 17.8990 - val_loss: 17.8832 - val_MinusLogProbMetric: 17.8832 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 441/1000
2023-10-23 19:23:42.354 
Epoch 441/1000 
	 loss: 18.0322, MinusLogProbMetric: 18.0322, val_loss: 18.2393, val_MinusLogProbMetric: 18.2393

Epoch 441: val_loss did not improve from 17.85782
196/196 - 52s - loss: 18.0322 - MinusLogProbMetric: 18.0322 - val_loss: 18.2393 - val_MinusLogProbMetric: 18.2393 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 442/1000
2023-10-23 19:24:32.212 
Epoch 442/1000 
	 loss: 17.9375, MinusLogProbMetric: 17.9375, val_loss: 17.9988, val_MinusLogProbMetric: 17.9988

Epoch 442: val_loss did not improve from 17.85782
196/196 - 50s - loss: 17.9375 - MinusLogProbMetric: 17.9375 - val_loss: 17.9988 - val_MinusLogProbMetric: 17.9988 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 443/1000
2023-10-23 19:25:21.595 
Epoch 443/1000 
	 loss: 18.0894, MinusLogProbMetric: 18.0894, val_loss: 18.1854, val_MinusLogProbMetric: 18.1854

Epoch 443: val_loss did not improve from 17.85782
196/196 - 49s - loss: 18.0894 - MinusLogProbMetric: 18.0894 - val_loss: 18.1854 - val_MinusLogProbMetric: 18.1854 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 444/1000
2023-10-23 19:26:11.114 
Epoch 444/1000 
	 loss: 17.8698, MinusLogProbMetric: 17.8698, val_loss: 17.8433, val_MinusLogProbMetric: 17.8433

Epoch 444: val_loss improved from 17.85782 to 17.84332, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 17.8698 - MinusLogProbMetric: 17.8698 - val_loss: 17.8433 - val_MinusLogProbMetric: 17.8433 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 445/1000
2023-10-23 19:27:01.510 
Epoch 445/1000 
	 loss: 17.9225, MinusLogProbMetric: 17.9225, val_loss: 17.9791, val_MinusLogProbMetric: 17.9791

Epoch 445: val_loss did not improve from 17.84332
196/196 - 50s - loss: 17.9225 - MinusLogProbMetric: 17.9225 - val_loss: 17.9791 - val_MinusLogProbMetric: 17.9791 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 446/1000
2023-10-23 19:27:52.361 
Epoch 446/1000 
	 loss: 17.9874, MinusLogProbMetric: 17.9874, val_loss: 17.9115, val_MinusLogProbMetric: 17.9115

Epoch 446: val_loss did not improve from 17.84332
196/196 - 51s - loss: 17.9874 - MinusLogProbMetric: 17.9874 - val_loss: 17.9115 - val_MinusLogProbMetric: 17.9115 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 447/1000
2023-10-23 19:28:43.784 
Epoch 447/1000 
	 loss: 17.9787, MinusLogProbMetric: 17.9787, val_loss: 18.0015, val_MinusLogProbMetric: 18.0015

Epoch 447: val_loss did not improve from 17.84332
196/196 - 51s - loss: 17.9787 - MinusLogProbMetric: 17.9787 - val_loss: 18.0015 - val_MinusLogProbMetric: 18.0015 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 448/1000
2023-10-23 19:29:34.562 
Epoch 448/1000 
	 loss: 18.0808, MinusLogProbMetric: 18.0808, val_loss: 18.0355, val_MinusLogProbMetric: 18.0355

Epoch 448: val_loss did not improve from 17.84332
196/196 - 51s - loss: 18.0808 - MinusLogProbMetric: 18.0808 - val_loss: 18.0355 - val_MinusLogProbMetric: 18.0355 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 449/1000
2023-10-23 19:30:24.020 
Epoch 449/1000 
	 loss: 17.9313, MinusLogProbMetric: 17.9313, val_loss: 18.1522, val_MinusLogProbMetric: 18.1522

Epoch 449: val_loss did not improve from 17.84332
196/196 - 49s - loss: 17.9313 - MinusLogProbMetric: 17.9313 - val_loss: 18.1522 - val_MinusLogProbMetric: 18.1522 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 450/1000
2023-10-23 19:31:13.649 
Epoch 450/1000 
	 loss: 17.8806, MinusLogProbMetric: 17.8806, val_loss: 18.0014, val_MinusLogProbMetric: 18.0014

Epoch 450: val_loss did not improve from 17.84332
196/196 - 50s - loss: 17.8806 - MinusLogProbMetric: 17.8806 - val_loss: 18.0014 - val_MinusLogProbMetric: 18.0014 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 451/1000
2023-10-23 19:32:03.264 
Epoch 451/1000 
	 loss: 18.0218, MinusLogProbMetric: 18.0218, val_loss: 18.5429, val_MinusLogProbMetric: 18.5429

Epoch 451: val_loss did not improve from 17.84332
196/196 - 50s - loss: 18.0218 - MinusLogProbMetric: 18.0218 - val_loss: 18.5429 - val_MinusLogProbMetric: 18.5429 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 452/1000
2023-10-23 19:32:53.484 
Epoch 452/1000 
	 loss: 17.8919, MinusLogProbMetric: 17.8919, val_loss: 17.8850, val_MinusLogProbMetric: 17.8850

Epoch 452: val_loss did not improve from 17.84332
196/196 - 50s - loss: 17.8919 - MinusLogProbMetric: 17.8919 - val_loss: 17.8850 - val_MinusLogProbMetric: 17.8850 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 453/1000
2023-10-23 19:33:44.423 
Epoch 453/1000 
	 loss: 17.9355, MinusLogProbMetric: 17.9355, val_loss: 18.3298, val_MinusLogProbMetric: 18.3298

Epoch 453: val_loss did not improve from 17.84332
196/196 - 51s - loss: 17.9355 - MinusLogProbMetric: 17.9355 - val_loss: 18.3298 - val_MinusLogProbMetric: 18.3298 - lr: 1.1111e-04 - 51s/epoch - 260ms/step
Epoch 454/1000
2023-10-23 19:34:35.128 
Epoch 454/1000 
	 loss: 17.9496, MinusLogProbMetric: 17.9496, val_loss: 18.6708, val_MinusLogProbMetric: 18.6708

Epoch 454: val_loss did not improve from 17.84332
196/196 - 51s - loss: 17.9496 - MinusLogProbMetric: 17.9496 - val_loss: 18.6708 - val_MinusLogProbMetric: 18.6708 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 455/1000
2023-10-23 19:35:25.167 
Epoch 455/1000 
	 loss: 17.8544, MinusLogProbMetric: 17.8544, val_loss: 17.8592, val_MinusLogProbMetric: 17.8592

Epoch 455: val_loss did not improve from 17.84332
196/196 - 50s - loss: 17.8544 - MinusLogProbMetric: 17.8544 - val_loss: 17.8592 - val_MinusLogProbMetric: 17.8592 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 456/1000
2023-10-23 19:36:15.131 
Epoch 456/1000 
	 loss: 17.9190, MinusLogProbMetric: 17.9190, val_loss: 19.0341, val_MinusLogProbMetric: 19.0341

Epoch 456: val_loss did not improve from 17.84332
196/196 - 50s - loss: 17.9190 - MinusLogProbMetric: 17.9190 - val_loss: 19.0341 - val_MinusLogProbMetric: 19.0341 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 457/1000
2023-10-23 19:37:04.771 
Epoch 457/1000 
	 loss: 18.0479, MinusLogProbMetric: 18.0479, val_loss: 17.8732, val_MinusLogProbMetric: 17.8732

Epoch 457: val_loss did not improve from 17.84332
196/196 - 50s - loss: 18.0479 - MinusLogProbMetric: 18.0479 - val_loss: 17.8732 - val_MinusLogProbMetric: 17.8732 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 458/1000
2023-10-23 19:37:54.371 
Epoch 458/1000 
	 loss: 17.8963, MinusLogProbMetric: 17.8963, val_loss: 18.2483, val_MinusLogProbMetric: 18.2483

Epoch 458: val_loss did not improve from 17.84332
196/196 - 50s - loss: 17.8963 - MinusLogProbMetric: 17.8963 - val_loss: 18.2483 - val_MinusLogProbMetric: 18.2483 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 459/1000
2023-10-23 19:38:44.574 
Epoch 459/1000 
	 loss: 18.0872, MinusLogProbMetric: 18.0872, val_loss: 17.9380, val_MinusLogProbMetric: 17.9380

Epoch 459: val_loss did not improve from 17.84332
196/196 - 50s - loss: 18.0872 - MinusLogProbMetric: 18.0872 - val_loss: 17.9380 - val_MinusLogProbMetric: 17.9380 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 460/1000
2023-10-23 19:39:34.183 
Epoch 460/1000 
	 loss: 17.7953, MinusLogProbMetric: 17.7953, val_loss: 17.9585, val_MinusLogProbMetric: 17.9585

Epoch 460: val_loss did not improve from 17.84332
196/196 - 50s - loss: 17.7953 - MinusLogProbMetric: 17.7953 - val_loss: 17.9585 - val_MinusLogProbMetric: 17.9585 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 461/1000
2023-10-23 19:40:23.988 
Epoch 461/1000 
	 loss: 18.0368, MinusLogProbMetric: 18.0368, val_loss: 17.9721, val_MinusLogProbMetric: 17.9721

Epoch 461: val_loss did not improve from 17.84332
196/196 - 50s - loss: 18.0368 - MinusLogProbMetric: 18.0368 - val_loss: 17.9721 - val_MinusLogProbMetric: 17.9721 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 462/1000
2023-10-23 19:41:13.533 
Epoch 462/1000 
	 loss: 17.9009, MinusLogProbMetric: 17.9009, val_loss: 17.9388, val_MinusLogProbMetric: 17.9388

Epoch 462: val_loss did not improve from 17.84332
196/196 - 50s - loss: 17.9009 - MinusLogProbMetric: 17.9009 - val_loss: 17.9388 - val_MinusLogProbMetric: 17.9388 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 463/1000
2023-10-23 19:42:03.393 
Epoch 463/1000 
	 loss: 17.8787, MinusLogProbMetric: 17.8787, val_loss: 18.3757, val_MinusLogProbMetric: 18.3757

Epoch 463: val_loss did not improve from 17.84332
196/196 - 50s - loss: 17.8787 - MinusLogProbMetric: 17.8787 - val_loss: 18.3757 - val_MinusLogProbMetric: 18.3757 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 464/1000
2023-10-23 19:42:55.470 
Epoch 464/1000 
	 loss: 17.8585, MinusLogProbMetric: 17.8585, val_loss: 19.5969, val_MinusLogProbMetric: 19.5969

Epoch 464: val_loss did not improve from 17.84332
196/196 - 52s - loss: 17.8585 - MinusLogProbMetric: 17.8585 - val_loss: 19.5969 - val_MinusLogProbMetric: 19.5969 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 465/1000
2023-10-23 19:43:45.935 
Epoch 465/1000 
	 loss: 18.0582, MinusLogProbMetric: 18.0582, val_loss: 17.9224, val_MinusLogProbMetric: 17.9224

Epoch 465: val_loss did not improve from 17.84332
196/196 - 50s - loss: 18.0582 - MinusLogProbMetric: 18.0582 - val_loss: 17.9224 - val_MinusLogProbMetric: 17.9224 - lr: 1.1111e-04 - 50s/epoch - 257ms/step
Epoch 466/1000
2023-10-23 19:44:36.459 
Epoch 466/1000 
	 loss: 17.9148, MinusLogProbMetric: 17.9148, val_loss: 17.8751, val_MinusLogProbMetric: 17.8751

Epoch 466: val_loss did not improve from 17.84332
196/196 - 51s - loss: 17.9148 - MinusLogProbMetric: 17.9148 - val_loss: 17.8751 - val_MinusLogProbMetric: 17.8751 - lr: 1.1111e-04 - 51s/epoch - 258ms/step
Epoch 467/1000
2023-10-23 19:45:26.341 
Epoch 467/1000 
	 loss: 17.9691, MinusLogProbMetric: 17.9691, val_loss: 17.8213, val_MinusLogProbMetric: 17.8213

Epoch 467: val_loss improved from 17.84332 to 17.82125, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 17.9691 - MinusLogProbMetric: 17.9691 - val_loss: 17.8213 - val_MinusLogProbMetric: 17.8213 - lr: 1.1111e-04 - 51s/epoch - 258ms/step
Epoch 468/1000
2023-10-23 19:46:16.556 
Epoch 468/1000 
	 loss: 17.8315, MinusLogProbMetric: 17.8315, val_loss: 18.2490, val_MinusLogProbMetric: 18.2490

Epoch 468: val_loss did not improve from 17.82125
196/196 - 49s - loss: 17.8315 - MinusLogProbMetric: 17.8315 - val_loss: 18.2490 - val_MinusLogProbMetric: 18.2490 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 469/1000
2023-10-23 19:47:06.852 
Epoch 469/1000 
	 loss: 17.9768, MinusLogProbMetric: 17.9768, val_loss: 18.4532, val_MinusLogProbMetric: 18.4532

Epoch 469: val_loss did not improve from 17.82125
196/196 - 50s - loss: 17.9768 - MinusLogProbMetric: 17.9768 - val_loss: 18.4532 - val_MinusLogProbMetric: 18.4532 - lr: 1.1111e-04 - 50s/epoch - 257ms/step
Epoch 470/1000
2023-10-23 19:47:56.901 
Epoch 470/1000 
	 loss: 18.0240, MinusLogProbMetric: 18.0240, val_loss: 19.0199, val_MinusLogProbMetric: 19.0199

Epoch 470: val_loss did not improve from 17.82125
196/196 - 50s - loss: 18.0240 - MinusLogProbMetric: 18.0240 - val_loss: 19.0199 - val_MinusLogProbMetric: 19.0199 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 471/1000
2023-10-23 19:48:46.365 
Epoch 471/1000 
	 loss: 17.9272, MinusLogProbMetric: 17.9272, val_loss: 18.3298, val_MinusLogProbMetric: 18.3298

Epoch 471: val_loss did not improve from 17.82125
196/196 - 49s - loss: 17.9272 - MinusLogProbMetric: 17.9272 - val_loss: 18.3298 - val_MinusLogProbMetric: 18.3298 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 472/1000
2023-10-23 19:49:36.741 
Epoch 472/1000 
	 loss: 17.8653, MinusLogProbMetric: 17.8653, val_loss: 18.1814, val_MinusLogProbMetric: 18.1814

Epoch 472: val_loss did not improve from 17.82125
196/196 - 50s - loss: 17.8653 - MinusLogProbMetric: 17.8653 - val_loss: 18.1814 - val_MinusLogProbMetric: 18.1814 - lr: 1.1111e-04 - 50s/epoch - 257ms/step
Epoch 473/1000
2023-10-23 19:50:26.320 
Epoch 473/1000 
	 loss: 17.9900, MinusLogProbMetric: 17.9900, val_loss: 18.2569, val_MinusLogProbMetric: 18.2569

Epoch 473: val_loss did not improve from 17.82125
196/196 - 50s - loss: 17.9900 - MinusLogProbMetric: 17.9900 - val_loss: 18.2569 - val_MinusLogProbMetric: 18.2569 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 474/1000
2023-10-23 19:51:15.304 
Epoch 474/1000 
	 loss: 17.8693, MinusLogProbMetric: 17.8693, val_loss: 17.8915, val_MinusLogProbMetric: 17.8915

Epoch 474: val_loss did not improve from 17.82125
196/196 - 49s - loss: 17.8693 - MinusLogProbMetric: 17.8693 - val_loss: 17.8915 - val_MinusLogProbMetric: 17.8915 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 475/1000
2023-10-23 19:52:04.495 
Epoch 475/1000 
	 loss: 17.9040, MinusLogProbMetric: 17.9040, val_loss: 18.0863, val_MinusLogProbMetric: 18.0863

Epoch 475: val_loss did not improve from 17.82125
196/196 - 49s - loss: 17.9040 - MinusLogProbMetric: 17.9040 - val_loss: 18.0863 - val_MinusLogProbMetric: 18.0863 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 476/1000
2023-10-23 19:52:55.015 
Epoch 476/1000 
	 loss: 18.1095, MinusLogProbMetric: 18.1095, val_loss: 18.8065, val_MinusLogProbMetric: 18.8065

Epoch 476: val_loss did not improve from 17.82125
196/196 - 51s - loss: 18.1095 - MinusLogProbMetric: 18.1095 - val_loss: 18.8065 - val_MinusLogProbMetric: 18.8065 - lr: 1.1111e-04 - 51s/epoch - 258ms/step
Epoch 477/1000
2023-10-23 19:53:48.310 
Epoch 477/1000 
	 loss: 17.8742, MinusLogProbMetric: 17.8742, val_loss: 18.3479, val_MinusLogProbMetric: 18.3479

Epoch 477: val_loss did not improve from 17.82125
196/196 - 53s - loss: 17.8742 - MinusLogProbMetric: 17.8742 - val_loss: 18.3479 - val_MinusLogProbMetric: 18.3479 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 478/1000
2023-10-23 19:54:42.627 
Epoch 478/1000 
	 loss: 17.9764, MinusLogProbMetric: 17.9764, val_loss: 18.0816, val_MinusLogProbMetric: 18.0816

Epoch 478: val_loss did not improve from 17.82125
196/196 - 54s - loss: 17.9764 - MinusLogProbMetric: 17.9764 - val_loss: 18.0816 - val_MinusLogProbMetric: 18.0816 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 479/1000
2023-10-23 19:55:33.630 
Epoch 479/1000 
	 loss: 17.9319, MinusLogProbMetric: 17.9319, val_loss: 18.6062, val_MinusLogProbMetric: 18.6062

Epoch 479: val_loss did not improve from 17.82125
196/196 - 51s - loss: 17.9319 - MinusLogProbMetric: 17.9319 - val_loss: 18.6062 - val_MinusLogProbMetric: 18.6062 - lr: 1.1111e-04 - 51s/epoch - 260ms/step
Epoch 480/1000
2023-10-23 19:56:23.442 
Epoch 480/1000 
	 loss: 17.7667, MinusLogProbMetric: 17.7667, val_loss: 17.8307, val_MinusLogProbMetric: 17.8307

Epoch 480: val_loss did not improve from 17.82125
196/196 - 50s - loss: 17.7667 - MinusLogProbMetric: 17.7667 - val_loss: 17.8307 - val_MinusLogProbMetric: 17.8307 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 481/1000
2023-10-23 19:57:12.702 
Epoch 481/1000 
	 loss: 17.8377, MinusLogProbMetric: 17.8377, val_loss: 17.8749, val_MinusLogProbMetric: 17.8749

Epoch 481: val_loss did not improve from 17.82125
196/196 - 49s - loss: 17.8377 - MinusLogProbMetric: 17.8377 - val_loss: 17.8749 - val_MinusLogProbMetric: 17.8749 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 482/1000
2023-10-23 19:58:02.026 
Epoch 482/1000 
	 loss: 17.8205, MinusLogProbMetric: 17.8205, val_loss: 18.0289, val_MinusLogProbMetric: 18.0289

Epoch 482: val_loss did not improve from 17.82125
196/196 - 49s - loss: 17.8205 - MinusLogProbMetric: 17.8205 - val_loss: 18.0289 - val_MinusLogProbMetric: 18.0289 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 483/1000
2023-10-23 19:58:51.395 
Epoch 483/1000 
	 loss: 18.0526, MinusLogProbMetric: 18.0526, val_loss: 17.8153, val_MinusLogProbMetric: 17.8153

Epoch 483: val_loss improved from 17.82125 to 17.81534, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 18.0526 - MinusLogProbMetric: 18.0526 - val_loss: 17.8153 - val_MinusLogProbMetric: 17.8153 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 484/1000
2023-10-23 19:59:42.069 
Epoch 484/1000 
	 loss: 17.9257, MinusLogProbMetric: 17.9257, val_loss: 17.9932, val_MinusLogProbMetric: 17.9932

Epoch 484: val_loss did not improve from 17.81534
196/196 - 50s - loss: 17.9257 - MinusLogProbMetric: 17.9257 - val_loss: 17.9932 - val_MinusLogProbMetric: 17.9932 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 485/1000
2023-10-23 20:00:31.247 
Epoch 485/1000 
	 loss: 17.9346, MinusLogProbMetric: 17.9346, val_loss: 17.9856, val_MinusLogProbMetric: 17.9856

Epoch 485: val_loss did not improve from 17.81534
196/196 - 49s - loss: 17.9346 - MinusLogProbMetric: 17.9346 - val_loss: 17.9856 - val_MinusLogProbMetric: 17.9856 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 486/1000
2023-10-23 20:01:20.848 
Epoch 486/1000 
	 loss: 17.7998, MinusLogProbMetric: 17.7998, val_loss: 18.2588, val_MinusLogProbMetric: 18.2588

Epoch 486: val_loss did not improve from 17.81534
196/196 - 50s - loss: 17.7998 - MinusLogProbMetric: 17.7998 - val_loss: 18.2588 - val_MinusLogProbMetric: 18.2588 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 487/1000
2023-10-23 20:02:11.387 
Epoch 487/1000 
	 loss: 17.9567, MinusLogProbMetric: 17.9567, val_loss: 18.3502, val_MinusLogProbMetric: 18.3502

Epoch 487: val_loss did not improve from 17.81534
196/196 - 51s - loss: 17.9567 - MinusLogProbMetric: 17.9567 - val_loss: 18.3502 - val_MinusLogProbMetric: 18.3502 - lr: 1.1111e-04 - 51s/epoch - 258ms/step
Epoch 488/1000
2023-10-23 20:03:00.673 
Epoch 488/1000 
	 loss: 18.1423, MinusLogProbMetric: 18.1423, val_loss: 19.2129, val_MinusLogProbMetric: 19.2129

Epoch 488: val_loss did not improve from 17.81534
196/196 - 49s - loss: 18.1423 - MinusLogProbMetric: 18.1423 - val_loss: 19.2129 - val_MinusLogProbMetric: 19.2129 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 489/1000
2023-10-23 20:03:50.508 
Epoch 489/1000 
	 loss: 17.8662, MinusLogProbMetric: 17.8662, val_loss: 17.8976, val_MinusLogProbMetric: 17.8976

Epoch 489: val_loss did not improve from 17.81534
196/196 - 50s - loss: 17.8662 - MinusLogProbMetric: 17.8662 - val_loss: 17.8976 - val_MinusLogProbMetric: 17.8976 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 490/1000
2023-10-23 20:04:40.440 
Epoch 490/1000 
	 loss: 18.0108, MinusLogProbMetric: 18.0108, val_loss: 18.3304, val_MinusLogProbMetric: 18.3304

Epoch 490: val_loss did not improve from 17.81534
196/196 - 50s - loss: 18.0108 - MinusLogProbMetric: 18.0108 - val_loss: 18.3304 - val_MinusLogProbMetric: 18.3304 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 491/1000
2023-10-23 20:05:31.057 
Epoch 491/1000 
	 loss: 17.8418, MinusLogProbMetric: 17.8418, val_loss: 17.7965, val_MinusLogProbMetric: 17.7965

Epoch 491: val_loss improved from 17.81534 to 17.79650, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 17.8418 - MinusLogProbMetric: 17.8418 - val_loss: 17.7965 - val_MinusLogProbMetric: 17.7965 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 492/1000
2023-10-23 20:06:21.635 
Epoch 492/1000 
	 loss: 17.8141, MinusLogProbMetric: 17.8141, val_loss: 18.6211, val_MinusLogProbMetric: 18.6211

Epoch 492: val_loss did not improve from 17.79650
196/196 - 50s - loss: 17.8141 - MinusLogProbMetric: 17.8141 - val_loss: 18.6211 - val_MinusLogProbMetric: 18.6211 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 493/1000
2023-10-23 20:07:11.043 
Epoch 493/1000 
	 loss: 17.9910, MinusLogProbMetric: 17.9910, val_loss: 17.8135, val_MinusLogProbMetric: 17.8135

Epoch 493: val_loss did not improve from 17.79650
196/196 - 49s - loss: 17.9910 - MinusLogProbMetric: 17.9910 - val_loss: 17.8135 - val_MinusLogProbMetric: 17.8135 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 494/1000
2023-10-23 20:08:01.343 
Epoch 494/1000 
	 loss: 17.9016, MinusLogProbMetric: 17.9016, val_loss: 18.4887, val_MinusLogProbMetric: 18.4887

Epoch 494: val_loss did not improve from 17.79650
196/196 - 50s - loss: 17.9016 - MinusLogProbMetric: 17.9016 - val_loss: 18.4887 - val_MinusLogProbMetric: 18.4887 - lr: 1.1111e-04 - 50s/epoch - 257ms/step
Epoch 495/1000
2023-10-23 20:08:50.953 
Epoch 495/1000 
	 loss: 17.8479, MinusLogProbMetric: 17.8479, val_loss: 17.9461, val_MinusLogProbMetric: 17.9461

Epoch 495: val_loss did not improve from 17.79650
196/196 - 50s - loss: 17.8479 - MinusLogProbMetric: 17.8479 - val_loss: 17.9461 - val_MinusLogProbMetric: 17.9461 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 496/1000
2023-10-23 20:09:40.510 
Epoch 496/1000 
	 loss: 18.0277, MinusLogProbMetric: 18.0277, val_loss: 18.1305, val_MinusLogProbMetric: 18.1305

Epoch 496: val_loss did not improve from 17.79650
196/196 - 50s - loss: 18.0277 - MinusLogProbMetric: 18.0277 - val_loss: 18.1305 - val_MinusLogProbMetric: 18.1305 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 497/1000
2023-10-23 20:10:29.563 
Epoch 497/1000 
	 loss: 17.7960, MinusLogProbMetric: 17.7960, val_loss: 17.7543, val_MinusLogProbMetric: 17.7543

Epoch 497: val_loss improved from 17.79650 to 17.75430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 17.7960 - MinusLogProbMetric: 17.7960 - val_loss: 17.7543 - val_MinusLogProbMetric: 17.7543 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 498/1000
2023-10-23 20:11:19.606 
Epoch 498/1000 
	 loss: 17.8288, MinusLogProbMetric: 17.8288, val_loss: 17.9704, val_MinusLogProbMetric: 17.9704

Epoch 498: val_loss did not improve from 17.75430
196/196 - 49s - loss: 17.8288 - MinusLogProbMetric: 17.8288 - val_loss: 17.9704 - val_MinusLogProbMetric: 17.9704 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 499/1000
2023-10-23 20:12:09.247 
Epoch 499/1000 
	 loss: 17.7824, MinusLogProbMetric: 17.7824, val_loss: 17.9643, val_MinusLogProbMetric: 17.9643

Epoch 499: val_loss did not improve from 17.75430
196/196 - 50s - loss: 17.7824 - MinusLogProbMetric: 17.7824 - val_loss: 17.9643 - val_MinusLogProbMetric: 17.9643 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 500/1000
2023-10-23 20:12:59.421 
Epoch 500/1000 
	 loss: 17.8251, MinusLogProbMetric: 17.8251, val_loss: 18.0357, val_MinusLogProbMetric: 18.0357

Epoch 500: val_loss did not improve from 17.75430
196/196 - 50s - loss: 17.8251 - MinusLogProbMetric: 17.8251 - val_loss: 18.0357 - val_MinusLogProbMetric: 18.0357 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 501/1000
2023-10-23 20:13:49.508 
Epoch 501/1000 
	 loss: 17.7205, MinusLogProbMetric: 17.7205, val_loss: 17.9813, val_MinusLogProbMetric: 17.9813

Epoch 501: val_loss did not improve from 17.75430
196/196 - 50s - loss: 17.7205 - MinusLogProbMetric: 17.7205 - val_loss: 17.9813 - val_MinusLogProbMetric: 17.9813 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 502/1000
2023-10-23 20:14:39.803 
Epoch 502/1000 
	 loss: 17.7840, MinusLogProbMetric: 17.7840, val_loss: 17.8966, val_MinusLogProbMetric: 17.8966

Epoch 502: val_loss did not improve from 17.75430
196/196 - 50s - loss: 17.7840 - MinusLogProbMetric: 17.7840 - val_loss: 17.8966 - val_MinusLogProbMetric: 17.8966 - lr: 1.1111e-04 - 50s/epoch - 257ms/step
Epoch 503/1000
2023-10-23 20:15:29.369 
Epoch 503/1000 
	 loss: 17.8871, MinusLogProbMetric: 17.8871, val_loss: 17.8243, val_MinusLogProbMetric: 17.8243

Epoch 503: val_loss did not improve from 17.75430
196/196 - 50s - loss: 17.8871 - MinusLogProbMetric: 17.8871 - val_loss: 17.8243 - val_MinusLogProbMetric: 17.8243 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 504/1000
2023-10-23 20:16:19.016 
Epoch 504/1000 
	 loss: 17.7432, MinusLogProbMetric: 17.7432, val_loss: 17.8473, val_MinusLogProbMetric: 17.8473

Epoch 504: val_loss did not improve from 17.75430
196/196 - 50s - loss: 17.7432 - MinusLogProbMetric: 17.7432 - val_loss: 17.8473 - val_MinusLogProbMetric: 17.8473 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 505/1000
2023-10-23 20:17:10.027 
Epoch 505/1000 
	 loss: 17.7834, MinusLogProbMetric: 17.7834, val_loss: 17.9482, val_MinusLogProbMetric: 17.9482

Epoch 505: val_loss did not improve from 17.75430
196/196 - 51s - loss: 17.7834 - MinusLogProbMetric: 17.7834 - val_loss: 17.9482 - val_MinusLogProbMetric: 17.9482 - lr: 1.1111e-04 - 51s/epoch - 260ms/step
Epoch 506/1000
2023-10-23 20:17:59.354 
Epoch 506/1000 
	 loss: 18.3270, MinusLogProbMetric: 18.3270, val_loss: 17.9146, val_MinusLogProbMetric: 17.9146

Epoch 506: val_loss did not improve from 17.75430
196/196 - 49s - loss: 18.3270 - MinusLogProbMetric: 18.3270 - val_loss: 17.9146 - val_MinusLogProbMetric: 17.9146 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 507/1000
2023-10-23 20:18:49.069 
Epoch 507/1000 
	 loss: 17.7386, MinusLogProbMetric: 17.7386, val_loss: 18.3369, val_MinusLogProbMetric: 18.3369

Epoch 507: val_loss did not improve from 17.75430
196/196 - 50s - loss: 17.7386 - MinusLogProbMetric: 17.7386 - val_loss: 18.3369 - val_MinusLogProbMetric: 18.3369 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 508/1000
2023-10-23 20:19:39.308 
Epoch 508/1000 
	 loss: 17.7342, MinusLogProbMetric: 17.7342, val_loss: 18.3458, val_MinusLogProbMetric: 18.3458

Epoch 508: val_loss did not improve from 17.75430
196/196 - 50s - loss: 17.7342 - MinusLogProbMetric: 17.7342 - val_loss: 18.3458 - val_MinusLogProbMetric: 18.3458 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 509/1000
2023-10-23 20:20:29.415 
Epoch 509/1000 
	 loss: 17.9074, MinusLogProbMetric: 17.9074, val_loss: 17.9806, val_MinusLogProbMetric: 17.9806

Epoch 509: val_loss did not improve from 17.75430
196/196 - 50s - loss: 17.9074 - MinusLogProbMetric: 17.9074 - val_loss: 17.9806 - val_MinusLogProbMetric: 17.9806 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 510/1000
2023-10-23 20:21:19.622 
Epoch 510/1000 
	 loss: 17.7958, MinusLogProbMetric: 17.7958, val_loss: 19.2797, val_MinusLogProbMetric: 19.2797

Epoch 510: val_loss did not improve from 17.75430
196/196 - 50s - loss: 17.7958 - MinusLogProbMetric: 17.7958 - val_loss: 19.2797 - val_MinusLogProbMetric: 19.2797 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 511/1000
2023-10-23 20:22:09.374 
Epoch 511/1000 
	 loss: 17.8953, MinusLogProbMetric: 17.8953, val_loss: 18.0014, val_MinusLogProbMetric: 18.0014

Epoch 511: val_loss did not improve from 17.75430
196/196 - 50s - loss: 17.8953 - MinusLogProbMetric: 17.8953 - val_loss: 18.0014 - val_MinusLogProbMetric: 18.0014 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 512/1000
2023-10-23 20:22:59.149 
Epoch 512/1000 
	 loss: 17.9297, MinusLogProbMetric: 17.9297, val_loss: 17.8760, val_MinusLogProbMetric: 17.8760

Epoch 512: val_loss did not improve from 17.75430
196/196 - 50s - loss: 17.9297 - MinusLogProbMetric: 17.9297 - val_loss: 17.8760 - val_MinusLogProbMetric: 17.8760 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 513/1000
2023-10-23 20:23:48.837 
Epoch 513/1000 
	 loss: 17.8728, MinusLogProbMetric: 17.8728, val_loss: 18.1184, val_MinusLogProbMetric: 18.1184

Epoch 513: val_loss did not improve from 17.75430
196/196 - 50s - loss: 17.8728 - MinusLogProbMetric: 17.8728 - val_loss: 18.1184 - val_MinusLogProbMetric: 18.1184 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 514/1000
2023-10-23 20:24:37.823 
Epoch 514/1000 
	 loss: 17.8015, MinusLogProbMetric: 17.8015, val_loss: 18.3754, val_MinusLogProbMetric: 18.3754

Epoch 514: val_loss did not improve from 17.75430
196/196 - 49s - loss: 17.8015 - MinusLogProbMetric: 17.8015 - val_loss: 18.3754 - val_MinusLogProbMetric: 18.3754 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 515/1000
2023-10-23 20:25:27.515 
Epoch 515/1000 
	 loss: 17.8481, MinusLogProbMetric: 17.8481, val_loss: 19.0592, val_MinusLogProbMetric: 19.0592

Epoch 515: val_loss did not improve from 17.75430
196/196 - 50s - loss: 17.8481 - MinusLogProbMetric: 17.8481 - val_loss: 19.0592 - val_MinusLogProbMetric: 19.0592 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 516/1000
2023-10-23 20:26:17.504 
Epoch 516/1000 
	 loss: 17.7730, MinusLogProbMetric: 17.7730, val_loss: 17.8375, val_MinusLogProbMetric: 17.8375

Epoch 516: val_loss did not improve from 17.75430
196/196 - 50s - loss: 17.7730 - MinusLogProbMetric: 17.7730 - val_loss: 17.8375 - val_MinusLogProbMetric: 17.8375 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 517/1000
2023-10-23 20:27:07.546 
Epoch 517/1000 
	 loss: 17.8237, MinusLogProbMetric: 17.8237, val_loss: 17.7882, val_MinusLogProbMetric: 17.7882

Epoch 517: val_loss did not improve from 17.75430
196/196 - 50s - loss: 17.8237 - MinusLogProbMetric: 17.8237 - val_loss: 17.7882 - val_MinusLogProbMetric: 17.7882 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 518/1000
2023-10-23 20:27:57.695 
Epoch 518/1000 
	 loss: 17.7144, MinusLogProbMetric: 17.7144, val_loss: 17.6514, val_MinusLogProbMetric: 17.6514

Epoch 518: val_loss improved from 17.75430 to 17.65137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 17.7144 - MinusLogProbMetric: 17.7144 - val_loss: 17.6514 - val_MinusLogProbMetric: 17.6514 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 519/1000
2023-10-23 20:28:47.761 
Epoch 519/1000 
	 loss: 17.8197, MinusLogProbMetric: 17.8197, val_loss: 18.2097, val_MinusLogProbMetric: 18.2097

Epoch 519: val_loss did not improve from 17.65137
196/196 - 49s - loss: 17.8197 - MinusLogProbMetric: 17.8197 - val_loss: 18.2097 - val_MinusLogProbMetric: 18.2097 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 520/1000
2023-10-23 20:29:37.636 
Epoch 520/1000 
	 loss: 17.8013, MinusLogProbMetric: 17.8013, val_loss: 18.0538, val_MinusLogProbMetric: 18.0538

Epoch 520: val_loss did not improve from 17.65137
196/196 - 50s - loss: 17.8013 - MinusLogProbMetric: 17.8013 - val_loss: 18.0538 - val_MinusLogProbMetric: 18.0538 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 521/1000
2023-10-23 20:30:27.218 
Epoch 521/1000 
	 loss: 17.8586, MinusLogProbMetric: 17.8586, val_loss: 18.4616, val_MinusLogProbMetric: 18.4616

Epoch 521: val_loss did not improve from 17.65137
196/196 - 50s - loss: 17.8586 - MinusLogProbMetric: 17.8586 - val_loss: 18.4616 - val_MinusLogProbMetric: 18.4616 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 522/1000
2023-10-23 20:31:18.537 
Epoch 522/1000 
	 loss: 17.7052, MinusLogProbMetric: 17.7052, val_loss: 17.9121, val_MinusLogProbMetric: 17.9121

Epoch 522: val_loss did not improve from 17.65137
196/196 - 51s - loss: 17.7052 - MinusLogProbMetric: 17.7052 - val_loss: 17.9121 - val_MinusLogProbMetric: 17.9121 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 523/1000
2023-10-23 20:32:10.162 
Epoch 523/1000 
	 loss: 17.8952, MinusLogProbMetric: 17.8952, val_loss: 18.1642, val_MinusLogProbMetric: 18.1642

Epoch 523: val_loss did not improve from 17.65137
196/196 - 52s - loss: 17.8952 - MinusLogProbMetric: 17.8952 - val_loss: 18.1642 - val_MinusLogProbMetric: 18.1642 - lr: 1.1111e-04 - 52s/epoch - 263ms/step
Epoch 524/1000
2023-10-23 20:33:04.026 
Epoch 524/1000 
	 loss: 17.8771, MinusLogProbMetric: 17.8771, val_loss: 18.8522, val_MinusLogProbMetric: 18.8522

Epoch 524: val_loss did not improve from 17.65137
196/196 - 54s - loss: 17.8771 - MinusLogProbMetric: 17.8771 - val_loss: 18.8522 - val_MinusLogProbMetric: 18.8522 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 525/1000
2023-10-23 20:33:55.176 
Epoch 525/1000 
	 loss: 17.7934, MinusLogProbMetric: 17.7934, val_loss: 18.5616, val_MinusLogProbMetric: 18.5616

Epoch 525: val_loss did not improve from 17.65137
196/196 - 51s - loss: 17.7934 - MinusLogProbMetric: 17.7934 - val_loss: 18.5616 - val_MinusLogProbMetric: 18.5616 - lr: 1.1111e-04 - 51s/epoch - 261ms/step
Epoch 526/1000
2023-10-23 20:34:47.152 
Epoch 526/1000 
	 loss: 17.7906, MinusLogProbMetric: 17.7906, val_loss: 17.7463, val_MinusLogProbMetric: 17.7463

Epoch 526: val_loss did not improve from 17.65137
196/196 - 52s - loss: 17.7906 - MinusLogProbMetric: 17.7906 - val_loss: 17.7463 - val_MinusLogProbMetric: 17.7463 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 527/1000
2023-10-23 20:35:42.364 
Epoch 527/1000 
	 loss: 17.8953, MinusLogProbMetric: 17.8953, val_loss: 18.6993, val_MinusLogProbMetric: 18.6993

Epoch 527: val_loss did not improve from 17.65137
196/196 - 55s - loss: 17.8953 - MinusLogProbMetric: 17.8953 - val_loss: 18.6993 - val_MinusLogProbMetric: 18.6993 - lr: 1.1111e-04 - 55s/epoch - 282ms/step
Epoch 528/1000
2023-10-23 20:36:33.571 
Epoch 528/1000 
	 loss: 17.8246, MinusLogProbMetric: 17.8246, val_loss: 18.4277, val_MinusLogProbMetric: 18.4277

Epoch 528: val_loss did not improve from 17.65137
196/196 - 51s - loss: 17.8246 - MinusLogProbMetric: 17.8246 - val_loss: 18.4277 - val_MinusLogProbMetric: 18.4277 - lr: 1.1111e-04 - 51s/epoch - 261ms/step
Epoch 529/1000
2023-10-23 20:37:31.689 
Epoch 529/1000 
	 loss: 17.9803, MinusLogProbMetric: 17.9803, val_loss: 18.1926, val_MinusLogProbMetric: 18.1926

Epoch 529: val_loss did not improve from 17.65137
196/196 - 58s - loss: 17.9803 - MinusLogProbMetric: 17.9803 - val_loss: 18.1926 - val_MinusLogProbMetric: 18.1926 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 530/1000
2023-10-23 20:38:26.484 
Epoch 530/1000 
	 loss: 17.7888, MinusLogProbMetric: 17.7888, val_loss: 18.3413, val_MinusLogProbMetric: 18.3413

Epoch 530: val_loss did not improve from 17.65137
196/196 - 55s - loss: 17.7888 - MinusLogProbMetric: 17.7888 - val_loss: 18.3413 - val_MinusLogProbMetric: 18.3413 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 531/1000
2023-10-23 20:39:22.357 
Epoch 531/1000 
	 loss: 17.7476, MinusLogProbMetric: 17.7476, val_loss: 17.8063, val_MinusLogProbMetric: 17.8063

Epoch 531: val_loss did not improve from 17.65137
196/196 - 56s - loss: 17.7476 - MinusLogProbMetric: 17.7476 - val_loss: 17.8063 - val_MinusLogProbMetric: 17.8063 - lr: 1.1111e-04 - 56s/epoch - 285ms/step
Epoch 532/1000
2023-10-23 20:40:18.586 
Epoch 532/1000 
	 loss: 17.7732, MinusLogProbMetric: 17.7732, val_loss: 17.7972, val_MinusLogProbMetric: 17.7972

Epoch 532: val_loss did not improve from 17.65137
196/196 - 56s - loss: 17.7732 - MinusLogProbMetric: 17.7732 - val_loss: 17.7972 - val_MinusLogProbMetric: 17.7972 - lr: 1.1111e-04 - 56s/epoch - 287ms/step
Epoch 533/1000
2023-10-23 20:41:09.053 
Epoch 533/1000 
	 loss: 17.7614, MinusLogProbMetric: 17.7614, val_loss: 17.6908, val_MinusLogProbMetric: 17.6908

Epoch 533: val_loss did not improve from 17.65137
196/196 - 50s - loss: 17.7614 - MinusLogProbMetric: 17.7614 - val_loss: 17.6908 - val_MinusLogProbMetric: 17.6908 - lr: 1.1111e-04 - 50s/epoch - 257ms/step
Epoch 534/1000
2023-10-23 20:41:59.525 
Epoch 534/1000 
	 loss: 17.8917, MinusLogProbMetric: 17.8917, val_loss: 18.0970, val_MinusLogProbMetric: 18.0970

Epoch 534: val_loss did not improve from 17.65137
196/196 - 50s - loss: 17.8917 - MinusLogProbMetric: 17.8917 - val_loss: 18.0970 - val_MinusLogProbMetric: 18.0970 - lr: 1.1111e-04 - 50s/epoch - 257ms/step
Epoch 535/1000
2023-10-23 20:42:51.442 
Epoch 535/1000 
	 loss: 17.9142, MinusLogProbMetric: 17.9142, val_loss: 18.0684, val_MinusLogProbMetric: 18.0684

Epoch 535: val_loss did not improve from 17.65137
196/196 - 52s - loss: 17.9142 - MinusLogProbMetric: 17.9142 - val_loss: 18.0684 - val_MinusLogProbMetric: 18.0684 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 536/1000
2023-10-23 20:43:46.526 
Epoch 536/1000 
	 loss: 17.8540, MinusLogProbMetric: 17.8540, val_loss: 17.8208, val_MinusLogProbMetric: 17.8208

Epoch 536: val_loss did not improve from 17.65137
196/196 - 55s - loss: 17.8540 - MinusLogProbMetric: 17.8540 - val_loss: 17.8208 - val_MinusLogProbMetric: 17.8208 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 537/1000
2023-10-23 20:44:41.059 
Epoch 537/1000 
	 loss: 17.9194, MinusLogProbMetric: 17.9194, val_loss: 17.9316, val_MinusLogProbMetric: 17.9316

Epoch 537: val_loss did not improve from 17.65137
196/196 - 55s - loss: 17.9194 - MinusLogProbMetric: 17.9194 - val_loss: 17.9316 - val_MinusLogProbMetric: 17.9316 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 538/1000
2023-10-23 20:45:36.053 
Epoch 538/1000 
	 loss: 17.8755, MinusLogProbMetric: 17.8755, val_loss: 17.7296, val_MinusLogProbMetric: 17.7296

Epoch 538: val_loss did not improve from 17.65137
196/196 - 55s - loss: 17.8755 - MinusLogProbMetric: 17.8755 - val_loss: 17.7296 - val_MinusLogProbMetric: 17.7296 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 539/1000
2023-10-23 20:46:33.242 
Epoch 539/1000 
	 loss: 17.8386, MinusLogProbMetric: 17.8386, val_loss: 18.2768, val_MinusLogProbMetric: 18.2768

Epoch 539: val_loss did not improve from 17.65137
196/196 - 57s - loss: 17.8386 - MinusLogProbMetric: 17.8386 - val_loss: 18.2768 - val_MinusLogProbMetric: 18.2768 - lr: 1.1111e-04 - 57s/epoch - 292ms/step
Epoch 540/1000
2023-10-23 20:47:27.204 
Epoch 540/1000 
	 loss: 17.8703, MinusLogProbMetric: 17.8703, val_loss: 17.8375, val_MinusLogProbMetric: 17.8375

Epoch 540: val_loss did not improve from 17.65137
196/196 - 54s - loss: 17.8703 - MinusLogProbMetric: 17.8703 - val_loss: 17.8375 - val_MinusLogProbMetric: 17.8375 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 541/1000
2023-10-23 20:48:19.689 
Epoch 541/1000 
	 loss: 17.7169, MinusLogProbMetric: 17.7169, val_loss: 17.8498, val_MinusLogProbMetric: 17.8498

Epoch 541: val_loss did not improve from 17.65137
196/196 - 52s - loss: 17.7169 - MinusLogProbMetric: 17.7169 - val_loss: 17.8498 - val_MinusLogProbMetric: 17.8498 - lr: 1.1111e-04 - 52s/epoch - 268ms/step
Epoch 542/1000
2023-10-23 20:49:12.111 
Epoch 542/1000 
	 loss: 17.8622, MinusLogProbMetric: 17.8622, val_loss: 17.7806, val_MinusLogProbMetric: 17.7806

Epoch 542: val_loss did not improve from 17.65137
196/196 - 52s - loss: 17.8622 - MinusLogProbMetric: 17.8622 - val_loss: 17.7806 - val_MinusLogProbMetric: 17.7806 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 543/1000
2023-10-23 20:50:03.244 
Epoch 543/1000 
	 loss: 17.9101, MinusLogProbMetric: 17.9101, val_loss: 17.7502, val_MinusLogProbMetric: 17.7502

Epoch 543: val_loss did not improve from 17.65137
196/196 - 51s - loss: 17.9101 - MinusLogProbMetric: 17.9101 - val_loss: 17.7502 - val_MinusLogProbMetric: 17.7502 - lr: 1.1111e-04 - 51s/epoch - 261ms/step
Epoch 544/1000
2023-10-23 20:50:57.552 
Epoch 544/1000 
	 loss: 17.8330, MinusLogProbMetric: 17.8330, val_loss: 18.0800, val_MinusLogProbMetric: 18.0800

Epoch 544: val_loss did not improve from 17.65137
196/196 - 54s - loss: 17.8330 - MinusLogProbMetric: 17.8330 - val_loss: 18.0800 - val_MinusLogProbMetric: 18.0800 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 545/1000
2023-10-23 20:51:54.468 
Epoch 545/1000 
	 loss: 17.7571, MinusLogProbMetric: 17.7571, val_loss: 17.7250, val_MinusLogProbMetric: 17.7250

Epoch 545: val_loss did not improve from 17.65137
196/196 - 57s - loss: 17.7571 - MinusLogProbMetric: 17.7571 - val_loss: 17.7250 - val_MinusLogProbMetric: 17.7250 - lr: 1.1111e-04 - 57s/epoch - 290ms/step
Epoch 546/1000
2023-10-23 20:52:51.166 
Epoch 546/1000 
	 loss: 17.8819, MinusLogProbMetric: 17.8819, val_loss: 18.1546, val_MinusLogProbMetric: 18.1546

Epoch 546: val_loss did not improve from 17.65137
196/196 - 57s - loss: 17.8819 - MinusLogProbMetric: 17.8819 - val_loss: 18.1546 - val_MinusLogProbMetric: 18.1546 - lr: 1.1111e-04 - 57s/epoch - 289ms/step
Epoch 547/1000
2023-10-23 20:53:43.361 
Epoch 547/1000 
	 loss: 17.7969, MinusLogProbMetric: 17.7969, val_loss: 17.8340, val_MinusLogProbMetric: 17.8340

Epoch 547: val_loss did not improve from 17.65137
196/196 - 52s - loss: 17.7969 - MinusLogProbMetric: 17.7969 - val_loss: 17.8340 - val_MinusLogProbMetric: 17.8340 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 548/1000
2023-10-23 20:54:37.817 
Epoch 548/1000 
	 loss: 17.6825, MinusLogProbMetric: 17.6825, val_loss: 17.7911, val_MinusLogProbMetric: 17.7911

Epoch 548: val_loss did not improve from 17.65137
196/196 - 54s - loss: 17.6825 - MinusLogProbMetric: 17.6825 - val_loss: 17.7911 - val_MinusLogProbMetric: 17.7911 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 549/1000
2023-10-23 20:55:32.645 
Epoch 549/1000 
	 loss: 17.8522, MinusLogProbMetric: 17.8522, val_loss: 18.7580, val_MinusLogProbMetric: 18.7580

Epoch 549: val_loss did not improve from 17.65137
196/196 - 55s - loss: 17.8522 - MinusLogProbMetric: 17.8522 - val_loss: 18.7580 - val_MinusLogProbMetric: 18.7580 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 550/1000
2023-10-23 20:56:25.413 
Epoch 550/1000 
	 loss: 17.8245, MinusLogProbMetric: 17.8245, val_loss: 17.9055, val_MinusLogProbMetric: 17.9055

Epoch 550: val_loss did not improve from 17.65137
196/196 - 53s - loss: 17.8245 - MinusLogProbMetric: 17.8245 - val_loss: 17.9055 - val_MinusLogProbMetric: 17.9055 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 551/1000
2023-10-23 20:57:19.979 
Epoch 551/1000 
	 loss: 17.8890, MinusLogProbMetric: 17.8890, val_loss: 17.7690, val_MinusLogProbMetric: 17.7690

Epoch 551: val_loss did not improve from 17.65137
196/196 - 55s - loss: 17.8890 - MinusLogProbMetric: 17.8890 - val_loss: 17.7690 - val_MinusLogProbMetric: 17.7690 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 552/1000
2023-10-23 20:58:18.175 
Epoch 552/1000 
	 loss: 17.9147, MinusLogProbMetric: 17.9147, val_loss: 18.7745, val_MinusLogProbMetric: 18.7745

Epoch 552: val_loss did not improve from 17.65137
196/196 - 58s - loss: 17.9147 - MinusLogProbMetric: 17.9147 - val_loss: 18.7745 - val_MinusLogProbMetric: 18.7745 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 553/1000
2023-10-23 20:59:11.845 
Epoch 553/1000 
	 loss: 17.8390, MinusLogProbMetric: 17.8390, val_loss: 17.7866, val_MinusLogProbMetric: 17.7866

Epoch 553: val_loss did not improve from 17.65137
196/196 - 54s - loss: 17.8390 - MinusLogProbMetric: 17.8390 - val_loss: 17.7866 - val_MinusLogProbMetric: 17.7866 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 554/1000
2023-10-23 21:00:07.629 
Epoch 554/1000 
	 loss: 17.7340, MinusLogProbMetric: 17.7340, val_loss: 17.8612, val_MinusLogProbMetric: 17.8612

Epoch 554: val_loss did not improve from 17.65137
196/196 - 56s - loss: 17.7340 - MinusLogProbMetric: 17.7340 - val_loss: 17.8612 - val_MinusLogProbMetric: 17.8612 - lr: 1.1111e-04 - 56s/epoch - 285ms/step
Epoch 555/1000
2023-10-23 21:01:01.940 
Epoch 555/1000 
	 loss: 17.7776, MinusLogProbMetric: 17.7776, val_loss: 18.0895, val_MinusLogProbMetric: 18.0895

Epoch 555: val_loss did not improve from 17.65137
196/196 - 54s - loss: 17.7776 - MinusLogProbMetric: 17.7776 - val_loss: 18.0895 - val_MinusLogProbMetric: 18.0895 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 556/1000
2023-10-23 21:01:58.609 
Epoch 556/1000 
	 loss: 17.8734, MinusLogProbMetric: 17.8734, val_loss: 17.8522, val_MinusLogProbMetric: 17.8522

Epoch 556: val_loss did not improve from 17.65137
196/196 - 57s - loss: 17.8734 - MinusLogProbMetric: 17.8734 - val_loss: 17.8522 - val_MinusLogProbMetric: 17.8522 - lr: 1.1111e-04 - 57s/epoch - 289ms/step
Epoch 557/1000
2023-10-23 21:02:51.814 
Epoch 557/1000 
	 loss: 17.7656, MinusLogProbMetric: 17.7656, val_loss: 19.0945, val_MinusLogProbMetric: 19.0945

Epoch 557: val_loss did not improve from 17.65137
196/196 - 53s - loss: 17.7656 - MinusLogProbMetric: 17.7656 - val_loss: 19.0945 - val_MinusLogProbMetric: 19.0945 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 558/1000
2023-10-23 21:03:51.340 
Epoch 558/1000 
	 loss: 17.7201, MinusLogProbMetric: 17.7201, val_loss: 18.2748, val_MinusLogProbMetric: 18.2748

Epoch 558: val_loss did not improve from 17.65137
196/196 - 60s - loss: 17.7201 - MinusLogProbMetric: 17.7201 - val_loss: 18.2748 - val_MinusLogProbMetric: 18.2748 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 559/1000
2023-10-23 21:04:49.283 
Epoch 559/1000 
	 loss: 17.7639, MinusLogProbMetric: 17.7639, val_loss: 17.8540, val_MinusLogProbMetric: 17.8540

Epoch 559: val_loss did not improve from 17.65137
196/196 - 58s - loss: 17.7639 - MinusLogProbMetric: 17.7639 - val_loss: 17.8540 - val_MinusLogProbMetric: 17.8540 - lr: 1.1111e-04 - 58s/epoch - 296ms/step
Epoch 560/1000
2023-10-23 21:05:43.424 
Epoch 560/1000 
	 loss: 17.6725, MinusLogProbMetric: 17.6725, val_loss: 17.9200, val_MinusLogProbMetric: 17.9200

Epoch 560: val_loss did not improve from 17.65137
196/196 - 54s - loss: 17.6725 - MinusLogProbMetric: 17.6725 - val_loss: 17.9200 - val_MinusLogProbMetric: 17.9200 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 561/1000
2023-10-23 21:06:38.527 
Epoch 561/1000 
	 loss: 17.8436, MinusLogProbMetric: 17.8436, val_loss: 17.9410, val_MinusLogProbMetric: 17.9410

Epoch 561: val_loss did not improve from 17.65137
196/196 - 55s - loss: 17.8436 - MinusLogProbMetric: 17.8436 - val_loss: 17.9410 - val_MinusLogProbMetric: 17.9410 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 562/1000
2023-10-23 21:07:37.617 
Epoch 562/1000 
	 loss: 17.7649, MinusLogProbMetric: 17.7649, val_loss: 17.8267, val_MinusLogProbMetric: 17.8267

Epoch 562: val_loss did not improve from 17.65137
196/196 - 59s - loss: 17.7649 - MinusLogProbMetric: 17.7649 - val_loss: 17.8267 - val_MinusLogProbMetric: 17.8267 - lr: 1.1111e-04 - 59s/epoch - 301ms/step
Epoch 563/1000
2023-10-23 21:08:27.783 
Epoch 563/1000 
	 loss: 17.7818, MinusLogProbMetric: 17.7818, val_loss: 17.8296, val_MinusLogProbMetric: 17.8296

Epoch 563: val_loss did not improve from 17.65137
196/196 - 50s - loss: 17.7818 - MinusLogProbMetric: 17.7818 - val_loss: 17.8296 - val_MinusLogProbMetric: 17.8296 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 564/1000
2023-10-23 21:09:16.787 
Epoch 564/1000 
	 loss: 17.6615, MinusLogProbMetric: 17.6615, val_loss: 17.8436, val_MinusLogProbMetric: 17.8436

Epoch 564: val_loss did not improve from 17.65137
196/196 - 49s - loss: 17.6615 - MinusLogProbMetric: 17.6615 - val_loss: 17.8436 - val_MinusLogProbMetric: 17.8436 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 565/1000
2023-10-23 21:10:06.696 
Epoch 565/1000 
	 loss: 17.7619, MinusLogProbMetric: 17.7619, val_loss: 18.1504, val_MinusLogProbMetric: 18.1504

Epoch 565: val_loss did not improve from 17.65137
196/196 - 50s - loss: 17.7619 - MinusLogProbMetric: 17.7619 - val_loss: 18.1504 - val_MinusLogProbMetric: 18.1504 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 566/1000
2023-10-23 21:11:00.922 
Epoch 566/1000 
	 loss: 17.7604, MinusLogProbMetric: 17.7604, val_loss: 17.7927, val_MinusLogProbMetric: 17.7927

Epoch 566: val_loss did not improve from 17.65137
196/196 - 54s - loss: 17.7604 - MinusLogProbMetric: 17.7604 - val_loss: 17.7927 - val_MinusLogProbMetric: 17.7927 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 567/1000
2023-10-23 21:11:49.938 
Epoch 567/1000 
	 loss: 17.7387, MinusLogProbMetric: 17.7387, val_loss: 17.7886, val_MinusLogProbMetric: 17.7886

Epoch 567: val_loss did not improve from 17.65137
196/196 - 49s - loss: 17.7387 - MinusLogProbMetric: 17.7387 - val_loss: 17.7886 - val_MinusLogProbMetric: 17.7886 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 568/1000
2023-10-23 21:12:39.022 
Epoch 568/1000 
	 loss: 18.0772, MinusLogProbMetric: 18.0772, val_loss: 17.7550, val_MinusLogProbMetric: 17.7550

Epoch 568: val_loss did not improve from 17.65137
196/196 - 49s - loss: 18.0772 - MinusLogProbMetric: 18.0772 - val_loss: 17.7550 - val_MinusLogProbMetric: 17.7550 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 569/1000
2023-10-23 21:13:28.102 
Epoch 569/1000 
	 loss: 17.4227, MinusLogProbMetric: 17.4227, val_loss: 17.5375, val_MinusLogProbMetric: 17.5375

Epoch 569: val_loss improved from 17.65137 to 17.53751, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 17.4227 - MinusLogProbMetric: 17.4227 - val_loss: 17.5375 - val_MinusLogProbMetric: 17.5375 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 570/1000
2023-10-23 21:14:17.881 
Epoch 570/1000 
	 loss: 17.4165, MinusLogProbMetric: 17.4165, val_loss: 17.6063, val_MinusLogProbMetric: 17.6063

Epoch 570: val_loss did not improve from 17.53751
196/196 - 49s - loss: 17.4165 - MinusLogProbMetric: 17.4165 - val_loss: 17.6063 - val_MinusLogProbMetric: 17.6063 - lr: 5.5556e-05 - 49s/epoch - 249ms/step
Epoch 571/1000
2023-10-23 21:15:08.646 
Epoch 571/1000 
	 loss: 17.4645, MinusLogProbMetric: 17.4645, val_loss: 17.5934, val_MinusLogProbMetric: 17.5934

Epoch 571: val_loss did not improve from 17.53751
196/196 - 51s - loss: 17.4645 - MinusLogProbMetric: 17.4645 - val_loss: 17.5934 - val_MinusLogProbMetric: 17.5934 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 572/1000
2023-10-23 21:15:56.840 
Epoch 572/1000 
	 loss: 17.4262, MinusLogProbMetric: 17.4262, val_loss: 17.5464, val_MinusLogProbMetric: 17.5464

Epoch 572: val_loss did not improve from 17.53751
196/196 - 48s - loss: 17.4262 - MinusLogProbMetric: 17.4262 - val_loss: 17.5464 - val_MinusLogProbMetric: 17.5464 - lr: 5.5556e-05 - 48s/epoch - 246ms/step
Epoch 573/1000
2023-10-23 21:16:46.012 
Epoch 573/1000 
	 loss: 17.4123, MinusLogProbMetric: 17.4123, val_loss: 17.5938, val_MinusLogProbMetric: 17.5938

Epoch 573: val_loss did not improve from 17.53751
196/196 - 49s - loss: 17.4123 - MinusLogProbMetric: 17.4123 - val_loss: 17.5938 - val_MinusLogProbMetric: 17.5938 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 574/1000
2023-10-23 21:17:34.017 
Epoch 574/1000 
	 loss: 17.4174, MinusLogProbMetric: 17.4174, val_loss: 17.5120, val_MinusLogProbMetric: 17.5120

Epoch 574: val_loss improved from 17.53751 to 17.51196, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 49s - loss: 17.4174 - MinusLogProbMetric: 17.4174 - val_loss: 17.5120 - val_MinusLogProbMetric: 17.5120 - lr: 5.5556e-05 - 49s/epoch - 249ms/step
Epoch 575/1000
2023-10-23 21:18:22.927 
Epoch 575/1000 
	 loss: 17.4396, MinusLogProbMetric: 17.4396, val_loss: 17.5572, val_MinusLogProbMetric: 17.5572

Epoch 575: val_loss did not improve from 17.51196
196/196 - 48s - loss: 17.4396 - MinusLogProbMetric: 17.4396 - val_loss: 17.5572 - val_MinusLogProbMetric: 17.5572 - lr: 5.5556e-05 - 48s/epoch - 246ms/step
Epoch 576/1000
2023-10-23 21:19:11.044 
Epoch 576/1000 
	 loss: 17.4603, MinusLogProbMetric: 17.4603, val_loss: 17.6942, val_MinusLogProbMetric: 17.6942

Epoch 576: val_loss did not improve from 17.51196
196/196 - 48s - loss: 17.4603 - MinusLogProbMetric: 17.4603 - val_loss: 17.6942 - val_MinusLogProbMetric: 17.6942 - lr: 5.5556e-05 - 48s/epoch - 245ms/step
Epoch 577/1000
2023-10-23 21:19:59.538 
Epoch 577/1000 
	 loss: 17.4417, MinusLogProbMetric: 17.4417, val_loss: 17.5896, val_MinusLogProbMetric: 17.5896

Epoch 577: val_loss did not improve from 17.51196
196/196 - 48s - loss: 17.4417 - MinusLogProbMetric: 17.4417 - val_loss: 17.5896 - val_MinusLogProbMetric: 17.5896 - lr: 5.5556e-05 - 48s/epoch - 247ms/step
Epoch 578/1000
2023-10-23 21:20:49.028 
Epoch 578/1000 
	 loss: 17.4068, MinusLogProbMetric: 17.4068, val_loss: 17.5029, val_MinusLogProbMetric: 17.5029

Epoch 578: val_loss improved from 17.51196 to 17.50287, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 17.4068 - MinusLogProbMetric: 17.4068 - val_loss: 17.5029 - val_MinusLogProbMetric: 17.5029 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 579/1000
2023-10-23 21:21:38.147 
Epoch 579/1000 
	 loss: 17.3828, MinusLogProbMetric: 17.3828, val_loss: 17.5420, val_MinusLogProbMetric: 17.5420

Epoch 579: val_loss did not improve from 17.50287
196/196 - 48s - loss: 17.3828 - MinusLogProbMetric: 17.3828 - val_loss: 17.5420 - val_MinusLogProbMetric: 17.5420 - lr: 5.5556e-05 - 48s/epoch - 247ms/step
Epoch 580/1000
2023-10-23 21:22:26.788 
Epoch 580/1000 
	 loss: 17.4139, MinusLogProbMetric: 17.4139, val_loss: 17.5617, val_MinusLogProbMetric: 17.5617

Epoch 580: val_loss did not improve from 17.50287
196/196 - 49s - loss: 17.4139 - MinusLogProbMetric: 17.4139 - val_loss: 17.5617 - val_MinusLogProbMetric: 17.5617 - lr: 5.5556e-05 - 49s/epoch - 248ms/step
Epoch 581/1000
2023-10-23 21:23:16.048 
Epoch 581/1000 
	 loss: 17.3945, MinusLogProbMetric: 17.3945, val_loss: 17.5785, val_MinusLogProbMetric: 17.5785

Epoch 581: val_loss did not improve from 17.50287
196/196 - 49s - loss: 17.3945 - MinusLogProbMetric: 17.3945 - val_loss: 17.5785 - val_MinusLogProbMetric: 17.5785 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 582/1000
2023-10-23 21:24:04.316 
Epoch 582/1000 
	 loss: 17.3785, MinusLogProbMetric: 17.3785, val_loss: 17.5337, val_MinusLogProbMetric: 17.5337

Epoch 582: val_loss did not improve from 17.50287
196/196 - 48s - loss: 17.3785 - MinusLogProbMetric: 17.3785 - val_loss: 17.5337 - val_MinusLogProbMetric: 17.5337 - lr: 5.5556e-05 - 48s/epoch - 246ms/step
Epoch 583/1000
2023-10-23 21:24:54.016 
Epoch 583/1000 
	 loss: 17.4582, MinusLogProbMetric: 17.4582, val_loss: 17.6567, val_MinusLogProbMetric: 17.6567

Epoch 583: val_loss did not improve from 17.50287
196/196 - 50s - loss: 17.4582 - MinusLogProbMetric: 17.4582 - val_loss: 17.6567 - val_MinusLogProbMetric: 17.6567 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 584/1000
2023-10-23 21:25:49.011 
Epoch 584/1000 
	 loss: 17.4242, MinusLogProbMetric: 17.4242, val_loss: 17.5360, val_MinusLogProbMetric: 17.5360

Epoch 584: val_loss did not improve from 17.50287
196/196 - 55s - loss: 17.4242 - MinusLogProbMetric: 17.4242 - val_loss: 17.5360 - val_MinusLogProbMetric: 17.5360 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 585/1000
2023-10-23 21:26:50.552 
Epoch 585/1000 
	 loss: 17.4023, MinusLogProbMetric: 17.4023, val_loss: 17.5567, val_MinusLogProbMetric: 17.5567

Epoch 585: val_loss did not improve from 17.50287
196/196 - 62s - loss: 17.4023 - MinusLogProbMetric: 17.4023 - val_loss: 17.5567 - val_MinusLogProbMetric: 17.5567 - lr: 5.5556e-05 - 62s/epoch - 314ms/step
Epoch 586/1000
2023-10-23 21:27:51.186 
Epoch 586/1000 
	 loss: 17.4480, MinusLogProbMetric: 17.4480, val_loss: 17.6973, val_MinusLogProbMetric: 17.6973

Epoch 586: val_loss did not improve from 17.50287
196/196 - 61s - loss: 17.4480 - MinusLogProbMetric: 17.4480 - val_loss: 17.6973 - val_MinusLogProbMetric: 17.6973 - lr: 5.5556e-05 - 61s/epoch - 309ms/step
Epoch 587/1000
2023-10-23 21:28:48.607 
Epoch 587/1000 
	 loss: 17.3867, MinusLogProbMetric: 17.3867, val_loss: 17.7384, val_MinusLogProbMetric: 17.7384

Epoch 587: val_loss did not improve from 17.50287
196/196 - 57s - loss: 17.3867 - MinusLogProbMetric: 17.3867 - val_loss: 17.7384 - val_MinusLogProbMetric: 17.7384 - lr: 5.5556e-05 - 57s/epoch - 293ms/step
Epoch 588/1000
2023-10-23 21:29:52.519 
Epoch 588/1000 
	 loss: 17.4378, MinusLogProbMetric: 17.4378, val_loss: 17.7368, val_MinusLogProbMetric: 17.7368

Epoch 588: val_loss did not improve from 17.50287
196/196 - 64s - loss: 17.4378 - MinusLogProbMetric: 17.4378 - val_loss: 17.7368 - val_MinusLogProbMetric: 17.7368 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 589/1000
2023-10-23 21:30:53.647 
Epoch 589/1000 
	 loss: 17.3922, MinusLogProbMetric: 17.3922, val_loss: 17.5737, val_MinusLogProbMetric: 17.5737

Epoch 589: val_loss did not improve from 17.50287
196/196 - 61s - loss: 17.3922 - MinusLogProbMetric: 17.3922 - val_loss: 17.5737 - val_MinusLogProbMetric: 17.5737 - lr: 5.5556e-05 - 61s/epoch - 312ms/step
Epoch 590/1000
2023-10-23 21:31:53.639 
Epoch 590/1000 
	 loss: 17.3939, MinusLogProbMetric: 17.3939, val_loss: 17.6305, val_MinusLogProbMetric: 17.6305

Epoch 590: val_loss did not improve from 17.50287
196/196 - 60s - loss: 17.3939 - MinusLogProbMetric: 17.3939 - val_loss: 17.6305 - val_MinusLogProbMetric: 17.6305 - lr: 5.5556e-05 - 60s/epoch - 306ms/step
Epoch 591/1000
2023-10-23 21:32:54.270 
Epoch 591/1000 
	 loss: 17.4509, MinusLogProbMetric: 17.4509, val_loss: 17.6438, val_MinusLogProbMetric: 17.6438

Epoch 591: val_loss did not improve from 17.50287
196/196 - 61s - loss: 17.4509 - MinusLogProbMetric: 17.4509 - val_loss: 17.6438 - val_MinusLogProbMetric: 17.6438 - lr: 5.5556e-05 - 61s/epoch - 309ms/step
Epoch 592/1000
2023-10-23 21:34:02.545 
Epoch 592/1000 
	 loss: 17.3796, MinusLogProbMetric: 17.3796, val_loss: 18.2081, val_MinusLogProbMetric: 18.2081

Epoch 592: val_loss did not improve from 17.50287
196/196 - 68s - loss: 17.3796 - MinusLogProbMetric: 17.3796 - val_loss: 18.2081 - val_MinusLogProbMetric: 18.2081 - lr: 5.5556e-05 - 68s/epoch - 348ms/step
Epoch 593/1000
2023-10-23 21:35:08.557 
Epoch 593/1000 
	 loss: 17.4298, MinusLogProbMetric: 17.4298, val_loss: 17.5134, val_MinusLogProbMetric: 17.5134

Epoch 593: val_loss did not improve from 17.50287
196/196 - 66s - loss: 17.4298 - MinusLogProbMetric: 17.4298 - val_loss: 17.5134 - val_MinusLogProbMetric: 17.5134 - lr: 5.5556e-05 - 66s/epoch - 337ms/step
Epoch 594/1000
2023-10-23 21:36:16.006 
Epoch 594/1000 
	 loss: 17.3843, MinusLogProbMetric: 17.3843, val_loss: 17.8498, val_MinusLogProbMetric: 17.8498

Epoch 594: val_loss did not improve from 17.50287
196/196 - 67s - loss: 17.3843 - MinusLogProbMetric: 17.3843 - val_loss: 17.8498 - val_MinusLogProbMetric: 17.8498 - lr: 5.5556e-05 - 67s/epoch - 344ms/step
Epoch 595/1000
2023-10-23 21:37:23.928 
Epoch 595/1000 
	 loss: 17.4205, MinusLogProbMetric: 17.4205, val_loss: 17.5319, val_MinusLogProbMetric: 17.5319

Epoch 595: val_loss did not improve from 17.50287
196/196 - 68s - loss: 17.4205 - MinusLogProbMetric: 17.4205 - val_loss: 17.5319 - val_MinusLogProbMetric: 17.5319 - lr: 5.5556e-05 - 68s/epoch - 347ms/step
Epoch 596/1000
2023-10-23 21:38:27.823 
Epoch 596/1000 
	 loss: 17.4062, MinusLogProbMetric: 17.4062, val_loss: 17.5774, val_MinusLogProbMetric: 17.5774

Epoch 596: val_loss did not improve from 17.50287
196/196 - 64s - loss: 17.4062 - MinusLogProbMetric: 17.4062 - val_loss: 17.5774 - val_MinusLogProbMetric: 17.5774 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 597/1000
2023-10-23 21:39:36.402 
Epoch 597/1000 
	 loss: 17.5223, MinusLogProbMetric: 17.5223, val_loss: 17.5011, val_MinusLogProbMetric: 17.5011

Epoch 597: val_loss improved from 17.50287 to 17.50105, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 69s - loss: 17.5223 - MinusLogProbMetric: 17.5223 - val_loss: 17.5011 - val_MinusLogProbMetric: 17.5011 - lr: 5.5556e-05 - 69s/epoch - 355ms/step
Epoch 598/1000
2023-10-23 21:40:44.198 
Epoch 598/1000 
	 loss: 17.4361, MinusLogProbMetric: 17.4361, val_loss: 17.4542, val_MinusLogProbMetric: 17.4542

Epoch 598: val_loss improved from 17.50105 to 17.45422, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 68s - loss: 17.4361 - MinusLogProbMetric: 17.4361 - val_loss: 17.4542 - val_MinusLogProbMetric: 17.4542 - lr: 5.5556e-05 - 68s/epoch - 346ms/step
Epoch 599/1000
2023-10-23 21:41:52.266 
Epoch 599/1000 
	 loss: 17.3797, MinusLogProbMetric: 17.3797, val_loss: 17.5768, val_MinusLogProbMetric: 17.5768

Epoch 599: val_loss did not improve from 17.45422
196/196 - 67s - loss: 17.3797 - MinusLogProbMetric: 17.3797 - val_loss: 17.5768 - val_MinusLogProbMetric: 17.5768 - lr: 5.5556e-05 - 67s/epoch - 342ms/step
Epoch 600/1000
2023-10-23 21:42:55.918 
Epoch 600/1000 
	 loss: 17.4048, MinusLogProbMetric: 17.4048, val_loss: 17.6909, val_MinusLogProbMetric: 17.6909

Epoch 600: val_loss did not improve from 17.45422
196/196 - 64s - loss: 17.4048 - MinusLogProbMetric: 17.4048 - val_loss: 17.6909 - val_MinusLogProbMetric: 17.6909 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 601/1000
2023-10-23 21:44:00.324 
Epoch 601/1000 
	 loss: 17.3803, MinusLogProbMetric: 17.3803, val_loss: 17.6222, val_MinusLogProbMetric: 17.6222

Epoch 601: val_loss did not improve from 17.45422
196/196 - 64s - loss: 17.3803 - MinusLogProbMetric: 17.3803 - val_loss: 17.6222 - val_MinusLogProbMetric: 17.6222 - lr: 5.5556e-05 - 64s/epoch - 329ms/step
Epoch 602/1000
2023-10-23 21:45:02.412 
Epoch 602/1000 
	 loss: 17.4608, MinusLogProbMetric: 17.4608, val_loss: 17.4842, val_MinusLogProbMetric: 17.4842

Epoch 602: val_loss did not improve from 17.45422
196/196 - 62s - loss: 17.4608 - MinusLogProbMetric: 17.4608 - val_loss: 17.4842 - val_MinusLogProbMetric: 17.4842 - lr: 5.5556e-05 - 62s/epoch - 317ms/step
Epoch 603/1000
2023-10-23 21:46:06.246 
Epoch 603/1000 
	 loss: 17.4671, MinusLogProbMetric: 17.4671, val_loss: 17.5071, val_MinusLogProbMetric: 17.5071

Epoch 603: val_loss did not improve from 17.45422
196/196 - 64s - loss: 17.4671 - MinusLogProbMetric: 17.4671 - val_loss: 17.5071 - val_MinusLogProbMetric: 17.5071 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 604/1000
2023-10-23 21:47:09.133 
Epoch 604/1000 
	 loss: 17.4320, MinusLogProbMetric: 17.4320, val_loss: 17.5723, val_MinusLogProbMetric: 17.5723

Epoch 604: val_loss did not improve from 17.45422
196/196 - 63s - loss: 17.4320 - MinusLogProbMetric: 17.4320 - val_loss: 17.5723 - val_MinusLogProbMetric: 17.5723 - lr: 5.5556e-05 - 63s/epoch - 321ms/step
Epoch 605/1000
2023-10-23 21:48:16.720 
Epoch 605/1000 
	 loss: 17.3987, MinusLogProbMetric: 17.3987, val_loss: 18.0850, val_MinusLogProbMetric: 18.0850

Epoch 605: val_loss did not improve from 17.45422
196/196 - 68s - loss: 17.3987 - MinusLogProbMetric: 17.3987 - val_loss: 18.0850 - val_MinusLogProbMetric: 18.0850 - lr: 5.5556e-05 - 68s/epoch - 345ms/step
Epoch 606/1000
2023-10-23 21:49:15.609 
Epoch 606/1000 
	 loss: 17.4309, MinusLogProbMetric: 17.4309, val_loss: 17.6418, val_MinusLogProbMetric: 17.6418

Epoch 606: val_loss did not improve from 17.45422
196/196 - 59s - loss: 17.4309 - MinusLogProbMetric: 17.4309 - val_loss: 17.6418 - val_MinusLogProbMetric: 17.6418 - lr: 5.5556e-05 - 59s/epoch - 300ms/step
Epoch 607/1000
2023-10-23 21:50:16.897 
Epoch 607/1000 
	 loss: 17.4090, MinusLogProbMetric: 17.4090, val_loss: 17.5863, val_MinusLogProbMetric: 17.5863

Epoch 607: val_loss did not improve from 17.45422
196/196 - 61s - loss: 17.4090 - MinusLogProbMetric: 17.4090 - val_loss: 17.5863 - val_MinusLogProbMetric: 17.5863 - lr: 5.5556e-05 - 61s/epoch - 313ms/step
Epoch 608/1000
2023-10-23 21:51:19.661 
Epoch 608/1000 
	 loss: 17.3619, MinusLogProbMetric: 17.3619, val_loss: 17.5991, val_MinusLogProbMetric: 17.5991

Epoch 608: val_loss did not improve from 17.45422
196/196 - 63s - loss: 17.3619 - MinusLogProbMetric: 17.3619 - val_loss: 17.5991 - val_MinusLogProbMetric: 17.5991 - lr: 5.5556e-05 - 63s/epoch - 320ms/step
Epoch 609/1000
2023-10-23 21:52:25.875 
Epoch 609/1000 
	 loss: 17.4159, MinusLogProbMetric: 17.4159, val_loss: 17.5966, val_MinusLogProbMetric: 17.5966

Epoch 609: val_loss did not improve from 17.45422
196/196 - 66s - loss: 17.4159 - MinusLogProbMetric: 17.4159 - val_loss: 17.5966 - val_MinusLogProbMetric: 17.5966 - lr: 5.5556e-05 - 66s/epoch - 338ms/step
Epoch 610/1000
2023-10-23 21:53:29.686 
Epoch 610/1000 
	 loss: 17.4644, MinusLogProbMetric: 17.4644, val_loss: 17.4770, val_MinusLogProbMetric: 17.4770

Epoch 610: val_loss did not improve from 17.45422
196/196 - 64s - loss: 17.4644 - MinusLogProbMetric: 17.4644 - val_loss: 17.4770 - val_MinusLogProbMetric: 17.4770 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 611/1000
2023-10-23 21:54:31.996 
Epoch 611/1000 
	 loss: 17.3459, MinusLogProbMetric: 17.3459, val_loss: 17.5346, val_MinusLogProbMetric: 17.5346

Epoch 611: val_loss did not improve from 17.45422
196/196 - 62s - loss: 17.3459 - MinusLogProbMetric: 17.3459 - val_loss: 17.5346 - val_MinusLogProbMetric: 17.5346 - lr: 5.5556e-05 - 62s/epoch - 318ms/step
Epoch 612/1000
2023-10-23 21:55:31.196 
Epoch 612/1000 
	 loss: 17.4014, MinusLogProbMetric: 17.4014, val_loss: 17.5332, val_MinusLogProbMetric: 17.5332

Epoch 612: val_loss did not improve from 17.45422
196/196 - 59s - loss: 17.4014 - MinusLogProbMetric: 17.4014 - val_loss: 17.5332 - val_MinusLogProbMetric: 17.5332 - lr: 5.5556e-05 - 59s/epoch - 302ms/step
Epoch 613/1000
2023-10-23 21:56:28.385 
Epoch 613/1000 
	 loss: 17.3838, MinusLogProbMetric: 17.3838, val_loss: 17.6718, val_MinusLogProbMetric: 17.6718

Epoch 613: val_loss did not improve from 17.45422
196/196 - 57s - loss: 17.3838 - MinusLogProbMetric: 17.3838 - val_loss: 17.6718 - val_MinusLogProbMetric: 17.6718 - lr: 5.5556e-05 - 57s/epoch - 292ms/step
Epoch 614/1000
2023-10-23 21:57:28.019 
Epoch 614/1000 
	 loss: 17.3750, MinusLogProbMetric: 17.3750, val_loss: 17.5352, val_MinusLogProbMetric: 17.5352

Epoch 614: val_loss did not improve from 17.45422
196/196 - 60s - loss: 17.3750 - MinusLogProbMetric: 17.3750 - val_loss: 17.5352 - val_MinusLogProbMetric: 17.5352 - lr: 5.5556e-05 - 60s/epoch - 304ms/step
Epoch 615/1000
2023-10-23 21:58:23.768 
Epoch 615/1000 
	 loss: 17.4310, MinusLogProbMetric: 17.4310, val_loss: 17.7945, val_MinusLogProbMetric: 17.7945

Epoch 615: val_loss did not improve from 17.45422
196/196 - 56s - loss: 17.4310 - MinusLogProbMetric: 17.4310 - val_loss: 17.7945 - val_MinusLogProbMetric: 17.7945 - lr: 5.5556e-05 - 56s/epoch - 284ms/step
Epoch 616/1000
2023-10-23 21:59:27.964 
Epoch 616/1000 
	 loss: 17.4608, MinusLogProbMetric: 17.4608, val_loss: 17.4538, val_MinusLogProbMetric: 17.4538

Epoch 616: val_loss improved from 17.45422 to 17.45380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 65s - loss: 17.4608 - MinusLogProbMetric: 17.4608 - val_loss: 17.4538 - val_MinusLogProbMetric: 17.4538 - lr: 5.5556e-05 - 65s/epoch - 332ms/step
Epoch 617/1000
2023-10-23 22:00:32.251 
Epoch 617/1000 
	 loss: 17.3462, MinusLogProbMetric: 17.3462, val_loss: 17.4263, val_MinusLogProbMetric: 17.4263

Epoch 617: val_loss improved from 17.45380 to 17.42628, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 64s - loss: 17.3462 - MinusLogProbMetric: 17.3462 - val_loss: 17.4263 - val_MinusLogProbMetric: 17.4263 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 618/1000
2023-10-23 22:01:29.941 
Epoch 618/1000 
	 loss: 17.3846, MinusLogProbMetric: 17.3846, val_loss: 17.5542, val_MinusLogProbMetric: 17.5542

Epoch 618: val_loss did not improve from 17.42628
196/196 - 57s - loss: 17.3846 - MinusLogProbMetric: 17.3846 - val_loss: 17.5542 - val_MinusLogProbMetric: 17.5542 - lr: 5.5556e-05 - 57s/epoch - 291ms/step
Epoch 619/1000
2023-10-23 22:02:34.219 
Epoch 619/1000 
	 loss: 17.4455, MinusLogProbMetric: 17.4455, val_loss: 17.5369, val_MinusLogProbMetric: 17.5369

Epoch 619: val_loss did not improve from 17.42628
196/196 - 64s - loss: 17.4455 - MinusLogProbMetric: 17.4455 - val_loss: 17.5369 - val_MinusLogProbMetric: 17.5369 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 620/1000
2023-10-23 22:03:36.405 
Epoch 620/1000 
	 loss: 17.3275, MinusLogProbMetric: 17.3275, val_loss: 17.5782, val_MinusLogProbMetric: 17.5782

Epoch 620: val_loss did not improve from 17.42628
196/196 - 62s - loss: 17.3275 - MinusLogProbMetric: 17.3275 - val_loss: 17.5782 - val_MinusLogProbMetric: 17.5782 - lr: 5.5556e-05 - 62s/epoch - 317ms/step
Epoch 621/1000
2023-10-23 22:04:41.707 
Epoch 621/1000 
	 loss: 17.4018, MinusLogProbMetric: 17.4018, val_loss: 19.7537, val_MinusLogProbMetric: 19.7537

Epoch 621: val_loss did not improve from 17.42628
196/196 - 65s - loss: 17.4018 - MinusLogProbMetric: 17.4018 - val_loss: 19.7537 - val_MinusLogProbMetric: 19.7537 - lr: 5.5556e-05 - 65s/epoch - 333ms/step
Epoch 622/1000
2023-10-23 22:05:39.086 
Epoch 622/1000 
	 loss: 17.4824, MinusLogProbMetric: 17.4824, val_loss: 17.4556, val_MinusLogProbMetric: 17.4556

Epoch 622: val_loss did not improve from 17.42628
196/196 - 57s - loss: 17.4824 - MinusLogProbMetric: 17.4824 - val_loss: 17.4556 - val_MinusLogProbMetric: 17.4556 - lr: 5.5556e-05 - 57s/epoch - 293ms/step
Epoch 623/1000
2023-10-23 22:06:44.024 
Epoch 623/1000 
	 loss: 17.4008, MinusLogProbMetric: 17.4008, val_loss: 17.5315, val_MinusLogProbMetric: 17.5315

Epoch 623: val_loss did not improve from 17.42628
196/196 - 65s - loss: 17.4008 - MinusLogProbMetric: 17.4008 - val_loss: 17.5315 - val_MinusLogProbMetric: 17.5315 - lr: 5.5556e-05 - 65s/epoch - 331ms/step
Epoch 624/1000
2023-10-23 22:07:46.279 
Epoch 624/1000 
	 loss: 17.4080, MinusLogProbMetric: 17.4080, val_loss: 17.6305, val_MinusLogProbMetric: 17.6305

Epoch 624: val_loss did not improve from 17.42628
196/196 - 62s - loss: 17.4080 - MinusLogProbMetric: 17.4080 - val_loss: 17.6305 - val_MinusLogProbMetric: 17.6305 - lr: 5.5556e-05 - 62s/epoch - 318ms/step
Epoch 625/1000
2023-10-23 22:08:51.547 
Epoch 625/1000 
	 loss: 17.3782, MinusLogProbMetric: 17.3782, val_loss: 17.4815, val_MinusLogProbMetric: 17.4815

Epoch 625: val_loss did not improve from 17.42628
196/196 - 65s - loss: 17.3782 - MinusLogProbMetric: 17.3782 - val_loss: 17.4815 - val_MinusLogProbMetric: 17.4815 - lr: 5.5556e-05 - 65s/epoch - 333ms/step
Epoch 626/1000
2023-10-23 22:09:56.124 
Epoch 626/1000 
	 loss: 17.3324, MinusLogProbMetric: 17.3324, val_loss: 17.5514, val_MinusLogProbMetric: 17.5514

Epoch 626: val_loss did not improve from 17.42628
196/196 - 65s - loss: 17.3324 - MinusLogProbMetric: 17.3324 - val_loss: 17.5514 - val_MinusLogProbMetric: 17.5514 - lr: 5.5556e-05 - 65s/epoch - 329ms/step
Epoch 627/1000
2023-10-23 22:10:56.766 
Epoch 627/1000 
	 loss: 17.4905, MinusLogProbMetric: 17.4905, val_loss: 17.8608, val_MinusLogProbMetric: 17.8608

Epoch 627: val_loss did not improve from 17.42628
196/196 - 61s - loss: 17.4905 - MinusLogProbMetric: 17.4905 - val_loss: 17.8608 - val_MinusLogProbMetric: 17.8608 - lr: 5.5556e-05 - 61s/epoch - 309ms/step
Epoch 628/1000
2023-10-23 22:11:58.910 
Epoch 628/1000 
	 loss: 17.4380, MinusLogProbMetric: 17.4380, val_loss: 17.4904, val_MinusLogProbMetric: 17.4904

Epoch 628: val_loss did not improve from 17.42628
196/196 - 62s - loss: 17.4380 - MinusLogProbMetric: 17.4380 - val_loss: 17.4904 - val_MinusLogProbMetric: 17.4904 - lr: 5.5556e-05 - 62s/epoch - 317ms/step
Epoch 629/1000
2023-10-23 22:13:03.577 
Epoch 629/1000 
	 loss: 17.3823, MinusLogProbMetric: 17.3823, val_loss: 17.5591, val_MinusLogProbMetric: 17.5591

Epoch 629: val_loss did not improve from 17.42628
196/196 - 65s - loss: 17.3823 - MinusLogProbMetric: 17.3823 - val_loss: 17.5591 - val_MinusLogProbMetric: 17.5591 - lr: 5.5556e-05 - 65s/epoch - 330ms/step
Epoch 630/1000
2023-10-23 22:14:08.409 
Epoch 630/1000 
	 loss: 17.3690, MinusLogProbMetric: 17.3690, val_loss: 17.4688, val_MinusLogProbMetric: 17.4688

Epoch 630: val_loss did not improve from 17.42628
196/196 - 65s - loss: 17.3690 - MinusLogProbMetric: 17.3690 - val_loss: 17.4688 - val_MinusLogProbMetric: 17.4688 - lr: 5.5556e-05 - 65s/epoch - 331ms/step
Epoch 631/1000
2023-10-23 22:15:12.461 
Epoch 631/1000 
	 loss: 17.3413, MinusLogProbMetric: 17.3413, val_loss: 17.8372, val_MinusLogProbMetric: 17.8372

Epoch 631: val_loss did not improve from 17.42628
196/196 - 64s - loss: 17.3413 - MinusLogProbMetric: 17.3413 - val_loss: 17.8372 - val_MinusLogProbMetric: 17.8372 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 632/1000
2023-10-23 22:16:14.011 
Epoch 632/1000 
	 loss: 17.4253, MinusLogProbMetric: 17.4253, val_loss: 17.5762, val_MinusLogProbMetric: 17.5762

Epoch 632: val_loss did not improve from 17.42628
196/196 - 62s - loss: 17.4253 - MinusLogProbMetric: 17.4253 - val_loss: 17.5762 - val_MinusLogProbMetric: 17.5762 - lr: 5.5556e-05 - 62s/epoch - 314ms/step
Epoch 633/1000
2023-10-23 22:17:18.690 
Epoch 633/1000 
	 loss: 17.4616, MinusLogProbMetric: 17.4616, val_loss: 17.4508, val_MinusLogProbMetric: 17.4508

Epoch 633: val_loss did not improve from 17.42628
196/196 - 65s - loss: 17.4616 - MinusLogProbMetric: 17.4616 - val_loss: 17.4508 - val_MinusLogProbMetric: 17.4508 - lr: 5.5556e-05 - 65s/epoch - 330ms/step
Epoch 634/1000
2023-10-23 22:18:19.583 
Epoch 634/1000 
	 loss: 17.4515, MinusLogProbMetric: 17.4515, val_loss: 17.5902, val_MinusLogProbMetric: 17.5902

Epoch 634: val_loss did not improve from 17.42628
196/196 - 61s - loss: 17.4515 - MinusLogProbMetric: 17.4515 - val_loss: 17.5902 - val_MinusLogProbMetric: 17.5902 - lr: 5.5556e-05 - 61s/epoch - 311ms/step
Epoch 635/1000
2023-10-23 22:19:22.861 
Epoch 635/1000 
	 loss: 17.3909, MinusLogProbMetric: 17.3909, val_loss: 17.8263, val_MinusLogProbMetric: 17.8263

Epoch 635: val_loss did not improve from 17.42628
196/196 - 63s - loss: 17.3909 - MinusLogProbMetric: 17.3909 - val_loss: 17.8263 - val_MinusLogProbMetric: 17.8263 - lr: 5.5556e-05 - 63s/epoch - 323ms/step
Epoch 636/1000
2023-10-23 22:20:25.596 
Epoch 636/1000 
	 loss: 17.5155, MinusLogProbMetric: 17.5155, val_loss: 17.4971, val_MinusLogProbMetric: 17.4971

Epoch 636: val_loss did not improve from 17.42628
196/196 - 63s - loss: 17.5155 - MinusLogProbMetric: 17.5155 - val_loss: 17.4971 - val_MinusLogProbMetric: 17.4971 - lr: 5.5556e-05 - 63s/epoch - 320ms/step
Epoch 637/1000
2023-10-23 22:21:28.886 
Epoch 637/1000 
	 loss: 17.4032, MinusLogProbMetric: 17.4032, val_loss: 17.7670, val_MinusLogProbMetric: 17.7670

Epoch 637: val_loss did not improve from 17.42628
196/196 - 63s - loss: 17.4032 - MinusLogProbMetric: 17.4032 - val_loss: 17.7670 - val_MinusLogProbMetric: 17.7670 - lr: 5.5556e-05 - 63s/epoch - 323ms/step
Epoch 638/1000
2023-10-23 22:22:30.329 
Epoch 638/1000 
	 loss: 17.4078, MinusLogProbMetric: 17.4078, val_loss: 17.5985, val_MinusLogProbMetric: 17.5985

Epoch 638: val_loss did not improve from 17.42628
196/196 - 61s - loss: 17.4078 - MinusLogProbMetric: 17.4078 - val_loss: 17.5985 - val_MinusLogProbMetric: 17.5985 - lr: 5.5556e-05 - 61s/epoch - 313ms/step
Epoch 639/1000
2023-10-23 22:23:35.000 
Epoch 639/1000 
	 loss: 17.3871, MinusLogProbMetric: 17.3871, val_loss: 17.4505, val_MinusLogProbMetric: 17.4505

Epoch 639: val_loss did not improve from 17.42628
196/196 - 65s - loss: 17.3871 - MinusLogProbMetric: 17.3871 - val_loss: 17.4505 - val_MinusLogProbMetric: 17.4505 - lr: 5.5556e-05 - 65s/epoch - 330ms/step
Epoch 640/1000
2023-10-23 22:24:38.294 
Epoch 640/1000 
	 loss: 17.3917, MinusLogProbMetric: 17.3917, val_loss: 17.4584, val_MinusLogProbMetric: 17.4584

Epoch 640: val_loss did not improve from 17.42628
196/196 - 63s - loss: 17.3917 - MinusLogProbMetric: 17.3917 - val_loss: 17.4584 - val_MinusLogProbMetric: 17.4584 - lr: 5.5556e-05 - 63s/epoch - 323ms/step
Epoch 641/1000
2023-10-23 22:25:40.137 
Epoch 641/1000 
	 loss: 17.4560, MinusLogProbMetric: 17.4560, val_loss: 17.5157, val_MinusLogProbMetric: 17.5157

Epoch 641: val_loss did not improve from 17.42628
196/196 - 62s - loss: 17.4560 - MinusLogProbMetric: 17.4560 - val_loss: 17.5157 - val_MinusLogProbMetric: 17.5157 - lr: 5.5556e-05 - 62s/epoch - 316ms/step
Epoch 642/1000
2023-10-23 22:26:41.918 
Epoch 642/1000 
	 loss: 17.3872, MinusLogProbMetric: 17.3872, val_loss: 17.4453, val_MinusLogProbMetric: 17.4453

Epoch 642: val_loss did not improve from 17.42628
196/196 - 62s - loss: 17.3872 - MinusLogProbMetric: 17.3872 - val_loss: 17.4453 - val_MinusLogProbMetric: 17.4453 - lr: 5.5556e-05 - 62s/epoch - 315ms/step
Epoch 643/1000
2023-10-23 22:27:46.224 
Epoch 643/1000 
	 loss: 17.4125, MinusLogProbMetric: 17.4125, val_loss: 17.7879, val_MinusLogProbMetric: 17.7879

Epoch 643: val_loss did not improve from 17.42628
196/196 - 64s - loss: 17.4125 - MinusLogProbMetric: 17.4125 - val_loss: 17.7879 - val_MinusLogProbMetric: 17.7879 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 644/1000
2023-10-23 22:28:50.386 
Epoch 644/1000 
	 loss: 17.4902, MinusLogProbMetric: 17.4902, val_loss: 17.5659, val_MinusLogProbMetric: 17.5659

Epoch 644: val_loss did not improve from 17.42628
196/196 - 64s - loss: 17.4902 - MinusLogProbMetric: 17.4902 - val_loss: 17.5659 - val_MinusLogProbMetric: 17.5659 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 645/1000
2023-10-23 22:29:52.085 
Epoch 645/1000 
	 loss: 17.3387, MinusLogProbMetric: 17.3387, val_loss: 17.5619, val_MinusLogProbMetric: 17.5619

Epoch 645: val_loss did not improve from 17.42628
196/196 - 62s - loss: 17.3387 - MinusLogProbMetric: 17.3387 - val_loss: 17.5619 - val_MinusLogProbMetric: 17.5619 - lr: 5.5556e-05 - 62s/epoch - 315ms/step
Epoch 646/1000
2023-10-23 22:30:55.581 
Epoch 646/1000 
	 loss: 17.3731, MinusLogProbMetric: 17.3731, val_loss: 17.8982, val_MinusLogProbMetric: 17.8982

Epoch 646: val_loss did not improve from 17.42628
196/196 - 63s - loss: 17.3731 - MinusLogProbMetric: 17.3731 - val_loss: 17.8982 - val_MinusLogProbMetric: 17.8982 - lr: 5.5556e-05 - 63s/epoch - 324ms/step
Epoch 647/1000
2023-10-23 22:31:58.817 
Epoch 647/1000 
	 loss: 17.4270, MinusLogProbMetric: 17.4270, val_loss: 17.6272, val_MinusLogProbMetric: 17.6272

Epoch 647: val_loss did not improve from 17.42628
196/196 - 63s - loss: 17.4270 - MinusLogProbMetric: 17.4270 - val_loss: 17.6272 - val_MinusLogProbMetric: 17.6272 - lr: 5.5556e-05 - 63s/epoch - 323ms/step
Epoch 648/1000
2023-10-23 22:32:59.855 
Epoch 648/1000 
	 loss: 17.3224, MinusLogProbMetric: 17.3224, val_loss: 17.4852, val_MinusLogProbMetric: 17.4852

Epoch 648: val_loss did not improve from 17.42628
196/196 - 61s - loss: 17.3224 - MinusLogProbMetric: 17.3224 - val_loss: 17.4852 - val_MinusLogProbMetric: 17.4852 - lr: 5.5556e-05 - 61s/epoch - 311ms/step
Epoch 649/1000
2023-10-23 22:34:01.236 
Epoch 649/1000 
	 loss: 17.4562, MinusLogProbMetric: 17.4562, val_loss: 17.8106, val_MinusLogProbMetric: 17.8106

Epoch 649: val_loss did not improve from 17.42628
196/196 - 61s - loss: 17.4562 - MinusLogProbMetric: 17.4562 - val_loss: 17.8106 - val_MinusLogProbMetric: 17.8106 - lr: 5.5556e-05 - 61s/epoch - 313ms/step
Epoch 650/1000
2023-10-23 22:35:03.987 
Epoch 650/1000 
	 loss: 17.3829, MinusLogProbMetric: 17.3829, val_loss: 17.4309, val_MinusLogProbMetric: 17.4309

Epoch 650: val_loss did not improve from 17.42628
196/196 - 63s - loss: 17.3829 - MinusLogProbMetric: 17.3829 - val_loss: 17.4309 - val_MinusLogProbMetric: 17.4309 - lr: 5.5556e-05 - 63s/epoch - 320ms/step
Epoch 651/1000
2023-10-23 22:36:06.185 
Epoch 651/1000 
	 loss: 17.3224, MinusLogProbMetric: 17.3224, val_loss: 17.6110, val_MinusLogProbMetric: 17.6110

Epoch 651: val_loss did not improve from 17.42628
196/196 - 62s - loss: 17.3224 - MinusLogProbMetric: 17.3224 - val_loss: 17.6110 - val_MinusLogProbMetric: 17.6110 - lr: 5.5556e-05 - 62s/epoch - 317ms/step
Epoch 652/1000
2023-10-23 22:37:05.067 
Epoch 652/1000 
	 loss: 17.4680, MinusLogProbMetric: 17.4680, val_loss: 17.5653, val_MinusLogProbMetric: 17.5653

Epoch 652: val_loss did not improve from 17.42628
196/196 - 59s - loss: 17.4680 - MinusLogProbMetric: 17.4680 - val_loss: 17.5653 - val_MinusLogProbMetric: 17.5653 - lr: 5.5556e-05 - 59s/epoch - 300ms/step
Epoch 653/1000
2023-10-23 22:38:06.873 
Epoch 653/1000 
	 loss: 17.3699, MinusLogProbMetric: 17.3699, val_loss: 17.6222, val_MinusLogProbMetric: 17.6222

Epoch 653: val_loss did not improve from 17.42628
196/196 - 62s - loss: 17.3699 - MinusLogProbMetric: 17.3699 - val_loss: 17.6222 - val_MinusLogProbMetric: 17.6222 - lr: 5.5556e-05 - 62s/epoch - 315ms/step
Epoch 654/1000
2023-10-23 22:39:09.170 
Epoch 654/1000 
	 loss: 17.4510, MinusLogProbMetric: 17.4510, val_loss: 17.8676, val_MinusLogProbMetric: 17.8676

Epoch 654: val_loss did not improve from 17.42628
196/196 - 62s - loss: 17.4510 - MinusLogProbMetric: 17.4510 - val_loss: 17.8676 - val_MinusLogProbMetric: 17.8676 - lr: 5.5556e-05 - 62s/epoch - 318ms/step
Epoch 655/1000
2023-10-23 22:40:09.742 
Epoch 655/1000 
	 loss: 17.5661, MinusLogProbMetric: 17.5661, val_loss: 17.6152, val_MinusLogProbMetric: 17.6152

Epoch 655: val_loss did not improve from 17.42628
196/196 - 61s - loss: 17.5661 - MinusLogProbMetric: 17.5661 - val_loss: 17.6152 - val_MinusLogProbMetric: 17.6152 - lr: 5.5556e-05 - 61s/epoch - 309ms/step
Epoch 656/1000
2023-10-23 22:41:11.331 
Epoch 656/1000 
	 loss: 17.4068, MinusLogProbMetric: 17.4068, val_loss: 17.4749, val_MinusLogProbMetric: 17.4749

Epoch 656: val_loss did not improve from 17.42628
196/196 - 62s - loss: 17.4068 - MinusLogProbMetric: 17.4068 - val_loss: 17.4749 - val_MinusLogProbMetric: 17.4749 - lr: 5.5556e-05 - 62s/epoch - 314ms/step
Epoch 657/1000
2023-10-23 22:42:14.986 
Epoch 657/1000 
	 loss: 17.3159, MinusLogProbMetric: 17.3159, val_loss: 17.4059, val_MinusLogProbMetric: 17.4059

Epoch 657: val_loss improved from 17.42628 to 17.40587, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 65s - loss: 17.3159 - MinusLogProbMetric: 17.3159 - val_loss: 17.4059 - val_MinusLogProbMetric: 17.4059 - lr: 5.5556e-05 - 65s/epoch - 329ms/step
Epoch 658/1000
2023-10-23 22:43:16.219 
Epoch 658/1000 
	 loss: 17.5103, MinusLogProbMetric: 17.5103, val_loss: 17.6001, val_MinusLogProbMetric: 17.6001

Epoch 658: val_loss did not improve from 17.40587
196/196 - 60s - loss: 17.5103 - MinusLogProbMetric: 17.5103 - val_loss: 17.6001 - val_MinusLogProbMetric: 17.6001 - lr: 5.5556e-05 - 60s/epoch - 308ms/step
Epoch 659/1000
2023-10-23 22:44:18.744 
Epoch 659/1000 
	 loss: 17.4262, MinusLogProbMetric: 17.4262, val_loss: 17.7169, val_MinusLogProbMetric: 17.7169

Epoch 659: val_loss did not improve from 17.40587
196/196 - 63s - loss: 17.4262 - MinusLogProbMetric: 17.4262 - val_loss: 17.7169 - val_MinusLogProbMetric: 17.7169 - lr: 5.5556e-05 - 63s/epoch - 319ms/step
Epoch 660/1000
2023-10-23 22:45:17.401 
Epoch 660/1000 
	 loss: 17.3353, MinusLogProbMetric: 17.3353, val_loss: 17.5200, val_MinusLogProbMetric: 17.5200

Epoch 660: val_loss did not improve from 17.40587
196/196 - 59s - loss: 17.3353 - MinusLogProbMetric: 17.3353 - val_loss: 17.5200 - val_MinusLogProbMetric: 17.5200 - lr: 5.5556e-05 - 59s/epoch - 299ms/step
Epoch 661/1000
2023-10-23 22:46:24.483 
Epoch 661/1000 
	 loss: 17.4654, MinusLogProbMetric: 17.4654, val_loss: 17.4557, val_MinusLogProbMetric: 17.4557

Epoch 661: val_loss did not improve from 17.40587
196/196 - 67s - loss: 17.4654 - MinusLogProbMetric: 17.4654 - val_loss: 17.4557 - val_MinusLogProbMetric: 17.4557 - lr: 5.5556e-05 - 67s/epoch - 342ms/step
Epoch 662/1000
2023-10-23 22:47:27.333 
Epoch 662/1000 
	 loss: 17.3112, MinusLogProbMetric: 17.3112, val_loss: 17.4473, val_MinusLogProbMetric: 17.4473

Epoch 662: val_loss did not improve from 17.40587
196/196 - 63s - loss: 17.3112 - MinusLogProbMetric: 17.3112 - val_loss: 17.4473 - val_MinusLogProbMetric: 17.4473 - lr: 5.5556e-05 - 63s/epoch - 321ms/step
Epoch 663/1000
2023-10-23 22:48:30.148 
Epoch 663/1000 
	 loss: 17.3768, MinusLogProbMetric: 17.3768, val_loss: 17.4572, val_MinusLogProbMetric: 17.4572

Epoch 663: val_loss did not improve from 17.40587
196/196 - 63s - loss: 17.3768 - MinusLogProbMetric: 17.3768 - val_loss: 17.4572 - val_MinusLogProbMetric: 17.4572 - lr: 5.5556e-05 - 63s/epoch - 320ms/step
Epoch 664/1000
2023-10-23 22:49:33.640 
Epoch 664/1000 
	 loss: 17.3842, MinusLogProbMetric: 17.3842, val_loss: 17.5097, val_MinusLogProbMetric: 17.5097

Epoch 664: val_loss did not improve from 17.40587
196/196 - 63s - loss: 17.3842 - MinusLogProbMetric: 17.3842 - val_loss: 17.5097 - val_MinusLogProbMetric: 17.5097 - lr: 5.5556e-05 - 63s/epoch - 324ms/step
Epoch 665/1000
2023-10-23 22:50:37.629 
Epoch 665/1000 
	 loss: 17.3669, MinusLogProbMetric: 17.3669, val_loss: 17.4975, val_MinusLogProbMetric: 17.4975

Epoch 665: val_loss did not improve from 17.40587
196/196 - 64s - loss: 17.3669 - MinusLogProbMetric: 17.3669 - val_loss: 17.4975 - val_MinusLogProbMetric: 17.4975 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 666/1000
2023-10-23 22:51:42.572 
Epoch 666/1000 
	 loss: 17.4356, MinusLogProbMetric: 17.4356, val_loss: 17.4666, val_MinusLogProbMetric: 17.4666

Epoch 666: val_loss did not improve from 17.40587
196/196 - 65s - loss: 17.4356 - MinusLogProbMetric: 17.4356 - val_loss: 17.4666 - val_MinusLogProbMetric: 17.4666 - lr: 5.5556e-05 - 65s/epoch - 331ms/step
Epoch 667/1000
2023-10-23 22:52:46.578 
Epoch 667/1000 
	 loss: 17.3567, MinusLogProbMetric: 17.3567, val_loss: 17.4394, val_MinusLogProbMetric: 17.4394

Epoch 667: val_loss did not improve from 17.40587
196/196 - 64s - loss: 17.3567 - MinusLogProbMetric: 17.3567 - val_loss: 17.4394 - val_MinusLogProbMetric: 17.4394 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 668/1000
2023-10-23 22:53:52.760 
Epoch 668/1000 
	 loss: 17.3164, MinusLogProbMetric: 17.3164, val_loss: 17.4848, val_MinusLogProbMetric: 17.4848

Epoch 668: val_loss did not improve from 17.40587
196/196 - 66s - loss: 17.3164 - MinusLogProbMetric: 17.3164 - val_loss: 17.4848 - val_MinusLogProbMetric: 17.4848 - lr: 5.5556e-05 - 66s/epoch - 338ms/step
Epoch 669/1000
2023-10-23 22:54:56.949 
Epoch 669/1000 
	 loss: 17.4322, MinusLogProbMetric: 17.4322, val_loss: 17.4971, val_MinusLogProbMetric: 17.4971

Epoch 669: val_loss did not improve from 17.40587
196/196 - 64s - loss: 17.4322 - MinusLogProbMetric: 17.4322 - val_loss: 17.4971 - val_MinusLogProbMetric: 17.4971 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 670/1000
2023-10-23 22:55:58.272 
Epoch 670/1000 
	 loss: 17.3745, MinusLogProbMetric: 17.3745, val_loss: 17.4564, val_MinusLogProbMetric: 17.4564

Epoch 670: val_loss did not improve from 17.40587
196/196 - 61s - loss: 17.3745 - MinusLogProbMetric: 17.3745 - val_loss: 17.4564 - val_MinusLogProbMetric: 17.4564 - lr: 5.5556e-05 - 61s/epoch - 313ms/step
Epoch 671/1000
2023-10-23 22:57:02.317 
Epoch 671/1000 
	 loss: 17.3448, MinusLogProbMetric: 17.3448, val_loss: 17.4979, val_MinusLogProbMetric: 17.4979

Epoch 671: val_loss did not improve from 17.40587
196/196 - 64s - loss: 17.3448 - MinusLogProbMetric: 17.3448 - val_loss: 17.4979 - val_MinusLogProbMetric: 17.4979 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 672/1000
2023-10-23 22:58:09.718 
Epoch 672/1000 
	 loss: 17.4301, MinusLogProbMetric: 17.4301, val_loss: 17.5105, val_MinusLogProbMetric: 17.5105

Epoch 672: val_loss did not improve from 17.40587
196/196 - 67s - loss: 17.4301 - MinusLogProbMetric: 17.4301 - val_loss: 17.5105 - val_MinusLogProbMetric: 17.5105 - lr: 5.5556e-05 - 67s/epoch - 344ms/step
Epoch 673/1000
2023-10-23 22:59:17.422 
Epoch 673/1000 
	 loss: 17.3919, MinusLogProbMetric: 17.3919, val_loss: 17.5697, val_MinusLogProbMetric: 17.5697

Epoch 673: val_loss did not improve from 17.40587
196/196 - 68s - loss: 17.3919 - MinusLogProbMetric: 17.3919 - val_loss: 17.5697 - val_MinusLogProbMetric: 17.5697 - lr: 5.5556e-05 - 68s/epoch - 345ms/step
Epoch 674/1000
2023-10-23 23:00:24.937 
Epoch 674/1000 
	 loss: 17.3541, MinusLogProbMetric: 17.3541, val_loss: 17.5010, val_MinusLogProbMetric: 17.5010

Epoch 674: val_loss did not improve from 17.40587
196/196 - 68s - loss: 17.3541 - MinusLogProbMetric: 17.3541 - val_loss: 17.5010 - val_MinusLogProbMetric: 17.5010 - lr: 5.5556e-05 - 68s/epoch - 344ms/step
Epoch 675/1000
2023-10-23 23:01:28.532 
Epoch 675/1000 
	 loss: 17.4009, MinusLogProbMetric: 17.4009, val_loss: 17.4838, val_MinusLogProbMetric: 17.4838

Epoch 675: val_loss did not improve from 17.40587
196/196 - 64s - loss: 17.4009 - MinusLogProbMetric: 17.4009 - val_loss: 17.4838 - val_MinusLogProbMetric: 17.4838 - lr: 5.5556e-05 - 64s/epoch - 324ms/step
Epoch 676/1000
2023-10-23 23:02:31.921 
Epoch 676/1000 
	 loss: 17.3077, MinusLogProbMetric: 17.3077, val_loss: 17.4556, val_MinusLogProbMetric: 17.4556

Epoch 676: val_loss did not improve from 17.40587
196/196 - 63s - loss: 17.3077 - MinusLogProbMetric: 17.3077 - val_loss: 17.4556 - val_MinusLogProbMetric: 17.4556 - lr: 5.5556e-05 - 63s/epoch - 323ms/step
Epoch 677/1000
2023-10-23 23:03:35.312 
Epoch 677/1000 
	 loss: 17.4046, MinusLogProbMetric: 17.4046, val_loss: 17.6382, val_MinusLogProbMetric: 17.6382

Epoch 677: val_loss did not improve from 17.40587
196/196 - 63s - loss: 17.4046 - MinusLogProbMetric: 17.4046 - val_loss: 17.6382 - val_MinusLogProbMetric: 17.6382 - lr: 5.5556e-05 - 63s/epoch - 323ms/step
Epoch 678/1000
2023-10-23 23:04:39.056 
Epoch 678/1000 
	 loss: 17.3532, MinusLogProbMetric: 17.3532, val_loss: 17.4459, val_MinusLogProbMetric: 17.4459

Epoch 678: val_loss did not improve from 17.40587
196/196 - 64s - loss: 17.3532 - MinusLogProbMetric: 17.3532 - val_loss: 17.4459 - val_MinusLogProbMetric: 17.4459 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 679/1000
2023-10-23 23:05:45.548 
Epoch 679/1000 
	 loss: 17.3370, MinusLogProbMetric: 17.3370, val_loss: 17.5265, val_MinusLogProbMetric: 17.5265

Epoch 679: val_loss did not improve from 17.40587
196/196 - 66s - loss: 17.3370 - MinusLogProbMetric: 17.3370 - val_loss: 17.5265 - val_MinusLogProbMetric: 17.5265 - lr: 5.5556e-05 - 66s/epoch - 339ms/step
Epoch 680/1000
2023-10-23 23:06:48.317 
Epoch 680/1000 
	 loss: 17.5656, MinusLogProbMetric: 17.5656, val_loss: 17.5890, val_MinusLogProbMetric: 17.5890

Epoch 680: val_loss did not improve from 17.40587
196/196 - 63s - loss: 17.5656 - MinusLogProbMetric: 17.5656 - val_loss: 17.5890 - val_MinusLogProbMetric: 17.5890 - lr: 5.5556e-05 - 63s/epoch - 320ms/step
Epoch 681/1000
2023-10-23 23:07:49.706 
Epoch 681/1000 
	 loss: 17.3658, MinusLogProbMetric: 17.3658, val_loss: 17.6472, val_MinusLogProbMetric: 17.6472

Epoch 681: val_loss did not improve from 17.40587
196/196 - 61s - loss: 17.3658 - MinusLogProbMetric: 17.3658 - val_loss: 17.6472 - val_MinusLogProbMetric: 17.6472 - lr: 5.5556e-05 - 61s/epoch - 313ms/step
Epoch 682/1000
2023-10-23 23:08:48.570 
Epoch 682/1000 
	 loss: 17.3135, MinusLogProbMetric: 17.3135, val_loss: 18.0599, val_MinusLogProbMetric: 18.0599

Epoch 682: val_loss did not improve from 17.40587
196/196 - 59s - loss: 17.3135 - MinusLogProbMetric: 17.3135 - val_loss: 18.0599 - val_MinusLogProbMetric: 18.0599 - lr: 5.5556e-05 - 59s/epoch - 300ms/step
Epoch 683/1000
2023-10-23 23:09:47.728 
Epoch 683/1000 
	 loss: 17.3896, MinusLogProbMetric: 17.3896, val_loss: 17.8115, val_MinusLogProbMetric: 17.8115

Epoch 683: val_loss did not improve from 17.40587
196/196 - 59s - loss: 17.3896 - MinusLogProbMetric: 17.3896 - val_loss: 17.8115 - val_MinusLogProbMetric: 17.8115 - lr: 5.5556e-05 - 59s/epoch - 302ms/step
Epoch 684/1000
2023-10-23 23:10:54.909 
Epoch 684/1000 
	 loss: 17.3515, MinusLogProbMetric: 17.3515, val_loss: 17.4945, val_MinusLogProbMetric: 17.4945

Epoch 684: val_loss did not improve from 17.40587
196/196 - 67s - loss: 17.3515 - MinusLogProbMetric: 17.3515 - val_loss: 17.4945 - val_MinusLogProbMetric: 17.4945 - lr: 5.5556e-05 - 67s/epoch - 343ms/step
Epoch 685/1000
2023-10-23 23:11:55.266 
Epoch 685/1000 
	 loss: 17.3486, MinusLogProbMetric: 17.3486, val_loss: 17.6269, val_MinusLogProbMetric: 17.6269

Epoch 685: val_loss did not improve from 17.40587
196/196 - 60s - loss: 17.3486 - MinusLogProbMetric: 17.3486 - val_loss: 17.6269 - val_MinusLogProbMetric: 17.6269 - lr: 5.5556e-05 - 60s/epoch - 308ms/step
Epoch 686/1000
2023-10-23 23:12:56.012 
Epoch 686/1000 
	 loss: 17.3956, MinusLogProbMetric: 17.3956, val_loss: 17.4508, val_MinusLogProbMetric: 17.4508

Epoch 686: val_loss did not improve from 17.40587
196/196 - 61s - loss: 17.3956 - MinusLogProbMetric: 17.3956 - val_loss: 17.4508 - val_MinusLogProbMetric: 17.4508 - lr: 5.5556e-05 - 61s/epoch - 310ms/step
Epoch 687/1000
2023-10-23 23:13:57.978 
Epoch 687/1000 
	 loss: 17.3435, MinusLogProbMetric: 17.3435, val_loss: 17.5893, val_MinusLogProbMetric: 17.5893

Epoch 687: val_loss did not improve from 17.40587
196/196 - 62s - loss: 17.3435 - MinusLogProbMetric: 17.3435 - val_loss: 17.5893 - val_MinusLogProbMetric: 17.5893 - lr: 5.5556e-05 - 62s/epoch - 316ms/step
Epoch 688/1000
2023-10-23 23:14:59.451 
Epoch 688/1000 
	 loss: 17.4133, MinusLogProbMetric: 17.4133, val_loss: 17.4727, val_MinusLogProbMetric: 17.4727

Epoch 688: val_loss did not improve from 17.40587
196/196 - 61s - loss: 17.4133 - MinusLogProbMetric: 17.4133 - val_loss: 17.4727 - val_MinusLogProbMetric: 17.4727 - lr: 5.5556e-05 - 61s/epoch - 314ms/step
Epoch 689/1000
2023-10-23 23:16:00.552 
Epoch 689/1000 
	 loss: 17.3172, MinusLogProbMetric: 17.3172, val_loss: 17.4606, val_MinusLogProbMetric: 17.4606

Epoch 689: val_loss did not improve from 17.40587
196/196 - 61s - loss: 17.3172 - MinusLogProbMetric: 17.3172 - val_loss: 17.4606 - val_MinusLogProbMetric: 17.4606 - lr: 5.5556e-05 - 61s/epoch - 312ms/step
Epoch 690/1000
2023-10-23 23:17:04.329 
Epoch 690/1000 
	 loss: 17.3920, MinusLogProbMetric: 17.3920, val_loss: 17.4758, val_MinusLogProbMetric: 17.4758

Epoch 690: val_loss did not improve from 17.40587
196/196 - 64s - loss: 17.3920 - MinusLogProbMetric: 17.3920 - val_loss: 17.4758 - val_MinusLogProbMetric: 17.4758 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 691/1000
2023-10-23 23:18:03.762 
Epoch 691/1000 
	 loss: 17.3978, MinusLogProbMetric: 17.3978, val_loss: 17.5207, val_MinusLogProbMetric: 17.5207

Epoch 691: val_loss did not improve from 17.40587
196/196 - 59s - loss: 17.3978 - MinusLogProbMetric: 17.3978 - val_loss: 17.5207 - val_MinusLogProbMetric: 17.5207 - lr: 5.5556e-05 - 59s/epoch - 303ms/step
Epoch 692/1000
2023-10-23 23:19:06.135 
Epoch 692/1000 
	 loss: 17.3722, MinusLogProbMetric: 17.3722, val_loss: 17.4259, val_MinusLogProbMetric: 17.4259

Epoch 692: val_loss did not improve from 17.40587
196/196 - 62s - loss: 17.3722 - MinusLogProbMetric: 17.3722 - val_loss: 17.4259 - val_MinusLogProbMetric: 17.4259 - lr: 5.5556e-05 - 62s/epoch - 318ms/step
Epoch 693/1000
2023-10-23 23:20:08.976 
Epoch 693/1000 
	 loss: 17.4321, MinusLogProbMetric: 17.4321, val_loss: 17.4692, val_MinusLogProbMetric: 17.4692

Epoch 693: val_loss did not improve from 17.40587
196/196 - 63s - loss: 17.4321 - MinusLogProbMetric: 17.4321 - val_loss: 17.4692 - val_MinusLogProbMetric: 17.4692 - lr: 5.5556e-05 - 63s/epoch - 321ms/step
Epoch 694/1000
2023-10-23 23:21:13.449 
Epoch 694/1000 
	 loss: 17.5574, MinusLogProbMetric: 17.5574, val_loss: 17.4674, val_MinusLogProbMetric: 17.4674

Epoch 694: val_loss did not improve from 17.40587
196/196 - 64s - loss: 17.5574 - MinusLogProbMetric: 17.5574 - val_loss: 17.4674 - val_MinusLogProbMetric: 17.4674 - lr: 5.5556e-05 - 64s/epoch - 329ms/step
Epoch 695/1000
2023-10-23 23:22:15.449 
Epoch 695/1000 
	 loss: 17.3180, MinusLogProbMetric: 17.3180, val_loss: 17.4400, val_MinusLogProbMetric: 17.4400

Epoch 695: val_loss did not improve from 17.40587
196/196 - 62s - loss: 17.3180 - MinusLogProbMetric: 17.3180 - val_loss: 17.4400 - val_MinusLogProbMetric: 17.4400 - lr: 5.5556e-05 - 62s/epoch - 316ms/step
Epoch 696/1000
2023-10-23 23:23:12.527 
Epoch 696/1000 
	 loss: 17.3408, MinusLogProbMetric: 17.3408, val_loss: 17.8049, val_MinusLogProbMetric: 17.8049

Epoch 696: val_loss did not improve from 17.40587
196/196 - 57s - loss: 17.3408 - MinusLogProbMetric: 17.3408 - val_loss: 17.8049 - val_MinusLogProbMetric: 17.8049 - lr: 5.5556e-05 - 57s/epoch - 291ms/step
Epoch 697/1000
2023-10-23 23:24:13.637 
Epoch 697/1000 
	 loss: 17.3096, MinusLogProbMetric: 17.3096, val_loss: 17.6417, val_MinusLogProbMetric: 17.6417

Epoch 697: val_loss did not improve from 17.40587
196/196 - 61s - loss: 17.3096 - MinusLogProbMetric: 17.3096 - val_loss: 17.6417 - val_MinusLogProbMetric: 17.6417 - lr: 5.5556e-05 - 61s/epoch - 312ms/step
Epoch 698/1000
2023-10-23 23:25:18.429 
Epoch 698/1000 
	 loss: 17.5699, MinusLogProbMetric: 17.5699, val_loss: 17.4101, val_MinusLogProbMetric: 17.4101

Epoch 698: val_loss did not improve from 17.40587
196/196 - 65s - loss: 17.5699 - MinusLogProbMetric: 17.5699 - val_loss: 17.4101 - val_MinusLogProbMetric: 17.4101 - lr: 5.5556e-05 - 65s/epoch - 331ms/step
Epoch 699/1000
2023-10-23 23:26:20.692 
Epoch 699/1000 
	 loss: 17.3958, MinusLogProbMetric: 17.3958, val_loss: 17.5304, val_MinusLogProbMetric: 17.5304

Epoch 699: val_loss did not improve from 17.40587
196/196 - 62s - loss: 17.3958 - MinusLogProbMetric: 17.3958 - val_loss: 17.5304 - val_MinusLogProbMetric: 17.5304 - lr: 5.5556e-05 - 62s/epoch - 318ms/step
Epoch 700/1000
2023-10-23 23:27:23.600 
Epoch 700/1000 
	 loss: 17.4152, MinusLogProbMetric: 17.4152, val_loss: 17.4391, val_MinusLogProbMetric: 17.4391

Epoch 700: val_loss did not improve from 17.40587
196/196 - 63s - loss: 17.4152 - MinusLogProbMetric: 17.4152 - val_loss: 17.4391 - val_MinusLogProbMetric: 17.4391 - lr: 5.5556e-05 - 63s/epoch - 321ms/step
Epoch 701/1000
2023-10-23 23:28:29.146 
Epoch 701/1000 
	 loss: 17.4230, MinusLogProbMetric: 17.4230, val_loss: 17.4619, val_MinusLogProbMetric: 17.4619

Epoch 701: val_loss did not improve from 17.40587
196/196 - 66s - loss: 17.4230 - MinusLogProbMetric: 17.4230 - val_loss: 17.4619 - val_MinusLogProbMetric: 17.4619 - lr: 5.5556e-05 - 66s/epoch - 334ms/step
Epoch 702/1000
2023-10-23 23:29:27.342 
Epoch 702/1000 
	 loss: 17.3732, MinusLogProbMetric: 17.3732, val_loss: 17.4943, val_MinusLogProbMetric: 17.4943

Epoch 702: val_loss did not improve from 17.40587
196/196 - 58s - loss: 17.3732 - MinusLogProbMetric: 17.3732 - val_loss: 17.4943 - val_MinusLogProbMetric: 17.4943 - lr: 5.5556e-05 - 58s/epoch - 297ms/step
Epoch 703/1000
2023-10-23 23:30:27.255 
Epoch 703/1000 
	 loss: 17.3219, MinusLogProbMetric: 17.3219, val_loss: 17.7399, val_MinusLogProbMetric: 17.7399

Epoch 703: val_loss did not improve from 17.40587
196/196 - 60s - loss: 17.3219 - MinusLogProbMetric: 17.3219 - val_loss: 17.7399 - val_MinusLogProbMetric: 17.7399 - lr: 5.5556e-05 - 60s/epoch - 306ms/step
Epoch 704/1000
2023-10-23 23:31:32.613 
Epoch 704/1000 
	 loss: 17.4302, MinusLogProbMetric: 17.4302, val_loss: 17.5148, val_MinusLogProbMetric: 17.5148

Epoch 704: val_loss did not improve from 17.40587
196/196 - 65s - loss: 17.4302 - MinusLogProbMetric: 17.4302 - val_loss: 17.5148 - val_MinusLogProbMetric: 17.5148 - lr: 5.5556e-05 - 65s/epoch - 333ms/step
Epoch 705/1000
2023-10-23 23:32:35.096 
Epoch 705/1000 
	 loss: 17.3615, MinusLogProbMetric: 17.3615, val_loss: 17.5882, val_MinusLogProbMetric: 17.5882

Epoch 705: val_loss did not improve from 17.40587
196/196 - 62s - loss: 17.3615 - MinusLogProbMetric: 17.3615 - val_loss: 17.5882 - val_MinusLogProbMetric: 17.5882 - lr: 5.5556e-05 - 62s/epoch - 319ms/step
Epoch 706/1000
2023-10-23 23:33:38.416 
Epoch 706/1000 
	 loss: 17.3541, MinusLogProbMetric: 17.3541, val_loss: 17.6002, val_MinusLogProbMetric: 17.6002

Epoch 706: val_loss did not improve from 17.40587
196/196 - 63s - loss: 17.3541 - MinusLogProbMetric: 17.3541 - val_loss: 17.6002 - val_MinusLogProbMetric: 17.6002 - lr: 5.5556e-05 - 63s/epoch - 323ms/step
Epoch 707/1000
2023-10-23 23:34:41.845 
Epoch 707/1000 
	 loss: 17.3248, MinusLogProbMetric: 17.3248, val_loss: 17.6299, val_MinusLogProbMetric: 17.6299

Epoch 707: val_loss did not improve from 17.40587
196/196 - 63s - loss: 17.3248 - MinusLogProbMetric: 17.3248 - val_loss: 17.6299 - val_MinusLogProbMetric: 17.6299 - lr: 5.5556e-05 - 63s/epoch - 324ms/step
Epoch 708/1000
2023-10-23 23:35:49.090 
Epoch 708/1000 
	 loss: 17.2020, MinusLogProbMetric: 17.2020, val_loss: 17.3900, val_MinusLogProbMetric: 17.3900

Epoch 708: val_loss improved from 17.40587 to 17.39003, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 68s - loss: 17.2020 - MinusLogProbMetric: 17.2020 - val_loss: 17.3900 - val_MinusLogProbMetric: 17.3900 - lr: 2.7778e-05 - 68s/epoch - 347ms/step
Epoch 709/1000
2023-10-23 23:36:52.792 
Epoch 709/1000 
	 loss: 17.1907, MinusLogProbMetric: 17.1907, val_loss: 17.6900, val_MinusLogProbMetric: 17.6900

Epoch 709: val_loss did not improve from 17.39003
196/196 - 63s - loss: 17.1907 - MinusLogProbMetric: 17.1907 - val_loss: 17.6900 - val_MinusLogProbMetric: 17.6900 - lr: 2.7778e-05 - 63s/epoch - 321ms/step
Epoch 710/1000
2023-10-23 23:37:53.474 
Epoch 710/1000 
	 loss: 17.2729, MinusLogProbMetric: 17.2729, val_loss: 17.3718, val_MinusLogProbMetric: 17.3718

Epoch 710: val_loss improved from 17.39003 to 17.37177, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 61s - loss: 17.2729 - MinusLogProbMetric: 17.2729 - val_loss: 17.3718 - val_MinusLogProbMetric: 17.3718 - lr: 2.7778e-05 - 61s/epoch - 314ms/step
Epoch 711/1000
2023-10-23 23:38:54.337 
Epoch 711/1000 
	 loss: 17.2120, MinusLogProbMetric: 17.2120, val_loss: 17.4047, val_MinusLogProbMetric: 17.4047

Epoch 711: val_loss did not improve from 17.37177
196/196 - 60s - loss: 17.2120 - MinusLogProbMetric: 17.2120 - val_loss: 17.4047 - val_MinusLogProbMetric: 17.4047 - lr: 2.7778e-05 - 60s/epoch - 306ms/step
Epoch 712/1000
2023-10-23 23:39:52.305 
Epoch 712/1000 
	 loss: 17.2235, MinusLogProbMetric: 17.2235, val_loss: 17.4987, val_MinusLogProbMetric: 17.4987

Epoch 712: val_loss did not improve from 17.37177
196/196 - 58s - loss: 17.2235 - MinusLogProbMetric: 17.2235 - val_loss: 17.4987 - val_MinusLogProbMetric: 17.4987 - lr: 2.7778e-05 - 58s/epoch - 296ms/step
Epoch 713/1000
2023-10-23 23:40:51.031 
Epoch 713/1000 
	 loss: 17.2578, MinusLogProbMetric: 17.2578, val_loss: 17.3399, val_MinusLogProbMetric: 17.3399

Epoch 713: val_loss improved from 17.37177 to 17.33991, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 17.2578 - MinusLogProbMetric: 17.2578 - val_loss: 17.3399 - val_MinusLogProbMetric: 17.3399 - lr: 2.7778e-05 - 60s/epoch - 304ms/step
Epoch 714/1000
2023-10-23 23:41:50.876 
Epoch 714/1000 
	 loss: 17.2143, MinusLogProbMetric: 17.2143, val_loss: 17.4906, val_MinusLogProbMetric: 17.4906

Epoch 714: val_loss did not improve from 17.33991
196/196 - 59s - loss: 17.2143 - MinusLogProbMetric: 17.2143 - val_loss: 17.4906 - val_MinusLogProbMetric: 17.4906 - lr: 2.7778e-05 - 59s/epoch - 301ms/step
Epoch 715/1000
2023-10-23 23:42:51.479 
Epoch 715/1000 
	 loss: 17.1860, MinusLogProbMetric: 17.1860, val_loss: 17.4108, val_MinusLogProbMetric: 17.4108

Epoch 715: val_loss did not improve from 17.33991
196/196 - 61s - loss: 17.1860 - MinusLogProbMetric: 17.1860 - val_loss: 17.4108 - val_MinusLogProbMetric: 17.4108 - lr: 2.7778e-05 - 61s/epoch - 309ms/step
Epoch 716/1000
2023-10-23 23:43:55.228 
Epoch 716/1000 
	 loss: 17.2691, MinusLogProbMetric: 17.2691, val_loss: 17.4699, val_MinusLogProbMetric: 17.4699

Epoch 716: val_loss did not improve from 17.33991
196/196 - 64s - loss: 17.2691 - MinusLogProbMetric: 17.2691 - val_loss: 17.4699 - val_MinusLogProbMetric: 17.4699 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 717/1000
2023-10-23 23:44:56.410 
Epoch 717/1000 
	 loss: 17.2368, MinusLogProbMetric: 17.2368, val_loss: 17.4239, val_MinusLogProbMetric: 17.4239

Epoch 717: val_loss did not improve from 17.33991
196/196 - 61s - loss: 17.2368 - MinusLogProbMetric: 17.2368 - val_loss: 17.4239 - val_MinusLogProbMetric: 17.4239 - lr: 2.7778e-05 - 61s/epoch - 312ms/step
Epoch 718/1000
2023-10-23 23:45:59.163 
Epoch 718/1000 
	 loss: 17.2097, MinusLogProbMetric: 17.2097, val_loss: 17.3609, val_MinusLogProbMetric: 17.3609

Epoch 718: val_loss did not improve from 17.33991
196/196 - 63s - loss: 17.2097 - MinusLogProbMetric: 17.2097 - val_loss: 17.3609 - val_MinusLogProbMetric: 17.3609 - lr: 2.7778e-05 - 63s/epoch - 320ms/step
Epoch 719/1000
2023-10-23 23:47:00.305 
Epoch 719/1000 
	 loss: 17.2021, MinusLogProbMetric: 17.2021, val_loss: 17.4050, val_MinusLogProbMetric: 17.4050

Epoch 719: val_loss did not improve from 17.33991
196/196 - 61s - loss: 17.2021 - MinusLogProbMetric: 17.2021 - val_loss: 17.4050 - val_MinusLogProbMetric: 17.4050 - lr: 2.7778e-05 - 61s/epoch - 312ms/step
Epoch 720/1000
2023-10-23 23:47:59.100 
Epoch 720/1000 
	 loss: 17.2121, MinusLogProbMetric: 17.2121, val_loss: 17.3562, val_MinusLogProbMetric: 17.3562

Epoch 720: val_loss did not improve from 17.33991
196/196 - 59s - loss: 17.2121 - MinusLogProbMetric: 17.2121 - val_loss: 17.3562 - val_MinusLogProbMetric: 17.3562 - lr: 2.7778e-05 - 59s/epoch - 300ms/step
Epoch 721/1000
2023-10-23 23:49:00.288 
Epoch 721/1000 
	 loss: 17.2130, MinusLogProbMetric: 17.2130, val_loss: 17.3178, val_MinusLogProbMetric: 17.3178

Epoch 721: val_loss improved from 17.33991 to 17.31780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 62s - loss: 17.2130 - MinusLogProbMetric: 17.2130 - val_loss: 17.3178 - val_MinusLogProbMetric: 17.3178 - lr: 2.7778e-05 - 62s/epoch - 316ms/step
Epoch 722/1000
2023-10-23 23:50:02.980 
Epoch 722/1000 
	 loss: 17.1894, MinusLogProbMetric: 17.1894, val_loss: 17.3487, val_MinusLogProbMetric: 17.3487

Epoch 722: val_loss did not improve from 17.31780
196/196 - 62s - loss: 17.1894 - MinusLogProbMetric: 17.1894 - val_loss: 17.3487 - val_MinusLogProbMetric: 17.3487 - lr: 2.7778e-05 - 62s/epoch - 316ms/step
Epoch 723/1000
2023-10-23 23:51:07.495 
Epoch 723/1000 
	 loss: 17.2575, MinusLogProbMetric: 17.2575, val_loss: 17.4140, val_MinusLogProbMetric: 17.4140

Epoch 723: val_loss did not improve from 17.31780
196/196 - 65s - loss: 17.2575 - MinusLogProbMetric: 17.2575 - val_loss: 17.4140 - val_MinusLogProbMetric: 17.4140 - lr: 2.7778e-05 - 65s/epoch - 329ms/step
Epoch 724/1000
2023-10-23 23:52:10.177 
Epoch 724/1000 
	 loss: 17.1841, MinusLogProbMetric: 17.1841, val_loss: 17.4840, val_MinusLogProbMetric: 17.4840

Epoch 724: val_loss did not improve from 17.31780
196/196 - 63s - loss: 17.1841 - MinusLogProbMetric: 17.1841 - val_loss: 17.4840 - val_MinusLogProbMetric: 17.4840 - lr: 2.7778e-05 - 63s/epoch - 320ms/step
Epoch 725/1000
2023-10-23 23:53:13.417 
Epoch 725/1000 
	 loss: 17.2301, MinusLogProbMetric: 17.2301, val_loss: 17.4633, val_MinusLogProbMetric: 17.4633

Epoch 725: val_loss did not improve from 17.31780
196/196 - 63s - loss: 17.2301 - MinusLogProbMetric: 17.2301 - val_loss: 17.4633 - val_MinusLogProbMetric: 17.4633 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 726/1000
2023-10-23 23:54:13.931 
Epoch 726/1000 
	 loss: 17.2352, MinusLogProbMetric: 17.2352, val_loss: 17.4238, val_MinusLogProbMetric: 17.4238

Epoch 726: val_loss did not improve from 17.31780
196/196 - 61s - loss: 17.2352 - MinusLogProbMetric: 17.2352 - val_loss: 17.4238 - val_MinusLogProbMetric: 17.4238 - lr: 2.7778e-05 - 61s/epoch - 309ms/step
Epoch 727/1000
2023-10-23 23:55:17.815 
Epoch 727/1000 
	 loss: 17.1880, MinusLogProbMetric: 17.1880, val_loss: 17.3670, val_MinusLogProbMetric: 17.3670

Epoch 727: val_loss did not improve from 17.31780
196/196 - 64s - loss: 17.1880 - MinusLogProbMetric: 17.1880 - val_loss: 17.3670 - val_MinusLogProbMetric: 17.3670 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 728/1000
2023-10-23 23:56:21.555 
Epoch 728/1000 
	 loss: 17.2020, MinusLogProbMetric: 17.2020, val_loss: 17.3412, val_MinusLogProbMetric: 17.3412

Epoch 728: val_loss did not improve from 17.31780
196/196 - 64s - loss: 17.2020 - MinusLogProbMetric: 17.2020 - val_loss: 17.3412 - val_MinusLogProbMetric: 17.3412 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 729/1000
2023-10-23 23:57:24.873 
Epoch 729/1000 
	 loss: 17.2637, MinusLogProbMetric: 17.2637, val_loss: 17.4115, val_MinusLogProbMetric: 17.4115

Epoch 729: val_loss did not improve from 17.31780
196/196 - 63s - loss: 17.2637 - MinusLogProbMetric: 17.2637 - val_loss: 17.4115 - val_MinusLogProbMetric: 17.4115 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 730/1000
2023-10-23 23:58:28.448 
Epoch 730/1000 
	 loss: 17.1931, MinusLogProbMetric: 17.1931, val_loss: 17.3575, val_MinusLogProbMetric: 17.3575

Epoch 730: val_loss did not improve from 17.31780
196/196 - 64s - loss: 17.1931 - MinusLogProbMetric: 17.1931 - val_loss: 17.3575 - val_MinusLogProbMetric: 17.3575 - lr: 2.7778e-05 - 64s/epoch - 324ms/step
Epoch 731/1000
2023-10-23 23:59:30.215 
Epoch 731/1000 
	 loss: 17.2183, MinusLogProbMetric: 17.2183, val_loss: 17.3270, val_MinusLogProbMetric: 17.3270

Epoch 731: val_loss did not improve from 17.31780
196/196 - 62s - loss: 17.2183 - MinusLogProbMetric: 17.2183 - val_loss: 17.3270 - val_MinusLogProbMetric: 17.3270 - lr: 2.7778e-05 - 62s/epoch - 315ms/step
Epoch 732/1000
2023-10-24 00:00:33.897 
Epoch 732/1000 
	 loss: 17.2083, MinusLogProbMetric: 17.2083, val_loss: 17.6268, val_MinusLogProbMetric: 17.6268

Epoch 732: val_loss did not improve from 17.31780
196/196 - 64s - loss: 17.2083 - MinusLogProbMetric: 17.2083 - val_loss: 17.6268 - val_MinusLogProbMetric: 17.6268 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 733/1000
2023-10-24 00:01:36.585 
Epoch 733/1000 
	 loss: 17.1936, MinusLogProbMetric: 17.1936, val_loss: 17.3909, val_MinusLogProbMetric: 17.3909

Epoch 733: val_loss did not improve from 17.31780
196/196 - 63s - loss: 17.1936 - MinusLogProbMetric: 17.1936 - val_loss: 17.3909 - val_MinusLogProbMetric: 17.3909 - lr: 2.7778e-05 - 63s/epoch - 320ms/step
Epoch 734/1000
2023-10-24 00:02:39.409 
Epoch 734/1000 
	 loss: 17.2102, MinusLogProbMetric: 17.2102, val_loss: 17.3484, val_MinusLogProbMetric: 17.3484

Epoch 734: val_loss did not improve from 17.31780
196/196 - 63s - loss: 17.2102 - MinusLogProbMetric: 17.2102 - val_loss: 17.3484 - val_MinusLogProbMetric: 17.3484 - lr: 2.7778e-05 - 63s/epoch - 321ms/step
Epoch 735/1000
2023-10-24 00:03:39.817 
Epoch 735/1000 
	 loss: 17.2361, MinusLogProbMetric: 17.2361, val_loss: 17.3934, val_MinusLogProbMetric: 17.3934

Epoch 735: val_loss did not improve from 17.31780
196/196 - 60s - loss: 17.2361 - MinusLogProbMetric: 17.2361 - val_loss: 17.3934 - val_MinusLogProbMetric: 17.3934 - lr: 2.7778e-05 - 60s/epoch - 308ms/step
Epoch 736/1000
2023-10-24 00:04:42.420 
Epoch 736/1000 
	 loss: 17.2155, MinusLogProbMetric: 17.2155, val_loss: 17.4198, val_MinusLogProbMetric: 17.4198

Epoch 736: val_loss did not improve from 17.31780
196/196 - 63s - loss: 17.2155 - MinusLogProbMetric: 17.2155 - val_loss: 17.4198 - val_MinusLogProbMetric: 17.4198 - lr: 2.7778e-05 - 63s/epoch - 319ms/step
Epoch 737/1000
2023-10-24 00:05:44.160 
Epoch 737/1000 
	 loss: 17.2423, MinusLogProbMetric: 17.2423, val_loss: 17.7421, val_MinusLogProbMetric: 17.7421

Epoch 737: val_loss did not improve from 17.31780
196/196 - 62s - loss: 17.2423 - MinusLogProbMetric: 17.2423 - val_loss: 17.7421 - val_MinusLogProbMetric: 17.7421 - lr: 2.7778e-05 - 62s/epoch - 315ms/step
Epoch 738/1000
2023-10-24 00:06:45.350 
Epoch 738/1000 
	 loss: 17.2087, MinusLogProbMetric: 17.2087, val_loss: 17.3936, val_MinusLogProbMetric: 17.3936

Epoch 738: val_loss did not improve from 17.31780
196/196 - 61s - loss: 17.2087 - MinusLogProbMetric: 17.2087 - val_loss: 17.3936 - val_MinusLogProbMetric: 17.3936 - lr: 2.7778e-05 - 61s/epoch - 312ms/step
Epoch 739/1000
2023-10-24 00:07:42.949 
Epoch 739/1000 
	 loss: 17.1953, MinusLogProbMetric: 17.1953, val_loss: 17.4549, val_MinusLogProbMetric: 17.4549

Epoch 739: val_loss did not improve from 17.31780
196/196 - 58s - loss: 17.1953 - MinusLogProbMetric: 17.1953 - val_loss: 17.4549 - val_MinusLogProbMetric: 17.4549 - lr: 2.7778e-05 - 58s/epoch - 294ms/step
Epoch 740/1000
2023-10-24 00:08:45.661 
Epoch 740/1000 
	 loss: 17.2085, MinusLogProbMetric: 17.2085, val_loss: 17.3481, val_MinusLogProbMetric: 17.3481

Epoch 740: val_loss did not improve from 17.31780
196/196 - 63s - loss: 17.2085 - MinusLogProbMetric: 17.2085 - val_loss: 17.3481 - val_MinusLogProbMetric: 17.3481 - lr: 2.7778e-05 - 63s/epoch - 320ms/step
Epoch 741/1000
2023-10-24 00:09:42.827 
Epoch 741/1000 
	 loss: 17.2353, MinusLogProbMetric: 17.2353, val_loss: 17.3302, val_MinusLogProbMetric: 17.3302

Epoch 741: val_loss did not improve from 17.31780
196/196 - 57s - loss: 17.2353 - MinusLogProbMetric: 17.2353 - val_loss: 17.3302 - val_MinusLogProbMetric: 17.3302 - lr: 2.7778e-05 - 57s/epoch - 292ms/step
Epoch 742/1000
2023-10-24 00:10:41.823 
Epoch 742/1000 
	 loss: 17.2433, MinusLogProbMetric: 17.2433, val_loss: 17.4174, val_MinusLogProbMetric: 17.4174

Epoch 742: val_loss did not improve from 17.31780
196/196 - 59s - loss: 17.2433 - MinusLogProbMetric: 17.2433 - val_loss: 17.4174 - val_MinusLogProbMetric: 17.4174 - lr: 2.7778e-05 - 59s/epoch - 301ms/step
Epoch 743/1000
2023-10-24 00:11:41.522 
Epoch 743/1000 
	 loss: 17.2244, MinusLogProbMetric: 17.2244, val_loss: 18.3391, val_MinusLogProbMetric: 18.3391

Epoch 743: val_loss did not improve from 17.31780
196/196 - 60s - loss: 17.2244 - MinusLogProbMetric: 17.2244 - val_loss: 18.3391 - val_MinusLogProbMetric: 18.3391 - lr: 2.7778e-05 - 60s/epoch - 305ms/step
Epoch 744/1000
2023-10-24 00:12:41.063 
Epoch 744/1000 
	 loss: 17.2726, MinusLogProbMetric: 17.2726, val_loss: 17.3477, val_MinusLogProbMetric: 17.3477

Epoch 744: val_loss did not improve from 17.31780
196/196 - 60s - loss: 17.2726 - MinusLogProbMetric: 17.2726 - val_loss: 17.3477 - val_MinusLogProbMetric: 17.3477 - lr: 2.7778e-05 - 60s/epoch - 304ms/step
Epoch 745/1000
2023-10-24 00:13:43.614 
Epoch 745/1000 
	 loss: 17.2228, MinusLogProbMetric: 17.2228, val_loss: 17.4193, val_MinusLogProbMetric: 17.4193

Epoch 745: val_loss did not improve from 17.31780
196/196 - 63s - loss: 17.2228 - MinusLogProbMetric: 17.2228 - val_loss: 17.4193 - val_MinusLogProbMetric: 17.4193 - lr: 2.7778e-05 - 63s/epoch - 319ms/step
Epoch 746/1000
2023-10-24 00:14:44.928 
Epoch 746/1000 
	 loss: 17.1957, MinusLogProbMetric: 17.1957, val_loss: 17.3375, val_MinusLogProbMetric: 17.3375

Epoch 746: val_loss did not improve from 17.31780
196/196 - 61s - loss: 17.1957 - MinusLogProbMetric: 17.1957 - val_loss: 17.3375 - val_MinusLogProbMetric: 17.3375 - lr: 2.7778e-05 - 61s/epoch - 313ms/step
Epoch 747/1000
2023-10-24 00:15:46.226 
Epoch 747/1000 
	 loss: 17.2275, MinusLogProbMetric: 17.2275, val_loss: 17.3503, val_MinusLogProbMetric: 17.3503

Epoch 747: val_loss did not improve from 17.31780
196/196 - 61s - loss: 17.2275 - MinusLogProbMetric: 17.2275 - val_loss: 17.3503 - val_MinusLogProbMetric: 17.3503 - lr: 2.7778e-05 - 61s/epoch - 313ms/step
Epoch 748/1000
2023-10-24 00:16:47.909 
Epoch 748/1000 
	 loss: 17.2271, MinusLogProbMetric: 17.2271, val_loss: 17.3762, val_MinusLogProbMetric: 17.3762

Epoch 748: val_loss did not improve from 17.31780
196/196 - 62s - loss: 17.2271 - MinusLogProbMetric: 17.2271 - val_loss: 17.3762 - val_MinusLogProbMetric: 17.3762 - lr: 2.7778e-05 - 62s/epoch - 315ms/step
Epoch 749/1000
2023-10-24 00:17:49.226 
Epoch 749/1000 
	 loss: 17.1946, MinusLogProbMetric: 17.1946, val_loss: 17.4543, val_MinusLogProbMetric: 17.4543

Epoch 749: val_loss did not improve from 17.31780
196/196 - 61s - loss: 17.1946 - MinusLogProbMetric: 17.1946 - val_loss: 17.4543 - val_MinusLogProbMetric: 17.4543 - lr: 2.7778e-05 - 61s/epoch - 313ms/step
Epoch 750/1000
2023-10-24 00:18:51.560 
Epoch 750/1000 
	 loss: 17.1889, MinusLogProbMetric: 17.1889, val_loss: 17.3420, val_MinusLogProbMetric: 17.3420

Epoch 750: val_loss did not improve from 17.31780
196/196 - 62s - loss: 17.1889 - MinusLogProbMetric: 17.1889 - val_loss: 17.3420 - val_MinusLogProbMetric: 17.3420 - lr: 2.7778e-05 - 62s/epoch - 318ms/step
Epoch 751/1000
2023-10-24 00:19:54.815 
Epoch 751/1000 
	 loss: 17.1797, MinusLogProbMetric: 17.1797, val_loss: 17.3535, val_MinusLogProbMetric: 17.3535

Epoch 751: val_loss did not improve from 17.31780
196/196 - 63s - loss: 17.1797 - MinusLogProbMetric: 17.1797 - val_loss: 17.3535 - val_MinusLogProbMetric: 17.3535 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 752/1000
2023-10-24 00:20:58.668 
Epoch 752/1000 
	 loss: 17.2334, MinusLogProbMetric: 17.2334, val_loss: 17.4385, val_MinusLogProbMetric: 17.4385

Epoch 752: val_loss did not improve from 17.31780
196/196 - 64s - loss: 17.2334 - MinusLogProbMetric: 17.2334 - val_loss: 17.4385 - val_MinusLogProbMetric: 17.4385 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 753/1000
2023-10-24 00:22:01.169 
Epoch 753/1000 
	 loss: 17.2243, MinusLogProbMetric: 17.2243, val_loss: 17.3786, val_MinusLogProbMetric: 17.3786

Epoch 753: val_loss did not improve from 17.31780
196/196 - 62s - loss: 17.2243 - MinusLogProbMetric: 17.2243 - val_loss: 17.3786 - val_MinusLogProbMetric: 17.3786 - lr: 2.7778e-05 - 62s/epoch - 319ms/step
Epoch 754/1000
2023-10-24 00:23:03.882 
Epoch 754/1000 
	 loss: 17.2133, MinusLogProbMetric: 17.2133, val_loss: 17.4293, val_MinusLogProbMetric: 17.4293

Epoch 754: val_loss did not improve from 17.31780
196/196 - 63s - loss: 17.2133 - MinusLogProbMetric: 17.2133 - val_loss: 17.4293 - val_MinusLogProbMetric: 17.4293 - lr: 2.7778e-05 - 63s/epoch - 320ms/step
Epoch 755/1000
2023-10-24 00:24:07.776 
Epoch 755/1000 
	 loss: 17.1910, MinusLogProbMetric: 17.1910, val_loss: 17.4672, val_MinusLogProbMetric: 17.4672

Epoch 755: val_loss did not improve from 17.31780
196/196 - 64s - loss: 17.1910 - MinusLogProbMetric: 17.1910 - val_loss: 17.4672 - val_MinusLogProbMetric: 17.4672 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 756/1000
2023-10-24 00:25:11.769 
Epoch 756/1000 
	 loss: 17.1926, MinusLogProbMetric: 17.1926, val_loss: 17.4548, val_MinusLogProbMetric: 17.4548

Epoch 756: val_loss did not improve from 17.31780
196/196 - 64s - loss: 17.1926 - MinusLogProbMetric: 17.1926 - val_loss: 17.4548 - val_MinusLogProbMetric: 17.4548 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 757/1000
2023-10-24 00:26:13.407 
Epoch 757/1000 
	 loss: 17.1973, MinusLogProbMetric: 17.1973, val_loss: 17.3592, val_MinusLogProbMetric: 17.3592

Epoch 757: val_loss did not improve from 17.31780
196/196 - 62s - loss: 17.1973 - MinusLogProbMetric: 17.1973 - val_loss: 17.3592 - val_MinusLogProbMetric: 17.3592 - lr: 2.7778e-05 - 62s/epoch - 314ms/step
Epoch 758/1000
2023-10-24 00:27:16.028 
Epoch 758/1000 
	 loss: 17.1925, MinusLogProbMetric: 17.1925, val_loss: 17.3311, val_MinusLogProbMetric: 17.3311

Epoch 758: val_loss did not improve from 17.31780
196/196 - 63s - loss: 17.1925 - MinusLogProbMetric: 17.1925 - val_loss: 17.3311 - val_MinusLogProbMetric: 17.3311 - lr: 2.7778e-05 - 63s/epoch - 319ms/step
Epoch 759/1000
2023-10-24 00:28:17.841 
Epoch 759/1000 
	 loss: 17.1883, MinusLogProbMetric: 17.1883, val_loss: 17.4267, val_MinusLogProbMetric: 17.4267

Epoch 759: val_loss did not improve from 17.31780
196/196 - 62s - loss: 17.1883 - MinusLogProbMetric: 17.1883 - val_loss: 17.4267 - val_MinusLogProbMetric: 17.4267 - lr: 2.7778e-05 - 62s/epoch - 315ms/step
Epoch 760/1000
2023-10-24 00:29:21.274 
Epoch 760/1000 
	 loss: 17.1985, MinusLogProbMetric: 17.1985, val_loss: 17.3353, val_MinusLogProbMetric: 17.3353

Epoch 760: val_loss did not improve from 17.31780
196/196 - 63s - loss: 17.1985 - MinusLogProbMetric: 17.1985 - val_loss: 17.3353 - val_MinusLogProbMetric: 17.3353 - lr: 2.7778e-05 - 63s/epoch - 324ms/step
Epoch 761/1000
2023-10-24 00:30:22.361 
Epoch 761/1000 
	 loss: 17.2129, MinusLogProbMetric: 17.2129, val_loss: 17.3867, val_MinusLogProbMetric: 17.3867

Epoch 761: val_loss did not improve from 17.31780
196/196 - 61s - loss: 17.2129 - MinusLogProbMetric: 17.2129 - val_loss: 17.3867 - val_MinusLogProbMetric: 17.3867 - lr: 2.7778e-05 - 61s/epoch - 312ms/step
Epoch 762/1000
2023-10-24 00:31:26.096 
Epoch 762/1000 
	 loss: 17.1840, MinusLogProbMetric: 17.1840, val_loss: 17.3276, val_MinusLogProbMetric: 17.3276

Epoch 762: val_loss did not improve from 17.31780
196/196 - 64s - loss: 17.1840 - MinusLogProbMetric: 17.1840 - val_loss: 17.3276 - val_MinusLogProbMetric: 17.3276 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 763/1000
2023-10-24 00:32:25.600 
Epoch 763/1000 
	 loss: 17.1988, MinusLogProbMetric: 17.1988, val_loss: 17.5041, val_MinusLogProbMetric: 17.5041

Epoch 763: val_loss did not improve from 17.31780
196/196 - 59s - loss: 17.1988 - MinusLogProbMetric: 17.1988 - val_loss: 17.5041 - val_MinusLogProbMetric: 17.5041 - lr: 2.7778e-05 - 59s/epoch - 304ms/step
Epoch 764/1000
2023-10-24 00:33:22.650 
Epoch 764/1000 
	 loss: 17.1850, MinusLogProbMetric: 17.1850, val_loss: 17.5781, val_MinusLogProbMetric: 17.5781

Epoch 764: val_loss did not improve from 17.31780
196/196 - 57s - loss: 17.1850 - MinusLogProbMetric: 17.1850 - val_loss: 17.5781 - val_MinusLogProbMetric: 17.5781 - lr: 2.7778e-05 - 57s/epoch - 291ms/step
Epoch 765/1000
2023-10-24 00:34:26.248 
Epoch 765/1000 
	 loss: 17.1796, MinusLogProbMetric: 17.1796, val_loss: 17.4644, val_MinusLogProbMetric: 17.4644

Epoch 765: val_loss did not improve from 17.31780
196/196 - 64s - loss: 17.1796 - MinusLogProbMetric: 17.1796 - val_loss: 17.4644 - val_MinusLogProbMetric: 17.4644 - lr: 2.7778e-05 - 64s/epoch - 324ms/step
Epoch 766/1000
2023-10-24 00:35:27.110 
Epoch 766/1000 
	 loss: 17.1937, MinusLogProbMetric: 17.1937, val_loss: 17.4790, val_MinusLogProbMetric: 17.4790

Epoch 766: val_loss did not improve from 17.31780
196/196 - 61s - loss: 17.1937 - MinusLogProbMetric: 17.1937 - val_loss: 17.4790 - val_MinusLogProbMetric: 17.4790 - lr: 2.7778e-05 - 61s/epoch - 310ms/step
Epoch 767/1000
2023-10-24 00:36:29.294 
Epoch 767/1000 
	 loss: 17.2010, MinusLogProbMetric: 17.2010, val_loss: 17.3593, val_MinusLogProbMetric: 17.3593

Epoch 767: val_loss did not improve from 17.31780
196/196 - 62s - loss: 17.2010 - MinusLogProbMetric: 17.2010 - val_loss: 17.3593 - val_MinusLogProbMetric: 17.3593 - lr: 2.7778e-05 - 62s/epoch - 317ms/step
Epoch 768/1000
2023-10-24 00:37:26.598 
Epoch 768/1000 
	 loss: 17.1691, MinusLogProbMetric: 17.1691, val_loss: 17.3359, val_MinusLogProbMetric: 17.3359

Epoch 768: val_loss did not improve from 17.31780
196/196 - 57s - loss: 17.1691 - MinusLogProbMetric: 17.1691 - val_loss: 17.3359 - val_MinusLogProbMetric: 17.3359 - lr: 2.7778e-05 - 57s/epoch - 292ms/step
Epoch 769/1000
2023-10-24 00:38:27.686 
Epoch 769/1000 
	 loss: 17.2041, MinusLogProbMetric: 17.2041, val_loss: 17.4485, val_MinusLogProbMetric: 17.4485

Epoch 769: val_loss did not improve from 17.31780
196/196 - 61s - loss: 17.2041 - MinusLogProbMetric: 17.2041 - val_loss: 17.4485 - val_MinusLogProbMetric: 17.4485 - lr: 2.7778e-05 - 61s/epoch - 312ms/step
Epoch 770/1000
2023-10-24 00:39:30.222 
Epoch 770/1000 
	 loss: 17.2084, MinusLogProbMetric: 17.2084, val_loss: 17.3812, val_MinusLogProbMetric: 17.3812

Epoch 770: val_loss did not improve from 17.31780
196/196 - 63s - loss: 17.2084 - MinusLogProbMetric: 17.2084 - val_loss: 17.3812 - val_MinusLogProbMetric: 17.3812 - lr: 2.7778e-05 - 63s/epoch - 319ms/step
Epoch 771/1000
2023-10-24 00:40:33.938 
Epoch 771/1000 
	 loss: 17.2177, MinusLogProbMetric: 17.2177, val_loss: 17.3893, val_MinusLogProbMetric: 17.3893

Epoch 771: val_loss did not improve from 17.31780
196/196 - 64s - loss: 17.2177 - MinusLogProbMetric: 17.2177 - val_loss: 17.3893 - val_MinusLogProbMetric: 17.3893 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 772/1000
2023-10-24 00:41:35.332 
Epoch 772/1000 
	 loss: 17.1452, MinusLogProbMetric: 17.1452, val_loss: 17.2991, val_MinusLogProbMetric: 17.2991

Epoch 772: val_loss improved from 17.31780 to 17.29906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 62s - loss: 17.1452 - MinusLogProbMetric: 17.1452 - val_loss: 17.2991 - val_MinusLogProbMetric: 17.2991 - lr: 1.3889e-05 - 62s/epoch - 318ms/step
Epoch 773/1000
2023-10-24 00:42:36.608 
Epoch 773/1000 
	 loss: 17.1338, MinusLogProbMetric: 17.1338, val_loss: 17.3378, val_MinusLogProbMetric: 17.3378

Epoch 773: val_loss did not improve from 17.29906
196/196 - 60s - loss: 17.1338 - MinusLogProbMetric: 17.1338 - val_loss: 17.3378 - val_MinusLogProbMetric: 17.3378 - lr: 1.3889e-05 - 60s/epoch - 308ms/step
Epoch 774/1000
2023-10-24 00:43:36.119 
Epoch 774/1000 
	 loss: 17.1237, MinusLogProbMetric: 17.1237, val_loss: 17.2976, val_MinusLogProbMetric: 17.2976

Epoch 774: val_loss improved from 17.29906 to 17.29756, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 17.1237 - MinusLogProbMetric: 17.1237 - val_loss: 17.2976 - val_MinusLogProbMetric: 17.2976 - lr: 1.3889e-05 - 60s/epoch - 308ms/step
Epoch 775/1000
2023-10-24 00:44:34.683 
Epoch 775/1000 
	 loss: 17.1315, MinusLogProbMetric: 17.1315, val_loss: 17.3008, val_MinusLogProbMetric: 17.3008

Epoch 775: val_loss did not improve from 17.29756
196/196 - 58s - loss: 17.1315 - MinusLogProbMetric: 17.1315 - val_loss: 17.3008 - val_MinusLogProbMetric: 17.3008 - lr: 1.3889e-05 - 58s/epoch - 294ms/step
Epoch 776/1000
2023-10-24 00:45:33.097 
Epoch 776/1000 
	 loss: 17.1372, MinusLogProbMetric: 17.1372, val_loss: 17.3002, val_MinusLogProbMetric: 17.3002

Epoch 776: val_loss did not improve from 17.29756
196/196 - 58s - loss: 17.1372 - MinusLogProbMetric: 17.1372 - val_loss: 17.3002 - val_MinusLogProbMetric: 17.3002 - lr: 1.3889e-05 - 58s/epoch - 298ms/step
Epoch 777/1000
2023-10-24 00:46:34.599 
Epoch 777/1000 
	 loss: 17.1670, MinusLogProbMetric: 17.1670, val_loss: 17.3386, val_MinusLogProbMetric: 17.3386

Epoch 777: val_loss did not improve from 17.29756
196/196 - 61s - loss: 17.1670 - MinusLogProbMetric: 17.1670 - val_loss: 17.3386 - val_MinusLogProbMetric: 17.3386 - lr: 1.3889e-05 - 61s/epoch - 314ms/step
Epoch 778/1000
2023-10-24 00:47:38.011 
Epoch 778/1000 
	 loss: 17.1281, MinusLogProbMetric: 17.1281, val_loss: 17.2878, val_MinusLogProbMetric: 17.2878

Epoch 778: val_loss improved from 17.29756 to 17.28784, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 64s - loss: 17.1281 - MinusLogProbMetric: 17.1281 - val_loss: 17.2878 - val_MinusLogProbMetric: 17.2878 - lr: 1.3889e-05 - 64s/epoch - 328ms/step
Epoch 779/1000
2023-10-24 00:48:40.386 
Epoch 779/1000 
	 loss: 17.1468, MinusLogProbMetric: 17.1468, val_loss: 17.3349, val_MinusLogProbMetric: 17.3349

Epoch 779: val_loss did not improve from 17.28784
196/196 - 61s - loss: 17.1468 - MinusLogProbMetric: 17.1468 - val_loss: 17.3349 - val_MinusLogProbMetric: 17.3349 - lr: 1.3889e-05 - 61s/epoch - 314ms/step
Epoch 780/1000
2023-10-24 00:49:43.470 
Epoch 780/1000 
	 loss: 17.1243, MinusLogProbMetric: 17.1243, val_loss: 17.3248, val_MinusLogProbMetric: 17.3248

Epoch 780: val_loss did not improve from 17.28784
196/196 - 63s - loss: 17.1243 - MinusLogProbMetric: 17.1243 - val_loss: 17.3248 - val_MinusLogProbMetric: 17.3248 - lr: 1.3889e-05 - 63s/epoch - 322ms/step
Epoch 781/1000
2023-10-24 00:50:45.164 
Epoch 781/1000 
	 loss: 17.1272, MinusLogProbMetric: 17.1272, val_loss: 17.2850, val_MinusLogProbMetric: 17.2850

Epoch 781: val_loss improved from 17.28784 to 17.28503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 63s - loss: 17.1272 - MinusLogProbMetric: 17.1272 - val_loss: 17.2850 - val_MinusLogProbMetric: 17.2850 - lr: 1.3889e-05 - 63s/epoch - 319ms/step
Epoch 782/1000
2023-10-24 00:51:44.772 
Epoch 782/1000 
	 loss: 17.1317, MinusLogProbMetric: 17.1317, val_loss: 17.3161, val_MinusLogProbMetric: 17.3161

Epoch 782: val_loss did not improve from 17.28503
196/196 - 59s - loss: 17.1317 - MinusLogProbMetric: 17.1317 - val_loss: 17.3161 - val_MinusLogProbMetric: 17.3161 - lr: 1.3889e-05 - 59s/epoch - 300ms/step
Epoch 783/1000
2023-10-24 00:52:45.129 
Epoch 783/1000 
	 loss: 17.1415, MinusLogProbMetric: 17.1415, val_loss: 17.2945, val_MinusLogProbMetric: 17.2945

Epoch 783: val_loss did not improve from 17.28503
196/196 - 60s - loss: 17.1415 - MinusLogProbMetric: 17.1415 - val_loss: 17.2945 - val_MinusLogProbMetric: 17.2945 - lr: 1.3889e-05 - 60s/epoch - 308ms/step
Epoch 784/1000
2023-10-24 00:53:48.039 
Epoch 784/1000 
	 loss: 17.1273, MinusLogProbMetric: 17.1273, val_loss: 17.2988, val_MinusLogProbMetric: 17.2988

Epoch 784: val_loss did not improve from 17.28503
196/196 - 63s - loss: 17.1273 - MinusLogProbMetric: 17.1273 - val_loss: 17.2988 - val_MinusLogProbMetric: 17.2988 - lr: 1.3889e-05 - 63s/epoch - 321ms/step
Epoch 785/1000
2023-10-24 00:54:51.579 
Epoch 785/1000 
	 loss: 17.1264, MinusLogProbMetric: 17.1264, val_loss: 17.3125, val_MinusLogProbMetric: 17.3125

Epoch 785: val_loss did not improve from 17.28503
196/196 - 64s - loss: 17.1264 - MinusLogProbMetric: 17.1264 - val_loss: 17.3125 - val_MinusLogProbMetric: 17.3125 - lr: 1.3889e-05 - 64s/epoch - 324ms/step
Epoch 786/1000
2023-10-24 00:55:54.940 
Epoch 786/1000 
	 loss: 17.1332, MinusLogProbMetric: 17.1332, val_loss: 17.3151, val_MinusLogProbMetric: 17.3151

Epoch 786: val_loss did not improve from 17.28503
196/196 - 63s - loss: 17.1332 - MinusLogProbMetric: 17.1332 - val_loss: 17.3151 - val_MinusLogProbMetric: 17.3151 - lr: 1.3889e-05 - 63s/epoch - 323ms/step
Epoch 787/1000
2023-10-24 00:56:57.298 
Epoch 787/1000 
	 loss: 17.1252, MinusLogProbMetric: 17.1252, val_loss: 17.3056, val_MinusLogProbMetric: 17.3056

Epoch 787: val_loss did not improve from 17.28503
196/196 - 62s - loss: 17.1252 - MinusLogProbMetric: 17.1252 - val_loss: 17.3056 - val_MinusLogProbMetric: 17.3056 - lr: 1.3889e-05 - 62s/epoch - 318ms/step
Epoch 788/1000
2023-10-24 00:58:00.355 
Epoch 788/1000 
	 loss: 17.1262, MinusLogProbMetric: 17.1262, val_loss: 17.3177, val_MinusLogProbMetric: 17.3177

Epoch 788: val_loss did not improve from 17.28503
196/196 - 63s - loss: 17.1262 - MinusLogProbMetric: 17.1262 - val_loss: 17.3177 - val_MinusLogProbMetric: 17.3177 - lr: 1.3889e-05 - 63s/epoch - 322ms/step
Epoch 789/1000
2023-10-24 00:59:00.702 
Epoch 789/1000 
	 loss: 17.1262, MinusLogProbMetric: 17.1262, val_loss: 17.2828, val_MinusLogProbMetric: 17.2828

Epoch 789: val_loss improved from 17.28503 to 17.28280, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 61s - loss: 17.1262 - MinusLogProbMetric: 17.1262 - val_loss: 17.2828 - val_MinusLogProbMetric: 17.2828 - lr: 1.3889e-05 - 61s/epoch - 312ms/step
Epoch 790/1000
2023-10-24 01:00:00.461 
Epoch 790/1000 
	 loss: 17.1284, MinusLogProbMetric: 17.1284, val_loss: 17.3032, val_MinusLogProbMetric: 17.3032

Epoch 790: val_loss did not improve from 17.28280
196/196 - 59s - loss: 17.1284 - MinusLogProbMetric: 17.1284 - val_loss: 17.3032 - val_MinusLogProbMetric: 17.3032 - lr: 1.3889e-05 - 59s/epoch - 300ms/step
Epoch 791/1000
2023-10-24 01:01:02.259 
Epoch 791/1000 
	 loss: 17.1302, MinusLogProbMetric: 17.1302, val_loss: 17.3354, val_MinusLogProbMetric: 17.3354

Epoch 791: val_loss did not improve from 17.28280
196/196 - 62s - loss: 17.1302 - MinusLogProbMetric: 17.1302 - val_loss: 17.3354 - val_MinusLogProbMetric: 17.3354 - lr: 1.3889e-05 - 62s/epoch - 315ms/step
Epoch 792/1000
2023-10-24 01:02:06.343 
Epoch 792/1000 
	 loss: 17.1367, MinusLogProbMetric: 17.1367, val_loss: 17.3151, val_MinusLogProbMetric: 17.3151

Epoch 792: val_loss did not improve from 17.28280
196/196 - 64s - loss: 17.1367 - MinusLogProbMetric: 17.1367 - val_loss: 17.3151 - val_MinusLogProbMetric: 17.3151 - lr: 1.3889e-05 - 64s/epoch - 327ms/step
Epoch 793/1000
2023-10-24 01:03:09.645 
Epoch 793/1000 
	 loss: 17.1337, MinusLogProbMetric: 17.1337, val_loss: 17.2988, val_MinusLogProbMetric: 17.2988

Epoch 793: val_loss did not improve from 17.28280
196/196 - 63s - loss: 17.1337 - MinusLogProbMetric: 17.1337 - val_loss: 17.2988 - val_MinusLogProbMetric: 17.2988 - lr: 1.3889e-05 - 63s/epoch - 323ms/step
Epoch 794/1000
2023-10-24 01:04:10.989 
Epoch 794/1000 
	 loss: 17.1435, MinusLogProbMetric: 17.1435, val_loss: 17.3890, val_MinusLogProbMetric: 17.3890

Epoch 794: val_loss did not improve from 17.28280
196/196 - 61s - loss: 17.1435 - MinusLogProbMetric: 17.1435 - val_loss: 17.3890 - val_MinusLogProbMetric: 17.3890 - lr: 1.3889e-05 - 61s/epoch - 313ms/step
Epoch 795/1000
2023-10-24 01:05:12.083 
Epoch 795/1000 
	 loss: 17.1287, MinusLogProbMetric: 17.1287, val_loss: 17.2916, val_MinusLogProbMetric: 17.2916

Epoch 795: val_loss did not improve from 17.28280
196/196 - 61s - loss: 17.1287 - MinusLogProbMetric: 17.1287 - val_loss: 17.2916 - val_MinusLogProbMetric: 17.2916 - lr: 1.3889e-05 - 61s/epoch - 312ms/step
Epoch 796/1000
2023-10-24 01:06:14.694 
Epoch 796/1000 
	 loss: 17.1328, MinusLogProbMetric: 17.1328, val_loss: 17.2894, val_MinusLogProbMetric: 17.2894

Epoch 796: val_loss did not improve from 17.28280
196/196 - 63s - loss: 17.1328 - MinusLogProbMetric: 17.1328 - val_loss: 17.2894 - val_MinusLogProbMetric: 17.2894 - lr: 1.3889e-05 - 63s/epoch - 319ms/step
Epoch 797/1000
2023-10-24 01:07:17.269 
Epoch 797/1000 
	 loss: 17.1299, MinusLogProbMetric: 17.1299, val_loss: 17.2988, val_MinusLogProbMetric: 17.2988

Epoch 797: val_loss did not improve from 17.28280
196/196 - 63s - loss: 17.1299 - MinusLogProbMetric: 17.1299 - val_loss: 17.2988 - val_MinusLogProbMetric: 17.2988 - lr: 1.3889e-05 - 63s/epoch - 319ms/step
Epoch 798/1000
2023-10-24 01:08:21.079 
Epoch 798/1000 
	 loss: 17.1394, MinusLogProbMetric: 17.1394, val_loss: 17.2900, val_MinusLogProbMetric: 17.2900

Epoch 798: val_loss did not improve from 17.28280
196/196 - 64s - loss: 17.1394 - MinusLogProbMetric: 17.1394 - val_loss: 17.2900 - val_MinusLogProbMetric: 17.2900 - lr: 1.3889e-05 - 64s/epoch - 326ms/step
Epoch 799/1000
2023-10-24 01:09:22.376 
Epoch 799/1000 
	 loss: 17.1296, MinusLogProbMetric: 17.1296, val_loss: 17.2856, val_MinusLogProbMetric: 17.2856

Epoch 799: val_loss did not improve from 17.28280
196/196 - 61s - loss: 17.1296 - MinusLogProbMetric: 17.1296 - val_loss: 17.2856 - val_MinusLogProbMetric: 17.2856 - lr: 1.3889e-05 - 61s/epoch - 313ms/step
Epoch 800/1000
2023-10-24 01:10:24.657 
Epoch 800/1000 
	 loss: 17.1391, MinusLogProbMetric: 17.1391, val_loss: 17.5209, val_MinusLogProbMetric: 17.5209

Epoch 800: val_loss did not improve from 17.28280
196/196 - 62s - loss: 17.1391 - MinusLogProbMetric: 17.1391 - val_loss: 17.5209 - val_MinusLogProbMetric: 17.5209 - lr: 1.3889e-05 - 62s/epoch - 318ms/step
Epoch 801/1000
2023-10-24 01:11:24.612 
Epoch 801/1000 
	 loss: 17.1424, MinusLogProbMetric: 17.1424, val_loss: 17.2779, val_MinusLogProbMetric: 17.2779

Epoch 801: val_loss improved from 17.28280 to 17.27792, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 61s - loss: 17.1424 - MinusLogProbMetric: 17.1424 - val_loss: 17.2779 - val_MinusLogProbMetric: 17.2779 - lr: 1.3889e-05 - 61s/epoch - 311ms/step
Epoch 802/1000
2023-10-24 01:12:26.547 
Epoch 802/1000 
	 loss: 17.1608, MinusLogProbMetric: 17.1608, val_loss: 17.2962, val_MinusLogProbMetric: 17.2962

Epoch 802: val_loss did not improve from 17.27792
196/196 - 61s - loss: 17.1608 - MinusLogProbMetric: 17.1608 - val_loss: 17.2962 - val_MinusLogProbMetric: 17.2962 - lr: 1.3889e-05 - 61s/epoch - 311ms/step
Epoch 803/1000
2023-10-24 01:13:27.339 
Epoch 803/1000 
	 loss: 17.1272, MinusLogProbMetric: 17.1272, val_loss: 17.3108, val_MinusLogProbMetric: 17.3108

Epoch 803: val_loss did not improve from 17.27792
196/196 - 61s - loss: 17.1272 - MinusLogProbMetric: 17.1272 - val_loss: 17.3108 - val_MinusLogProbMetric: 17.3108 - lr: 1.3889e-05 - 61s/epoch - 310ms/step
Epoch 804/1000
2023-10-24 01:14:28.782 
Epoch 804/1000 
	 loss: 17.1385, MinusLogProbMetric: 17.1385, val_loss: 17.2916, val_MinusLogProbMetric: 17.2916

Epoch 804: val_loss did not improve from 17.27792
196/196 - 61s - loss: 17.1385 - MinusLogProbMetric: 17.1385 - val_loss: 17.2916 - val_MinusLogProbMetric: 17.2916 - lr: 1.3889e-05 - 61s/epoch - 313ms/step
Epoch 805/1000
2023-10-24 01:15:30.607 
Epoch 805/1000 
	 loss: 17.1331, MinusLogProbMetric: 17.1331, val_loss: 17.3857, val_MinusLogProbMetric: 17.3857

Epoch 805: val_loss did not improve from 17.27792
196/196 - 62s - loss: 17.1331 - MinusLogProbMetric: 17.1331 - val_loss: 17.3857 - val_MinusLogProbMetric: 17.3857 - lr: 1.3889e-05 - 62s/epoch - 315ms/step
Epoch 806/1000
2023-10-24 01:16:29.982 
Epoch 806/1000 
	 loss: 17.1359, MinusLogProbMetric: 17.1359, val_loss: 17.3565, val_MinusLogProbMetric: 17.3565

Epoch 806: val_loss did not improve from 17.27792
196/196 - 59s - loss: 17.1359 - MinusLogProbMetric: 17.1359 - val_loss: 17.3565 - val_MinusLogProbMetric: 17.3565 - lr: 1.3889e-05 - 59s/epoch - 303ms/step
Epoch 807/1000
2023-10-24 01:17:28.868 
Epoch 807/1000 
	 loss: 17.1275, MinusLogProbMetric: 17.1275, val_loss: 17.3629, val_MinusLogProbMetric: 17.3629

Epoch 807: val_loss did not improve from 17.27792
196/196 - 59s - loss: 17.1275 - MinusLogProbMetric: 17.1275 - val_loss: 17.3629 - val_MinusLogProbMetric: 17.3629 - lr: 1.3889e-05 - 59s/epoch - 300ms/step
Epoch 808/1000
2023-10-24 01:18:27.765 
Epoch 808/1000 
	 loss: 17.1318, MinusLogProbMetric: 17.1318, val_loss: 17.3844, val_MinusLogProbMetric: 17.3844

Epoch 808: val_loss did not improve from 17.27792
196/196 - 59s - loss: 17.1318 - MinusLogProbMetric: 17.1318 - val_loss: 17.3844 - val_MinusLogProbMetric: 17.3844 - lr: 1.3889e-05 - 59s/epoch - 300ms/step
Epoch 809/1000
2023-10-24 01:19:28.346 
Epoch 809/1000 
	 loss: 17.1315, MinusLogProbMetric: 17.1315, val_loss: 17.3066, val_MinusLogProbMetric: 17.3066

Epoch 809: val_loss did not improve from 17.27792
196/196 - 61s - loss: 17.1315 - MinusLogProbMetric: 17.1315 - val_loss: 17.3066 - val_MinusLogProbMetric: 17.3066 - lr: 1.3889e-05 - 61s/epoch - 309ms/step
Epoch 810/1000
2023-10-24 01:20:27.291 
Epoch 810/1000 
	 loss: 17.1512, MinusLogProbMetric: 17.1512, val_loss: 17.3131, val_MinusLogProbMetric: 17.3131

Epoch 810: val_loss did not improve from 17.27792
196/196 - 59s - loss: 17.1512 - MinusLogProbMetric: 17.1512 - val_loss: 17.3131 - val_MinusLogProbMetric: 17.3131 - lr: 1.3889e-05 - 59s/epoch - 301ms/step
Epoch 811/1000
2023-10-24 01:21:30.181 
Epoch 811/1000 
	 loss: 17.1285, MinusLogProbMetric: 17.1285, val_loss: 17.3068, val_MinusLogProbMetric: 17.3068

Epoch 811: val_loss did not improve from 17.27792
196/196 - 63s - loss: 17.1285 - MinusLogProbMetric: 17.1285 - val_loss: 17.3068 - val_MinusLogProbMetric: 17.3068 - lr: 1.3889e-05 - 63s/epoch - 321ms/step
Epoch 812/1000
2023-10-24 01:22:32.915 
Epoch 812/1000 
	 loss: 17.1307, MinusLogProbMetric: 17.1307, val_loss: 17.2993, val_MinusLogProbMetric: 17.2993

Epoch 812: val_loss did not improve from 17.27792
196/196 - 63s - loss: 17.1307 - MinusLogProbMetric: 17.1307 - val_loss: 17.2993 - val_MinusLogProbMetric: 17.2993 - lr: 1.3889e-05 - 63s/epoch - 320ms/step
Epoch 813/1000
2023-10-24 01:23:30.248 
Epoch 813/1000 
	 loss: 17.1431, MinusLogProbMetric: 17.1431, val_loss: 17.4489, val_MinusLogProbMetric: 17.4489

Epoch 813: val_loss did not improve from 17.27792
196/196 - 57s - loss: 17.1431 - MinusLogProbMetric: 17.1431 - val_loss: 17.4489 - val_MinusLogProbMetric: 17.4489 - lr: 1.3889e-05 - 57s/epoch - 293ms/step
Epoch 814/1000
2023-10-24 01:24:31.941 
Epoch 814/1000 
	 loss: 17.1397, MinusLogProbMetric: 17.1397, val_loss: 17.2907, val_MinusLogProbMetric: 17.2907

Epoch 814: val_loss did not improve from 17.27792
196/196 - 62s - loss: 17.1397 - MinusLogProbMetric: 17.1397 - val_loss: 17.2907 - val_MinusLogProbMetric: 17.2907 - lr: 1.3889e-05 - 62s/epoch - 315ms/step
Epoch 815/1000
2023-10-24 01:25:32.243 
Epoch 815/1000 
	 loss: 17.1317, MinusLogProbMetric: 17.1317, val_loss: 17.3447, val_MinusLogProbMetric: 17.3447

Epoch 815: val_loss did not improve from 17.27792
196/196 - 60s - loss: 17.1317 - MinusLogProbMetric: 17.1317 - val_loss: 17.3447 - val_MinusLogProbMetric: 17.3447 - lr: 1.3889e-05 - 60s/epoch - 308ms/step
Epoch 816/1000
2023-10-24 01:26:34.168 
Epoch 816/1000 
	 loss: 17.1361, MinusLogProbMetric: 17.1361, val_loss: 17.2888, val_MinusLogProbMetric: 17.2888

Epoch 816: val_loss did not improve from 17.27792
196/196 - 62s - loss: 17.1361 - MinusLogProbMetric: 17.1361 - val_loss: 17.2888 - val_MinusLogProbMetric: 17.2888 - lr: 1.3889e-05 - 62s/epoch - 316ms/step
Epoch 817/1000
2023-10-24 01:27:36.085 
Epoch 817/1000 
	 loss: 17.1237, MinusLogProbMetric: 17.1237, val_loss: 17.2947, val_MinusLogProbMetric: 17.2947

Epoch 817: val_loss did not improve from 17.27792
196/196 - 62s - loss: 17.1237 - MinusLogProbMetric: 17.1237 - val_loss: 17.2947 - val_MinusLogProbMetric: 17.2947 - lr: 1.3889e-05 - 62s/epoch - 316ms/step
Epoch 818/1000
2023-10-24 01:28:39.317 
Epoch 818/1000 
	 loss: 17.1325, MinusLogProbMetric: 17.1325, val_loss: 17.3472, val_MinusLogProbMetric: 17.3472

Epoch 818: val_loss did not improve from 17.27792
196/196 - 63s - loss: 17.1325 - MinusLogProbMetric: 17.1325 - val_loss: 17.3472 - val_MinusLogProbMetric: 17.3472 - lr: 1.3889e-05 - 63s/epoch - 323ms/step
Epoch 819/1000
2023-10-24 01:29:42.248 
Epoch 819/1000 
	 loss: 17.1308, MinusLogProbMetric: 17.1308, val_loss: 17.3282, val_MinusLogProbMetric: 17.3282

Epoch 819: val_loss did not improve from 17.27792
196/196 - 63s - loss: 17.1308 - MinusLogProbMetric: 17.1308 - val_loss: 17.3282 - val_MinusLogProbMetric: 17.3282 - lr: 1.3889e-05 - 63s/epoch - 321ms/step
Epoch 820/1000
2023-10-24 01:30:44.170 
Epoch 820/1000 
	 loss: 17.1398, MinusLogProbMetric: 17.1398, val_loss: 17.3310, val_MinusLogProbMetric: 17.3310

Epoch 820: val_loss did not improve from 17.27792
196/196 - 62s - loss: 17.1398 - MinusLogProbMetric: 17.1398 - val_loss: 17.3310 - val_MinusLogProbMetric: 17.3310 - lr: 1.3889e-05 - 62s/epoch - 316ms/step
Epoch 821/1000
2023-10-24 01:31:42.767 
Epoch 821/1000 
	 loss: 17.1276, MinusLogProbMetric: 17.1276, val_loss: 17.2996, val_MinusLogProbMetric: 17.2996

Epoch 821: val_loss did not improve from 17.27792
196/196 - 59s - loss: 17.1276 - MinusLogProbMetric: 17.1276 - val_loss: 17.2996 - val_MinusLogProbMetric: 17.2996 - lr: 1.3889e-05 - 59s/epoch - 299ms/step
Epoch 822/1000
2023-10-24 01:32:44.176 
Epoch 822/1000 
	 loss: 17.1248, MinusLogProbMetric: 17.1248, val_loss: 17.2851, val_MinusLogProbMetric: 17.2851

Epoch 822: val_loss did not improve from 17.27792
196/196 - 61s - loss: 17.1248 - MinusLogProbMetric: 17.1248 - val_loss: 17.2851 - val_MinusLogProbMetric: 17.2851 - lr: 1.3889e-05 - 61s/epoch - 313ms/step
Epoch 823/1000
2023-10-24 01:33:48.033 
Epoch 823/1000 
	 loss: 17.1332, MinusLogProbMetric: 17.1332, val_loss: 17.2720, val_MinusLogProbMetric: 17.2720

Epoch 823: val_loss improved from 17.27792 to 17.27196, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 65s - loss: 17.1332 - MinusLogProbMetric: 17.1332 - val_loss: 17.2720 - val_MinusLogProbMetric: 17.2720 - lr: 1.3889e-05 - 65s/epoch - 331ms/step
Epoch 824/1000
2023-10-24 01:34:51.923 
Epoch 824/1000 
	 loss: 17.1275, MinusLogProbMetric: 17.1275, val_loss: 17.2917, val_MinusLogProbMetric: 17.2917

Epoch 824: val_loss did not improve from 17.27196
196/196 - 63s - loss: 17.1275 - MinusLogProbMetric: 17.1275 - val_loss: 17.2917 - val_MinusLogProbMetric: 17.2917 - lr: 1.3889e-05 - 63s/epoch - 321ms/step
Epoch 825/1000
2023-10-24 01:35:51.423 
Epoch 825/1000 
	 loss: 17.1323, MinusLogProbMetric: 17.1323, val_loss: 17.2926, val_MinusLogProbMetric: 17.2926

Epoch 825: val_loss did not improve from 17.27196
196/196 - 59s - loss: 17.1323 - MinusLogProbMetric: 17.1323 - val_loss: 17.2926 - val_MinusLogProbMetric: 17.2926 - lr: 1.3889e-05 - 59s/epoch - 304ms/step
Epoch 826/1000
2023-10-24 01:36:52.555 
Epoch 826/1000 
	 loss: 17.1315, MinusLogProbMetric: 17.1315, val_loss: 17.2836, val_MinusLogProbMetric: 17.2836

Epoch 826: val_loss did not improve from 17.27196
196/196 - 61s - loss: 17.1315 - MinusLogProbMetric: 17.1315 - val_loss: 17.2836 - val_MinusLogProbMetric: 17.2836 - lr: 1.3889e-05 - 61s/epoch - 312ms/step
Epoch 827/1000
2023-10-24 01:37:54.956 
Epoch 827/1000 
	 loss: 17.1340, MinusLogProbMetric: 17.1340, val_loss: 17.3177, val_MinusLogProbMetric: 17.3177

Epoch 827: val_loss did not improve from 17.27196
196/196 - 62s - loss: 17.1340 - MinusLogProbMetric: 17.1340 - val_loss: 17.3177 - val_MinusLogProbMetric: 17.3177 - lr: 1.3889e-05 - 62s/epoch - 318ms/step
Epoch 828/1000
2023-10-24 01:38:57.425 
Epoch 828/1000 
	 loss: 17.1402, MinusLogProbMetric: 17.1402, val_loss: 17.3679, val_MinusLogProbMetric: 17.3679

Epoch 828: val_loss did not improve from 17.27196
196/196 - 62s - loss: 17.1402 - MinusLogProbMetric: 17.1402 - val_loss: 17.3679 - val_MinusLogProbMetric: 17.3679 - lr: 1.3889e-05 - 62s/epoch - 319ms/step
Epoch 829/1000
2023-10-24 01:39:57.897 
Epoch 829/1000 
	 loss: 17.1276, MinusLogProbMetric: 17.1276, val_loss: 17.3422, val_MinusLogProbMetric: 17.3422

Epoch 829: val_loss did not improve from 17.27196
196/196 - 60s - loss: 17.1276 - MinusLogProbMetric: 17.1276 - val_loss: 17.3422 - val_MinusLogProbMetric: 17.3422 - lr: 1.3889e-05 - 60s/epoch - 309ms/step
Epoch 830/1000
2023-10-24 01:40:55.182 
Epoch 830/1000 
	 loss: 17.1433, MinusLogProbMetric: 17.1433, val_loss: 17.3213, val_MinusLogProbMetric: 17.3213

Epoch 830: val_loss did not improve from 17.27196
196/196 - 57s - loss: 17.1433 - MinusLogProbMetric: 17.1433 - val_loss: 17.3213 - val_MinusLogProbMetric: 17.3213 - lr: 1.3889e-05 - 57s/epoch - 292ms/step
Epoch 831/1000
2023-10-24 01:41:54.843 
Epoch 831/1000 
	 loss: 17.1286, MinusLogProbMetric: 17.1286, val_loss: 17.3079, val_MinusLogProbMetric: 17.3079

Epoch 831: val_loss did not improve from 17.27196
196/196 - 60s - loss: 17.1286 - MinusLogProbMetric: 17.1286 - val_loss: 17.3079 - val_MinusLogProbMetric: 17.3079 - lr: 1.3889e-05 - 60s/epoch - 304ms/step
Epoch 832/1000
2023-10-24 01:42:55.074 
Epoch 832/1000 
	 loss: 17.1346, MinusLogProbMetric: 17.1346, val_loss: 17.2857, val_MinusLogProbMetric: 17.2857

Epoch 832: val_loss did not improve from 17.27196
196/196 - 60s - loss: 17.1346 - MinusLogProbMetric: 17.1346 - val_loss: 17.2857 - val_MinusLogProbMetric: 17.2857 - lr: 1.3889e-05 - 60s/epoch - 307ms/step
Epoch 833/1000
2023-10-24 01:43:53.712 
Epoch 833/1000 
	 loss: 17.1366, MinusLogProbMetric: 17.1366, val_loss: 17.3093, val_MinusLogProbMetric: 17.3093

Epoch 833: val_loss did not improve from 17.27196
196/196 - 59s - loss: 17.1366 - MinusLogProbMetric: 17.1366 - val_loss: 17.3093 - val_MinusLogProbMetric: 17.3093 - lr: 1.3889e-05 - 59s/epoch - 299ms/step
Epoch 834/1000
2023-10-24 01:44:54.843 
Epoch 834/1000 
	 loss: 17.1191, MinusLogProbMetric: 17.1191, val_loss: 17.3172, val_MinusLogProbMetric: 17.3172

Epoch 834: val_loss did not improve from 17.27196
196/196 - 61s - loss: 17.1191 - MinusLogProbMetric: 17.1191 - val_loss: 17.3172 - val_MinusLogProbMetric: 17.3172 - lr: 1.3889e-05 - 61s/epoch - 312ms/step
Epoch 835/1000
2023-10-24 01:45:56.739 
Epoch 835/1000 
	 loss: 17.1186, MinusLogProbMetric: 17.1186, val_loss: 17.3174, val_MinusLogProbMetric: 17.3174

Epoch 835: val_loss did not improve from 17.27196
196/196 - 62s - loss: 17.1186 - MinusLogProbMetric: 17.1186 - val_loss: 17.3174 - val_MinusLogProbMetric: 17.3174 - lr: 1.3889e-05 - 62s/epoch - 316ms/step
Epoch 836/1000
2023-10-24 01:46:59.423 
Epoch 836/1000 
	 loss: 17.1318, MinusLogProbMetric: 17.1318, val_loss: 17.3185, val_MinusLogProbMetric: 17.3185

Epoch 836: val_loss did not improve from 17.27196
196/196 - 63s - loss: 17.1318 - MinusLogProbMetric: 17.1318 - val_loss: 17.3185 - val_MinusLogProbMetric: 17.3185 - lr: 1.3889e-05 - 63s/epoch - 320ms/step
Epoch 837/1000
2023-10-24 01:48:01.649 
Epoch 837/1000 
	 loss: 17.1224, MinusLogProbMetric: 17.1224, val_loss: 17.2989, val_MinusLogProbMetric: 17.2989

Epoch 837: val_loss did not improve from 17.27196
196/196 - 62s - loss: 17.1224 - MinusLogProbMetric: 17.1224 - val_loss: 17.2989 - val_MinusLogProbMetric: 17.2989 - lr: 1.3889e-05 - 62s/epoch - 317ms/step
Epoch 838/1000
2023-10-24 01:48:59.878 
Epoch 838/1000 
	 loss: 17.1250, MinusLogProbMetric: 17.1250, val_loss: 17.3115, val_MinusLogProbMetric: 17.3115

Epoch 838: val_loss did not improve from 17.27196
196/196 - 58s - loss: 17.1250 - MinusLogProbMetric: 17.1250 - val_loss: 17.3115 - val_MinusLogProbMetric: 17.3115 - lr: 1.3889e-05 - 58s/epoch - 297ms/step
Epoch 839/1000
2023-10-24 01:49:59.346 
Epoch 839/1000 
	 loss: 17.1139, MinusLogProbMetric: 17.1139, val_loss: 17.2823, val_MinusLogProbMetric: 17.2823

Epoch 839: val_loss did not improve from 17.27196
196/196 - 59s - loss: 17.1139 - MinusLogProbMetric: 17.1139 - val_loss: 17.2823 - val_MinusLogProbMetric: 17.2823 - lr: 1.3889e-05 - 59s/epoch - 303ms/step
Epoch 840/1000
2023-10-24 01:51:00.409 
Epoch 840/1000 
	 loss: 17.1318, MinusLogProbMetric: 17.1318, val_loss: 17.3291, val_MinusLogProbMetric: 17.3291

Epoch 840: val_loss did not improve from 17.27196
196/196 - 61s - loss: 17.1318 - MinusLogProbMetric: 17.1318 - val_loss: 17.3291 - val_MinusLogProbMetric: 17.3291 - lr: 1.3889e-05 - 61s/epoch - 312ms/step
Epoch 841/1000
2023-10-24 01:52:04.416 
Epoch 841/1000 
	 loss: 17.1195, MinusLogProbMetric: 17.1195, val_loss: 17.3820, val_MinusLogProbMetric: 17.3820

Epoch 841: val_loss did not improve from 17.27196
196/196 - 64s - loss: 17.1195 - MinusLogProbMetric: 17.1195 - val_loss: 17.3820 - val_MinusLogProbMetric: 17.3820 - lr: 1.3889e-05 - 64s/epoch - 327ms/step
Epoch 842/1000
2023-10-24 01:53:05.542 
Epoch 842/1000 
	 loss: 17.1467, MinusLogProbMetric: 17.1467, val_loss: 17.3256, val_MinusLogProbMetric: 17.3256

Epoch 842: val_loss did not improve from 17.27196
196/196 - 61s - loss: 17.1467 - MinusLogProbMetric: 17.1467 - val_loss: 17.3256 - val_MinusLogProbMetric: 17.3256 - lr: 1.3889e-05 - 61s/epoch - 312ms/step
Epoch 843/1000
2023-10-24 01:54:07.876 
Epoch 843/1000 
	 loss: 17.1377, MinusLogProbMetric: 17.1377, val_loss: 17.2881, val_MinusLogProbMetric: 17.2881

Epoch 843: val_loss did not improve from 17.27196
196/196 - 62s - loss: 17.1377 - MinusLogProbMetric: 17.1377 - val_loss: 17.2881 - val_MinusLogProbMetric: 17.2881 - lr: 1.3889e-05 - 62s/epoch - 318ms/step
Epoch 844/1000
2023-10-24 01:55:07.361 
Epoch 844/1000 
	 loss: 17.1239, MinusLogProbMetric: 17.1239, val_loss: 17.4256, val_MinusLogProbMetric: 17.4256

Epoch 844: val_loss did not improve from 17.27196
196/196 - 59s - loss: 17.1239 - MinusLogProbMetric: 17.1239 - val_loss: 17.4256 - val_MinusLogProbMetric: 17.4256 - lr: 1.3889e-05 - 59s/epoch - 303ms/step
Epoch 845/1000
2023-10-24 01:56:11.136 
Epoch 845/1000 
	 loss: 17.1400, MinusLogProbMetric: 17.1400, val_loss: 17.2843, val_MinusLogProbMetric: 17.2843

Epoch 845: val_loss did not improve from 17.27196
196/196 - 64s - loss: 17.1400 - MinusLogProbMetric: 17.1400 - val_loss: 17.2843 - val_MinusLogProbMetric: 17.2843 - lr: 1.3889e-05 - 64s/epoch - 325ms/step
Epoch 846/1000
2023-10-24 01:57:15.025 
Epoch 846/1000 
	 loss: 17.1474, MinusLogProbMetric: 17.1474, val_loss: 17.2799, val_MinusLogProbMetric: 17.2799

Epoch 846: val_loss did not improve from 17.27196
196/196 - 64s - loss: 17.1474 - MinusLogProbMetric: 17.1474 - val_loss: 17.2799 - val_MinusLogProbMetric: 17.2799 - lr: 1.3889e-05 - 64s/epoch - 326ms/step
Epoch 847/1000
2023-10-24 01:58:19.502 
Epoch 847/1000 
	 loss: 17.1239, MinusLogProbMetric: 17.1239, val_loss: 17.2844, val_MinusLogProbMetric: 17.2844

Epoch 847: val_loss did not improve from 17.27196
196/196 - 64s - loss: 17.1239 - MinusLogProbMetric: 17.1239 - val_loss: 17.2844 - val_MinusLogProbMetric: 17.2844 - lr: 1.3889e-05 - 64s/epoch - 329ms/step
Epoch 848/1000
2023-10-24 01:59:23.190 
Epoch 848/1000 
	 loss: 17.1357, MinusLogProbMetric: 17.1357, val_loss: 17.2833, val_MinusLogProbMetric: 17.2833

Epoch 848: val_loss did not improve from 17.27196
196/196 - 64s - loss: 17.1357 - MinusLogProbMetric: 17.1357 - val_loss: 17.2833 - val_MinusLogProbMetric: 17.2833 - lr: 1.3889e-05 - 64s/epoch - 325ms/step
Epoch 849/1000
2023-10-24 02:00:25.749 
Epoch 849/1000 
	 loss: 17.1325, MinusLogProbMetric: 17.1325, val_loss: 17.2964, val_MinusLogProbMetric: 17.2964

Epoch 849: val_loss did not improve from 17.27196
196/196 - 63s - loss: 17.1325 - MinusLogProbMetric: 17.1325 - val_loss: 17.2964 - val_MinusLogProbMetric: 17.2964 - lr: 1.3889e-05 - 63s/epoch - 319ms/step
Epoch 850/1000
2023-10-24 02:01:27.580 
Epoch 850/1000 
	 loss: 17.1199, MinusLogProbMetric: 17.1199, val_loss: 17.2881, val_MinusLogProbMetric: 17.2881

Epoch 850: val_loss did not improve from 17.27196
196/196 - 62s - loss: 17.1199 - MinusLogProbMetric: 17.1199 - val_loss: 17.2881 - val_MinusLogProbMetric: 17.2881 - lr: 1.3889e-05 - 62s/epoch - 315ms/step
Epoch 851/1000
2023-10-24 02:02:28.417 
Epoch 851/1000 
	 loss: 17.1330, MinusLogProbMetric: 17.1330, val_loss: 17.2826, val_MinusLogProbMetric: 17.2826

Epoch 851: val_loss did not improve from 17.27196
196/196 - 61s - loss: 17.1330 - MinusLogProbMetric: 17.1330 - val_loss: 17.2826 - val_MinusLogProbMetric: 17.2826 - lr: 1.3889e-05 - 61s/epoch - 310ms/step
Epoch 852/1000
2023-10-24 02:03:30.583 
Epoch 852/1000 
	 loss: 17.1206, MinusLogProbMetric: 17.1206, val_loss: 17.2719, val_MinusLogProbMetric: 17.2719

Epoch 852: val_loss improved from 17.27196 to 17.27190, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 63s - loss: 17.1206 - MinusLogProbMetric: 17.1206 - val_loss: 17.2719 - val_MinusLogProbMetric: 17.2719 - lr: 1.3889e-05 - 63s/epoch - 321ms/step
Epoch 853/1000
2023-10-24 02:04:32.621 
Epoch 853/1000 
	 loss: 17.1305, MinusLogProbMetric: 17.1305, val_loss: 17.2900, val_MinusLogProbMetric: 17.2900

Epoch 853: val_loss did not improve from 17.27190
196/196 - 61s - loss: 17.1305 - MinusLogProbMetric: 17.1305 - val_loss: 17.2900 - val_MinusLogProbMetric: 17.2900 - lr: 1.3889e-05 - 61s/epoch - 313ms/step
Epoch 854/1000
2023-10-24 02:05:34.647 
Epoch 854/1000 
	 loss: 17.1207, MinusLogProbMetric: 17.1207, val_loss: 17.2918, val_MinusLogProbMetric: 17.2918

Epoch 854: val_loss did not improve from 17.27190
196/196 - 62s - loss: 17.1207 - MinusLogProbMetric: 17.1207 - val_loss: 17.2918 - val_MinusLogProbMetric: 17.2918 - lr: 1.3889e-05 - 62s/epoch - 316ms/step
Epoch 855/1000
2023-10-24 02:06:38.600 
Epoch 855/1000 
	 loss: 17.1153, MinusLogProbMetric: 17.1153, val_loss: 17.3028, val_MinusLogProbMetric: 17.3028

Epoch 855: val_loss did not improve from 17.27190
196/196 - 64s - loss: 17.1153 - MinusLogProbMetric: 17.1153 - val_loss: 17.3028 - val_MinusLogProbMetric: 17.3028 - lr: 1.3889e-05 - 64s/epoch - 326ms/step
Epoch 856/1000
2023-10-24 02:07:41.353 
Epoch 856/1000 
	 loss: 17.1289, MinusLogProbMetric: 17.1289, val_loss: 17.2721, val_MinusLogProbMetric: 17.2721

Epoch 856: val_loss did not improve from 17.27190
196/196 - 63s - loss: 17.1289 - MinusLogProbMetric: 17.1289 - val_loss: 17.2721 - val_MinusLogProbMetric: 17.2721 - lr: 1.3889e-05 - 63s/epoch - 320ms/step
Epoch 857/1000
2023-10-24 02:08:44.900 
Epoch 857/1000 
	 loss: 17.1283, MinusLogProbMetric: 17.1283, val_loss: 17.2767, val_MinusLogProbMetric: 17.2767

Epoch 857: val_loss did not improve from 17.27190
196/196 - 64s - loss: 17.1283 - MinusLogProbMetric: 17.1283 - val_loss: 17.2767 - val_MinusLogProbMetric: 17.2767 - lr: 1.3889e-05 - 64s/epoch - 324ms/step
Epoch 858/1000
2023-10-24 02:09:45.872 
Epoch 858/1000 
	 loss: 17.1266, MinusLogProbMetric: 17.1266, val_loss: 17.3498, val_MinusLogProbMetric: 17.3498

Epoch 858: val_loss did not improve from 17.27190
196/196 - 61s - loss: 17.1266 - MinusLogProbMetric: 17.1266 - val_loss: 17.3498 - val_MinusLogProbMetric: 17.3498 - lr: 1.3889e-05 - 61s/epoch - 311ms/step
Epoch 859/1000
2023-10-24 02:10:48.491 
Epoch 859/1000 
	 loss: 17.1233, MinusLogProbMetric: 17.1233, val_loss: 17.3044, val_MinusLogProbMetric: 17.3044

Epoch 859: val_loss did not improve from 17.27190
196/196 - 63s - loss: 17.1233 - MinusLogProbMetric: 17.1233 - val_loss: 17.3044 - val_MinusLogProbMetric: 17.3044 - lr: 1.3889e-05 - 63s/epoch - 319ms/step
Epoch 860/1000
2023-10-24 02:11:52.170 
Epoch 860/1000 
	 loss: 17.1161, MinusLogProbMetric: 17.1161, val_loss: 17.2949, val_MinusLogProbMetric: 17.2949

Epoch 860: val_loss did not improve from 17.27190
196/196 - 64s - loss: 17.1161 - MinusLogProbMetric: 17.1161 - val_loss: 17.2949 - val_MinusLogProbMetric: 17.2949 - lr: 1.3889e-05 - 64s/epoch - 325ms/step
Epoch 861/1000
2023-10-24 02:12:54.197 
Epoch 861/1000 
	 loss: 17.1172, MinusLogProbMetric: 17.1172, val_loss: 17.3247, val_MinusLogProbMetric: 17.3247

Epoch 861: val_loss did not improve from 17.27190
196/196 - 62s - loss: 17.1172 - MinusLogProbMetric: 17.1172 - val_loss: 17.3247 - val_MinusLogProbMetric: 17.3247 - lr: 1.3889e-05 - 62s/epoch - 316ms/step
Epoch 862/1000
2023-10-24 02:13:57.308 
Epoch 862/1000 
	 loss: 17.1331, MinusLogProbMetric: 17.1331, val_loss: 17.2956, val_MinusLogProbMetric: 17.2956

Epoch 862: val_loss did not improve from 17.27190
196/196 - 63s - loss: 17.1331 - MinusLogProbMetric: 17.1331 - val_loss: 17.2956 - val_MinusLogProbMetric: 17.2956 - lr: 1.3889e-05 - 63s/epoch - 322ms/step
Epoch 863/1000
2023-10-24 02:14:57.124 
Epoch 863/1000 
	 loss: 17.1304, MinusLogProbMetric: 17.1304, val_loss: 17.3093, val_MinusLogProbMetric: 17.3093

Epoch 863: val_loss did not improve from 17.27190
196/196 - 60s - loss: 17.1304 - MinusLogProbMetric: 17.1304 - val_loss: 17.3093 - val_MinusLogProbMetric: 17.3093 - lr: 1.3889e-05 - 60s/epoch - 305ms/step
Epoch 864/1000
2023-10-24 02:15:57.540 
Epoch 864/1000 
	 loss: 17.1243, MinusLogProbMetric: 17.1243, val_loss: 17.2911, val_MinusLogProbMetric: 17.2911

Epoch 864: val_loss did not improve from 17.27190
196/196 - 60s - loss: 17.1243 - MinusLogProbMetric: 17.1243 - val_loss: 17.2911 - val_MinusLogProbMetric: 17.2911 - lr: 1.3889e-05 - 60s/epoch - 308ms/step
Epoch 865/1000
2023-10-24 02:17:00.507 
Epoch 865/1000 
	 loss: 17.1198, MinusLogProbMetric: 17.1198, val_loss: 17.3162, val_MinusLogProbMetric: 17.3162

Epoch 865: val_loss did not improve from 17.27190
196/196 - 63s - loss: 17.1198 - MinusLogProbMetric: 17.1198 - val_loss: 17.3162 - val_MinusLogProbMetric: 17.3162 - lr: 1.3889e-05 - 63s/epoch - 321ms/step
Epoch 866/1000
2023-10-24 02:18:03.579 
Epoch 866/1000 
	 loss: 17.1215, MinusLogProbMetric: 17.1215, val_loss: 17.2969, val_MinusLogProbMetric: 17.2969

Epoch 866: val_loss did not improve from 17.27190
196/196 - 63s - loss: 17.1215 - MinusLogProbMetric: 17.1215 - val_loss: 17.2969 - val_MinusLogProbMetric: 17.2969 - lr: 1.3889e-05 - 63s/epoch - 322ms/step
Epoch 867/1000
2023-10-24 02:19:07.776 
Epoch 867/1000 
	 loss: 17.1456, MinusLogProbMetric: 17.1456, val_loss: 17.3232, val_MinusLogProbMetric: 17.3232

Epoch 867: val_loss did not improve from 17.27190
196/196 - 64s - loss: 17.1456 - MinusLogProbMetric: 17.1456 - val_loss: 17.3232 - val_MinusLogProbMetric: 17.3232 - lr: 1.3889e-05 - 64s/epoch - 328ms/step
Epoch 868/1000
2023-10-24 02:20:05.720 
Epoch 868/1000 
	 loss: 17.1395, MinusLogProbMetric: 17.1395, val_loss: 17.3072, val_MinusLogProbMetric: 17.3072

Epoch 868: val_loss did not improve from 17.27190
196/196 - 58s - loss: 17.1395 - MinusLogProbMetric: 17.1395 - val_loss: 17.3072 - val_MinusLogProbMetric: 17.3072 - lr: 1.3889e-05 - 58s/epoch - 296ms/step
Epoch 869/1000
2023-10-24 02:21:07.592 
Epoch 869/1000 
	 loss: 17.1099, MinusLogProbMetric: 17.1099, val_loss: 17.2961, val_MinusLogProbMetric: 17.2961

Epoch 869: val_loss did not improve from 17.27190
196/196 - 62s - loss: 17.1099 - MinusLogProbMetric: 17.1099 - val_loss: 17.2961 - val_MinusLogProbMetric: 17.2961 - lr: 1.3889e-05 - 62s/epoch - 316ms/step
Epoch 870/1000
2023-10-24 02:22:11.350 
Epoch 870/1000 
	 loss: 17.1276, MinusLogProbMetric: 17.1276, val_loss: 17.2873, val_MinusLogProbMetric: 17.2873

Epoch 870: val_loss did not improve from 17.27190
196/196 - 64s - loss: 17.1276 - MinusLogProbMetric: 17.1276 - val_loss: 17.2873 - val_MinusLogProbMetric: 17.2873 - lr: 1.3889e-05 - 64s/epoch - 325ms/step
Epoch 871/1000
2023-10-24 02:23:11.735 
Epoch 871/1000 
	 loss: 17.1361, MinusLogProbMetric: 17.1361, val_loss: 17.2970, val_MinusLogProbMetric: 17.2970

Epoch 871: val_loss did not improve from 17.27190
196/196 - 60s - loss: 17.1361 - MinusLogProbMetric: 17.1361 - val_loss: 17.2970 - val_MinusLogProbMetric: 17.2970 - lr: 1.3889e-05 - 60s/epoch - 308ms/step
Epoch 872/1000
2023-10-24 02:24:08.620 
Epoch 872/1000 
	 loss: 17.1444, MinusLogProbMetric: 17.1444, val_loss: 17.4151, val_MinusLogProbMetric: 17.4151

Epoch 872: val_loss did not improve from 17.27190
196/196 - 57s - loss: 17.1444 - MinusLogProbMetric: 17.1444 - val_loss: 17.4151 - val_MinusLogProbMetric: 17.4151 - lr: 1.3889e-05 - 57s/epoch - 290ms/step
Epoch 873/1000
2023-10-24 02:25:09.147 
Epoch 873/1000 
	 loss: 17.1375, MinusLogProbMetric: 17.1375, val_loss: 17.2865, val_MinusLogProbMetric: 17.2865

Epoch 873: val_loss did not improve from 17.27190
196/196 - 61s - loss: 17.1375 - MinusLogProbMetric: 17.1375 - val_loss: 17.2865 - val_MinusLogProbMetric: 17.2865 - lr: 1.3889e-05 - 61s/epoch - 309ms/step
Epoch 874/1000
2023-10-24 02:26:10.899 
Epoch 874/1000 
	 loss: 17.0942, MinusLogProbMetric: 17.0942, val_loss: 17.2921, val_MinusLogProbMetric: 17.2921

Epoch 874: val_loss did not improve from 17.27190
196/196 - 62s - loss: 17.0942 - MinusLogProbMetric: 17.0942 - val_loss: 17.2921 - val_MinusLogProbMetric: 17.2921 - lr: 6.9444e-06 - 62s/epoch - 315ms/step
Epoch 875/1000
2023-10-24 02:27:15.564 
Epoch 875/1000 
	 loss: 17.0926, MinusLogProbMetric: 17.0926, val_loss: 17.2789, val_MinusLogProbMetric: 17.2789

Epoch 875: val_loss did not improve from 17.27190
196/196 - 65s - loss: 17.0926 - MinusLogProbMetric: 17.0926 - val_loss: 17.2789 - val_MinusLogProbMetric: 17.2789 - lr: 6.9444e-06 - 65s/epoch - 330ms/step
Epoch 876/1000
2023-10-24 02:28:21.550 
Epoch 876/1000 
	 loss: 17.0893, MinusLogProbMetric: 17.0893, val_loss: 17.2738, val_MinusLogProbMetric: 17.2738

Epoch 876: val_loss did not improve from 17.27190
196/196 - 66s - loss: 17.0893 - MinusLogProbMetric: 17.0893 - val_loss: 17.2738 - val_MinusLogProbMetric: 17.2738 - lr: 6.9444e-06 - 66s/epoch - 337ms/step
Epoch 877/1000
2023-10-24 02:29:28.097 
Epoch 877/1000 
	 loss: 17.0912, MinusLogProbMetric: 17.0912, val_loss: 17.2665, val_MinusLogProbMetric: 17.2665

Epoch 877: val_loss improved from 17.27190 to 17.26654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 68s - loss: 17.0912 - MinusLogProbMetric: 17.0912 - val_loss: 17.2665 - val_MinusLogProbMetric: 17.2665 - lr: 6.9444e-06 - 68s/epoch - 345ms/step
Epoch 878/1000
2023-10-24 02:30:28.331 
Epoch 878/1000 
	 loss: 17.0906, MinusLogProbMetric: 17.0906, val_loss: 17.2730, val_MinusLogProbMetric: 17.2730

Epoch 878: val_loss did not improve from 17.26654
196/196 - 59s - loss: 17.0906 - MinusLogProbMetric: 17.0906 - val_loss: 17.2730 - val_MinusLogProbMetric: 17.2730 - lr: 6.9444e-06 - 59s/epoch - 302ms/step
Epoch 879/1000
2023-10-24 02:31:32.290 
Epoch 879/1000 
	 loss: 17.0920, MinusLogProbMetric: 17.0920, val_loss: 17.2618, val_MinusLogProbMetric: 17.2618

Epoch 879: val_loss improved from 17.26654 to 17.26178, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 65s - loss: 17.0920 - MinusLogProbMetric: 17.0920 - val_loss: 17.2618 - val_MinusLogProbMetric: 17.2618 - lr: 6.9444e-06 - 65s/epoch - 332ms/step
Epoch 880/1000
2023-10-24 02:32:37.453 
Epoch 880/1000 
	 loss: 17.0944, MinusLogProbMetric: 17.0944, val_loss: 17.2825, val_MinusLogProbMetric: 17.2825

Epoch 880: val_loss did not improve from 17.26178
196/196 - 64s - loss: 17.0944 - MinusLogProbMetric: 17.0944 - val_loss: 17.2825 - val_MinusLogProbMetric: 17.2825 - lr: 6.9444e-06 - 64s/epoch - 327ms/step
Epoch 881/1000
2023-10-24 02:33:36.799 
Epoch 881/1000 
	 loss: 17.0924, MinusLogProbMetric: 17.0924, val_loss: 17.2727, val_MinusLogProbMetric: 17.2727

Epoch 881: val_loss did not improve from 17.26178
196/196 - 59s - loss: 17.0924 - MinusLogProbMetric: 17.0924 - val_loss: 17.2727 - val_MinusLogProbMetric: 17.2727 - lr: 6.9444e-06 - 59s/epoch - 303ms/step
Epoch 882/1000
2023-10-24 02:34:41.414 
Epoch 882/1000 
	 loss: 17.0882, MinusLogProbMetric: 17.0882, val_loss: 17.2652, val_MinusLogProbMetric: 17.2652

Epoch 882: val_loss did not improve from 17.26178
196/196 - 65s - loss: 17.0882 - MinusLogProbMetric: 17.0882 - val_loss: 17.2652 - val_MinusLogProbMetric: 17.2652 - lr: 6.9444e-06 - 65s/epoch - 330ms/step
Epoch 883/1000
2023-10-24 02:35:41.980 
Epoch 883/1000 
	 loss: 17.0990, MinusLogProbMetric: 17.0990, val_loss: 17.3169, val_MinusLogProbMetric: 17.3169

Epoch 883: val_loss did not improve from 17.26178
196/196 - 61s - loss: 17.0990 - MinusLogProbMetric: 17.0990 - val_loss: 17.3169 - val_MinusLogProbMetric: 17.3169 - lr: 6.9444e-06 - 61s/epoch - 309ms/step
Epoch 884/1000
2023-10-24 02:36:42.782 
Epoch 884/1000 
	 loss: 17.1011, MinusLogProbMetric: 17.1011, val_loss: 17.2658, val_MinusLogProbMetric: 17.2658

Epoch 884: val_loss did not improve from 17.26178
196/196 - 61s - loss: 17.1011 - MinusLogProbMetric: 17.1011 - val_loss: 17.2658 - val_MinusLogProbMetric: 17.2658 - lr: 6.9444e-06 - 61s/epoch - 310ms/step
Epoch 885/1000
2023-10-24 02:37:40.250 
Epoch 885/1000 
	 loss: 17.1000, MinusLogProbMetric: 17.1000, val_loss: 17.3160, val_MinusLogProbMetric: 17.3160

Epoch 885: val_loss did not improve from 17.26178
196/196 - 57s - loss: 17.1000 - MinusLogProbMetric: 17.1000 - val_loss: 17.3160 - val_MinusLogProbMetric: 17.3160 - lr: 6.9444e-06 - 57s/epoch - 293ms/step
Epoch 886/1000
2023-10-24 02:38:40.668 
Epoch 886/1000 
	 loss: 17.0931, MinusLogProbMetric: 17.0931, val_loss: 17.2799, val_MinusLogProbMetric: 17.2799

Epoch 886: val_loss did not improve from 17.26178
196/196 - 60s - loss: 17.0931 - MinusLogProbMetric: 17.0931 - val_loss: 17.2799 - val_MinusLogProbMetric: 17.2799 - lr: 6.9444e-06 - 60s/epoch - 308ms/step
Epoch 887/1000
2023-10-24 02:39:38.274 
Epoch 887/1000 
	 loss: 17.0959, MinusLogProbMetric: 17.0959, val_loss: 17.4224, val_MinusLogProbMetric: 17.4224

Epoch 887: val_loss did not improve from 17.26178
196/196 - 58s - loss: 17.0959 - MinusLogProbMetric: 17.0959 - val_loss: 17.4224 - val_MinusLogProbMetric: 17.4224 - lr: 6.9444e-06 - 58s/epoch - 294ms/step
Epoch 888/1000
2023-10-24 02:40:40.791 
Epoch 888/1000 
	 loss: 17.1053, MinusLogProbMetric: 17.1053, val_loss: 17.2733, val_MinusLogProbMetric: 17.2733

Epoch 888: val_loss did not improve from 17.26178
196/196 - 63s - loss: 17.1053 - MinusLogProbMetric: 17.1053 - val_loss: 17.2733 - val_MinusLogProbMetric: 17.2733 - lr: 6.9444e-06 - 63s/epoch - 319ms/step
Epoch 889/1000
2023-10-24 02:41:38.034 
Epoch 889/1000 
	 loss: 17.0902, MinusLogProbMetric: 17.0902, val_loss: 17.2599, val_MinusLogProbMetric: 17.2599

Epoch 889: val_loss improved from 17.26178 to 17.25988, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 58s - loss: 17.0902 - MinusLogProbMetric: 17.0902 - val_loss: 17.2599 - val_MinusLogProbMetric: 17.2599 - lr: 6.9444e-06 - 58s/epoch - 297ms/step
Epoch 890/1000
2023-10-24 02:42:36.672 
Epoch 890/1000 
	 loss: 17.0914, MinusLogProbMetric: 17.0914, val_loss: 17.2863, val_MinusLogProbMetric: 17.2863

Epoch 890: val_loss did not improve from 17.25988
196/196 - 58s - loss: 17.0914 - MinusLogProbMetric: 17.0914 - val_loss: 17.2863 - val_MinusLogProbMetric: 17.2863 - lr: 6.9444e-06 - 58s/epoch - 294ms/step
Epoch 891/1000
2023-10-24 02:43:34.380 
Epoch 891/1000 
	 loss: 17.0940, MinusLogProbMetric: 17.0940, val_loss: 17.3710, val_MinusLogProbMetric: 17.3710

Epoch 891: val_loss did not improve from 17.25988
196/196 - 58s - loss: 17.0940 - MinusLogProbMetric: 17.0940 - val_loss: 17.3710 - val_MinusLogProbMetric: 17.3710 - lr: 6.9444e-06 - 58s/epoch - 294ms/step
Epoch 892/1000
2023-10-24 02:44:31.742 
Epoch 892/1000 
	 loss: 17.1087, MinusLogProbMetric: 17.1087, val_loss: 17.2887, val_MinusLogProbMetric: 17.2887

Epoch 892: val_loss did not improve from 17.25988
196/196 - 57s - loss: 17.1087 - MinusLogProbMetric: 17.1087 - val_loss: 17.2887 - val_MinusLogProbMetric: 17.2887 - lr: 6.9444e-06 - 57s/epoch - 293ms/step
Epoch 893/1000
2023-10-24 02:45:33.323 
Epoch 893/1000 
	 loss: 17.0956, MinusLogProbMetric: 17.0956, val_loss: 17.2675, val_MinusLogProbMetric: 17.2675

Epoch 893: val_loss did not improve from 17.25988
196/196 - 62s - loss: 17.0956 - MinusLogProbMetric: 17.0956 - val_loss: 17.2675 - val_MinusLogProbMetric: 17.2675 - lr: 6.9444e-06 - 62s/epoch - 314ms/step
Epoch 894/1000
2023-10-24 02:46:30.938 
Epoch 894/1000 
	 loss: 17.0971, MinusLogProbMetric: 17.0971, val_loss: 17.2660, val_MinusLogProbMetric: 17.2660

Epoch 894: val_loss did not improve from 17.25988
196/196 - 58s - loss: 17.0971 - MinusLogProbMetric: 17.0971 - val_loss: 17.2660 - val_MinusLogProbMetric: 17.2660 - lr: 6.9444e-06 - 58s/epoch - 294ms/step
Epoch 895/1000
2023-10-24 02:47:28.351 
Epoch 895/1000 
	 loss: 17.0968, MinusLogProbMetric: 17.0968, val_loss: 17.2610, val_MinusLogProbMetric: 17.2610

Epoch 895: val_loss did not improve from 17.25988
196/196 - 57s - loss: 17.0968 - MinusLogProbMetric: 17.0968 - val_loss: 17.2610 - val_MinusLogProbMetric: 17.2610 - lr: 6.9444e-06 - 57s/epoch - 293ms/step
Epoch 896/1000
2023-10-24 02:48:27.853 
Epoch 896/1000 
	 loss: 17.0924, MinusLogProbMetric: 17.0924, val_loss: 17.2726, val_MinusLogProbMetric: 17.2726

Epoch 896: val_loss did not improve from 17.25988
196/196 - 59s - loss: 17.0924 - MinusLogProbMetric: 17.0924 - val_loss: 17.2726 - val_MinusLogProbMetric: 17.2726 - lr: 6.9444e-06 - 59s/epoch - 304ms/step
Epoch 897/1000
2023-10-24 02:49:27.730 
Epoch 897/1000 
	 loss: 17.0942, MinusLogProbMetric: 17.0942, val_loss: 17.2658, val_MinusLogProbMetric: 17.2658

Epoch 897: val_loss did not improve from 17.25988
196/196 - 60s - loss: 17.0942 - MinusLogProbMetric: 17.0942 - val_loss: 17.2658 - val_MinusLogProbMetric: 17.2658 - lr: 6.9444e-06 - 60s/epoch - 305ms/step
Epoch 898/1000
2023-10-24 02:50:27.931 
Epoch 898/1000 
	 loss: 17.0888, MinusLogProbMetric: 17.0888, val_loss: 17.2699, val_MinusLogProbMetric: 17.2699

Epoch 898: val_loss did not improve from 17.25988
196/196 - 60s - loss: 17.0888 - MinusLogProbMetric: 17.0888 - val_loss: 17.2699 - val_MinusLogProbMetric: 17.2699 - lr: 6.9444e-06 - 60s/epoch - 307ms/step
Epoch 899/1000
2023-10-24 02:51:25.712 
Epoch 899/1000 
	 loss: 17.0907, MinusLogProbMetric: 17.0907, val_loss: 17.2605, val_MinusLogProbMetric: 17.2605

Epoch 899: val_loss did not improve from 17.25988
196/196 - 58s - loss: 17.0907 - MinusLogProbMetric: 17.0907 - val_loss: 17.2605 - val_MinusLogProbMetric: 17.2605 - lr: 6.9444e-06 - 58s/epoch - 295ms/step
Epoch 900/1000
2023-10-24 02:52:23.840 
Epoch 900/1000 
	 loss: 17.0972, MinusLogProbMetric: 17.0972, val_loss: 17.2677, val_MinusLogProbMetric: 17.2677

Epoch 900: val_loss did not improve from 17.25988
196/196 - 58s - loss: 17.0972 - MinusLogProbMetric: 17.0972 - val_loss: 17.2677 - val_MinusLogProbMetric: 17.2677 - lr: 6.9444e-06 - 58s/epoch - 297ms/step
Epoch 901/1000
2023-10-24 02:53:23.562 
Epoch 901/1000 
	 loss: 17.0858, MinusLogProbMetric: 17.0858, val_loss: 17.2867, val_MinusLogProbMetric: 17.2867

Epoch 901: val_loss did not improve from 17.25988
196/196 - 60s - loss: 17.0858 - MinusLogProbMetric: 17.0858 - val_loss: 17.2867 - val_MinusLogProbMetric: 17.2867 - lr: 6.9444e-06 - 60s/epoch - 305ms/step
Epoch 902/1000
2023-10-24 02:54:24.096 
Epoch 902/1000 
	 loss: 17.0903, MinusLogProbMetric: 17.0903, val_loss: 17.2610, val_MinusLogProbMetric: 17.2610

Epoch 902: val_loss did not improve from 17.25988
196/196 - 61s - loss: 17.0903 - MinusLogProbMetric: 17.0903 - val_loss: 17.2610 - val_MinusLogProbMetric: 17.2610 - lr: 6.9444e-06 - 61s/epoch - 309ms/step
Epoch 903/1000
2023-10-24 02:55:24.629 
Epoch 903/1000 
	 loss: 17.0923, MinusLogProbMetric: 17.0923, val_loss: 17.2772, val_MinusLogProbMetric: 17.2772

Epoch 903: val_loss did not improve from 17.25988
196/196 - 61s - loss: 17.0923 - MinusLogProbMetric: 17.0923 - val_loss: 17.2772 - val_MinusLogProbMetric: 17.2772 - lr: 6.9444e-06 - 61s/epoch - 309ms/step
Epoch 904/1000
2023-10-24 02:56:25.292 
Epoch 904/1000 
	 loss: 17.0971, MinusLogProbMetric: 17.0971, val_loss: 17.2689, val_MinusLogProbMetric: 17.2689

Epoch 904: val_loss did not improve from 17.25988
196/196 - 61s - loss: 17.0971 - MinusLogProbMetric: 17.0971 - val_loss: 17.2689 - val_MinusLogProbMetric: 17.2689 - lr: 6.9444e-06 - 61s/epoch - 309ms/step
Epoch 905/1000
2023-10-24 02:57:26.287 
Epoch 905/1000 
	 loss: 17.0901, MinusLogProbMetric: 17.0901, val_loss: 17.2573, val_MinusLogProbMetric: 17.2573

Epoch 905: val_loss improved from 17.25988 to 17.25729, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 62s - loss: 17.0901 - MinusLogProbMetric: 17.0901 - val_loss: 17.2573 - val_MinusLogProbMetric: 17.2573 - lr: 6.9444e-06 - 62s/epoch - 316ms/step
Epoch 906/1000
2023-10-24 02:58:25.550 
Epoch 906/1000 
	 loss: 17.0893, MinusLogProbMetric: 17.0893, val_loss: 17.2963, val_MinusLogProbMetric: 17.2963

Epoch 906: val_loss did not improve from 17.25729
196/196 - 58s - loss: 17.0893 - MinusLogProbMetric: 17.0893 - val_loss: 17.2963 - val_MinusLogProbMetric: 17.2963 - lr: 6.9444e-06 - 58s/epoch - 297ms/step
Epoch 907/1000
2023-10-24 02:59:23.571 
Epoch 907/1000 
	 loss: 17.0913, MinusLogProbMetric: 17.0913, val_loss: 17.2514, val_MinusLogProbMetric: 17.2514

Epoch 907: val_loss improved from 17.25729 to 17.25142, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 59s - loss: 17.0913 - MinusLogProbMetric: 17.0913 - val_loss: 17.2514 - val_MinusLogProbMetric: 17.2514 - lr: 6.9444e-06 - 59s/epoch - 301ms/step
Epoch 908/1000
2023-10-24 03:00:23.815 
Epoch 908/1000 
	 loss: 17.0897, MinusLogProbMetric: 17.0897, val_loss: 17.2525, val_MinusLogProbMetric: 17.2525

Epoch 908: val_loss did not improve from 17.25142
196/196 - 59s - loss: 17.0897 - MinusLogProbMetric: 17.0897 - val_loss: 17.2525 - val_MinusLogProbMetric: 17.2525 - lr: 6.9444e-06 - 59s/epoch - 302ms/step
Epoch 909/1000
2023-10-24 03:01:21.700 
Epoch 909/1000 
	 loss: 17.1130, MinusLogProbMetric: 17.1130, val_loss: 17.2918, val_MinusLogProbMetric: 17.2918

Epoch 909: val_loss did not improve from 17.25142
196/196 - 58s - loss: 17.1130 - MinusLogProbMetric: 17.1130 - val_loss: 17.2918 - val_MinusLogProbMetric: 17.2918 - lr: 6.9444e-06 - 58s/epoch - 295ms/step
Epoch 910/1000
2023-10-24 03:02:21.229 
Epoch 910/1000 
	 loss: 17.0954, MinusLogProbMetric: 17.0954, val_loss: 17.2644, val_MinusLogProbMetric: 17.2644

Epoch 910: val_loss did not improve from 17.25142
196/196 - 60s - loss: 17.0954 - MinusLogProbMetric: 17.0954 - val_loss: 17.2644 - val_MinusLogProbMetric: 17.2644 - lr: 6.9444e-06 - 60s/epoch - 304ms/step
Epoch 911/1000
2023-10-24 03:03:19.092 
Epoch 911/1000 
	 loss: 17.0898, MinusLogProbMetric: 17.0898, val_loss: 17.2684, val_MinusLogProbMetric: 17.2684

Epoch 911: val_loss did not improve from 17.25142
196/196 - 58s - loss: 17.0898 - MinusLogProbMetric: 17.0898 - val_loss: 17.2684 - val_MinusLogProbMetric: 17.2684 - lr: 6.9444e-06 - 58s/epoch - 295ms/step
Epoch 912/1000
2023-10-24 03:04:19.563 
Epoch 912/1000 
	 loss: 17.0871, MinusLogProbMetric: 17.0871, val_loss: 17.2609, val_MinusLogProbMetric: 17.2609

Epoch 912: val_loss did not improve from 17.25142
196/196 - 60s - loss: 17.0871 - MinusLogProbMetric: 17.0871 - val_loss: 17.2609 - val_MinusLogProbMetric: 17.2609 - lr: 6.9444e-06 - 60s/epoch - 309ms/step
Epoch 913/1000
2023-10-24 03:05:17.828 
Epoch 913/1000 
	 loss: 17.0939, MinusLogProbMetric: 17.0939, val_loss: 17.2630, val_MinusLogProbMetric: 17.2630

Epoch 913: val_loss did not improve from 17.25142
196/196 - 58s - loss: 17.0939 - MinusLogProbMetric: 17.0939 - val_loss: 17.2630 - val_MinusLogProbMetric: 17.2630 - lr: 6.9444e-06 - 58s/epoch - 297ms/step
Epoch 914/1000
2023-10-24 03:06:15.052 
Epoch 914/1000 
	 loss: 17.0843, MinusLogProbMetric: 17.0843, val_loss: 17.2577, val_MinusLogProbMetric: 17.2577

Epoch 914: val_loss did not improve from 17.25142
196/196 - 57s - loss: 17.0843 - MinusLogProbMetric: 17.0843 - val_loss: 17.2577 - val_MinusLogProbMetric: 17.2577 - lr: 6.9444e-06 - 57s/epoch - 292ms/step
Epoch 915/1000
2023-10-24 03:07:13.046 
Epoch 915/1000 
	 loss: 17.0927, MinusLogProbMetric: 17.0927, val_loss: 17.2603, val_MinusLogProbMetric: 17.2603

Epoch 915: val_loss did not improve from 17.25142
196/196 - 58s - loss: 17.0927 - MinusLogProbMetric: 17.0927 - val_loss: 17.2603 - val_MinusLogProbMetric: 17.2603 - lr: 6.9444e-06 - 58s/epoch - 296ms/step
Epoch 916/1000
2023-10-24 03:08:11.656 
Epoch 916/1000 
	 loss: 17.0915, MinusLogProbMetric: 17.0915, val_loss: 17.2580, val_MinusLogProbMetric: 17.2580

Epoch 916: val_loss did not improve from 17.25142
196/196 - 59s - loss: 17.0915 - MinusLogProbMetric: 17.0915 - val_loss: 17.2580 - val_MinusLogProbMetric: 17.2580 - lr: 6.9444e-06 - 59s/epoch - 299ms/step
Epoch 917/1000
2023-10-24 03:09:11.073 
Epoch 917/1000 
	 loss: 17.0984, MinusLogProbMetric: 17.0984, val_loss: 17.2712, val_MinusLogProbMetric: 17.2712

Epoch 917: val_loss did not improve from 17.25142
196/196 - 59s - loss: 17.0984 - MinusLogProbMetric: 17.0984 - val_loss: 17.2712 - val_MinusLogProbMetric: 17.2712 - lr: 6.9444e-06 - 59s/epoch - 303ms/step
Epoch 918/1000
2023-10-24 03:10:11.878 
Epoch 918/1000 
	 loss: 17.0883, MinusLogProbMetric: 17.0883, val_loss: 17.2780, val_MinusLogProbMetric: 17.2780

Epoch 918: val_loss did not improve from 17.25142
196/196 - 61s - loss: 17.0883 - MinusLogProbMetric: 17.0883 - val_loss: 17.2780 - val_MinusLogProbMetric: 17.2780 - lr: 6.9444e-06 - 61s/epoch - 310ms/step
Epoch 919/1000
2023-10-24 03:11:11.775 
Epoch 919/1000 
	 loss: 17.1074, MinusLogProbMetric: 17.1074, val_loss: 17.2707, val_MinusLogProbMetric: 17.2707

Epoch 919: val_loss did not improve from 17.25142
196/196 - 60s - loss: 17.1074 - MinusLogProbMetric: 17.1074 - val_loss: 17.2707 - val_MinusLogProbMetric: 17.2707 - lr: 6.9444e-06 - 60s/epoch - 306ms/step
Epoch 920/1000
2023-10-24 03:12:11.246 
Epoch 920/1000 
	 loss: 17.0896, MinusLogProbMetric: 17.0896, val_loss: 17.2644, val_MinusLogProbMetric: 17.2644

Epoch 920: val_loss did not improve from 17.25142
196/196 - 59s - loss: 17.0896 - MinusLogProbMetric: 17.0896 - val_loss: 17.2644 - val_MinusLogProbMetric: 17.2644 - lr: 6.9444e-06 - 59s/epoch - 303ms/step
Epoch 921/1000
2023-10-24 03:13:11.225 
Epoch 921/1000 
	 loss: 17.0923, MinusLogProbMetric: 17.0923, val_loss: 17.2734, val_MinusLogProbMetric: 17.2734

Epoch 921: val_loss did not improve from 17.25142
196/196 - 60s - loss: 17.0923 - MinusLogProbMetric: 17.0923 - val_loss: 17.2734 - val_MinusLogProbMetric: 17.2734 - lr: 6.9444e-06 - 60s/epoch - 306ms/step
Epoch 922/1000
2023-10-24 03:14:10.963 
Epoch 922/1000 
	 loss: 17.0905, MinusLogProbMetric: 17.0905, val_loss: 17.2570, val_MinusLogProbMetric: 17.2570

Epoch 922: val_loss did not improve from 17.25142
196/196 - 60s - loss: 17.0905 - MinusLogProbMetric: 17.0905 - val_loss: 17.2570 - val_MinusLogProbMetric: 17.2570 - lr: 6.9444e-06 - 60s/epoch - 305ms/step
Epoch 923/1000
2023-10-24 03:15:10.138 
Epoch 923/1000 
	 loss: 17.0887, MinusLogProbMetric: 17.0887, val_loss: 17.3299, val_MinusLogProbMetric: 17.3299

Epoch 923: val_loss did not improve from 17.25142
196/196 - 59s - loss: 17.0887 - MinusLogProbMetric: 17.0887 - val_loss: 17.3299 - val_MinusLogProbMetric: 17.3299 - lr: 6.9444e-06 - 59s/epoch - 302ms/step
Epoch 924/1000
2023-10-24 03:16:10.156 
Epoch 924/1000 
	 loss: 17.1026, MinusLogProbMetric: 17.1026, val_loss: 17.2944, val_MinusLogProbMetric: 17.2944

Epoch 924: val_loss did not improve from 17.25142
196/196 - 60s - loss: 17.1026 - MinusLogProbMetric: 17.1026 - val_loss: 17.2944 - val_MinusLogProbMetric: 17.2944 - lr: 6.9444e-06 - 60s/epoch - 306ms/step
Epoch 925/1000
2023-10-24 03:17:07.084 
Epoch 925/1000 
	 loss: 17.1044, MinusLogProbMetric: 17.1044, val_loss: 17.2698, val_MinusLogProbMetric: 17.2698

Epoch 925: val_loss did not improve from 17.25142
196/196 - 57s - loss: 17.1044 - MinusLogProbMetric: 17.1044 - val_loss: 17.2698 - val_MinusLogProbMetric: 17.2698 - lr: 6.9444e-06 - 57s/epoch - 290ms/step
Epoch 926/1000
2023-10-24 03:18:04.064 
Epoch 926/1000 
	 loss: 17.0963, MinusLogProbMetric: 17.0963, val_loss: 17.2615, val_MinusLogProbMetric: 17.2615

Epoch 926: val_loss did not improve from 17.25142
196/196 - 57s - loss: 17.0963 - MinusLogProbMetric: 17.0963 - val_loss: 17.2615 - val_MinusLogProbMetric: 17.2615 - lr: 6.9444e-06 - 57s/epoch - 291ms/step
Epoch 927/1000
2023-10-24 03:19:06.458 
Epoch 927/1000 
	 loss: 17.0927, MinusLogProbMetric: 17.0927, val_loss: 17.2596, val_MinusLogProbMetric: 17.2596

Epoch 927: val_loss did not improve from 17.25142
196/196 - 62s - loss: 17.0927 - MinusLogProbMetric: 17.0927 - val_loss: 17.2596 - val_MinusLogProbMetric: 17.2596 - lr: 6.9444e-06 - 62s/epoch - 318ms/step
Epoch 928/1000
2023-10-24 03:20:09.628 
Epoch 928/1000 
	 loss: 17.0945, MinusLogProbMetric: 17.0945, val_loss: 17.2617, val_MinusLogProbMetric: 17.2617

Epoch 928: val_loss did not improve from 17.25142
196/196 - 63s - loss: 17.0945 - MinusLogProbMetric: 17.0945 - val_loss: 17.2617 - val_MinusLogProbMetric: 17.2617 - lr: 6.9444e-06 - 63s/epoch - 322ms/step
Epoch 929/1000
2023-10-24 03:21:07.215 
Epoch 929/1000 
	 loss: 17.0975, MinusLogProbMetric: 17.0975, val_loss: 17.2579, val_MinusLogProbMetric: 17.2579

Epoch 929: val_loss did not improve from 17.25142
196/196 - 58s - loss: 17.0975 - MinusLogProbMetric: 17.0975 - val_loss: 17.2579 - val_MinusLogProbMetric: 17.2579 - lr: 6.9444e-06 - 58s/epoch - 294ms/step
Epoch 930/1000
2023-10-24 03:22:07.314 
Epoch 930/1000 
	 loss: 17.0940, MinusLogProbMetric: 17.0940, val_loss: 17.2832, val_MinusLogProbMetric: 17.2832

Epoch 930: val_loss did not improve from 17.25142
196/196 - 60s - loss: 17.0940 - MinusLogProbMetric: 17.0940 - val_loss: 17.2832 - val_MinusLogProbMetric: 17.2832 - lr: 6.9444e-06 - 60s/epoch - 307ms/step
Epoch 931/1000
2023-10-24 03:23:05.688 
Epoch 931/1000 
	 loss: 17.0885, MinusLogProbMetric: 17.0885, val_loss: 17.2936, val_MinusLogProbMetric: 17.2936

Epoch 931: val_loss did not improve from 17.25142
196/196 - 58s - loss: 17.0885 - MinusLogProbMetric: 17.0885 - val_loss: 17.2936 - val_MinusLogProbMetric: 17.2936 - lr: 6.9444e-06 - 58s/epoch - 298ms/step
Epoch 932/1000
2023-10-24 03:24:05.608 
Epoch 932/1000 
	 loss: 17.0890, MinusLogProbMetric: 17.0890, val_loss: 17.3041, val_MinusLogProbMetric: 17.3041

Epoch 932: val_loss did not improve from 17.25142
196/196 - 60s - loss: 17.0890 - MinusLogProbMetric: 17.0890 - val_loss: 17.3041 - val_MinusLogProbMetric: 17.3041 - lr: 6.9444e-06 - 60s/epoch - 306ms/step
Epoch 933/1000
2023-10-24 03:25:03.212 
Epoch 933/1000 
	 loss: 17.0858, MinusLogProbMetric: 17.0858, val_loss: 17.2596, val_MinusLogProbMetric: 17.2596

Epoch 933: val_loss did not improve from 17.25142
196/196 - 58s - loss: 17.0858 - MinusLogProbMetric: 17.0858 - val_loss: 17.2596 - val_MinusLogProbMetric: 17.2596 - lr: 6.9444e-06 - 58s/epoch - 294ms/step
Epoch 934/1000
2023-10-24 03:26:00.764 
Epoch 934/1000 
	 loss: 17.0919, MinusLogProbMetric: 17.0919, val_loss: 17.2833, val_MinusLogProbMetric: 17.2833

Epoch 934: val_loss did not improve from 17.25142
196/196 - 58s - loss: 17.0919 - MinusLogProbMetric: 17.0919 - val_loss: 17.2833 - val_MinusLogProbMetric: 17.2833 - lr: 6.9444e-06 - 58s/epoch - 294ms/step
Epoch 935/1000
2023-10-24 03:26:56.925 
Epoch 935/1000 
	 loss: 17.0837, MinusLogProbMetric: 17.0837, val_loss: 17.2589, val_MinusLogProbMetric: 17.2589

Epoch 935: val_loss did not improve from 17.25142
196/196 - 56s - loss: 17.0837 - MinusLogProbMetric: 17.0837 - val_loss: 17.2589 - val_MinusLogProbMetric: 17.2589 - lr: 6.9444e-06 - 56s/epoch - 287ms/step
Epoch 936/1000
2023-10-24 03:27:56.515 
Epoch 936/1000 
	 loss: 17.0947, MinusLogProbMetric: 17.0947, val_loss: 17.2836, val_MinusLogProbMetric: 17.2836

Epoch 936: val_loss did not improve from 17.25142
196/196 - 60s - loss: 17.0947 - MinusLogProbMetric: 17.0947 - val_loss: 17.2836 - val_MinusLogProbMetric: 17.2836 - lr: 6.9444e-06 - 60s/epoch - 304ms/step
Epoch 937/1000
2023-10-24 03:28:53.877 
Epoch 937/1000 
	 loss: 17.0869, MinusLogProbMetric: 17.0869, val_loss: 17.2506, val_MinusLogProbMetric: 17.2506

Epoch 937: val_loss improved from 17.25142 to 17.25057, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 58s - loss: 17.0869 - MinusLogProbMetric: 17.0869 - val_loss: 17.2506 - val_MinusLogProbMetric: 17.2506 - lr: 6.9444e-06 - 58s/epoch - 298ms/step
Epoch 938/1000
2023-10-24 03:29:51.588 
Epoch 938/1000 
	 loss: 17.0952, MinusLogProbMetric: 17.0952, val_loss: 17.2611, val_MinusLogProbMetric: 17.2611

Epoch 938: val_loss did not improve from 17.25057
196/196 - 57s - loss: 17.0952 - MinusLogProbMetric: 17.0952 - val_loss: 17.2611 - val_MinusLogProbMetric: 17.2611 - lr: 6.9444e-06 - 57s/epoch - 290ms/step
Epoch 939/1000
2023-10-24 03:30:48.506 
Epoch 939/1000 
	 loss: 17.0996, MinusLogProbMetric: 17.0996, val_loss: 17.2605, val_MinusLogProbMetric: 17.2605

Epoch 939: val_loss did not improve from 17.25057
196/196 - 57s - loss: 17.0996 - MinusLogProbMetric: 17.0996 - val_loss: 17.2605 - val_MinusLogProbMetric: 17.2605 - lr: 6.9444e-06 - 57s/epoch - 290ms/step
Epoch 940/1000
2023-10-24 03:31:48.267 
Epoch 940/1000 
	 loss: 17.0977, MinusLogProbMetric: 17.0977, val_loss: 17.2689, val_MinusLogProbMetric: 17.2689

Epoch 940: val_loss did not improve from 17.25057
196/196 - 60s - loss: 17.0977 - MinusLogProbMetric: 17.0977 - val_loss: 17.2689 - val_MinusLogProbMetric: 17.2689 - lr: 6.9444e-06 - 60s/epoch - 305ms/step
Epoch 941/1000
2023-10-24 03:32:46.835 
Epoch 941/1000 
	 loss: 17.0893, MinusLogProbMetric: 17.0893, val_loss: 17.2670, val_MinusLogProbMetric: 17.2670

Epoch 941: val_loss did not improve from 17.25057
196/196 - 59s - loss: 17.0893 - MinusLogProbMetric: 17.0893 - val_loss: 17.2670 - val_MinusLogProbMetric: 17.2670 - lr: 6.9444e-06 - 59s/epoch - 299ms/step
Epoch 942/1000
2023-10-24 03:33:45.132 
Epoch 942/1000 
	 loss: 17.1014, MinusLogProbMetric: 17.1014, val_loss: 17.2668, val_MinusLogProbMetric: 17.2668

Epoch 942: val_loss did not improve from 17.25057
196/196 - 58s - loss: 17.1014 - MinusLogProbMetric: 17.1014 - val_loss: 17.2668 - val_MinusLogProbMetric: 17.2668 - lr: 6.9444e-06 - 58s/epoch - 297ms/step
Epoch 943/1000
2023-10-24 03:34:41.702 
Epoch 943/1000 
	 loss: 17.0898, MinusLogProbMetric: 17.0898, val_loss: 17.2633, val_MinusLogProbMetric: 17.2633

Epoch 943: val_loss did not improve from 17.25057
196/196 - 57s - loss: 17.0898 - MinusLogProbMetric: 17.0898 - val_loss: 17.2633 - val_MinusLogProbMetric: 17.2633 - lr: 6.9444e-06 - 57s/epoch - 289ms/step
Epoch 944/1000
2023-10-24 03:35:39.008 
Epoch 944/1000 
	 loss: 17.0956, MinusLogProbMetric: 17.0956, val_loss: 17.2624, val_MinusLogProbMetric: 17.2624

Epoch 944: val_loss did not improve from 17.25057
196/196 - 57s - loss: 17.0956 - MinusLogProbMetric: 17.0956 - val_loss: 17.2624 - val_MinusLogProbMetric: 17.2624 - lr: 6.9444e-06 - 57s/epoch - 292ms/step
Epoch 945/1000
2023-10-24 03:36:38.854 
Epoch 945/1000 
	 loss: 17.0849, MinusLogProbMetric: 17.0849, val_loss: 17.2716, val_MinusLogProbMetric: 17.2716

Epoch 945: val_loss did not improve from 17.25057
196/196 - 60s - loss: 17.0849 - MinusLogProbMetric: 17.0849 - val_loss: 17.2716 - val_MinusLogProbMetric: 17.2716 - lr: 6.9444e-06 - 60s/epoch - 305ms/step
Epoch 946/1000
2023-10-24 03:37:35.322 
Epoch 946/1000 
	 loss: 17.0833, MinusLogProbMetric: 17.0833, val_loss: 17.2594, val_MinusLogProbMetric: 17.2594

Epoch 946: val_loss did not improve from 17.25057
196/196 - 56s - loss: 17.0833 - MinusLogProbMetric: 17.0833 - val_loss: 17.2594 - val_MinusLogProbMetric: 17.2594 - lr: 6.9444e-06 - 56s/epoch - 288ms/step
Epoch 947/1000
2023-10-24 03:38:34.754 
Epoch 947/1000 
	 loss: 17.0836, MinusLogProbMetric: 17.0836, val_loss: 17.2885, val_MinusLogProbMetric: 17.2885

Epoch 947: val_loss did not improve from 17.25057
196/196 - 59s - loss: 17.0836 - MinusLogProbMetric: 17.0836 - val_loss: 17.2885 - val_MinusLogProbMetric: 17.2885 - lr: 6.9444e-06 - 59s/epoch - 303ms/step
Epoch 948/1000
2023-10-24 03:39:34.535 
Epoch 948/1000 
	 loss: 17.0953, MinusLogProbMetric: 17.0953, val_loss: 17.2789, val_MinusLogProbMetric: 17.2789

Epoch 948: val_loss did not improve from 17.25057
196/196 - 60s - loss: 17.0953 - MinusLogProbMetric: 17.0953 - val_loss: 17.2789 - val_MinusLogProbMetric: 17.2789 - lr: 6.9444e-06 - 60s/epoch - 305ms/step
Epoch 949/1000
2023-10-24 03:40:33.552 
Epoch 949/1000 
	 loss: 17.0939, MinusLogProbMetric: 17.0939, val_loss: 17.2557, val_MinusLogProbMetric: 17.2557

Epoch 949: val_loss did not improve from 17.25057
196/196 - 59s - loss: 17.0939 - MinusLogProbMetric: 17.0939 - val_loss: 17.2557 - val_MinusLogProbMetric: 17.2557 - lr: 6.9444e-06 - 59s/epoch - 301ms/step
Epoch 950/1000
2023-10-24 03:41:32.912 
Epoch 950/1000 
	 loss: 17.1019, MinusLogProbMetric: 17.1019, val_loss: 17.2576, val_MinusLogProbMetric: 17.2576

Epoch 950: val_loss did not improve from 17.25057
196/196 - 59s - loss: 17.1019 - MinusLogProbMetric: 17.1019 - val_loss: 17.2576 - val_MinusLogProbMetric: 17.2576 - lr: 6.9444e-06 - 59s/epoch - 303ms/step
Epoch 951/1000
2023-10-24 03:42:30.041 
Epoch 951/1000 
	 loss: 17.0891, MinusLogProbMetric: 17.0891, val_loss: 17.2824, val_MinusLogProbMetric: 17.2824

Epoch 951: val_loss did not improve from 17.25057
196/196 - 57s - loss: 17.0891 - MinusLogProbMetric: 17.0891 - val_loss: 17.2824 - val_MinusLogProbMetric: 17.2824 - lr: 6.9444e-06 - 57s/epoch - 291ms/step
Epoch 952/1000
2023-10-24 03:43:29.881 
Epoch 952/1000 
	 loss: 17.0913, MinusLogProbMetric: 17.0913, val_loss: 17.2686, val_MinusLogProbMetric: 17.2686

Epoch 952: val_loss did not improve from 17.25057
196/196 - 60s - loss: 17.0913 - MinusLogProbMetric: 17.0913 - val_loss: 17.2686 - val_MinusLogProbMetric: 17.2686 - lr: 6.9444e-06 - 60s/epoch - 305ms/step
Epoch 953/1000
2023-10-24 03:44:28.069 
Epoch 953/1000 
	 loss: 17.0979, MinusLogProbMetric: 17.0979, val_loss: 17.2622, val_MinusLogProbMetric: 17.2622

Epoch 953: val_loss did not improve from 17.25057
196/196 - 58s - loss: 17.0979 - MinusLogProbMetric: 17.0979 - val_loss: 17.2622 - val_MinusLogProbMetric: 17.2622 - lr: 6.9444e-06 - 58s/epoch - 297ms/step
Epoch 954/1000
2023-10-24 03:45:25.644 
Epoch 954/1000 
	 loss: 17.0860, MinusLogProbMetric: 17.0860, val_loss: 17.2538, val_MinusLogProbMetric: 17.2538

Epoch 954: val_loss did not improve from 17.25057
196/196 - 58s - loss: 17.0860 - MinusLogProbMetric: 17.0860 - val_loss: 17.2538 - val_MinusLogProbMetric: 17.2538 - lr: 6.9444e-06 - 58s/epoch - 294ms/step
Epoch 955/1000
2023-10-24 03:46:23.333 
Epoch 955/1000 
	 loss: 17.0875, MinusLogProbMetric: 17.0875, val_loss: 17.2630, val_MinusLogProbMetric: 17.2630

Epoch 955: val_loss did not improve from 17.25057
196/196 - 58s - loss: 17.0875 - MinusLogProbMetric: 17.0875 - val_loss: 17.2630 - val_MinusLogProbMetric: 17.2630 - lr: 6.9444e-06 - 58s/epoch - 294ms/step
Epoch 956/1000
2023-10-24 03:47:25.733 
Epoch 956/1000 
	 loss: 17.0873, MinusLogProbMetric: 17.0873, val_loss: 17.2489, val_MinusLogProbMetric: 17.2489

Epoch 956: val_loss improved from 17.25057 to 17.24889, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 63s - loss: 17.0873 - MinusLogProbMetric: 17.0873 - val_loss: 17.2489 - val_MinusLogProbMetric: 17.2489 - lr: 6.9444e-06 - 63s/epoch - 323ms/step
Epoch 957/1000
2023-10-24 03:48:25.791 
Epoch 957/1000 
	 loss: 17.0859, MinusLogProbMetric: 17.0859, val_loss: 17.2798, val_MinusLogProbMetric: 17.2798

Epoch 957: val_loss did not improve from 17.24889
196/196 - 59s - loss: 17.0859 - MinusLogProbMetric: 17.0859 - val_loss: 17.2798 - val_MinusLogProbMetric: 17.2798 - lr: 6.9444e-06 - 59s/epoch - 302ms/step
Epoch 958/1000
2023-10-24 03:49:27.457 
Epoch 958/1000 
	 loss: 17.0953, MinusLogProbMetric: 17.0953, val_loss: 17.2576, val_MinusLogProbMetric: 17.2576

Epoch 958: val_loss did not improve from 17.24889
196/196 - 62s - loss: 17.0953 - MinusLogProbMetric: 17.0953 - val_loss: 17.2576 - val_MinusLogProbMetric: 17.2576 - lr: 6.9444e-06 - 62s/epoch - 315ms/step
Epoch 959/1000
2023-10-24 03:50:26.099 
Epoch 959/1000 
	 loss: 17.1022, MinusLogProbMetric: 17.1022, val_loss: 17.2617, val_MinusLogProbMetric: 17.2617

Epoch 959: val_loss did not improve from 17.24889
196/196 - 59s - loss: 17.1022 - MinusLogProbMetric: 17.1022 - val_loss: 17.2617 - val_MinusLogProbMetric: 17.2617 - lr: 6.9444e-06 - 59s/epoch - 299ms/step
Epoch 960/1000
2023-10-24 03:51:26.348 
Epoch 960/1000 
	 loss: 17.0914, MinusLogProbMetric: 17.0914, val_loss: 17.2616, val_MinusLogProbMetric: 17.2616

Epoch 960: val_loss did not improve from 17.24889
196/196 - 60s - loss: 17.0914 - MinusLogProbMetric: 17.0914 - val_loss: 17.2616 - val_MinusLogProbMetric: 17.2616 - lr: 6.9444e-06 - 60s/epoch - 307ms/step
Epoch 961/1000
2023-10-24 03:52:24.754 
Epoch 961/1000 
	 loss: 17.0948, MinusLogProbMetric: 17.0948, val_loss: 17.2631, val_MinusLogProbMetric: 17.2631

Epoch 961: val_loss did not improve from 17.24889
196/196 - 58s - loss: 17.0948 - MinusLogProbMetric: 17.0948 - val_loss: 17.2631 - val_MinusLogProbMetric: 17.2631 - lr: 6.9444e-06 - 58s/epoch - 298ms/step
Epoch 962/1000
2023-10-24 03:53:25.348 
Epoch 962/1000 
	 loss: 17.0905, MinusLogProbMetric: 17.0905, val_loss: 17.2707, val_MinusLogProbMetric: 17.2707

Epoch 962: val_loss did not improve from 17.24889
196/196 - 61s - loss: 17.0905 - MinusLogProbMetric: 17.0905 - val_loss: 17.2707 - val_MinusLogProbMetric: 17.2707 - lr: 6.9444e-06 - 61s/epoch - 309ms/step
Epoch 963/1000
2023-10-24 03:54:24.128 
Epoch 963/1000 
	 loss: 17.0855, MinusLogProbMetric: 17.0855, val_loss: 17.2955, val_MinusLogProbMetric: 17.2955

Epoch 963: val_loss did not improve from 17.24889
196/196 - 59s - loss: 17.0855 - MinusLogProbMetric: 17.0855 - val_loss: 17.2955 - val_MinusLogProbMetric: 17.2955 - lr: 6.9444e-06 - 59s/epoch - 300ms/step
Epoch 964/1000
2023-10-24 03:55:22.338 
Epoch 964/1000 
	 loss: 17.0895, MinusLogProbMetric: 17.0895, val_loss: 17.2559, val_MinusLogProbMetric: 17.2559

Epoch 964: val_loss did not improve from 17.24889
196/196 - 58s - loss: 17.0895 - MinusLogProbMetric: 17.0895 - val_loss: 17.2559 - val_MinusLogProbMetric: 17.2559 - lr: 6.9444e-06 - 58s/epoch - 297ms/step
Epoch 965/1000
2023-10-24 03:56:19.424 
Epoch 965/1000 
	 loss: 17.0893, MinusLogProbMetric: 17.0893, val_loss: 17.2514, val_MinusLogProbMetric: 17.2514

Epoch 965: val_loss did not improve from 17.24889
196/196 - 57s - loss: 17.0893 - MinusLogProbMetric: 17.0893 - val_loss: 17.2514 - val_MinusLogProbMetric: 17.2514 - lr: 6.9444e-06 - 57s/epoch - 291ms/step
Epoch 966/1000
2023-10-24 03:57:17.419 
Epoch 966/1000 
	 loss: 17.0832, MinusLogProbMetric: 17.0832, val_loss: 17.2645, val_MinusLogProbMetric: 17.2645

Epoch 966: val_loss did not improve from 17.24889
196/196 - 58s - loss: 17.0832 - MinusLogProbMetric: 17.0832 - val_loss: 17.2645 - val_MinusLogProbMetric: 17.2645 - lr: 6.9444e-06 - 58s/epoch - 296ms/step
Epoch 967/1000
2023-10-24 03:58:21.818 
Epoch 967/1000 
	 loss: 17.0873, MinusLogProbMetric: 17.0873, val_loss: 17.2598, val_MinusLogProbMetric: 17.2598

Epoch 967: val_loss did not improve from 17.24889
196/196 - 64s - loss: 17.0873 - MinusLogProbMetric: 17.0873 - val_loss: 17.2598 - val_MinusLogProbMetric: 17.2598 - lr: 6.9444e-06 - 64s/epoch - 329ms/step
Epoch 968/1000
2023-10-24 03:59:25.338 
Epoch 968/1000 
	 loss: 17.0820, MinusLogProbMetric: 17.0820, val_loss: 17.2825, val_MinusLogProbMetric: 17.2825

Epoch 968: val_loss did not improve from 17.24889
196/196 - 64s - loss: 17.0820 - MinusLogProbMetric: 17.0820 - val_loss: 17.2825 - val_MinusLogProbMetric: 17.2825 - lr: 6.9444e-06 - 64s/epoch - 324ms/step
Epoch 969/1000
2023-10-24 04:00:24.883 
Epoch 969/1000 
	 loss: 17.0906, MinusLogProbMetric: 17.0906, val_loss: 17.2523, val_MinusLogProbMetric: 17.2523

Epoch 969: val_loss did not improve from 17.24889
196/196 - 60s - loss: 17.0906 - MinusLogProbMetric: 17.0906 - val_loss: 17.2523 - val_MinusLogProbMetric: 17.2523 - lr: 6.9444e-06 - 60s/epoch - 304ms/step
Epoch 970/1000
2023-10-24 04:01:22.353 
Epoch 970/1000 
	 loss: 17.0826, MinusLogProbMetric: 17.0826, val_loss: 17.2607, val_MinusLogProbMetric: 17.2607

Epoch 970: val_loss did not improve from 17.24889
196/196 - 57s - loss: 17.0826 - MinusLogProbMetric: 17.0826 - val_loss: 17.2607 - val_MinusLogProbMetric: 17.2607 - lr: 6.9444e-06 - 57s/epoch - 293ms/step
Epoch 971/1000
2023-10-24 04:02:21.047 
Epoch 971/1000 
	 loss: 17.0879, MinusLogProbMetric: 17.0879, val_loss: 17.2599, val_MinusLogProbMetric: 17.2599

Epoch 971: val_loss did not improve from 17.24889
196/196 - 59s - loss: 17.0879 - MinusLogProbMetric: 17.0879 - val_loss: 17.2599 - val_MinusLogProbMetric: 17.2599 - lr: 6.9444e-06 - 59s/epoch - 299ms/step
Epoch 972/1000
2023-10-24 04:03:19.132 
Epoch 972/1000 
	 loss: 17.0870, MinusLogProbMetric: 17.0870, val_loss: 17.2609, val_MinusLogProbMetric: 17.2609

Epoch 972: val_loss did not improve from 17.24889
196/196 - 58s - loss: 17.0870 - MinusLogProbMetric: 17.0870 - val_loss: 17.2609 - val_MinusLogProbMetric: 17.2609 - lr: 6.9444e-06 - 58s/epoch - 296ms/step
Epoch 973/1000
2023-10-24 04:04:16.059 
Epoch 973/1000 
	 loss: 17.0815, MinusLogProbMetric: 17.0815, val_loss: 17.2805, val_MinusLogProbMetric: 17.2805

Epoch 973: val_loss did not improve from 17.24889
196/196 - 57s - loss: 17.0815 - MinusLogProbMetric: 17.0815 - val_loss: 17.2805 - val_MinusLogProbMetric: 17.2805 - lr: 6.9444e-06 - 57s/epoch - 290ms/step
Epoch 974/1000
2023-10-24 04:05:15.370 
Epoch 974/1000 
	 loss: 17.0911, MinusLogProbMetric: 17.0911, val_loss: 17.2891, val_MinusLogProbMetric: 17.2891

Epoch 974: val_loss did not improve from 17.24889
196/196 - 59s - loss: 17.0911 - MinusLogProbMetric: 17.0911 - val_loss: 17.2891 - val_MinusLogProbMetric: 17.2891 - lr: 6.9444e-06 - 59s/epoch - 303ms/step
Epoch 975/1000
2023-10-24 04:06:15.637 
Epoch 975/1000 
	 loss: 17.0900, MinusLogProbMetric: 17.0900, val_loss: 17.2599, val_MinusLogProbMetric: 17.2599

Epoch 975: val_loss did not improve from 17.24889
196/196 - 60s - loss: 17.0900 - MinusLogProbMetric: 17.0900 - val_loss: 17.2599 - val_MinusLogProbMetric: 17.2599 - lr: 6.9444e-06 - 60s/epoch - 307ms/step
Epoch 976/1000
2023-10-24 04:07:15.607 
Epoch 976/1000 
	 loss: 17.0825, MinusLogProbMetric: 17.0825, val_loss: 17.2702, val_MinusLogProbMetric: 17.2702

Epoch 976: val_loss did not improve from 17.24889
196/196 - 60s - loss: 17.0825 - MinusLogProbMetric: 17.0825 - val_loss: 17.2702 - val_MinusLogProbMetric: 17.2702 - lr: 6.9444e-06 - 60s/epoch - 306ms/step
Epoch 977/1000
2023-10-24 04:08:12.422 
Epoch 977/1000 
	 loss: 17.0927, MinusLogProbMetric: 17.0927, val_loss: 17.2692, val_MinusLogProbMetric: 17.2692

Epoch 977: val_loss did not improve from 17.24889
196/196 - 57s - loss: 17.0927 - MinusLogProbMetric: 17.0927 - val_loss: 17.2692 - val_MinusLogProbMetric: 17.2692 - lr: 6.9444e-06 - 57s/epoch - 290ms/step
Epoch 978/1000
2023-10-24 04:09:09.642 
Epoch 978/1000 
	 loss: 17.1004, MinusLogProbMetric: 17.1004, val_loss: 17.2650, val_MinusLogProbMetric: 17.2650

Epoch 978: val_loss did not improve from 17.24889
196/196 - 57s - loss: 17.1004 - MinusLogProbMetric: 17.1004 - val_loss: 17.2650 - val_MinusLogProbMetric: 17.2650 - lr: 6.9444e-06 - 57s/epoch - 292ms/step
Epoch 979/1000
2023-10-24 04:10:06.637 
Epoch 979/1000 
	 loss: 17.0835, MinusLogProbMetric: 17.0835, val_loss: 17.2798, val_MinusLogProbMetric: 17.2798

Epoch 979: val_loss did not improve from 17.24889
196/196 - 57s - loss: 17.0835 - MinusLogProbMetric: 17.0835 - val_loss: 17.2798 - val_MinusLogProbMetric: 17.2798 - lr: 6.9444e-06 - 57s/epoch - 291ms/step
Epoch 980/1000
2023-10-24 04:11:04.984 
Epoch 980/1000 
	 loss: 17.0839, MinusLogProbMetric: 17.0839, val_loss: 17.2562, val_MinusLogProbMetric: 17.2562

Epoch 980: val_loss did not improve from 17.24889
196/196 - 58s - loss: 17.0839 - MinusLogProbMetric: 17.0839 - val_loss: 17.2562 - val_MinusLogProbMetric: 17.2562 - lr: 6.9444e-06 - 58s/epoch - 298ms/step
Epoch 981/1000
2023-10-24 04:12:05.955 
Epoch 981/1000 
	 loss: 17.0874, MinusLogProbMetric: 17.0874, val_loss: 17.2743, val_MinusLogProbMetric: 17.2743

Epoch 981: val_loss did not improve from 17.24889
196/196 - 61s - loss: 17.0874 - MinusLogProbMetric: 17.0874 - val_loss: 17.2743 - val_MinusLogProbMetric: 17.2743 - lr: 6.9444e-06 - 61s/epoch - 311ms/step
Epoch 982/1000
2023-10-24 04:13:05.452 
Epoch 982/1000 
	 loss: 17.0807, MinusLogProbMetric: 17.0807, val_loss: 17.2706, val_MinusLogProbMetric: 17.2706

Epoch 982: val_loss did not improve from 17.24889
196/196 - 59s - loss: 17.0807 - MinusLogProbMetric: 17.0807 - val_loss: 17.2706 - val_MinusLogProbMetric: 17.2706 - lr: 6.9444e-06 - 59s/epoch - 304ms/step
Epoch 983/1000
2023-10-24 04:14:01.977 
Epoch 983/1000 
	 loss: 17.0846, MinusLogProbMetric: 17.0846, val_loss: 17.2529, val_MinusLogProbMetric: 17.2529

Epoch 983: val_loss did not improve from 17.24889
196/196 - 57s - loss: 17.0846 - MinusLogProbMetric: 17.0846 - val_loss: 17.2529 - val_MinusLogProbMetric: 17.2529 - lr: 6.9444e-06 - 57s/epoch - 288ms/step
Epoch 984/1000
2023-10-24 04:14:59.108 
Epoch 984/1000 
	 loss: 17.0964, MinusLogProbMetric: 17.0964, val_loss: 17.2686, val_MinusLogProbMetric: 17.2686

Epoch 984: val_loss did not improve from 17.24889
196/196 - 57s - loss: 17.0964 - MinusLogProbMetric: 17.0964 - val_loss: 17.2686 - val_MinusLogProbMetric: 17.2686 - lr: 6.9444e-06 - 57s/epoch - 291ms/step
Epoch 985/1000
2023-10-24 04:15:57.774 
Epoch 985/1000 
	 loss: 17.1014, MinusLogProbMetric: 17.1014, val_loss: 17.2606, val_MinusLogProbMetric: 17.2606

Epoch 985: val_loss did not improve from 17.24889
196/196 - 59s - loss: 17.1014 - MinusLogProbMetric: 17.1014 - val_loss: 17.2606 - val_MinusLogProbMetric: 17.2606 - lr: 6.9444e-06 - 59s/epoch - 299ms/step
Epoch 986/1000
2023-10-24 04:16:56.513 
Epoch 986/1000 
	 loss: 17.0898, MinusLogProbMetric: 17.0898, val_loss: 17.2551, val_MinusLogProbMetric: 17.2551

Epoch 986: val_loss did not improve from 17.24889
196/196 - 59s - loss: 17.0898 - MinusLogProbMetric: 17.0898 - val_loss: 17.2551 - val_MinusLogProbMetric: 17.2551 - lr: 6.9444e-06 - 59s/epoch - 300ms/step
Epoch 987/1000
2023-10-24 04:17:54.317 
Epoch 987/1000 
	 loss: 17.0869, MinusLogProbMetric: 17.0869, val_loss: 17.2537, val_MinusLogProbMetric: 17.2537

Epoch 987: val_loss did not improve from 17.24889
196/196 - 58s - loss: 17.0869 - MinusLogProbMetric: 17.0869 - val_loss: 17.2537 - val_MinusLogProbMetric: 17.2537 - lr: 6.9444e-06 - 58s/epoch - 295ms/step
Epoch 988/1000
2023-10-24 04:18:55.653 
Epoch 988/1000 
	 loss: 17.0837, MinusLogProbMetric: 17.0837, val_loss: 17.2623, val_MinusLogProbMetric: 17.2623

Epoch 988: val_loss did not improve from 17.24889
196/196 - 61s - loss: 17.0837 - MinusLogProbMetric: 17.0837 - val_loss: 17.2623 - val_MinusLogProbMetric: 17.2623 - lr: 6.9444e-06 - 61s/epoch - 313ms/step
Epoch 989/1000
2023-10-24 04:19:56.292 
Epoch 989/1000 
	 loss: 17.0854, MinusLogProbMetric: 17.0854, val_loss: 17.2615, val_MinusLogProbMetric: 17.2615

Epoch 989: val_loss did not improve from 17.24889
196/196 - 61s - loss: 17.0854 - MinusLogProbMetric: 17.0854 - val_loss: 17.2615 - val_MinusLogProbMetric: 17.2615 - lr: 6.9444e-06 - 61s/epoch - 309ms/step
Epoch 990/1000
2023-10-24 04:20:53.902 
Epoch 990/1000 
	 loss: 17.0820, MinusLogProbMetric: 17.0820, val_loss: 17.2486, val_MinusLogProbMetric: 17.2486

Epoch 990: val_loss improved from 17.24889 to 17.24860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 58s - loss: 17.0820 - MinusLogProbMetric: 17.0820 - val_loss: 17.2486 - val_MinusLogProbMetric: 17.2486 - lr: 6.9444e-06 - 58s/epoch - 298ms/step
Epoch 991/1000
2023-10-24 04:21:54.856 
Epoch 991/1000 
	 loss: 17.0853, MinusLogProbMetric: 17.0853, val_loss: 17.2969, val_MinusLogProbMetric: 17.2969

Epoch 991: val_loss did not improve from 17.24860
196/196 - 60s - loss: 17.0853 - MinusLogProbMetric: 17.0853 - val_loss: 17.2969 - val_MinusLogProbMetric: 17.2969 - lr: 6.9444e-06 - 60s/epoch - 307ms/step
Epoch 992/1000
2023-10-24 04:22:53.144 
Epoch 992/1000 
	 loss: 17.0871, MinusLogProbMetric: 17.0871, val_loss: 17.2697, val_MinusLogProbMetric: 17.2697

Epoch 992: val_loss did not improve from 17.24860
196/196 - 58s - loss: 17.0871 - MinusLogProbMetric: 17.0871 - val_loss: 17.2697 - val_MinusLogProbMetric: 17.2697 - lr: 6.9444e-06 - 58s/epoch - 297ms/step
Epoch 993/1000
2023-10-24 04:23:51.419 
Epoch 993/1000 
	 loss: 17.0949, MinusLogProbMetric: 17.0949, val_loss: 17.2544, val_MinusLogProbMetric: 17.2544

Epoch 993: val_loss did not improve from 17.24860
196/196 - 58s - loss: 17.0949 - MinusLogProbMetric: 17.0949 - val_loss: 17.2544 - val_MinusLogProbMetric: 17.2544 - lr: 6.9444e-06 - 58s/epoch - 297ms/step
Epoch 994/1000
2023-10-24 04:24:49.338 
Epoch 994/1000 
	 loss: 17.1256, MinusLogProbMetric: 17.1256, val_loss: 17.2882, val_MinusLogProbMetric: 17.2882

Epoch 994: val_loss did not improve from 17.24860
196/196 - 58s - loss: 17.1256 - MinusLogProbMetric: 17.1256 - val_loss: 17.2882 - val_MinusLogProbMetric: 17.2882 - lr: 6.9444e-06 - 58s/epoch - 295ms/step
Epoch 995/1000
2023-10-24 04:25:50.317 
Epoch 995/1000 
	 loss: 17.0863, MinusLogProbMetric: 17.0863, val_loss: 17.2659, val_MinusLogProbMetric: 17.2659

Epoch 995: val_loss did not improve from 17.24860
196/196 - 61s - loss: 17.0863 - MinusLogProbMetric: 17.0863 - val_loss: 17.2659 - val_MinusLogProbMetric: 17.2659 - lr: 6.9444e-06 - 61s/epoch - 311ms/step
Epoch 996/1000
2023-10-24 04:26:52.838 
Epoch 996/1000 
	 loss: 17.0816, MinusLogProbMetric: 17.0816, val_loss: 17.2687, val_MinusLogProbMetric: 17.2687

Epoch 996: val_loss did not improve from 17.24860
196/196 - 63s - loss: 17.0816 - MinusLogProbMetric: 17.0816 - val_loss: 17.2687 - val_MinusLogProbMetric: 17.2687 - lr: 6.9444e-06 - 63s/epoch - 319ms/step
Epoch 997/1000
2023-10-24 04:27:55.320 
Epoch 997/1000 
	 loss: 17.0921, MinusLogProbMetric: 17.0921, val_loss: 17.2615, val_MinusLogProbMetric: 17.2615

Epoch 997: val_loss did not improve from 17.24860
196/196 - 62s - loss: 17.0921 - MinusLogProbMetric: 17.0921 - val_loss: 17.2615 - val_MinusLogProbMetric: 17.2615 - lr: 6.9444e-06 - 62s/epoch - 319ms/step
Epoch 998/1000
2023-10-24 04:28:55.802 
Epoch 998/1000 
	 loss: 17.0876, MinusLogProbMetric: 17.0876, val_loss: 17.2566, val_MinusLogProbMetric: 17.2566

Epoch 998: val_loss did not improve from 17.24860
196/196 - 60s - loss: 17.0876 - MinusLogProbMetric: 17.0876 - val_loss: 17.2566 - val_MinusLogProbMetric: 17.2566 - lr: 6.9444e-06 - 60s/epoch - 309ms/step
Epoch 999/1000
2023-10-24 04:29:50.046 
Epoch 999/1000 
	 loss: 17.0821, MinusLogProbMetric: 17.0821, val_loss: 17.2997, val_MinusLogProbMetric: 17.2997

Epoch 999: val_loss did not improve from 17.24860
196/196 - 54s - loss: 17.0821 - MinusLogProbMetric: 17.0821 - val_loss: 17.2997 - val_MinusLogProbMetric: 17.2997 - lr: 6.9444e-06 - 54s/epoch - 277ms/step
Epoch 1000/1000
2023-10-24 04:30:54.210 
Epoch 1000/1000 
	 loss: 17.0797, MinusLogProbMetric: 17.0797, val_loss: 17.2465, val_MinusLogProbMetric: 17.2465

Epoch 1000: val_loss improved from 17.24860 to 17.24651, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 65s - loss: 17.0797 - MinusLogProbMetric: 17.0797 - val_loss: 17.2465 - val_MinusLogProbMetric: 17.2465 - lr: 6.9444e-06 - 65s/epoch - 332ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 0.
Model trained in 56981.63 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.29 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.82 s.
===========
Run 245/720 done in 57499.00 s.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_319/ already exists.
Skipping it.
===========
Run 319/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_321/ already exists.
Skipping it.
===========
Run 321/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_322/ already exists.
Skipping it.
===========
Run 322/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_323/ already exists.
Skipping it.
===========
Run 323/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_324/ already exists.
Skipping it.
===========
Run 324/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_325/ already exists.
Skipping it.
===========
Run 325/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_326/ already exists.
Skipping it.
===========
Run 326/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_327/ already exists.
Skipping it.
===========
Run 327/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_328/ already exists.
Skipping it.
===========
Run 328/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_329/ already exists.
Skipping it.
===========
Run 329/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_330/ already exists.
Skipping it.
===========
Run 330/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_331/ already exists.
Skipping it.
===========
Run 331/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_332/ already exists.
Skipping it.
===========
Run 332/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_333/ already exists.
Skipping it.
===========
Run 333/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_334/ already exists.
Skipping it.
===========
Run 334/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_335/ already exists.
Skipping it.
===========
Run 335/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_336/ already exists.
Skipping it.
===========
Run 336/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_337/ already exists.
Skipping it.
===========
Run 337/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_338/ already exists.
Skipping it.
===========
Run 338/720 already exists. Skipping it.
===========

===========
Generating train data for run 339.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_339/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_339/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_339/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_339
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_38"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_39 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  908640    
 yer)                                                            
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7f1e995e9ed0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1e991c6ec0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1e991c6ec0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f18010317e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1801372fb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1801373520>, <keras.callbacks.ModelCheckpoint object at 0x7f18013735e0>, <keras.callbacks.EarlyStopping object at 0x7f1801373850>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1801373880>, <keras.callbacks.TerminateOnNaN object at 0x7f18013734c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_339/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 339/720 with hyperparameters:
timestamp = 2023-10-24 04:31:01.755422
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-24 04:32:07.949 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6308.4302, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 66s - loss: nan - MinusLogProbMetric: 6308.4302 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 66s/epoch - 337ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 339.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_339/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_339/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_339/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_339
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_44"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_45 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  908640    
 yer)                                                            
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7f1e6f50eda0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1e6f727e50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1e6f727e50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1e881f1750>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1e6f5654e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1e6f565a50>, <keras.callbacks.ModelCheckpoint object at 0x7f1e6f565b10>, <keras.callbacks.EarlyStopping object at 0x7f1e6f565d80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1e6f565db0>, <keras.callbacks.TerminateOnNaN object at 0x7f1e6f5659f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_339/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 339/720 with hyperparameters:
timestamp = 2023-10-24 04:32:13.868556
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
2023-10-24 04:33:54.435 
Epoch 1/1000 
	 loss: 1308.1064, MinusLogProbMetric: 1308.1064, val_loss: 316.6650, val_MinusLogProbMetric: 316.6650

Epoch 1: val_loss improved from inf to 316.66498, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 101s - loss: 1308.1064 - MinusLogProbMetric: 1308.1064 - val_loss: 316.6650 - val_MinusLogProbMetric: 316.6650 - lr: 3.3333e-04 - 101s/epoch - 515ms/step
Epoch 2/1000
2023-10-24 04:34:33.554 
Epoch 2/1000 
	 loss: 239.6642, MinusLogProbMetric: 239.6642, val_loss: 190.8300, val_MinusLogProbMetric: 190.8300

Epoch 2: val_loss improved from 316.66498 to 190.82999, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 239.6642 - MinusLogProbMetric: 239.6642 - val_loss: 190.8300 - val_MinusLogProbMetric: 190.8300 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 3/1000
2023-10-24 04:35:14.292 
Epoch 3/1000 
	 loss: 169.1767, MinusLogProbMetric: 169.1767, val_loss: 150.8958, val_MinusLogProbMetric: 150.8958

Epoch 3: val_loss improved from 190.82999 to 150.89575, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 41s - loss: 169.1767 - MinusLogProbMetric: 169.1767 - val_loss: 150.8958 - val_MinusLogProbMetric: 150.8958 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 4/1000
2023-10-24 04:35:56.023 
Epoch 4/1000 
	 loss: 139.1582, MinusLogProbMetric: 139.1582, val_loss: 128.7443, val_MinusLogProbMetric: 128.7443

Epoch 4: val_loss improved from 150.89575 to 128.74429, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 139.1582 - MinusLogProbMetric: 139.1582 - val_loss: 128.7443 - val_MinusLogProbMetric: 128.7443 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 5/1000
2023-10-24 04:36:34.556 
Epoch 5/1000 
	 loss: 124.9886, MinusLogProbMetric: 124.9886, val_loss: 116.3965, val_MinusLogProbMetric: 116.3965

Epoch 5: val_loss improved from 128.74429 to 116.39648, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 38s - loss: 124.9886 - MinusLogProbMetric: 124.9886 - val_loss: 116.3965 - val_MinusLogProbMetric: 116.3965 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 6/1000
2023-10-24 04:37:13.719 
Epoch 6/1000 
	 loss: 108.1754, MinusLogProbMetric: 108.1754, val_loss: 103.3379, val_MinusLogProbMetric: 103.3379

Epoch 6: val_loss improved from 116.39648 to 103.33791, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 108.1754 - MinusLogProbMetric: 108.1754 - val_loss: 103.3379 - val_MinusLogProbMetric: 103.3379 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 7/1000
2023-10-24 04:37:53.346 
Epoch 7/1000 
	 loss: 97.6068, MinusLogProbMetric: 97.6068, val_loss: 93.1208, val_MinusLogProbMetric: 93.1208

Epoch 7: val_loss improved from 103.33791 to 93.12078, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 40s - loss: 97.6068 - MinusLogProbMetric: 97.6068 - val_loss: 93.1208 - val_MinusLogProbMetric: 93.1208 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 8/1000
2023-10-24 04:38:35.662 
Epoch 8/1000 
	 loss: 103.3338, MinusLogProbMetric: 103.3338, val_loss: 92.2737, val_MinusLogProbMetric: 92.2737

Epoch 8: val_loss improved from 93.12078 to 92.27367, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 103.3338 - MinusLogProbMetric: 103.3338 - val_loss: 92.2737 - val_MinusLogProbMetric: 92.2737 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 9/1000
2023-10-24 04:39:13.395 
Epoch 9/1000 
	 loss: 86.0033, MinusLogProbMetric: 86.0033, val_loss: 81.6894, val_MinusLogProbMetric: 81.6894

Epoch 9: val_loss improved from 92.27367 to 81.68944, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 38s - loss: 86.0033 - MinusLogProbMetric: 86.0033 - val_loss: 81.6894 - val_MinusLogProbMetric: 81.6894 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 10/1000
2023-10-24 04:39:52.949 
Epoch 10/1000 
	 loss: 79.1897, MinusLogProbMetric: 79.1897, val_loss: 75.8473, val_MinusLogProbMetric: 75.8473

Epoch 10: val_loss improved from 81.68944 to 75.84733, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 40s - loss: 79.1897 - MinusLogProbMetric: 79.1897 - val_loss: 75.8473 - val_MinusLogProbMetric: 75.8473 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 11/1000
2023-10-24 04:40:30.856 
Epoch 11/1000 
	 loss: 74.1865, MinusLogProbMetric: 74.1865, val_loss: 72.2332, val_MinusLogProbMetric: 72.2332

Epoch 11: val_loss improved from 75.84733 to 72.23321, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 38s - loss: 74.1865 - MinusLogProbMetric: 74.1865 - val_loss: 72.2332 - val_MinusLogProbMetric: 72.2332 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 12/1000
2023-10-24 04:41:11.240 
Epoch 12/1000 
	 loss: 69.4539, MinusLogProbMetric: 69.4539, val_loss: 67.0641, val_MinusLogProbMetric: 67.0641

Epoch 12: val_loss improved from 72.23321 to 67.06414, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 40s - loss: 69.4539 - MinusLogProbMetric: 69.4539 - val_loss: 67.0641 - val_MinusLogProbMetric: 67.0641 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 13/1000
2023-10-24 04:41:51.951 
Epoch 13/1000 
	 loss: 65.4636, MinusLogProbMetric: 65.4636, val_loss: 64.1571, val_MinusLogProbMetric: 64.1571

Epoch 13: val_loss improved from 67.06414 to 64.15706, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 41s - loss: 65.4636 - MinusLogProbMetric: 65.4636 - val_loss: 64.1571 - val_MinusLogProbMetric: 64.1571 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 14/1000
2023-10-24 04:42:33.348 
Epoch 14/1000 
	 loss: 61.8312, MinusLogProbMetric: 61.8312, val_loss: 60.0961, val_MinusLogProbMetric: 60.0961

Epoch 14: val_loss improved from 64.15706 to 60.09612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 41s - loss: 61.8312 - MinusLogProbMetric: 61.8312 - val_loss: 60.0961 - val_MinusLogProbMetric: 60.0961 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 15/1000
2023-10-24 04:43:14.786 
Epoch 15/1000 
	 loss: 58.9928, MinusLogProbMetric: 58.9928, val_loss: 57.8754, val_MinusLogProbMetric: 57.8754

Epoch 15: val_loss improved from 60.09612 to 57.87543, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 41s - loss: 58.9928 - MinusLogProbMetric: 58.9928 - val_loss: 57.8754 - val_MinusLogProbMetric: 57.8754 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 16/1000
2023-10-24 04:43:53.137 
Epoch 16/1000 
	 loss: 56.5459, MinusLogProbMetric: 56.5459, val_loss: 56.2285, val_MinusLogProbMetric: 56.2285

Epoch 16: val_loss improved from 57.87543 to 56.22849, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 38s - loss: 56.5459 - MinusLogProbMetric: 56.5459 - val_loss: 56.2285 - val_MinusLogProbMetric: 56.2285 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 17/1000
2023-10-24 04:44:34.274 
Epoch 17/1000 
	 loss: 54.5878, MinusLogProbMetric: 54.5878, val_loss: 54.3779, val_MinusLogProbMetric: 54.3779

Epoch 17: val_loss improved from 56.22849 to 54.37791, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 41s - loss: 54.5878 - MinusLogProbMetric: 54.5878 - val_loss: 54.3779 - val_MinusLogProbMetric: 54.3779 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 18/1000
2023-10-24 04:45:15.560 
Epoch 18/1000 
	 loss: 53.7443, MinusLogProbMetric: 53.7443, val_loss: 64.7323, val_MinusLogProbMetric: 64.7323

Epoch 18: val_loss did not improve from 54.37791
196/196 - 41s - loss: 53.7443 - MinusLogProbMetric: 53.7443 - val_loss: 64.7323 - val_MinusLogProbMetric: 64.7323 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 19/1000
2023-10-24 04:45:53.236 
Epoch 19/1000 
	 loss: 53.7390, MinusLogProbMetric: 53.7390, val_loss: 51.7098, val_MinusLogProbMetric: 51.7098

Epoch 19: val_loss improved from 54.37791 to 51.70979, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 38s - loss: 53.7390 - MinusLogProbMetric: 53.7390 - val_loss: 51.7098 - val_MinusLogProbMetric: 51.7098 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 20/1000
2023-10-24 04:46:34.170 
Epoch 20/1000 
	 loss: 50.6856, MinusLogProbMetric: 50.6856, val_loss: 50.7803, val_MinusLogProbMetric: 50.7803

Epoch 20: val_loss improved from 51.70979 to 50.78034, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 41s - loss: 50.6856 - MinusLogProbMetric: 50.6856 - val_loss: 50.7803 - val_MinusLogProbMetric: 50.7803 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 21/1000
2023-10-24 04:47:15.050 
Epoch 21/1000 
	 loss: 49.1007, MinusLogProbMetric: 49.1007, val_loss: 48.7144, val_MinusLogProbMetric: 48.7144

Epoch 21: val_loss improved from 50.78034 to 48.71443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 41s - loss: 49.1007 - MinusLogProbMetric: 49.1007 - val_loss: 48.7144 - val_MinusLogProbMetric: 48.7144 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 22/1000
2023-10-24 04:47:55.364 
Epoch 22/1000 
	 loss: 47.8587, MinusLogProbMetric: 47.8587, val_loss: 47.5938, val_MinusLogProbMetric: 47.5938

Epoch 22: val_loss improved from 48.71443 to 47.59380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 40s - loss: 47.8587 - MinusLogProbMetric: 47.8587 - val_loss: 47.5938 - val_MinusLogProbMetric: 47.5938 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 23/1000
2023-10-24 04:48:38.164 
Epoch 23/1000 
	 loss: 46.7287, MinusLogProbMetric: 46.7287, val_loss: 46.8571, val_MinusLogProbMetric: 46.8571

Epoch 23: val_loss improved from 47.59380 to 46.85711, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 43s - loss: 46.7287 - MinusLogProbMetric: 46.7287 - val_loss: 46.8571 - val_MinusLogProbMetric: 46.8571 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 24/1000
2023-10-24 04:49:18.814 
Epoch 24/1000 
	 loss: 45.9222, MinusLogProbMetric: 45.9222, val_loss: 48.0328, val_MinusLogProbMetric: 48.0328

Epoch 24: val_loss did not improve from 46.85711
196/196 - 40s - loss: 45.9222 - MinusLogProbMetric: 45.9222 - val_loss: 48.0328 - val_MinusLogProbMetric: 48.0328 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 25/1000
2023-10-24 04:49:56.149 
Epoch 25/1000 
	 loss: 45.4937, MinusLogProbMetric: 45.4937, val_loss: 45.1578, val_MinusLogProbMetric: 45.1578

Epoch 25: val_loss improved from 46.85711 to 45.15777, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 38s - loss: 45.4937 - MinusLogProbMetric: 45.4937 - val_loss: 45.1578 - val_MinusLogProbMetric: 45.1578 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 26/1000
2023-10-24 04:50:35.959 
Epoch 26/1000 
	 loss: 44.6701, MinusLogProbMetric: 44.6701, val_loss: 47.5781, val_MinusLogProbMetric: 47.5781

Epoch 26: val_loss did not improve from 45.15777
196/196 - 39s - loss: 44.6701 - MinusLogProbMetric: 44.6701 - val_loss: 47.5781 - val_MinusLogProbMetric: 47.5781 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 27/1000
2023-10-24 04:51:17.089 
Epoch 27/1000 
	 loss: 43.9557, MinusLogProbMetric: 43.9557, val_loss: 43.7776, val_MinusLogProbMetric: 43.7776

Epoch 27: val_loss improved from 45.15777 to 43.77756, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 43.9557 - MinusLogProbMetric: 43.9557 - val_loss: 43.7776 - val_MinusLogProbMetric: 43.7776 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 28/1000
2023-10-24 04:51:55.900 
Epoch 28/1000 
	 loss: 43.3224, MinusLogProbMetric: 43.3224, val_loss: 43.3859, val_MinusLogProbMetric: 43.3859

Epoch 28: val_loss improved from 43.77756 to 43.38589, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 43.3224 - MinusLogProbMetric: 43.3224 - val_loss: 43.3859 - val_MinusLogProbMetric: 43.3859 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 29/1000
2023-10-24 04:52:35.899 
Epoch 29/1000 
	 loss: 42.6444, MinusLogProbMetric: 42.6444, val_loss: 42.1174, val_MinusLogProbMetric: 42.1174

Epoch 29: val_loss improved from 43.38589 to 42.11736, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 40s - loss: 42.6444 - MinusLogProbMetric: 42.6444 - val_loss: 42.1174 - val_MinusLogProbMetric: 42.1174 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 30/1000
2023-10-24 04:53:13.993 
Epoch 30/1000 
	 loss: 41.9213, MinusLogProbMetric: 41.9213, val_loss: 41.9510, val_MinusLogProbMetric: 41.9510

Epoch 30: val_loss improved from 42.11736 to 41.95098, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 38s - loss: 41.9213 - MinusLogProbMetric: 41.9213 - val_loss: 41.9510 - val_MinusLogProbMetric: 41.9510 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 31/1000
2023-10-24 04:53:52.956 
Epoch 31/1000 
	 loss: 41.5678, MinusLogProbMetric: 41.5678, val_loss: 41.2780, val_MinusLogProbMetric: 41.2780

Epoch 31: val_loss improved from 41.95098 to 41.27805, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 41.5678 - MinusLogProbMetric: 41.5678 - val_loss: 41.2780 - val_MinusLogProbMetric: 41.2780 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 32/1000
2023-10-24 04:54:31.416 
Epoch 32/1000 
	 loss: 51.6949, MinusLogProbMetric: 51.6949, val_loss: 45.2119, val_MinusLogProbMetric: 45.2119

Epoch 32: val_loss did not improve from 41.27805
196/196 - 38s - loss: 51.6949 - MinusLogProbMetric: 51.6949 - val_loss: 45.2119 - val_MinusLogProbMetric: 45.2119 - lr: 3.3333e-04 - 38s/epoch - 193ms/step
Epoch 33/1000
2023-10-24 04:55:11.411 
Epoch 33/1000 
	 loss: 42.1481, MinusLogProbMetric: 42.1481, val_loss: 42.0611, val_MinusLogProbMetric: 42.0611

Epoch 33: val_loss did not improve from 41.27805
196/196 - 40s - loss: 42.1481 - MinusLogProbMetric: 42.1481 - val_loss: 42.0611 - val_MinusLogProbMetric: 42.0611 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 34/1000
2023-10-24 04:55:50.541 
Epoch 34/1000 
	 loss: 40.7069, MinusLogProbMetric: 40.7069, val_loss: 40.4830, val_MinusLogProbMetric: 40.4830

Epoch 34: val_loss improved from 41.27805 to 40.48296, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 40s - loss: 40.7069 - MinusLogProbMetric: 40.7069 - val_loss: 40.4830 - val_MinusLogProbMetric: 40.4830 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 35/1000
2023-10-24 04:56:29.817 
Epoch 35/1000 
	 loss: 40.1326, MinusLogProbMetric: 40.1326, val_loss: 40.9257, val_MinusLogProbMetric: 40.9257

Epoch 35: val_loss did not improve from 40.48296
196/196 - 39s - loss: 40.1326 - MinusLogProbMetric: 40.1326 - val_loss: 40.9257 - val_MinusLogProbMetric: 40.9257 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 36/1000
2023-10-24 04:57:08.172 
Epoch 36/1000 
	 loss: 39.8090, MinusLogProbMetric: 39.8090, val_loss: 39.8728, val_MinusLogProbMetric: 39.8728

Epoch 36: val_loss improved from 40.48296 to 39.87279, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 39.8090 - MinusLogProbMetric: 39.8090 - val_loss: 39.8728 - val_MinusLogProbMetric: 39.8728 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 37/1000
2023-10-24 04:57:48.129 
Epoch 37/1000 
	 loss: 39.2970, MinusLogProbMetric: 39.2970, val_loss: 39.7911, val_MinusLogProbMetric: 39.7911

Epoch 37: val_loss improved from 39.87279 to 39.79106, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 40s - loss: 39.2970 - MinusLogProbMetric: 39.2970 - val_loss: 39.7911 - val_MinusLogProbMetric: 39.7911 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 38/1000
2023-10-24 04:58:27.034 
Epoch 38/1000 
	 loss: 39.0516, MinusLogProbMetric: 39.0516, val_loss: 39.2256, val_MinusLogProbMetric: 39.2256

Epoch 38: val_loss improved from 39.79106 to 39.22561, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 39.0516 - MinusLogProbMetric: 39.0516 - val_loss: 39.2256 - val_MinusLogProbMetric: 39.2256 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 39/1000
2023-10-24 04:59:06.885 
Epoch 39/1000 
	 loss: 38.7927, MinusLogProbMetric: 38.7927, val_loss: 38.9580, val_MinusLogProbMetric: 38.9580

Epoch 39: val_loss improved from 39.22561 to 38.95797, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 40s - loss: 38.7927 - MinusLogProbMetric: 38.7927 - val_loss: 38.9580 - val_MinusLogProbMetric: 38.9580 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 40/1000
2023-10-24 04:59:48.204 
Epoch 40/1000 
	 loss: 38.4512, MinusLogProbMetric: 38.4512, val_loss: 38.7688, val_MinusLogProbMetric: 38.7688

Epoch 40: val_loss improved from 38.95797 to 38.76876, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 41s - loss: 38.4512 - MinusLogProbMetric: 38.4512 - val_loss: 38.7688 - val_MinusLogProbMetric: 38.7688 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 41/1000
2023-10-24 05:00:27.523 
Epoch 41/1000 
	 loss: 38.1699, MinusLogProbMetric: 38.1699, val_loss: 38.9013, val_MinusLogProbMetric: 38.9013

Epoch 41: val_loss did not improve from 38.76876
196/196 - 39s - loss: 38.1699 - MinusLogProbMetric: 38.1699 - val_loss: 38.9013 - val_MinusLogProbMetric: 38.9013 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 42/1000
2023-10-24 05:01:05.705 
Epoch 42/1000 
	 loss: 38.0459, MinusLogProbMetric: 38.0459, val_loss: 39.5367, val_MinusLogProbMetric: 39.5367

Epoch 42: val_loss did not improve from 38.76876
196/196 - 38s - loss: 38.0459 - MinusLogProbMetric: 38.0459 - val_loss: 39.5367 - val_MinusLogProbMetric: 39.5367 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 43/1000
2023-10-24 05:01:45.103 
Epoch 43/1000 
	 loss: 37.7432, MinusLogProbMetric: 37.7432, val_loss: 37.9441, val_MinusLogProbMetric: 37.9441

Epoch 43: val_loss improved from 38.76876 to 37.94408, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 40s - loss: 37.7432 - MinusLogProbMetric: 37.7432 - val_loss: 37.9441 - val_MinusLogProbMetric: 37.9441 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 44/1000
2023-10-24 05:02:25.352 
Epoch 44/1000 
	 loss: 37.5024, MinusLogProbMetric: 37.5024, val_loss: 37.6361, val_MinusLogProbMetric: 37.6361

Epoch 44: val_loss improved from 37.94408 to 37.63610, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 40s - loss: 37.5024 - MinusLogProbMetric: 37.5024 - val_loss: 37.6361 - val_MinusLogProbMetric: 37.6361 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 45/1000
2023-10-24 05:03:04.536 
Epoch 45/1000 
	 loss: 37.2127, MinusLogProbMetric: 37.2127, val_loss: 37.0619, val_MinusLogProbMetric: 37.0619

Epoch 45: val_loss improved from 37.63610 to 37.06192, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 37.2127 - MinusLogProbMetric: 37.2127 - val_loss: 37.0619 - val_MinusLogProbMetric: 37.0619 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 46/1000
2023-10-24 05:03:44.961 
Epoch 46/1000 
	 loss: 37.1154, MinusLogProbMetric: 37.1154, val_loss: 36.9668, val_MinusLogProbMetric: 36.9668

Epoch 46: val_loss improved from 37.06192 to 36.96677, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 40s - loss: 37.1154 - MinusLogProbMetric: 37.1154 - val_loss: 36.9668 - val_MinusLogProbMetric: 36.9668 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 47/1000
2023-10-24 05:04:23.516 
Epoch 47/1000 
	 loss: 36.7243, MinusLogProbMetric: 36.7243, val_loss: 37.0842, val_MinusLogProbMetric: 37.0842

Epoch 47: val_loss did not improve from 36.96677
196/196 - 38s - loss: 36.7243 - MinusLogProbMetric: 36.7243 - val_loss: 37.0842 - val_MinusLogProbMetric: 37.0842 - lr: 3.3333e-04 - 38s/epoch - 193ms/step
Epoch 48/1000
2023-10-24 05:05:03.636 
Epoch 48/1000 
	 loss: 36.9941, MinusLogProbMetric: 36.9941, val_loss: 36.8929, val_MinusLogProbMetric: 36.8929

Epoch 48: val_loss improved from 36.96677 to 36.89287, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 41s - loss: 36.9941 - MinusLogProbMetric: 36.9941 - val_loss: 36.8929 - val_MinusLogProbMetric: 36.8929 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 49/1000
2023-10-24 05:05:43.707 
Epoch 49/1000 
	 loss: 37.0774, MinusLogProbMetric: 37.0774, val_loss: 37.3673, val_MinusLogProbMetric: 37.3673

Epoch 49: val_loss did not improve from 36.89287
196/196 - 39s - loss: 37.0774 - MinusLogProbMetric: 37.0774 - val_loss: 37.3673 - val_MinusLogProbMetric: 37.3673 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 50/1000
2023-10-24 05:06:20.392 
Epoch 50/1000 
	 loss: 36.2296, MinusLogProbMetric: 36.2296, val_loss: 36.0651, val_MinusLogProbMetric: 36.0651

Epoch 50: val_loss improved from 36.89287 to 36.06507, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 37s - loss: 36.2296 - MinusLogProbMetric: 36.2296 - val_loss: 36.0651 - val_MinusLogProbMetric: 36.0651 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 51/1000
2023-10-24 05:06:58.601 
Epoch 51/1000 
	 loss: 36.3189, MinusLogProbMetric: 36.3189, val_loss: 36.9073, val_MinusLogProbMetric: 36.9073

Epoch 51: val_loss did not improve from 36.06507
196/196 - 38s - loss: 36.3189 - MinusLogProbMetric: 36.3189 - val_loss: 36.9073 - val_MinusLogProbMetric: 36.9073 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 52/1000
2023-10-24 05:07:37.048 
Epoch 52/1000 
	 loss: 35.9432, MinusLogProbMetric: 35.9432, val_loss: 36.1796, val_MinusLogProbMetric: 36.1796

Epoch 52: val_loss did not improve from 36.06507
196/196 - 38s - loss: 35.9432 - MinusLogProbMetric: 35.9432 - val_loss: 36.1796 - val_MinusLogProbMetric: 36.1796 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 53/1000
2023-10-24 05:08:15.840 
Epoch 53/1000 
	 loss: 36.1288, MinusLogProbMetric: 36.1288, val_loss: 36.1056, val_MinusLogProbMetric: 36.1056

Epoch 53: val_loss did not improve from 36.06507
196/196 - 39s - loss: 36.1288 - MinusLogProbMetric: 36.1288 - val_loss: 36.1056 - val_MinusLogProbMetric: 36.1056 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 54/1000
2023-10-24 05:08:57.577 
Epoch 54/1000 
	 loss: 35.6443, MinusLogProbMetric: 35.6443, val_loss: 36.5755, val_MinusLogProbMetric: 36.5755

Epoch 54: val_loss did not improve from 36.06507
196/196 - 42s - loss: 35.6443 - MinusLogProbMetric: 35.6443 - val_loss: 36.5755 - val_MinusLogProbMetric: 36.5755 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 55/1000
2023-10-24 05:09:36.031 
Epoch 55/1000 
	 loss: 35.6688, MinusLogProbMetric: 35.6688, val_loss: 35.5904, val_MinusLogProbMetric: 35.5904

Epoch 55: val_loss improved from 36.06507 to 35.59035, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 35.6688 - MinusLogProbMetric: 35.6688 - val_loss: 35.5904 - val_MinusLogProbMetric: 35.5904 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 56/1000
2023-10-24 05:10:14.671 
Epoch 56/1000 
	 loss: 35.6035, MinusLogProbMetric: 35.6035, val_loss: 35.8048, val_MinusLogProbMetric: 35.8048

Epoch 56: val_loss did not improve from 35.59035
196/196 - 38s - loss: 35.6035 - MinusLogProbMetric: 35.6035 - val_loss: 35.8048 - val_MinusLogProbMetric: 35.8048 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 57/1000
2023-10-24 05:10:56.204 
Epoch 57/1000 
	 loss: 35.5416, MinusLogProbMetric: 35.5416, val_loss: 35.8771, val_MinusLogProbMetric: 35.8771

Epoch 57: val_loss did not improve from 35.59035
196/196 - 42s - loss: 35.5416 - MinusLogProbMetric: 35.5416 - val_loss: 35.8771 - val_MinusLogProbMetric: 35.8771 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 58/1000
2023-10-24 05:11:38.339 
Epoch 58/1000 
	 loss: 35.4144, MinusLogProbMetric: 35.4144, val_loss: 36.6150, val_MinusLogProbMetric: 36.6150

Epoch 58: val_loss did not improve from 35.59035
196/196 - 42s - loss: 35.4144 - MinusLogProbMetric: 35.4144 - val_loss: 36.6150 - val_MinusLogProbMetric: 36.6150 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 59/1000
2023-10-24 05:12:20.217 
Epoch 59/1000 
	 loss: 35.0655, MinusLogProbMetric: 35.0655, val_loss: 35.0646, val_MinusLogProbMetric: 35.0646

Epoch 59: val_loss improved from 35.59035 to 35.06462, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 35.0655 - MinusLogProbMetric: 35.0655 - val_loss: 35.0646 - val_MinusLogProbMetric: 35.0646 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 60/1000
2023-10-24 05:12:59.014 
Epoch 60/1000 
	 loss: 35.1294, MinusLogProbMetric: 35.1294, val_loss: 35.0531, val_MinusLogProbMetric: 35.0531

Epoch 60: val_loss improved from 35.06462 to 35.05312, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 35.1294 - MinusLogProbMetric: 35.1294 - val_loss: 35.0531 - val_MinusLogProbMetric: 35.0531 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 61/1000
2023-10-24 05:13:39.341 
Epoch 61/1000 
	 loss: 35.0028, MinusLogProbMetric: 35.0028, val_loss: 37.4204, val_MinusLogProbMetric: 37.4204

Epoch 61: val_loss did not improve from 35.05312
196/196 - 40s - loss: 35.0028 - MinusLogProbMetric: 35.0028 - val_loss: 37.4204 - val_MinusLogProbMetric: 37.4204 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 62/1000
2023-10-24 05:14:17.596 
Epoch 62/1000 
	 loss: 34.9782, MinusLogProbMetric: 34.9782, val_loss: 34.7465, val_MinusLogProbMetric: 34.7465

Epoch 62: val_loss improved from 35.05312 to 34.74648, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 34.9782 - MinusLogProbMetric: 34.9782 - val_loss: 34.7465 - val_MinusLogProbMetric: 34.7465 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 63/1000
2023-10-24 05:14:58.457 
Epoch 63/1000 
	 loss: 34.6839, MinusLogProbMetric: 34.6839, val_loss: 36.0340, val_MinusLogProbMetric: 36.0340

Epoch 63: val_loss did not improve from 34.74648
196/196 - 40s - loss: 34.6839 - MinusLogProbMetric: 34.6839 - val_loss: 36.0340 - val_MinusLogProbMetric: 36.0340 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 64/1000
2023-10-24 05:15:37.124 
Epoch 64/1000 
	 loss: 39.4304, MinusLogProbMetric: 39.4304, val_loss: 35.1078, val_MinusLogProbMetric: 35.1078

Epoch 64: val_loss did not improve from 34.74648
196/196 - 39s - loss: 39.4304 - MinusLogProbMetric: 39.4304 - val_loss: 35.1078 - val_MinusLogProbMetric: 35.1078 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 65/1000
2023-10-24 05:16:16.609 
Epoch 65/1000 
	 loss: 34.5072, MinusLogProbMetric: 34.5072, val_loss: 34.6473, val_MinusLogProbMetric: 34.6473

Epoch 65: val_loss improved from 34.74648 to 34.64725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 40s - loss: 34.5072 - MinusLogProbMetric: 34.5072 - val_loss: 34.6473 - val_MinusLogProbMetric: 34.6473 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 66/1000
2023-10-24 05:16:54.912 
Epoch 66/1000 
	 loss: 34.3968, MinusLogProbMetric: 34.3968, val_loss: 34.3337, val_MinusLogProbMetric: 34.3337

Epoch 66: val_loss improved from 34.64725 to 34.33366, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 38s - loss: 34.3968 - MinusLogProbMetric: 34.3968 - val_loss: 34.3337 - val_MinusLogProbMetric: 34.3337 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 67/1000
2023-10-24 05:17:34.156 
Epoch 67/1000 
	 loss: 34.1715, MinusLogProbMetric: 34.1715, val_loss: 34.9462, val_MinusLogProbMetric: 34.9462

Epoch 67: val_loss did not improve from 34.33366
196/196 - 39s - loss: 34.1715 - MinusLogProbMetric: 34.1715 - val_loss: 34.9462 - val_MinusLogProbMetric: 34.9462 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 68/1000
2023-10-24 05:18:12.099 
Epoch 68/1000 
	 loss: 34.2166, MinusLogProbMetric: 34.2166, val_loss: 34.5553, val_MinusLogProbMetric: 34.5553

Epoch 68: val_loss did not improve from 34.33366
196/196 - 38s - loss: 34.2166 - MinusLogProbMetric: 34.2166 - val_loss: 34.5553 - val_MinusLogProbMetric: 34.5553 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 69/1000
2023-10-24 05:18:50.724 
Epoch 69/1000 
	 loss: 34.2896, MinusLogProbMetric: 34.2896, val_loss: 34.7213, val_MinusLogProbMetric: 34.7213

Epoch 69: val_loss did not improve from 34.33366
196/196 - 39s - loss: 34.2896 - MinusLogProbMetric: 34.2896 - val_loss: 34.7213 - val_MinusLogProbMetric: 34.7213 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 70/1000
2023-10-24 05:19:28.240 
Epoch 70/1000 
	 loss: 34.1403, MinusLogProbMetric: 34.1403, val_loss: 33.9928, val_MinusLogProbMetric: 33.9928

Epoch 70: val_loss improved from 34.33366 to 33.99278, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 38s - loss: 34.1403 - MinusLogProbMetric: 34.1403 - val_loss: 33.9928 - val_MinusLogProbMetric: 33.9928 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 71/1000
2023-10-24 05:20:06.411 
Epoch 71/1000 
	 loss: 34.0151, MinusLogProbMetric: 34.0151, val_loss: 34.2574, val_MinusLogProbMetric: 34.2574

Epoch 71: val_loss did not improve from 33.99278
196/196 - 38s - loss: 34.0151 - MinusLogProbMetric: 34.0151 - val_loss: 34.2574 - val_MinusLogProbMetric: 34.2574 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 72/1000
2023-10-24 05:20:43.736 
Epoch 72/1000 
	 loss: 33.8576, MinusLogProbMetric: 33.8576, val_loss: 35.1842, val_MinusLogProbMetric: 35.1842

Epoch 72: val_loss did not improve from 33.99278
196/196 - 37s - loss: 33.8576 - MinusLogProbMetric: 33.8576 - val_loss: 35.1842 - val_MinusLogProbMetric: 35.1842 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 73/1000
2023-10-24 05:21:22.389 
Epoch 73/1000 
	 loss: 33.8582, MinusLogProbMetric: 33.8582, val_loss: 38.4263, val_MinusLogProbMetric: 38.4263

Epoch 73: val_loss did not improve from 33.99278
196/196 - 39s - loss: 33.8582 - MinusLogProbMetric: 33.8582 - val_loss: 38.4263 - val_MinusLogProbMetric: 38.4263 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 74/1000
2023-10-24 05:22:00.725 
Epoch 74/1000 
	 loss: 34.1861, MinusLogProbMetric: 34.1861, val_loss: 34.4504, val_MinusLogProbMetric: 34.4504

Epoch 74: val_loss did not improve from 33.99278
196/196 - 38s - loss: 34.1861 - MinusLogProbMetric: 34.1861 - val_loss: 34.4504 - val_MinusLogProbMetric: 34.4504 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 75/1000
2023-10-24 05:22:38.093 
Epoch 75/1000 
	 loss: 33.9157, MinusLogProbMetric: 33.9157, val_loss: 34.3072, val_MinusLogProbMetric: 34.3072

Epoch 75: val_loss did not improve from 33.99278
196/196 - 37s - loss: 33.9157 - MinusLogProbMetric: 33.9157 - val_loss: 34.3072 - val_MinusLogProbMetric: 34.3072 - lr: 3.3333e-04 - 37s/epoch - 191ms/step
Epoch 76/1000
2023-10-24 05:23:18.625 
Epoch 76/1000 
	 loss: 33.6072, MinusLogProbMetric: 33.6072, val_loss: 33.3625, val_MinusLogProbMetric: 33.3625

Epoch 76: val_loss improved from 33.99278 to 33.36247, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 41s - loss: 33.6072 - MinusLogProbMetric: 33.6072 - val_loss: 33.3625 - val_MinusLogProbMetric: 33.3625 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 77/1000
2023-10-24 05:23:58.573 
Epoch 77/1000 
	 loss: 33.5866, MinusLogProbMetric: 33.5866, val_loss: 33.5522, val_MinusLogProbMetric: 33.5522

Epoch 77: val_loss did not improve from 33.36247
196/196 - 39s - loss: 33.5866 - MinusLogProbMetric: 33.5866 - val_loss: 33.5522 - val_MinusLogProbMetric: 33.5522 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 78/1000
2023-10-24 05:24:36.085 
Epoch 78/1000 
	 loss: 33.9171, MinusLogProbMetric: 33.9171, val_loss: 34.2594, val_MinusLogProbMetric: 34.2594

Epoch 78: val_loss did not improve from 33.36247
196/196 - 38s - loss: 33.9171 - MinusLogProbMetric: 33.9171 - val_loss: 34.2594 - val_MinusLogProbMetric: 34.2594 - lr: 3.3333e-04 - 38s/epoch - 191ms/step
Epoch 79/1000
2023-10-24 05:25:15.276 
Epoch 79/1000 
	 loss: 33.8534, MinusLogProbMetric: 33.8534, val_loss: 33.6970, val_MinusLogProbMetric: 33.6970

Epoch 79: val_loss did not improve from 33.36247
196/196 - 39s - loss: 33.8534 - MinusLogProbMetric: 33.8534 - val_loss: 33.6970 - val_MinusLogProbMetric: 33.6970 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 80/1000
2023-10-24 05:25:54.862 
Epoch 80/1000 
	 loss: 33.4454, MinusLogProbMetric: 33.4454, val_loss: 33.7880, val_MinusLogProbMetric: 33.7880

Epoch 80: val_loss did not improve from 33.36247
196/196 - 40s - loss: 33.4454 - MinusLogProbMetric: 33.4454 - val_loss: 33.7880 - val_MinusLogProbMetric: 33.7880 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 81/1000
2023-10-24 05:26:34.765 
Epoch 81/1000 
	 loss: 33.3685, MinusLogProbMetric: 33.3685, val_loss: 33.4215, val_MinusLogProbMetric: 33.4215

Epoch 81: val_loss did not improve from 33.36247
196/196 - 40s - loss: 33.3685 - MinusLogProbMetric: 33.3685 - val_loss: 33.4215 - val_MinusLogProbMetric: 33.4215 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 82/1000
2023-10-24 05:27:11.242 
Epoch 82/1000 
	 loss: 33.2504, MinusLogProbMetric: 33.2504, val_loss: 33.8015, val_MinusLogProbMetric: 33.8015

Epoch 82: val_loss did not improve from 33.36247
196/196 - 36s - loss: 33.2504 - MinusLogProbMetric: 33.2504 - val_loss: 33.8015 - val_MinusLogProbMetric: 33.8015 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 83/1000
2023-10-24 05:27:50.104 
Epoch 83/1000 
	 loss: 33.2073, MinusLogProbMetric: 33.2073, val_loss: 33.8803, val_MinusLogProbMetric: 33.8803

Epoch 83: val_loss did not improve from 33.36247
196/196 - 39s - loss: 33.2073 - MinusLogProbMetric: 33.2073 - val_loss: 33.8803 - val_MinusLogProbMetric: 33.8803 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 84/1000
2023-10-24 05:28:27.147 
Epoch 84/1000 
	 loss: 33.2322, MinusLogProbMetric: 33.2322, val_loss: 32.9440, val_MinusLogProbMetric: 32.9440

Epoch 84: val_loss improved from 33.36247 to 32.94402, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 38s - loss: 33.2322 - MinusLogProbMetric: 33.2322 - val_loss: 32.9440 - val_MinusLogProbMetric: 32.9440 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 85/1000
2023-10-24 05:29:06.374 
Epoch 85/1000 
	 loss: 34.1425, MinusLogProbMetric: 34.1425, val_loss: 33.3308, val_MinusLogProbMetric: 33.3308

Epoch 85: val_loss did not improve from 32.94402
196/196 - 39s - loss: 34.1425 - MinusLogProbMetric: 34.1425 - val_loss: 33.3308 - val_MinusLogProbMetric: 33.3308 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 86/1000
2023-10-24 05:29:46.542 
Epoch 86/1000 
	 loss: 33.3156, MinusLogProbMetric: 33.3156, val_loss: 33.7486, val_MinusLogProbMetric: 33.7486

Epoch 86: val_loss did not improve from 32.94402
196/196 - 40s - loss: 33.3156 - MinusLogProbMetric: 33.3156 - val_loss: 33.7486 - val_MinusLogProbMetric: 33.7486 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 87/1000
2023-10-24 05:30:24.567 
Epoch 87/1000 
	 loss: 33.0588, MinusLogProbMetric: 33.0588, val_loss: 33.5626, val_MinusLogProbMetric: 33.5626

Epoch 87: val_loss did not improve from 32.94402
196/196 - 38s - loss: 33.0588 - MinusLogProbMetric: 33.0588 - val_loss: 33.5626 - val_MinusLogProbMetric: 33.5626 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 88/1000
2023-10-24 05:31:02.949 
Epoch 88/1000 
	 loss: 33.0200, MinusLogProbMetric: 33.0200, val_loss: 34.8649, val_MinusLogProbMetric: 34.8649

Epoch 88: val_loss did not improve from 32.94402
196/196 - 38s - loss: 33.0200 - MinusLogProbMetric: 33.0200 - val_loss: 34.8649 - val_MinusLogProbMetric: 34.8649 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 89/1000
2023-10-24 05:31:41.777 
Epoch 89/1000 
	 loss: 33.1834, MinusLogProbMetric: 33.1834, val_loss: 33.3483, val_MinusLogProbMetric: 33.3483

Epoch 89: val_loss did not improve from 32.94402
196/196 - 39s - loss: 33.1834 - MinusLogProbMetric: 33.1834 - val_loss: 33.3483 - val_MinusLogProbMetric: 33.3483 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 90/1000
2023-10-24 05:32:18.961 
Epoch 90/1000 
	 loss: 32.8073, MinusLogProbMetric: 32.8073, val_loss: 33.2482, val_MinusLogProbMetric: 33.2482

Epoch 90: val_loss did not improve from 32.94402
196/196 - 37s - loss: 32.8073 - MinusLogProbMetric: 32.8073 - val_loss: 33.2482 - val_MinusLogProbMetric: 33.2482 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 91/1000
2023-10-24 05:32:58.952 
Epoch 91/1000 
	 loss: 32.8977, MinusLogProbMetric: 32.8977, val_loss: 33.0107, val_MinusLogProbMetric: 33.0107

Epoch 91: val_loss did not improve from 32.94402
196/196 - 40s - loss: 32.8977 - MinusLogProbMetric: 32.8977 - val_loss: 33.0107 - val_MinusLogProbMetric: 33.0107 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 92/1000
2023-10-24 05:33:35.720 
Epoch 92/1000 
	 loss: 33.0005, MinusLogProbMetric: 33.0005, val_loss: 34.0747, val_MinusLogProbMetric: 34.0747

Epoch 92: val_loss did not improve from 32.94402
196/196 - 37s - loss: 33.0005 - MinusLogProbMetric: 33.0005 - val_loss: 34.0747 - val_MinusLogProbMetric: 34.0747 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 93/1000
2023-10-24 05:34:16.394 
Epoch 93/1000 
	 loss: 32.7996, MinusLogProbMetric: 32.7996, val_loss: 32.7327, val_MinusLogProbMetric: 32.7327

Epoch 93: val_loss improved from 32.94402 to 32.73266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 41s - loss: 32.7996 - MinusLogProbMetric: 32.7996 - val_loss: 32.7327 - val_MinusLogProbMetric: 32.7327 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 94/1000
2023-10-24 05:34:59.519 
Epoch 94/1000 
	 loss: 32.7949, MinusLogProbMetric: 32.7949, val_loss: 33.0618, val_MinusLogProbMetric: 33.0618

Epoch 94: val_loss did not improve from 32.73266
196/196 - 42s - loss: 32.7949 - MinusLogProbMetric: 32.7949 - val_loss: 33.0618 - val_MinusLogProbMetric: 33.0618 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 95/1000
2023-10-24 05:35:37.015 
Epoch 95/1000 
	 loss: 32.7833, MinusLogProbMetric: 32.7833, val_loss: 33.2702, val_MinusLogProbMetric: 33.2702

Epoch 95: val_loss did not improve from 32.73266
196/196 - 37s - loss: 32.7833 - MinusLogProbMetric: 32.7833 - val_loss: 33.2702 - val_MinusLogProbMetric: 33.2702 - lr: 3.3333e-04 - 37s/epoch - 191ms/step
Epoch 96/1000
2023-10-24 05:36:14.664 
Epoch 96/1000 
	 loss: 32.7675, MinusLogProbMetric: 32.7675, val_loss: 33.2125, val_MinusLogProbMetric: 33.2125

Epoch 96: val_loss did not improve from 32.73266
196/196 - 38s - loss: 32.7675 - MinusLogProbMetric: 32.7675 - val_loss: 33.2125 - val_MinusLogProbMetric: 33.2125 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 97/1000
2023-10-24 05:36:57.212 
Epoch 97/1000 
	 loss: 32.7697, MinusLogProbMetric: 32.7697, val_loss: 33.0531, val_MinusLogProbMetric: 33.0531

Epoch 97: val_loss did not improve from 32.73266
196/196 - 43s - loss: 32.7697 - MinusLogProbMetric: 32.7697 - val_loss: 33.0531 - val_MinusLogProbMetric: 33.0531 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 98/1000
2023-10-24 05:37:34.303 
Epoch 98/1000 
	 loss: 32.6300, MinusLogProbMetric: 32.6300, val_loss: 33.4702, val_MinusLogProbMetric: 33.4702

Epoch 98: val_loss did not improve from 32.73266
196/196 - 37s - loss: 32.6300 - MinusLogProbMetric: 32.6300 - val_loss: 33.4702 - val_MinusLogProbMetric: 33.4702 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 99/1000
2023-10-24 05:38:12.264 
Epoch 99/1000 
	 loss: 32.5417, MinusLogProbMetric: 32.5417, val_loss: 32.8128, val_MinusLogProbMetric: 32.8128

Epoch 99: val_loss did not improve from 32.73266
196/196 - 38s - loss: 32.5417 - MinusLogProbMetric: 32.5417 - val_loss: 32.8128 - val_MinusLogProbMetric: 32.8128 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 100/1000
2023-10-24 05:38:51.079 
Epoch 100/1000 
	 loss: 32.5536, MinusLogProbMetric: 32.5536, val_loss: 32.6093, val_MinusLogProbMetric: 32.6093

Epoch 100: val_loss improved from 32.73266 to 32.60935, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 32.5536 - MinusLogProbMetric: 32.5536 - val_loss: 32.6093 - val_MinusLogProbMetric: 32.6093 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 101/1000
2023-10-24 05:39:30.669 
Epoch 101/1000 
	 loss: 32.4413, MinusLogProbMetric: 32.4413, val_loss: 32.7682, val_MinusLogProbMetric: 32.7682

Epoch 101: val_loss did not improve from 32.60935
196/196 - 39s - loss: 32.4413 - MinusLogProbMetric: 32.4413 - val_loss: 32.7682 - val_MinusLogProbMetric: 32.7682 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 102/1000
2023-10-24 05:40:12.603 
Epoch 102/1000 
	 loss: 32.3377, MinusLogProbMetric: 32.3377, val_loss: 33.2546, val_MinusLogProbMetric: 33.2546

Epoch 102: val_loss did not improve from 32.60935
196/196 - 42s - loss: 32.3377 - MinusLogProbMetric: 32.3377 - val_loss: 33.2546 - val_MinusLogProbMetric: 33.2546 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 103/1000
2023-10-24 05:40:53.813 
Epoch 103/1000 
	 loss: 32.4834, MinusLogProbMetric: 32.4834, val_loss: 32.5767, val_MinusLogProbMetric: 32.5767

Epoch 103: val_loss improved from 32.60935 to 32.57673, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 32.4834 - MinusLogProbMetric: 32.4834 - val_loss: 32.5767 - val_MinusLogProbMetric: 32.5767 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 104/1000
2023-10-24 05:41:32.613 
Epoch 104/1000 
	 loss: 32.5413, MinusLogProbMetric: 32.5413, val_loss: 32.7535, val_MinusLogProbMetric: 32.7535

Epoch 104: val_loss did not improve from 32.57673
196/196 - 38s - loss: 32.5413 - MinusLogProbMetric: 32.5413 - val_loss: 32.7535 - val_MinusLogProbMetric: 32.7535 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 105/1000
2023-10-24 05:42:09.944 
Epoch 105/1000 
	 loss: 32.8033, MinusLogProbMetric: 32.8033, val_loss: 32.7790, val_MinusLogProbMetric: 32.7790

Epoch 105: val_loss did not improve from 32.57673
196/196 - 37s - loss: 32.8033 - MinusLogProbMetric: 32.8033 - val_loss: 32.7790 - val_MinusLogProbMetric: 32.7790 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 106/1000
2023-10-24 05:42:48.715 
Epoch 106/1000 
	 loss: 32.1729, MinusLogProbMetric: 32.1729, val_loss: 32.8386, val_MinusLogProbMetric: 32.8386

Epoch 106: val_loss did not improve from 32.57673
196/196 - 39s - loss: 32.1729 - MinusLogProbMetric: 32.1729 - val_loss: 32.8386 - val_MinusLogProbMetric: 32.8386 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 107/1000
2023-10-24 05:43:23.495 
Epoch 107/1000 
	 loss: 32.2627, MinusLogProbMetric: 32.2627, val_loss: 32.5542, val_MinusLogProbMetric: 32.5542

Epoch 107: val_loss improved from 32.57673 to 32.55420, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 32.2627 - MinusLogProbMetric: 32.2627 - val_loss: 32.5542 - val_MinusLogProbMetric: 32.5542 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 108/1000
2023-10-24 05:44:01.341 
Epoch 108/1000 
	 loss: 32.4866, MinusLogProbMetric: 32.4866, val_loss: 32.3631, val_MinusLogProbMetric: 32.3631

Epoch 108: val_loss improved from 32.55420 to 32.36311, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 38s - loss: 32.4866 - MinusLogProbMetric: 32.4866 - val_loss: 32.3631 - val_MinusLogProbMetric: 32.3631 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 109/1000
2023-10-24 05:44:40.750 
Epoch 109/1000 
	 loss: 32.1041, MinusLogProbMetric: 32.1041, val_loss: 32.1978, val_MinusLogProbMetric: 32.1978

Epoch 109: val_loss improved from 32.36311 to 32.19780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 32.1041 - MinusLogProbMetric: 32.1041 - val_loss: 32.1978 - val_MinusLogProbMetric: 32.1978 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 110/1000
2023-10-24 05:45:20.974 
Epoch 110/1000 
	 loss: 32.3549, MinusLogProbMetric: 32.3549, val_loss: 32.9007, val_MinusLogProbMetric: 32.9007

Epoch 110: val_loss did not improve from 32.19780
196/196 - 40s - loss: 32.3549 - MinusLogProbMetric: 32.3549 - val_loss: 32.9007 - val_MinusLogProbMetric: 32.9007 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 111/1000
2023-10-24 05:45:58.142 
Epoch 111/1000 
	 loss: 32.1714, MinusLogProbMetric: 32.1714, val_loss: 33.4612, val_MinusLogProbMetric: 33.4612

Epoch 111: val_loss did not improve from 32.19780
196/196 - 37s - loss: 32.1714 - MinusLogProbMetric: 32.1714 - val_loss: 33.4612 - val_MinusLogProbMetric: 33.4612 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 112/1000
2023-10-24 05:46:36.677 
Epoch 112/1000 
	 loss: 32.0494, MinusLogProbMetric: 32.0494, val_loss: 32.1399, val_MinusLogProbMetric: 32.1399

Epoch 112: val_loss improved from 32.19780 to 32.13990, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 32.0494 - MinusLogProbMetric: 32.0494 - val_loss: 32.1399 - val_MinusLogProbMetric: 32.1399 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 113/1000
2023-10-24 05:47:15.791 
Epoch 113/1000 
	 loss: 31.9654, MinusLogProbMetric: 31.9654, val_loss: 32.1141, val_MinusLogProbMetric: 32.1141

Epoch 113: val_loss improved from 32.13990 to 32.11408, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 31.9654 - MinusLogProbMetric: 31.9654 - val_loss: 32.1141 - val_MinusLogProbMetric: 32.1141 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 114/1000
2023-10-24 05:47:54.995 
Epoch 114/1000 
	 loss: 32.1302, MinusLogProbMetric: 32.1302, val_loss: 32.3331, val_MinusLogProbMetric: 32.3331

Epoch 114: val_loss did not improve from 32.11408
196/196 - 39s - loss: 32.1302 - MinusLogProbMetric: 32.1302 - val_loss: 32.3331 - val_MinusLogProbMetric: 32.3331 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 115/1000
2023-10-24 05:48:32.206 
Epoch 115/1000 
	 loss: 32.1429, MinusLogProbMetric: 32.1429, val_loss: 32.2216, val_MinusLogProbMetric: 32.2216

Epoch 115: val_loss did not improve from 32.11408
196/196 - 37s - loss: 32.1429 - MinusLogProbMetric: 32.1429 - val_loss: 32.2216 - val_MinusLogProbMetric: 32.2216 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 116/1000
2023-10-24 05:49:11.000 
Epoch 116/1000 
	 loss: 31.9669, MinusLogProbMetric: 31.9669, val_loss: 32.3060, val_MinusLogProbMetric: 32.3060

Epoch 116: val_loss did not improve from 32.11408
196/196 - 39s - loss: 31.9669 - MinusLogProbMetric: 31.9669 - val_loss: 32.3060 - val_MinusLogProbMetric: 32.3060 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 117/1000
2023-10-24 05:49:48.971 
Epoch 117/1000 
	 loss: 32.2553, MinusLogProbMetric: 32.2553, val_loss: 32.0297, val_MinusLogProbMetric: 32.0297

Epoch 117: val_loss improved from 32.11408 to 32.02966, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 32.2553 - MinusLogProbMetric: 32.2553 - val_loss: 32.0297 - val_MinusLogProbMetric: 32.0297 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 118/1000
2023-10-24 05:50:29.745 
Epoch 118/1000 
	 loss: 31.9442, MinusLogProbMetric: 31.9442, val_loss: 32.7896, val_MinusLogProbMetric: 32.7896

Epoch 118: val_loss did not improve from 32.02966
196/196 - 40s - loss: 31.9442 - MinusLogProbMetric: 31.9442 - val_loss: 32.7896 - val_MinusLogProbMetric: 32.7896 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 119/1000
2023-10-24 05:51:07.455 
Epoch 119/1000 
	 loss: 31.8757, MinusLogProbMetric: 31.8757, val_loss: 34.6595, val_MinusLogProbMetric: 34.6595

Epoch 119: val_loss did not improve from 32.02966
196/196 - 38s - loss: 31.8757 - MinusLogProbMetric: 31.8757 - val_loss: 34.6595 - val_MinusLogProbMetric: 34.6595 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 120/1000
2023-10-24 05:51:44.936 
Epoch 120/1000 
	 loss: 31.8681, MinusLogProbMetric: 31.8681, val_loss: 32.2140, val_MinusLogProbMetric: 32.2140

Epoch 120: val_loss did not improve from 32.02966
196/196 - 37s - loss: 31.8681 - MinusLogProbMetric: 31.8681 - val_loss: 32.2140 - val_MinusLogProbMetric: 32.2140 - lr: 3.3333e-04 - 37s/epoch - 191ms/step
Epoch 121/1000
2023-10-24 05:52:25.747 
Epoch 121/1000 
	 loss: 31.8996, MinusLogProbMetric: 31.8996, val_loss: 32.2304, val_MinusLogProbMetric: 32.2304

Epoch 121: val_loss did not improve from 32.02966
196/196 - 41s - loss: 31.8996 - MinusLogProbMetric: 31.8996 - val_loss: 32.2304 - val_MinusLogProbMetric: 32.2304 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 122/1000
2023-10-24 05:53:04.050 
Epoch 122/1000 
	 loss: 32.0462, MinusLogProbMetric: 32.0462, val_loss: 32.6286, val_MinusLogProbMetric: 32.6286

Epoch 122: val_loss did not improve from 32.02966
196/196 - 38s - loss: 32.0462 - MinusLogProbMetric: 32.0462 - val_loss: 32.6286 - val_MinusLogProbMetric: 32.6286 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 123/1000
2023-10-24 05:53:41.559 
Epoch 123/1000 
	 loss: 31.7812, MinusLogProbMetric: 31.7812, val_loss: 33.2654, val_MinusLogProbMetric: 33.2654

Epoch 123: val_loss did not improve from 32.02966
196/196 - 38s - loss: 31.7812 - MinusLogProbMetric: 31.7812 - val_loss: 33.2654 - val_MinusLogProbMetric: 33.2654 - lr: 3.3333e-04 - 38s/epoch - 191ms/step
Epoch 124/1000
2023-10-24 05:54:19.146 
Epoch 124/1000 
	 loss: 31.8198, MinusLogProbMetric: 31.8198, val_loss: 31.9467, val_MinusLogProbMetric: 31.9467

Epoch 124: val_loss improved from 32.02966 to 31.94668, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 38s - loss: 31.8198 - MinusLogProbMetric: 31.8198 - val_loss: 31.9467 - val_MinusLogProbMetric: 31.9467 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 125/1000
2023-10-24 05:54:57.444 
Epoch 125/1000 
	 loss: 31.6839, MinusLogProbMetric: 31.6839, val_loss: 32.0456, val_MinusLogProbMetric: 32.0456

Epoch 125: val_loss did not improve from 31.94668
196/196 - 38s - loss: 31.6839 - MinusLogProbMetric: 31.6839 - val_loss: 32.0456 - val_MinusLogProbMetric: 32.0456 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 126/1000
2023-10-24 05:55:35.767 
Epoch 126/1000 
	 loss: 31.7981, MinusLogProbMetric: 31.7981, val_loss: 32.2982, val_MinusLogProbMetric: 32.2982

Epoch 126: val_loss did not improve from 31.94668
196/196 - 38s - loss: 31.7981 - MinusLogProbMetric: 31.7981 - val_loss: 32.2982 - val_MinusLogProbMetric: 32.2982 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 127/1000
2023-10-24 05:56:15.519 
Epoch 127/1000 
	 loss: 31.6418, MinusLogProbMetric: 31.6418, val_loss: 32.1767, val_MinusLogProbMetric: 32.1767

Epoch 127: val_loss did not improve from 31.94668
196/196 - 40s - loss: 31.6418 - MinusLogProbMetric: 31.6418 - val_loss: 32.1767 - val_MinusLogProbMetric: 32.1767 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 128/1000
2023-10-24 05:56:56.709 
Epoch 128/1000 
	 loss: 31.5882, MinusLogProbMetric: 31.5882, val_loss: 31.8990, val_MinusLogProbMetric: 31.8990

Epoch 128: val_loss improved from 31.94668 to 31.89896, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 31.5882 - MinusLogProbMetric: 31.5882 - val_loss: 31.8990 - val_MinusLogProbMetric: 31.8990 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 129/1000
2023-10-24 05:57:34.735 
Epoch 129/1000 
	 loss: 31.6160, MinusLogProbMetric: 31.6160, val_loss: 32.5610, val_MinusLogProbMetric: 32.5610

Epoch 129: val_loss did not improve from 31.89896
196/196 - 37s - loss: 31.6160 - MinusLogProbMetric: 31.6160 - val_loss: 32.5610 - val_MinusLogProbMetric: 32.5610 - lr: 3.3333e-04 - 37s/epoch - 191ms/step
Epoch 130/1000
2023-10-24 05:58:15.157 
Epoch 130/1000 
	 loss: 31.6661, MinusLogProbMetric: 31.6661, val_loss: 32.6856, val_MinusLogProbMetric: 32.6856

Epoch 130: val_loss did not improve from 31.89896
196/196 - 40s - loss: 31.6661 - MinusLogProbMetric: 31.6661 - val_loss: 32.6856 - val_MinusLogProbMetric: 32.6856 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 131/1000
2023-10-24 05:58:54.065 
Epoch 131/1000 
	 loss: 31.5298, MinusLogProbMetric: 31.5298, val_loss: 31.8171, val_MinusLogProbMetric: 31.8171

Epoch 131: val_loss improved from 31.89896 to 31.81713, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 31.5298 - MinusLogProbMetric: 31.5298 - val_loss: 31.8171 - val_MinusLogProbMetric: 31.8171 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 132/1000
2023-10-24 05:59:32.354 
Epoch 132/1000 
	 loss: 31.6297, MinusLogProbMetric: 31.6297, val_loss: 32.0385, val_MinusLogProbMetric: 32.0385

Epoch 132: val_loss did not improve from 31.81713
196/196 - 38s - loss: 31.6297 - MinusLogProbMetric: 31.6297 - val_loss: 32.0385 - val_MinusLogProbMetric: 32.0385 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 133/1000
2023-10-24 06:00:12.790 
Epoch 133/1000 
	 loss: 31.5257, MinusLogProbMetric: 31.5257, val_loss: 31.8646, val_MinusLogProbMetric: 31.8646

Epoch 133: val_loss did not improve from 31.81713
196/196 - 40s - loss: 31.5257 - MinusLogProbMetric: 31.5257 - val_loss: 31.8646 - val_MinusLogProbMetric: 31.8646 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 134/1000
2023-10-24 06:00:51.217 
Epoch 134/1000 
	 loss: 31.6015, MinusLogProbMetric: 31.6015, val_loss: 32.4463, val_MinusLogProbMetric: 32.4463

Epoch 134: val_loss did not improve from 31.81713
196/196 - 38s - loss: 31.6015 - MinusLogProbMetric: 31.6015 - val_loss: 32.4463 - val_MinusLogProbMetric: 32.4463 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 135/1000
2023-10-24 06:01:28.242 
Epoch 135/1000 
	 loss: 31.5319, MinusLogProbMetric: 31.5319, val_loss: 32.2918, val_MinusLogProbMetric: 32.2918

Epoch 135: val_loss did not improve from 31.81713
196/196 - 37s - loss: 31.5319 - MinusLogProbMetric: 31.5319 - val_loss: 32.2918 - val_MinusLogProbMetric: 32.2918 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 136/1000
2023-10-24 06:02:06.873 
Epoch 136/1000 
	 loss: 31.4713, MinusLogProbMetric: 31.4713, val_loss: 32.2572, val_MinusLogProbMetric: 32.2572

Epoch 136: val_loss did not improve from 31.81713
196/196 - 39s - loss: 31.4713 - MinusLogProbMetric: 31.4713 - val_loss: 32.2572 - val_MinusLogProbMetric: 32.2572 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 137/1000
2023-10-24 06:02:45.823 
Epoch 137/1000 
	 loss: 31.4316, MinusLogProbMetric: 31.4316, val_loss: 32.0676, val_MinusLogProbMetric: 32.0676

Epoch 137: val_loss did not improve from 31.81713
196/196 - 39s - loss: 31.4316 - MinusLogProbMetric: 31.4316 - val_loss: 32.0676 - val_MinusLogProbMetric: 32.0676 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 138/1000
2023-10-24 06:03:28.489 
Epoch 138/1000 
	 loss: 31.4485, MinusLogProbMetric: 31.4485, val_loss: 31.7116, val_MinusLogProbMetric: 31.7116

Epoch 138: val_loss improved from 31.81713 to 31.71160, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 43s - loss: 31.4485 - MinusLogProbMetric: 31.4485 - val_loss: 31.7116 - val_MinusLogProbMetric: 31.7116 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 139/1000
2023-10-24 06:04:10.841 
Epoch 139/1000 
	 loss: 31.4892, MinusLogProbMetric: 31.4892, val_loss: 31.5232, val_MinusLogProbMetric: 31.5232

Epoch 139: val_loss improved from 31.71160 to 31.52324, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 31.4892 - MinusLogProbMetric: 31.4892 - val_loss: 31.5232 - val_MinusLogProbMetric: 31.5232 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 140/1000
2023-10-24 06:04:50.483 
Epoch 140/1000 
	 loss: 31.5248, MinusLogProbMetric: 31.5248, val_loss: 31.6493, val_MinusLogProbMetric: 31.6493

Epoch 140: val_loss did not improve from 31.52324
196/196 - 39s - loss: 31.5248 - MinusLogProbMetric: 31.5248 - val_loss: 31.6493 - val_MinusLogProbMetric: 31.6493 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 141/1000
2023-10-24 06:05:27.356 
Epoch 141/1000 
	 loss: 31.3629, MinusLogProbMetric: 31.3629, val_loss: 31.9455, val_MinusLogProbMetric: 31.9455

Epoch 141: val_loss did not improve from 31.52324
196/196 - 37s - loss: 31.3629 - MinusLogProbMetric: 31.3629 - val_loss: 31.9455 - val_MinusLogProbMetric: 31.9455 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 142/1000
2023-10-24 06:06:07.014 
Epoch 142/1000 
	 loss: 31.4815, MinusLogProbMetric: 31.4815, val_loss: 34.0868, val_MinusLogProbMetric: 34.0868

Epoch 142: val_loss did not improve from 31.52324
196/196 - 40s - loss: 31.4815 - MinusLogProbMetric: 31.4815 - val_loss: 34.0868 - val_MinusLogProbMetric: 34.0868 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 143/1000
2023-10-24 06:06:45.558 
Epoch 143/1000 
	 loss: 31.6162, MinusLogProbMetric: 31.6162, val_loss: 31.5118, val_MinusLogProbMetric: 31.5118

Epoch 143: val_loss improved from 31.52324 to 31.51176, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 31.6162 - MinusLogProbMetric: 31.6162 - val_loss: 31.5118 - val_MinusLogProbMetric: 31.5118 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 144/1000
2023-10-24 06:07:25.962 
Epoch 144/1000 
	 loss: 31.1741, MinusLogProbMetric: 31.1741, val_loss: 32.0762, val_MinusLogProbMetric: 32.0762

Epoch 144: val_loss did not improve from 31.51176
196/196 - 40s - loss: 31.1741 - MinusLogProbMetric: 31.1741 - val_loss: 32.0762 - val_MinusLogProbMetric: 32.0762 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 145/1000
2023-10-24 06:08:04.680 
Epoch 145/1000 
	 loss: 31.2262, MinusLogProbMetric: 31.2262, val_loss: 31.2710, val_MinusLogProbMetric: 31.2710

Epoch 145: val_loss improved from 31.51176 to 31.27098, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 31.2262 - MinusLogProbMetric: 31.2262 - val_loss: 31.2710 - val_MinusLogProbMetric: 31.2710 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 146/1000
2023-10-24 06:08:43.837 
Epoch 146/1000 
	 loss: 31.1688, MinusLogProbMetric: 31.1688, val_loss: 31.4901, val_MinusLogProbMetric: 31.4901

Epoch 146: val_loss did not improve from 31.27098
196/196 - 39s - loss: 31.1688 - MinusLogProbMetric: 31.1688 - val_loss: 31.4901 - val_MinusLogProbMetric: 31.4901 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 147/1000
2023-10-24 06:09:22.998 
Epoch 147/1000 
	 loss: 31.2277, MinusLogProbMetric: 31.2277, val_loss: 31.7954, val_MinusLogProbMetric: 31.7954

Epoch 147: val_loss did not improve from 31.27098
196/196 - 39s - loss: 31.2277 - MinusLogProbMetric: 31.2277 - val_loss: 31.7954 - val_MinusLogProbMetric: 31.7954 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 148/1000
2023-10-24 06:10:01.634 
Epoch 148/1000 
	 loss: 31.3607, MinusLogProbMetric: 31.3607, val_loss: 31.6176, val_MinusLogProbMetric: 31.6176

Epoch 148: val_loss did not improve from 31.27098
196/196 - 39s - loss: 31.3607 - MinusLogProbMetric: 31.3607 - val_loss: 31.6176 - val_MinusLogProbMetric: 31.6176 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 149/1000
2023-10-24 06:10:39.974 
Epoch 149/1000 
	 loss: 31.0945, MinusLogProbMetric: 31.0945, val_loss: 31.2524, val_MinusLogProbMetric: 31.2524

Epoch 149: val_loss improved from 31.27098 to 31.25238, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 31.0945 - MinusLogProbMetric: 31.0945 - val_loss: 31.2524 - val_MinusLogProbMetric: 31.2524 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 150/1000
2023-10-24 06:11:18.844 
Epoch 150/1000 
	 loss: 31.0373, MinusLogProbMetric: 31.0373, val_loss: 31.4757, val_MinusLogProbMetric: 31.4757

Epoch 150: val_loss did not improve from 31.25238
196/196 - 38s - loss: 31.0373 - MinusLogProbMetric: 31.0373 - val_loss: 31.4757 - val_MinusLogProbMetric: 31.4757 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 151/1000
2023-10-24 06:11:59.114 
Epoch 151/1000 
	 loss: 31.2838, MinusLogProbMetric: 31.2838, val_loss: 31.1927, val_MinusLogProbMetric: 31.1927

Epoch 151: val_loss improved from 31.25238 to 31.19267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 41s - loss: 31.2838 - MinusLogProbMetric: 31.2838 - val_loss: 31.1927 - val_MinusLogProbMetric: 31.1927 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 152/1000
2023-10-24 06:12:37.711 
Epoch 152/1000 
	 loss: 31.3957, MinusLogProbMetric: 31.3957, val_loss: 31.0952, val_MinusLogProbMetric: 31.0952

Epoch 152: val_loss improved from 31.19267 to 31.09516, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 31.3957 - MinusLogProbMetric: 31.3957 - val_loss: 31.0952 - val_MinusLogProbMetric: 31.0952 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 153/1000
2023-10-24 06:13:18.646 
Epoch 153/1000 
	 loss: 31.2104, MinusLogProbMetric: 31.2104, val_loss: 31.3082, val_MinusLogProbMetric: 31.3082

Epoch 153: val_loss did not improve from 31.09516
196/196 - 40s - loss: 31.2104 - MinusLogProbMetric: 31.2104 - val_loss: 31.3082 - val_MinusLogProbMetric: 31.3082 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 154/1000
2023-10-24 06:13:55.766 
Epoch 154/1000 
	 loss: 31.1198, MinusLogProbMetric: 31.1198, val_loss: 31.6086, val_MinusLogProbMetric: 31.6086

Epoch 154: val_loss did not improve from 31.09516
196/196 - 37s - loss: 31.1198 - MinusLogProbMetric: 31.1198 - val_loss: 31.6086 - val_MinusLogProbMetric: 31.6086 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 155/1000
2023-10-24 06:14:35.114 
Epoch 155/1000 
	 loss: 31.0621, MinusLogProbMetric: 31.0621, val_loss: 31.6397, val_MinusLogProbMetric: 31.6397

Epoch 155: val_loss did not improve from 31.09516
196/196 - 39s - loss: 31.0621 - MinusLogProbMetric: 31.0621 - val_loss: 31.6397 - val_MinusLogProbMetric: 31.6397 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 156/1000
2023-10-24 06:15:15.313 
Epoch 156/1000 
	 loss: 30.9478, MinusLogProbMetric: 30.9478, val_loss: 31.5038, val_MinusLogProbMetric: 31.5038

Epoch 156: val_loss did not improve from 31.09516
196/196 - 40s - loss: 30.9478 - MinusLogProbMetric: 30.9478 - val_loss: 31.5038 - val_MinusLogProbMetric: 31.5038 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 157/1000
2023-10-24 06:15:52.225 
Epoch 157/1000 
	 loss: 30.9933, MinusLogProbMetric: 30.9933, val_loss: 31.3788, val_MinusLogProbMetric: 31.3788

Epoch 157: val_loss did not improve from 31.09516
196/196 - 37s - loss: 30.9933 - MinusLogProbMetric: 30.9933 - val_loss: 31.3788 - val_MinusLogProbMetric: 31.3788 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 158/1000
2023-10-24 06:16:32.432 
Epoch 158/1000 
	 loss: 30.9393, MinusLogProbMetric: 30.9393, val_loss: 31.3153, val_MinusLogProbMetric: 31.3153

Epoch 158: val_loss did not improve from 31.09516
196/196 - 40s - loss: 30.9393 - MinusLogProbMetric: 30.9393 - val_loss: 31.3153 - val_MinusLogProbMetric: 31.3153 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 159/1000
2023-10-24 06:17:10.040 
Epoch 159/1000 
	 loss: 31.1179, MinusLogProbMetric: 31.1179, val_loss: 30.9819, val_MinusLogProbMetric: 30.9819

Epoch 159: val_loss improved from 31.09516 to 30.98195, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 38s - loss: 31.1179 - MinusLogProbMetric: 31.1179 - val_loss: 30.9819 - val_MinusLogProbMetric: 30.9819 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 160/1000
2023-10-24 06:17:50.497 
Epoch 160/1000 
	 loss: 31.0382, MinusLogProbMetric: 31.0382, val_loss: 31.1997, val_MinusLogProbMetric: 31.1997

Epoch 160: val_loss did not improve from 30.98195
196/196 - 40s - loss: 31.0382 - MinusLogProbMetric: 31.0382 - val_loss: 31.1997 - val_MinusLogProbMetric: 31.1997 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 161/1000
2023-10-24 06:18:32.346 
Epoch 161/1000 
	 loss: 30.9916, MinusLogProbMetric: 30.9916, val_loss: 31.6027, val_MinusLogProbMetric: 31.6027

Epoch 161: val_loss did not improve from 30.98195
196/196 - 42s - loss: 30.9916 - MinusLogProbMetric: 30.9916 - val_loss: 31.6027 - val_MinusLogProbMetric: 31.6027 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 162/1000
2023-10-24 06:19:13.167 
Epoch 162/1000 
	 loss: 31.0222, MinusLogProbMetric: 31.0222, val_loss: 31.7219, val_MinusLogProbMetric: 31.7219

Epoch 162: val_loss did not improve from 30.98195
196/196 - 41s - loss: 31.0222 - MinusLogProbMetric: 31.0222 - val_loss: 31.7219 - val_MinusLogProbMetric: 31.7219 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 163/1000
2023-10-24 06:19:55.087 
Epoch 163/1000 
	 loss: 31.0219, MinusLogProbMetric: 31.0219, val_loss: 31.5041, val_MinusLogProbMetric: 31.5041

Epoch 163: val_loss did not improve from 30.98195
196/196 - 42s - loss: 31.0219 - MinusLogProbMetric: 31.0219 - val_loss: 31.5041 - val_MinusLogProbMetric: 31.5041 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 164/1000
2023-10-24 06:20:36.888 
Epoch 164/1000 
	 loss: 31.1960, MinusLogProbMetric: 31.1960, val_loss: 31.0375, val_MinusLogProbMetric: 31.0375

Epoch 164: val_loss did not improve from 30.98195
196/196 - 42s - loss: 31.1960 - MinusLogProbMetric: 31.1960 - val_loss: 31.0375 - val_MinusLogProbMetric: 31.0375 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 165/1000
2023-10-24 06:21:18.784 
Epoch 165/1000 
	 loss: 31.1407, MinusLogProbMetric: 31.1407, val_loss: 31.3682, val_MinusLogProbMetric: 31.3682

Epoch 165: val_loss did not improve from 30.98195
196/196 - 42s - loss: 31.1407 - MinusLogProbMetric: 31.1407 - val_loss: 31.3682 - val_MinusLogProbMetric: 31.3682 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 166/1000
2023-10-24 06:21:58.065 
Epoch 166/1000 
	 loss: 30.7919, MinusLogProbMetric: 30.7919, val_loss: 31.7892, val_MinusLogProbMetric: 31.7892

Epoch 166: val_loss did not improve from 30.98195
196/196 - 39s - loss: 30.7919 - MinusLogProbMetric: 30.7919 - val_loss: 31.7892 - val_MinusLogProbMetric: 31.7892 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 167/1000
2023-10-24 06:22:40.166 
Epoch 167/1000 
	 loss: 31.2958, MinusLogProbMetric: 31.2958, val_loss: 30.7245, val_MinusLogProbMetric: 30.7245

Epoch 167: val_loss improved from 30.98195 to 30.72452, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 43s - loss: 31.2958 - MinusLogProbMetric: 31.2958 - val_loss: 30.7245 - val_MinusLogProbMetric: 30.7245 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 168/1000
2023-10-24 06:23:22.355 
Epoch 168/1000 
	 loss: 30.9994, MinusLogProbMetric: 30.9994, val_loss: 31.6520, val_MinusLogProbMetric: 31.6520

Epoch 168: val_loss did not improve from 30.72452
196/196 - 42s - loss: 30.9994 - MinusLogProbMetric: 30.9994 - val_loss: 31.6520 - val_MinusLogProbMetric: 31.6520 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 169/1000
2023-10-24 06:24:01.847 
Epoch 169/1000 
	 loss: 30.8786, MinusLogProbMetric: 30.8786, val_loss: 30.8011, val_MinusLogProbMetric: 30.8011

Epoch 169: val_loss did not improve from 30.72452
196/196 - 39s - loss: 30.8786 - MinusLogProbMetric: 30.8786 - val_loss: 30.8011 - val_MinusLogProbMetric: 30.8011 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 170/1000
2023-10-24 06:24:43.684 
Epoch 170/1000 
	 loss: 30.7943, MinusLogProbMetric: 30.7943, val_loss: 30.8731, val_MinusLogProbMetric: 30.8731

Epoch 170: val_loss did not improve from 30.72452
196/196 - 42s - loss: 30.7943 - MinusLogProbMetric: 30.7943 - val_loss: 30.8731 - val_MinusLogProbMetric: 30.8731 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 171/1000
2023-10-24 06:25:25.257 
Epoch 171/1000 
	 loss: 30.8025, MinusLogProbMetric: 30.8025, val_loss: 31.3467, val_MinusLogProbMetric: 31.3467

Epoch 171: val_loss did not improve from 30.72452
196/196 - 42s - loss: 30.8025 - MinusLogProbMetric: 30.8025 - val_loss: 31.3467 - val_MinusLogProbMetric: 31.3467 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 172/1000
2023-10-24 06:26:05.388 
Epoch 172/1000 
	 loss: 31.0292, MinusLogProbMetric: 31.0292, val_loss: 31.3489, val_MinusLogProbMetric: 31.3489

Epoch 172: val_loss did not improve from 30.72452
196/196 - 40s - loss: 31.0292 - MinusLogProbMetric: 31.0292 - val_loss: 31.3489 - val_MinusLogProbMetric: 31.3489 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 173/1000
2023-10-24 06:26:44.380 
Epoch 173/1000 
	 loss: 30.8387, MinusLogProbMetric: 30.8387, val_loss: 31.0588, val_MinusLogProbMetric: 31.0588

Epoch 173: val_loss did not improve from 30.72452
196/196 - 39s - loss: 30.8387 - MinusLogProbMetric: 30.8387 - val_loss: 31.0588 - val_MinusLogProbMetric: 31.0588 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 174/1000
2023-10-24 06:27:25.989 
Epoch 174/1000 
	 loss: 30.7351, MinusLogProbMetric: 30.7351, val_loss: 30.7674, val_MinusLogProbMetric: 30.7674

Epoch 174: val_loss did not improve from 30.72452
196/196 - 42s - loss: 30.7351 - MinusLogProbMetric: 30.7351 - val_loss: 30.7674 - val_MinusLogProbMetric: 30.7674 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 175/1000
2023-10-24 06:28:07.950 
Epoch 175/1000 
	 loss: 30.7674, MinusLogProbMetric: 30.7674, val_loss: 30.8403, val_MinusLogProbMetric: 30.8403

Epoch 175: val_loss did not improve from 30.72452
196/196 - 42s - loss: 30.7674 - MinusLogProbMetric: 30.7674 - val_loss: 30.8403 - val_MinusLogProbMetric: 30.8403 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 176/1000
2023-10-24 06:28:49.518 
Epoch 176/1000 
	 loss: 30.5987, MinusLogProbMetric: 30.5987, val_loss: 30.8652, val_MinusLogProbMetric: 30.8652

Epoch 176: val_loss did not improve from 30.72452
196/196 - 42s - loss: 30.5987 - MinusLogProbMetric: 30.5987 - val_loss: 30.8652 - val_MinusLogProbMetric: 30.8652 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 177/1000
2023-10-24 06:29:31.471 
Epoch 177/1000 
	 loss: 30.9968, MinusLogProbMetric: 30.9968, val_loss: 31.1228, val_MinusLogProbMetric: 31.1228

Epoch 177: val_loss did not improve from 30.72452
196/196 - 42s - loss: 30.9968 - MinusLogProbMetric: 30.9968 - val_loss: 31.1228 - val_MinusLogProbMetric: 31.1228 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 178/1000
2023-10-24 06:30:12.267 
Epoch 178/1000 
	 loss: 30.6010, MinusLogProbMetric: 30.6010, val_loss: 31.5572, val_MinusLogProbMetric: 31.5572

Epoch 178: val_loss did not improve from 30.72452
196/196 - 41s - loss: 30.6010 - MinusLogProbMetric: 30.6010 - val_loss: 31.5572 - val_MinusLogProbMetric: 31.5572 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 179/1000
2023-10-24 06:30:54.285 
Epoch 179/1000 
	 loss: 30.6401, MinusLogProbMetric: 30.6401, val_loss: 31.0148, val_MinusLogProbMetric: 31.0148

Epoch 179: val_loss did not improve from 30.72452
196/196 - 42s - loss: 30.6401 - MinusLogProbMetric: 30.6401 - val_loss: 31.0148 - val_MinusLogProbMetric: 31.0148 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 180/1000
2023-10-24 06:31:36.165 
Epoch 180/1000 
	 loss: 30.6901, MinusLogProbMetric: 30.6901, val_loss: 31.0377, val_MinusLogProbMetric: 31.0377

Epoch 180: val_loss did not improve from 30.72452
196/196 - 42s - loss: 30.6901 - MinusLogProbMetric: 30.6901 - val_loss: 31.0377 - val_MinusLogProbMetric: 31.0377 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 181/1000
2023-10-24 06:32:17.602 
Epoch 181/1000 
	 loss: 30.6873, MinusLogProbMetric: 30.6873, val_loss: 30.5265, val_MinusLogProbMetric: 30.5265

Epoch 181: val_loss improved from 30.72452 to 30.52648, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 30.6873 - MinusLogProbMetric: 30.6873 - val_loss: 30.5265 - val_MinusLogProbMetric: 30.5265 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 182/1000
2023-10-24 06:32:58.434 
Epoch 182/1000 
	 loss: 30.7988, MinusLogProbMetric: 30.7988, val_loss: 31.1475, val_MinusLogProbMetric: 31.1475

Epoch 182: val_loss did not improve from 30.52648
196/196 - 40s - loss: 30.7988 - MinusLogProbMetric: 30.7988 - val_loss: 31.1475 - val_MinusLogProbMetric: 31.1475 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 183/1000
2023-10-24 06:33:37.142 
Epoch 183/1000 
	 loss: 30.6045, MinusLogProbMetric: 30.6045, val_loss: 32.3170, val_MinusLogProbMetric: 32.3170

Epoch 183: val_loss did not improve from 30.52648
196/196 - 39s - loss: 30.6045 - MinusLogProbMetric: 30.6045 - val_loss: 32.3170 - val_MinusLogProbMetric: 32.3170 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 184/1000
2023-10-24 06:34:17.102 
Epoch 184/1000 
	 loss: 30.8404, MinusLogProbMetric: 30.8404, val_loss: 33.6486, val_MinusLogProbMetric: 33.6486

Epoch 184: val_loss did not improve from 30.52648
196/196 - 40s - loss: 30.8404 - MinusLogProbMetric: 30.8404 - val_loss: 33.6486 - val_MinusLogProbMetric: 33.6486 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 185/1000
2023-10-24 06:34:58.735 
Epoch 185/1000 
	 loss: 30.6417, MinusLogProbMetric: 30.6417, val_loss: 31.3448, val_MinusLogProbMetric: 31.3448

Epoch 185: val_loss did not improve from 30.52648
196/196 - 42s - loss: 30.6417 - MinusLogProbMetric: 30.6417 - val_loss: 31.3448 - val_MinusLogProbMetric: 31.3448 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 186/1000
2023-10-24 06:35:38.288 
Epoch 186/1000 
	 loss: 30.6379, MinusLogProbMetric: 30.6379, val_loss: 30.9364, val_MinusLogProbMetric: 30.9364

Epoch 186: val_loss did not improve from 30.52648
196/196 - 40s - loss: 30.6379 - MinusLogProbMetric: 30.6379 - val_loss: 30.9364 - val_MinusLogProbMetric: 30.9364 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 187/1000
2023-10-24 06:36:18.780 
Epoch 187/1000 
	 loss: 30.5397, MinusLogProbMetric: 30.5397, val_loss: 30.8808, val_MinusLogProbMetric: 30.8808

Epoch 187: val_loss did not improve from 30.52648
196/196 - 40s - loss: 30.5397 - MinusLogProbMetric: 30.5397 - val_loss: 30.8808 - val_MinusLogProbMetric: 30.8808 - lr: 3.3333e-04 - 40s/epoch - 207ms/step
Epoch 188/1000
2023-10-24 06:37:00.455 
Epoch 188/1000 
	 loss: 30.4985, MinusLogProbMetric: 30.4985, val_loss: 31.5622, val_MinusLogProbMetric: 31.5622

Epoch 188: val_loss did not improve from 30.52648
196/196 - 42s - loss: 30.4985 - MinusLogProbMetric: 30.4985 - val_loss: 31.5622 - val_MinusLogProbMetric: 31.5622 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 189/1000
2023-10-24 06:37:42.360 
Epoch 189/1000 
	 loss: 30.5832, MinusLogProbMetric: 30.5832, val_loss: 31.3288, val_MinusLogProbMetric: 31.3288

Epoch 189: val_loss did not improve from 30.52648
196/196 - 42s - loss: 30.5832 - MinusLogProbMetric: 30.5832 - val_loss: 31.3288 - val_MinusLogProbMetric: 31.3288 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 190/1000
2023-10-24 06:38:24.365 
Epoch 190/1000 
	 loss: 30.5175, MinusLogProbMetric: 30.5175, val_loss: 31.4411, val_MinusLogProbMetric: 31.4411

Epoch 190: val_loss did not improve from 30.52648
196/196 - 42s - loss: 30.5175 - MinusLogProbMetric: 30.5175 - val_loss: 31.4411 - val_MinusLogProbMetric: 31.4411 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 191/1000
2023-10-24 06:39:05.434 
Epoch 191/1000 
	 loss: 30.4661, MinusLogProbMetric: 30.4661, val_loss: 30.4952, val_MinusLogProbMetric: 30.4952

Epoch 191: val_loss improved from 30.52648 to 30.49516, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 30.4661 - MinusLogProbMetric: 30.4661 - val_loss: 30.4952 - val_MinusLogProbMetric: 30.4952 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 192/1000
2023-10-24 06:39:46.748 
Epoch 192/1000 
	 loss: 30.4800, MinusLogProbMetric: 30.4800, val_loss: 30.8144, val_MinusLogProbMetric: 30.8144

Epoch 192: val_loss did not improve from 30.49516
196/196 - 41s - loss: 30.4800 - MinusLogProbMetric: 30.4800 - val_loss: 30.8144 - val_MinusLogProbMetric: 30.8144 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 193/1000
2023-10-24 06:40:29.074 
Epoch 193/1000 
	 loss: 30.4752, MinusLogProbMetric: 30.4752, val_loss: 32.7526, val_MinusLogProbMetric: 32.7526

Epoch 193: val_loss did not improve from 30.49516
196/196 - 42s - loss: 30.4752 - MinusLogProbMetric: 30.4752 - val_loss: 32.7526 - val_MinusLogProbMetric: 32.7526 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 194/1000
2023-10-24 06:41:10.844 
Epoch 194/1000 
	 loss: 30.5677, MinusLogProbMetric: 30.5677, val_loss: 30.6098, val_MinusLogProbMetric: 30.6098

Epoch 194: val_loss did not improve from 30.49516
196/196 - 42s - loss: 30.5677 - MinusLogProbMetric: 30.5677 - val_loss: 30.6098 - val_MinusLogProbMetric: 30.6098 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 195/1000
2023-10-24 06:41:49.863 
Epoch 195/1000 
	 loss: 30.3836, MinusLogProbMetric: 30.3836, val_loss: 30.5858, val_MinusLogProbMetric: 30.5858

Epoch 195: val_loss did not improve from 30.49516
196/196 - 39s - loss: 30.3836 - MinusLogProbMetric: 30.3836 - val_loss: 30.5858 - val_MinusLogProbMetric: 30.5858 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 196/1000
2023-10-24 06:42:31.161 
Epoch 196/1000 
	 loss: 30.5257, MinusLogProbMetric: 30.5257, val_loss: 31.4011, val_MinusLogProbMetric: 31.4011

Epoch 196: val_loss did not improve from 30.49516
196/196 - 41s - loss: 30.5257 - MinusLogProbMetric: 30.5257 - val_loss: 31.4011 - val_MinusLogProbMetric: 31.4011 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 197/1000
2023-10-24 06:43:13.110 
Epoch 197/1000 
	 loss: 30.4413, MinusLogProbMetric: 30.4413, val_loss: 31.1705, val_MinusLogProbMetric: 31.1705

Epoch 197: val_loss did not improve from 30.49516
196/196 - 42s - loss: 30.4413 - MinusLogProbMetric: 30.4413 - val_loss: 31.1705 - val_MinusLogProbMetric: 31.1705 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 198/1000
2023-10-24 06:43:54.075 
Epoch 198/1000 
	 loss: 30.4492, MinusLogProbMetric: 30.4492, val_loss: 31.7178, val_MinusLogProbMetric: 31.7178

Epoch 198: val_loss did not improve from 30.49516
196/196 - 41s - loss: 30.4492 - MinusLogProbMetric: 30.4492 - val_loss: 31.7178 - val_MinusLogProbMetric: 31.7178 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 199/1000
2023-10-24 06:44:35.485 
Epoch 199/1000 
	 loss: 30.5116, MinusLogProbMetric: 30.5116, val_loss: 30.3523, val_MinusLogProbMetric: 30.3523

Epoch 199: val_loss improved from 30.49516 to 30.35229, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 30.5116 - MinusLogProbMetric: 30.5116 - val_loss: 30.3523 - val_MinusLogProbMetric: 30.3523 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 200/1000
2023-10-24 06:45:17.483 
Epoch 200/1000 
	 loss: 30.3739, MinusLogProbMetric: 30.3739, val_loss: 30.7745, val_MinusLogProbMetric: 30.7745

Epoch 200: val_loss did not improve from 30.35229
196/196 - 41s - loss: 30.3739 - MinusLogProbMetric: 30.3739 - val_loss: 30.7745 - val_MinusLogProbMetric: 30.7745 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 201/1000
2023-10-24 06:45:58.831 
Epoch 201/1000 
	 loss: 30.3922, MinusLogProbMetric: 30.3922, val_loss: 30.6279, val_MinusLogProbMetric: 30.6279

Epoch 201: val_loss did not improve from 30.35229
196/196 - 41s - loss: 30.3922 - MinusLogProbMetric: 30.3922 - val_loss: 30.6279 - val_MinusLogProbMetric: 30.6279 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 202/1000
2023-10-24 06:46:40.144 
Epoch 202/1000 
	 loss: 30.3497, MinusLogProbMetric: 30.3497, val_loss: 30.2943, val_MinusLogProbMetric: 30.2943

Epoch 202: val_loss improved from 30.35229 to 30.29426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 30.3497 - MinusLogProbMetric: 30.3497 - val_loss: 30.2943 - val_MinusLogProbMetric: 30.2943 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 203/1000
2023-10-24 06:47:22.360 
Epoch 203/1000 
	 loss: 30.3780, MinusLogProbMetric: 30.3780, val_loss: 30.7784, val_MinusLogProbMetric: 30.7784

Epoch 203: val_loss did not improve from 30.29426
196/196 - 42s - loss: 30.3780 - MinusLogProbMetric: 30.3780 - val_loss: 30.7784 - val_MinusLogProbMetric: 30.7784 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 204/1000
2023-10-24 06:48:04.211 
Epoch 204/1000 
	 loss: 30.3673, MinusLogProbMetric: 30.3673, val_loss: 31.0982, val_MinusLogProbMetric: 31.0982

Epoch 204: val_loss did not improve from 30.29426
196/196 - 42s - loss: 30.3673 - MinusLogProbMetric: 30.3673 - val_loss: 31.0982 - val_MinusLogProbMetric: 31.0982 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 205/1000
2023-10-24 06:48:46.125 
Epoch 205/1000 
	 loss: 30.3936, MinusLogProbMetric: 30.3936, val_loss: 31.7964, val_MinusLogProbMetric: 31.7964

Epoch 205: val_loss did not improve from 30.29426
196/196 - 42s - loss: 30.3936 - MinusLogProbMetric: 30.3936 - val_loss: 31.7964 - val_MinusLogProbMetric: 31.7964 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 206/1000
2023-10-24 06:49:27.695 
Epoch 206/1000 
	 loss: 30.2790, MinusLogProbMetric: 30.2790, val_loss: 30.6825, val_MinusLogProbMetric: 30.6825

Epoch 206: val_loss did not improve from 30.29426
196/196 - 42s - loss: 30.2790 - MinusLogProbMetric: 30.2790 - val_loss: 30.6825 - val_MinusLogProbMetric: 30.6825 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 207/1000
2023-10-24 06:50:09.077 
Epoch 207/1000 
	 loss: 30.2739, MinusLogProbMetric: 30.2739, val_loss: 31.1440, val_MinusLogProbMetric: 31.1440

Epoch 207: val_loss did not improve from 30.29426
196/196 - 41s - loss: 30.2739 - MinusLogProbMetric: 30.2739 - val_loss: 31.1440 - val_MinusLogProbMetric: 31.1440 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 208/1000
2023-10-24 06:50:50.903 
Epoch 208/1000 
	 loss: 30.2631, MinusLogProbMetric: 30.2631, val_loss: 30.5254, val_MinusLogProbMetric: 30.5254

Epoch 208: val_loss did not improve from 30.29426
196/196 - 42s - loss: 30.2631 - MinusLogProbMetric: 30.2631 - val_loss: 30.5254 - val_MinusLogProbMetric: 30.5254 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 209/1000
2023-10-24 06:51:32.220 
Epoch 209/1000 
	 loss: 30.3039, MinusLogProbMetric: 30.3039, val_loss: 30.9811, val_MinusLogProbMetric: 30.9811

Epoch 209: val_loss did not improve from 30.29426
196/196 - 41s - loss: 30.3039 - MinusLogProbMetric: 30.3039 - val_loss: 30.9811 - val_MinusLogProbMetric: 30.9811 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 210/1000
2023-10-24 06:52:13.588 
Epoch 210/1000 
	 loss: 30.3117, MinusLogProbMetric: 30.3117, val_loss: 30.4667, val_MinusLogProbMetric: 30.4667

Epoch 210: val_loss did not improve from 30.29426
196/196 - 41s - loss: 30.3117 - MinusLogProbMetric: 30.3117 - val_loss: 30.4667 - val_MinusLogProbMetric: 30.4667 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 211/1000
2023-10-24 06:52:54.426 
Epoch 211/1000 
	 loss: 30.3627, MinusLogProbMetric: 30.3627, val_loss: 31.3188, val_MinusLogProbMetric: 31.3188

Epoch 211: val_loss did not improve from 30.29426
196/196 - 41s - loss: 30.3627 - MinusLogProbMetric: 30.3627 - val_loss: 31.3188 - val_MinusLogProbMetric: 31.3188 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 212/1000
2023-10-24 06:53:35.869 
Epoch 212/1000 
	 loss: 30.2399, MinusLogProbMetric: 30.2399, val_loss: 30.3720, val_MinusLogProbMetric: 30.3720

Epoch 212: val_loss did not improve from 30.29426
196/196 - 41s - loss: 30.2399 - MinusLogProbMetric: 30.2399 - val_loss: 30.3720 - val_MinusLogProbMetric: 30.3720 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 213/1000
2023-10-24 06:54:17.687 
Epoch 213/1000 
	 loss: 30.3431, MinusLogProbMetric: 30.3431, val_loss: 30.3519, val_MinusLogProbMetric: 30.3519

Epoch 213: val_loss did not improve from 30.29426
196/196 - 42s - loss: 30.3431 - MinusLogProbMetric: 30.3431 - val_loss: 30.3519 - val_MinusLogProbMetric: 30.3519 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 214/1000
2023-10-24 06:54:59.323 
Epoch 214/1000 
	 loss: 30.2132, MinusLogProbMetric: 30.2132, val_loss: 31.0609, val_MinusLogProbMetric: 31.0609

Epoch 214: val_loss did not improve from 30.29426
196/196 - 42s - loss: 30.2132 - MinusLogProbMetric: 30.2132 - val_loss: 31.0609 - val_MinusLogProbMetric: 31.0609 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 215/1000
2023-10-24 06:55:41.095 
Epoch 215/1000 
	 loss: 30.4539, MinusLogProbMetric: 30.4539, val_loss: 30.7022, val_MinusLogProbMetric: 30.7022

Epoch 215: val_loss did not improve from 30.29426
196/196 - 42s - loss: 30.4539 - MinusLogProbMetric: 30.4539 - val_loss: 30.7022 - val_MinusLogProbMetric: 30.7022 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 216/1000
2023-10-24 06:56:22.183 
Epoch 216/1000 
	 loss: 30.1727, MinusLogProbMetric: 30.1727, val_loss: 31.0162, val_MinusLogProbMetric: 31.0162

Epoch 216: val_loss did not improve from 30.29426
196/196 - 41s - loss: 30.1727 - MinusLogProbMetric: 30.1727 - val_loss: 31.0162 - val_MinusLogProbMetric: 31.0162 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 217/1000
2023-10-24 06:57:03.835 
Epoch 217/1000 
	 loss: 30.1689, MinusLogProbMetric: 30.1689, val_loss: 30.7935, val_MinusLogProbMetric: 30.7935

Epoch 217: val_loss did not improve from 30.29426
196/196 - 42s - loss: 30.1689 - MinusLogProbMetric: 30.1689 - val_loss: 30.7935 - val_MinusLogProbMetric: 30.7935 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 218/1000
2023-10-24 06:57:45.427 
Epoch 218/1000 
	 loss: 30.2333, MinusLogProbMetric: 30.2333, val_loss: 30.2411, val_MinusLogProbMetric: 30.2411

Epoch 218: val_loss improved from 30.29426 to 30.24107, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 30.2333 - MinusLogProbMetric: 30.2333 - val_loss: 30.2411 - val_MinusLogProbMetric: 30.2411 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 219/1000
2023-10-24 06:58:27.808 
Epoch 219/1000 
	 loss: 30.1519, MinusLogProbMetric: 30.1519, val_loss: 30.4356, val_MinusLogProbMetric: 30.4356

Epoch 219: val_loss did not improve from 30.24107
196/196 - 42s - loss: 30.1519 - MinusLogProbMetric: 30.1519 - val_loss: 30.4356 - val_MinusLogProbMetric: 30.4356 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 220/1000
2023-10-24 06:59:10.056 
Epoch 220/1000 
	 loss: 30.2356, MinusLogProbMetric: 30.2356, val_loss: 30.3794, val_MinusLogProbMetric: 30.3794

Epoch 220: val_loss did not improve from 30.24107
196/196 - 42s - loss: 30.2356 - MinusLogProbMetric: 30.2356 - val_loss: 30.3794 - val_MinusLogProbMetric: 30.3794 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 221/1000
2023-10-24 06:59:50.705 
Epoch 221/1000 
	 loss: 30.1880, MinusLogProbMetric: 30.1880, val_loss: 30.5499, val_MinusLogProbMetric: 30.5499

Epoch 221: val_loss did not improve from 30.24107
196/196 - 41s - loss: 30.1880 - MinusLogProbMetric: 30.1880 - val_loss: 30.5499 - val_MinusLogProbMetric: 30.5499 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 222/1000
2023-10-24 07:00:32.671 
Epoch 222/1000 
	 loss: 30.1264, MinusLogProbMetric: 30.1264, val_loss: 30.2246, val_MinusLogProbMetric: 30.2246

Epoch 222: val_loss improved from 30.24107 to 30.22457, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 43s - loss: 30.1264 - MinusLogProbMetric: 30.1264 - val_loss: 30.2246 - val_MinusLogProbMetric: 30.2246 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 223/1000
2023-10-24 07:01:15.504 
Epoch 223/1000 
	 loss: 30.0885, MinusLogProbMetric: 30.0885, val_loss: 30.8679, val_MinusLogProbMetric: 30.8679

Epoch 223: val_loss did not improve from 30.22457
196/196 - 42s - loss: 30.0885 - MinusLogProbMetric: 30.0885 - val_loss: 30.8679 - val_MinusLogProbMetric: 30.8679 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 224/1000
2023-10-24 07:01:57.087 
Epoch 224/1000 
	 loss: 30.0748, MinusLogProbMetric: 30.0748, val_loss: 30.8096, val_MinusLogProbMetric: 30.8096

Epoch 224: val_loss did not improve from 30.22457
196/196 - 42s - loss: 30.0748 - MinusLogProbMetric: 30.0748 - val_loss: 30.8096 - val_MinusLogProbMetric: 30.8096 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 225/1000
2023-10-24 07:02:38.725 
Epoch 225/1000 
	 loss: 30.0630, MinusLogProbMetric: 30.0630, val_loss: 30.2433, val_MinusLogProbMetric: 30.2433

Epoch 225: val_loss did not improve from 30.22457
196/196 - 42s - loss: 30.0630 - MinusLogProbMetric: 30.0630 - val_loss: 30.2433 - val_MinusLogProbMetric: 30.2433 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 226/1000
2023-10-24 07:03:19.281 
Epoch 226/1000 
	 loss: 30.1372, MinusLogProbMetric: 30.1372, val_loss: 30.4302, val_MinusLogProbMetric: 30.4302

Epoch 226: val_loss did not improve from 30.22457
196/196 - 41s - loss: 30.1372 - MinusLogProbMetric: 30.1372 - val_loss: 30.4302 - val_MinusLogProbMetric: 30.4302 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 227/1000
2023-10-24 07:04:01.124 
Epoch 227/1000 
	 loss: 30.1406, MinusLogProbMetric: 30.1406, val_loss: 30.5350, val_MinusLogProbMetric: 30.5350

Epoch 227: val_loss did not improve from 30.22457
196/196 - 42s - loss: 30.1406 - MinusLogProbMetric: 30.1406 - val_loss: 30.5350 - val_MinusLogProbMetric: 30.5350 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 228/1000
2023-10-24 07:04:42.396 
Epoch 228/1000 
	 loss: 30.2176, MinusLogProbMetric: 30.2176, val_loss: 30.3211, val_MinusLogProbMetric: 30.3211

Epoch 228: val_loss did not improve from 30.22457
196/196 - 41s - loss: 30.2176 - MinusLogProbMetric: 30.2176 - val_loss: 30.3211 - val_MinusLogProbMetric: 30.3211 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 229/1000
2023-10-24 07:05:24.286 
Epoch 229/1000 
	 loss: 30.2216, MinusLogProbMetric: 30.2216, val_loss: 30.3811, val_MinusLogProbMetric: 30.3811

Epoch 229: val_loss did not improve from 30.22457
196/196 - 42s - loss: 30.2216 - MinusLogProbMetric: 30.2216 - val_loss: 30.3811 - val_MinusLogProbMetric: 30.3811 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 230/1000
2023-10-24 07:06:01.956 
Epoch 230/1000 
	 loss: 30.1193, MinusLogProbMetric: 30.1193, val_loss: 30.7315, val_MinusLogProbMetric: 30.7315

Epoch 230: val_loss did not improve from 30.22457
196/196 - 38s - loss: 30.1193 - MinusLogProbMetric: 30.1193 - val_loss: 30.7315 - val_MinusLogProbMetric: 30.7315 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 231/1000
2023-10-24 07:06:42.006 
Epoch 231/1000 
	 loss: 30.0890, MinusLogProbMetric: 30.0890, val_loss: 30.3249, val_MinusLogProbMetric: 30.3249

Epoch 231: val_loss did not improve from 30.22457
196/196 - 40s - loss: 30.0890 - MinusLogProbMetric: 30.0890 - val_loss: 30.3249 - val_MinusLogProbMetric: 30.3249 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 232/1000
2023-10-24 07:07:24.186 
Epoch 232/1000 
	 loss: 30.0507, MinusLogProbMetric: 30.0507, val_loss: 30.4179, val_MinusLogProbMetric: 30.4179

Epoch 232: val_loss did not improve from 30.22457
196/196 - 42s - loss: 30.0507 - MinusLogProbMetric: 30.0507 - val_loss: 30.4179 - val_MinusLogProbMetric: 30.4179 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 233/1000
2023-10-24 07:08:05.526 
Epoch 233/1000 
	 loss: 30.1524, MinusLogProbMetric: 30.1524, val_loss: 30.4393, val_MinusLogProbMetric: 30.4393

Epoch 233: val_loss did not improve from 30.22457
196/196 - 41s - loss: 30.1524 - MinusLogProbMetric: 30.1524 - val_loss: 30.4393 - val_MinusLogProbMetric: 30.4393 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 234/1000
2023-10-24 07:08:47.524 
Epoch 234/1000 
	 loss: 30.0474, MinusLogProbMetric: 30.0474, val_loss: 30.5565, val_MinusLogProbMetric: 30.5565

Epoch 234: val_loss did not improve from 30.22457
196/196 - 42s - loss: 30.0474 - MinusLogProbMetric: 30.0474 - val_loss: 30.5565 - val_MinusLogProbMetric: 30.5565 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 235/1000
2023-10-24 07:09:29.242 
Epoch 235/1000 
	 loss: 30.0996, MinusLogProbMetric: 30.0996, val_loss: 30.2787, val_MinusLogProbMetric: 30.2787

Epoch 235: val_loss did not improve from 30.22457
196/196 - 42s - loss: 30.0996 - MinusLogProbMetric: 30.0996 - val_loss: 30.2787 - val_MinusLogProbMetric: 30.2787 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 236/1000
2023-10-24 07:10:10.384 
Epoch 236/1000 
	 loss: 29.9658, MinusLogProbMetric: 29.9658, val_loss: 31.3722, val_MinusLogProbMetric: 31.3722

Epoch 236: val_loss did not improve from 30.22457
196/196 - 41s - loss: 29.9658 - MinusLogProbMetric: 29.9658 - val_loss: 31.3722 - val_MinusLogProbMetric: 31.3722 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 237/1000
2023-10-24 07:10:52.506 
Epoch 237/1000 
	 loss: 30.1055, MinusLogProbMetric: 30.1055, val_loss: 30.6247, val_MinusLogProbMetric: 30.6247

Epoch 237: val_loss did not improve from 30.22457
196/196 - 42s - loss: 30.1055 - MinusLogProbMetric: 30.1055 - val_loss: 30.6247 - val_MinusLogProbMetric: 30.6247 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 238/1000
2023-10-24 07:11:33.882 
Epoch 238/1000 
	 loss: 29.9334, MinusLogProbMetric: 29.9334, val_loss: 30.2359, val_MinusLogProbMetric: 30.2359

Epoch 238: val_loss did not improve from 30.22457
196/196 - 41s - loss: 29.9334 - MinusLogProbMetric: 29.9334 - val_loss: 30.2359 - val_MinusLogProbMetric: 30.2359 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 239/1000
2023-10-24 07:12:15.056 
Epoch 239/1000 
	 loss: 30.0056, MinusLogProbMetric: 30.0056, val_loss: 30.3065, val_MinusLogProbMetric: 30.3065

Epoch 239: val_loss did not improve from 30.22457
196/196 - 41s - loss: 30.0056 - MinusLogProbMetric: 30.0056 - val_loss: 30.3065 - val_MinusLogProbMetric: 30.3065 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 240/1000
2023-10-24 07:12:57.212 
Epoch 240/1000 
	 loss: 30.0308, MinusLogProbMetric: 30.0308, val_loss: 30.0541, val_MinusLogProbMetric: 30.0541

Epoch 240: val_loss improved from 30.22457 to 30.05409, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 43s - loss: 30.0308 - MinusLogProbMetric: 30.0308 - val_loss: 30.0541 - val_MinusLogProbMetric: 30.0541 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 241/1000
2023-10-24 07:13:39.549 
Epoch 241/1000 
	 loss: 29.8603, MinusLogProbMetric: 29.8603, val_loss: 30.3307, val_MinusLogProbMetric: 30.3307

Epoch 241: val_loss did not improve from 30.05409
196/196 - 42s - loss: 29.8603 - MinusLogProbMetric: 29.8603 - val_loss: 30.3307 - val_MinusLogProbMetric: 30.3307 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 242/1000
2023-10-24 07:14:19.909 
Epoch 242/1000 
	 loss: 29.8497, MinusLogProbMetric: 29.8497, val_loss: 30.0924, val_MinusLogProbMetric: 30.0924

Epoch 242: val_loss did not improve from 30.05409
196/196 - 40s - loss: 29.8497 - MinusLogProbMetric: 29.8497 - val_loss: 30.0924 - val_MinusLogProbMetric: 30.0924 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 243/1000
2023-10-24 07:15:01.378 
Epoch 243/1000 
	 loss: 30.0162, MinusLogProbMetric: 30.0162, val_loss: 30.3795, val_MinusLogProbMetric: 30.3795

Epoch 243: val_loss did not improve from 30.05409
196/196 - 41s - loss: 30.0162 - MinusLogProbMetric: 30.0162 - val_loss: 30.3795 - val_MinusLogProbMetric: 30.3795 - lr: 3.3333e-04 - 41s/epoch - 212ms/step
Epoch 244/1000
2023-10-24 07:15:42.115 
Epoch 244/1000 
	 loss: 29.9329, MinusLogProbMetric: 29.9329, val_loss: 30.4908, val_MinusLogProbMetric: 30.4908

Epoch 244: val_loss did not improve from 30.05409
196/196 - 41s - loss: 29.9329 - MinusLogProbMetric: 29.9329 - val_loss: 30.4908 - val_MinusLogProbMetric: 30.4908 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 245/1000
2023-10-24 07:16:24.384 
Epoch 245/1000 
	 loss: 30.0702, MinusLogProbMetric: 30.0702, val_loss: 30.5880, val_MinusLogProbMetric: 30.5880

Epoch 245: val_loss did not improve from 30.05409
196/196 - 42s - loss: 30.0702 - MinusLogProbMetric: 30.0702 - val_loss: 30.5880 - val_MinusLogProbMetric: 30.5880 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 246/1000
2023-10-24 07:17:06.047 
Epoch 246/1000 
	 loss: 29.8859, MinusLogProbMetric: 29.8859, val_loss: 30.8554, val_MinusLogProbMetric: 30.8554

Epoch 246: val_loss did not improve from 30.05409
196/196 - 42s - loss: 29.8859 - MinusLogProbMetric: 29.8859 - val_loss: 30.8554 - val_MinusLogProbMetric: 30.8554 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 247/1000
2023-10-24 07:17:47.172 
Epoch 247/1000 
	 loss: 29.9014, MinusLogProbMetric: 29.9014, val_loss: 30.2058, val_MinusLogProbMetric: 30.2058

Epoch 247: val_loss did not improve from 30.05409
196/196 - 41s - loss: 29.9014 - MinusLogProbMetric: 29.9014 - val_loss: 30.2058 - val_MinusLogProbMetric: 30.2058 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 248/1000
2023-10-24 07:18:27.153 
Epoch 248/1000 
	 loss: 29.9420, MinusLogProbMetric: 29.9420, val_loss: 30.1572, val_MinusLogProbMetric: 30.1572

Epoch 248: val_loss did not improve from 30.05409
196/196 - 40s - loss: 29.9420 - MinusLogProbMetric: 29.9420 - val_loss: 30.1572 - val_MinusLogProbMetric: 30.1572 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 249/1000
2023-10-24 07:19:07.894 
Epoch 249/1000 
	 loss: 29.9127, MinusLogProbMetric: 29.9127, val_loss: 31.0852, val_MinusLogProbMetric: 31.0852

Epoch 249: val_loss did not improve from 30.05409
196/196 - 41s - loss: 29.9127 - MinusLogProbMetric: 29.9127 - val_loss: 31.0852 - val_MinusLogProbMetric: 31.0852 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 250/1000
2023-10-24 07:19:45.794 
Epoch 250/1000 
	 loss: 29.9050, MinusLogProbMetric: 29.9050, val_loss: 30.3598, val_MinusLogProbMetric: 30.3598

Epoch 250: val_loss did not improve from 30.05409
196/196 - 38s - loss: 29.9050 - MinusLogProbMetric: 29.9050 - val_loss: 30.3598 - val_MinusLogProbMetric: 30.3598 - lr: 3.3333e-04 - 38s/epoch - 193ms/step
Epoch 251/1000
2023-10-24 07:20:20.747 
Epoch 251/1000 
	 loss: 29.7696, MinusLogProbMetric: 29.7696, val_loss: 30.5427, val_MinusLogProbMetric: 30.5427

Epoch 251: val_loss did not improve from 30.05409
196/196 - 35s - loss: 29.7696 - MinusLogProbMetric: 29.7696 - val_loss: 30.5427 - val_MinusLogProbMetric: 30.5427 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 252/1000
2023-10-24 07:21:01.024 
Epoch 252/1000 
	 loss: 29.8800, MinusLogProbMetric: 29.8800, val_loss: 30.0139, val_MinusLogProbMetric: 30.0139

Epoch 252: val_loss improved from 30.05409 to 30.01385, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 41s - loss: 29.8800 - MinusLogProbMetric: 29.8800 - val_loss: 30.0139 - val_MinusLogProbMetric: 30.0139 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 253/1000
2023-10-24 07:21:41.449 
Epoch 253/1000 
	 loss: 29.7948, MinusLogProbMetric: 29.7948, val_loss: 30.4579, val_MinusLogProbMetric: 30.4579

Epoch 253: val_loss did not improve from 30.01385
196/196 - 40s - loss: 29.7948 - MinusLogProbMetric: 29.7948 - val_loss: 30.4579 - val_MinusLogProbMetric: 30.4579 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 254/1000
2023-10-24 07:22:18.659 
Epoch 254/1000 
	 loss: 29.7729, MinusLogProbMetric: 29.7729, val_loss: 31.3007, val_MinusLogProbMetric: 31.3007

Epoch 254: val_loss did not improve from 30.01385
196/196 - 37s - loss: 29.7729 - MinusLogProbMetric: 29.7729 - val_loss: 31.3007 - val_MinusLogProbMetric: 31.3007 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 255/1000
2023-10-24 07:23:00.614 
Epoch 255/1000 
	 loss: 29.8580, MinusLogProbMetric: 29.8580, val_loss: 31.2094, val_MinusLogProbMetric: 31.2094

Epoch 255: val_loss did not improve from 30.01385
196/196 - 42s - loss: 29.8580 - MinusLogProbMetric: 29.8580 - val_loss: 31.2094 - val_MinusLogProbMetric: 31.2094 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 256/1000
2023-10-24 07:23:42.704 
Epoch 256/1000 
	 loss: 29.8728, MinusLogProbMetric: 29.8728, val_loss: 30.0609, val_MinusLogProbMetric: 30.0609

Epoch 256: val_loss did not improve from 30.01385
196/196 - 42s - loss: 29.8728 - MinusLogProbMetric: 29.8728 - val_loss: 30.0609 - val_MinusLogProbMetric: 30.0609 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 257/1000
2023-10-24 07:24:25.045 
Epoch 257/1000 
	 loss: 29.8333, MinusLogProbMetric: 29.8333, val_loss: 30.0271, val_MinusLogProbMetric: 30.0271

Epoch 257: val_loss did not improve from 30.01385
196/196 - 42s - loss: 29.8333 - MinusLogProbMetric: 29.8333 - val_loss: 30.0271 - val_MinusLogProbMetric: 30.0271 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 258/1000
2023-10-24 07:25:07.459 
Epoch 258/1000 
	 loss: 29.7938, MinusLogProbMetric: 29.7938, val_loss: 30.2212, val_MinusLogProbMetric: 30.2212

Epoch 258: val_loss did not improve from 30.01385
196/196 - 42s - loss: 29.7938 - MinusLogProbMetric: 29.7938 - val_loss: 30.2212 - val_MinusLogProbMetric: 30.2212 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 259/1000
2023-10-24 07:25:49.780 
Epoch 259/1000 
	 loss: 29.8046, MinusLogProbMetric: 29.8046, val_loss: 29.8458, val_MinusLogProbMetric: 29.8458

Epoch 259: val_loss improved from 30.01385 to 29.84579, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 43s - loss: 29.8046 - MinusLogProbMetric: 29.8046 - val_loss: 29.8458 - val_MinusLogProbMetric: 29.8458 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 260/1000
2023-10-24 07:26:32.300 
Epoch 260/1000 
	 loss: 29.7621, MinusLogProbMetric: 29.7621, val_loss: 29.9078, val_MinusLogProbMetric: 29.9078

Epoch 260: val_loss did not improve from 29.84579
196/196 - 42s - loss: 29.7621 - MinusLogProbMetric: 29.7621 - val_loss: 29.9078 - val_MinusLogProbMetric: 29.9078 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 261/1000
2023-10-24 07:27:14.073 
Epoch 261/1000 
	 loss: 29.6918, MinusLogProbMetric: 29.6918, val_loss: 30.0404, val_MinusLogProbMetric: 30.0404

Epoch 261: val_loss did not improve from 29.84579
196/196 - 42s - loss: 29.6918 - MinusLogProbMetric: 29.6918 - val_loss: 30.0404 - val_MinusLogProbMetric: 30.0404 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 262/1000
2023-10-24 07:27:56.509 
Epoch 262/1000 
	 loss: 29.7378, MinusLogProbMetric: 29.7378, val_loss: 30.0748, val_MinusLogProbMetric: 30.0748

Epoch 262: val_loss did not improve from 29.84579
196/196 - 42s - loss: 29.7378 - MinusLogProbMetric: 29.7378 - val_loss: 30.0748 - val_MinusLogProbMetric: 30.0748 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 263/1000
2023-10-24 07:28:38.638 
Epoch 263/1000 
	 loss: 29.7020, MinusLogProbMetric: 29.7020, val_loss: 30.6650, val_MinusLogProbMetric: 30.6650

Epoch 263: val_loss did not improve from 29.84579
196/196 - 42s - loss: 29.7020 - MinusLogProbMetric: 29.7020 - val_loss: 30.6650 - val_MinusLogProbMetric: 30.6650 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 264/1000
2023-10-24 07:29:20.966 
Epoch 264/1000 
	 loss: 29.6877, MinusLogProbMetric: 29.6877, val_loss: 29.9336, val_MinusLogProbMetric: 29.9336

Epoch 264: val_loss did not improve from 29.84579
196/196 - 42s - loss: 29.6877 - MinusLogProbMetric: 29.6877 - val_loss: 29.9336 - val_MinusLogProbMetric: 29.9336 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 265/1000
2023-10-24 07:30:02.916 
Epoch 265/1000 
	 loss: 29.7030, MinusLogProbMetric: 29.7030, val_loss: 30.2542, val_MinusLogProbMetric: 30.2542

Epoch 265: val_loss did not improve from 29.84579
196/196 - 42s - loss: 29.7030 - MinusLogProbMetric: 29.7030 - val_loss: 30.2542 - val_MinusLogProbMetric: 30.2542 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 266/1000
2023-10-24 07:30:45.000 
Epoch 266/1000 
	 loss: 29.7000, MinusLogProbMetric: 29.7000, val_loss: 31.2387, val_MinusLogProbMetric: 31.2387

Epoch 266: val_loss did not improve from 29.84579
196/196 - 42s - loss: 29.7000 - MinusLogProbMetric: 29.7000 - val_loss: 31.2387 - val_MinusLogProbMetric: 31.2387 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 267/1000
2023-10-24 07:31:27.063 
Epoch 267/1000 
	 loss: 29.8016, MinusLogProbMetric: 29.8016, val_loss: 29.8630, val_MinusLogProbMetric: 29.8630

Epoch 267: val_loss did not improve from 29.84579
196/196 - 42s - loss: 29.8016 - MinusLogProbMetric: 29.8016 - val_loss: 29.8630 - val_MinusLogProbMetric: 29.8630 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 268/1000
2023-10-24 07:32:09.068 
Epoch 268/1000 
	 loss: 29.5988, MinusLogProbMetric: 29.5988, val_loss: 29.7036, val_MinusLogProbMetric: 29.7036

Epoch 268: val_loss improved from 29.84579 to 29.70357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 43s - loss: 29.5988 - MinusLogProbMetric: 29.5988 - val_loss: 29.7036 - val_MinusLogProbMetric: 29.7036 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 269/1000
2023-10-24 07:32:51.424 
Epoch 269/1000 
	 loss: 29.7402, MinusLogProbMetric: 29.7402, val_loss: 29.9283, val_MinusLogProbMetric: 29.9283

Epoch 269: val_loss did not improve from 29.70357
196/196 - 42s - loss: 29.7402 - MinusLogProbMetric: 29.7402 - val_loss: 29.9283 - val_MinusLogProbMetric: 29.9283 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 270/1000
2023-10-24 07:33:33.515 
Epoch 270/1000 
	 loss: 29.6877, MinusLogProbMetric: 29.6877, val_loss: 29.7807, val_MinusLogProbMetric: 29.7807

Epoch 270: val_loss did not improve from 29.70357
196/196 - 42s - loss: 29.6877 - MinusLogProbMetric: 29.6877 - val_loss: 29.7807 - val_MinusLogProbMetric: 29.7807 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 271/1000
2023-10-24 07:34:15.849 
Epoch 271/1000 
	 loss: 29.5942, MinusLogProbMetric: 29.5942, val_loss: 29.8242, val_MinusLogProbMetric: 29.8242

Epoch 271: val_loss did not improve from 29.70357
196/196 - 42s - loss: 29.5942 - MinusLogProbMetric: 29.5942 - val_loss: 29.8242 - val_MinusLogProbMetric: 29.8242 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 272/1000
2023-10-24 07:34:55.263 
Epoch 272/1000 
	 loss: 29.6625, MinusLogProbMetric: 29.6625, val_loss: 29.8318, val_MinusLogProbMetric: 29.8318

Epoch 272: val_loss did not improve from 29.70357
196/196 - 39s - loss: 29.6625 - MinusLogProbMetric: 29.6625 - val_loss: 29.8318 - val_MinusLogProbMetric: 29.8318 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 273/1000
2023-10-24 07:35:35.779 
Epoch 273/1000 
	 loss: 29.6350, MinusLogProbMetric: 29.6350, val_loss: 29.9396, val_MinusLogProbMetric: 29.9396

Epoch 273: val_loss did not improve from 29.70357
196/196 - 41s - loss: 29.6350 - MinusLogProbMetric: 29.6350 - val_loss: 29.9396 - val_MinusLogProbMetric: 29.9396 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 274/1000
2023-10-24 07:36:17.740 
Epoch 274/1000 
	 loss: 29.6282, MinusLogProbMetric: 29.6282, val_loss: 29.8414, val_MinusLogProbMetric: 29.8414

Epoch 274: val_loss did not improve from 29.70357
196/196 - 42s - loss: 29.6282 - MinusLogProbMetric: 29.6282 - val_loss: 29.8414 - val_MinusLogProbMetric: 29.8414 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 275/1000
2023-10-24 07:36:59.265 
Epoch 275/1000 
	 loss: 29.6345, MinusLogProbMetric: 29.6345, val_loss: 30.1386, val_MinusLogProbMetric: 30.1386

Epoch 275: val_loss did not improve from 29.70357
196/196 - 42s - loss: 29.6345 - MinusLogProbMetric: 29.6345 - val_loss: 30.1386 - val_MinusLogProbMetric: 30.1386 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 276/1000
2023-10-24 07:37:41.032 
Epoch 276/1000 
	 loss: 29.5786, MinusLogProbMetric: 29.5786, val_loss: 29.6171, val_MinusLogProbMetric: 29.6171

Epoch 276: val_loss improved from 29.70357 to 29.61712, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 29.5786 - MinusLogProbMetric: 29.5786 - val_loss: 29.6171 - val_MinusLogProbMetric: 29.6171 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 277/1000
2023-10-24 07:38:23.474 
Epoch 277/1000 
	 loss: 29.5667, MinusLogProbMetric: 29.5667, val_loss: 29.6498, val_MinusLogProbMetric: 29.6498

Epoch 277: val_loss did not improve from 29.61712
196/196 - 42s - loss: 29.5667 - MinusLogProbMetric: 29.5667 - val_loss: 29.6498 - val_MinusLogProbMetric: 29.6498 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 278/1000
2023-10-24 07:39:05.418 
Epoch 278/1000 
	 loss: 29.6703, MinusLogProbMetric: 29.6703, val_loss: 30.4097, val_MinusLogProbMetric: 30.4097

Epoch 278: val_loss did not improve from 29.61712
196/196 - 42s - loss: 29.6703 - MinusLogProbMetric: 29.6703 - val_loss: 30.4097 - val_MinusLogProbMetric: 30.4097 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 279/1000
2023-10-24 07:39:47.537 
Epoch 279/1000 
	 loss: 29.6389, MinusLogProbMetric: 29.6389, val_loss: 30.0380, val_MinusLogProbMetric: 30.0380

Epoch 279: val_loss did not improve from 29.61712
196/196 - 42s - loss: 29.6389 - MinusLogProbMetric: 29.6389 - val_loss: 30.0380 - val_MinusLogProbMetric: 30.0380 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 280/1000
2023-10-24 07:40:29.177 
Epoch 280/1000 
	 loss: 29.5883, MinusLogProbMetric: 29.5883, val_loss: 29.7736, val_MinusLogProbMetric: 29.7736

Epoch 280: val_loss did not improve from 29.61712
196/196 - 42s - loss: 29.5883 - MinusLogProbMetric: 29.5883 - val_loss: 29.7736 - val_MinusLogProbMetric: 29.7736 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 281/1000
2023-10-24 07:41:08.595 
Epoch 281/1000 
	 loss: 29.6066, MinusLogProbMetric: 29.6066, val_loss: 29.8012, val_MinusLogProbMetric: 29.8012

Epoch 281: val_loss did not improve from 29.61712
196/196 - 39s - loss: 29.6066 - MinusLogProbMetric: 29.6066 - val_loss: 29.8012 - val_MinusLogProbMetric: 29.8012 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 282/1000
2023-10-24 07:41:49.226 
Epoch 282/1000 
	 loss: 29.5240, MinusLogProbMetric: 29.5240, val_loss: 29.7657, val_MinusLogProbMetric: 29.7657

Epoch 282: val_loss did not improve from 29.61712
196/196 - 41s - loss: 29.5240 - MinusLogProbMetric: 29.5240 - val_loss: 29.7657 - val_MinusLogProbMetric: 29.7657 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 283/1000
2023-10-24 07:42:30.319 
Epoch 283/1000 
	 loss: 29.6438, MinusLogProbMetric: 29.6438, val_loss: 30.1187, val_MinusLogProbMetric: 30.1187

Epoch 283: val_loss did not improve from 29.61712
196/196 - 41s - loss: 29.6438 - MinusLogProbMetric: 29.6438 - val_loss: 30.1187 - val_MinusLogProbMetric: 30.1187 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 284/1000
2023-10-24 07:43:12.070 
Epoch 284/1000 
	 loss: 29.5149, MinusLogProbMetric: 29.5149, val_loss: 30.2891, val_MinusLogProbMetric: 30.2891

Epoch 284: val_loss did not improve from 29.61712
196/196 - 42s - loss: 29.5149 - MinusLogProbMetric: 29.5149 - val_loss: 30.2891 - val_MinusLogProbMetric: 30.2891 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 285/1000
2023-10-24 07:43:54.330 
Epoch 285/1000 
	 loss: 29.6658, MinusLogProbMetric: 29.6658, val_loss: 31.7830, val_MinusLogProbMetric: 31.7830

Epoch 285: val_loss did not improve from 29.61712
196/196 - 42s - loss: 29.6658 - MinusLogProbMetric: 29.6658 - val_loss: 31.7830 - val_MinusLogProbMetric: 31.7830 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 286/1000
2023-10-24 07:44:36.632 
Epoch 286/1000 
	 loss: 29.5207, MinusLogProbMetric: 29.5207, val_loss: 29.9112, val_MinusLogProbMetric: 29.9112

Epoch 286: val_loss did not improve from 29.61712
196/196 - 42s - loss: 29.5207 - MinusLogProbMetric: 29.5207 - val_loss: 29.9112 - val_MinusLogProbMetric: 29.9112 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 287/1000
2023-10-24 07:45:18.375 
Epoch 287/1000 
	 loss: 29.6763, MinusLogProbMetric: 29.6763, val_loss: 29.8888, val_MinusLogProbMetric: 29.8888

Epoch 287: val_loss did not improve from 29.61712
196/196 - 42s - loss: 29.6763 - MinusLogProbMetric: 29.6763 - val_loss: 29.8888 - val_MinusLogProbMetric: 29.8888 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 288/1000
2023-10-24 07:46:00.048 
Epoch 288/1000 
	 loss: 29.5367, MinusLogProbMetric: 29.5367, val_loss: 29.9278, val_MinusLogProbMetric: 29.9278

Epoch 288: val_loss did not improve from 29.61712
196/196 - 42s - loss: 29.5367 - MinusLogProbMetric: 29.5367 - val_loss: 29.9278 - val_MinusLogProbMetric: 29.9278 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 289/1000
2023-10-24 07:46:42.165 
Epoch 289/1000 
	 loss: 29.5155, MinusLogProbMetric: 29.5155, val_loss: 29.5798, val_MinusLogProbMetric: 29.5798

Epoch 289: val_loss improved from 29.61712 to 29.57980, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 43s - loss: 29.5155 - MinusLogProbMetric: 29.5155 - val_loss: 29.5798 - val_MinusLogProbMetric: 29.5798 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 290/1000
2023-10-24 07:47:23.680 
Epoch 290/1000 
	 loss: 29.5646, MinusLogProbMetric: 29.5646, val_loss: 29.9645, val_MinusLogProbMetric: 29.9645

Epoch 290: val_loss did not improve from 29.57980
196/196 - 41s - loss: 29.5646 - MinusLogProbMetric: 29.5646 - val_loss: 29.9645 - val_MinusLogProbMetric: 29.9645 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 291/1000
2023-10-24 07:48:05.621 
Epoch 291/1000 
	 loss: 29.5569, MinusLogProbMetric: 29.5569, val_loss: 29.8522, val_MinusLogProbMetric: 29.8522

Epoch 291: val_loss did not improve from 29.57980
196/196 - 42s - loss: 29.5569 - MinusLogProbMetric: 29.5569 - val_loss: 29.8522 - val_MinusLogProbMetric: 29.8522 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 292/1000
2023-10-24 07:48:46.690 
Epoch 292/1000 
	 loss: 29.4622, MinusLogProbMetric: 29.4622, val_loss: 29.8150, val_MinusLogProbMetric: 29.8150

Epoch 292: val_loss did not improve from 29.57980
196/196 - 41s - loss: 29.4622 - MinusLogProbMetric: 29.4622 - val_loss: 29.8150 - val_MinusLogProbMetric: 29.8150 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 293/1000
2023-10-24 07:49:28.626 
Epoch 293/1000 
	 loss: 29.4564, MinusLogProbMetric: 29.4564, val_loss: 29.9363, val_MinusLogProbMetric: 29.9363

Epoch 293: val_loss did not improve from 29.57980
196/196 - 42s - loss: 29.4564 - MinusLogProbMetric: 29.4564 - val_loss: 29.9363 - val_MinusLogProbMetric: 29.9363 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 294/1000
2023-10-24 07:50:10.372 
Epoch 294/1000 
	 loss: 29.4933, MinusLogProbMetric: 29.4933, val_loss: 30.0078, val_MinusLogProbMetric: 30.0078

Epoch 294: val_loss did not improve from 29.57980
196/196 - 42s - loss: 29.4933 - MinusLogProbMetric: 29.4933 - val_loss: 30.0078 - val_MinusLogProbMetric: 30.0078 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 295/1000
2023-10-24 07:50:52.543 
Epoch 295/1000 
	 loss: 29.4665, MinusLogProbMetric: 29.4665, val_loss: 29.6551, val_MinusLogProbMetric: 29.6551

Epoch 295: val_loss did not improve from 29.57980
196/196 - 42s - loss: 29.4665 - MinusLogProbMetric: 29.4665 - val_loss: 29.6551 - val_MinusLogProbMetric: 29.6551 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 296/1000
2023-10-24 07:51:34.625 
Epoch 296/1000 
	 loss: 29.5069, MinusLogProbMetric: 29.5069, val_loss: 29.5313, val_MinusLogProbMetric: 29.5313

Epoch 296: val_loss improved from 29.57980 to 29.53132, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 43s - loss: 29.5069 - MinusLogProbMetric: 29.5069 - val_loss: 29.5313 - val_MinusLogProbMetric: 29.5313 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 297/1000
2023-10-24 07:52:17.352 
Epoch 297/1000 
	 loss: 29.5018, MinusLogProbMetric: 29.5018, val_loss: 30.2291, val_MinusLogProbMetric: 30.2291

Epoch 297: val_loss did not improve from 29.53132
196/196 - 42s - loss: 29.5018 - MinusLogProbMetric: 29.5018 - val_loss: 30.2291 - val_MinusLogProbMetric: 30.2291 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 298/1000
2023-10-24 07:52:59.689 
Epoch 298/1000 
	 loss: 29.4520, MinusLogProbMetric: 29.4520, val_loss: 29.8308, val_MinusLogProbMetric: 29.8308

Epoch 298: val_loss did not improve from 29.53132
196/196 - 42s - loss: 29.4520 - MinusLogProbMetric: 29.4520 - val_loss: 29.8308 - val_MinusLogProbMetric: 29.8308 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 299/1000
2023-10-24 07:53:41.398 
Epoch 299/1000 
	 loss: 29.4746, MinusLogProbMetric: 29.4746, val_loss: 30.1215, val_MinusLogProbMetric: 30.1215

Epoch 299: val_loss did not improve from 29.53132
196/196 - 42s - loss: 29.4746 - MinusLogProbMetric: 29.4746 - val_loss: 30.1215 - val_MinusLogProbMetric: 30.1215 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 300/1000
2023-10-24 07:54:23.285 
Epoch 300/1000 
	 loss: 29.4195, MinusLogProbMetric: 29.4195, val_loss: 30.0433, val_MinusLogProbMetric: 30.0433

Epoch 300: val_loss did not improve from 29.53132
196/196 - 42s - loss: 29.4195 - MinusLogProbMetric: 29.4195 - val_loss: 30.0433 - val_MinusLogProbMetric: 30.0433 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 301/1000
2023-10-24 07:55:03.801 
Epoch 301/1000 
	 loss: 29.4221, MinusLogProbMetric: 29.4221, val_loss: 30.4206, val_MinusLogProbMetric: 30.4206

Epoch 301: val_loss did not improve from 29.53132
196/196 - 41s - loss: 29.4221 - MinusLogProbMetric: 29.4221 - val_loss: 30.4206 - val_MinusLogProbMetric: 30.4206 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 302/1000
2023-10-24 07:55:41.750 
Epoch 302/1000 
	 loss: 29.3718, MinusLogProbMetric: 29.3718, val_loss: 30.0567, val_MinusLogProbMetric: 30.0567

Epoch 302: val_loss did not improve from 29.53132
196/196 - 38s - loss: 29.3718 - MinusLogProbMetric: 29.3718 - val_loss: 30.0567 - val_MinusLogProbMetric: 30.0567 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 303/1000
2023-10-24 07:56:21.452 
Epoch 303/1000 
	 loss: 29.4695, MinusLogProbMetric: 29.4695, val_loss: 29.5092, val_MinusLogProbMetric: 29.5092

Epoch 303: val_loss improved from 29.53132 to 29.50922, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 40s - loss: 29.4695 - MinusLogProbMetric: 29.4695 - val_loss: 29.5092 - val_MinusLogProbMetric: 29.5092 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 304/1000
2023-10-24 07:57:04.272 
Epoch 304/1000 
	 loss: 29.3286, MinusLogProbMetric: 29.3286, val_loss: 29.9421, val_MinusLogProbMetric: 29.9421

Epoch 304: val_loss did not improve from 29.50922
196/196 - 42s - loss: 29.3286 - MinusLogProbMetric: 29.3286 - val_loss: 29.9421 - val_MinusLogProbMetric: 29.9421 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 305/1000
2023-10-24 07:57:46.781 
Epoch 305/1000 
	 loss: 29.5173, MinusLogProbMetric: 29.5173, val_loss: 29.7256, val_MinusLogProbMetric: 29.7256

Epoch 305: val_loss did not improve from 29.50922
196/196 - 43s - loss: 29.5173 - MinusLogProbMetric: 29.5173 - val_loss: 29.7256 - val_MinusLogProbMetric: 29.7256 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 306/1000
2023-10-24 07:58:29.069 
Epoch 306/1000 
	 loss: 29.4168, MinusLogProbMetric: 29.4168, val_loss: 29.6213, val_MinusLogProbMetric: 29.6213

Epoch 306: val_loss did not improve from 29.50922
196/196 - 42s - loss: 29.4168 - MinusLogProbMetric: 29.4168 - val_loss: 29.6213 - val_MinusLogProbMetric: 29.6213 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 307/1000
2023-10-24 07:59:10.675 
Epoch 307/1000 
	 loss: 29.4553, MinusLogProbMetric: 29.4553, val_loss: 29.7444, val_MinusLogProbMetric: 29.7444

Epoch 307: val_loss did not improve from 29.50922
196/196 - 42s - loss: 29.4553 - MinusLogProbMetric: 29.4553 - val_loss: 29.7444 - val_MinusLogProbMetric: 29.7444 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 308/1000
2023-10-24 07:59:52.776 
Epoch 308/1000 
	 loss: 29.3750, MinusLogProbMetric: 29.3750, val_loss: 29.5499, val_MinusLogProbMetric: 29.5499

Epoch 308: val_loss did not improve from 29.50922
196/196 - 42s - loss: 29.3750 - MinusLogProbMetric: 29.3750 - val_loss: 29.5499 - val_MinusLogProbMetric: 29.5499 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 309/1000
2023-10-24 08:00:34.571 
Epoch 309/1000 
	 loss: 29.3942, MinusLogProbMetric: 29.3942, val_loss: 29.6911, val_MinusLogProbMetric: 29.6911

Epoch 309: val_loss did not improve from 29.50922
196/196 - 42s - loss: 29.3942 - MinusLogProbMetric: 29.3942 - val_loss: 29.6911 - val_MinusLogProbMetric: 29.6911 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 310/1000
2023-10-24 08:01:16.835 
Epoch 310/1000 
	 loss: 29.4047, MinusLogProbMetric: 29.4047, val_loss: 29.4465, val_MinusLogProbMetric: 29.4465

Epoch 310: val_loss improved from 29.50922 to 29.44652, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 43s - loss: 29.4047 - MinusLogProbMetric: 29.4047 - val_loss: 29.4465 - val_MinusLogProbMetric: 29.4465 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 311/1000
2023-10-24 08:01:59.595 
Epoch 311/1000 
	 loss: 29.4551, MinusLogProbMetric: 29.4551, val_loss: 29.5888, val_MinusLogProbMetric: 29.5888

Epoch 311: val_loss did not improve from 29.44652
196/196 - 42s - loss: 29.4551 - MinusLogProbMetric: 29.4551 - val_loss: 29.5888 - val_MinusLogProbMetric: 29.5888 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 312/1000
2023-10-24 08:02:41.607 
Epoch 312/1000 
	 loss: 29.3930, MinusLogProbMetric: 29.3930, val_loss: 30.0440, val_MinusLogProbMetric: 30.0440

Epoch 312: val_loss did not improve from 29.44652
196/196 - 42s - loss: 29.3930 - MinusLogProbMetric: 29.3930 - val_loss: 30.0440 - val_MinusLogProbMetric: 30.0440 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 313/1000
2023-10-24 08:03:23.859 
Epoch 313/1000 
	 loss: 29.3469, MinusLogProbMetric: 29.3469, val_loss: 30.0634, val_MinusLogProbMetric: 30.0634

Epoch 313: val_loss did not improve from 29.44652
196/196 - 42s - loss: 29.3469 - MinusLogProbMetric: 29.3469 - val_loss: 30.0634 - val_MinusLogProbMetric: 30.0634 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 314/1000
2023-10-24 08:04:06.404 
Epoch 314/1000 
	 loss: 29.3489, MinusLogProbMetric: 29.3489, val_loss: 29.9044, val_MinusLogProbMetric: 29.9044

Epoch 314: val_loss did not improve from 29.44652
196/196 - 43s - loss: 29.3489 - MinusLogProbMetric: 29.3489 - val_loss: 29.9044 - val_MinusLogProbMetric: 29.9044 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 315/1000
2023-10-24 08:04:48.196 
Epoch 315/1000 
	 loss: 29.2930, MinusLogProbMetric: 29.2930, val_loss: 30.0231, val_MinusLogProbMetric: 30.0231

Epoch 315: val_loss did not improve from 29.44652
196/196 - 42s - loss: 29.2930 - MinusLogProbMetric: 29.2930 - val_loss: 30.0231 - val_MinusLogProbMetric: 30.0231 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 316/1000
2023-10-24 08:05:22.594 
Epoch 316/1000 
	 loss: 29.2980, MinusLogProbMetric: 29.2980, val_loss: 29.8442, val_MinusLogProbMetric: 29.8442

Epoch 316: val_loss did not improve from 29.44652
196/196 - 34s - loss: 29.2980 - MinusLogProbMetric: 29.2980 - val_loss: 29.8442 - val_MinusLogProbMetric: 29.8442 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 317/1000
2023-10-24 08:05:57.396 
Epoch 317/1000 
	 loss: 29.3326, MinusLogProbMetric: 29.3326, val_loss: 29.5714, val_MinusLogProbMetric: 29.5714

Epoch 317: val_loss did not improve from 29.44652
196/196 - 35s - loss: 29.3326 - MinusLogProbMetric: 29.3326 - val_loss: 29.5714 - val_MinusLogProbMetric: 29.5714 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 318/1000
2023-10-24 08:06:33.452 
Epoch 318/1000 
	 loss: 29.2711, MinusLogProbMetric: 29.2711, val_loss: 29.5738, val_MinusLogProbMetric: 29.5738

Epoch 318: val_loss did not improve from 29.44652
196/196 - 36s - loss: 29.2711 - MinusLogProbMetric: 29.2711 - val_loss: 29.5738 - val_MinusLogProbMetric: 29.5738 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 319/1000
2023-10-24 08:07:10.499 
Epoch 319/1000 
	 loss: 29.3595, MinusLogProbMetric: 29.3595, val_loss: 29.8062, val_MinusLogProbMetric: 29.8062

Epoch 319: val_loss did not improve from 29.44652
196/196 - 37s - loss: 29.3595 - MinusLogProbMetric: 29.3595 - val_loss: 29.8062 - val_MinusLogProbMetric: 29.8062 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 320/1000
2023-10-24 08:07:45.442 
Epoch 320/1000 
	 loss: 29.2898, MinusLogProbMetric: 29.2898, val_loss: 29.6313, val_MinusLogProbMetric: 29.6313

Epoch 320: val_loss did not improve from 29.44652
196/196 - 35s - loss: 29.2898 - MinusLogProbMetric: 29.2898 - val_loss: 29.6313 - val_MinusLogProbMetric: 29.6313 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 321/1000
2023-10-24 08:08:20.747 
Epoch 321/1000 
	 loss: 29.4141, MinusLogProbMetric: 29.4141, val_loss: 30.3024, val_MinusLogProbMetric: 30.3024

Epoch 321: val_loss did not improve from 29.44652
196/196 - 35s - loss: 29.4141 - MinusLogProbMetric: 29.4141 - val_loss: 30.3024 - val_MinusLogProbMetric: 30.3024 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 322/1000
2023-10-24 08:08:56.268 
Epoch 322/1000 
	 loss: 29.3327, MinusLogProbMetric: 29.3327, val_loss: 29.8152, val_MinusLogProbMetric: 29.8152

Epoch 322: val_loss did not improve from 29.44652
196/196 - 36s - loss: 29.3327 - MinusLogProbMetric: 29.3327 - val_loss: 29.8152 - val_MinusLogProbMetric: 29.8152 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 323/1000
2023-10-24 08:09:33.931 
Epoch 323/1000 
	 loss: 29.2399, MinusLogProbMetric: 29.2399, val_loss: 29.7612, val_MinusLogProbMetric: 29.7612

Epoch 323: val_loss did not improve from 29.44652
196/196 - 38s - loss: 29.2399 - MinusLogProbMetric: 29.2399 - val_loss: 29.7612 - val_MinusLogProbMetric: 29.7612 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 324/1000
2023-10-24 08:10:09.494 
Epoch 324/1000 
	 loss: 29.3819, MinusLogProbMetric: 29.3819, val_loss: 29.9104, val_MinusLogProbMetric: 29.9104

Epoch 324: val_loss did not improve from 29.44652
196/196 - 36s - loss: 29.3819 - MinusLogProbMetric: 29.3819 - val_loss: 29.9104 - val_MinusLogProbMetric: 29.9104 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 325/1000
2023-10-24 08:10:45.707 
Epoch 325/1000 
	 loss: 29.2577, MinusLogProbMetric: 29.2577, val_loss: 31.8575, val_MinusLogProbMetric: 31.8575

Epoch 325: val_loss did not improve from 29.44652
196/196 - 36s - loss: 29.2577 - MinusLogProbMetric: 29.2577 - val_loss: 31.8575 - val_MinusLogProbMetric: 31.8575 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 326/1000
2023-10-24 08:11:21.950 
Epoch 326/1000 
	 loss: 29.2953, MinusLogProbMetric: 29.2953, val_loss: 30.1390, val_MinusLogProbMetric: 30.1390

Epoch 326: val_loss did not improve from 29.44652
196/196 - 36s - loss: 29.2953 - MinusLogProbMetric: 29.2953 - val_loss: 30.1390 - val_MinusLogProbMetric: 30.1390 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 327/1000
2023-10-24 08:11:57.551 
Epoch 327/1000 
	 loss: 29.3013, MinusLogProbMetric: 29.3013, val_loss: 29.8281, val_MinusLogProbMetric: 29.8281

Epoch 327: val_loss did not improve from 29.44652
196/196 - 36s - loss: 29.3013 - MinusLogProbMetric: 29.3013 - val_loss: 29.8281 - val_MinusLogProbMetric: 29.8281 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 328/1000
2023-10-24 08:12:33.108 
Epoch 328/1000 
	 loss: 29.2116, MinusLogProbMetric: 29.2116, val_loss: 29.5279, val_MinusLogProbMetric: 29.5279

Epoch 328: val_loss did not improve from 29.44652
196/196 - 36s - loss: 29.2116 - MinusLogProbMetric: 29.2116 - val_loss: 29.5279 - val_MinusLogProbMetric: 29.5279 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 329/1000
2023-10-24 08:13:08.621 
Epoch 329/1000 
	 loss: 29.2628, MinusLogProbMetric: 29.2628, val_loss: 29.8487, val_MinusLogProbMetric: 29.8487

Epoch 329: val_loss did not improve from 29.44652
196/196 - 36s - loss: 29.2628 - MinusLogProbMetric: 29.2628 - val_loss: 29.8487 - val_MinusLogProbMetric: 29.8487 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 330/1000
2023-10-24 08:13:47.046 
Epoch 330/1000 
	 loss: 29.2881, MinusLogProbMetric: 29.2881, val_loss: 29.5380, val_MinusLogProbMetric: 29.5380

Epoch 330: val_loss did not improve from 29.44652
196/196 - 38s - loss: 29.2881 - MinusLogProbMetric: 29.2881 - val_loss: 29.5380 - val_MinusLogProbMetric: 29.5380 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 331/1000
2023-10-24 08:14:25.192 
Epoch 331/1000 
	 loss: 29.3120, MinusLogProbMetric: 29.3120, val_loss: 29.5862, val_MinusLogProbMetric: 29.5862

Epoch 331: val_loss did not improve from 29.44652
196/196 - 38s - loss: 29.3120 - MinusLogProbMetric: 29.3120 - val_loss: 29.5862 - val_MinusLogProbMetric: 29.5862 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 332/1000
2023-10-24 08:15:02.504 
Epoch 332/1000 
	 loss: 29.3105, MinusLogProbMetric: 29.3105, val_loss: 29.9568, val_MinusLogProbMetric: 29.9568

Epoch 332: val_loss did not improve from 29.44652
196/196 - 37s - loss: 29.3105 - MinusLogProbMetric: 29.3105 - val_loss: 29.9568 - val_MinusLogProbMetric: 29.9568 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 333/1000
2023-10-24 08:15:38.214 
Epoch 333/1000 
	 loss: 29.3163, MinusLogProbMetric: 29.3163, val_loss: 29.9907, val_MinusLogProbMetric: 29.9907

Epoch 333: val_loss did not improve from 29.44652
196/196 - 36s - loss: 29.3163 - MinusLogProbMetric: 29.3163 - val_loss: 29.9907 - val_MinusLogProbMetric: 29.9907 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 334/1000
2023-10-24 08:16:13.262 
Epoch 334/1000 
	 loss: 29.2553, MinusLogProbMetric: 29.2553, val_loss: 29.5394, val_MinusLogProbMetric: 29.5394

Epoch 334: val_loss did not improve from 29.44652
196/196 - 35s - loss: 29.2553 - MinusLogProbMetric: 29.2553 - val_loss: 29.5394 - val_MinusLogProbMetric: 29.5394 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 335/1000
2023-10-24 08:16:49.611 
Epoch 335/1000 
	 loss: 29.2985, MinusLogProbMetric: 29.2985, val_loss: 30.0522, val_MinusLogProbMetric: 30.0522

Epoch 335: val_loss did not improve from 29.44652
196/196 - 36s - loss: 29.2985 - MinusLogProbMetric: 29.2985 - val_loss: 30.0522 - val_MinusLogProbMetric: 30.0522 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 336/1000
2023-10-24 08:17:24.866 
Epoch 336/1000 
	 loss: 29.2148, MinusLogProbMetric: 29.2148, val_loss: 29.7374, val_MinusLogProbMetric: 29.7374

Epoch 336: val_loss did not improve from 29.44652
196/196 - 35s - loss: 29.2148 - MinusLogProbMetric: 29.2148 - val_loss: 29.7374 - val_MinusLogProbMetric: 29.7374 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 337/1000
2023-10-24 08:17:57.859 
Epoch 337/1000 
	 loss: 29.3214, MinusLogProbMetric: 29.3214, val_loss: 29.7711, val_MinusLogProbMetric: 29.7711

Epoch 337: val_loss did not improve from 29.44652
196/196 - 33s - loss: 29.3214 - MinusLogProbMetric: 29.3214 - val_loss: 29.7711 - val_MinusLogProbMetric: 29.7711 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 338/1000
2023-10-24 08:18:33.294 
Epoch 338/1000 
	 loss: 29.2016, MinusLogProbMetric: 29.2016, val_loss: 29.4560, val_MinusLogProbMetric: 29.4560

Epoch 338: val_loss did not improve from 29.44652
196/196 - 35s - loss: 29.2016 - MinusLogProbMetric: 29.2016 - val_loss: 29.4560 - val_MinusLogProbMetric: 29.4560 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 339/1000
2023-10-24 08:19:07.649 
Epoch 339/1000 
	 loss: 29.1601, MinusLogProbMetric: 29.1601, val_loss: 29.3516, val_MinusLogProbMetric: 29.3516

Epoch 339: val_loss improved from 29.44652 to 29.35164, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 35s - loss: 29.1601 - MinusLogProbMetric: 29.1601 - val_loss: 29.3516 - val_MinusLogProbMetric: 29.3516 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 340/1000
2023-10-24 08:19:47.120 
Epoch 340/1000 
	 loss: 29.2462, MinusLogProbMetric: 29.2462, val_loss: 29.6784, val_MinusLogProbMetric: 29.6784

Epoch 340: val_loss did not improve from 29.35164
196/196 - 39s - loss: 29.2462 - MinusLogProbMetric: 29.2462 - val_loss: 29.6784 - val_MinusLogProbMetric: 29.6784 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 341/1000
2023-10-24 08:20:23.048 
Epoch 341/1000 
	 loss: 29.1978, MinusLogProbMetric: 29.1978, val_loss: 29.9981, val_MinusLogProbMetric: 29.9981

Epoch 341: val_loss did not improve from 29.35164
196/196 - 36s - loss: 29.1978 - MinusLogProbMetric: 29.1978 - val_loss: 29.9981 - val_MinusLogProbMetric: 29.9981 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 342/1000
2023-10-24 08:20:57.411 
Epoch 342/1000 
	 loss: 29.1272, MinusLogProbMetric: 29.1272, val_loss: 29.4794, val_MinusLogProbMetric: 29.4794

Epoch 342: val_loss did not improve from 29.35164
196/196 - 34s - loss: 29.1272 - MinusLogProbMetric: 29.1272 - val_loss: 29.4794 - val_MinusLogProbMetric: 29.4794 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 343/1000
2023-10-24 08:21:33.089 
Epoch 343/1000 
	 loss: 29.2280, MinusLogProbMetric: 29.2280, val_loss: 30.8003, val_MinusLogProbMetric: 30.8003

Epoch 343: val_loss did not improve from 29.35164
196/196 - 36s - loss: 29.2280 - MinusLogProbMetric: 29.2280 - val_loss: 30.8003 - val_MinusLogProbMetric: 30.8003 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 344/1000
2023-10-24 08:22:11.708 
Epoch 344/1000 
	 loss: 29.2518, MinusLogProbMetric: 29.2518, val_loss: 29.9314, val_MinusLogProbMetric: 29.9314

Epoch 344: val_loss did not improve from 29.35164
196/196 - 39s - loss: 29.2518 - MinusLogProbMetric: 29.2518 - val_loss: 29.9314 - val_MinusLogProbMetric: 29.9314 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 345/1000
2023-10-24 08:22:46.819 
Epoch 345/1000 
	 loss: 29.2970, MinusLogProbMetric: 29.2970, val_loss: 29.7281, val_MinusLogProbMetric: 29.7281

Epoch 345: val_loss did not improve from 29.35164
196/196 - 35s - loss: 29.2970 - MinusLogProbMetric: 29.2970 - val_loss: 29.7281 - val_MinusLogProbMetric: 29.7281 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 346/1000
2023-10-24 08:23:21.101 
Epoch 346/1000 
	 loss: 29.2273, MinusLogProbMetric: 29.2273, val_loss: 29.4590, val_MinusLogProbMetric: 29.4590

Epoch 346: val_loss did not improve from 29.35164
196/196 - 34s - loss: 29.2273 - MinusLogProbMetric: 29.2273 - val_loss: 29.4590 - val_MinusLogProbMetric: 29.4590 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 347/1000
2023-10-24 08:23:56.205 
Epoch 347/1000 
	 loss: 29.2626, MinusLogProbMetric: 29.2626, val_loss: 29.6942, val_MinusLogProbMetric: 29.6942

Epoch 347: val_loss did not improve from 29.35164
196/196 - 35s - loss: 29.2626 - MinusLogProbMetric: 29.2626 - val_loss: 29.6942 - val_MinusLogProbMetric: 29.6942 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 348/1000
2023-10-24 08:24:33.078 
Epoch 348/1000 
	 loss: 29.2143, MinusLogProbMetric: 29.2143, val_loss: 29.3654, val_MinusLogProbMetric: 29.3654

Epoch 348: val_loss did not improve from 29.35164
196/196 - 37s - loss: 29.2143 - MinusLogProbMetric: 29.2143 - val_loss: 29.3654 - val_MinusLogProbMetric: 29.3654 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 349/1000
2023-10-24 08:25:08.511 
Epoch 349/1000 
	 loss: 29.2191, MinusLogProbMetric: 29.2191, val_loss: 30.8026, val_MinusLogProbMetric: 30.8026

Epoch 349: val_loss did not improve from 29.35164
196/196 - 35s - loss: 29.2191 - MinusLogProbMetric: 29.2191 - val_loss: 30.8026 - val_MinusLogProbMetric: 30.8026 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 350/1000
2023-10-24 08:25:44.669 
Epoch 350/1000 
	 loss: 29.1848, MinusLogProbMetric: 29.1848, val_loss: 29.5103, val_MinusLogProbMetric: 29.5103

Epoch 350: val_loss did not improve from 29.35164
196/196 - 36s - loss: 29.1848 - MinusLogProbMetric: 29.1848 - val_loss: 29.5103 - val_MinusLogProbMetric: 29.5103 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 351/1000
2023-10-24 08:26:19.912 
Epoch 351/1000 
	 loss: 29.2434, MinusLogProbMetric: 29.2434, val_loss: 29.3048, val_MinusLogProbMetric: 29.3048

Epoch 351: val_loss improved from 29.35164 to 29.30485, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 36s - loss: 29.2434 - MinusLogProbMetric: 29.2434 - val_loss: 29.3048 - val_MinusLogProbMetric: 29.3048 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 352/1000
2023-10-24 08:26:56.163 
Epoch 352/1000 
	 loss: 29.1755, MinusLogProbMetric: 29.1755, val_loss: 29.6001, val_MinusLogProbMetric: 29.6001

Epoch 352: val_loss did not improve from 29.30485
196/196 - 36s - loss: 29.1755 - MinusLogProbMetric: 29.1755 - val_loss: 29.6001 - val_MinusLogProbMetric: 29.6001 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 353/1000
2023-10-24 08:27:32.417 
Epoch 353/1000 
	 loss: 29.1610, MinusLogProbMetric: 29.1610, val_loss: 29.5023, val_MinusLogProbMetric: 29.5023

Epoch 353: val_loss did not improve from 29.30485
196/196 - 36s - loss: 29.1610 - MinusLogProbMetric: 29.1610 - val_loss: 29.5023 - val_MinusLogProbMetric: 29.5023 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 354/1000
2023-10-24 08:28:07.107 
Epoch 354/1000 
	 loss: 29.1885, MinusLogProbMetric: 29.1885, val_loss: 29.7251, val_MinusLogProbMetric: 29.7251

Epoch 354: val_loss did not improve from 29.30485
196/196 - 35s - loss: 29.1885 - MinusLogProbMetric: 29.1885 - val_loss: 29.7251 - val_MinusLogProbMetric: 29.7251 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 355/1000
2023-10-24 08:28:42.740 
Epoch 355/1000 
	 loss: 29.3189, MinusLogProbMetric: 29.3189, val_loss: 29.9141, val_MinusLogProbMetric: 29.9141

Epoch 355: val_loss did not improve from 29.30485
196/196 - 36s - loss: 29.3189 - MinusLogProbMetric: 29.3189 - val_loss: 29.9141 - val_MinusLogProbMetric: 29.9141 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 356/1000
2023-10-24 08:29:17.495 
Epoch 356/1000 
	 loss: 29.2236, MinusLogProbMetric: 29.2236, val_loss: 29.5300, val_MinusLogProbMetric: 29.5300

Epoch 356: val_loss did not improve from 29.30485
196/196 - 35s - loss: 29.2236 - MinusLogProbMetric: 29.2236 - val_loss: 29.5300 - val_MinusLogProbMetric: 29.5300 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 357/1000
2023-10-24 08:29:52.161 
Epoch 357/1000 
	 loss: 29.2294, MinusLogProbMetric: 29.2294, val_loss: 30.1183, val_MinusLogProbMetric: 30.1183

Epoch 357: val_loss did not improve from 29.30485
196/196 - 35s - loss: 29.2294 - MinusLogProbMetric: 29.2294 - val_loss: 30.1183 - val_MinusLogProbMetric: 30.1183 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 358/1000
2023-10-24 08:30:29.493 
Epoch 358/1000 
	 loss: 29.2946, MinusLogProbMetric: 29.2946, val_loss: 30.4056, val_MinusLogProbMetric: 30.4056

Epoch 358: val_loss did not improve from 29.30485
196/196 - 37s - loss: 29.2946 - MinusLogProbMetric: 29.2946 - val_loss: 30.4056 - val_MinusLogProbMetric: 30.4056 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 359/1000
2023-10-24 08:31:04.586 
Epoch 359/1000 
	 loss: 29.1798, MinusLogProbMetric: 29.1798, val_loss: 29.4499, val_MinusLogProbMetric: 29.4499

Epoch 359: val_loss did not improve from 29.30485
196/196 - 35s - loss: 29.1798 - MinusLogProbMetric: 29.1798 - val_loss: 29.4499 - val_MinusLogProbMetric: 29.4499 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 360/1000
2023-10-24 08:31:40.015 
Epoch 360/1000 
	 loss: 29.0662, MinusLogProbMetric: 29.0662, val_loss: 29.3088, val_MinusLogProbMetric: 29.3088

Epoch 360: val_loss did not improve from 29.30485
196/196 - 35s - loss: 29.0662 - MinusLogProbMetric: 29.0662 - val_loss: 29.3088 - val_MinusLogProbMetric: 29.3088 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 361/1000
2023-10-24 08:32:17.575 
Epoch 361/1000 
	 loss: 29.1322, MinusLogProbMetric: 29.1322, val_loss: 30.0585, val_MinusLogProbMetric: 30.0585

Epoch 361: val_loss did not improve from 29.30485
196/196 - 38s - loss: 29.1322 - MinusLogProbMetric: 29.1322 - val_loss: 30.0585 - val_MinusLogProbMetric: 30.0585 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 362/1000
2023-10-24 08:32:54.737 
Epoch 362/1000 
	 loss: 29.2662, MinusLogProbMetric: 29.2662, val_loss: 29.6768, val_MinusLogProbMetric: 29.6768

Epoch 362: val_loss did not improve from 29.30485
196/196 - 37s - loss: 29.2662 - MinusLogProbMetric: 29.2662 - val_loss: 29.6768 - val_MinusLogProbMetric: 29.6768 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 363/1000
2023-10-24 08:33:28.751 
Epoch 363/1000 
	 loss: 29.2135, MinusLogProbMetric: 29.2135, val_loss: 29.7831, val_MinusLogProbMetric: 29.7831

Epoch 363: val_loss did not improve from 29.30485
196/196 - 34s - loss: 29.2135 - MinusLogProbMetric: 29.2135 - val_loss: 29.7831 - val_MinusLogProbMetric: 29.7831 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 364/1000
2023-10-24 08:34:01.718 
Epoch 364/1000 
	 loss: 29.1777, MinusLogProbMetric: 29.1777, val_loss: 29.4519, val_MinusLogProbMetric: 29.4519

Epoch 364: val_loss did not improve from 29.30485
196/196 - 33s - loss: 29.1777 - MinusLogProbMetric: 29.1777 - val_loss: 29.4519 - val_MinusLogProbMetric: 29.4519 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 365/1000
2023-10-24 08:34:34.310 
Epoch 365/1000 
	 loss: 29.1010, MinusLogProbMetric: 29.1010, val_loss: 29.7113, val_MinusLogProbMetric: 29.7113

Epoch 365: val_loss did not improve from 29.30485
196/196 - 33s - loss: 29.1010 - MinusLogProbMetric: 29.1010 - val_loss: 29.7113 - val_MinusLogProbMetric: 29.7113 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 366/1000
2023-10-24 08:35:08.834 
Epoch 366/1000 
	 loss: 29.0782, MinusLogProbMetric: 29.0782, val_loss: 29.3335, val_MinusLogProbMetric: 29.3335

Epoch 366: val_loss did not improve from 29.30485
196/196 - 35s - loss: 29.0782 - MinusLogProbMetric: 29.0782 - val_loss: 29.3335 - val_MinusLogProbMetric: 29.3335 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 367/1000
2023-10-24 08:35:43.173 
Epoch 367/1000 
	 loss: 29.0726, MinusLogProbMetric: 29.0726, val_loss: 30.9552, val_MinusLogProbMetric: 30.9552

Epoch 367: val_loss did not improve from 29.30485
196/196 - 34s - loss: 29.0726 - MinusLogProbMetric: 29.0726 - val_loss: 30.9552 - val_MinusLogProbMetric: 30.9552 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 368/1000
2023-10-24 08:36:17.718 
Epoch 368/1000 
	 loss: 29.2084, MinusLogProbMetric: 29.2084, val_loss: 29.6550, val_MinusLogProbMetric: 29.6550

Epoch 368: val_loss did not improve from 29.30485
196/196 - 35s - loss: 29.2084 - MinusLogProbMetric: 29.2084 - val_loss: 29.6550 - val_MinusLogProbMetric: 29.6550 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 369/1000
2023-10-24 08:36:52.133 
Epoch 369/1000 
	 loss: 29.0388, MinusLogProbMetric: 29.0388, val_loss: 30.2348, val_MinusLogProbMetric: 30.2348

Epoch 369: val_loss did not improve from 29.30485
196/196 - 34s - loss: 29.0388 - MinusLogProbMetric: 29.0388 - val_loss: 30.2348 - val_MinusLogProbMetric: 30.2348 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 370/1000
2023-10-24 08:37:29.214 
Epoch 370/1000 
	 loss: 29.1406, MinusLogProbMetric: 29.1406, val_loss: 29.1625, val_MinusLogProbMetric: 29.1625

Epoch 370: val_loss improved from 29.30485 to 29.16248, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 38s - loss: 29.1406 - MinusLogProbMetric: 29.1406 - val_loss: 29.1625 - val_MinusLogProbMetric: 29.1625 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 371/1000
2023-10-24 08:38:09.969 
Epoch 371/1000 
	 loss: 29.1069, MinusLogProbMetric: 29.1069, val_loss: 29.9394, val_MinusLogProbMetric: 29.9394

Epoch 371: val_loss did not improve from 29.16248
196/196 - 40s - loss: 29.1069 - MinusLogProbMetric: 29.1069 - val_loss: 29.9394 - val_MinusLogProbMetric: 29.9394 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 372/1000
2023-10-24 08:38:47.389 
Epoch 372/1000 
	 loss: 29.0692, MinusLogProbMetric: 29.0692, val_loss: 29.3557, val_MinusLogProbMetric: 29.3557

Epoch 372: val_loss did not improve from 29.16248
196/196 - 37s - loss: 29.0692 - MinusLogProbMetric: 29.0692 - val_loss: 29.3557 - val_MinusLogProbMetric: 29.3557 - lr: 3.3333e-04 - 37s/epoch - 191ms/step
Epoch 373/1000
2023-10-24 08:39:21.332 
Epoch 373/1000 
	 loss: 29.1711, MinusLogProbMetric: 29.1711, val_loss: 29.5410, val_MinusLogProbMetric: 29.5410

Epoch 373: val_loss did not improve from 29.16248
196/196 - 34s - loss: 29.1711 - MinusLogProbMetric: 29.1711 - val_loss: 29.5410 - val_MinusLogProbMetric: 29.5410 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 374/1000
2023-10-24 08:39:55.427 
Epoch 374/1000 
	 loss: 29.0802, MinusLogProbMetric: 29.0802, val_loss: 29.3560, val_MinusLogProbMetric: 29.3560

Epoch 374: val_loss did not improve from 29.16248
196/196 - 34s - loss: 29.0802 - MinusLogProbMetric: 29.0802 - val_loss: 29.3560 - val_MinusLogProbMetric: 29.3560 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 375/1000
2023-10-24 08:40:30.606 
Epoch 375/1000 
	 loss: 29.0913, MinusLogProbMetric: 29.0913, val_loss: 29.4441, val_MinusLogProbMetric: 29.4441

Epoch 375: val_loss did not improve from 29.16248
196/196 - 35s - loss: 29.0913 - MinusLogProbMetric: 29.0913 - val_loss: 29.4441 - val_MinusLogProbMetric: 29.4441 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 376/1000
2023-10-24 08:41:11.244 
Epoch 376/1000 
	 loss: 29.0516, MinusLogProbMetric: 29.0516, val_loss: 29.6450, val_MinusLogProbMetric: 29.6450

Epoch 376: val_loss did not improve from 29.16248
196/196 - 41s - loss: 29.0516 - MinusLogProbMetric: 29.0516 - val_loss: 29.6450 - val_MinusLogProbMetric: 29.6450 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 377/1000
2023-10-24 08:41:52.277 
Epoch 377/1000 
	 loss: 29.0882, MinusLogProbMetric: 29.0882, val_loss: 29.4253, val_MinusLogProbMetric: 29.4253

Epoch 377: val_loss did not improve from 29.16248
196/196 - 41s - loss: 29.0882 - MinusLogProbMetric: 29.0882 - val_loss: 29.4253 - val_MinusLogProbMetric: 29.4253 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 378/1000
2023-10-24 08:42:33.694 
Epoch 378/1000 
	 loss: 29.0713, MinusLogProbMetric: 29.0713, val_loss: 29.5823, val_MinusLogProbMetric: 29.5823

Epoch 378: val_loss did not improve from 29.16248
196/196 - 41s - loss: 29.0713 - MinusLogProbMetric: 29.0713 - val_loss: 29.5823 - val_MinusLogProbMetric: 29.5823 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 379/1000
2023-10-24 08:43:08.933 
Epoch 379/1000 
	 loss: 29.1866, MinusLogProbMetric: 29.1866, val_loss: 29.8393, val_MinusLogProbMetric: 29.8393

Epoch 379: val_loss did not improve from 29.16248
196/196 - 35s - loss: 29.1866 - MinusLogProbMetric: 29.1866 - val_loss: 29.8393 - val_MinusLogProbMetric: 29.8393 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 380/1000
2023-10-24 08:43:44.514 
Epoch 380/1000 
	 loss: 29.1148, MinusLogProbMetric: 29.1148, val_loss: 29.3361, val_MinusLogProbMetric: 29.3361

Epoch 380: val_loss did not improve from 29.16248
196/196 - 36s - loss: 29.1148 - MinusLogProbMetric: 29.1148 - val_loss: 29.3361 - val_MinusLogProbMetric: 29.3361 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 381/1000
2023-10-24 08:44:20.149 
Epoch 381/1000 
	 loss: 29.0659, MinusLogProbMetric: 29.0659, val_loss: 29.3680, val_MinusLogProbMetric: 29.3680

Epoch 381: val_loss did not improve from 29.16248
196/196 - 36s - loss: 29.0659 - MinusLogProbMetric: 29.0659 - val_loss: 29.3680 - val_MinusLogProbMetric: 29.3680 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 382/1000
2023-10-24 08:44:57.816 
Epoch 382/1000 
	 loss: 29.1433, MinusLogProbMetric: 29.1433, val_loss: 29.7244, val_MinusLogProbMetric: 29.7244

Epoch 382: val_loss did not improve from 29.16248
196/196 - 38s - loss: 29.1433 - MinusLogProbMetric: 29.1433 - val_loss: 29.7244 - val_MinusLogProbMetric: 29.7244 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 383/1000
2023-10-24 08:45:39.614 
Epoch 383/1000 
	 loss: 29.0456, MinusLogProbMetric: 29.0456, val_loss: 29.3974, val_MinusLogProbMetric: 29.3974

Epoch 383: val_loss did not improve from 29.16248
196/196 - 42s - loss: 29.0456 - MinusLogProbMetric: 29.0456 - val_loss: 29.3974 - val_MinusLogProbMetric: 29.3974 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 384/1000
2023-10-24 08:46:17.865 
Epoch 384/1000 
	 loss: 29.0528, MinusLogProbMetric: 29.0528, val_loss: 29.3244, val_MinusLogProbMetric: 29.3244

Epoch 384: val_loss did not improve from 29.16248
196/196 - 38s - loss: 29.0528 - MinusLogProbMetric: 29.0528 - val_loss: 29.3244 - val_MinusLogProbMetric: 29.3244 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 385/1000
2023-10-24 08:46:58.984 
Epoch 385/1000 
	 loss: 29.0362, MinusLogProbMetric: 29.0362, val_loss: 29.7341, val_MinusLogProbMetric: 29.7341

Epoch 385: val_loss did not improve from 29.16248
196/196 - 41s - loss: 29.0362 - MinusLogProbMetric: 29.0362 - val_loss: 29.7341 - val_MinusLogProbMetric: 29.7341 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 386/1000
2023-10-24 08:47:35.976 
Epoch 386/1000 
	 loss: 29.0719, MinusLogProbMetric: 29.0719, val_loss: 29.4210, val_MinusLogProbMetric: 29.4210

Epoch 386: val_loss did not improve from 29.16248
196/196 - 37s - loss: 29.0719 - MinusLogProbMetric: 29.0719 - val_loss: 29.4210 - val_MinusLogProbMetric: 29.4210 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 387/1000
2023-10-24 08:48:08.759 
Epoch 387/1000 
	 loss: 29.0465, MinusLogProbMetric: 29.0465, val_loss: 29.6166, val_MinusLogProbMetric: 29.6166

Epoch 387: val_loss did not improve from 29.16248
196/196 - 33s - loss: 29.0465 - MinusLogProbMetric: 29.0465 - val_loss: 29.6166 - val_MinusLogProbMetric: 29.6166 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 388/1000
2023-10-24 08:48:44.211 
Epoch 388/1000 
	 loss: 29.0408, MinusLogProbMetric: 29.0408, val_loss: 29.2784, val_MinusLogProbMetric: 29.2784

Epoch 388: val_loss did not improve from 29.16248
196/196 - 35s - loss: 29.0408 - MinusLogProbMetric: 29.0408 - val_loss: 29.2784 - val_MinusLogProbMetric: 29.2784 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 389/1000
2023-10-24 08:49:20.003 
Epoch 389/1000 
	 loss: 28.9964, MinusLogProbMetric: 28.9964, val_loss: 30.0331, val_MinusLogProbMetric: 30.0331

Epoch 389: val_loss did not improve from 29.16248
196/196 - 36s - loss: 28.9964 - MinusLogProbMetric: 28.9964 - val_loss: 30.0331 - val_MinusLogProbMetric: 30.0331 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 390/1000
2023-10-24 08:49:58.159 
Epoch 390/1000 
	 loss: 29.0710, MinusLogProbMetric: 29.0710, val_loss: 29.6489, val_MinusLogProbMetric: 29.6489

Epoch 390: val_loss did not improve from 29.16248
196/196 - 38s - loss: 29.0710 - MinusLogProbMetric: 29.0710 - val_loss: 29.6489 - val_MinusLogProbMetric: 29.6489 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 391/1000
2023-10-24 08:50:34.411 
Epoch 391/1000 
	 loss: 29.0657, MinusLogProbMetric: 29.0657, val_loss: 29.2870, val_MinusLogProbMetric: 29.2870

Epoch 391: val_loss did not improve from 29.16248
196/196 - 36s - loss: 29.0657 - MinusLogProbMetric: 29.0657 - val_loss: 29.2870 - val_MinusLogProbMetric: 29.2870 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 392/1000
2023-10-24 08:51:16.536 
Epoch 392/1000 
	 loss: 29.0139, MinusLogProbMetric: 29.0139, val_loss: 29.9079, val_MinusLogProbMetric: 29.9079

Epoch 392: val_loss did not improve from 29.16248
196/196 - 42s - loss: 29.0139 - MinusLogProbMetric: 29.0139 - val_loss: 29.9079 - val_MinusLogProbMetric: 29.9079 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 393/1000
2023-10-24 08:51:58.339 
Epoch 393/1000 
	 loss: 29.0504, MinusLogProbMetric: 29.0504, val_loss: 29.2976, val_MinusLogProbMetric: 29.2976

Epoch 393: val_loss did not improve from 29.16248
196/196 - 42s - loss: 29.0504 - MinusLogProbMetric: 29.0504 - val_loss: 29.2976 - val_MinusLogProbMetric: 29.2976 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 394/1000
2023-10-24 08:52:40.055 
Epoch 394/1000 
	 loss: 29.0180, MinusLogProbMetric: 29.0180, val_loss: 29.5883, val_MinusLogProbMetric: 29.5883

Epoch 394: val_loss did not improve from 29.16248
196/196 - 42s - loss: 29.0180 - MinusLogProbMetric: 29.0180 - val_loss: 29.5883 - val_MinusLogProbMetric: 29.5883 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 395/1000
2023-10-24 08:53:21.888 
Epoch 395/1000 
	 loss: 28.9954, MinusLogProbMetric: 28.9954, val_loss: 29.4756, val_MinusLogProbMetric: 29.4756

Epoch 395: val_loss did not improve from 29.16248
196/196 - 42s - loss: 28.9954 - MinusLogProbMetric: 28.9954 - val_loss: 29.4756 - val_MinusLogProbMetric: 29.4756 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 396/1000
2023-10-24 08:54:02.173 
Epoch 396/1000 
	 loss: 29.0429, MinusLogProbMetric: 29.0429, val_loss: 29.6103, val_MinusLogProbMetric: 29.6103

Epoch 396: val_loss did not improve from 29.16248
196/196 - 40s - loss: 29.0429 - MinusLogProbMetric: 29.0429 - val_loss: 29.6103 - val_MinusLogProbMetric: 29.6103 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 397/1000
2023-10-24 08:54:44.075 
Epoch 397/1000 
	 loss: 28.9931, MinusLogProbMetric: 28.9931, val_loss: 29.2880, val_MinusLogProbMetric: 29.2880

Epoch 397: val_loss did not improve from 29.16248
196/196 - 42s - loss: 28.9931 - MinusLogProbMetric: 28.9931 - val_loss: 29.2880 - val_MinusLogProbMetric: 29.2880 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 398/1000
2023-10-24 08:55:26.077 
Epoch 398/1000 
	 loss: 29.0194, MinusLogProbMetric: 29.0194, val_loss: 29.2775, val_MinusLogProbMetric: 29.2775

Epoch 398: val_loss did not improve from 29.16248
196/196 - 42s - loss: 29.0194 - MinusLogProbMetric: 29.0194 - val_loss: 29.2775 - val_MinusLogProbMetric: 29.2775 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 399/1000
2023-10-24 08:56:06.738 
Epoch 399/1000 
	 loss: 29.0830, MinusLogProbMetric: 29.0830, val_loss: 29.2343, val_MinusLogProbMetric: 29.2343

Epoch 399: val_loss did not improve from 29.16248
196/196 - 41s - loss: 29.0830 - MinusLogProbMetric: 29.0830 - val_loss: 29.2343 - val_MinusLogProbMetric: 29.2343 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 400/1000
2023-10-24 08:56:47.686 
Epoch 400/1000 
	 loss: 29.0074, MinusLogProbMetric: 29.0074, val_loss: 29.3126, val_MinusLogProbMetric: 29.3126

Epoch 400: val_loss did not improve from 29.16248
196/196 - 41s - loss: 29.0074 - MinusLogProbMetric: 29.0074 - val_loss: 29.3126 - val_MinusLogProbMetric: 29.3126 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 401/1000
2023-10-24 08:57:28.990 
Epoch 401/1000 
	 loss: 29.0299, MinusLogProbMetric: 29.0299, val_loss: 29.5249, val_MinusLogProbMetric: 29.5249

Epoch 401: val_loss did not improve from 29.16248
196/196 - 41s - loss: 29.0299 - MinusLogProbMetric: 29.0299 - val_loss: 29.5249 - val_MinusLogProbMetric: 29.5249 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 402/1000
2023-10-24 08:58:11.073 
Epoch 402/1000 
	 loss: 29.0057, MinusLogProbMetric: 29.0057, val_loss: 29.7931, val_MinusLogProbMetric: 29.7931

Epoch 402: val_loss did not improve from 29.16248
196/196 - 42s - loss: 29.0057 - MinusLogProbMetric: 29.0057 - val_loss: 29.7931 - val_MinusLogProbMetric: 29.7931 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 403/1000
2023-10-24 08:58:52.923 
Epoch 403/1000 
	 loss: 29.0031, MinusLogProbMetric: 29.0031, val_loss: 29.3418, val_MinusLogProbMetric: 29.3418

Epoch 403: val_loss did not improve from 29.16248
196/196 - 42s - loss: 29.0031 - MinusLogProbMetric: 29.0031 - val_loss: 29.3418 - val_MinusLogProbMetric: 29.3418 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 404/1000
2023-10-24 08:59:33.075 
Epoch 404/1000 
	 loss: 28.9961, MinusLogProbMetric: 28.9961, val_loss: 29.0568, val_MinusLogProbMetric: 29.0568

Epoch 404: val_loss improved from 29.16248 to 29.05682, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 41s - loss: 28.9961 - MinusLogProbMetric: 28.9961 - val_loss: 29.0568 - val_MinusLogProbMetric: 29.0568 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 405/1000
2023-10-24 09:00:13.062 
Epoch 405/1000 
	 loss: 28.9447, MinusLogProbMetric: 28.9447, val_loss: 29.5468, val_MinusLogProbMetric: 29.5468

Epoch 405: val_loss did not improve from 29.05682
196/196 - 39s - loss: 28.9447 - MinusLogProbMetric: 28.9447 - val_loss: 29.5468 - val_MinusLogProbMetric: 29.5468 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 406/1000
2023-10-24 09:00:52.832 
Epoch 406/1000 
	 loss: 29.0204, MinusLogProbMetric: 29.0204, val_loss: 29.3986, val_MinusLogProbMetric: 29.3986

Epoch 406: val_loss did not improve from 29.05682
196/196 - 40s - loss: 29.0204 - MinusLogProbMetric: 29.0204 - val_loss: 29.3986 - val_MinusLogProbMetric: 29.3986 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 407/1000
2023-10-24 09:01:32.929 
Epoch 407/1000 
	 loss: 28.9746, MinusLogProbMetric: 28.9746, val_loss: 29.5030, val_MinusLogProbMetric: 29.5030

Epoch 407: val_loss did not improve from 29.05682
196/196 - 40s - loss: 28.9746 - MinusLogProbMetric: 28.9746 - val_loss: 29.5030 - val_MinusLogProbMetric: 29.5030 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 408/1000
2023-10-24 09:02:12.549 
Epoch 408/1000 
	 loss: 28.9834, MinusLogProbMetric: 28.9834, val_loss: 29.4601, val_MinusLogProbMetric: 29.4601

Epoch 408: val_loss did not improve from 29.05682
196/196 - 40s - loss: 28.9834 - MinusLogProbMetric: 28.9834 - val_loss: 29.4601 - val_MinusLogProbMetric: 29.4601 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 409/1000
2023-10-24 09:02:51.971 
Epoch 409/1000 
	 loss: 28.9849, MinusLogProbMetric: 28.9849, val_loss: 29.5529, val_MinusLogProbMetric: 29.5529

Epoch 409: val_loss did not improve from 29.05682
196/196 - 39s - loss: 28.9849 - MinusLogProbMetric: 28.9849 - val_loss: 29.5529 - val_MinusLogProbMetric: 29.5529 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 410/1000
2023-10-24 09:03:33.368 
Epoch 410/1000 
	 loss: 28.9234, MinusLogProbMetric: 28.9234, val_loss: 29.1844, val_MinusLogProbMetric: 29.1844

Epoch 410: val_loss did not improve from 29.05682
196/196 - 41s - loss: 28.9234 - MinusLogProbMetric: 28.9234 - val_loss: 29.1844 - val_MinusLogProbMetric: 29.1844 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 411/1000
2023-10-24 09:04:15.522 
Epoch 411/1000 
	 loss: 29.0345, MinusLogProbMetric: 29.0345, val_loss: 29.4286, val_MinusLogProbMetric: 29.4286

Epoch 411: val_loss did not improve from 29.05682
196/196 - 42s - loss: 29.0345 - MinusLogProbMetric: 29.0345 - val_loss: 29.4286 - val_MinusLogProbMetric: 29.4286 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 412/1000
2023-10-24 09:04:56.211 
Epoch 412/1000 
	 loss: 28.9318, MinusLogProbMetric: 28.9318, val_loss: 29.2375, val_MinusLogProbMetric: 29.2375

Epoch 412: val_loss did not improve from 29.05682
196/196 - 41s - loss: 28.9318 - MinusLogProbMetric: 28.9318 - val_loss: 29.2375 - val_MinusLogProbMetric: 29.2375 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 413/1000
2023-10-24 09:05:38.383 
Epoch 413/1000 
	 loss: 29.0850, MinusLogProbMetric: 29.0850, val_loss: 29.2212, val_MinusLogProbMetric: 29.2212

Epoch 413: val_loss did not improve from 29.05682
196/196 - 42s - loss: 29.0850 - MinusLogProbMetric: 29.0850 - val_loss: 29.2212 - val_MinusLogProbMetric: 29.2212 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 414/1000
2023-10-24 09:06:20.294 
Epoch 414/1000 
	 loss: 28.9117, MinusLogProbMetric: 28.9117, val_loss: 29.6090, val_MinusLogProbMetric: 29.6090

Epoch 414: val_loss did not improve from 29.05682
196/196 - 42s - loss: 28.9117 - MinusLogProbMetric: 28.9117 - val_loss: 29.6090 - val_MinusLogProbMetric: 29.6090 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 415/1000
2023-10-24 09:07:02.693 
Epoch 415/1000 
	 loss: 28.9867, MinusLogProbMetric: 28.9867, val_loss: 29.4217, val_MinusLogProbMetric: 29.4217

Epoch 415: val_loss did not improve from 29.05682
196/196 - 42s - loss: 28.9867 - MinusLogProbMetric: 28.9867 - val_loss: 29.4217 - val_MinusLogProbMetric: 29.4217 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 416/1000
2023-10-24 09:07:44.773 
Epoch 416/1000 
	 loss: 28.9555, MinusLogProbMetric: 28.9555, val_loss: 29.3416, val_MinusLogProbMetric: 29.3416

Epoch 416: val_loss did not improve from 29.05682
196/196 - 42s - loss: 28.9555 - MinusLogProbMetric: 28.9555 - val_loss: 29.3416 - val_MinusLogProbMetric: 29.3416 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 417/1000
2023-10-24 09:08:24.092 
Epoch 417/1000 
	 loss: 28.9362, MinusLogProbMetric: 28.9362, val_loss: 29.5633, val_MinusLogProbMetric: 29.5633

Epoch 417: val_loss did not improve from 29.05682
196/196 - 39s - loss: 28.9362 - MinusLogProbMetric: 28.9362 - val_loss: 29.5633 - val_MinusLogProbMetric: 29.5633 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 418/1000
2023-10-24 09:09:05.604 
Epoch 418/1000 
	 loss: 28.8941, MinusLogProbMetric: 28.8941, val_loss: 29.3246, val_MinusLogProbMetric: 29.3246

Epoch 418: val_loss did not improve from 29.05682
196/196 - 42s - loss: 28.8941 - MinusLogProbMetric: 28.8941 - val_loss: 29.3246 - val_MinusLogProbMetric: 29.3246 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 419/1000
2023-10-24 09:09:45.948 
Epoch 419/1000 
	 loss: 29.0301, MinusLogProbMetric: 29.0301, val_loss: 29.5606, val_MinusLogProbMetric: 29.5606

Epoch 419: val_loss did not improve from 29.05682
196/196 - 40s - loss: 29.0301 - MinusLogProbMetric: 29.0301 - val_loss: 29.5606 - val_MinusLogProbMetric: 29.5606 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 420/1000
2023-10-24 09:10:27.639 
Epoch 420/1000 
	 loss: 28.9299, MinusLogProbMetric: 28.9299, val_loss: 30.0394, val_MinusLogProbMetric: 30.0394

Epoch 420: val_loss did not improve from 29.05682
196/196 - 42s - loss: 28.9299 - MinusLogProbMetric: 28.9299 - val_loss: 30.0394 - val_MinusLogProbMetric: 30.0394 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 421/1000
2023-10-24 09:11:07.651 
Epoch 421/1000 
	 loss: 29.0168, MinusLogProbMetric: 29.0168, val_loss: 29.9847, val_MinusLogProbMetric: 29.9847

Epoch 421: val_loss did not improve from 29.05682
196/196 - 40s - loss: 29.0168 - MinusLogProbMetric: 29.0168 - val_loss: 29.9847 - val_MinusLogProbMetric: 29.9847 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 422/1000
2023-10-24 09:11:46.405 
Epoch 422/1000 
	 loss: 28.9547, MinusLogProbMetric: 28.9547, val_loss: 29.3894, val_MinusLogProbMetric: 29.3894

Epoch 422: val_loss did not improve from 29.05682
196/196 - 39s - loss: 28.9547 - MinusLogProbMetric: 28.9547 - val_loss: 29.3894 - val_MinusLogProbMetric: 29.3894 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 423/1000
2023-10-24 09:12:24.118 
Epoch 423/1000 
	 loss: 28.9424, MinusLogProbMetric: 28.9424, val_loss: 30.6045, val_MinusLogProbMetric: 30.6045

Epoch 423: val_loss did not improve from 29.05682
196/196 - 38s - loss: 28.9424 - MinusLogProbMetric: 28.9424 - val_loss: 30.6045 - val_MinusLogProbMetric: 30.6045 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 424/1000
2023-10-24 09:13:02.810 
Epoch 424/1000 
	 loss: 29.4227, MinusLogProbMetric: 29.4227, val_loss: 29.2693, val_MinusLogProbMetric: 29.2693

Epoch 424: val_loss did not improve from 29.05682
196/196 - 39s - loss: 29.4227 - MinusLogProbMetric: 29.4227 - val_loss: 29.2693 - val_MinusLogProbMetric: 29.2693 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 425/1000
2023-10-24 09:13:42.241 
Epoch 425/1000 
	 loss: 28.9339, MinusLogProbMetric: 28.9339, val_loss: 29.0533, val_MinusLogProbMetric: 29.0533

Epoch 425: val_loss improved from 29.05682 to 29.05335, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 40s - loss: 28.9339 - MinusLogProbMetric: 28.9339 - val_loss: 29.0533 - val_MinusLogProbMetric: 29.0533 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 426/1000
2023-10-24 09:14:25.024 
Epoch 426/1000 
	 loss: 29.0071, MinusLogProbMetric: 29.0071, val_loss: 29.1730, val_MinusLogProbMetric: 29.1730

Epoch 426: val_loss did not improve from 29.05335
196/196 - 42s - loss: 29.0071 - MinusLogProbMetric: 29.0071 - val_loss: 29.1730 - val_MinusLogProbMetric: 29.1730 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 427/1000
2023-10-24 09:15:05.837 
Epoch 427/1000 
	 loss: 29.0126, MinusLogProbMetric: 29.0126, val_loss: 29.5066, val_MinusLogProbMetric: 29.5066

Epoch 427: val_loss did not improve from 29.05335
196/196 - 41s - loss: 29.0126 - MinusLogProbMetric: 29.0126 - val_loss: 29.5066 - val_MinusLogProbMetric: 29.5066 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 428/1000
2023-10-24 09:15:48.174 
Epoch 428/1000 
	 loss: 28.9729, MinusLogProbMetric: 28.9729, val_loss: 29.2037, val_MinusLogProbMetric: 29.2037

Epoch 428: val_loss did not improve from 29.05335
196/196 - 42s - loss: 28.9729 - MinusLogProbMetric: 28.9729 - val_loss: 29.2037 - val_MinusLogProbMetric: 29.2037 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 429/1000
2023-10-24 09:16:26.174 
Epoch 429/1000 
	 loss: 28.8750, MinusLogProbMetric: 28.8750, val_loss: 29.5712, val_MinusLogProbMetric: 29.5712

Epoch 429: val_loss did not improve from 29.05335
196/196 - 38s - loss: 28.8750 - MinusLogProbMetric: 28.8750 - val_loss: 29.5712 - val_MinusLogProbMetric: 29.5712 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 430/1000
2023-10-24 09:17:06.954 
Epoch 430/1000 
	 loss: 28.9470, MinusLogProbMetric: 28.9470, val_loss: 29.4212, val_MinusLogProbMetric: 29.4212

Epoch 430: val_loss did not improve from 29.05335
196/196 - 41s - loss: 28.9470 - MinusLogProbMetric: 28.9470 - val_loss: 29.4212 - val_MinusLogProbMetric: 29.4212 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 431/1000
2023-10-24 09:17:47.645 
Epoch 431/1000 
	 loss: 28.9550, MinusLogProbMetric: 28.9550, val_loss: 29.4636, val_MinusLogProbMetric: 29.4636

Epoch 431: val_loss did not improve from 29.05335
196/196 - 41s - loss: 28.9550 - MinusLogProbMetric: 28.9550 - val_loss: 29.4636 - val_MinusLogProbMetric: 29.4636 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 432/1000
2023-10-24 09:18:27.507 
Epoch 432/1000 
	 loss: 28.8848, MinusLogProbMetric: 28.8848, val_loss: 29.1300, val_MinusLogProbMetric: 29.1300

Epoch 432: val_loss did not improve from 29.05335
196/196 - 40s - loss: 28.8848 - MinusLogProbMetric: 28.8848 - val_loss: 29.1300 - val_MinusLogProbMetric: 29.1300 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 433/1000
2023-10-24 09:19:09.456 
Epoch 433/1000 
	 loss: 28.8745, MinusLogProbMetric: 28.8745, val_loss: 29.3199, val_MinusLogProbMetric: 29.3199

Epoch 433: val_loss did not improve from 29.05335
196/196 - 42s - loss: 28.8745 - MinusLogProbMetric: 28.8745 - val_loss: 29.3199 - val_MinusLogProbMetric: 29.3199 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 434/1000
2023-10-24 09:19:51.673 
Epoch 434/1000 
	 loss: 28.9605, MinusLogProbMetric: 28.9605, val_loss: 29.2314, val_MinusLogProbMetric: 29.2314

Epoch 434: val_loss did not improve from 29.05335
196/196 - 42s - loss: 28.9605 - MinusLogProbMetric: 28.9605 - val_loss: 29.2314 - val_MinusLogProbMetric: 29.2314 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 435/1000
2023-10-24 09:20:31.976 
Epoch 435/1000 
	 loss: 28.8537, MinusLogProbMetric: 28.8537, val_loss: 29.1637, val_MinusLogProbMetric: 29.1637

Epoch 435: val_loss did not improve from 29.05335
196/196 - 40s - loss: 28.8537 - MinusLogProbMetric: 28.8537 - val_loss: 29.1637 - val_MinusLogProbMetric: 29.1637 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 436/1000
2023-10-24 09:21:13.829 
Epoch 436/1000 
	 loss: 28.9542, MinusLogProbMetric: 28.9542, val_loss: 29.2608, val_MinusLogProbMetric: 29.2608

Epoch 436: val_loss did not improve from 29.05335
196/196 - 42s - loss: 28.9542 - MinusLogProbMetric: 28.9542 - val_loss: 29.2608 - val_MinusLogProbMetric: 29.2608 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 437/1000
2023-10-24 09:21:53.705 
Epoch 437/1000 
	 loss: 28.9706, MinusLogProbMetric: 28.9706, val_loss: 29.0811, val_MinusLogProbMetric: 29.0811

Epoch 437: val_loss did not improve from 29.05335
196/196 - 40s - loss: 28.9706 - MinusLogProbMetric: 28.9706 - val_loss: 29.0811 - val_MinusLogProbMetric: 29.0811 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 438/1000
2023-10-24 09:22:34.175 
Epoch 438/1000 
	 loss: 28.9032, MinusLogProbMetric: 28.9032, val_loss: 29.4198, val_MinusLogProbMetric: 29.4198

Epoch 438: val_loss did not improve from 29.05335
196/196 - 40s - loss: 28.9032 - MinusLogProbMetric: 28.9032 - val_loss: 29.4198 - val_MinusLogProbMetric: 29.4198 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 439/1000
2023-10-24 09:23:16.067 
Epoch 439/1000 
	 loss: 28.8908, MinusLogProbMetric: 28.8908, val_loss: 29.2810, val_MinusLogProbMetric: 29.2810

Epoch 439: val_loss did not improve from 29.05335
196/196 - 42s - loss: 28.8908 - MinusLogProbMetric: 28.8908 - val_loss: 29.2810 - val_MinusLogProbMetric: 29.2810 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 440/1000
2023-10-24 09:23:57.844 
Epoch 440/1000 
	 loss: 28.8906, MinusLogProbMetric: 28.8906, val_loss: 29.1757, val_MinusLogProbMetric: 29.1757

Epoch 440: val_loss did not improve from 29.05335
196/196 - 42s - loss: 28.8906 - MinusLogProbMetric: 28.8906 - val_loss: 29.1757 - val_MinusLogProbMetric: 29.1757 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 441/1000
2023-10-24 09:24:39.486 
Epoch 441/1000 
	 loss: 28.9180, MinusLogProbMetric: 28.9180, val_loss: 29.0537, val_MinusLogProbMetric: 29.0537

Epoch 441: val_loss did not improve from 29.05335
196/196 - 42s - loss: 28.9180 - MinusLogProbMetric: 28.9180 - val_loss: 29.0537 - val_MinusLogProbMetric: 29.0537 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 442/1000
2023-10-24 09:25:19.348 
Epoch 442/1000 
	 loss: 28.8990, MinusLogProbMetric: 28.8990, val_loss: 29.5071, val_MinusLogProbMetric: 29.5071

Epoch 442: val_loss did not improve from 29.05335
196/196 - 40s - loss: 28.8990 - MinusLogProbMetric: 28.8990 - val_loss: 29.5071 - val_MinusLogProbMetric: 29.5071 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 443/1000
2023-10-24 09:26:01.984 
Epoch 443/1000 
	 loss: 28.8925, MinusLogProbMetric: 28.8925, val_loss: 29.5743, val_MinusLogProbMetric: 29.5743

Epoch 443: val_loss did not improve from 29.05335
196/196 - 43s - loss: 28.8925 - MinusLogProbMetric: 28.8925 - val_loss: 29.5743 - val_MinusLogProbMetric: 29.5743 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 444/1000
2023-10-24 09:26:41.456 
Epoch 444/1000 
	 loss: 28.9007, MinusLogProbMetric: 28.9007, val_loss: 29.2532, val_MinusLogProbMetric: 29.2532

Epoch 444: val_loss did not improve from 29.05335
196/196 - 39s - loss: 28.9007 - MinusLogProbMetric: 28.9007 - val_loss: 29.2532 - val_MinusLogProbMetric: 29.2532 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 445/1000
2023-10-24 09:27:20.889 
Epoch 445/1000 
	 loss: 28.8668, MinusLogProbMetric: 28.8668, val_loss: 29.3599, val_MinusLogProbMetric: 29.3599

Epoch 445: val_loss did not improve from 29.05335
196/196 - 39s - loss: 28.8668 - MinusLogProbMetric: 28.8668 - val_loss: 29.3599 - val_MinusLogProbMetric: 29.3599 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 446/1000
2023-10-24 09:28:02.810 
Epoch 446/1000 
	 loss: 28.9467, MinusLogProbMetric: 28.9467, val_loss: 29.4233, val_MinusLogProbMetric: 29.4233

Epoch 446: val_loss did not improve from 29.05335
196/196 - 42s - loss: 28.9467 - MinusLogProbMetric: 28.9467 - val_loss: 29.4233 - val_MinusLogProbMetric: 29.4233 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 447/1000
2023-10-24 09:28:44.607 
Epoch 447/1000 
	 loss: 28.8755, MinusLogProbMetric: 28.8755, val_loss: 29.6556, val_MinusLogProbMetric: 29.6556

Epoch 447: val_loss did not improve from 29.05335
196/196 - 42s - loss: 28.8755 - MinusLogProbMetric: 28.8755 - val_loss: 29.6556 - val_MinusLogProbMetric: 29.6556 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 448/1000
2023-10-24 09:29:25.642 
Epoch 448/1000 
	 loss: 28.9499, MinusLogProbMetric: 28.9499, val_loss: 29.0938, val_MinusLogProbMetric: 29.0938

Epoch 448: val_loss did not improve from 29.05335
196/196 - 41s - loss: 28.9499 - MinusLogProbMetric: 28.9499 - val_loss: 29.0938 - val_MinusLogProbMetric: 29.0938 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 449/1000
2023-10-24 09:30:05.987 
Epoch 449/1000 
	 loss: 28.8166, MinusLogProbMetric: 28.8166, val_loss: 29.1514, val_MinusLogProbMetric: 29.1514

Epoch 449: val_loss did not improve from 29.05335
196/196 - 40s - loss: 28.8166 - MinusLogProbMetric: 28.8166 - val_loss: 29.1514 - val_MinusLogProbMetric: 29.1514 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 450/1000
2023-10-24 09:30:47.931 
Epoch 450/1000 
	 loss: 28.9028, MinusLogProbMetric: 28.9028, val_loss: 29.0850, val_MinusLogProbMetric: 29.0850

Epoch 450: val_loss did not improve from 29.05335
196/196 - 42s - loss: 28.9028 - MinusLogProbMetric: 28.9028 - val_loss: 29.0850 - val_MinusLogProbMetric: 29.0850 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 451/1000
2023-10-24 09:31:28.401 
Epoch 451/1000 
	 loss: 28.8604, MinusLogProbMetric: 28.8604, val_loss: 29.1383, val_MinusLogProbMetric: 29.1383

Epoch 451: val_loss did not improve from 29.05335
196/196 - 40s - loss: 28.8604 - MinusLogProbMetric: 28.8604 - val_loss: 29.1383 - val_MinusLogProbMetric: 29.1383 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 452/1000
2023-10-24 09:32:08.354 
Epoch 452/1000 
	 loss: 28.7822, MinusLogProbMetric: 28.7822, val_loss: 29.1458, val_MinusLogProbMetric: 29.1458

Epoch 452: val_loss did not improve from 29.05335
196/196 - 40s - loss: 28.7822 - MinusLogProbMetric: 28.7822 - val_loss: 29.1458 - val_MinusLogProbMetric: 29.1458 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 453/1000
2023-10-24 09:32:47.811 
Epoch 453/1000 
	 loss: 28.8617, MinusLogProbMetric: 28.8617, val_loss: 29.1846, val_MinusLogProbMetric: 29.1846

Epoch 453: val_loss did not improve from 29.05335
196/196 - 39s - loss: 28.8617 - MinusLogProbMetric: 28.8617 - val_loss: 29.1846 - val_MinusLogProbMetric: 29.1846 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 454/1000
2023-10-24 09:33:29.505 
Epoch 454/1000 
	 loss: 28.8187, MinusLogProbMetric: 28.8187, val_loss: 29.4323, val_MinusLogProbMetric: 29.4323

Epoch 454: val_loss did not improve from 29.05335
196/196 - 42s - loss: 28.8187 - MinusLogProbMetric: 28.8187 - val_loss: 29.4323 - val_MinusLogProbMetric: 29.4323 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 455/1000
2023-10-24 09:34:09.294 
Epoch 455/1000 
	 loss: 28.9528, MinusLogProbMetric: 28.9528, val_loss: 29.2385, val_MinusLogProbMetric: 29.2385

Epoch 455: val_loss did not improve from 29.05335
196/196 - 40s - loss: 28.9528 - MinusLogProbMetric: 28.9528 - val_loss: 29.2385 - val_MinusLogProbMetric: 29.2385 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 456/1000
2023-10-24 09:34:50.503 
Epoch 456/1000 
	 loss: 28.8497, MinusLogProbMetric: 28.8497, val_loss: 29.8596, val_MinusLogProbMetric: 29.8596

Epoch 456: val_loss did not improve from 29.05335
196/196 - 41s - loss: 28.8497 - MinusLogProbMetric: 28.8497 - val_loss: 29.8596 - val_MinusLogProbMetric: 29.8596 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 457/1000
2023-10-24 09:35:32.194 
Epoch 457/1000 
	 loss: 28.8595, MinusLogProbMetric: 28.8595, val_loss: 29.5218, val_MinusLogProbMetric: 29.5218

Epoch 457: val_loss did not improve from 29.05335
196/196 - 42s - loss: 28.8595 - MinusLogProbMetric: 28.8595 - val_loss: 29.5218 - val_MinusLogProbMetric: 29.5218 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 458/1000
2023-10-24 09:36:13.496 
Epoch 458/1000 
	 loss: 28.8308, MinusLogProbMetric: 28.8308, val_loss: 29.5239, val_MinusLogProbMetric: 29.5239

Epoch 458: val_loss did not improve from 29.05335
196/196 - 41s - loss: 28.8308 - MinusLogProbMetric: 28.8308 - val_loss: 29.5239 - val_MinusLogProbMetric: 29.5239 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 459/1000
2023-10-24 09:36:55.846 
Epoch 459/1000 
	 loss: 28.8492, MinusLogProbMetric: 28.8492, val_loss: 29.3481, val_MinusLogProbMetric: 29.3481

Epoch 459: val_loss did not improve from 29.05335
196/196 - 42s - loss: 28.8492 - MinusLogProbMetric: 28.8492 - val_loss: 29.3481 - val_MinusLogProbMetric: 29.3481 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 460/1000
2023-10-24 09:37:37.417 
Epoch 460/1000 
	 loss: 28.8350, MinusLogProbMetric: 28.8350, val_loss: 29.2374, val_MinusLogProbMetric: 29.2374

Epoch 460: val_loss did not improve from 29.05335
196/196 - 42s - loss: 28.8350 - MinusLogProbMetric: 28.8350 - val_loss: 29.2374 - val_MinusLogProbMetric: 29.2374 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 461/1000
2023-10-24 09:38:18.904 
Epoch 461/1000 
	 loss: 28.8331, MinusLogProbMetric: 28.8331, val_loss: 29.5851, val_MinusLogProbMetric: 29.5851

Epoch 461: val_loss did not improve from 29.05335
196/196 - 41s - loss: 28.8331 - MinusLogProbMetric: 28.8331 - val_loss: 29.5851 - val_MinusLogProbMetric: 29.5851 - lr: 3.3333e-04 - 41s/epoch - 212ms/step
Epoch 462/1000
2023-10-24 09:38:57.721 
Epoch 462/1000 
	 loss: 28.8782, MinusLogProbMetric: 28.8782, val_loss: 29.2136, val_MinusLogProbMetric: 29.2136

Epoch 462: val_loss did not improve from 29.05335
196/196 - 39s - loss: 28.8782 - MinusLogProbMetric: 28.8782 - val_loss: 29.2136 - val_MinusLogProbMetric: 29.2136 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 463/1000
2023-10-24 09:39:38.305 
Epoch 463/1000 
	 loss: 28.8994, MinusLogProbMetric: 28.8994, val_loss: 29.4612, val_MinusLogProbMetric: 29.4612

Epoch 463: val_loss did not improve from 29.05335
196/196 - 41s - loss: 28.8994 - MinusLogProbMetric: 28.8994 - val_loss: 29.4612 - val_MinusLogProbMetric: 29.4612 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 464/1000
2023-10-24 09:40:20.684 
Epoch 464/1000 
	 loss: 28.8253, MinusLogProbMetric: 28.8253, val_loss: 29.0854, val_MinusLogProbMetric: 29.0854

Epoch 464: val_loss did not improve from 29.05335
196/196 - 42s - loss: 28.8253 - MinusLogProbMetric: 28.8253 - val_loss: 29.0854 - val_MinusLogProbMetric: 29.0854 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 465/1000
2023-10-24 09:41:00.544 
Epoch 465/1000 
	 loss: 28.8905, MinusLogProbMetric: 28.8905, val_loss: 29.0961, val_MinusLogProbMetric: 29.0961

Epoch 465: val_loss did not improve from 29.05335
196/196 - 40s - loss: 28.8905 - MinusLogProbMetric: 28.8905 - val_loss: 29.0961 - val_MinusLogProbMetric: 29.0961 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 466/1000
2023-10-24 09:41:38.755 
Epoch 466/1000 
	 loss: 28.8247, MinusLogProbMetric: 28.8247, val_loss: 28.9772, val_MinusLogProbMetric: 28.9772

Epoch 466: val_loss improved from 29.05335 to 28.97717, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 28.8247 - MinusLogProbMetric: 28.8247 - val_loss: 28.9772 - val_MinusLogProbMetric: 28.9772 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 467/1000
2023-10-24 09:42:19.762 
Epoch 467/1000 
	 loss: 28.7780, MinusLogProbMetric: 28.7780, val_loss: 29.7895, val_MinusLogProbMetric: 29.7895

Epoch 467: val_loss did not improve from 28.97717
196/196 - 40s - loss: 28.7780 - MinusLogProbMetric: 28.7780 - val_loss: 29.7895 - val_MinusLogProbMetric: 29.7895 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 468/1000
2023-10-24 09:42:58.467 
Epoch 468/1000 
	 loss: 28.9325, MinusLogProbMetric: 28.9325, val_loss: 29.2580, val_MinusLogProbMetric: 29.2580

Epoch 468: val_loss did not improve from 28.97717
196/196 - 39s - loss: 28.9325 - MinusLogProbMetric: 28.9325 - val_loss: 29.2580 - val_MinusLogProbMetric: 29.2580 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 469/1000
2023-10-24 09:43:38.091 
Epoch 469/1000 
	 loss: 28.7832, MinusLogProbMetric: 28.7832, val_loss: 29.0227, val_MinusLogProbMetric: 29.0227

Epoch 469: val_loss did not improve from 28.97717
196/196 - 40s - loss: 28.7832 - MinusLogProbMetric: 28.7832 - val_loss: 29.0227 - val_MinusLogProbMetric: 29.0227 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 470/1000
2023-10-24 09:44:18.854 
Epoch 470/1000 
	 loss: 28.8024, MinusLogProbMetric: 28.8024, val_loss: 29.0464, val_MinusLogProbMetric: 29.0464

Epoch 470: val_loss did not improve from 28.97717
196/196 - 41s - loss: 28.8024 - MinusLogProbMetric: 28.8024 - val_loss: 29.0464 - val_MinusLogProbMetric: 29.0464 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 471/1000
2023-10-24 09:44:58.203 
Epoch 471/1000 
	 loss: 28.8074, MinusLogProbMetric: 28.8074, val_loss: 29.3154, val_MinusLogProbMetric: 29.3154

Epoch 471: val_loss did not improve from 28.97717
196/196 - 39s - loss: 28.8074 - MinusLogProbMetric: 28.8074 - val_loss: 29.3154 - val_MinusLogProbMetric: 29.3154 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 472/1000
2023-10-24 09:45:38.533 
Epoch 472/1000 
	 loss: 28.8772, MinusLogProbMetric: 28.8772, val_loss: 29.2023, val_MinusLogProbMetric: 29.2023

Epoch 472: val_loss did not improve from 28.97717
196/196 - 40s - loss: 28.8772 - MinusLogProbMetric: 28.8772 - val_loss: 29.2023 - val_MinusLogProbMetric: 29.2023 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 473/1000
2023-10-24 09:46:17.749 
Epoch 473/1000 
	 loss: 28.7689, MinusLogProbMetric: 28.7689, val_loss: 29.3767, val_MinusLogProbMetric: 29.3767

Epoch 473: val_loss did not improve from 28.97717
196/196 - 39s - loss: 28.7689 - MinusLogProbMetric: 28.7689 - val_loss: 29.3767 - val_MinusLogProbMetric: 29.3767 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 474/1000
2023-10-24 09:46:55.862 
Epoch 474/1000 
	 loss: 28.8280, MinusLogProbMetric: 28.8280, val_loss: 29.5907, val_MinusLogProbMetric: 29.5907

Epoch 474: val_loss did not improve from 28.97717
196/196 - 38s - loss: 28.8280 - MinusLogProbMetric: 28.8280 - val_loss: 29.5907 - val_MinusLogProbMetric: 29.5907 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 475/1000
2023-10-24 09:47:37.231 
Epoch 475/1000 
	 loss: 28.7677, MinusLogProbMetric: 28.7677, val_loss: 30.7441, val_MinusLogProbMetric: 30.7441

Epoch 475: val_loss did not improve from 28.97717
196/196 - 41s - loss: 28.7677 - MinusLogProbMetric: 28.7677 - val_loss: 30.7441 - val_MinusLogProbMetric: 30.7441 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 476/1000
2023-10-24 09:48:17.880 
Epoch 476/1000 
	 loss: 28.9177, MinusLogProbMetric: 28.9177, val_loss: 29.4127, val_MinusLogProbMetric: 29.4127

Epoch 476: val_loss did not improve from 28.97717
196/196 - 41s - loss: 28.9177 - MinusLogProbMetric: 28.9177 - val_loss: 29.4127 - val_MinusLogProbMetric: 29.4127 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 477/1000
2023-10-24 09:48:58.487 
Epoch 477/1000 
	 loss: 28.8337, MinusLogProbMetric: 28.8337, val_loss: 29.1049, val_MinusLogProbMetric: 29.1049

Epoch 477: val_loss did not improve from 28.97717
196/196 - 41s - loss: 28.8337 - MinusLogProbMetric: 28.8337 - val_loss: 29.1049 - val_MinusLogProbMetric: 29.1049 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 478/1000
2023-10-24 09:49:39.760 
Epoch 478/1000 
	 loss: 28.8092, MinusLogProbMetric: 28.8092, val_loss: 29.0926, val_MinusLogProbMetric: 29.0926

Epoch 478: val_loss did not improve from 28.97717
196/196 - 41s - loss: 28.8092 - MinusLogProbMetric: 28.8092 - val_loss: 29.0926 - val_MinusLogProbMetric: 29.0926 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 479/1000
2023-10-24 09:50:18.656 
Epoch 479/1000 
	 loss: 28.8063, MinusLogProbMetric: 28.8063, val_loss: 29.0462, val_MinusLogProbMetric: 29.0462

Epoch 479: val_loss did not improve from 28.97717
196/196 - 39s - loss: 28.8063 - MinusLogProbMetric: 28.8063 - val_loss: 29.0462 - val_MinusLogProbMetric: 29.0462 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 480/1000
2023-10-24 09:50:59.314 
Epoch 480/1000 
	 loss: 28.8364, MinusLogProbMetric: 28.8364, val_loss: 29.1345, val_MinusLogProbMetric: 29.1345

Epoch 480: val_loss did not improve from 28.97717
196/196 - 41s - loss: 28.8364 - MinusLogProbMetric: 28.8364 - val_loss: 29.1345 - val_MinusLogProbMetric: 29.1345 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 481/1000
2023-10-24 09:51:39.530 
Epoch 481/1000 
	 loss: 28.8036, MinusLogProbMetric: 28.8036, val_loss: 29.0669, val_MinusLogProbMetric: 29.0669

Epoch 481: val_loss did not improve from 28.97717
196/196 - 40s - loss: 28.8036 - MinusLogProbMetric: 28.8036 - val_loss: 29.0669 - val_MinusLogProbMetric: 29.0669 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 482/1000
2023-10-24 09:52:18.328 
Epoch 482/1000 
	 loss: 28.7643, MinusLogProbMetric: 28.7643, val_loss: 28.9728, val_MinusLogProbMetric: 28.9728

Epoch 482: val_loss improved from 28.97717 to 28.97284, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 39s - loss: 28.7643 - MinusLogProbMetric: 28.7643 - val_loss: 28.9728 - val_MinusLogProbMetric: 28.9728 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 483/1000
2023-10-24 09:52:59.838 
Epoch 483/1000 
	 loss: 28.7934, MinusLogProbMetric: 28.7934, val_loss: 29.6866, val_MinusLogProbMetric: 29.6866

Epoch 483: val_loss did not improve from 28.97284
196/196 - 41s - loss: 28.7934 - MinusLogProbMetric: 28.7934 - val_loss: 29.6866 - val_MinusLogProbMetric: 29.6866 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 484/1000
2023-10-24 09:53:40.991 
Epoch 484/1000 
	 loss: 28.7825, MinusLogProbMetric: 28.7825, val_loss: 29.1181, val_MinusLogProbMetric: 29.1181

Epoch 484: val_loss did not improve from 28.97284
196/196 - 41s - loss: 28.7825 - MinusLogProbMetric: 28.7825 - val_loss: 29.1181 - val_MinusLogProbMetric: 29.1181 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 485/1000
2023-10-24 09:54:20.773 
Epoch 485/1000 
	 loss: 28.7896, MinusLogProbMetric: 28.7896, val_loss: 29.6147, val_MinusLogProbMetric: 29.6147

Epoch 485: val_loss did not improve from 28.97284
196/196 - 40s - loss: 28.7896 - MinusLogProbMetric: 28.7896 - val_loss: 29.6147 - val_MinusLogProbMetric: 29.6147 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 486/1000
2023-10-24 09:54:59.268 
Epoch 486/1000 
	 loss: 28.8187, MinusLogProbMetric: 28.8187, val_loss: 29.4584, val_MinusLogProbMetric: 29.4584

Epoch 486: val_loss did not improve from 28.97284
196/196 - 38s - loss: 28.8187 - MinusLogProbMetric: 28.8187 - val_loss: 29.4584 - val_MinusLogProbMetric: 29.4584 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 487/1000
2023-10-24 09:55:41.045 
Epoch 487/1000 
	 loss: 28.7737, MinusLogProbMetric: 28.7737, val_loss: 28.9819, val_MinusLogProbMetric: 28.9819

Epoch 487: val_loss did not improve from 28.97284
196/196 - 42s - loss: 28.7737 - MinusLogProbMetric: 28.7737 - val_loss: 28.9819 - val_MinusLogProbMetric: 28.9819 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 488/1000
2023-10-24 09:56:21.907 
Epoch 488/1000 
	 loss: 28.8034, MinusLogProbMetric: 28.8034, val_loss: 29.2175, val_MinusLogProbMetric: 29.2175

Epoch 488: val_loss did not improve from 28.97284
196/196 - 41s - loss: 28.8034 - MinusLogProbMetric: 28.8034 - val_loss: 29.2175 - val_MinusLogProbMetric: 29.2175 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 489/1000
2023-10-24 09:57:04.497 
Epoch 489/1000 
	 loss: 28.7085, MinusLogProbMetric: 28.7085, val_loss: 29.1185, val_MinusLogProbMetric: 29.1185

Epoch 489: val_loss did not improve from 28.97284
196/196 - 43s - loss: 28.7085 - MinusLogProbMetric: 28.7085 - val_loss: 29.1185 - val_MinusLogProbMetric: 29.1185 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 490/1000
2023-10-24 09:57:45.156 
Epoch 490/1000 
	 loss: 28.7729, MinusLogProbMetric: 28.7729, val_loss: 29.2419, val_MinusLogProbMetric: 29.2419

Epoch 490: val_loss did not improve from 28.97284
196/196 - 41s - loss: 28.7729 - MinusLogProbMetric: 28.7729 - val_loss: 29.2419 - val_MinusLogProbMetric: 29.2419 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 491/1000
2023-10-24 09:58:25.815 
Epoch 491/1000 
	 loss: 28.8263, MinusLogProbMetric: 28.8263, val_loss: 29.1990, val_MinusLogProbMetric: 29.1990

Epoch 491: val_loss did not improve from 28.97284
196/196 - 41s - loss: 28.8263 - MinusLogProbMetric: 28.8263 - val_loss: 29.1990 - val_MinusLogProbMetric: 29.1990 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 492/1000
2023-10-24 09:59:05.667 
Epoch 492/1000 
	 loss: 28.7881, MinusLogProbMetric: 28.7881, val_loss: 29.0903, val_MinusLogProbMetric: 29.0903

Epoch 492: val_loss did not improve from 28.97284
196/196 - 40s - loss: 28.7881 - MinusLogProbMetric: 28.7881 - val_loss: 29.0903 - val_MinusLogProbMetric: 29.0903 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 493/1000
2023-10-24 09:59:47.280 
Epoch 493/1000 
	 loss: 28.7403, MinusLogProbMetric: 28.7403, val_loss: 29.3098, val_MinusLogProbMetric: 29.3098

Epoch 493: val_loss did not improve from 28.97284
196/196 - 42s - loss: 28.7403 - MinusLogProbMetric: 28.7403 - val_loss: 29.3098 - val_MinusLogProbMetric: 29.3098 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 494/1000
2023-10-24 10:00:25.710 
Epoch 494/1000 
	 loss: 28.8885, MinusLogProbMetric: 28.8885, val_loss: 29.1065, val_MinusLogProbMetric: 29.1065

Epoch 494: val_loss did not improve from 28.97284
196/196 - 38s - loss: 28.8885 - MinusLogProbMetric: 28.8885 - val_loss: 29.1065 - val_MinusLogProbMetric: 29.1065 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 495/1000
2023-10-24 10:01:06.347 
Epoch 495/1000 
	 loss: 28.8351, MinusLogProbMetric: 28.8351, val_loss: 30.4405, val_MinusLogProbMetric: 30.4405

Epoch 495: val_loss did not improve from 28.97284
196/196 - 41s - loss: 28.8351 - MinusLogProbMetric: 28.8351 - val_loss: 30.4405 - val_MinusLogProbMetric: 30.4405 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 496/1000
2023-10-24 10:01:44.852 
Epoch 496/1000 
	 loss: 28.9068, MinusLogProbMetric: 28.9068, val_loss: 29.3091, val_MinusLogProbMetric: 29.3091

Epoch 496: val_loss did not improve from 28.97284
196/196 - 39s - loss: 28.9068 - MinusLogProbMetric: 28.9068 - val_loss: 29.3091 - val_MinusLogProbMetric: 29.3091 - lr: 3.3333e-04 - 39s/epoch - 196ms/step
Epoch 497/1000
2023-10-24 10:02:23.463 
Epoch 497/1000 
	 loss: 28.7749, MinusLogProbMetric: 28.7749, val_loss: 29.5001, val_MinusLogProbMetric: 29.5001

Epoch 497: val_loss did not improve from 28.97284
196/196 - 39s - loss: 28.7749 - MinusLogProbMetric: 28.7749 - val_loss: 29.5001 - val_MinusLogProbMetric: 29.5001 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 498/1000
2023-10-24 10:03:01.076 
Epoch 498/1000 
	 loss: 28.8064, MinusLogProbMetric: 28.8064, val_loss: 29.4234, val_MinusLogProbMetric: 29.4234

Epoch 498: val_loss did not improve from 28.97284
196/196 - 38s - loss: 28.8064 - MinusLogProbMetric: 28.8064 - val_loss: 29.4234 - val_MinusLogProbMetric: 29.4234 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 499/1000
2023-10-24 10:03:39.385 
Epoch 499/1000 
	 loss: 28.7113, MinusLogProbMetric: 28.7113, val_loss: 29.3247, val_MinusLogProbMetric: 29.3247

Epoch 499: val_loss did not improve from 28.97284
196/196 - 38s - loss: 28.7113 - MinusLogProbMetric: 28.7113 - val_loss: 29.3247 - val_MinusLogProbMetric: 29.3247 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 500/1000
2023-10-24 10:04:18.750 
Epoch 500/1000 
	 loss: 28.7967, MinusLogProbMetric: 28.7967, val_loss: 29.0179, val_MinusLogProbMetric: 29.0179

Epoch 500: val_loss did not improve from 28.97284
196/196 - 39s - loss: 28.7967 - MinusLogProbMetric: 28.7967 - val_loss: 29.0179 - val_MinusLogProbMetric: 29.0179 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 501/1000
2023-10-24 10:05:00.652 
Epoch 501/1000 
	 loss: 28.7186, MinusLogProbMetric: 28.7186, val_loss: 29.2840, val_MinusLogProbMetric: 29.2840

Epoch 501: val_loss did not improve from 28.97284
196/196 - 42s - loss: 28.7186 - MinusLogProbMetric: 28.7186 - val_loss: 29.2840 - val_MinusLogProbMetric: 29.2840 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 502/1000
2023-10-24 10:05:43.081 
Epoch 502/1000 
	 loss: 28.8539, MinusLogProbMetric: 28.8539, val_loss: 29.1613, val_MinusLogProbMetric: 29.1613

Epoch 502: val_loss did not improve from 28.97284
196/196 - 42s - loss: 28.8539 - MinusLogProbMetric: 28.8539 - val_loss: 29.1613 - val_MinusLogProbMetric: 29.1613 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 503/1000
2023-10-24 10:06:27.321 
Epoch 503/1000 
	 loss: 28.7710, MinusLogProbMetric: 28.7710, val_loss: 29.1898, val_MinusLogProbMetric: 29.1898

Epoch 503: val_loss did not improve from 28.97284
196/196 - 44s - loss: 28.7710 - MinusLogProbMetric: 28.7710 - val_loss: 29.1898 - val_MinusLogProbMetric: 29.1898 - lr: 3.3333e-04 - 44s/epoch - 226ms/step
Epoch 504/1000
2023-10-24 10:07:10.847 
Epoch 504/1000 
	 loss: 28.7413, MinusLogProbMetric: 28.7413, val_loss: 29.2538, val_MinusLogProbMetric: 29.2538

Epoch 504: val_loss did not improve from 28.97284
196/196 - 44s - loss: 28.7413 - MinusLogProbMetric: 28.7413 - val_loss: 29.2538 - val_MinusLogProbMetric: 29.2538 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 505/1000
2023-10-24 10:07:54.208 
Epoch 505/1000 
	 loss: 28.7255, MinusLogProbMetric: 28.7255, val_loss: 29.5943, val_MinusLogProbMetric: 29.5943

Epoch 505: val_loss did not improve from 28.97284
196/196 - 43s - loss: 28.7255 - MinusLogProbMetric: 28.7255 - val_loss: 29.5943 - val_MinusLogProbMetric: 29.5943 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 506/1000
2023-10-24 10:08:37.163 
Epoch 506/1000 
	 loss: 28.8130, MinusLogProbMetric: 28.8130, val_loss: 29.2945, val_MinusLogProbMetric: 29.2945

Epoch 506: val_loss did not improve from 28.97284
196/196 - 43s - loss: 28.8130 - MinusLogProbMetric: 28.8130 - val_loss: 29.2945 - val_MinusLogProbMetric: 29.2945 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 507/1000
2023-10-24 10:09:20.976 
Epoch 507/1000 
	 loss: 28.7325, MinusLogProbMetric: 28.7325, val_loss: 29.5472, val_MinusLogProbMetric: 29.5472

Epoch 507: val_loss did not improve from 28.97284
196/196 - 44s - loss: 28.7325 - MinusLogProbMetric: 28.7325 - val_loss: 29.5472 - val_MinusLogProbMetric: 29.5472 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 508/1000
2023-10-24 10:10:03.104 
Epoch 508/1000 
	 loss: 28.8453, MinusLogProbMetric: 28.8453, val_loss: 29.1698, val_MinusLogProbMetric: 29.1698

Epoch 508: val_loss did not improve from 28.97284
196/196 - 42s - loss: 28.8453 - MinusLogProbMetric: 28.8453 - val_loss: 29.1698 - val_MinusLogProbMetric: 29.1698 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 509/1000
2023-10-24 10:10:44.659 
Epoch 509/1000 
	 loss: 28.7538, MinusLogProbMetric: 28.7538, val_loss: 29.1631, val_MinusLogProbMetric: 29.1631

Epoch 509: val_loss did not improve from 28.97284
196/196 - 42s - loss: 28.7538 - MinusLogProbMetric: 28.7538 - val_loss: 29.1631 - val_MinusLogProbMetric: 29.1631 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 510/1000
2023-10-24 10:11:26.537 
Epoch 510/1000 
	 loss: 28.7852, MinusLogProbMetric: 28.7852, val_loss: 29.3734, val_MinusLogProbMetric: 29.3734

Epoch 510: val_loss did not improve from 28.97284
196/196 - 42s - loss: 28.7852 - MinusLogProbMetric: 28.7852 - val_loss: 29.3734 - val_MinusLogProbMetric: 29.3734 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 511/1000
2023-10-24 10:12:07.157 
Epoch 511/1000 
	 loss: 28.7048, MinusLogProbMetric: 28.7048, val_loss: 29.0640, val_MinusLogProbMetric: 29.0640

Epoch 511: val_loss did not improve from 28.97284
196/196 - 41s - loss: 28.7048 - MinusLogProbMetric: 28.7048 - val_loss: 29.0640 - val_MinusLogProbMetric: 29.0640 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 512/1000
2023-10-24 10:12:48.639 
Epoch 512/1000 
	 loss: 28.8275, MinusLogProbMetric: 28.8275, val_loss: 29.4753, val_MinusLogProbMetric: 29.4753

Epoch 512: val_loss did not improve from 28.97284
196/196 - 41s - loss: 28.8275 - MinusLogProbMetric: 28.8275 - val_loss: 29.4753 - val_MinusLogProbMetric: 29.4753 - lr: 3.3333e-04 - 41s/epoch - 212ms/step
Epoch 513/1000
2023-10-24 10:13:30.514 
Epoch 513/1000 
	 loss: 28.7421, MinusLogProbMetric: 28.7421, val_loss: 29.0708, val_MinusLogProbMetric: 29.0708

Epoch 513: val_loss did not improve from 28.97284
196/196 - 42s - loss: 28.7421 - MinusLogProbMetric: 28.7421 - val_loss: 29.0708 - val_MinusLogProbMetric: 29.0708 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 514/1000
2023-10-24 10:14:12.038 
Epoch 514/1000 
	 loss: 28.7761, MinusLogProbMetric: 28.7761, val_loss: 29.0730, val_MinusLogProbMetric: 29.0730

Epoch 514: val_loss did not improve from 28.97284
196/196 - 42s - loss: 28.7761 - MinusLogProbMetric: 28.7761 - val_loss: 29.0730 - val_MinusLogProbMetric: 29.0730 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 515/1000
2023-10-24 10:14:53.400 
Epoch 515/1000 
	 loss: 28.6825, MinusLogProbMetric: 28.6825, val_loss: 29.2032, val_MinusLogProbMetric: 29.2032

Epoch 515: val_loss did not improve from 28.97284
196/196 - 41s - loss: 28.6825 - MinusLogProbMetric: 28.6825 - val_loss: 29.2032 - val_MinusLogProbMetric: 29.2032 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 516/1000
2023-10-24 10:15:35.723 
Epoch 516/1000 
	 loss: 28.8128, MinusLogProbMetric: 28.8128, val_loss: 29.1012, val_MinusLogProbMetric: 29.1012

Epoch 516: val_loss did not improve from 28.97284
196/196 - 42s - loss: 28.8128 - MinusLogProbMetric: 28.8128 - val_loss: 29.1012 - val_MinusLogProbMetric: 29.1012 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 517/1000
2023-10-24 10:16:17.858 
Epoch 517/1000 
	 loss: 28.7487, MinusLogProbMetric: 28.7487, val_loss: 29.0672, val_MinusLogProbMetric: 29.0672

Epoch 517: val_loss did not improve from 28.97284
196/196 - 42s - loss: 28.7487 - MinusLogProbMetric: 28.7487 - val_loss: 29.0672 - val_MinusLogProbMetric: 29.0672 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 518/1000
2023-10-24 10:16:59.352 
Epoch 518/1000 
	 loss: 28.7637, MinusLogProbMetric: 28.7637, val_loss: 29.0695, val_MinusLogProbMetric: 29.0695

Epoch 518: val_loss did not improve from 28.97284
196/196 - 41s - loss: 28.7637 - MinusLogProbMetric: 28.7637 - val_loss: 29.0695 - val_MinusLogProbMetric: 29.0695 - lr: 3.3333e-04 - 41s/epoch - 212ms/step
Epoch 519/1000
2023-10-24 10:17:41.208 
Epoch 519/1000 
	 loss: 28.7401, MinusLogProbMetric: 28.7401, val_loss: 29.2583, val_MinusLogProbMetric: 29.2583

Epoch 519: val_loss did not improve from 28.97284
196/196 - 42s - loss: 28.7401 - MinusLogProbMetric: 28.7401 - val_loss: 29.2583 - val_MinusLogProbMetric: 29.2583 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 520/1000
2023-10-24 10:18:23.651 
Epoch 520/1000 
	 loss: 28.8208, MinusLogProbMetric: 28.8208, val_loss: 29.7560, val_MinusLogProbMetric: 29.7560

Epoch 520: val_loss did not improve from 28.97284
196/196 - 42s - loss: 28.8208 - MinusLogProbMetric: 28.8208 - val_loss: 29.7560 - val_MinusLogProbMetric: 29.7560 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 521/1000
2023-10-24 10:19:05.299 
Epoch 521/1000 
	 loss: 28.7997, MinusLogProbMetric: 28.7997, val_loss: 29.9319, val_MinusLogProbMetric: 29.9319

Epoch 521: val_loss did not improve from 28.97284
196/196 - 42s - loss: 28.7997 - MinusLogProbMetric: 28.7997 - val_loss: 29.9319 - val_MinusLogProbMetric: 29.9319 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 522/1000
2023-10-24 10:19:47.790 
Epoch 522/1000 
	 loss: 28.7305, MinusLogProbMetric: 28.7305, val_loss: 29.0392, val_MinusLogProbMetric: 29.0392

Epoch 522: val_loss did not improve from 28.97284
196/196 - 42s - loss: 28.7305 - MinusLogProbMetric: 28.7305 - val_loss: 29.0392 - val_MinusLogProbMetric: 29.0392 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 523/1000
2023-10-24 10:20:30.199 
Epoch 523/1000 
	 loss: 28.6795, MinusLogProbMetric: 28.6795, val_loss: 29.6751, val_MinusLogProbMetric: 29.6751

Epoch 523: val_loss did not improve from 28.97284
196/196 - 42s - loss: 28.6795 - MinusLogProbMetric: 28.6795 - val_loss: 29.6751 - val_MinusLogProbMetric: 29.6751 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 524/1000
2023-10-24 10:21:12.114 
Epoch 524/1000 
	 loss: 28.7419, MinusLogProbMetric: 28.7419, val_loss: 29.2523, val_MinusLogProbMetric: 29.2523

Epoch 524: val_loss did not improve from 28.97284
196/196 - 42s - loss: 28.7419 - MinusLogProbMetric: 28.7419 - val_loss: 29.2523 - val_MinusLogProbMetric: 29.2523 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 525/1000
2023-10-24 10:21:54.057 
Epoch 525/1000 
	 loss: 28.7567, MinusLogProbMetric: 28.7567, val_loss: 29.2658, val_MinusLogProbMetric: 29.2658

Epoch 525: val_loss did not improve from 28.97284
196/196 - 42s - loss: 28.7567 - MinusLogProbMetric: 28.7567 - val_loss: 29.2658 - val_MinusLogProbMetric: 29.2658 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 526/1000
2023-10-24 10:22:35.730 
Epoch 526/1000 
	 loss: 28.6365, MinusLogProbMetric: 28.6365, val_loss: 29.9282, val_MinusLogProbMetric: 29.9282

Epoch 526: val_loss did not improve from 28.97284
196/196 - 42s - loss: 28.6365 - MinusLogProbMetric: 28.6365 - val_loss: 29.9282 - val_MinusLogProbMetric: 29.9282 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 527/1000
2023-10-24 10:23:15.795 
Epoch 527/1000 
	 loss: 28.7996, MinusLogProbMetric: 28.7996, val_loss: 28.9185, val_MinusLogProbMetric: 28.9185

Epoch 527: val_loss improved from 28.97284 to 28.91854, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 41s - loss: 28.7996 - MinusLogProbMetric: 28.7996 - val_loss: 28.9185 - val_MinusLogProbMetric: 28.9185 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 528/1000
2023-10-24 10:23:57.194 
Epoch 528/1000 
	 loss: 28.6922, MinusLogProbMetric: 28.6922, val_loss: 29.3176, val_MinusLogProbMetric: 29.3176

Epoch 528: val_loss did not improve from 28.91854
196/196 - 41s - loss: 28.6922 - MinusLogProbMetric: 28.6922 - val_loss: 29.3176 - val_MinusLogProbMetric: 29.3176 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 529/1000
2023-10-24 10:24:39.004 
Epoch 529/1000 
	 loss: 28.7309, MinusLogProbMetric: 28.7309, val_loss: 28.9880, val_MinusLogProbMetric: 28.9880

Epoch 529: val_loss did not improve from 28.91854
196/196 - 42s - loss: 28.7309 - MinusLogProbMetric: 28.7309 - val_loss: 28.9880 - val_MinusLogProbMetric: 28.9880 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 530/1000
2023-10-24 10:25:20.106 
Epoch 530/1000 
	 loss: 28.7061, MinusLogProbMetric: 28.7061, val_loss: 29.0462, val_MinusLogProbMetric: 29.0462

Epoch 530: val_loss did not improve from 28.91854
196/196 - 41s - loss: 28.7061 - MinusLogProbMetric: 28.7061 - val_loss: 29.0462 - val_MinusLogProbMetric: 29.0462 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 531/1000
2023-10-24 10:26:02.098 
Epoch 531/1000 
	 loss: 28.7405, MinusLogProbMetric: 28.7405, val_loss: 29.0943, val_MinusLogProbMetric: 29.0943

Epoch 531: val_loss did not improve from 28.91854
196/196 - 42s - loss: 28.7405 - MinusLogProbMetric: 28.7405 - val_loss: 29.0943 - val_MinusLogProbMetric: 29.0943 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 532/1000
2023-10-24 10:26:44.128 
Epoch 532/1000 
	 loss: 28.6310, MinusLogProbMetric: 28.6310, val_loss: 28.8732, val_MinusLogProbMetric: 28.8732

Epoch 532: val_loss improved from 28.91854 to 28.87322, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 43s - loss: 28.6310 - MinusLogProbMetric: 28.6310 - val_loss: 28.8732 - val_MinusLogProbMetric: 28.8732 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 533/1000
2023-10-24 10:27:26.170 
Epoch 533/1000 
	 loss: 28.7247, MinusLogProbMetric: 28.7247, val_loss: 28.9616, val_MinusLogProbMetric: 28.9616

Epoch 533: val_loss did not improve from 28.87322
196/196 - 41s - loss: 28.7247 - MinusLogProbMetric: 28.7247 - val_loss: 28.9616 - val_MinusLogProbMetric: 28.9616 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 534/1000
2023-10-24 10:28:08.052 
Epoch 534/1000 
	 loss: 28.6880, MinusLogProbMetric: 28.6880, val_loss: 29.0358, val_MinusLogProbMetric: 29.0358

Epoch 534: val_loss did not improve from 28.87322
196/196 - 42s - loss: 28.6880 - MinusLogProbMetric: 28.6880 - val_loss: 29.0358 - val_MinusLogProbMetric: 29.0358 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 535/1000
2023-10-24 10:28:50.151 
Epoch 535/1000 
	 loss: 28.7345, MinusLogProbMetric: 28.7345, val_loss: 29.2664, val_MinusLogProbMetric: 29.2664

Epoch 535: val_loss did not improve from 28.87322
196/196 - 42s - loss: 28.7345 - MinusLogProbMetric: 28.7345 - val_loss: 29.2664 - val_MinusLogProbMetric: 29.2664 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 536/1000
2023-10-24 10:29:31.855 
Epoch 536/1000 
	 loss: 28.7559, MinusLogProbMetric: 28.7559, val_loss: 28.9680, val_MinusLogProbMetric: 28.9680

Epoch 536: val_loss did not improve from 28.87322
196/196 - 42s - loss: 28.7559 - MinusLogProbMetric: 28.7559 - val_loss: 28.9680 - val_MinusLogProbMetric: 28.9680 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 537/1000
2023-10-24 10:30:14.146 
Epoch 537/1000 
	 loss: 28.7202, MinusLogProbMetric: 28.7202, val_loss: 28.9797, val_MinusLogProbMetric: 28.9797

Epoch 537: val_loss did not improve from 28.87322
196/196 - 42s - loss: 28.7202 - MinusLogProbMetric: 28.7202 - val_loss: 28.9797 - val_MinusLogProbMetric: 28.9797 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 538/1000
2023-10-24 10:30:56.151 
Epoch 538/1000 
	 loss: 28.6031, MinusLogProbMetric: 28.6031, val_loss: 29.1435, val_MinusLogProbMetric: 29.1435

Epoch 538: val_loss did not improve from 28.87322
196/196 - 42s - loss: 28.6031 - MinusLogProbMetric: 28.6031 - val_loss: 29.1435 - val_MinusLogProbMetric: 29.1435 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 539/1000
2023-10-24 10:31:36.182 
Epoch 539/1000 
	 loss: 28.6016, MinusLogProbMetric: 28.6016, val_loss: 29.1350, val_MinusLogProbMetric: 29.1350

Epoch 539: val_loss did not improve from 28.87322
196/196 - 40s - loss: 28.6016 - MinusLogProbMetric: 28.6016 - val_loss: 29.1350 - val_MinusLogProbMetric: 29.1350 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 540/1000
2023-10-24 10:32:17.978 
Epoch 540/1000 
	 loss: 28.6471, MinusLogProbMetric: 28.6471, val_loss: 29.3067, val_MinusLogProbMetric: 29.3067

Epoch 540: val_loss did not improve from 28.87322
196/196 - 42s - loss: 28.6471 - MinusLogProbMetric: 28.6471 - val_loss: 29.3067 - val_MinusLogProbMetric: 29.3067 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 541/1000
2023-10-24 10:33:00.036 
Epoch 541/1000 
	 loss: 28.6331, MinusLogProbMetric: 28.6331, val_loss: 29.3683, val_MinusLogProbMetric: 29.3683

Epoch 541: val_loss did not improve from 28.87322
196/196 - 42s - loss: 28.6331 - MinusLogProbMetric: 28.6331 - val_loss: 29.3683 - val_MinusLogProbMetric: 29.3683 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 542/1000
2023-10-24 10:33:41.242 
Epoch 542/1000 
	 loss: 28.6469, MinusLogProbMetric: 28.6469, val_loss: 29.0542, val_MinusLogProbMetric: 29.0542

Epoch 542: val_loss did not improve from 28.87322
196/196 - 41s - loss: 28.6469 - MinusLogProbMetric: 28.6469 - val_loss: 29.0542 - val_MinusLogProbMetric: 29.0542 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 543/1000
2023-10-24 10:34:21.558 
Epoch 543/1000 
	 loss: 28.7318, MinusLogProbMetric: 28.7318, val_loss: 29.2233, val_MinusLogProbMetric: 29.2233

Epoch 543: val_loss did not improve from 28.87322
196/196 - 40s - loss: 28.7318 - MinusLogProbMetric: 28.7318 - val_loss: 29.2233 - val_MinusLogProbMetric: 29.2233 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 544/1000
2023-10-24 10:35:03.694 
Epoch 544/1000 
	 loss: 28.6276, MinusLogProbMetric: 28.6276, val_loss: 28.7961, val_MinusLogProbMetric: 28.7961

Epoch 544: val_loss improved from 28.87322 to 28.79614, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 43s - loss: 28.6276 - MinusLogProbMetric: 28.6276 - val_loss: 28.7961 - val_MinusLogProbMetric: 28.7961 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 545/1000
2023-10-24 10:35:46.924 
Epoch 545/1000 
	 loss: 28.6118, MinusLogProbMetric: 28.6118, val_loss: 29.2173, val_MinusLogProbMetric: 29.2173

Epoch 545: val_loss did not improve from 28.79614
196/196 - 43s - loss: 28.6118 - MinusLogProbMetric: 28.6118 - val_loss: 29.2173 - val_MinusLogProbMetric: 29.2173 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 546/1000
2023-10-24 10:36:29.006 
Epoch 546/1000 
	 loss: 28.6240, MinusLogProbMetric: 28.6240, val_loss: 28.9213, val_MinusLogProbMetric: 28.9213

Epoch 546: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6240 - MinusLogProbMetric: 28.6240 - val_loss: 28.9213 - val_MinusLogProbMetric: 28.9213 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 547/1000
2023-10-24 10:37:11.362 
Epoch 547/1000 
	 loss: 28.6617, MinusLogProbMetric: 28.6617, val_loss: 29.0285, val_MinusLogProbMetric: 29.0285

Epoch 547: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6617 - MinusLogProbMetric: 28.6617 - val_loss: 29.0285 - val_MinusLogProbMetric: 29.0285 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 548/1000
2023-10-24 10:37:52.868 
Epoch 548/1000 
	 loss: 28.6647, MinusLogProbMetric: 28.6647, val_loss: 28.9629, val_MinusLogProbMetric: 28.9629

Epoch 548: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6647 - MinusLogProbMetric: 28.6647 - val_loss: 28.9629 - val_MinusLogProbMetric: 28.9629 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 549/1000
2023-10-24 10:38:34.599 
Epoch 549/1000 
	 loss: 28.7954, MinusLogProbMetric: 28.7954, val_loss: 29.1346, val_MinusLogProbMetric: 29.1346

Epoch 549: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.7954 - MinusLogProbMetric: 28.7954 - val_loss: 29.1346 - val_MinusLogProbMetric: 29.1346 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 550/1000
2023-10-24 10:39:16.050 
Epoch 550/1000 
	 loss: 28.6542, MinusLogProbMetric: 28.6542, val_loss: 29.1299, val_MinusLogProbMetric: 29.1299

Epoch 550: val_loss did not improve from 28.79614
196/196 - 41s - loss: 28.6542 - MinusLogProbMetric: 28.6542 - val_loss: 29.1299 - val_MinusLogProbMetric: 29.1299 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 551/1000
2023-10-24 10:39:52.015 
Epoch 551/1000 
	 loss: 28.7093, MinusLogProbMetric: 28.7093, val_loss: 28.9661, val_MinusLogProbMetric: 28.9661

Epoch 551: val_loss did not improve from 28.79614
196/196 - 36s - loss: 28.7093 - MinusLogProbMetric: 28.7093 - val_loss: 28.9661 - val_MinusLogProbMetric: 28.9661 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 552/1000
2023-10-24 10:40:32.833 
Epoch 552/1000 
	 loss: 28.7523, MinusLogProbMetric: 28.7523, val_loss: 29.0379, val_MinusLogProbMetric: 29.0379

Epoch 552: val_loss did not improve from 28.79614
196/196 - 41s - loss: 28.7523 - MinusLogProbMetric: 28.7523 - val_loss: 29.0379 - val_MinusLogProbMetric: 29.0379 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 553/1000
2023-10-24 10:41:15.210 
Epoch 553/1000 
	 loss: 28.5824, MinusLogProbMetric: 28.5824, val_loss: 29.3220, val_MinusLogProbMetric: 29.3220

Epoch 553: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.5824 - MinusLogProbMetric: 28.5824 - val_loss: 29.3220 - val_MinusLogProbMetric: 29.3220 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 554/1000
2023-10-24 10:41:57.023 
Epoch 554/1000 
	 loss: 28.6158, MinusLogProbMetric: 28.6158, val_loss: 28.9439, val_MinusLogProbMetric: 28.9439

Epoch 554: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6158 - MinusLogProbMetric: 28.6158 - val_loss: 28.9439 - val_MinusLogProbMetric: 28.9439 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 555/1000
2023-10-24 10:42:38.065 
Epoch 555/1000 
	 loss: 28.7368, MinusLogProbMetric: 28.7368, val_loss: 29.3072, val_MinusLogProbMetric: 29.3072

Epoch 555: val_loss did not improve from 28.79614
196/196 - 41s - loss: 28.7368 - MinusLogProbMetric: 28.7368 - val_loss: 29.3072 - val_MinusLogProbMetric: 29.3072 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 556/1000
2023-10-24 10:43:20.330 
Epoch 556/1000 
	 loss: 28.6698, MinusLogProbMetric: 28.6698, val_loss: 29.3326, val_MinusLogProbMetric: 29.3326

Epoch 556: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6698 - MinusLogProbMetric: 28.6698 - val_loss: 29.3326 - val_MinusLogProbMetric: 29.3326 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 557/1000
2023-10-24 10:44:02.064 
Epoch 557/1000 
	 loss: 28.6714, MinusLogProbMetric: 28.6714, val_loss: 28.9798, val_MinusLogProbMetric: 28.9798

Epoch 557: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6714 - MinusLogProbMetric: 28.6714 - val_loss: 28.9798 - val_MinusLogProbMetric: 28.9798 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 558/1000
2023-10-24 10:44:43.315 
Epoch 558/1000 
	 loss: 28.6967, MinusLogProbMetric: 28.6967, val_loss: 29.0869, val_MinusLogProbMetric: 29.0869

Epoch 558: val_loss did not improve from 28.79614
196/196 - 41s - loss: 28.6967 - MinusLogProbMetric: 28.6967 - val_loss: 29.0869 - val_MinusLogProbMetric: 29.0869 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 559/1000
2023-10-24 10:45:25.040 
Epoch 559/1000 
	 loss: 28.7876, MinusLogProbMetric: 28.7876, val_loss: 29.5005, val_MinusLogProbMetric: 29.5005

Epoch 559: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.7876 - MinusLogProbMetric: 28.7876 - val_loss: 29.5005 - val_MinusLogProbMetric: 29.5005 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 560/1000
2023-10-24 10:46:07.138 
Epoch 560/1000 
	 loss: 28.6645, MinusLogProbMetric: 28.6645, val_loss: 29.5396, val_MinusLogProbMetric: 29.5396

Epoch 560: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6645 - MinusLogProbMetric: 28.6645 - val_loss: 29.5396 - val_MinusLogProbMetric: 29.5396 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 561/1000
2023-10-24 10:46:49.543 
Epoch 561/1000 
	 loss: 28.6537, MinusLogProbMetric: 28.6537, val_loss: 28.9966, val_MinusLogProbMetric: 28.9966

Epoch 561: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6537 - MinusLogProbMetric: 28.6537 - val_loss: 28.9966 - val_MinusLogProbMetric: 28.9966 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 562/1000
2023-10-24 10:47:30.726 
Epoch 562/1000 
	 loss: 28.6661, MinusLogProbMetric: 28.6661, val_loss: 28.8538, val_MinusLogProbMetric: 28.8538

Epoch 562: val_loss did not improve from 28.79614
196/196 - 41s - loss: 28.6661 - MinusLogProbMetric: 28.6661 - val_loss: 28.8538 - val_MinusLogProbMetric: 28.8538 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 563/1000
2023-10-24 10:48:12.544 
Epoch 563/1000 
	 loss: 28.6657, MinusLogProbMetric: 28.6657, val_loss: 29.0346, val_MinusLogProbMetric: 29.0346

Epoch 563: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6657 - MinusLogProbMetric: 28.6657 - val_loss: 29.0346 - val_MinusLogProbMetric: 29.0346 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 564/1000
2023-10-24 10:48:54.583 
Epoch 564/1000 
	 loss: 28.5915, MinusLogProbMetric: 28.5915, val_loss: 29.2437, val_MinusLogProbMetric: 29.2437

Epoch 564: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.5915 - MinusLogProbMetric: 28.5915 - val_loss: 29.2437 - val_MinusLogProbMetric: 29.2437 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 565/1000
2023-10-24 10:49:36.754 
Epoch 565/1000 
	 loss: 28.6871, MinusLogProbMetric: 28.6871, val_loss: 29.0141, val_MinusLogProbMetric: 29.0141

Epoch 565: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6871 - MinusLogProbMetric: 28.6871 - val_loss: 29.0141 - val_MinusLogProbMetric: 29.0141 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 566/1000
2023-10-24 10:50:19.015 
Epoch 566/1000 
	 loss: 28.6115, MinusLogProbMetric: 28.6115, val_loss: 29.2514, val_MinusLogProbMetric: 29.2514

Epoch 566: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6115 - MinusLogProbMetric: 28.6115 - val_loss: 29.2514 - val_MinusLogProbMetric: 29.2514 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 567/1000
2023-10-24 10:51:01.276 
Epoch 567/1000 
	 loss: 28.6959, MinusLogProbMetric: 28.6959, val_loss: 29.3739, val_MinusLogProbMetric: 29.3739

Epoch 567: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6959 - MinusLogProbMetric: 28.6959 - val_loss: 29.3739 - val_MinusLogProbMetric: 29.3739 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 568/1000
2023-10-24 10:51:42.339 
Epoch 568/1000 
	 loss: 28.6418, MinusLogProbMetric: 28.6418, val_loss: 29.0843, val_MinusLogProbMetric: 29.0843

Epoch 568: val_loss did not improve from 28.79614
196/196 - 41s - loss: 28.6418 - MinusLogProbMetric: 28.6418 - val_loss: 29.0843 - val_MinusLogProbMetric: 29.0843 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 569/1000
2023-10-24 10:52:24.507 
Epoch 569/1000 
	 loss: 28.6208, MinusLogProbMetric: 28.6208, val_loss: 29.2274, val_MinusLogProbMetric: 29.2274

Epoch 569: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6208 - MinusLogProbMetric: 28.6208 - val_loss: 29.2274 - val_MinusLogProbMetric: 29.2274 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 570/1000
2023-10-24 10:53:06.389 
Epoch 570/1000 
	 loss: 28.6594, MinusLogProbMetric: 28.6594, val_loss: 29.5464, val_MinusLogProbMetric: 29.5464

Epoch 570: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6594 - MinusLogProbMetric: 28.6594 - val_loss: 29.5464 - val_MinusLogProbMetric: 29.5464 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 571/1000
2023-10-24 10:53:48.698 
Epoch 571/1000 
	 loss: 28.6448, MinusLogProbMetric: 28.6448, val_loss: 29.0149, val_MinusLogProbMetric: 29.0149

Epoch 571: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6448 - MinusLogProbMetric: 28.6448 - val_loss: 29.0149 - val_MinusLogProbMetric: 29.0149 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 572/1000
2023-10-24 10:54:30.826 
Epoch 572/1000 
	 loss: 28.6689, MinusLogProbMetric: 28.6689, val_loss: 28.8867, val_MinusLogProbMetric: 28.8867

Epoch 572: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6689 - MinusLogProbMetric: 28.6689 - val_loss: 28.8867 - val_MinusLogProbMetric: 28.8867 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 573/1000
2023-10-24 10:55:12.743 
Epoch 573/1000 
	 loss: 28.5983, MinusLogProbMetric: 28.5983, val_loss: 29.0269, val_MinusLogProbMetric: 29.0269

Epoch 573: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.5983 - MinusLogProbMetric: 28.5983 - val_loss: 29.0269 - val_MinusLogProbMetric: 29.0269 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 574/1000
2023-10-24 10:55:55.557 
Epoch 574/1000 
	 loss: 28.5693, MinusLogProbMetric: 28.5693, val_loss: 29.0543, val_MinusLogProbMetric: 29.0543

Epoch 574: val_loss did not improve from 28.79614
196/196 - 43s - loss: 28.5693 - MinusLogProbMetric: 28.5693 - val_loss: 29.0543 - val_MinusLogProbMetric: 29.0543 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 575/1000
2023-10-24 10:56:36.770 
Epoch 575/1000 
	 loss: 28.5433, MinusLogProbMetric: 28.5433, val_loss: 29.2544, val_MinusLogProbMetric: 29.2544

Epoch 575: val_loss did not improve from 28.79614
196/196 - 41s - loss: 28.5433 - MinusLogProbMetric: 28.5433 - val_loss: 29.2544 - val_MinusLogProbMetric: 29.2544 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 576/1000
2023-10-24 10:57:18.912 
Epoch 576/1000 
	 loss: 28.5795, MinusLogProbMetric: 28.5795, val_loss: 28.9267, val_MinusLogProbMetric: 28.9267

Epoch 576: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.5795 - MinusLogProbMetric: 28.5795 - val_loss: 28.9267 - val_MinusLogProbMetric: 28.9267 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 577/1000
2023-10-24 10:58:00.182 
Epoch 577/1000 
	 loss: 28.6665, MinusLogProbMetric: 28.6665, val_loss: 29.0367, val_MinusLogProbMetric: 29.0367

Epoch 577: val_loss did not improve from 28.79614
196/196 - 41s - loss: 28.6665 - MinusLogProbMetric: 28.6665 - val_loss: 29.0367 - val_MinusLogProbMetric: 29.0367 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 578/1000
2023-10-24 10:58:41.648 
Epoch 578/1000 
	 loss: 28.6282, MinusLogProbMetric: 28.6282, val_loss: 29.1446, val_MinusLogProbMetric: 29.1446

Epoch 578: val_loss did not improve from 28.79614
196/196 - 41s - loss: 28.6282 - MinusLogProbMetric: 28.6282 - val_loss: 29.1446 - val_MinusLogProbMetric: 29.1446 - lr: 3.3333e-04 - 41s/epoch - 212ms/step
Epoch 579/1000
2023-10-24 10:59:23.840 
Epoch 579/1000 
	 loss: 28.7211, MinusLogProbMetric: 28.7211, val_loss: 29.3813, val_MinusLogProbMetric: 29.3813

Epoch 579: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.7211 - MinusLogProbMetric: 28.7211 - val_loss: 29.3813 - val_MinusLogProbMetric: 29.3813 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 580/1000
2023-10-24 11:00:05.660 
Epoch 580/1000 
	 loss: 28.6148, MinusLogProbMetric: 28.6148, val_loss: 28.9685, val_MinusLogProbMetric: 28.9685

Epoch 580: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6148 - MinusLogProbMetric: 28.6148 - val_loss: 28.9685 - val_MinusLogProbMetric: 28.9685 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 581/1000
2023-10-24 11:00:44.566 
Epoch 581/1000 
	 loss: 28.6157, MinusLogProbMetric: 28.6157, val_loss: 28.9670, val_MinusLogProbMetric: 28.9670

Epoch 581: val_loss did not improve from 28.79614
196/196 - 39s - loss: 28.6157 - MinusLogProbMetric: 28.6157 - val_loss: 28.9670 - val_MinusLogProbMetric: 28.9670 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 582/1000
2023-10-24 11:01:26.487 
Epoch 582/1000 
	 loss: 28.6507, MinusLogProbMetric: 28.6507, val_loss: 28.9831, val_MinusLogProbMetric: 28.9831

Epoch 582: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6507 - MinusLogProbMetric: 28.6507 - val_loss: 28.9831 - val_MinusLogProbMetric: 28.9831 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 583/1000
2023-10-24 11:02:08.237 
Epoch 583/1000 
	 loss: 28.7189, MinusLogProbMetric: 28.7189, val_loss: 28.8994, val_MinusLogProbMetric: 28.8994

Epoch 583: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.7189 - MinusLogProbMetric: 28.7189 - val_loss: 28.8994 - val_MinusLogProbMetric: 28.8994 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 584/1000
2023-10-24 11:02:50.452 
Epoch 584/1000 
	 loss: 28.6531, MinusLogProbMetric: 28.6531, val_loss: 29.0603, val_MinusLogProbMetric: 29.0603

Epoch 584: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6531 - MinusLogProbMetric: 28.6531 - val_loss: 29.0603 - val_MinusLogProbMetric: 29.0603 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 585/1000
2023-10-24 11:03:32.128 
Epoch 585/1000 
	 loss: 28.6200, MinusLogProbMetric: 28.6200, val_loss: 29.0722, val_MinusLogProbMetric: 29.0722

Epoch 585: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6200 - MinusLogProbMetric: 28.6200 - val_loss: 29.0722 - val_MinusLogProbMetric: 29.0722 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 586/1000
2023-10-24 11:04:09.964 
Epoch 586/1000 
	 loss: 28.5831, MinusLogProbMetric: 28.5831, val_loss: 29.2456, val_MinusLogProbMetric: 29.2456

Epoch 586: val_loss did not improve from 28.79614
196/196 - 38s - loss: 28.5831 - MinusLogProbMetric: 28.5831 - val_loss: 29.2456 - val_MinusLogProbMetric: 29.2456 - lr: 3.3333e-04 - 38s/epoch - 193ms/step
Epoch 587/1000
2023-10-24 11:04:52.003 
Epoch 587/1000 
	 loss: 28.7445, MinusLogProbMetric: 28.7445, val_loss: 28.8765, val_MinusLogProbMetric: 28.8765

Epoch 587: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.7445 - MinusLogProbMetric: 28.7445 - val_loss: 28.8765 - val_MinusLogProbMetric: 28.8765 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 588/1000
2023-10-24 11:05:33.953 
Epoch 588/1000 
	 loss: 28.5779, MinusLogProbMetric: 28.5779, val_loss: 29.0331, val_MinusLogProbMetric: 29.0331

Epoch 588: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.5779 - MinusLogProbMetric: 28.5779 - val_loss: 29.0331 - val_MinusLogProbMetric: 29.0331 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 589/1000
2023-10-24 11:06:16.382 
Epoch 589/1000 
	 loss: 28.5018, MinusLogProbMetric: 28.5018, val_loss: 28.9877, val_MinusLogProbMetric: 28.9877

Epoch 589: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.5018 - MinusLogProbMetric: 28.5018 - val_loss: 28.9877 - val_MinusLogProbMetric: 28.9877 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 590/1000
2023-10-24 11:06:58.442 
Epoch 590/1000 
	 loss: 28.5985, MinusLogProbMetric: 28.5985, val_loss: 29.2474, val_MinusLogProbMetric: 29.2474

Epoch 590: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.5985 - MinusLogProbMetric: 28.5985 - val_loss: 29.2474 - val_MinusLogProbMetric: 29.2474 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 591/1000
2023-10-24 11:07:40.363 
Epoch 591/1000 
	 loss: 28.6173, MinusLogProbMetric: 28.6173, val_loss: 29.1985, val_MinusLogProbMetric: 29.1985

Epoch 591: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.6173 - MinusLogProbMetric: 28.6173 - val_loss: 29.1985 - val_MinusLogProbMetric: 29.1985 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 592/1000
2023-10-24 11:08:22.212 
Epoch 592/1000 
	 loss: 28.9602, MinusLogProbMetric: 28.9602, val_loss: 29.0602, val_MinusLogProbMetric: 29.0602

Epoch 592: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.9602 - MinusLogProbMetric: 28.9602 - val_loss: 29.0602 - val_MinusLogProbMetric: 29.0602 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 593/1000
2023-10-24 11:09:04.156 
Epoch 593/1000 
	 loss: 28.5596, MinusLogProbMetric: 28.5596, val_loss: 28.9184, val_MinusLogProbMetric: 28.9184

Epoch 593: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.5596 - MinusLogProbMetric: 28.5596 - val_loss: 28.9184 - val_MinusLogProbMetric: 28.9184 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 594/1000
2023-10-24 11:09:46.008 
Epoch 594/1000 
	 loss: 28.5928, MinusLogProbMetric: 28.5928, val_loss: 28.8936, val_MinusLogProbMetric: 28.8936

Epoch 594: val_loss did not improve from 28.79614
196/196 - 42s - loss: 28.5928 - MinusLogProbMetric: 28.5928 - val_loss: 28.8936 - val_MinusLogProbMetric: 28.8936 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 595/1000
2023-10-24 11:10:27.398 
Epoch 595/1000 
	 loss: 28.1957, MinusLogProbMetric: 28.1957, val_loss: 28.6317, val_MinusLogProbMetric: 28.6317

Epoch 595: val_loss improved from 28.79614 to 28.63168, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 28.1957 - MinusLogProbMetric: 28.1957 - val_loss: 28.6317 - val_MinusLogProbMetric: 28.6317 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 596/1000
2023-10-24 11:11:09.979 
Epoch 596/1000 
	 loss: 28.1827, MinusLogProbMetric: 28.1827, val_loss: 28.6733, val_MinusLogProbMetric: 28.6733

Epoch 596: val_loss did not improve from 28.63168
196/196 - 42s - loss: 28.1827 - MinusLogProbMetric: 28.1827 - val_loss: 28.6733 - val_MinusLogProbMetric: 28.6733 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 597/1000
2023-10-24 11:11:51.998 
Epoch 597/1000 
	 loss: 28.1682, MinusLogProbMetric: 28.1682, val_loss: 28.6847, val_MinusLogProbMetric: 28.6847

Epoch 597: val_loss did not improve from 28.63168
196/196 - 42s - loss: 28.1682 - MinusLogProbMetric: 28.1682 - val_loss: 28.6847 - val_MinusLogProbMetric: 28.6847 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 598/1000
2023-10-24 11:12:34.256 
Epoch 598/1000 
	 loss: 28.2256, MinusLogProbMetric: 28.2256, val_loss: 28.7844, val_MinusLogProbMetric: 28.7844

Epoch 598: val_loss did not improve from 28.63168
196/196 - 42s - loss: 28.2256 - MinusLogProbMetric: 28.2256 - val_loss: 28.7844 - val_MinusLogProbMetric: 28.7844 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 599/1000
2023-10-24 11:13:16.247 
Epoch 599/1000 
	 loss: 28.2138, MinusLogProbMetric: 28.2138, val_loss: 28.6206, val_MinusLogProbMetric: 28.6206

Epoch 599: val_loss improved from 28.63168 to 28.62062, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 43s - loss: 28.2138 - MinusLogProbMetric: 28.2138 - val_loss: 28.6206 - val_MinusLogProbMetric: 28.6206 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 600/1000
2023-10-24 11:13:58.752 
Epoch 600/1000 
	 loss: 28.1740, MinusLogProbMetric: 28.1740, val_loss: 28.6188, val_MinusLogProbMetric: 28.6188

Epoch 600: val_loss improved from 28.62062 to 28.61884, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 28.1740 - MinusLogProbMetric: 28.1740 - val_loss: 28.6188 - val_MinusLogProbMetric: 28.6188 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 601/1000
2023-10-24 11:14:41.385 
Epoch 601/1000 
	 loss: 28.1941, MinusLogProbMetric: 28.1941, val_loss: 28.6321, val_MinusLogProbMetric: 28.6321

Epoch 601: val_loss did not improve from 28.61884
196/196 - 42s - loss: 28.1941 - MinusLogProbMetric: 28.1941 - val_loss: 28.6321 - val_MinusLogProbMetric: 28.6321 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 602/1000
2023-10-24 11:15:23.202 
Epoch 602/1000 
	 loss: 28.1749, MinusLogProbMetric: 28.1749, val_loss: 28.7156, val_MinusLogProbMetric: 28.7156

Epoch 602: val_loss did not improve from 28.61884
196/196 - 42s - loss: 28.1749 - MinusLogProbMetric: 28.1749 - val_loss: 28.7156 - val_MinusLogProbMetric: 28.7156 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 603/1000
2023-10-24 11:16:05.277 
Epoch 603/1000 
	 loss: 28.1900, MinusLogProbMetric: 28.1900, val_loss: 28.7431, val_MinusLogProbMetric: 28.7431

Epoch 603: val_loss did not improve from 28.61884
196/196 - 42s - loss: 28.1900 - MinusLogProbMetric: 28.1900 - val_loss: 28.7431 - val_MinusLogProbMetric: 28.7431 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 604/1000
2023-10-24 11:16:47.196 
Epoch 604/1000 
	 loss: 28.1745, MinusLogProbMetric: 28.1745, val_loss: 28.5558, val_MinusLogProbMetric: 28.5558

Epoch 604: val_loss improved from 28.61884 to 28.55577, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 43s - loss: 28.1745 - MinusLogProbMetric: 28.1745 - val_loss: 28.5558 - val_MinusLogProbMetric: 28.5558 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 605/1000
2023-10-24 11:17:26.837 
Epoch 605/1000 
	 loss: 28.1925, MinusLogProbMetric: 28.1925, val_loss: 28.5289, val_MinusLogProbMetric: 28.5289

Epoch 605: val_loss improved from 28.55577 to 28.52892, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 40s - loss: 28.1925 - MinusLogProbMetric: 28.1925 - val_loss: 28.5289 - val_MinusLogProbMetric: 28.5289 - lr: 1.6667e-04 - 40s/epoch - 202ms/step
Epoch 606/1000
2023-10-24 11:18:09.114 
Epoch 606/1000 
	 loss: 28.1712, MinusLogProbMetric: 28.1712, val_loss: 28.6383, val_MinusLogProbMetric: 28.6383

Epoch 606: val_loss did not improve from 28.52892
196/196 - 42s - loss: 28.1712 - MinusLogProbMetric: 28.1712 - val_loss: 28.6383 - val_MinusLogProbMetric: 28.6383 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 607/1000
2023-10-24 11:18:50.767 
Epoch 607/1000 
	 loss: 28.1786, MinusLogProbMetric: 28.1786, val_loss: 28.6358, val_MinusLogProbMetric: 28.6358

Epoch 607: val_loss did not improve from 28.52892
196/196 - 42s - loss: 28.1786 - MinusLogProbMetric: 28.1786 - val_loss: 28.6358 - val_MinusLogProbMetric: 28.6358 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 608/1000
2023-10-24 11:19:32.459 
Epoch 608/1000 
	 loss: 28.1797, MinusLogProbMetric: 28.1797, val_loss: 28.6116, val_MinusLogProbMetric: 28.6116

Epoch 608: val_loss did not improve from 28.52892
196/196 - 42s - loss: 28.1797 - MinusLogProbMetric: 28.1797 - val_loss: 28.6116 - val_MinusLogProbMetric: 28.6116 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 609/1000
2023-10-24 11:20:14.478 
Epoch 609/1000 
	 loss: 28.1798, MinusLogProbMetric: 28.1798, val_loss: 28.6112, val_MinusLogProbMetric: 28.6112

Epoch 609: val_loss did not improve from 28.52892
196/196 - 42s - loss: 28.1798 - MinusLogProbMetric: 28.1798 - val_loss: 28.6112 - val_MinusLogProbMetric: 28.6112 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 610/1000
2023-10-24 11:20:56.315 
Epoch 610/1000 
	 loss: 28.1650, MinusLogProbMetric: 28.1650, val_loss: 28.7148, val_MinusLogProbMetric: 28.7148

Epoch 610: val_loss did not improve from 28.52892
196/196 - 42s - loss: 28.1650 - MinusLogProbMetric: 28.1650 - val_loss: 28.7148 - val_MinusLogProbMetric: 28.7148 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 611/1000
2023-10-24 11:21:38.004 
Epoch 611/1000 
	 loss: 28.1785, MinusLogProbMetric: 28.1785, val_loss: 28.7327, val_MinusLogProbMetric: 28.7327

Epoch 611: val_loss did not improve from 28.52892
196/196 - 42s - loss: 28.1785 - MinusLogProbMetric: 28.1785 - val_loss: 28.7327 - val_MinusLogProbMetric: 28.7327 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 612/1000
2023-10-24 11:22:19.621 
Epoch 612/1000 
	 loss: 28.1775, MinusLogProbMetric: 28.1775, val_loss: 28.5382, val_MinusLogProbMetric: 28.5382

Epoch 612: val_loss did not improve from 28.52892
196/196 - 42s - loss: 28.1775 - MinusLogProbMetric: 28.1775 - val_loss: 28.5382 - val_MinusLogProbMetric: 28.5382 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 613/1000
2023-10-24 11:23:01.875 
Epoch 613/1000 
	 loss: 28.1715, MinusLogProbMetric: 28.1715, val_loss: 28.5210, val_MinusLogProbMetric: 28.5210

Epoch 613: val_loss improved from 28.52892 to 28.52104, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 43s - loss: 28.1715 - MinusLogProbMetric: 28.1715 - val_loss: 28.5210 - val_MinusLogProbMetric: 28.5210 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 614/1000
2023-10-24 11:23:44.599 
Epoch 614/1000 
	 loss: 28.1839, MinusLogProbMetric: 28.1839, val_loss: 28.6454, val_MinusLogProbMetric: 28.6454

Epoch 614: val_loss did not improve from 28.52104
196/196 - 42s - loss: 28.1839 - MinusLogProbMetric: 28.1839 - val_loss: 28.6454 - val_MinusLogProbMetric: 28.6454 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 615/1000
2023-10-24 11:24:26.629 
Epoch 615/1000 
	 loss: 28.1803, MinusLogProbMetric: 28.1803, val_loss: 28.7046, val_MinusLogProbMetric: 28.7046

Epoch 615: val_loss did not improve from 28.52104
196/196 - 42s - loss: 28.1803 - MinusLogProbMetric: 28.1803 - val_loss: 28.7046 - val_MinusLogProbMetric: 28.7046 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 616/1000
2023-10-24 11:25:08.821 
Epoch 616/1000 
	 loss: 28.1820, MinusLogProbMetric: 28.1820, val_loss: 28.4929, val_MinusLogProbMetric: 28.4929

Epoch 616: val_loss improved from 28.52104 to 28.49288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 43s - loss: 28.1820 - MinusLogProbMetric: 28.1820 - val_loss: 28.4929 - val_MinusLogProbMetric: 28.4929 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 617/1000
2023-10-24 11:25:52.159 
Epoch 617/1000 
	 loss: 28.1827, MinusLogProbMetric: 28.1827, val_loss: 29.0271, val_MinusLogProbMetric: 29.0271

Epoch 617: val_loss did not improve from 28.49288
196/196 - 43s - loss: 28.1827 - MinusLogProbMetric: 28.1827 - val_loss: 29.0271 - val_MinusLogProbMetric: 29.0271 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 618/1000
2023-10-24 11:26:34.108 
Epoch 618/1000 
	 loss: 28.2028, MinusLogProbMetric: 28.2028, val_loss: 28.5706, val_MinusLogProbMetric: 28.5706

Epoch 618: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.2028 - MinusLogProbMetric: 28.2028 - val_loss: 28.5706 - val_MinusLogProbMetric: 28.5706 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 619/1000
2023-10-24 11:27:16.078 
Epoch 619/1000 
	 loss: 28.2148, MinusLogProbMetric: 28.2148, val_loss: 28.7362, val_MinusLogProbMetric: 28.7362

Epoch 619: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.2148 - MinusLogProbMetric: 28.2148 - val_loss: 28.7362 - val_MinusLogProbMetric: 28.7362 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 620/1000
2023-10-24 11:27:56.816 
Epoch 620/1000 
	 loss: 28.1675, MinusLogProbMetric: 28.1675, val_loss: 28.7182, val_MinusLogProbMetric: 28.7182

Epoch 620: val_loss did not improve from 28.49288
196/196 - 41s - loss: 28.1675 - MinusLogProbMetric: 28.1675 - val_loss: 28.7182 - val_MinusLogProbMetric: 28.7182 - lr: 1.6667e-04 - 41s/epoch - 208ms/step
Epoch 621/1000
2023-10-24 11:28:38.741 
Epoch 621/1000 
	 loss: 28.1929, MinusLogProbMetric: 28.1929, val_loss: 28.5573, val_MinusLogProbMetric: 28.5573

Epoch 621: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1929 - MinusLogProbMetric: 28.1929 - val_loss: 28.5573 - val_MinusLogProbMetric: 28.5573 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 622/1000
2023-10-24 11:29:20.993 
Epoch 622/1000 
	 loss: 28.1567, MinusLogProbMetric: 28.1567, val_loss: 28.6916, val_MinusLogProbMetric: 28.6916

Epoch 622: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1567 - MinusLogProbMetric: 28.1567 - val_loss: 28.6916 - val_MinusLogProbMetric: 28.6916 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 623/1000
2023-10-24 11:30:03.204 
Epoch 623/1000 
	 loss: 28.1708, MinusLogProbMetric: 28.1708, val_loss: 28.5673, val_MinusLogProbMetric: 28.5673

Epoch 623: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1708 - MinusLogProbMetric: 28.1708 - val_loss: 28.5673 - val_MinusLogProbMetric: 28.5673 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 624/1000
2023-10-24 11:30:44.428 
Epoch 624/1000 
	 loss: 28.1678, MinusLogProbMetric: 28.1678, val_loss: 28.5584, val_MinusLogProbMetric: 28.5584

Epoch 624: val_loss did not improve from 28.49288
196/196 - 41s - loss: 28.1678 - MinusLogProbMetric: 28.1678 - val_loss: 28.5584 - val_MinusLogProbMetric: 28.5584 - lr: 1.6667e-04 - 41s/epoch - 210ms/step
Epoch 625/1000
2023-10-24 11:31:26.427 
Epoch 625/1000 
	 loss: 28.1720, MinusLogProbMetric: 28.1720, val_loss: 28.5673, val_MinusLogProbMetric: 28.5673

Epoch 625: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1720 - MinusLogProbMetric: 28.1720 - val_loss: 28.5673 - val_MinusLogProbMetric: 28.5673 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 626/1000
2023-10-24 11:32:07.862 
Epoch 626/1000 
	 loss: 28.1769, MinusLogProbMetric: 28.1769, val_loss: 28.6460, val_MinusLogProbMetric: 28.6460

Epoch 626: val_loss did not improve from 28.49288
196/196 - 41s - loss: 28.1769 - MinusLogProbMetric: 28.1769 - val_loss: 28.6460 - val_MinusLogProbMetric: 28.6460 - lr: 1.6667e-04 - 41s/epoch - 211ms/step
Epoch 627/1000
2023-10-24 11:32:50.259 
Epoch 627/1000 
	 loss: 28.1557, MinusLogProbMetric: 28.1557, val_loss: 28.5150, val_MinusLogProbMetric: 28.5150

Epoch 627: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1557 - MinusLogProbMetric: 28.1557 - val_loss: 28.5150 - val_MinusLogProbMetric: 28.5150 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 628/1000
2023-10-24 11:33:32.689 
Epoch 628/1000 
	 loss: 28.1393, MinusLogProbMetric: 28.1393, val_loss: 28.5579, val_MinusLogProbMetric: 28.5579

Epoch 628: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1393 - MinusLogProbMetric: 28.1393 - val_loss: 28.5579 - val_MinusLogProbMetric: 28.5579 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 629/1000
2023-10-24 11:34:14.060 
Epoch 629/1000 
	 loss: 28.1643, MinusLogProbMetric: 28.1643, val_loss: 28.8086, val_MinusLogProbMetric: 28.8086

Epoch 629: val_loss did not improve from 28.49288
196/196 - 41s - loss: 28.1643 - MinusLogProbMetric: 28.1643 - val_loss: 28.8086 - val_MinusLogProbMetric: 28.8086 - lr: 1.6667e-04 - 41s/epoch - 211ms/step
Epoch 630/1000
2023-10-24 11:34:56.077 
Epoch 630/1000 
	 loss: 28.1376, MinusLogProbMetric: 28.1376, val_loss: 28.8236, val_MinusLogProbMetric: 28.8236

Epoch 630: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1376 - MinusLogProbMetric: 28.1376 - val_loss: 28.8236 - val_MinusLogProbMetric: 28.8236 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 631/1000
2023-10-24 11:35:38.382 
Epoch 631/1000 
	 loss: 28.1840, MinusLogProbMetric: 28.1840, val_loss: 28.7447, val_MinusLogProbMetric: 28.7447

Epoch 631: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1840 - MinusLogProbMetric: 28.1840 - val_loss: 28.7447 - val_MinusLogProbMetric: 28.7447 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 632/1000
2023-10-24 11:36:20.145 
Epoch 632/1000 
	 loss: 28.1793, MinusLogProbMetric: 28.1793, val_loss: 28.8263, val_MinusLogProbMetric: 28.8263

Epoch 632: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1793 - MinusLogProbMetric: 28.1793 - val_loss: 28.8263 - val_MinusLogProbMetric: 28.8263 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 633/1000
2023-10-24 11:37:02.165 
Epoch 633/1000 
	 loss: 28.1523, MinusLogProbMetric: 28.1523, val_loss: 28.6055, val_MinusLogProbMetric: 28.6055

Epoch 633: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1523 - MinusLogProbMetric: 28.1523 - val_loss: 28.6055 - val_MinusLogProbMetric: 28.6055 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 634/1000
2023-10-24 11:37:44.941 
Epoch 634/1000 
	 loss: 28.1702, MinusLogProbMetric: 28.1702, val_loss: 28.6246, val_MinusLogProbMetric: 28.6246

Epoch 634: val_loss did not improve from 28.49288
196/196 - 43s - loss: 28.1702 - MinusLogProbMetric: 28.1702 - val_loss: 28.6246 - val_MinusLogProbMetric: 28.6246 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 635/1000
2023-10-24 11:38:27.583 
Epoch 635/1000 
	 loss: 28.1581, MinusLogProbMetric: 28.1581, val_loss: 28.5663, val_MinusLogProbMetric: 28.5663

Epoch 635: val_loss did not improve from 28.49288
196/196 - 43s - loss: 28.1581 - MinusLogProbMetric: 28.1581 - val_loss: 28.5663 - val_MinusLogProbMetric: 28.5663 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 636/1000
2023-10-24 11:39:09.630 
Epoch 636/1000 
	 loss: 28.1468, MinusLogProbMetric: 28.1468, val_loss: 28.5243, val_MinusLogProbMetric: 28.5243

Epoch 636: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1468 - MinusLogProbMetric: 28.1468 - val_loss: 28.5243 - val_MinusLogProbMetric: 28.5243 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 637/1000
2023-10-24 11:39:51.127 
Epoch 637/1000 
	 loss: 28.1707, MinusLogProbMetric: 28.1707, val_loss: 28.6113, val_MinusLogProbMetric: 28.6113

Epoch 637: val_loss did not improve from 28.49288
196/196 - 41s - loss: 28.1707 - MinusLogProbMetric: 28.1707 - val_loss: 28.6113 - val_MinusLogProbMetric: 28.6113 - lr: 1.6667e-04 - 41s/epoch - 212ms/step
Epoch 638/1000
2023-10-24 11:40:32.789 
Epoch 638/1000 
	 loss: 28.1655, MinusLogProbMetric: 28.1655, val_loss: 28.6372, val_MinusLogProbMetric: 28.6372

Epoch 638: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1655 - MinusLogProbMetric: 28.1655 - val_loss: 28.6372 - val_MinusLogProbMetric: 28.6372 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 639/1000
2023-10-24 11:41:14.818 
Epoch 639/1000 
	 loss: 28.1502, MinusLogProbMetric: 28.1502, val_loss: 28.6814, val_MinusLogProbMetric: 28.6814

Epoch 639: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1502 - MinusLogProbMetric: 28.1502 - val_loss: 28.6814 - val_MinusLogProbMetric: 28.6814 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 640/1000
2023-10-24 11:41:56.145 
Epoch 640/1000 
	 loss: 28.1395, MinusLogProbMetric: 28.1395, val_loss: 28.5739, val_MinusLogProbMetric: 28.5739

Epoch 640: val_loss did not improve from 28.49288
196/196 - 41s - loss: 28.1395 - MinusLogProbMetric: 28.1395 - val_loss: 28.5739 - val_MinusLogProbMetric: 28.5739 - lr: 1.6667e-04 - 41s/epoch - 211ms/step
Epoch 641/1000
2023-10-24 11:42:37.880 
Epoch 641/1000 
	 loss: 28.1606, MinusLogProbMetric: 28.1606, val_loss: 28.6619, val_MinusLogProbMetric: 28.6619

Epoch 641: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1606 - MinusLogProbMetric: 28.1606 - val_loss: 28.6619 - val_MinusLogProbMetric: 28.6619 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 642/1000
2023-10-24 11:43:19.856 
Epoch 642/1000 
	 loss: 28.1691, MinusLogProbMetric: 28.1691, val_loss: 28.5747, val_MinusLogProbMetric: 28.5747

Epoch 642: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1691 - MinusLogProbMetric: 28.1691 - val_loss: 28.5747 - val_MinusLogProbMetric: 28.5747 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 643/1000
2023-10-24 11:44:02.133 
Epoch 643/1000 
	 loss: 28.1808, MinusLogProbMetric: 28.1808, val_loss: 28.5658, val_MinusLogProbMetric: 28.5658

Epoch 643: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1808 - MinusLogProbMetric: 28.1808 - val_loss: 28.5658 - val_MinusLogProbMetric: 28.5658 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 644/1000
2023-10-24 11:44:42.782 
Epoch 644/1000 
	 loss: 28.1599, MinusLogProbMetric: 28.1599, val_loss: 28.5209, val_MinusLogProbMetric: 28.5209

Epoch 644: val_loss did not improve from 28.49288
196/196 - 41s - loss: 28.1599 - MinusLogProbMetric: 28.1599 - val_loss: 28.5209 - val_MinusLogProbMetric: 28.5209 - lr: 1.6667e-04 - 41s/epoch - 207ms/step
Epoch 645/1000
2023-10-24 11:45:24.808 
Epoch 645/1000 
	 loss: 28.1291, MinusLogProbMetric: 28.1291, val_loss: 28.6179, val_MinusLogProbMetric: 28.6179

Epoch 645: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1291 - MinusLogProbMetric: 28.1291 - val_loss: 28.6179 - val_MinusLogProbMetric: 28.6179 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 646/1000
2023-10-24 11:46:06.590 
Epoch 646/1000 
	 loss: 28.1641, MinusLogProbMetric: 28.1641, val_loss: 28.6934, val_MinusLogProbMetric: 28.6934

Epoch 646: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1641 - MinusLogProbMetric: 28.1641 - val_loss: 28.6934 - val_MinusLogProbMetric: 28.6934 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 647/1000
2023-10-24 11:46:47.992 
Epoch 647/1000 
	 loss: 28.1233, MinusLogProbMetric: 28.1233, val_loss: 28.6152, val_MinusLogProbMetric: 28.6152

Epoch 647: val_loss did not improve from 28.49288
196/196 - 41s - loss: 28.1233 - MinusLogProbMetric: 28.1233 - val_loss: 28.6152 - val_MinusLogProbMetric: 28.6152 - lr: 1.6667e-04 - 41s/epoch - 211ms/step
Epoch 648/1000
2023-10-24 11:47:29.467 
Epoch 648/1000 
	 loss: 28.1410, MinusLogProbMetric: 28.1410, val_loss: 28.5668, val_MinusLogProbMetric: 28.5668

Epoch 648: val_loss did not improve from 28.49288
196/196 - 41s - loss: 28.1410 - MinusLogProbMetric: 28.1410 - val_loss: 28.5668 - val_MinusLogProbMetric: 28.5668 - lr: 1.6667e-04 - 41s/epoch - 212ms/step
Epoch 649/1000
2023-10-24 11:48:11.175 
Epoch 649/1000 
	 loss: 28.1373, MinusLogProbMetric: 28.1373, val_loss: 28.5690, val_MinusLogProbMetric: 28.5690

Epoch 649: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1373 - MinusLogProbMetric: 28.1373 - val_loss: 28.5690 - val_MinusLogProbMetric: 28.5690 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 650/1000
2023-10-24 11:48:53.286 
Epoch 650/1000 
	 loss: 28.1264, MinusLogProbMetric: 28.1264, val_loss: 28.7144, val_MinusLogProbMetric: 28.7144

Epoch 650: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1264 - MinusLogProbMetric: 28.1264 - val_loss: 28.7144 - val_MinusLogProbMetric: 28.7144 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 651/1000
2023-10-24 11:49:34.888 
Epoch 651/1000 
	 loss: 28.1359, MinusLogProbMetric: 28.1359, val_loss: 28.6757, val_MinusLogProbMetric: 28.6757

Epoch 651: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1359 - MinusLogProbMetric: 28.1359 - val_loss: 28.6757 - val_MinusLogProbMetric: 28.6757 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 652/1000
2023-10-24 11:50:17.064 
Epoch 652/1000 
	 loss: 28.1383, MinusLogProbMetric: 28.1383, val_loss: 28.5235, val_MinusLogProbMetric: 28.5235

Epoch 652: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1383 - MinusLogProbMetric: 28.1383 - val_loss: 28.5235 - val_MinusLogProbMetric: 28.5235 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 653/1000
2023-10-24 11:50:59.092 
Epoch 653/1000 
	 loss: 28.1167, MinusLogProbMetric: 28.1167, val_loss: 28.5913, val_MinusLogProbMetric: 28.5913

Epoch 653: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1167 - MinusLogProbMetric: 28.1167 - val_loss: 28.5913 - val_MinusLogProbMetric: 28.5913 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 654/1000
2023-10-24 11:51:38.868 
Epoch 654/1000 
	 loss: 28.1467, MinusLogProbMetric: 28.1467, val_loss: 28.5942, val_MinusLogProbMetric: 28.5942

Epoch 654: val_loss did not improve from 28.49288
196/196 - 40s - loss: 28.1467 - MinusLogProbMetric: 28.1467 - val_loss: 28.5942 - val_MinusLogProbMetric: 28.5942 - lr: 1.6667e-04 - 40s/epoch - 203ms/step
Epoch 655/1000
2023-10-24 11:52:20.149 
Epoch 655/1000 
	 loss: 28.1295, MinusLogProbMetric: 28.1295, val_loss: 28.5731, val_MinusLogProbMetric: 28.5731

Epoch 655: val_loss did not improve from 28.49288
196/196 - 41s - loss: 28.1295 - MinusLogProbMetric: 28.1295 - val_loss: 28.5731 - val_MinusLogProbMetric: 28.5731 - lr: 1.6667e-04 - 41s/epoch - 211ms/step
Epoch 656/1000
2023-10-24 11:53:02.168 
Epoch 656/1000 
	 loss: 28.1420, MinusLogProbMetric: 28.1420, val_loss: 28.5866, val_MinusLogProbMetric: 28.5866

Epoch 656: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1420 - MinusLogProbMetric: 28.1420 - val_loss: 28.5866 - val_MinusLogProbMetric: 28.5866 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 657/1000
2023-10-24 11:53:45.196 
Epoch 657/1000 
	 loss: 28.1342, MinusLogProbMetric: 28.1342, val_loss: 28.6058, val_MinusLogProbMetric: 28.6058

Epoch 657: val_loss did not improve from 28.49288
196/196 - 43s - loss: 28.1342 - MinusLogProbMetric: 28.1342 - val_loss: 28.6058 - val_MinusLogProbMetric: 28.6058 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 658/1000
2023-10-24 11:54:27.106 
Epoch 658/1000 
	 loss: 28.1276, MinusLogProbMetric: 28.1276, val_loss: 28.6595, val_MinusLogProbMetric: 28.6595

Epoch 658: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1276 - MinusLogProbMetric: 28.1276 - val_loss: 28.6595 - val_MinusLogProbMetric: 28.6595 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 659/1000
2023-10-24 11:55:08.704 
Epoch 659/1000 
	 loss: 28.1445, MinusLogProbMetric: 28.1445, val_loss: 28.5911, val_MinusLogProbMetric: 28.5911

Epoch 659: val_loss did not improve from 28.49288
196/196 - 42s - loss: 28.1445 - MinusLogProbMetric: 28.1445 - val_loss: 28.5911 - val_MinusLogProbMetric: 28.5911 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 660/1000
2023-10-24 11:55:49.704 
Epoch 660/1000 
	 loss: 28.1557, MinusLogProbMetric: 28.1557, val_loss: 28.5077, val_MinusLogProbMetric: 28.5077

Epoch 660: val_loss did not improve from 28.49288
196/196 - 41s - loss: 28.1557 - MinusLogProbMetric: 28.1557 - val_loss: 28.5077 - val_MinusLogProbMetric: 28.5077 - lr: 1.6667e-04 - 41s/epoch - 209ms/step
Epoch 661/1000
2023-10-24 11:56:28.356 
Epoch 661/1000 
	 loss: 28.1307, MinusLogProbMetric: 28.1307, val_loss: 28.6301, val_MinusLogProbMetric: 28.6301

Epoch 661: val_loss did not improve from 28.49288
196/196 - 39s - loss: 28.1307 - MinusLogProbMetric: 28.1307 - val_loss: 28.6301 - val_MinusLogProbMetric: 28.6301 - lr: 1.6667e-04 - 39s/epoch - 197ms/step
Epoch 662/1000
2023-10-24 11:57:07.685 
Epoch 662/1000 
	 loss: 28.1186, MinusLogProbMetric: 28.1186, val_loss: 28.6022, val_MinusLogProbMetric: 28.6022

Epoch 662: val_loss did not improve from 28.49288
196/196 - 39s - loss: 28.1186 - MinusLogProbMetric: 28.1186 - val_loss: 28.6022 - val_MinusLogProbMetric: 28.6022 - lr: 1.6667e-04 - 39s/epoch - 201ms/step
Epoch 663/1000
2023-10-24 11:57:47.163 
Epoch 663/1000 
	 loss: 28.1612, MinusLogProbMetric: 28.1612, val_loss: 28.6396, val_MinusLogProbMetric: 28.6396

Epoch 663: val_loss did not improve from 28.49288
196/196 - 39s - loss: 28.1612 - MinusLogProbMetric: 28.1612 - val_loss: 28.6396 - val_MinusLogProbMetric: 28.6396 - lr: 1.6667e-04 - 39s/epoch - 201ms/step
Epoch 664/1000
2023-10-24 11:58:28.044 
Epoch 664/1000 
	 loss: 28.1408, MinusLogProbMetric: 28.1408, val_loss: 28.6620, val_MinusLogProbMetric: 28.6620

Epoch 664: val_loss did not improve from 28.49288
196/196 - 41s - loss: 28.1408 - MinusLogProbMetric: 28.1408 - val_loss: 28.6620 - val_MinusLogProbMetric: 28.6620 - lr: 1.6667e-04 - 41s/epoch - 209ms/step
Epoch 665/1000
2023-10-24 11:59:08.041 
Epoch 665/1000 
	 loss: 28.1543, MinusLogProbMetric: 28.1543, val_loss: 28.5767, val_MinusLogProbMetric: 28.5767

Epoch 665: val_loss did not improve from 28.49288
196/196 - 40s - loss: 28.1543 - MinusLogProbMetric: 28.1543 - val_loss: 28.5767 - val_MinusLogProbMetric: 28.5767 - lr: 1.6667e-04 - 40s/epoch - 204ms/step
Epoch 666/1000
2023-10-24 11:59:46.613 
Epoch 666/1000 
	 loss: 28.1356, MinusLogProbMetric: 28.1356, val_loss: 28.5832, val_MinusLogProbMetric: 28.5832

Epoch 666: val_loss did not improve from 28.49288
196/196 - 39s - loss: 28.1356 - MinusLogProbMetric: 28.1356 - val_loss: 28.5832 - val_MinusLogProbMetric: 28.5832 - lr: 1.6667e-04 - 39s/epoch - 197ms/step
Epoch 667/1000
2023-10-24 12:00:26.356 
Epoch 667/1000 
	 loss: 27.9833, MinusLogProbMetric: 27.9833, val_loss: 28.4220, val_MinusLogProbMetric: 28.4220

Epoch 667: val_loss improved from 28.49288 to 28.42197, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 40s - loss: 27.9833 - MinusLogProbMetric: 27.9833 - val_loss: 28.4220 - val_MinusLogProbMetric: 28.4220 - lr: 8.3333e-05 - 40s/epoch - 206ms/step
Epoch 668/1000
2023-10-24 12:01:04.609 
Epoch 668/1000 
	 loss: 27.9882, MinusLogProbMetric: 27.9882, val_loss: 28.4816, val_MinusLogProbMetric: 28.4816

Epoch 668: val_loss did not improve from 28.42197
196/196 - 38s - loss: 27.9882 - MinusLogProbMetric: 27.9882 - val_loss: 28.4816 - val_MinusLogProbMetric: 28.4816 - lr: 8.3333e-05 - 38s/epoch - 192ms/step
Epoch 669/1000
2023-10-24 12:01:42.590 
Epoch 669/1000 
	 loss: 27.9824, MinusLogProbMetric: 27.9824, val_loss: 28.4703, val_MinusLogProbMetric: 28.4703

Epoch 669: val_loss did not improve from 28.42197
196/196 - 38s - loss: 27.9824 - MinusLogProbMetric: 27.9824 - val_loss: 28.4703 - val_MinusLogProbMetric: 28.4703 - lr: 8.3333e-05 - 38s/epoch - 194ms/step
Epoch 670/1000
2023-10-24 12:02:22.899 
Epoch 670/1000 
	 loss: 27.9900, MinusLogProbMetric: 27.9900, val_loss: 28.4493, val_MinusLogProbMetric: 28.4493

Epoch 670: val_loss did not improve from 28.42197
196/196 - 40s - loss: 27.9900 - MinusLogProbMetric: 27.9900 - val_loss: 28.4493 - val_MinusLogProbMetric: 28.4493 - lr: 8.3333e-05 - 40s/epoch - 206ms/step
Epoch 671/1000
2023-10-24 12:03:03.135 
Epoch 671/1000 
	 loss: 27.9842, MinusLogProbMetric: 27.9842, val_loss: 28.4657, val_MinusLogProbMetric: 28.4657

Epoch 671: val_loss did not improve from 28.42197
196/196 - 40s - loss: 27.9842 - MinusLogProbMetric: 27.9842 - val_loss: 28.4657 - val_MinusLogProbMetric: 28.4657 - lr: 8.3333e-05 - 40s/epoch - 205ms/step
Epoch 672/1000
2023-10-24 12:03:44.498 
Epoch 672/1000 
	 loss: 27.9884, MinusLogProbMetric: 27.9884, val_loss: 28.4148, val_MinusLogProbMetric: 28.4148

Epoch 672: val_loss improved from 28.42197 to 28.41480, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 27.9884 - MinusLogProbMetric: 27.9884 - val_loss: 28.4148 - val_MinusLogProbMetric: 28.4148 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 673/1000
2023-10-24 12:04:26.624 
Epoch 673/1000 
	 loss: 27.9798, MinusLogProbMetric: 27.9798, val_loss: 28.4583, val_MinusLogProbMetric: 28.4583

Epoch 673: val_loss did not improve from 28.41480
196/196 - 41s - loss: 27.9798 - MinusLogProbMetric: 27.9798 - val_loss: 28.4583 - val_MinusLogProbMetric: 28.4583 - lr: 8.3333e-05 - 41s/epoch - 212ms/step
Epoch 674/1000
2023-10-24 12:05:07.070 
Epoch 674/1000 
	 loss: 27.9875, MinusLogProbMetric: 27.9875, val_loss: 28.4324, val_MinusLogProbMetric: 28.4324

Epoch 674: val_loss did not improve from 28.41480
196/196 - 40s - loss: 27.9875 - MinusLogProbMetric: 27.9875 - val_loss: 28.4324 - val_MinusLogProbMetric: 28.4324 - lr: 8.3333e-05 - 40s/epoch - 206ms/step
Epoch 675/1000
2023-10-24 12:05:48.791 
Epoch 675/1000 
	 loss: 27.9928, MinusLogProbMetric: 27.9928, val_loss: 28.4141, val_MinusLogProbMetric: 28.4141

Epoch 675: val_loss improved from 28.41480 to 28.41412, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 27.9928 - MinusLogProbMetric: 27.9928 - val_loss: 28.4141 - val_MinusLogProbMetric: 28.4141 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 676/1000
2023-10-24 12:06:30.687 
Epoch 676/1000 
	 loss: 27.9795, MinusLogProbMetric: 27.9795, val_loss: 28.4287, val_MinusLogProbMetric: 28.4287

Epoch 676: val_loss did not improve from 28.41412
196/196 - 41s - loss: 27.9795 - MinusLogProbMetric: 27.9795 - val_loss: 28.4287 - val_MinusLogProbMetric: 28.4287 - lr: 8.3333e-05 - 41s/epoch - 210ms/step
Epoch 677/1000
2023-10-24 12:07:10.716 
Epoch 677/1000 
	 loss: 27.9909, MinusLogProbMetric: 27.9909, val_loss: 28.4462, val_MinusLogProbMetric: 28.4462

Epoch 677: val_loss did not improve from 28.41412
196/196 - 40s - loss: 27.9909 - MinusLogProbMetric: 27.9909 - val_loss: 28.4462 - val_MinusLogProbMetric: 28.4462 - lr: 8.3333e-05 - 40s/epoch - 204ms/step
Epoch 678/1000
2023-10-24 12:07:51.593 
Epoch 678/1000 
	 loss: 27.9786, MinusLogProbMetric: 27.9786, val_loss: 28.5913, val_MinusLogProbMetric: 28.5913

Epoch 678: val_loss did not improve from 28.41412
196/196 - 41s - loss: 27.9786 - MinusLogProbMetric: 27.9786 - val_loss: 28.5913 - val_MinusLogProbMetric: 28.5913 - lr: 8.3333e-05 - 41s/epoch - 209ms/step
Epoch 679/1000
2023-10-24 12:08:33.406 
Epoch 679/1000 
	 loss: 27.9861, MinusLogProbMetric: 27.9861, val_loss: 28.4726, val_MinusLogProbMetric: 28.4726

Epoch 679: val_loss did not improve from 28.41412
196/196 - 42s - loss: 27.9861 - MinusLogProbMetric: 27.9861 - val_loss: 28.4726 - val_MinusLogProbMetric: 28.4726 - lr: 8.3333e-05 - 42s/epoch - 213ms/step
Epoch 680/1000
2023-10-24 12:09:14.952 
Epoch 680/1000 
	 loss: 27.9816, MinusLogProbMetric: 27.9816, val_loss: 28.4636, val_MinusLogProbMetric: 28.4636

Epoch 680: val_loss did not improve from 28.41412
196/196 - 42s - loss: 27.9816 - MinusLogProbMetric: 27.9816 - val_loss: 28.4636 - val_MinusLogProbMetric: 28.4636 - lr: 8.3333e-05 - 42s/epoch - 212ms/step
Epoch 681/1000
2023-10-24 12:09:57.110 
Epoch 681/1000 
	 loss: 27.9944, MinusLogProbMetric: 27.9944, val_loss: 28.4256, val_MinusLogProbMetric: 28.4256

Epoch 681: val_loss did not improve from 28.41412
196/196 - 42s - loss: 27.9944 - MinusLogProbMetric: 27.9944 - val_loss: 28.4256 - val_MinusLogProbMetric: 28.4256 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 682/1000
2023-10-24 12:10:39.196 
Epoch 682/1000 
	 loss: 27.9915, MinusLogProbMetric: 27.9915, val_loss: 28.4281, val_MinusLogProbMetric: 28.4281

Epoch 682: val_loss did not improve from 28.41412
196/196 - 42s - loss: 27.9915 - MinusLogProbMetric: 27.9915 - val_loss: 28.4281 - val_MinusLogProbMetric: 28.4281 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 683/1000
2023-10-24 12:11:20.931 
Epoch 683/1000 
	 loss: 27.9828, MinusLogProbMetric: 27.9828, val_loss: 28.4241, val_MinusLogProbMetric: 28.4241

Epoch 683: val_loss did not improve from 28.41412
196/196 - 42s - loss: 27.9828 - MinusLogProbMetric: 27.9828 - val_loss: 28.4241 - val_MinusLogProbMetric: 28.4241 - lr: 8.3333e-05 - 42s/epoch - 213ms/step
Epoch 684/1000
2023-10-24 12:12:02.849 
Epoch 684/1000 
	 loss: 27.9790, MinusLogProbMetric: 27.9790, val_loss: 28.4406, val_MinusLogProbMetric: 28.4406

Epoch 684: val_loss did not improve from 28.41412
196/196 - 42s - loss: 27.9790 - MinusLogProbMetric: 27.9790 - val_loss: 28.4406 - val_MinusLogProbMetric: 28.4406 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 685/1000
2023-10-24 12:12:44.335 
Epoch 685/1000 
	 loss: 27.9830, MinusLogProbMetric: 27.9830, val_loss: 28.4892, val_MinusLogProbMetric: 28.4892

Epoch 685: val_loss did not improve from 28.41412
196/196 - 41s - loss: 27.9830 - MinusLogProbMetric: 27.9830 - val_loss: 28.4892 - val_MinusLogProbMetric: 28.4892 - lr: 8.3333e-05 - 41s/epoch - 212ms/step
Epoch 686/1000
2023-10-24 12:13:26.177 
Epoch 686/1000 
	 loss: 27.9761, MinusLogProbMetric: 27.9761, val_loss: 28.3866, val_MinusLogProbMetric: 28.3866

Epoch 686: val_loss improved from 28.41412 to 28.38662, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 27.9761 - MinusLogProbMetric: 27.9761 - val_loss: 28.3866 - val_MinusLogProbMetric: 28.3866 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 687/1000
2023-10-24 12:14:07.773 
Epoch 687/1000 
	 loss: 27.9700, MinusLogProbMetric: 27.9700, val_loss: 28.4736, val_MinusLogProbMetric: 28.4736

Epoch 687: val_loss did not improve from 28.38662
196/196 - 41s - loss: 27.9700 - MinusLogProbMetric: 27.9700 - val_loss: 28.4736 - val_MinusLogProbMetric: 28.4736 - lr: 8.3333e-05 - 41s/epoch - 209ms/step
Epoch 688/1000
2023-10-24 12:14:49.966 
Epoch 688/1000 
	 loss: 27.9798, MinusLogProbMetric: 27.9798, val_loss: 28.4069, val_MinusLogProbMetric: 28.4069

Epoch 688: val_loss did not improve from 28.38662
196/196 - 42s - loss: 27.9798 - MinusLogProbMetric: 27.9798 - val_loss: 28.4069 - val_MinusLogProbMetric: 28.4069 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 689/1000
2023-10-24 12:15:31.657 
Epoch 689/1000 
	 loss: 27.9684, MinusLogProbMetric: 27.9684, val_loss: 28.4422, val_MinusLogProbMetric: 28.4422

Epoch 689: val_loss did not improve from 28.38662
196/196 - 42s - loss: 27.9684 - MinusLogProbMetric: 27.9684 - val_loss: 28.4422 - val_MinusLogProbMetric: 28.4422 - lr: 8.3333e-05 - 42s/epoch - 213ms/step
Epoch 690/1000
2023-10-24 12:16:13.212 
Epoch 690/1000 
	 loss: 27.9706, MinusLogProbMetric: 27.9706, val_loss: 28.5141, val_MinusLogProbMetric: 28.5141

Epoch 690: val_loss did not improve from 28.38662
196/196 - 42s - loss: 27.9706 - MinusLogProbMetric: 27.9706 - val_loss: 28.5141 - val_MinusLogProbMetric: 28.5141 - lr: 8.3333e-05 - 42s/epoch - 212ms/step
Epoch 691/1000
2023-10-24 12:16:54.815 
Epoch 691/1000 
	 loss: 27.9816, MinusLogProbMetric: 27.9816, val_loss: 28.4628, val_MinusLogProbMetric: 28.4628

Epoch 691: val_loss did not improve from 28.38662
196/196 - 42s - loss: 27.9816 - MinusLogProbMetric: 27.9816 - val_loss: 28.4628 - val_MinusLogProbMetric: 28.4628 - lr: 8.3333e-05 - 42s/epoch - 212ms/step
Epoch 692/1000
2023-10-24 12:17:36.498 
Epoch 692/1000 
	 loss: 27.9788, MinusLogProbMetric: 27.9788, val_loss: 28.4490, val_MinusLogProbMetric: 28.4490

Epoch 692: val_loss did not improve from 28.38662
196/196 - 42s - loss: 27.9788 - MinusLogProbMetric: 27.9788 - val_loss: 28.4490 - val_MinusLogProbMetric: 28.4490 - lr: 8.3333e-05 - 42s/epoch - 213ms/step
Epoch 693/1000
2023-10-24 12:18:16.816 
Epoch 693/1000 
	 loss: 27.9711, MinusLogProbMetric: 27.9711, val_loss: 28.4294, val_MinusLogProbMetric: 28.4294

Epoch 693: val_loss did not improve from 28.38662
196/196 - 40s - loss: 27.9711 - MinusLogProbMetric: 27.9711 - val_loss: 28.4294 - val_MinusLogProbMetric: 28.4294 - lr: 8.3333e-05 - 40s/epoch - 206ms/step
Epoch 694/1000
2023-10-24 12:18:54.817 
Epoch 694/1000 
	 loss: 27.9813, MinusLogProbMetric: 27.9813, val_loss: 28.4534, val_MinusLogProbMetric: 28.4534

Epoch 694: val_loss did not improve from 28.38662
196/196 - 38s - loss: 27.9813 - MinusLogProbMetric: 27.9813 - val_loss: 28.4534 - val_MinusLogProbMetric: 28.4534 - lr: 8.3333e-05 - 38s/epoch - 194ms/step
Epoch 695/1000
2023-10-24 12:19:33.392 
Epoch 695/1000 
	 loss: 27.9869, MinusLogProbMetric: 27.9869, val_loss: 28.4198, val_MinusLogProbMetric: 28.4198

Epoch 695: val_loss did not improve from 28.38662
196/196 - 39s - loss: 27.9869 - MinusLogProbMetric: 27.9869 - val_loss: 28.4198 - val_MinusLogProbMetric: 28.4198 - lr: 8.3333e-05 - 39s/epoch - 197ms/step
Epoch 696/1000
2023-10-24 12:20:15.265 
Epoch 696/1000 
	 loss: 27.9699, MinusLogProbMetric: 27.9699, val_loss: 28.4632, val_MinusLogProbMetric: 28.4632

Epoch 696: val_loss did not improve from 28.38662
196/196 - 42s - loss: 27.9699 - MinusLogProbMetric: 27.9699 - val_loss: 28.4632 - val_MinusLogProbMetric: 28.4632 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 697/1000
2023-10-24 12:20:57.155 
Epoch 697/1000 
	 loss: 27.9650, MinusLogProbMetric: 27.9650, val_loss: 28.4035, val_MinusLogProbMetric: 28.4035

Epoch 697: val_loss did not improve from 28.38662
196/196 - 42s - loss: 27.9650 - MinusLogProbMetric: 27.9650 - val_loss: 28.4035 - val_MinusLogProbMetric: 28.4035 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 698/1000
2023-10-24 12:21:38.155 
Epoch 698/1000 
	 loss: 27.9711, MinusLogProbMetric: 27.9711, val_loss: 28.3732, val_MinusLogProbMetric: 28.3732

Epoch 698: val_loss improved from 28.38662 to 28.37318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 27.9711 - MinusLogProbMetric: 27.9711 - val_loss: 28.3732 - val_MinusLogProbMetric: 28.3732 - lr: 8.3333e-05 - 42s/epoch - 213ms/step
Epoch 699/1000
2023-10-24 12:22:18.141 
Epoch 699/1000 
	 loss: 27.9727, MinusLogProbMetric: 27.9727, val_loss: 28.3745, val_MinusLogProbMetric: 28.3745

Epoch 699: val_loss did not improve from 28.37318
196/196 - 39s - loss: 27.9727 - MinusLogProbMetric: 27.9727 - val_loss: 28.3745 - val_MinusLogProbMetric: 28.3745 - lr: 8.3333e-05 - 39s/epoch - 201ms/step
Epoch 700/1000
2023-10-24 12:22:59.746 
Epoch 700/1000 
	 loss: 27.9774, MinusLogProbMetric: 27.9774, val_loss: 28.4685, val_MinusLogProbMetric: 28.4685

Epoch 700: val_loss did not improve from 28.37318
196/196 - 42s - loss: 27.9774 - MinusLogProbMetric: 27.9774 - val_loss: 28.4685 - val_MinusLogProbMetric: 28.4685 - lr: 8.3333e-05 - 42s/epoch - 212ms/step
Epoch 701/1000
2023-10-24 12:23:42.054 
Epoch 701/1000 
	 loss: 27.9871, MinusLogProbMetric: 27.9871, val_loss: 28.4723, val_MinusLogProbMetric: 28.4723

Epoch 701: val_loss did not improve from 28.37318
196/196 - 42s - loss: 27.9871 - MinusLogProbMetric: 27.9871 - val_loss: 28.4723 - val_MinusLogProbMetric: 28.4723 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 702/1000
2023-10-24 12:24:23.293 
Epoch 702/1000 
	 loss: 27.9800, MinusLogProbMetric: 27.9800, val_loss: 28.4006, val_MinusLogProbMetric: 28.4006

Epoch 702: val_loss did not improve from 28.37318
196/196 - 41s - loss: 27.9800 - MinusLogProbMetric: 27.9800 - val_loss: 28.4006 - val_MinusLogProbMetric: 28.4006 - lr: 8.3333e-05 - 41s/epoch - 210ms/step
Epoch 703/1000
2023-10-24 12:25:04.778 
Epoch 703/1000 
	 loss: 27.9696, MinusLogProbMetric: 27.9696, val_loss: 28.4071, val_MinusLogProbMetric: 28.4071

Epoch 703: val_loss did not improve from 28.37318
196/196 - 41s - loss: 27.9696 - MinusLogProbMetric: 27.9696 - val_loss: 28.4071 - val_MinusLogProbMetric: 28.4071 - lr: 8.3333e-05 - 41s/epoch - 212ms/step
Epoch 704/1000
2023-10-24 12:25:46.202 
Epoch 704/1000 
	 loss: 27.9757, MinusLogProbMetric: 27.9757, val_loss: 28.4939, val_MinusLogProbMetric: 28.4939

Epoch 704: val_loss did not improve from 28.37318
196/196 - 41s - loss: 27.9757 - MinusLogProbMetric: 27.9757 - val_loss: 28.4939 - val_MinusLogProbMetric: 28.4939 - lr: 8.3333e-05 - 41s/epoch - 211ms/step
Epoch 705/1000
2023-10-24 12:26:27.407 
Epoch 705/1000 
	 loss: 27.9784, MinusLogProbMetric: 27.9784, val_loss: 28.4408, val_MinusLogProbMetric: 28.4408

Epoch 705: val_loss did not improve from 28.37318
196/196 - 41s - loss: 27.9784 - MinusLogProbMetric: 27.9784 - val_loss: 28.4408 - val_MinusLogProbMetric: 28.4408 - lr: 8.3333e-05 - 41s/epoch - 210ms/step
Epoch 706/1000
2023-10-24 12:27:07.374 
Epoch 706/1000 
	 loss: 27.9686, MinusLogProbMetric: 27.9686, val_loss: 28.4021, val_MinusLogProbMetric: 28.4021

Epoch 706: val_loss did not improve from 28.37318
196/196 - 40s - loss: 27.9686 - MinusLogProbMetric: 27.9686 - val_loss: 28.4021 - val_MinusLogProbMetric: 28.4021 - lr: 8.3333e-05 - 40s/epoch - 204ms/step
Epoch 707/1000
2023-10-24 12:27:45.235 
Epoch 707/1000 
	 loss: 27.9669, MinusLogProbMetric: 27.9669, val_loss: 28.4244, val_MinusLogProbMetric: 28.4244

Epoch 707: val_loss did not improve from 28.37318
196/196 - 38s - loss: 27.9669 - MinusLogProbMetric: 27.9669 - val_loss: 28.4244 - val_MinusLogProbMetric: 28.4244 - lr: 8.3333e-05 - 38s/epoch - 193ms/step
Epoch 708/1000
2023-10-24 12:28:23.664 
Epoch 708/1000 
	 loss: 27.9832, MinusLogProbMetric: 27.9832, val_loss: 28.4320, val_MinusLogProbMetric: 28.4320

Epoch 708: val_loss did not improve from 28.37318
196/196 - 38s - loss: 27.9832 - MinusLogProbMetric: 27.9832 - val_loss: 28.4320 - val_MinusLogProbMetric: 28.4320 - lr: 8.3333e-05 - 38s/epoch - 196ms/step
Epoch 709/1000
2023-10-24 12:29:02.098 
Epoch 709/1000 
	 loss: 27.9903, MinusLogProbMetric: 27.9903, val_loss: 28.4909, val_MinusLogProbMetric: 28.4909

Epoch 709: val_loss did not improve from 28.37318
196/196 - 38s - loss: 27.9903 - MinusLogProbMetric: 27.9903 - val_loss: 28.4909 - val_MinusLogProbMetric: 28.4909 - lr: 8.3333e-05 - 38s/epoch - 196ms/step
Epoch 710/1000
2023-10-24 12:29:42.035 
Epoch 710/1000 
	 loss: 27.9772, MinusLogProbMetric: 27.9772, val_loss: 28.4356, val_MinusLogProbMetric: 28.4356

Epoch 710: val_loss did not improve from 28.37318
196/196 - 40s - loss: 27.9772 - MinusLogProbMetric: 27.9772 - val_loss: 28.4356 - val_MinusLogProbMetric: 28.4356 - lr: 8.3333e-05 - 40s/epoch - 204ms/step
Epoch 711/1000
2023-10-24 12:30:24.128 
Epoch 711/1000 
	 loss: 27.9826, MinusLogProbMetric: 27.9826, val_loss: 28.4837, val_MinusLogProbMetric: 28.4837

Epoch 711: val_loss did not improve from 28.37318
196/196 - 42s - loss: 27.9826 - MinusLogProbMetric: 27.9826 - val_loss: 28.4837 - val_MinusLogProbMetric: 28.4837 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 712/1000
2023-10-24 12:31:04.051 
Epoch 712/1000 
	 loss: 27.9873, MinusLogProbMetric: 27.9873, val_loss: 28.4634, val_MinusLogProbMetric: 28.4634

Epoch 712: val_loss did not improve from 28.37318
196/196 - 40s - loss: 27.9873 - MinusLogProbMetric: 27.9873 - val_loss: 28.4634 - val_MinusLogProbMetric: 28.4634 - lr: 8.3333e-05 - 40s/epoch - 204ms/step
Epoch 713/1000
2023-10-24 12:31:43.167 
Epoch 713/1000 
	 loss: 27.9656, MinusLogProbMetric: 27.9656, val_loss: 28.5358, val_MinusLogProbMetric: 28.5358

Epoch 713: val_loss did not improve from 28.37318
196/196 - 39s - loss: 27.9656 - MinusLogProbMetric: 27.9656 - val_loss: 28.5358 - val_MinusLogProbMetric: 28.5358 - lr: 8.3333e-05 - 39s/epoch - 200ms/step
Epoch 714/1000
2023-10-24 12:32:24.023 
Epoch 714/1000 
	 loss: 27.9972, MinusLogProbMetric: 27.9972, val_loss: 28.4082, val_MinusLogProbMetric: 28.4082

Epoch 714: val_loss did not improve from 28.37318
196/196 - 41s - loss: 27.9972 - MinusLogProbMetric: 27.9972 - val_loss: 28.4082 - val_MinusLogProbMetric: 28.4082 - lr: 8.3333e-05 - 41s/epoch - 208ms/step
Epoch 715/1000
2023-10-24 12:33:03.582 
Epoch 715/1000 
	 loss: 27.9737, MinusLogProbMetric: 27.9737, val_loss: 28.4170, val_MinusLogProbMetric: 28.4170

Epoch 715: val_loss did not improve from 28.37318
196/196 - 40s - loss: 27.9737 - MinusLogProbMetric: 27.9737 - val_loss: 28.4170 - val_MinusLogProbMetric: 28.4170 - lr: 8.3333e-05 - 40s/epoch - 202ms/step
Epoch 716/1000
2023-10-24 12:33:42.982 
Epoch 716/1000 
	 loss: 27.9780, MinusLogProbMetric: 27.9780, val_loss: 28.4553, val_MinusLogProbMetric: 28.4553

Epoch 716: val_loss did not improve from 28.37318
196/196 - 39s - loss: 27.9780 - MinusLogProbMetric: 27.9780 - val_loss: 28.4553 - val_MinusLogProbMetric: 28.4553 - lr: 8.3333e-05 - 39s/epoch - 201ms/step
Epoch 717/1000
2023-10-24 12:34:22.931 
Epoch 717/1000 
	 loss: 27.9768, MinusLogProbMetric: 27.9768, val_loss: 28.4465, val_MinusLogProbMetric: 28.4465

Epoch 717: val_loss did not improve from 28.37318
196/196 - 40s - loss: 27.9768 - MinusLogProbMetric: 27.9768 - val_loss: 28.4465 - val_MinusLogProbMetric: 28.4465 - lr: 8.3333e-05 - 40s/epoch - 204ms/step
Epoch 718/1000
2023-10-24 12:35:00.056 
Epoch 718/1000 
	 loss: 27.9609, MinusLogProbMetric: 27.9609, val_loss: 28.4628, val_MinusLogProbMetric: 28.4628

Epoch 718: val_loss did not improve from 28.37318
196/196 - 37s - loss: 27.9609 - MinusLogProbMetric: 27.9609 - val_loss: 28.4628 - val_MinusLogProbMetric: 28.4628 - lr: 8.3333e-05 - 37s/epoch - 189ms/step
Epoch 719/1000
2023-10-24 12:35:38.564 
Epoch 719/1000 
	 loss: 27.9753, MinusLogProbMetric: 27.9753, val_loss: 28.4877, val_MinusLogProbMetric: 28.4877

Epoch 719: val_loss did not improve from 28.37318
196/196 - 39s - loss: 27.9753 - MinusLogProbMetric: 27.9753 - val_loss: 28.4877 - val_MinusLogProbMetric: 28.4877 - lr: 8.3333e-05 - 39s/epoch - 196ms/step
Epoch 720/1000
2023-10-24 12:36:18.354 
Epoch 720/1000 
	 loss: 27.9707, MinusLogProbMetric: 27.9707, val_loss: 28.4089, val_MinusLogProbMetric: 28.4089

Epoch 720: val_loss did not improve from 28.37318
196/196 - 40s - loss: 27.9707 - MinusLogProbMetric: 27.9707 - val_loss: 28.4089 - val_MinusLogProbMetric: 28.4089 - lr: 8.3333e-05 - 40s/epoch - 203ms/step
Epoch 721/1000
2023-10-24 12:36:58.257 
Epoch 721/1000 
	 loss: 27.9708, MinusLogProbMetric: 27.9708, val_loss: 28.4315, val_MinusLogProbMetric: 28.4315

Epoch 721: val_loss did not improve from 28.37318
196/196 - 40s - loss: 27.9708 - MinusLogProbMetric: 27.9708 - val_loss: 28.4315 - val_MinusLogProbMetric: 28.4315 - lr: 8.3333e-05 - 40s/epoch - 204ms/step
Epoch 722/1000
2023-10-24 12:37:39.841 
Epoch 722/1000 
	 loss: 27.9734, MinusLogProbMetric: 27.9734, val_loss: 28.4327, val_MinusLogProbMetric: 28.4327

Epoch 722: val_loss did not improve from 28.37318
196/196 - 42s - loss: 27.9734 - MinusLogProbMetric: 27.9734 - val_loss: 28.4327 - val_MinusLogProbMetric: 28.4327 - lr: 8.3333e-05 - 42s/epoch - 212ms/step
Epoch 723/1000
2023-10-24 12:38:20.598 
Epoch 723/1000 
	 loss: 27.9740, MinusLogProbMetric: 27.9740, val_loss: 28.4579, val_MinusLogProbMetric: 28.4579

Epoch 723: val_loss did not improve from 28.37318
196/196 - 41s - loss: 27.9740 - MinusLogProbMetric: 27.9740 - val_loss: 28.4579 - val_MinusLogProbMetric: 28.4579 - lr: 8.3333e-05 - 41s/epoch - 208ms/step
Epoch 724/1000
2023-10-24 12:38:59.910 
Epoch 724/1000 
	 loss: 27.9787, MinusLogProbMetric: 27.9787, val_loss: 28.4149, val_MinusLogProbMetric: 28.4149

Epoch 724: val_loss did not improve from 28.37318
196/196 - 39s - loss: 27.9787 - MinusLogProbMetric: 27.9787 - val_loss: 28.4149 - val_MinusLogProbMetric: 28.4149 - lr: 8.3333e-05 - 39s/epoch - 201ms/step
Epoch 725/1000
2023-10-24 12:39:41.173 
Epoch 725/1000 
	 loss: 27.9622, MinusLogProbMetric: 27.9622, val_loss: 28.4158, val_MinusLogProbMetric: 28.4158

Epoch 725: val_loss did not improve from 28.37318
196/196 - 41s - loss: 27.9622 - MinusLogProbMetric: 27.9622 - val_loss: 28.4158 - val_MinusLogProbMetric: 28.4158 - lr: 8.3333e-05 - 41s/epoch - 211ms/step
Epoch 726/1000
2023-10-24 12:40:20.062 
Epoch 726/1000 
	 loss: 27.9705, MinusLogProbMetric: 27.9705, val_loss: 28.4208, val_MinusLogProbMetric: 28.4208

Epoch 726: val_loss did not improve from 28.37318
196/196 - 39s - loss: 27.9705 - MinusLogProbMetric: 27.9705 - val_loss: 28.4208 - val_MinusLogProbMetric: 28.4208 - lr: 8.3333e-05 - 39s/epoch - 198ms/step
Epoch 727/1000
2023-10-24 12:40:57.873 
Epoch 727/1000 
	 loss: 27.9755, MinusLogProbMetric: 27.9755, val_loss: 28.5818, val_MinusLogProbMetric: 28.5818

Epoch 727: val_loss did not improve from 28.37318
196/196 - 38s - loss: 27.9755 - MinusLogProbMetric: 27.9755 - val_loss: 28.5818 - val_MinusLogProbMetric: 28.5818 - lr: 8.3333e-05 - 38s/epoch - 193ms/step
Epoch 728/1000
2023-10-24 12:41:36.141 
Epoch 728/1000 
	 loss: 27.9790, MinusLogProbMetric: 27.9790, val_loss: 28.4058, val_MinusLogProbMetric: 28.4058

Epoch 728: val_loss did not improve from 28.37318
196/196 - 38s - loss: 27.9790 - MinusLogProbMetric: 27.9790 - val_loss: 28.4058 - val_MinusLogProbMetric: 28.4058 - lr: 8.3333e-05 - 38s/epoch - 195ms/step
Epoch 729/1000
2023-10-24 12:42:13.475 
Epoch 729/1000 
	 loss: 27.9787, MinusLogProbMetric: 27.9787, val_loss: 28.4323, val_MinusLogProbMetric: 28.4323

Epoch 729: val_loss did not improve from 28.37318
196/196 - 37s - loss: 27.9787 - MinusLogProbMetric: 27.9787 - val_loss: 28.4323 - val_MinusLogProbMetric: 28.4323 - lr: 8.3333e-05 - 37s/epoch - 190ms/step
Epoch 730/1000
2023-10-24 12:42:51.664 
Epoch 730/1000 
	 loss: 27.9693, MinusLogProbMetric: 27.9693, val_loss: 28.4570, val_MinusLogProbMetric: 28.4570

Epoch 730: val_loss did not improve from 28.37318
196/196 - 38s - loss: 27.9693 - MinusLogProbMetric: 27.9693 - val_loss: 28.4570 - val_MinusLogProbMetric: 28.4570 - lr: 8.3333e-05 - 38s/epoch - 195ms/step
Epoch 731/1000
2023-10-24 12:43:30.160 
Epoch 731/1000 
	 loss: 27.9719, MinusLogProbMetric: 27.9719, val_loss: 28.4285, val_MinusLogProbMetric: 28.4285

Epoch 731: val_loss did not improve from 28.37318
196/196 - 38s - loss: 27.9719 - MinusLogProbMetric: 27.9719 - val_loss: 28.4285 - val_MinusLogProbMetric: 28.4285 - lr: 8.3333e-05 - 38s/epoch - 196ms/step
Epoch 732/1000
2023-10-24 12:44:11.275 
Epoch 732/1000 
	 loss: 27.9662, MinusLogProbMetric: 27.9662, val_loss: 28.4635, val_MinusLogProbMetric: 28.4635

Epoch 732: val_loss did not improve from 28.37318
196/196 - 41s - loss: 27.9662 - MinusLogProbMetric: 27.9662 - val_loss: 28.4635 - val_MinusLogProbMetric: 28.4635 - lr: 8.3333e-05 - 41s/epoch - 210ms/step
Epoch 733/1000
2023-10-24 12:44:52.726 
Epoch 733/1000 
	 loss: 27.9729, MinusLogProbMetric: 27.9729, val_loss: 28.4347, val_MinusLogProbMetric: 28.4347

Epoch 733: val_loss did not improve from 28.37318
196/196 - 41s - loss: 27.9729 - MinusLogProbMetric: 27.9729 - val_loss: 28.4347 - val_MinusLogProbMetric: 28.4347 - lr: 8.3333e-05 - 41s/epoch - 211ms/step
Epoch 734/1000
2023-10-24 12:45:34.711 
Epoch 734/1000 
	 loss: 27.9725, MinusLogProbMetric: 27.9725, val_loss: 28.5218, val_MinusLogProbMetric: 28.5218

Epoch 734: val_loss did not improve from 28.37318
196/196 - 42s - loss: 27.9725 - MinusLogProbMetric: 27.9725 - val_loss: 28.5218 - val_MinusLogProbMetric: 28.5218 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 735/1000
2023-10-24 12:46:13.254 
Epoch 735/1000 
	 loss: 27.9667, MinusLogProbMetric: 27.9667, val_loss: 28.5775, val_MinusLogProbMetric: 28.5775

Epoch 735: val_loss did not improve from 28.37318
196/196 - 39s - loss: 27.9667 - MinusLogProbMetric: 27.9667 - val_loss: 28.5775 - val_MinusLogProbMetric: 28.5775 - lr: 8.3333e-05 - 39s/epoch - 197ms/step
Epoch 736/1000
2023-10-24 12:46:50.779 
Epoch 736/1000 
	 loss: 28.0013, MinusLogProbMetric: 28.0013, val_loss: 28.5486, val_MinusLogProbMetric: 28.5486

Epoch 736: val_loss did not improve from 28.37318
196/196 - 38s - loss: 28.0013 - MinusLogProbMetric: 28.0013 - val_loss: 28.5486 - val_MinusLogProbMetric: 28.5486 - lr: 8.3333e-05 - 38s/epoch - 191ms/step
Epoch 737/1000
2023-10-24 12:47:29.961 
Epoch 737/1000 
	 loss: 27.9825, MinusLogProbMetric: 27.9825, val_loss: 28.4160, val_MinusLogProbMetric: 28.4160

Epoch 737: val_loss did not improve from 28.37318
196/196 - 39s - loss: 27.9825 - MinusLogProbMetric: 27.9825 - val_loss: 28.4160 - val_MinusLogProbMetric: 28.4160 - lr: 8.3333e-05 - 39s/epoch - 200ms/step
Epoch 738/1000
2023-10-24 12:48:10.064 
Epoch 738/1000 
	 loss: 27.9632, MinusLogProbMetric: 27.9632, val_loss: 28.4593, val_MinusLogProbMetric: 28.4593

Epoch 738: val_loss did not improve from 28.37318
196/196 - 40s - loss: 27.9632 - MinusLogProbMetric: 27.9632 - val_loss: 28.4593 - val_MinusLogProbMetric: 28.4593 - lr: 8.3333e-05 - 40s/epoch - 205ms/step
Epoch 739/1000
2023-10-24 12:48:50.807 
Epoch 739/1000 
	 loss: 27.9654, MinusLogProbMetric: 27.9654, val_loss: 28.4086, val_MinusLogProbMetric: 28.4086

Epoch 739: val_loss did not improve from 28.37318
196/196 - 41s - loss: 27.9654 - MinusLogProbMetric: 27.9654 - val_loss: 28.4086 - val_MinusLogProbMetric: 28.4086 - lr: 8.3333e-05 - 41s/epoch - 208ms/step
Epoch 740/1000
2023-10-24 12:49:29.945 
Epoch 740/1000 
	 loss: 27.9779, MinusLogProbMetric: 27.9779, val_loss: 28.4741, val_MinusLogProbMetric: 28.4741

Epoch 740: val_loss did not improve from 28.37318
196/196 - 39s - loss: 27.9779 - MinusLogProbMetric: 27.9779 - val_loss: 28.4741 - val_MinusLogProbMetric: 28.4741 - lr: 8.3333e-05 - 39s/epoch - 200ms/step
Epoch 741/1000
2023-10-24 12:50:10.434 
Epoch 741/1000 
	 loss: 27.9732, MinusLogProbMetric: 27.9732, val_loss: 28.4702, val_MinusLogProbMetric: 28.4702

Epoch 741: val_loss did not improve from 28.37318
196/196 - 40s - loss: 27.9732 - MinusLogProbMetric: 27.9732 - val_loss: 28.4702 - val_MinusLogProbMetric: 28.4702 - lr: 8.3333e-05 - 40s/epoch - 207ms/step
Epoch 742/1000
2023-10-24 12:50:51.368 
Epoch 742/1000 
	 loss: 27.9571, MinusLogProbMetric: 27.9571, val_loss: 28.5104, val_MinusLogProbMetric: 28.5104

Epoch 742: val_loss did not improve from 28.37318
196/196 - 41s - loss: 27.9571 - MinusLogProbMetric: 27.9571 - val_loss: 28.5104 - val_MinusLogProbMetric: 28.5104 - lr: 8.3333e-05 - 41s/epoch - 209ms/step
Epoch 743/1000
2023-10-24 12:51:30.242 
Epoch 743/1000 
	 loss: 27.9717, MinusLogProbMetric: 27.9717, val_loss: 28.3865, val_MinusLogProbMetric: 28.3865

Epoch 743: val_loss did not improve from 28.37318
196/196 - 39s - loss: 27.9717 - MinusLogProbMetric: 27.9717 - val_loss: 28.3865 - val_MinusLogProbMetric: 28.3865 - lr: 8.3333e-05 - 39s/epoch - 198ms/step
Epoch 744/1000
2023-10-24 12:52:10.509 
Epoch 744/1000 
	 loss: 27.9563, MinusLogProbMetric: 27.9563, val_loss: 28.4440, val_MinusLogProbMetric: 28.4440

Epoch 744: val_loss did not improve from 28.37318
196/196 - 40s - loss: 27.9563 - MinusLogProbMetric: 27.9563 - val_loss: 28.4440 - val_MinusLogProbMetric: 28.4440 - lr: 8.3333e-05 - 40s/epoch - 205ms/step
Epoch 745/1000
2023-10-24 12:52:48.230 
Epoch 745/1000 
	 loss: 27.9651, MinusLogProbMetric: 27.9651, val_loss: 28.4460, val_MinusLogProbMetric: 28.4460

Epoch 745: val_loss did not improve from 28.37318
196/196 - 38s - loss: 27.9651 - MinusLogProbMetric: 27.9651 - val_loss: 28.4460 - val_MinusLogProbMetric: 28.4460 - lr: 8.3333e-05 - 38s/epoch - 192ms/step
Epoch 746/1000
2023-10-24 12:53:28.711 
Epoch 746/1000 
	 loss: 27.9596, MinusLogProbMetric: 27.9596, val_loss: 28.4593, val_MinusLogProbMetric: 28.4593

Epoch 746: val_loss did not improve from 28.37318
196/196 - 40s - loss: 27.9596 - MinusLogProbMetric: 27.9596 - val_loss: 28.4593 - val_MinusLogProbMetric: 28.4593 - lr: 8.3333e-05 - 40s/epoch - 207ms/step
Epoch 747/1000
2023-10-24 12:54:10.156 
Epoch 747/1000 
	 loss: 27.9803, MinusLogProbMetric: 27.9803, val_loss: 28.3806, val_MinusLogProbMetric: 28.3806

Epoch 747: val_loss did not improve from 28.37318
196/196 - 41s - loss: 27.9803 - MinusLogProbMetric: 27.9803 - val_loss: 28.3806 - val_MinusLogProbMetric: 28.3806 - lr: 8.3333e-05 - 41s/epoch - 211ms/step
Epoch 748/1000
2023-10-24 12:54:49.682 
Epoch 748/1000 
	 loss: 27.9668, MinusLogProbMetric: 27.9668, val_loss: 28.4186, val_MinusLogProbMetric: 28.4186

Epoch 748: val_loss did not improve from 28.37318
196/196 - 40s - loss: 27.9668 - MinusLogProbMetric: 27.9668 - val_loss: 28.4186 - val_MinusLogProbMetric: 28.4186 - lr: 8.3333e-05 - 40s/epoch - 202ms/step
Epoch 749/1000
2023-10-24 12:55:30.255 
Epoch 749/1000 
	 loss: 27.8990, MinusLogProbMetric: 27.8990, val_loss: 28.3492, val_MinusLogProbMetric: 28.3492

Epoch 749: val_loss improved from 28.37318 to 28.34918, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 41s - loss: 27.8990 - MinusLogProbMetric: 27.8990 - val_loss: 28.3492 - val_MinusLogProbMetric: 28.3492 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 750/1000
2023-10-24 12:56:08.743 
Epoch 750/1000 
	 loss: 27.8970, MinusLogProbMetric: 27.8970, val_loss: 28.3856, val_MinusLogProbMetric: 28.3856

Epoch 750: val_loss did not improve from 28.34918
196/196 - 38s - loss: 27.8970 - MinusLogProbMetric: 27.8970 - val_loss: 28.3856 - val_MinusLogProbMetric: 28.3856 - lr: 4.1667e-05 - 38s/epoch - 193ms/step
Epoch 751/1000
2023-10-24 12:56:48.648 
Epoch 751/1000 
	 loss: 27.8974, MinusLogProbMetric: 27.8974, val_loss: 28.3535, val_MinusLogProbMetric: 28.3535

Epoch 751: val_loss did not improve from 28.34918
196/196 - 40s - loss: 27.8974 - MinusLogProbMetric: 27.8974 - val_loss: 28.3535 - val_MinusLogProbMetric: 28.3535 - lr: 4.1667e-05 - 40s/epoch - 204ms/step
Epoch 752/1000
2023-10-24 12:57:28.022 
Epoch 752/1000 
	 loss: 27.8971, MinusLogProbMetric: 27.8971, val_loss: 28.3837, val_MinusLogProbMetric: 28.3837

Epoch 752: val_loss did not improve from 28.34918
196/196 - 39s - loss: 27.8971 - MinusLogProbMetric: 27.8971 - val_loss: 28.3837 - val_MinusLogProbMetric: 28.3837 - lr: 4.1667e-05 - 39s/epoch - 201ms/step
Epoch 753/1000
2023-10-24 12:58:07.579 
Epoch 753/1000 
	 loss: 27.9041, MinusLogProbMetric: 27.9041, val_loss: 28.3438, val_MinusLogProbMetric: 28.3438

Epoch 753: val_loss improved from 28.34918 to 28.34385, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 40s - loss: 27.9041 - MinusLogProbMetric: 27.9041 - val_loss: 28.3438 - val_MinusLogProbMetric: 28.3438 - lr: 4.1667e-05 - 40s/epoch - 205ms/step
Epoch 754/1000
2023-10-24 12:58:48.208 
Epoch 754/1000 
	 loss: 27.9001, MinusLogProbMetric: 27.9001, val_loss: 28.3473, val_MinusLogProbMetric: 28.3473

Epoch 754: val_loss did not improve from 28.34385
196/196 - 40s - loss: 27.9001 - MinusLogProbMetric: 27.9001 - val_loss: 28.3473 - val_MinusLogProbMetric: 28.3473 - lr: 4.1667e-05 - 40s/epoch - 204ms/step
Epoch 755/1000
2023-10-24 12:59:28.238 
Epoch 755/1000 
	 loss: 27.8977, MinusLogProbMetric: 27.8977, val_loss: 28.3774, val_MinusLogProbMetric: 28.3774

Epoch 755: val_loss did not improve from 28.34385
196/196 - 40s - loss: 27.8977 - MinusLogProbMetric: 27.8977 - val_loss: 28.3774 - val_MinusLogProbMetric: 28.3774 - lr: 4.1667e-05 - 40s/epoch - 204ms/step
Epoch 756/1000
2023-10-24 13:00:08.012 
Epoch 756/1000 
	 loss: 27.9012, MinusLogProbMetric: 27.9012, val_loss: 28.3583, val_MinusLogProbMetric: 28.3583

Epoch 756: val_loss did not improve from 28.34385
196/196 - 40s - loss: 27.9012 - MinusLogProbMetric: 27.9012 - val_loss: 28.3583 - val_MinusLogProbMetric: 28.3583 - lr: 4.1667e-05 - 40s/epoch - 203ms/step
Epoch 757/1000
2023-10-24 13:00:46.428 
Epoch 757/1000 
	 loss: 27.8955, MinusLogProbMetric: 27.8955, val_loss: 28.3885, val_MinusLogProbMetric: 28.3885

Epoch 757: val_loss did not improve from 28.34385
196/196 - 38s - loss: 27.8955 - MinusLogProbMetric: 27.8955 - val_loss: 28.3885 - val_MinusLogProbMetric: 28.3885 - lr: 4.1667e-05 - 38s/epoch - 196ms/step
Epoch 758/1000
2023-10-24 13:01:27.227 
Epoch 758/1000 
	 loss: 27.8936, MinusLogProbMetric: 27.8936, val_loss: 28.3664, val_MinusLogProbMetric: 28.3664

Epoch 758: val_loss did not improve from 28.34385
196/196 - 41s - loss: 27.8936 - MinusLogProbMetric: 27.8936 - val_loss: 28.3664 - val_MinusLogProbMetric: 28.3664 - lr: 4.1667e-05 - 41s/epoch - 208ms/step
Epoch 759/1000
2023-10-24 13:02:08.196 
Epoch 759/1000 
	 loss: 27.8990, MinusLogProbMetric: 27.8990, val_loss: 28.3591, val_MinusLogProbMetric: 28.3591

Epoch 759: val_loss did not improve from 28.34385
196/196 - 41s - loss: 27.8990 - MinusLogProbMetric: 27.8990 - val_loss: 28.3591 - val_MinusLogProbMetric: 28.3591 - lr: 4.1667e-05 - 41s/epoch - 209ms/step
Epoch 760/1000
2023-10-24 13:02:48.009 
Epoch 760/1000 
	 loss: 27.8967, MinusLogProbMetric: 27.8967, val_loss: 28.3380, val_MinusLogProbMetric: 28.3380

Epoch 760: val_loss improved from 28.34385 to 28.33801, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 40s - loss: 27.8967 - MinusLogProbMetric: 27.8967 - val_loss: 28.3380 - val_MinusLogProbMetric: 28.3380 - lr: 4.1667e-05 - 40s/epoch - 206ms/step
Epoch 761/1000
2023-10-24 13:03:28.994 
Epoch 761/1000 
	 loss: 27.8986, MinusLogProbMetric: 27.8986, val_loss: 28.3805, val_MinusLogProbMetric: 28.3805

Epoch 761: val_loss did not improve from 28.33801
196/196 - 40s - loss: 27.8986 - MinusLogProbMetric: 27.8986 - val_loss: 28.3805 - val_MinusLogProbMetric: 28.3805 - lr: 4.1667e-05 - 40s/epoch - 206ms/step
Epoch 762/1000
2023-10-24 13:04:07.366 
Epoch 762/1000 
	 loss: 27.8971, MinusLogProbMetric: 27.8971, val_loss: 28.3974, val_MinusLogProbMetric: 28.3974

Epoch 762: val_loss did not improve from 28.33801
196/196 - 38s - loss: 27.8971 - MinusLogProbMetric: 27.8971 - val_loss: 28.3974 - val_MinusLogProbMetric: 28.3974 - lr: 4.1667e-05 - 38s/epoch - 196ms/step
Epoch 763/1000
2023-10-24 13:04:46.726 
Epoch 763/1000 
	 loss: 27.8952, MinusLogProbMetric: 27.8952, val_loss: 28.3472, val_MinusLogProbMetric: 28.3472

Epoch 763: val_loss did not improve from 28.33801
196/196 - 39s - loss: 27.8952 - MinusLogProbMetric: 27.8952 - val_loss: 28.3472 - val_MinusLogProbMetric: 28.3472 - lr: 4.1667e-05 - 39s/epoch - 201ms/step
Epoch 764/1000
2023-10-24 13:05:25.310 
Epoch 764/1000 
	 loss: 27.9002, MinusLogProbMetric: 27.9002, val_loss: 28.3725, val_MinusLogProbMetric: 28.3725

Epoch 764: val_loss did not improve from 28.33801
196/196 - 39s - loss: 27.9002 - MinusLogProbMetric: 27.9002 - val_loss: 28.3725 - val_MinusLogProbMetric: 28.3725 - lr: 4.1667e-05 - 39s/epoch - 197ms/step
Epoch 765/1000
2023-10-24 13:06:05.150 
Epoch 765/1000 
	 loss: 27.8944, MinusLogProbMetric: 27.8944, val_loss: 28.4174, val_MinusLogProbMetric: 28.4174

Epoch 765: val_loss did not improve from 28.33801
196/196 - 40s - loss: 27.8944 - MinusLogProbMetric: 27.8944 - val_loss: 28.4174 - val_MinusLogProbMetric: 28.4174 - lr: 4.1667e-05 - 40s/epoch - 203ms/step
Epoch 766/1000
2023-10-24 13:06:45.716 
Epoch 766/1000 
	 loss: 27.8980, MinusLogProbMetric: 27.8980, val_loss: 28.3561, val_MinusLogProbMetric: 28.3561

Epoch 766: val_loss did not improve from 28.33801
196/196 - 41s - loss: 27.8980 - MinusLogProbMetric: 27.8980 - val_loss: 28.3561 - val_MinusLogProbMetric: 28.3561 - lr: 4.1667e-05 - 41s/epoch - 207ms/step
Epoch 767/1000
2023-10-24 13:07:27.321 
Epoch 767/1000 
	 loss: 27.9014, MinusLogProbMetric: 27.9014, val_loss: 28.3494, val_MinusLogProbMetric: 28.3494

Epoch 767: val_loss did not improve from 28.33801
196/196 - 42s - loss: 27.9014 - MinusLogProbMetric: 27.9014 - val_loss: 28.3494 - val_MinusLogProbMetric: 28.3494 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 768/1000
2023-10-24 13:08:08.115 
Epoch 768/1000 
	 loss: 27.8969, MinusLogProbMetric: 27.8969, val_loss: 28.3650, val_MinusLogProbMetric: 28.3650

Epoch 768: val_loss did not improve from 28.33801
196/196 - 41s - loss: 27.8969 - MinusLogProbMetric: 27.8969 - val_loss: 28.3650 - val_MinusLogProbMetric: 28.3650 - lr: 4.1667e-05 - 41s/epoch - 208ms/step
Epoch 769/1000
2023-10-24 13:08:48.224 
Epoch 769/1000 
	 loss: 27.8926, MinusLogProbMetric: 27.8926, val_loss: 28.3831, val_MinusLogProbMetric: 28.3831

Epoch 769: val_loss did not improve from 28.33801
196/196 - 40s - loss: 27.8926 - MinusLogProbMetric: 27.8926 - val_loss: 28.3831 - val_MinusLogProbMetric: 28.3831 - lr: 4.1667e-05 - 40s/epoch - 205ms/step
Epoch 770/1000
2023-10-24 13:09:28.065 
Epoch 770/1000 
	 loss: 27.8934, MinusLogProbMetric: 27.8934, val_loss: 28.3741, val_MinusLogProbMetric: 28.3741

Epoch 770: val_loss did not improve from 28.33801
196/196 - 40s - loss: 27.8934 - MinusLogProbMetric: 27.8934 - val_loss: 28.3741 - val_MinusLogProbMetric: 28.3741 - lr: 4.1667e-05 - 40s/epoch - 203ms/step
Epoch 771/1000
2023-10-24 13:10:07.747 
Epoch 771/1000 
	 loss: 27.8972, MinusLogProbMetric: 27.8972, val_loss: 28.3419, val_MinusLogProbMetric: 28.3419

Epoch 771: val_loss did not improve from 28.33801
196/196 - 40s - loss: 27.8972 - MinusLogProbMetric: 27.8972 - val_loss: 28.3419 - val_MinusLogProbMetric: 28.3419 - lr: 4.1667e-05 - 40s/epoch - 202ms/step
Epoch 772/1000
2023-10-24 13:10:49.153 
Epoch 772/1000 
	 loss: 27.9026, MinusLogProbMetric: 27.9026, val_loss: 28.4060, val_MinusLogProbMetric: 28.4060

Epoch 772: val_loss did not improve from 28.33801
196/196 - 41s - loss: 27.9026 - MinusLogProbMetric: 27.9026 - val_loss: 28.4060 - val_MinusLogProbMetric: 28.4060 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 773/1000
2023-10-24 13:11:31.092 
Epoch 773/1000 
	 loss: 27.9007, MinusLogProbMetric: 27.9007, val_loss: 28.3546, val_MinusLogProbMetric: 28.3546

Epoch 773: val_loss did not improve from 28.33801
196/196 - 42s - loss: 27.9007 - MinusLogProbMetric: 27.9007 - val_loss: 28.3546 - val_MinusLogProbMetric: 28.3546 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 774/1000
2023-10-24 13:12:11.716 
Epoch 774/1000 
	 loss: 27.8937, MinusLogProbMetric: 27.8937, val_loss: 28.3527, val_MinusLogProbMetric: 28.3527

Epoch 774: val_loss did not improve from 28.33801
196/196 - 41s - loss: 27.8937 - MinusLogProbMetric: 27.8937 - val_loss: 28.3527 - val_MinusLogProbMetric: 28.3527 - lr: 4.1667e-05 - 41s/epoch - 207ms/step
Epoch 775/1000
2023-10-24 13:12:51.084 
Epoch 775/1000 
	 loss: 27.9011, MinusLogProbMetric: 27.9011, val_loss: 28.3547, val_MinusLogProbMetric: 28.3547

Epoch 775: val_loss did not improve from 28.33801
196/196 - 39s - loss: 27.9011 - MinusLogProbMetric: 27.9011 - val_loss: 28.3547 - val_MinusLogProbMetric: 28.3547 - lr: 4.1667e-05 - 39s/epoch - 201ms/step
Epoch 776/1000
2023-10-24 13:13:32.222 
Epoch 776/1000 
	 loss: 27.9067, MinusLogProbMetric: 27.9067, val_loss: 28.3622, val_MinusLogProbMetric: 28.3622

Epoch 776: val_loss did not improve from 28.33801
196/196 - 41s - loss: 27.9067 - MinusLogProbMetric: 27.9067 - val_loss: 28.3622 - val_MinusLogProbMetric: 28.3622 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 777/1000
2023-10-24 13:14:11.705 
Epoch 777/1000 
	 loss: 27.8939, MinusLogProbMetric: 27.8939, val_loss: 28.3668, val_MinusLogProbMetric: 28.3668

Epoch 777: val_loss did not improve from 28.33801
196/196 - 39s - loss: 27.8939 - MinusLogProbMetric: 27.8939 - val_loss: 28.3668 - val_MinusLogProbMetric: 28.3668 - lr: 4.1667e-05 - 39s/epoch - 201ms/step
Epoch 778/1000
2023-10-24 13:14:53.348 
Epoch 778/1000 
	 loss: 27.8962, MinusLogProbMetric: 27.8962, val_loss: 28.3642, val_MinusLogProbMetric: 28.3642

Epoch 778: val_loss did not improve from 28.33801
196/196 - 42s - loss: 27.8962 - MinusLogProbMetric: 27.8962 - val_loss: 28.3642 - val_MinusLogProbMetric: 28.3642 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 779/1000
2023-10-24 13:15:33.605 
Epoch 779/1000 
	 loss: 27.8930, MinusLogProbMetric: 27.8930, val_loss: 28.3511, val_MinusLogProbMetric: 28.3511

Epoch 779: val_loss did not improve from 28.33801
196/196 - 40s - loss: 27.8930 - MinusLogProbMetric: 27.8930 - val_loss: 28.3511 - val_MinusLogProbMetric: 28.3511 - lr: 4.1667e-05 - 40s/epoch - 205ms/step
Epoch 780/1000
2023-10-24 13:16:15.157 
Epoch 780/1000 
	 loss: 27.8925, MinusLogProbMetric: 27.8925, val_loss: 28.3802, val_MinusLogProbMetric: 28.3802

Epoch 780: val_loss did not improve from 28.33801
196/196 - 42s - loss: 27.8925 - MinusLogProbMetric: 27.8925 - val_loss: 28.3802 - val_MinusLogProbMetric: 28.3802 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 781/1000
2023-10-24 13:16:56.690 
Epoch 781/1000 
	 loss: 27.8980, MinusLogProbMetric: 27.8980, val_loss: 28.3704, val_MinusLogProbMetric: 28.3704

Epoch 781: val_loss did not improve from 28.33801
196/196 - 42s - loss: 27.8980 - MinusLogProbMetric: 27.8980 - val_loss: 28.3704 - val_MinusLogProbMetric: 28.3704 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 782/1000
2023-10-24 13:17:38.057 
Epoch 782/1000 
	 loss: 27.8980, MinusLogProbMetric: 27.8980, val_loss: 28.3548, val_MinusLogProbMetric: 28.3548

Epoch 782: val_loss did not improve from 28.33801
196/196 - 41s - loss: 27.8980 - MinusLogProbMetric: 27.8980 - val_loss: 28.3548 - val_MinusLogProbMetric: 28.3548 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 783/1000
2023-10-24 13:18:19.978 
Epoch 783/1000 
	 loss: 27.9005, MinusLogProbMetric: 27.9005, val_loss: 28.3645, val_MinusLogProbMetric: 28.3645

Epoch 783: val_loss did not improve from 28.33801
196/196 - 42s - loss: 27.9005 - MinusLogProbMetric: 27.9005 - val_loss: 28.3645 - val_MinusLogProbMetric: 28.3645 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 784/1000
2023-10-24 13:19:01.671 
Epoch 784/1000 
	 loss: 27.9011, MinusLogProbMetric: 27.9011, val_loss: 28.3678, val_MinusLogProbMetric: 28.3678

Epoch 784: val_loss did not improve from 28.33801
196/196 - 42s - loss: 27.9011 - MinusLogProbMetric: 27.9011 - val_loss: 28.3678 - val_MinusLogProbMetric: 28.3678 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 785/1000
2023-10-24 13:19:43.419 
Epoch 785/1000 
	 loss: 27.8943, MinusLogProbMetric: 27.8943, val_loss: 28.3379, val_MinusLogProbMetric: 28.3379

Epoch 785: val_loss improved from 28.33801 to 28.33786, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 27.8943 - MinusLogProbMetric: 27.8943 - val_loss: 28.3379 - val_MinusLogProbMetric: 28.3379 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 786/1000
2023-10-24 13:20:25.910 
Epoch 786/1000 
	 loss: 27.8929, MinusLogProbMetric: 27.8929, val_loss: 28.3576, val_MinusLogProbMetric: 28.3576

Epoch 786: val_loss did not improve from 28.33786
196/196 - 42s - loss: 27.8929 - MinusLogProbMetric: 27.8929 - val_loss: 28.3576 - val_MinusLogProbMetric: 28.3576 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 787/1000
2023-10-24 13:21:07.614 
Epoch 787/1000 
	 loss: 27.8980, MinusLogProbMetric: 27.8980, val_loss: 28.3574, val_MinusLogProbMetric: 28.3574

Epoch 787: val_loss did not improve from 28.33786
196/196 - 42s - loss: 27.8980 - MinusLogProbMetric: 27.8980 - val_loss: 28.3574 - val_MinusLogProbMetric: 28.3574 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 788/1000
2023-10-24 13:21:46.345 
Epoch 788/1000 
	 loss: 27.8949, MinusLogProbMetric: 27.8949, val_loss: 28.3493, val_MinusLogProbMetric: 28.3493

Epoch 788: val_loss did not improve from 28.33786
196/196 - 39s - loss: 27.8949 - MinusLogProbMetric: 27.8949 - val_loss: 28.3493 - val_MinusLogProbMetric: 28.3493 - lr: 4.1667e-05 - 39s/epoch - 198ms/step
Epoch 789/1000
2023-10-24 13:22:28.349 
Epoch 789/1000 
	 loss: 27.8933, MinusLogProbMetric: 27.8933, val_loss: 28.3482, val_MinusLogProbMetric: 28.3482

Epoch 789: val_loss did not improve from 28.33786
196/196 - 42s - loss: 27.8933 - MinusLogProbMetric: 27.8933 - val_loss: 28.3482 - val_MinusLogProbMetric: 28.3482 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 790/1000
2023-10-24 13:23:09.085 
Epoch 790/1000 
	 loss: 27.8949, MinusLogProbMetric: 27.8949, val_loss: 28.3492, val_MinusLogProbMetric: 28.3492

Epoch 790: val_loss did not improve from 28.33786
196/196 - 41s - loss: 27.8949 - MinusLogProbMetric: 27.8949 - val_loss: 28.3492 - val_MinusLogProbMetric: 28.3492 - lr: 4.1667e-05 - 41s/epoch - 208ms/step
Epoch 791/1000
2023-10-24 13:23:51.413 
Epoch 791/1000 
	 loss: 27.8920, MinusLogProbMetric: 27.8920, val_loss: 28.3608, val_MinusLogProbMetric: 28.3608

Epoch 791: val_loss did not improve from 28.33786
196/196 - 42s - loss: 27.8920 - MinusLogProbMetric: 27.8920 - val_loss: 28.3608 - val_MinusLogProbMetric: 28.3608 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 792/1000
2023-10-24 13:24:33.359 
Epoch 792/1000 
	 loss: 27.8977, MinusLogProbMetric: 27.8977, val_loss: 28.3481, val_MinusLogProbMetric: 28.3481

Epoch 792: val_loss did not improve from 28.33786
196/196 - 42s - loss: 27.8977 - MinusLogProbMetric: 27.8977 - val_loss: 28.3481 - val_MinusLogProbMetric: 28.3481 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 793/1000
2023-10-24 13:25:15.349 
Epoch 793/1000 
	 loss: 27.8912, MinusLogProbMetric: 27.8912, val_loss: 28.3433, val_MinusLogProbMetric: 28.3433

Epoch 793: val_loss did not improve from 28.33786
196/196 - 42s - loss: 27.8912 - MinusLogProbMetric: 27.8912 - val_loss: 28.3433 - val_MinusLogProbMetric: 28.3433 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 794/1000
2023-10-24 13:25:56.773 
Epoch 794/1000 
	 loss: 27.8904, MinusLogProbMetric: 27.8904, val_loss: 28.3584, val_MinusLogProbMetric: 28.3584

Epoch 794: val_loss did not improve from 28.33786
196/196 - 41s - loss: 27.8904 - MinusLogProbMetric: 27.8904 - val_loss: 28.3584 - val_MinusLogProbMetric: 28.3584 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 795/1000
2023-10-24 13:26:37.988 
Epoch 795/1000 
	 loss: 27.8899, MinusLogProbMetric: 27.8899, val_loss: 28.3574, val_MinusLogProbMetric: 28.3574

Epoch 795: val_loss did not improve from 28.33786
196/196 - 41s - loss: 27.8899 - MinusLogProbMetric: 27.8899 - val_loss: 28.3574 - val_MinusLogProbMetric: 28.3574 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 796/1000
2023-10-24 13:27:19.938 
Epoch 796/1000 
	 loss: 27.8940, MinusLogProbMetric: 27.8940, val_loss: 28.3567, val_MinusLogProbMetric: 28.3567

Epoch 796: val_loss did not improve from 28.33786
196/196 - 42s - loss: 27.8940 - MinusLogProbMetric: 27.8940 - val_loss: 28.3567 - val_MinusLogProbMetric: 28.3567 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 797/1000
2023-10-24 13:28:01.459 
Epoch 797/1000 
	 loss: 27.8969, MinusLogProbMetric: 27.8969, val_loss: 28.3781, val_MinusLogProbMetric: 28.3781

Epoch 797: val_loss did not improve from 28.33786
196/196 - 42s - loss: 27.8969 - MinusLogProbMetric: 27.8969 - val_loss: 28.3781 - val_MinusLogProbMetric: 28.3781 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 798/1000
2023-10-24 13:28:43.036 
Epoch 798/1000 
	 loss: 27.8932, MinusLogProbMetric: 27.8932, val_loss: 28.3719, val_MinusLogProbMetric: 28.3719

Epoch 798: val_loss did not improve from 28.33786
196/196 - 42s - loss: 27.8932 - MinusLogProbMetric: 27.8932 - val_loss: 28.3719 - val_MinusLogProbMetric: 28.3719 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 799/1000
2023-10-24 13:29:24.618 
Epoch 799/1000 
	 loss: 27.8929, MinusLogProbMetric: 27.8929, val_loss: 28.3948, val_MinusLogProbMetric: 28.3948

Epoch 799: val_loss did not improve from 28.33786
196/196 - 42s - loss: 27.8929 - MinusLogProbMetric: 27.8929 - val_loss: 28.3948 - val_MinusLogProbMetric: 28.3948 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 800/1000
2023-10-24 13:30:05.839 
Epoch 800/1000 
	 loss: 27.8927, MinusLogProbMetric: 27.8927, val_loss: 28.3497, val_MinusLogProbMetric: 28.3497

Epoch 800: val_loss did not improve from 28.33786
196/196 - 41s - loss: 27.8927 - MinusLogProbMetric: 27.8927 - val_loss: 28.3497 - val_MinusLogProbMetric: 28.3497 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 801/1000
2023-10-24 13:30:47.687 
Epoch 801/1000 
	 loss: 27.8893, MinusLogProbMetric: 27.8893, val_loss: 28.3551, val_MinusLogProbMetric: 28.3551

Epoch 801: val_loss did not improve from 28.33786
196/196 - 42s - loss: 27.8893 - MinusLogProbMetric: 27.8893 - val_loss: 28.3551 - val_MinusLogProbMetric: 28.3551 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 802/1000
2023-10-24 13:31:29.227 
Epoch 802/1000 
	 loss: 27.8900, MinusLogProbMetric: 27.8900, val_loss: 28.3684, val_MinusLogProbMetric: 28.3684

Epoch 802: val_loss did not improve from 28.33786
196/196 - 42s - loss: 27.8900 - MinusLogProbMetric: 27.8900 - val_loss: 28.3684 - val_MinusLogProbMetric: 28.3684 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 803/1000
2023-10-24 13:32:11.144 
Epoch 803/1000 
	 loss: 27.8867, MinusLogProbMetric: 27.8867, val_loss: 28.3762, val_MinusLogProbMetric: 28.3762

Epoch 803: val_loss did not improve from 28.33786
196/196 - 42s - loss: 27.8867 - MinusLogProbMetric: 27.8867 - val_loss: 28.3762 - val_MinusLogProbMetric: 28.3762 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 804/1000
2023-10-24 13:32:53.035 
Epoch 804/1000 
	 loss: 27.8877, MinusLogProbMetric: 27.8877, val_loss: 28.3598, val_MinusLogProbMetric: 28.3598

Epoch 804: val_loss did not improve from 28.33786
196/196 - 42s - loss: 27.8877 - MinusLogProbMetric: 27.8877 - val_loss: 28.3598 - val_MinusLogProbMetric: 28.3598 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 805/1000
2023-10-24 13:33:34.937 
Epoch 805/1000 
	 loss: 27.8879, MinusLogProbMetric: 27.8879, val_loss: 28.3653, val_MinusLogProbMetric: 28.3653

Epoch 805: val_loss did not improve from 28.33786
196/196 - 42s - loss: 27.8879 - MinusLogProbMetric: 27.8879 - val_loss: 28.3653 - val_MinusLogProbMetric: 28.3653 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 806/1000
2023-10-24 13:34:14.516 
Epoch 806/1000 
	 loss: 27.8901, MinusLogProbMetric: 27.8901, val_loss: 28.3543, val_MinusLogProbMetric: 28.3543

Epoch 806: val_loss did not improve from 28.33786
196/196 - 40s - loss: 27.8901 - MinusLogProbMetric: 27.8901 - val_loss: 28.3543 - val_MinusLogProbMetric: 28.3543 - lr: 4.1667e-05 - 40s/epoch - 202ms/step
Epoch 807/1000
2023-10-24 13:34:56.584 
Epoch 807/1000 
	 loss: 27.8902, MinusLogProbMetric: 27.8902, val_loss: 28.3729, val_MinusLogProbMetric: 28.3729

Epoch 807: val_loss did not improve from 28.33786
196/196 - 42s - loss: 27.8902 - MinusLogProbMetric: 27.8902 - val_loss: 28.3729 - val_MinusLogProbMetric: 28.3729 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 808/1000
2023-10-24 13:35:36.502 
Epoch 808/1000 
	 loss: 27.8936, MinusLogProbMetric: 27.8936, val_loss: 28.3654, val_MinusLogProbMetric: 28.3654

Epoch 808: val_loss did not improve from 28.33786
196/196 - 40s - loss: 27.8936 - MinusLogProbMetric: 27.8936 - val_loss: 28.3654 - val_MinusLogProbMetric: 28.3654 - lr: 4.1667e-05 - 40s/epoch - 204ms/step
Epoch 809/1000
2023-10-24 13:36:16.555 
Epoch 809/1000 
	 loss: 27.8924, MinusLogProbMetric: 27.8924, val_loss: 28.3566, val_MinusLogProbMetric: 28.3566

Epoch 809: val_loss did not improve from 28.33786
196/196 - 40s - loss: 27.8924 - MinusLogProbMetric: 27.8924 - val_loss: 28.3566 - val_MinusLogProbMetric: 28.3566 - lr: 4.1667e-05 - 40s/epoch - 204ms/step
Epoch 810/1000
2023-10-24 13:36:55.562 
Epoch 810/1000 
	 loss: 27.8982, MinusLogProbMetric: 27.8982, val_loss: 28.3494, val_MinusLogProbMetric: 28.3494

Epoch 810: val_loss did not improve from 28.33786
196/196 - 39s - loss: 27.8982 - MinusLogProbMetric: 27.8982 - val_loss: 28.3494 - val_MinusLogProbMetric: 28.3494 - lr: 4.1667e-05 - 39s/epoch - 199ms/step
Epoch 811/1000
2023-10-24 13:37:35.882 
Epoch 811/1000 
	 loss: 27.8882, MinusLogProbMetric: 27.8882, val_loss: 28.3313, val_MinusLogProbMetric: 28.3313

Epoch 811: val_loss improved from 28.33786 to 28.33125, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 41s - loss: 27.8882 - MinusLogProbMetric: 27.8882 - val_loss: 28.3313 - val_MinusLogProbMetric: 28.3313 - lr: 4.1667e-05 - 41s/epoch - 209ms/step
Epoch 812/1000
2023-10-24 13:38:16.113 
Epoch 812/1000 
	 loss: 27.8946, MinusLogProbMetric: 27.8946, val_loss: 28.3245, val_MinusLogProbMetric: 28.3245

Epoch 812: val_loss improved from 28.33125 to 28.32454, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 40s - loss: 27.8946 - MinusLogProbMetric: 27.8946 - val_loss: 28.3245 - val_MinusLogProbMetric: 28.3245 - lr: 4.1667e-05 - 40s/epoch - 205ms/step
Epoch 813/1000
2023-10-24 13:38:55.955 
Epoch 813/1000 
	 loss: 27.8902, MinusLogProbMetric: 27.8902, val_loss: 28.3842, val_MinusLogProbMetric: 28.3842

Epoch 813: val_loss did not improve from 28.32454
196/196 - 39s - loss: 27.8902 - MinusLogProbMetric: 27.8902 - val_loss: 28.3842 - val_MinusLogProbMetric: 28.3842 - lr: 4.1667e-05 - 39s/epoch - 200ms/step
Epoch 814/1000
2023-10-24 13:39:36.054 
Epoch 814/1000 
	 loss: 27.8917, MinusLogProbMetric: 27.8917, val_loss: 28.3544, val_MinusLogProbMetric: 28.3544

Epoch 814: val_loss did not improve from 28.32454
196/196 - 40s - loss: 27.8917 - MinusLogProbMetric: 27.8917 - val_loss: 28.3544 - val_MinusLogProbMetric: 28.3544 - lr: 4.1667e-05 - 40s/epoch - 205ms/step
Epoch 815/1000
2023-10-24 13:40:14.876 
Epoch 815/1000 
	 loss: 27.8881, MinusLogProbMetric: 27.8881, val_loss: 28.3928, val_MinusLogProbMetric: 28.3928

Epoch 815: val_loss did not improve from 28.32454
196/196 - 39s - loss: 27.8881 - MinusLogProbMetric: 27.8881 - val_loss: 28.3928 - val_MinusLogProbMetric: 28.3928 - lr: 4.1667e-05 - 39s/epoch - 198ms/step
Epoch 816/1000
2023-10-24 13:40:55.072 
Epoch 816/1000 
	 loss: 27.8869, MinusLogProbMetric: 27.8869, val_loss: 28.3599, val_MinusLogProbMetric: 28.3599

Epoch 816: val_loss did not improve from 28.32454
196/196 - 40s - loss: 27.8869 - MinusLogProbMetric: 27.8869 - val_loss: 28.3599 - val_MinusLogProbMetric: 28.3599 - lr: 4.1667e-05 - 40s/epoch - 205ms/step
Epoch 817/1000
2023-10-24 13:41:31.319 
Epoch 817/1000 
	 loss: 27.8895, MinusLogProbMetric: 27.8895, val_loss: 28.3527, val_MinusLogProbMetric: 28.3527

Epoch 817: val_loss did not improve from 28.32454
196/196 - 36s - loss: 27.8895 - MinusLogProbMetric: 27.8895 - val_loss: 28.3527 - val_MinusLogProbMetric: 28.3527 - lr: 4.1667e-05 - 36s/epoch - 185ms/step
Epoch 818/1000
2023-10-24 13:42:07.330 
Epoch 818/1000 
	 loss: 27.8848, MinusLogProbMetric: 27.8848, val_loss: 28.3373, val_MinusLogProbMetric: 28.3373

Epoch 818: val_loss did not improve from 28.32454
196/196 - 36s - loss: 27.8848 - MinusLogProbMetric: 27.8848 - val_loss: 28.3373 - val_MinusLogProbMetric: 28.3373 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 819/1000
2023-10-24 13:42:39.459 
Epoch 819/1000 
	 loss: 27.8910, MinusLogProbMetric: 27.8910, val_loss: 28.3790, val_MinusLogProbMetric: 28.3790

Epoch 819: val_loss did not improve from 28.32454
196/196 - 32s - loss: 27.8910 - MinusLogProbMetric: 27.8910 - val_loss: 28.3790 - val_MinusLogProbMetric: 28.3790 - lr: 4.1667e-05 - 32s/epoch - 164ms/step
Epoch 820/1000
2023-10-24 13:43:15.162 
Epoch 820/1000 
	 loss: 27.8851, MinusLogProbMetric: 27.8851, val_loss: 28.3625, val_MinusLogProbMetric: 28.3625

Epoch 820: val_loss did not improve from 28.32454
196/196 - 36s - loss: 27.8851 - MinusLogProbMetric: 27.8851 - val_loss: 28.3625 - val_MinusLogProbMetric: 28.3625 - lr: 4.1667e-05 - 36s/epoch - 182ms/step
Epoch 821/1000
2023-10-24 13:43:56.644 
Epoch 821/1000 
	 loss: 27.8848, MinusLogProbMetric: 27.8848, val_loss: 28.3907, val_MinusLogProbMetric: 28.3907

Epoch 821: val_loss did not improve from 28.32454
196/196 - 41s - loss: 27.8848 - MinusLogProbMetric: 27.8848 - val_loss: 28.3907 - val_MinusLogProbMetric: 28.3907 - lr: 4.1667e-05 - 41s/epoch - 212ms/step
Epoch 822/1000
2023-10-24 13:44:33.975 
Epoch 822/1000 
	 loss: 27.8850, MinusLogProbMetric: 27.8850, val_loss: 28.3418, val_MinusLogProbMetric: 28.3418

Epoch 822: val_loss did not improve from 28.32454
196/196 - 37s - loss: 27.8850 - MinusLogProbMetric: 27.8850 - val_loss: 28.3418 - val_MinusLogProbMetric: 28.3418 - lr: 4.1667e-05 - 37s/epoch - 190ms/step
Epoch 823/1000
2023-10-24 13:45:15.874 
Epoch 823/1000 
	 loss: 27.8906, MinusLogProbMetric: 27.8906, val_loss: 28.3464, val_MinusLogProbMetric: 28.3464

Epoch 823: val_loss did not improve from 28.32454
196/196 - 42s - loss: 27.8906 - MinusLogProbMetric: 27.8906 - val_loss: 28.3464 - val_MinusLogProbMetric: 28.3464 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 824/1000
2023-10-24 13:45:58.015 
Epoch 824/1000 
	 loss: 27.8858, MinusLogProbMetric: 27.8858, val_loss: 28.3466, val_MinusLogProbMetric: 28.3466

Epoch 824: val_loss did not improve from 28.32454
196/196 - 42s - loss: 27.8858 - MinusLogProbMetric: 27.8858 - val_loss: 28.3466 - val_MinusLogProbMetric: 28.3466 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 825/1000
2023-10-24 13:46:40.057 
Epoch 825/1000 
	 loss: 27.8876, MinusLogProbMetric: 27.8876, val_loss: 28.3343, val_MinusLogProbMetric: 28.3343

Epoch 825: val_loss did not improve from 28.32454
196/196 - 42s - loss: 27.8876 - MinusLogProbMetric: 27.8876 - val_loss: 28.3343 - val_MinusLogProbMetric: 28.3343 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 826/1000
2023-10-24 13:47:20.409 
Epoch 826/1000 
	 loss: 27.8882, MinusLogProbMetric: 27.8882, val_loss: 28.3562, val_MinusLogProbMetric: 28.3562

Epoch 826: val_loss did not improve from 28.32454
196/196 - 40s - loss: 27.8882 - MinusLogProbMetric: 27.8882 - val_loss: 28.3562 - val_MinusLogProbMetric: 28.3562 - lr: 4.1667e-05 - 40s/epoch - 206ms/step
Epoch 827/1000
2023-10-24 13:48:01.013 
Epoch 827/1000 
	 loss: 27.8869, MinusLogProbMetric: 27.8869, val_loss: 28.3517, val_MinusLogProbMetric: 28.3517

Epoch 827: val_loss did not improve from 28.32454
196/196 - 41s - loss: 27.8869 - MinusLogProbMetric: 27.8869 - val_loss: 28.3517 - val_MinusLogProbMetric: 28.3517 - lr: 4.1667e-05 - 41s/epoch - 207ms/step
Epoch 828/1000
2023-10-24 13:48:42.131 
Epoch 828/1000 
	 loss: 27.8886, MinusLogProbMetric: 27.8886, val_loss: 28.3543, val_MinusLogProbMetric: 28.3543

Epoch 828: val_loss did not improve from 28.32454
196/196 - 41s - loss: 27.8886 - MinusLogProbMetric: 27.8886 - val_loss: 28.3543 - val_MinusLogProbMetric: 28.3543 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 829/1000
2023-10-24 13:49:24.560 
Epoch 829/1000 
	 loss: 27.8844, MinusLogProbMetric: 27.8844, val_loss: 28.3505, val_MinusLogProbMetric: 28.3505

Epoch 829: val_loss did not improve from 28.32454
196/196 - 42s - loss: 27.8844 - MinusLogProbMetric: 27.8844 - val_loss: 28.3505 - val_MinusLogProbMetric: 28.3505 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 830/1000
2023-10-24 13:50:03.080 
Epoch 830/1000 
	 loss: 27.8925, MinusLogProbMetric: 27.8925, val_loss: 28.3446, val_MinusLogProbMetric: 28.3446

Epoch 830: val_loss did not improve from 28.32454
196/196 - 39s - loss: 27.8925 - MinusLogProbMetric: 27.8925 - val_loss: 28.3446 - val_MinusLogProbMetric: 28.3446 - lr: 4.1667e-05 - 39s/epoch - 196ms/step
Epoch 831/1000
2023-10-24 13:50:42.206 
Epoch 831/1000 
	 loss: 27.8849, MinusLogProbMetric: 27.8849, val_loss: 28.3959, val_MinusLogProbMetric: 28.3959

Epoch 831: val_loss did not improve from 28.32454
196/196 - 39s - loss: 27.8849 - MinusLogProbMetric: 27.8849 - val_loss: 28.3959 - val_MinusLogProbMetric: 28.3959 - lr: 4.1667e-05 - 39s/epoch - 200ms/step
Epoch 832/1000
2023-10-24 13:51:23.740 
Epoch 832/1000 
	 loss: 27.8874, MinusLogProbMetric: 27.8874, val_loss: 28.3673, val_MinusLogProbMetric: 28.3673

Epoch 832: val_loss did not improve from 28.32454
196/196 - 42s - loss: 27.8874 - MinusLogProbMetric: 27.8874 - val_loss: 28.3673 - val_MinusLogProbMetric: 28.3673 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 833/1000
2023-10-24 13:52:03.695 
Epoch 833/1000 
	 loss: 27.8918, MinusLogProbMetric: 27.8918, val_loss: 28.3835, val_MinusLogProbMetric: 28.3835

Epoch 833: val_loss did not improve from 28.32454
196/196 - 40s - loss: 27.8918 - MinusLogProbMetric: 27.8918 - val_loss: 28.3835 - val_MinusLogProbMetric: 28.3835 - lr: 4.1667e-05 - 40s/epoch - 204ms/step
Epoch 834/1000
2023-10-24 13:52:45.070 
Epoch 834/1000 
	 loss: 27.8872, MinusLogProbMetric: 27.8872, val_loss: 28.3452, val_MinusLogProbMetric: 28.3452

Epoch 834: val_loss did not improve from 28.32454
196/196 - 41s - loss: 27.8872 - MinusLogProbMetric: 27.8872 - val_loss: 28.3452 - val_MinusLogProbMetric: 28.3452 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 835/1000
2023-10-24 13:53:24.975 
Epoch 835/1000 
	 loss: 27.8853, MinusLogProbMetric: 27.8853, val_loss: 28.3692, val_MinusLogProbMetric: 28.3692

Epoch 835: val_loss did not improve from 28.32454
196/196 - 40s - loss: 27.8853 - MinusLogProbMetric: 27.8853 - val_loss: 28.3692 - val_MinusLogProbMetric: 28.3692 - lr: 4.1667e-05 - 40s/epoch - 204ms/step
Epoch 836/1000
2023-10-24 13:54:04.409 
Epoch 836/1000 
	 loss: 27.8861, MinusLogProbMetric: 27.8861, val_loss: 28.3725, val_MinusLogProbMetric: 28.3725

Epoch 836: val_loss did not improve from 28.32454
196/196 - 39s - loss: 27.8861 - MinusLogProbMetric: 27.8861 - val_loss: 28.3725 - val_MinusLogProbMetric: 28.3725 - lr: 4.1667e-05 - 39s/epoch - 201ms/step
Epoch 837/1000
2023-10-24 13:54:42.906 
Epoch 837/1000 
	 loss: 27.8882, MinusLogProbMetric: 27.8882, val_loss: 28.3407, val_MinusLogProbMetric: 28.3407

Epoch 837: val_loss did not improve from 28.32454
196/196 - 38s - loss: 27.8882 - MinusLogProbMetric: 27.8882 - val_loss: 28.3407 - val_MinusLogProbMetric: 28.3407 - lr: 4.1667e-05 - 38s/epoch - 196ms/step
Epoch 838/1000
2023-10-24 13:55:21.996 
Epoch 838/1000 
	 loss: 27.8895, MinusLogProbMetric: 27.8895, val_loss: 28.3630, val_MinusLogProbMetric: 28.3630

Epoch 838: val_loss did not improve from 28.32454
196/196 - 39s - loss: 27.8895 - MinusLogProbMetric: 27.8895 - val_loss: 28.3630 - val_MinusLogProbMetric: 28.3630 - lr: 4.1667e-05 - 39s/epoch - 199ms/step
Epoch 839/1000
2023-10-24 13:56:03.196 
Epoch 839/1000 
	 loss: 27.8837, MinusLogProbMetric: 27.8837, val_loss: 28.3649, val_MinusLogProbMetric: 28.3649

Epoch 839: val_loss did not improve from 28.32454
196/196 - 41s - loss: 27.8837 - MinusLogProbMetric: 27.8837 - val_loss: 28.3649 - val_MinusLogProbMetric: 28.3649 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 840/1000
2023-10-24 13:56:45.117 
Epoch 840/1000 
	 loss: 27.8866, MinusLogProbMetric: 27.8866, val_loss: 28.3665, val_MinusLogProbMetric: 28.3665

Epoch 840: val_loss did not improve from 28.32454
196/196 - 42s - loss: 27.8866 - MinusLogProbMetric: 27.8866 - val_loss: 28.3665 - val_MinusLogProbMetric: 28.3665 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 841/1000
2023-10-24 13:57:24.259 
Epoch 841/1000 
	 loss: 27.8917, MinusLogProbMetric: 27.8917, val_loss: 28.3683, val_MinusLogProbMetric: 28.3683

Epoch 841: val_loss did not improve from 28.32454
196/196 - 39s - loss: 27.8917 - MinusLogProbMetric: 27.8917 - val_loss: 28.3683 - val_MinusLogProbMetric: 28.3683 - lr: 4.1667e-05 - 39s/epoch - 200ms/step
Epoch 842/1000
2023-10-24 13:58:04.305 
Epoch 842/1000 
	 loss: 27.8851, MinusLogProbMetric: 27.8851, val_loss: 28.3293, val_MinusLogProbMetric: 28.3293

Epoch 842: val_loss did not improve from 28.32454
196/196 - 40s - loss: 27.8851 - MinusLogProbMetric: 27.8851 - val_loss: 28.3293 - val_MinusLogProbMetric: 28.3293 - lr: 4.1667e-05 - 40s/epoch - 204ms/step
Epoch 843/1000
2023-10-24 13:58:44.720 
Epoch 843/1000 
	 loss: 27.8882, MinusLogProbMetric: 27.8882, val_loss: 28.3600, val_MinusLogProbMetric: 28.3600

Epoch 843: val_loss did not improve from 28.32454
196/196 - 40s - loss: 27.8882 - MinusLogProbMetric: 27.8882 - val_loss: 28.3600 - val_MinusLogProbMetric: 28.3600 - lr: 4.1667e-05 - 40s/epoch - 206ms/step
Epoch 844/1000
2023-10-24 13:59:24.375 
Epoch 844/1000 
	 loss: 27.8890, MinusLogProbMetric: 27.8890, val_loss: 28.3605, val_MinusLogProbMetric: 28.3605

Epoch 844: val_loss did not improve from 28.32454
196/196 - 40s - loss: 27.8890 - MinusLogProbMetric: 27.8890 - val_loss: 28.3605 - val_MinusLogProbMetric: 28.3605 - lr: 4.1667e-05 - 40s/epoch - 202ms/step
Epoch 845/1000
2023-10-24 14:00:05.163 
Epoch 845/1000 
	 loss: 27.8864, MinusLogProbMetric: 27.8864, val_loss: 28.3713, val_MinusLogProbMetric: 28.3713

Epoch 845: val_loss did not improve from 28.32454
196/196 - 41s - loss: 27.8864 - MinusLogProbMetric: 27.8864 - val_loss: 28.3713 - val_MinusLogProbMetric: 28.3713 - lr: 4.1667e-05 - 41s/epoch - 208ms/step
Epoch 846/1000
2023-10-24 14:00:43.150 
Epoch 846/1000 
	 loss: 27.8884, MinusLogProbMetric: 27.8884, val_loss: 28.3606, val_MinusLogProbMetric: 28.3606

Epoch 846: val_loss did not improve from 28.32454
196/196 - 38s - loss: 27.8884 - MinusLogProbMetric: 27.8884 - val_loss: 28.3606 - val_MinusLogProbMetric: 28.3606 - lr: 4.1667e-05 - 38s/epoch - 194ms/step
Epoch 847/1000
2023-10-24 14:01:22.509 
Epoch 847/1000 
	 loss: 27.8861, MinusLogProbMetric: 27.8861, val_loss: 28.3708, val_MinusLogProbMetric: 28.3708

Epoch 847: val_loss did not improve from 28.32454
196/196 - 39s - loss: 27.8861 - MinusLogProbMetric: 27.8861 - val_loss: 28.3708 - val_MinusLogProbMetric: 28.3708 - lr: 4.1667e-05 - 39s/epoch - 201ms/step
Epoch 848/1000
2023-10-24 14:02:04.407 
Epoch 848/1000 
	 loss: 27.8847, MinusLogProbMetric: 27.8847, val_loss: 28.3373, val_MinusLogProbMetric: 28.3373

Epoch 848: val_loss did not improve from 28.32454
196/196 - 42s - loss: 27.8847 - MinusLogProbMetric: 27.8847 - val_loss: 28.3373 - val_MinusLogProbMetric: 28.3373 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 849/1000
2023-10-24 14:02:45.929 
Epoch 849/1000 
	 loss: 27.8890, MinusLogProbMetric: 27.8890, val_loss: 28.3435, val_MinusLogProbMetric: 28.3435

Epoch 849: val_loss did not improve from 28.32454
196/196 - 42s - loss: 27.8890 - MinusLogProbMetric: 27.8890 - val_loss: 28.3435 - val_MinusLogProbMetric: 28.3435 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 850/1000
2023-10-24 14:03:26.713 
Epoch 850/1000 
	 loss: 27.8896, MinusLogProbMetric: 27.8896, val_loss: 28.3414, val_MinusLogProbMetric: 28.3414

Epoch 850: val_loss did not improve from 28.32454
196/196 - 41s - loss: 27.8896 - MinusLogProbMetric: 27.8896 - val_loss: 28.3414 - val_MinusLogProbMetric: 28.3414 - lr: 4.1667e-05 - 41s/epoch - 208ms/step
Epoch 851/1000
2023-10-24 14:04:07.470 
Epoch 851/1000 
	 loss: 27.8843, MinusLogProbMetric: 27.8843, val_loss: 28.4078, val_MinusLogProbMetric: 28.4078

Epoch 851: val_loss did not improve from 28.32454
196/196 - 41s - loss: 27.8843 - MinusLogProbMetric: 27.8843 - val_loss: 28.4078 - val_MinusLogProbMetric: 28.4078 - lr: 4.1667e-05 - 41s/epoch - 208ms/step
Epoch 852/1000
2023-10-24 14:04:47.675 
Epoch 852/1000 
	 loss: 27.8874, MinusLogProbMetric: 27.8874, val_loss: 28.4199, val_MinusLogProbMetric: 28.4199

Epoch 852: val_loss did not improve from 28.32454
196/196 - 40s - loss: 27.8874 - MinusLogProbMetric: 27.8874 - val_loss: 28.4199 - val_MinusLogProbMetric: 28.4199 - lr: 4.1667e-05 - 40s/epoch - 205ms/step
Epoch 853/1000
2023-10-24 14:05:29.452 
Epoch 853/1000 
	 loss: 27.8862, MinusLogProbMetric: 27.8862, val_loss: 28.3215, val_MinusLogProbMetric: 28.3215

Epoch 853: val_loss improved from 28.32454 to 28.32151, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 27.8862 - MinusLogProbMetric: 27.8862 - val_loss: 28.3215 - val_MinusLogProbMetric: 28.3215 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 854/1000
2023-10-24 14:06:09.307 
Epoch 854/1000 
	 loss: 27.8821, MinusLogProbMetric: 27.8821, val_loss: 28.3787, val_MinusLogProbMetric: 28.3787

Epoch 854: val_loss did not improve from 28.32151
196/196 - 39s - loss: 27.8821 - MinusLogProbMetric: 27.8821 - val_loss: 28.3787 - val_MinusLogProbMetric: 28.3787 - lr: 4.1667e-05 - 39s/epoch - 200ms/step
Epoch 855/1000
2023-10-24 14:06:50.365 
Epoch 855/1000 
	 loss: 27.8814, MinusLogProbMetric: 27.8814, val_loss: 28.3639, val_MinusLogProbMetric: 28.3639

Epoch 855: val_loss did not improve from 28.32151
196/196 - 41s - loss: 27.8814 - MinusLogProbMetric: 27.8814 - val_loss: 28.3639 - val_MinusLogProbMetric: 28.3639 - lr: 4.1667e-05 - 41s/epoch - 209ms/step
Epoch 856/1000
2023-10-24 14:07:32.139 
Epoch 856/1000 
	 loss: 27.8815, MinusLogProbMetric: 27.8815, val_loss: 28.3437, val_MinusLogProbMetric: 28.3437

Epoch 856: val_loss did not improve from 28.32151
196/196 - 42s - loss: 27.8815 - MinusLogProbMetric: 27.8815 - val_loss: 28.3437 - val_MinusLogProbMetric: 28.3437 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 857/1000
2023-10-24 14:08:11.935 
Epoch 857/1000 
	 loss: 27.8895, MinusLogProbMetric: 27.8895, val_loss: 28.3449, val_MinusLogProbMetric: 28.3449

Epoch 857: val_loss did not improve from 28.32151
196/196 - 40s - loss: 27.8895 - MinusLogProbMetric: 27.8895 - val_loss: 28.3449 - val_MinusLogProbMetric: 28.3449 - lr: 4.1667e-05 - 40s/epoch - 203ms/step
Epoch 858/1000
2023-10-24 14:08:51.850 
Epoch 858/1000 
	 loss: 27.8893, MinusLogProbMetric: 27.8893, val_loss: 28.3436, val_MinusLogProbMetric: 28.3436

Epoch 858: val_loss did not improve from 28.32151
196/196 - 40s - loss: 27.8893 - MinusLogProbMetric: 27.8893 - val_loss: 28.3436 - val_MinusLogProbMetric: 28.3436 - lr: 4.1667e-05 - 40s/epoch - 204ms/step
Epoch 859/1000
2023-10-24 14:09:31.720 
Epoch 859/1000 
	 loss: 27.8868, MinusLogProbMetric: 27.8868, val_loss: 28.4022, val_MinusLogProbMetric: 28.4022

Epoch 859: val_loss did not improve from 28.32151
196/196 - 40s - loss: 27.8868 - MinusLogProbMetric: 27.8868 - val_loss: 28.4022 - val_MinusLogProbMetric: 28.4022 - lr: 4.1667e-05 - 40s/epoch - 203ms/step
Epoch 860/1000
2023-10-24 14:10:12.459 
Epoch 860/1000 
	 loss: 27.8833, MinusLogProbMetric: 27.8833, val_loss: 28.3726, val_MinusLogProbMetric: 28.3726

Epoch 860: val_loss did not improve from 28.32151
196/196 - 41s - loss: 27.8833 - MinusLogProbMetric: 27.8833 - val_loss: 28.3726 - val_MinusLogProbMetric: 28.3726 - lr: 4.1667e-05 - 41s/epoch - 208ms/step
Epoch 861/1000
2023-10-24 14:10:54.150 
Epoch 861/1000 
	 loss: 27.8880, MinusLogProbMetric: 27.8880, val_loss: 28.3828, val_MinusLogProbMetric: 28.3828

Epoch 861: val_loss did not improve from 28.32151
196/196 - 42s - loss: 27.8880 - MinusLogProbMetric: 27.8880 - val_loss: 28.3828 - val_MinusLogProbMetric: 28.3828 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 862/1000
2023-10-24 14:11:30.622 
Epoch 862/1000 
	 loss: 27.8809, MinusLogProbMetric: 27.8809, val_loss: 28.3530, val_MinusLogProbMetric: 28.3530

Epoch 862: val_loss did not improve from 28.32151
196/196 - 36s - loss: 27.8809 - MinusLogProbMetric: 27.8809 - val_loss: 28.3530 - val_MinusLogProbMetric: 28.3530 - lr: 4.1667e-05 - 36s/epoch - 186ms/step
Epoch 863/1000
2023-10-24 14:12:03.359 
Epoch 863/1000 
	 loss: 27.8837, MinusLogProbMetric: 27.8837, val_loss: 28.3927, val_MinusLogProbMetric: 28.3927

Epoch 863: val_loss did not improve from 28.32151
196/196 - 33s - loss: 27.8837 - MinusLogProbMetric: 27.8837 - val_loss: 28.3927 - val_MinusLogProbMetric: 28.3927 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 864/1000
2023-10-24 14:12:38.220 
Epoch 864/1000 
	 loss: 27.8817, MinusLogProbMetric: 27.8817, val_loss: 28.4374, val_MinusLogProbMetric: 28.4374

Epoch 864: val_loss did not improve from 28.32151
196/196 - 35s - loss: 27.8817 - MinusLogProbMetric: 27.8817 - val_loss: 28.4374 - val_MinusLogProbMetric: 28.4374 - lr: 4.1667e-05 - 35s/epoch - 178ms/step
Epoch 865/1000
2023-10-24 14:13:15.428 
Epoch 865/1000 
	 loss: 27.8895, MinusLogProbMetric: 27.8895, val_loss: 28.3404, val_MinusLogProbMetric: 28.3404

Epoch 865: val_loss did not improve from 28.32151
196/196 - 37s - loss: 27.8895 - MinusLogProbMetric: 27.8895 - val_loss: 28.3404 - val_MinusLogProbMetric: 28.3404 - lr: 4.1667e-05 - 37s/epoch - 190ms/step
Epoch 866/1000
2023-10-24 14:13:54.455 
Epoch 866/1000 
	 loss: 27.8817, MinusLogProbMetric: 27.8817, val_loss: 28.3245, val_MinusLogProbMetric: 28.3245

Epoch 866: val_loss did not improve from 28.32151
196/196 - 39s - loss: 27.8817 - MinusLogProbMetric: 27.8817 - val_loss: 28.3245 - val_MinusLogProbMetric: 28.3245 - lr: 4.1667e-05 - 39s/epoch - 199ms/step
Epoch 867/1000
2023-10-24 14:14:30.398 
Epoch 867/1000 
	 loss: 27.8845, MinusLogProbMetric: 27.8845, val_loss: 28.3490, val_MinusLogProbMetric: 28.3490

Epoch 867: val_loss did not improve from 28.32151
196/196 - 36s - loss: 27.8845 - MinusLogProbMetric: 27.8845 - val_loss: 28.3490 - val_MinusLogProbMetric: 28.3490 - lr: 4.1667e-05 - 36s/epoch - 183ms/step
Epoch 868/1000
2023-10-24 14:15:06.239 
Epoch 868/1000 
	 loss: 27.8848, MinusLogProbMetric: 27.8848, val_loss: 28.3901, val_MinusLogProbMetric: 28.3901

Epoch 868: val_loss did not improve from 28.32151
196/196 - 36s - loss: 27.8848 - MinusLogProbMetric: 27.8848 - val_loss: 28.3901 - val_MinusLogProbMetric: 28.3901 - lr: 4.1667e-05 - 36s/epoch - 183ms/step
Epoch 869/1000
2023-10-24 14:15:42.852 
Epoch 869/1000 
	 loss: 27.8845, MinusLogProbMetric: 27.8845, val_loss: 28.3666, val_MinusLogProbMetric: 28.3666

Epoch 869: val_loss did not improve from 28.32151
196/196 - 37s - loss: 27.8845 - MinusLogProbMetric: 27.8845 - val_loss: 28.3666 - val_MinusLogProbMetric: 28.3666 - lr: 4.1667e-05 - 37s/epoch - 187ms/step
Epoch 870/1000
2023-10-24 14:16:20.790 
Epoch 870/1000 
	 loss: 27.8825, MinusLogProbMetric: 27.8825, val_loss: 28.3426, val_MinusLogProbMetric: 28.3426

Epoch 870: val_loss did not improve from 28.32151
196/196 - 38s - loss: 27.8825 - MinusLogProbMetric: 27.8825 - val_loss: 28.3426 - val_MinusLogProbMetric: 28.3426 - lr: 4.1667e-05 - 38s/epoch - 194ms/step
Epoch 871/1000
2023-10-24 14:16:56.757 
Epoch 871/1000 
	 loss: 27.8795, MinusLogProbMetric: 27.8795, val_loss: 28.3573, val_MinusLogProbMetric: 28.3573

Epoch 871: val_loss did not improve from 28.32151
196/196 - 36s - loss: 27.8795 - MinusLogProbMetric: 27.8795 - val_loss: 28.3573 - val_MinusLogProbMetric: 28.3573 - lr: 4.1667e-05 - 36s/epoch - 183ms/step
Epoch 872/1000
2023-10-24 14:17:32.806 
Epoch 872/1000 
	 loss: 27.8817, MinusLogProbMetric: 27.8817, val_loss: 28.3414, val_MinusLogProbMetric: 28.3414

Epoch 872: val_loss did not improve from 28.32151
196/196 - 36s - loss: 27.8817 - MinusLogProbMetric: 27.8817 - val_loss: 28.3414 - val_MinusLogProbMetric: 28.3414 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 873/1000
2023-10-24 14:18:09.893 
Epoch 873/1000 
	 loss: 27.8776, MinusLogProbMetric: 27.8776, val_loss: 28.3316, val_MinusLogProbMetric: 28.3316

Epoch 873: val_loss did not improve from 28.32151
196/196 - 37s - loss: 27.8776 - MinusLogProbMetric: 27.8776 - val_loss: 28.3316 - val_MinusLogProbMetric: 28.3316 - lr: 4.1667e-05 - 37s/epoch - 189ms/step
Epoch 874/1000
2023-10-24 14:18:49.818 
Epoch 874/1000 
	 loss: 27.8801, MinusLogProbMetric: 27.8801, val_loss: 28.3612, val_MinusLogProbMetric: 28.3612

Epoch 874: val_loss did not improve from 28.32151
196/196 - 40s - loss: 27.8801 - MinusLogProbMetric: 27.8801 - val_loss: 28.3612 - val_MinusLogProbMetric: 28.3612 - lr: 4.1667e-05 - 40s/epoch - 204ms/step
Epoch 875/1000
2023-10-24 14:19:26.614 
Epoch 875/1000 
	 loss: 27.8783, MinusLogProbMetric: 27.8783, val_loss: 28.3815, val_MinusLogProbMetric: 28.3815

Epoch 875: val_loss did not improve from 28.32151
196/196 - 37s - loss: 27.8783 - MinusLogProbMetric: 27.8783 - val_loss: 28.3815 - val_MinusLogProbMetric: 28.3815 - lr: 4.1667e-05 - 37s/epoch - 188ms/step
Epoch 876/1000
2023-10-24 14:20:02.616 
Epoch 876/1000 
	 loss: 27.8817, MinusLogProbMetric: 27.8817, val_loss: 28.3331, val_MinusLogProbMetric: 28.3331

Epoch 876: val_loss did not improve from 28.32151
196/196 - 36s - loss: 27.8817 - MinusLogProbMetric: 27.8817 - val_loss: 28.3331 - val_MinusLogProbMetric: 28.3331 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 877/1000
2023-10-24 14:20:38.688 
Epoch 877/1000 
	 loss: 27.8797, MinusLogProbMetric: 27.8797, val_loss: 28.3858, val_MinusLogProbMetric: 28.3858

Epoch 877: val_loss did not improve from 28.32151
196/196 - 36s - loss: 27.8797 - MinusLogProbMetric: 27.8797 - val_loss: 28.3858 - val_MinusLogProbMetric: 28.3858 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 878/1000
2023-10-24 14:21:15.064 
Epoch 878/1000 
	 loss: 27.8758, MinusLogProbMetric: 27.8758, val_loss: 28.3580, val_MinusLogProbMetric: 28.3580

Epoch 878: val_loss did not improve from 28.32151
196/196 - 36s - loss: 27.8758 - MinusLogProbMetric: 27.8758 - val_loss: 28.3580 - val_MinusLogProbMetric: 28.3580 - lr: 4.1667e-05 - 36s/epoch - 186ms/step
Epoch 879/1000
2023-10-24 14:21:52.289 
Epoch 879/1000 
	 loss: 27.8786, MinusLogProbMetric: 27.8786, val_loss: 28.3297, val_MinusLogProbMetric: 28.3297

Epoch 879: val_loss did not improve from 28.32151
196/196 - 37s - loss: 27.8786 - MinusLogProbMetric: 27.8786 - val_loss: 28.3297 - val_MinusLogProbMetric: 28.3297 - lr: 4.1667e-05 - 37s/epoch - 190ms/step
Epoch 880/1000
2023-10-24 14:22:27.944 
Epoch 880/1000 
	 loss: 27.8783, MinusLogProbMetric: 27.8783, val_loss: 28.3295, val_MinusLogProbMetric: 28.3295

Epoch 880: val_loss did not improve from 28.32151
196/196 - 36s - loss: 27.8783 - MinusLogProbMetric: 27.8783 - val_loss: 28.3295 - val_MinusLogProbMetric: 28.3295 - lr: 4.1667e-05 - 36s/epoch - 182ms/step
Epoch 881/1000
2023-10-24 14:23:03.507 
Epoch 881/1000 
	 loss: 27.8785, MinusLogProbMetric: 27.8785, val_loss: 28.3396, val_MinusLogProbMetric: 28.3396

Epoch 881: val_loss did not improve from 28.32151
196/196 - 36s - loss: 27.8785 - MinusLogProbMetric: 27.8785 - val_loss: 28.3396 - val_MinusLogProbMetric: 28.3396 - lr: 4.1667e-05 - 36s/epoch - 181ms/step
Epoch 882/1000
2023-10-24 14:23:41.929 
Epoch 882/1000 
	 loss: 27.8834, MinusLogProbMetric: 27.8834, val_loss: 28.3725, val_MinusLogProbMetric: 28.3725

Epoch 882: val_loss did not improve from 28.32151
196/196 - 38s - loss: 27.8834 - MinusLogProbMetric: 27.8834 - val_loss: 28.3725 - val_MinusLogProbMetric: 28.3725 - lr: 4.1667e-05 - 38s/epoch - 196ms/step
Epoch 883/1000
2023-10-24 14:24:18.664 
Epoch 883/1000 
	 loss: 27.8792, MinusLogProbMetric: 27.8792, val_loss: 28.3556, val_MinusLogProbMetric: 28.3556

Epoch 883: val_loss did not improve from 28.32151
196/196 - 37s - loss: 27.8792 - MinusLogProbMetric: 27.8792 - val_loss: 28.3556 - val_MinusLogProbMetric: 28.3556 - lr: 4.1667e-05 - 37s/epoch - 187ms/step
Epoch 884/1000
2023-10-24 14:24:54.728 
Epoch 884/1000 
	 loss: 27.8749, MinusLogProbMetric: 27.8749, val_loss: 28.3728, val_MinusLogProbMetric: 28.3728

Epoch 884: val_loss did not improve from 28.32151
196/196 - 36s - loss: 27.8749 - MinusLogProbMetric: 27.8749 - val_loss: 28.3728 - val_MinusLogProbMetric: 28.3728 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 885/1000
2023-10-24 14:25:30.363 
Epoch 885/1000 
	 loss: 27.8785, MinusLogProbMetric: 27.8785, val_loss: 28.3459, val_MinusLogProbMetric: 28.3459

Epoch 885: val_loss did not improve from 28.32151
196/196 - 36s - loss: 27.8785 - MinusLogProbMetric: 27.8785 - val_loss: 28.3459 - val_MinusLogProbMetric: 28.3459 - lr: 4.1667e-05 - 36s/epoch - 182ms/step
Epoch 886/1000
2023-10-24 14:26:08.303 
Epoch 886/1000 
	 loss: 27.8753, MinusLogProbMetric: 27.8753, val_loss: 28.3750, val_MinusLogProbMetric: 28.3750

Epoch 886: val_loss did not improve from 28.32151
196/196 - 38s - loss: 27.8753 - MinusLogProbMetric: 27.8753 - val_loss: 28.3750 - val_MinusLogProbMetric: 28.3750 - lr: 4.1667e-05 - 38s/epoch - 194ms/step
Epoch 887/1000
2023-10-24 14:26:46.989 
Epoch 887/1000 
	 loss: 27.8757, MinusLogProbMetric: 27.8757, val_loss: 28.3687, val_MinusLogProbMetric: 28.3687

Epoch 887: val_loss did not improve from 28.32151
196/196 - 39s - loss: 27.8757 - MinusLogProbMetric: 27.8757 - val_loss: 28.3687 - val_MinusLogProbMetric: 28.3687 - lr: 4.1667e-05 - 39s/epoch - 197ms/step
Epoch 888/1000
2023-10-24 14:27:23.135 
Epoch 888/1000 
	 loss: 27.8782, MinusLogProbMetric: 27.8782, val_loss: 28.3447, val_MinusLogProbMetric: 28.3447

Epoch 888: val_loss did not improve from 28.32151
196/196 - 36s - loss: 27.8782 - MinusLogProbMetric: 27.8782 - val_loss: 28.3447 - val_MinusLogProbMetric: 28.3447 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 889/1000
2023-10-24 14:27:58.940 
Epoch 889/1000 
	 loss: 27.8769, MinusLogProbMetric: 27.8769, val_loss: 28.3457, val_MinusLogProbMetric: 28.3457

Epoch 889: val_loss did not improve from 28.32151
196/196 - 36s - loss: 27.8769 - MinusLogProbMetric: 27.8769 - val_loss: 28.3457 - val_MinusLogProbMetric: 28.3457 - lr: 4.1667e-05 - 36s/epoch - 183ms/step
Epoch 890/1000
2023-10-24 14:28:34.908 
Epoch 890/1000 
	 loss: 27.8749, MinusLogProbMetric: 27.8749, val_loss: 28.3581, val_MinusLogProbMetric: 28.3581

Epoch 890: val_loss did not improve from 28.32151
196/196 - 36s - loss: 27.8749 - MinusLogProbMetric: 27.8749 - val_loss: 28.3581 - val_MinusLogProbMetric: 28.3581 - lr: 4.1667e-05 - 36s/epoch - 183ms/step
Epoch 891/1000
2023-10-24 14:29:13.104 
Epoch 891/1000 
	 loss: 27.8717, MinusLogProbMetric: 27.8717, val_loss: 28.3294, val_MinusLogProbMetric: 28.3294

Epoch 891: val_loss did not improve from 28.32151
196/196 - 38s - loss: 27.8717 - MinusLogProbMetric: 27.8717 - val_loss: 28.3294 - val_MinusLogProbMetric: 28.3294 - lr: 4.1667e-05 - 38s/epoch - 195ms/step
Epoch 892/1000
2023-10-24 14:29:49.996 
Epoch 892/1000 
	 loss: 27.8742, MinusLogProbMetric: 27.8742, val_loss: 28.3627, val_MinusLogProbMetric: 28.3627

Epoch 892: val_loss did not improve from 28.32151
196/196 - 37s - loss: 27.8742 - MinusLogProbMetric: 27.8742 - val_loss: 28.3627 - val_MinusLogProbMetric: 28.3627 - lr: 4.1667e-05 - 37s/epoch - 188ms/step
Epoch 893/1000
2023-10-24 14:30:25.906 
Epoch 893/1000 
	 loss: 27.8739, MinusLogProbMetric: 27.8739, val_loss: 28.3507, val_MinusLogProbMetric: 28.3507

Epoch 893: val_loss did not improve from 28.32151
196/196 - 36s - loss: 27.8739 - MinusLogProbMetric: 27.8739 - val_loss: 28.3507 - val_MinusLogProbMetric: 28.3507 - lr: 4.1667e-05 - 36s/epoch - 183ms/step
Epoch 894/1000
2023-10-24 14:31:01.043 
Epoch 894/1000 
	 loss: 27.8745, MinusLogProbMetric: 27.8745, val_loss: 28.3582, val_MinusLogProbMetric: 28.3582

Epoch 894: val_loss did not improve from 28.32151
196/196 - 35s - loss: 27.8745 - MinusLogProbMetric: 27.8745 - val_loss: 28.3582 - val_MinusLogProbMetric: 28.3582 - lr: 4.1667e-05 - 35s/epoch - 179ms/step
Epoch 895/1000
2023-10-24 14:31:38.988 
Epoch 895/1000 
	 loss: 27.8784, MinusLogProbMetric: 27.8784, val_loss: 28.3402, val_MinusLogProbMetric: 28.3402

Epoch 895: val_loss did not improve from 28.32151
196/196 - 38s - loss: 27.8784 - MinusLogProbMetric: 27.8784 - val_loss: 28.3402 - val_MinusLogProbMetric: 28.3402 - lr: 4.1667e-05 - 38s/epoch - 194ms/step
Epoch 896/1000
2023-10-24 14:32:17.276 
Epoch 896/1000 
	 loss: 27.8780, MinusLogProbMetric: 27.8780, val_loss: 28.3717, val_MinusLogProbMetric: 28.3717

Epoch 896: val_loss did not improve from 28.32151
196/196 - 38s - loss: 27.8780 - MinusLogProbMetric: 27.8780 - val_loss: 28.3717 - val_MinusLogProbMetric: 28.3717 - lr: 4.1667e-05 - 38s/epoch - 195ms/step
Epoch 897/1000
2023-10-24 14:32:53.964 
Epoch 897/1000 
	 loss: 27.8728, MinusLogProbMetric: 27.8728, val_loss: 28.3212, val_MinusLogProbMetric: 28.3212

Epoch 897: val_loss improved from 28.32151 to 28.32122, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 37s - loss: 27.8728 - MinusLogProbMetric: 27.8728 - val_loss: 28.3212 - val_MinusLogProbMetric: 28.3212 - lr: 4.1667e-05 - 37s/epoch - 190ms/step
Epoch 898/1000
2023-10-24 14:33:30.730 
Epoch 898/1000 
	 loss: 27.8762, MinusLogProbMetric: 27.8762, val_loss: 28.3699, val_MinusLogProbMetric: 28.3699

Epoch 898: val_loss did not improve from 28.32122
196/196 - 36s - loss: 27.8762 - MinusLogProbMetric: 27.8762 - val_loss: 28.3699 - val_MinusLogProbMetric: 28.3699 - lr: 4.1667e-05 - 36s/epoch - 185ms/step
Epoch 899/1000
2023-10-24 14:34:08.890 
Epoch 899/1000 
	 loss: 27.8758, MinusLogProbMetric: 27.8758, val_loss: 28.3592, val_MinusLogProbMetric: 28.3592

Epoch 899: val_loss did not improve from 28.32122
196/196 - 38s - loss: 27.8758 - MinusLogProbMetric: 27.8758 - val_loss: 28.3592 - val_MinusLogProbMetric: 28.3592 - lr: 4.1667e-05 - 38s/epoch - 195ms/step
Epoch 900/1000
2023-10-24 14:34:44.078 
Epoch 900/1000 
	 loss: 27.8745, MinusLogProbMetric: 27.8745, val_loss: 28.3316, val_MinusLogProbMetric: 28.3316

Epoch 900: val_loss did not improve from 28.32122
196/196 - 35s - loss: 27.8745 - MinusLogProbMetric: 27.8745 - val_loss: 28.3316 - val_MinusLogProbMetric: 28.3316 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 901/1000
2023-10-24 14:35:19.604 
Epoch 901/1000 
	 loss: 27.8730, MinusLogProbMetric: 27.8730, val_loss: 28.3430, val_MinusLogProbMetric: 28.3430

Epoch 901: val_loss did not improve from 28.32122
196/196 - 36s - loss: 27.8730 - MinusLogProbMetric: 27.8730 - val_loss: 28.3430 - val_MinusLogProbMetric: 28.3430 - lr: 4.1667e-05 - 36s/epoch - 181ms/step
Epoch 902/1000
2023-10-24 14:35:55.013 
Epoch 902/1000 
	 loss: 27.8722, MinusLogProbMetric: 27.8722, val_loss: 28.4220, val_MinusLogProbMetric: 28.4220

Epoch 902: val_loss did not improve from 28.32122
196/196 - 35s - loss: 27.8722 - MinusLogProbMetric: 27.8722 - val_loss: 28.4220 - val_MinusLogProbMetric: 28.4220 - lr: 4.1667e-05 - 35s/epoch - 181ms/step
Epoch 903/1000
2023-10-24 14:36:31.184 
Epoch 903/1000 
	 loss: 27.8777, MinusLogProbMetric: 27.8777, val_loss: 28.3242, val_MinusLogProbMetric: 28.3242

Epoch 903: val_loss did not improve from 28.32122
196/196 - 36s - loss: 27.8777 - MinusLogProbMetric: 27.8777 - val_loss: 28.3242 - val_MinusLogProbMetric: 28.3242 - lr: 4.1667e-05 - 36s/epoch - 185ms/step
Epoch 904/1000
2023-10-24 14:37:11.042 
Epoch 904/1000 
	 loss: 27.8745, MinusLogProbMetric: 27.8745, val_loss: 28.3434, val_MinusLogProbMetric: 28.3434

Epoch 904: val_loss did not improve from 28.32122
196/196 - 40s - loss: 27.8745 - MinusLogProbMetric: 27.8745 - val_loss: 28.3434 - val_MinusLogProbMetric: 28.3434 - lr: 4.1667e-05 - 40s/epoch - 203ms/step
Epoch 905/1000
2023-10-24 14:37:48.040 
Epoch 905/1000 
	 loss: 27.8744, MinusLogProbMetric: 27.8744, val_loss: 28.3225, val_MinusLogProbMetric: 28.3225

Epoch 905: val_loss did not improve from 28.32122
196/196 - 37s - loss: 27.8744 - MinusLogProbMetric: 27.8744 - val_loss: 28.3225 - val_MinusLogProbMetric: 28.3225 - lr: 4.1667e-05 - 37s/epoch - 189ms/step
Epoch 906/1000
2023-10-24 14:38:24.009 
Epoch 906/1000 
	 loss: 27.8788, MinusLogProbMetric: 27.8788, val_loss: 28.3485, val_MinusLogProbMetric: 28.3485

Epoch 906: val_loss did not improve from 28.32122
196/196 - 36s - loss: 27.8788 - MinusLogProbMetric: 27.8788 - val_loss: 28.3485 - val_MinusLogProbMetric: 28.3485 - lr: 4.1667e-05 - 36s/epoch - 183ms/step
Epoch 907/1000
2023-10-24 14:38:59.885 
Epoch 907/1000 
	 loss: 27.8729, MinusLogProbMetric: 27.8729, val_loss: 28.3484, val_MinusLogProbMetric: 28.3484

Epoch 907: val_loss did not improve from 28.32122
196/196 - 36s - loss: 27.8729 - MinusLogProbMetric: 27.8729 - val_loss: 28.3484 - val_MinusLogProbMetric: 28.3484 - lr: 4.1667e-05 - 36s/epoch - 183ms/step
Epoch 908/1000
2023-10-24 14:39:37.321 
Epoch 908/1000 
	 loss: 27.8720, MinusLogProbMetric: 27.8720, val_loss: 28.3560, val_MinusLogProbMetric: 28.3560

Epoch 908: val_loss did not improve from 28.32122
196/196 - 37s - loss: 27.8720 - MinusLogProbMetric: 27.8720 - val_loss: 28.3560 - val_MinusLogProbMetric: 28.3560 - lr: 4.1667e-05 - 37s/epoch - 191ms/step
Epoch 909/1000
2023-10-24 14:40:16.004 
Epoch 909/1000 
	 loss: 27.8732, MinusLogProbMetric: 27.8732, val_loss: 28.3444, val_MinusLogProbMetric: 28.3444

Epoch 909: val_loss did not improve from 28.32122
196/196 - 39s - loss: 27.8732 - MinusLogProbMetric: 27.8732 - val_loss: 28.3444 - val_MinusLogProbMetric: 28.3444 - lr: 4.1667e-05 - 39s/epoch - 197ms/step
Epoch 910/1000
2023-10-24 14:40:51.893 
Epoch 910/1000 
	 loss: 27.8739, MinusLogProbMetric: 27.8739, val_loss: 28.3659, val_MinusLogProbMetric: 28.3659

Epoch 910: val_loss did not improve from 28.32122
196/196 - 36s - loss: 27.8739 - MinusLogProbMetric: 27.8739 - val_loss: 28.3659 - val_MinusLogProbMetric: 28.3659 - lr: 4.1667e-05 - 36s/epoch - 183ms/step
Epoch 911/1000
2023-10-24 14:41:27.844 
Epoch 911/1000 
	 loss: 27.8718, MinusLogProbMetric: 27.8718, val_loss: 28.3776, val_MinusLogProbMetric: 28.3776

Epoch 911: val_loss did not improve from 28.32122
196/196 - 36s - loss: 27.8718 - MinusLogProbMetric: 27.8718 - val_loss: 28.3776 - val_MinusLogProbMetric: 28.3776 - lr: 4.1667e-05 - 36s/epoch - 183ms/step
Epoch 912/1000
2023-10-24 14:42:05.796 
Epoch 912/1000 
	 loss: 27.8718, MinusLogProbMetric: 27.8718, val_loss: 28.3802, val_MinusLogProbMetric: 28.3802

Epoch 912: val_loss did not improve from 28.32122
196/196 - 38s - loss: 27.8718 - MinusLogProbMetric: 27.8718 - val_loss: 28.3802 - val_MinusLogProbMetric: 28.3802 - lr: 4.1667e-05 - 38s/epoch - 194ms/step
Epoch 913/1000
2023-10-24 14:42:45.527 
Epoch 913/1000 
	 loss: 27.8821, MinusLogProbMetric: 27.8821, val_loss: 28.3689, val_MinusLogProbMetric: 28.3689

Epoch 913: val_loss did not improve from 28.32122
196/196 - 40s - loss: 27.8821 - MinusLogProbMetric: 27.8821 - val_loss: 28.3689 - val_MinusLogProbMetric: 28.3689 - lr: 4.1667e-05 - 40s/epoch - 203ms/step
Epoch 914/1000
2023-10-24 14:43:21.634 
Epoch 914/1000 
	 loss: 27.8830, MinusLogProbMetric: 27.8830, val_loss: 28.3251, val_MinusLogProbMetric: 28.3251

Epoch 914: val_loss did not improve from 28.32122
196/196 - 36s - loss: 27.8830 - MinusLogProbMetric: 27.8830 - val_loss: 28.3251 - val_MinusLogProbMetric: 28.3251 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 915/1000
2023-10-24 14:43:57.730 
Epoch 915/1000 
	 loss: 27.8806, MinusLogProbMetric: 27.8806, val_loss: 28.3929, val_MinusLogProbMetric: 28.3929

Epoch 915: val_loss did not improve from 28.32122
196/196 - 36s - loss: 27.8806 - MinusLogProbMetric: 27.8806 - val_loss: 28.3929 - val_MinusLogProbMetric: 28.3929 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 916/1000
2023-10-24 14:44:34.094 
Epoch 916/1000 
	 loss: 27.8749, MinusLogProbMetric: 27.8749, val_loss: 28.4019, val_MinusLogProbMetric: 28.4019

Epoch 916: val_loss did not improve from 28.32122
196/196 - 36s - loss: 27.8749 - MinusLogProbMetric: 27.8749 - val_loss: 28.4019 - val_MinusLogProbMetric: 28.4019 - lr: 4.1667e-05 - 36s/epoch - 186ms/step
Epoch 917/1000
2023-10-24 14:45:13.914 
Epoch 917/1000 
	 loss: 27.8780, MinusLogProbMetric: 27.8780, val_loss: 28.3404, val_MinusLogProbMetric: 28.3404

Epoch 917: val_loss did not improve from 28.32122
196/196 - 40s - loss: 27.8780 - MinusLogProbMetric: 27.8780 - val_loss: 28.3404 - val_MinusLogProbMetric: 28.3404 - lr: 4.1667e-05 - 40s/epoch - 203ms/step
Epoch 918/1000
2023-10-24 14:45:52.191 
Epoch 918/1000 
	 loss: 27.8736, MinusLogProbMetric: 27.8736, val_loss: 28.3459, val_MinusLogProbMetric: 28.3459

Epoch 918: val_loss did not improve from 28.32122
196/196 - 38s - loss: 27.8736 - MinusLogProbMetric: 27.8736 - val_loss: 28.3459 - val_MinusLogProbMetric: 28.3459 - lr: 4.1667e-05 - 38s/epoch - 195ms/step
Epoch 919/1000
2023-10-24 14:46:28.644 
Epoch 919/1000 
	 loss: 27.8798, MinusLogProbMetric: 27.8798, val_loss: 28.3392, val_MinusLogProbMetric: 28.3392

Epoch 919: val_loss did not improve from 28.32122
196/196 - 36s - loss: 27.8798 - MinusLogProbMetric: 27.8798 - val_loss: 28.3392 - val_MinusLogProbMetric: 28.3392 - lr: 4.1667e-05 - 36s/epoch - 186ms/step
Epoch 920/1000
2023-10-24 14:47:04.712 
Epoch 920/1000 
	 loss: 27.8766, MinusLogProbMetric: 27.8766, val_loss: 28.3758, val_MinusLogProbMetric: 28.3758

Epoch 920: val_loss did not improve from 28.32122
196/196 - 36s - loss: 27.8766 - MinusLogProbMetric: 27.8766 - val_loss: 28.3758 - val_MinusLogProbMetric: 28.3758 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 921/1000
2023-10-24 14:47:42.928 
Epoch 921/1000 
	 loss: 27.8752, MinusLogProbMetric: 27.8752, val_loss: 28.3360, val_MinusLogProbMetric: 28.3360

Epoch 921: val_loss did not improve from 28.32122
196/196 - 38s - loss: 27.8752 - MinusLogProbMetric: 27.8752 - val_loss: 28.3360 - val_MinusLogProbMetric: 28.3360 - lr: 4.1667e-05 - 38s/epoch - 195ms/step
Epoch 922/1000
2023-10-24 14:48:23.767 
Epoch 922/1000 
	 loss: 27.8750, MinusLogProbMetric: 27.8750, val_loss: 28.3358, val_MinusLogProbMetric: 28.3358

Epoch 922: val_loss did not improve from 28.32122
196/196 - 41s - loss: 27.8750 - MinusLogProbMetric: 27.8750 - val_loss: 28.3358 - val_MinusLogProbMetric: 28.3358 - lr: 4.1667e-05 - 41s/epoch - 208ms/step
Epoch 923/1000
2023-10-24 14:49:05.540 
Epoch 923/1000 
	 loss: 27.8784, MinusLogProbMetric: 27.8784, val_loss: 28.3751, val_MinusLogProbMetric: 28.3751

Epoch 923: val_loss did not improve from 28.32122
196/196 - 42s - loss: 27.8784 - MinusLogProbMetric: 27.8784 - val_loss: 28.3751 - val_MinusLogProbMetric: 28.3751 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 924/1000
2023-10-24 14:49:47.301 
Epoch 924/1000 
	 loss: 27.8758, MinusLogProbMetric: 27.8758, val_loss: 28.3514, val_MinusLogProbMetric: 28.3514

Epoch 924: val_loss did not improve from 28.32122
196/196 - 42s - loss: 27.8758 - MinusLogProbMetric: 27.8758 - val_loss: 28.3514 - val_MinusLogProbMetric: 28.3514 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 925/1000
2023-10-24 14:50:28.030 
Epoch 925/1000 
	 loss: 27.8697, MinusLogProbMetric: 27.8697, val_loss: 28.3601, val_MinusLogProbMetric: 28.3601

Epoch 925: val_loss did not improve from 28.32122
196/196 - 41s - loss: 27.8697 - MinusLogProbMetric: 27.8697 - val_loss: 28.3601 - val_MinusLogProbMetric: 28.3601 - lr: 4.1667e-05 - 41s/epoch - 208ms/step
Epoch 926/1000
2023-10-24 14:51:06.234 
Epoch 926/1000 
	 loss: 27.8856, MinusLogProbMetric: 27.8856, val_loss: 28.3508, val_MinusLogProbMetric: 28.3508

Epoch 926: val_loss did not improve from 28.32122
196/196 - 38s - loss: 27.8856 - MinusLogProbMetric: 27.8856 - val_loss: 28.3508 - val_MinusLogProbMetric: 28.3508 - lr: 4.1667e-05 - 38s/epoch - 195ms/step
Epoch 927/1000
2023-10-24 14:51:43.052 
Epoch 927/1000 
	 loss: 27.8740, MinusLogProbMetric: 27.8740, val_loss: 28.3536, val_MinusLogProbMetric: 28.3536

Epoch 927: val_loss did not improve from 28.32122
196/196 - 37s - loss: 27.8740 - MinusLogProbMetric: 27.8740 - val_loss: 28.3536 - val_MinusLogProbMetric: 28.3536 - lr: 4.1667e-05 - 37s/epoch - 188ms/step
Epoch 928/1000
2023-10-24 14:52:20.006 
Epoch 928/1000 
	 loss: 27.8788, MinusLogProbMetric: 27.8788, val_loss: 28.4069, val_MinusLogProbMetric: 28.4069

Epoch 928: val_loss did not improve from 28.32122
196/196 - 37s - loss: 27.8788 - MinusLogProbMetric: 27.8788 - val_loss: 28.4069 - val_MinusLogProbMetric: 28.4069 - lr: 4.1667e-05 - 37s/epoch - 189ms/step
Epoch 929/1000
2023-10-24 14:52:59.414 
Epoch 929/1000 
	 loss: 27.8817, MinusLogProbMetric: 27.8817, val_loss: 28.3522, val_MinusLogProbMetric: 28.3522

Epoch 929: val_loss did not improve from 28.32122
196/196 - 39s - loss: 27.8817 - MinusLogProbMetric: 27.8817 - val_loss: 28.3522 - val_MinusLogProbMetric: 28.3522 - lr: 4.1667e-05 - 39s/epoch - 201ms/step
Epoch 930/1000
2023-10-24 14:53:40.019 
Epoch 930/1000 
	 loss: 27.8742, MinusLogProbMetric: 27.8742, val_loss: 28.3501, val_MinusLogProbMetric: 28.3501

Epoch 930: val_loss did not improve from 28.32122
196/196 - 41s - loss: 27.8742 - MinusLogProbMetric: 27.8742 - val_loss: 28.3501 - val_MinusLogProbMetric: 28.3501 - lr: 4.1667e-05 - 41s/epoch - 207ms/step
Epoch 931/1000
2023-10-24 14:54:15.237 
Epoch 931/1000 
	 loss: 27.8716, MinusLogProbMetric: 27.8716, val_loss: 28.3385, val_MinusLogProbMetric: 28.3385

Epoch 931: val_loss did not improve from 28.32122
196/196 - 35s - loss: 27.8716 - MinusLogProbMetric: 27.8716 - val_loss: 28.3385 - val_MinusLogProbMetric: 28.3385 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 932/1000
2023-10-24 14:54:50.393 
Epoch 932/1000 
	 loss: 27.8756, MinusLogProbMetric: 27.8756, val_loss: 28.3555, val_MinusLogProbMetric: 28.3555

Epoch 932: val_loss did not improve from 28.32122
196/196 - 35s - loss: 27.8756 - MinusLogProbMetric: 27.8756 - val_loss: 28.3555 - val_MinusLogProbMetric: 28.3555 - lr: 4.1667e-05 - 35s/epoch - 179ms/step
Epoch 933/1000
2023-10-24 14:55:27.431 
Epoch 933/1000 
	 loss: 27.8758, MinusLogProbMetric: 27.8758, val_loss: 28.3535, val_MinusLogProbMetric: 28.3535

Epoch 933: val_loss did not improve from 28.32122
196/196 - 37s - loss: 27.8758 - MinusLogProbMetric: 27.8758 - val_loss: 28.3535 - val_MinusLogProbMetric: 28.3535 - lr: 4.1667e-05 - 37s/epoch - 189ms/step
Epoch 934/1000
2023-10-24 14:56:06.706 
Epoch 934/1000 
	 loss: 27.8721, MinusLogProbMetric: 27.8721, val_loss: 28.3603, val_MinusLogProbMetric: 28.3603

Epoch 934: val_loss did not improve from 28.32122
196/196 - 39s - loss: 27.8721 - MinusLogProbMetric: 27.8721 - val_loss: 28.3603 - val_MinusLogProbMetric: 28.3603 - lr: 4.1667e-05 - 39s/epoch - 200ms/step
Epoch 935/1000
2023-10-24 14:56:48.010 
Epoch 935/1000 
	 loss: 27.8721, MinusLogProbMetric: 27.8721, val_loss: 28.3691, val_MinusLogProbMetric: 28.3691

Epoch 935: val_loss did not improve from 28.32122
196/196 - 41s - loss: 27.8721 - MinusLogProbMetric: 27.8721 - val_loss: 28.3691 - val_MinusLogProbMetric: 28.3691 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 936/1000
2023-10-24 14:57:26.024 
Epoch 936/1000 
	 loss: 27.8759, MinusLogProbMetric: 27.8759, val_loss: 28.3473, val_MinusLogProbMetric: 28.3473

Epoch 936: val_loss did not improve from 28.32122
196/196 - 38s - loss: 27.8759 - MinusLogProbMetric: 27.8759 - val_loss: 28.3473 - val_MinusLogProbMetric: 28.3473 - lr: 4.1667e-05 - 38s/epoch - 194ms/step
Epoch 937/1000
2023-10-24 14:58:08.312 
Epoch 937/1000 
	 loss: 27.8766, MinusLogProbMetric: 27.8766, val_loss: 28.3644, val_MinusLogProbMetric: 28.3644

Epoch 937: val_loss did not improve from 28.32122
196/196 - 42s - loss: 27.8766 - MinusLogProbMetric: 27.8766 - val_loss: 28.3644 - val_MinusLogProbMetric: 28.3644 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 938/1000
2023-10-24 14:58:50.138 
Epoch 938/1000 
	 loss: 27.8762, MinusLogProbMetric: 27.8762, val_loss: 28.3753, val_MinusLogProbMetric: 28.3753

Epoch 938: val_loss did not improve from 28.32122
196/196 - 42s - loss: 27.8762 - MinusLogProbMetric: 27.8762 - val_loss: 28.3753 - val_MinusLogProbMetric: 28.3753 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 939/1000
2023-10-24 14:59:29.739 
Epoch 939/1000 
	 loss: 27.8790, MinusLogProbMetric: 27.8790, val_loss: 28.3595, val_MinusLogProbMetric: 28.3595

Epoch 939: val_loss did not improve from 28.32122
196/196 - 40s - loss: 27.8790 - MinusLogProbMetric: 27.8790 - val_loss: 28.3595 - val_MinusLogProbMetric: 28.3595 - lr: 4.1667e-05 - 40s/epoch - 202ms/step
Epoch 940/1000
2023-10-24 15:00:11.644 
Epoch 940/1000 
	 loss: 27.8734, MinusLogProbMetric: 27.8734, val_loss: 28.3396, val_MinusLogProbMetric: 28.3396

Epoch 940: val_loss did not improve from 28.32122
196/196 - 42s - loss: 27.8734 - MinusLogProbMetric: 27.8734 - val_loss: 28.3396 - val_MinusLogProbMetric: 28.3396 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 941/1000
2023-10-24 15:00:52.274 
Epoch 941/1000 
	 loss: 27.8701, MinusLogProbMetric: 27.8701, val_loss: 28.3248, val_MinusLogProbMetric: 28.3248

Epoch 941: val_loss did not improve from 28.32122
196/196 - 41s - loss: 27.8701 - MinusLogProbMetric: 27.8701 - val_loss: 28.3248 - val_MinusLogProbMetric: 28.3248 - lr: 4.1667e-05 - 41s/epoch - 207ms/step
Epoch 942/1000
2023-10-24 15:01:33.850 
Epoch 942/1000 
	 loss: 27.8726, MinusLogProbMetric: 27.8726, val_loss: 28.3685, val_MinusLogProbMetric: 28.3685

Epoch 942: val_loss did not improve from 28.32122
196/196 - 42s - loss: 27.8726 - MinusLogProbMetric: 27.8726 - val_loss: 28.3685 - val_MinusLogProbMetric: 28.3685 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 943/1000
2023-10-24 15:02:14.459 
Epoch 943/1000 
	 loss: 27.8661, MinusLogProbMetric: 27.8661, val_loss: 28.3434, val_MinusLogProbMetric: 28.3434

Epoch 943: val_loss did not improve from 28.32122
196/196 - 41s - loss: 27.8661 - MinusLogProbMetric: 27.8661 - val_loss: 28.3434 - val_MinusLogProbMetric: 28.3434 - lr: 4.1667e-05 - 41s/epoch - 207ms/step
Epoch 944/1000
2023-10-24 15:02:55.863 
Epoch 944/1000 
	 loss: 27.8713, MinusLogProbMetric: 27.8713, val_loss: 28.3481, val_MinusLogProbMetric: 28.3481

Epoch 944: val_loss did not improve from 28.32122
196/196 - 41s - loss: 27.8713 - MinusLogProbMetric: 27.8713 - val_loss: 28.3481 - val_MinusLogProbMetric: 28.3481 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 945/1000
2023-10-24 15:03:38.254 
Epoch 945/1000 
	 loss: 27.8730, MinusLogProbMetric: 27.8730, val_loss: 28.3549, val_MinusLogProbMetric: 28.3549

Epoch 945: val_loss did not improve from 28.32122
196/196 - 42s - loss: 27.8730 - MinusLogProbMetric: 27.8730 - val_loss: 28.3549 - val_MinusLogProbMetric: 28.3549 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 946/1000
2023-10-24 15:04:13.537 
Epoch 946/1000 
	 loss: 27.8796, MinusLogProbMetric: 27.8796, val_loss: 28.3511, val_MinusLogProbMetric: 28.3511

Epoch 946: val_loss did not improve from 28.32122
196/196 - 35s - loss: 27.8796 - MinusLogProbMetric: 27.8796 - val_loss: 28.3511 - val_MinusLogProbMetric: 28.3511 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 947/1000
2023-10-24 15:04:49.282 
Epoch 947/1000 
	 loss: 27.8739, MinusLogProbMetric: 27.8739, val_loss: 28.3187, val_MinusLogProbMetric: 28.3187

Epoch 947: val_loss improved from 28.32122 to 28.31873, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 36s - loss: 27.8739 - MinusLogProbMetric: 27.8739 - val_loss: 28.3187 - val_MinusLogProbMetric: 28.3187 - lr: 4.1667e-05 - 36s/epoch - 185ms/step
Epoch 948/1000
2023-10-24 15:05:25.940 
Epoch 948/1000 
	 loss: 27.8706, MinusLogProbMetric: 27.8706, val_loss: 28.3493, val_MinusLogProbMetric: 28.3493

Epoch 948: val_loss did not improve from 28.31873
196/196 - 36s - loss: 27.8706 - MinusLogProbMetric: 27.8706 - val_loss: 28.3493 - val_MinusLogProbMetric: 28.3493 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 949/1000
2023-10-24 15:06:06.350 
Epoch 949/1000 
	 loss: 27.8692, MinusLogProbMetric: 27.8692, val_loss: 28.3360, val_MinusLogProbMetric: 28.3360

Epoch 949: val_loss did not improve from 28.31873
196/196 - 40s - loss: 27.8692 - MinusLogProbMetric: 27.8692 - val_loss: 28.3360 - val_MinusLogProbMetric: 28.3360 - lr: 4.1667e-05 - 40s/epoch - 206ms/step
Epoch 950/1000
2023-10-24 15:06:48.085 
Epoch 950/1000 
	 loss: 27.8665, MinusLogProbMetric: 27.8665, val_loss: 28.3575, val_MinusLogProbMetric: 28.3575

Epoch 950: val_loss did not improve from 28.31873
196/196 - 42s - loss: 27.8665 - MinusLogProbMetric: 27.8665 - val_loss: 28.3575 - val_MinusLogProbMetric: 28.3575 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 951/1000
2023-10-24 15:07:28.627 
Epoch 951/1000 
	 loss: 27.8749, MinusLogProbMetric: 27.8749, val_loss: 28.3601, val_MinusLogProbMetric: 28.3601

Epoch 951: val_loss did not improve from 28.31873
196/196 - 41s - loss: 27.8749 - MinusLogProbMetric: 27.8749 - val_loss: 28.3601 - val_MinusLogProbMetric: 28.3601 - lr: 4.1667e-05 - 41s/epoch - 207ms/step
Epoch 952/1000
2023-10-24 15:08:10.358 
Epoch 952/1000 
	 loss: 27.8711, MinusLogProbMetric: 27.8711, val_loss: 28.3660, val_MinusLogProbMetric: 28.3660

Epoch 952: val_loss did not improve from 28.31873
196/196 - 42s - loss: 27.8711 - MinusLogProbMetric: 27.8711 - val_loss: 28.3660 - val_MinusLogProbMetric: 28.3660 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 953/1000
2023-10-24 15:08:52.057 
Epoch 953/1000 
	 loss: 27.8700, MinusLogProbMetric: 27.8700, val_loss: 28.3563, val_MinusLogProbMetric: 28.3563

Epoch 953: val_loss did not improve from 28.31873
196/196 - 42s - loss: 27.8700 - MinusLogProbMetric: 27.8700 - val_loss: 28.3563 - val_MinusLogProbMetric: 28.3563 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 954/1000
2023-10-24 15:09:33.210 
Epoch 954/1000 
	 loss: 27.8760, MinusLogProbMetric: 27.8760, val_loss: 28.3526, val_MinusLogProbMetric: 28.3526

Epoch 954: val_loss did not improve from 28.31873
196/196 - 41s - loss: 27.8760 - MinusLogProbMetric: 27.8760 - val_loss: 28.3526 - val_MinusLogProbMetric: 28.3526 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 955/1000
2023-10-24 15:10:14.620 
Epoch 955/1000 
	 loss: 27.8696, MinusLogProbMetric: 27.8696, val_loss: 28.3445, val_MinusLogProbMetric: 28.3445

Epoch 955: val_loss did not improve from 28.31873
196/196 - 41s - loss: 27.8696 - MinusLogProbMetric: 27.8696 - val_loss: 28.3445 - val_MinusLogProbMetric: 28.3445 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 956/1000
2023-10-24 15:10:55.828 
Epoch 956/1000 
	 loss: 27.8730, MinusLogProbMetric: 27.8730, val_loss: 28.3606, val_MinusLogProbMetric: 28.3606

Epoch 956: val_loss did not improve from 28.31873
196/196 - 41s - loss: 27.8730 - MinusLogProbMetric: 27.8730 - val_loss: 28.3606 - val_MinusLogProbMetric: 28.3606 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 957/1000
2023-10-24 15:11:37.805 
Epoch 957/1000 
	 loss: 27.8698, MinusLogProbMetric: 27.8698, val_loss: 28.3498, val_MinusLogProbMetric: 28.3498

Epoch 957: val_loss did not improve from 28.31873
196/196 - 42s - loss: 27.8698 - MinusLogProbMetric: 27.8698 - val_loss: 28.3498 - val_MinusLogProbMetric: 28.3498 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 958/1000
2023-10-24 15:12:19.375 
Epoch 958/1000 
	 loss: 27.8828, MinusLogProbMetric: 27.8828, val_loss: 28.3862, val_MinusLogProbMetric: 28.3862

Epoch 958: val_loss did not improve from 28.31873
196/196 - 42s - loss: 27.8828 - MinusLogProbMetric: 27.8828 - val_loss: 28.3862 - val_MinusLogProbMetric: 28.3862 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 959/1000
2023-10-24 15:13:00.992 
Epoch 959/1000 
	 loss: 27.8698, MinusLogProbMetric: 27.8698, val_loss: 28.3364, val_MinusLogProbMetric: 28.3364

Epoch 959: val_loss did not improve from 28.31873
196/196 - 42s - loss: 27.8698 - MinusLogProbMetric: 27.8698 - val_loss: 28.3364 - val_MinusLogProbMetric: 28.3364 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 960/1000
2023-10-24 15:13:41.890 
Epoch 960/1000 
	 loss: 27.8712, MinusLogProbMetric: 27.8712, val_loss: 28.3350, val_MinusLogProbMetric: 28.3350

Epoch 960: val_loss did not improve from 28.31873
196/196 - 41s - loss: 27.8712 - MinusLogProbMetric: 27.8712 - val_loss: 28.3350 - val_MinusLogProbMetric: 28.3350 - lr: 4.1667e-05 - 41s/epoch - 209ms/step
Epoch 961/1000
2023-10-24 15:14:23.588 
Epoch 961/1000 
	 loss: 27.8722, MinusLogProbMetric: 27.8722, val_loss: 28.3534, val_MinusLogProbMetric: 28.3534

Epoch 961: val_loss did not improve from 28.31873
196/196 - 42s - loss: 27.8722 - MinusLogProbMetric: 27.8722 - val_loss: 28.3534 - val_MinusLogProbMetric: 28.3534 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 962/1000
2023-10-24 15:15:05.529 
Epoch 962/1000 
	 loss: 27.8695, MinusLogProbMetric: 27.8695, val_loss: 28.3533, val_MinusLogProbMetric: 28.3533

Epoch 962: val_loss did not improve from 28.31873
196/196 - 42s - loss: 27.8695 - MinusLogProbMetric: 27.8695 - val_loss: 28.3533 - val_MinusLogProbMetric: 28.3533 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 963/1000
2023-10-24 15:15:46.908 
Epoch 963/1000 
	 loss: 27.8723, MinusLogProbMetric: 27.8723, val_loss: 28.3515, val_MinusLogProbMetric: 28.3515

Epoch 963: val_loss did not improve from 28.31873
196/196 - 41s - loss: 27.8723 - MinusLogProbMetric: 27.8723 - val_loss: 28.3515 - val_MinusLogProbMetric: 28.3515 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 964/1000
2023-10-24 15:16:28.367 
Epoch 964/1000 
	 loss: 27.8662, MinusLogProbMetric: 27.8662, val_loss: 28.4003, val_MinusLogProbMetric: 28.4003

Epoch 964: val_loss did not improve from 28.31873
196/196 - 41s - loss: 27.8662 - MinusLogProbMetric: 27.8662 - val_loss: 28.4003 - val_MinusLogProbMetric: 28.4003 - lr: 4.1667e-05 - 41s/epoch - 212ms/step
Epoch 965/1000
2023-10-24 15:17:08.900 
Epoch 965/1000 
	 loss: 27.8694, MinusLogProbMetric: 27.8694, val_loss: 28.3981, val_MinusLogProbMetric: 28.3981

Epoch 965: val_loss did not improve from 28.31873
196/196 - 41s - loss: 27.8694 - MinusLogProbMetric: 27.8694 - val_loss: 28.3981 - val_MinusLogProbMetric: 28.3981 - lr: 4.1667e-05 - 41s/epoch - 207ms/step
Epoch 966/1000
2023-10-24 15:17:47.231 
Epoch 966/1000 
	 loss: 27.8680, MinusLogProbMetric: 27.8680, val_loss: 28.3556, val_MinusLogProbMetric: 28.3556

Epoch 966: val_loss did not improve from 28.31873
196/196 - 38s - loss: 27.8680 - MinusLogProbMetric: 27.8680 - val_loss: 28.3556 - val_MinusLogProbMetric: 28.3556 - lr: 4.1667e-05 - 38s/epoch - 196ms/step
Epoch 967/1000
2023-10-24 15:18:29.102 
Epoch 967/1000 
	 loss: 27.8630, MinusLogProbMetric: 27.8630, val_loss: 28.3411, val_MinusLogProbMetric: 28.3411

Epoch 967: val_loss did not improve from 28.31873
196/196 - 42s - loss: 27.8630 - MinusLogProbMetric: 27.8630 - val_loss: 28.3411 - val_MinusLogProbMetric: 28.3411 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 968/1000
2023-10-24 15:19:10.636 
Epoch 968/1000 
	 loss: 27.8743, MinusLogProbMetric: 27.8743, val_loss: 28.3670, val_MinusLogProbMetric: 28.3670

Epoch 968: val_loss did not improve from 28.31873
196/196 - 42s - loss: 27.8743 - MinusLogProbMetric: 27.8743 - val_loss: 28.3670 - val_MinusLogProbMetric: 28.3670 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 969/1000
2023-10-24 15:19:52.310 
Epoch 969/1000 
	 loss: 27.8689, MinusLogProbMetric: 27.8689, val_loss: 28.3293, val_MinusLogProbMetric: 28.3293

Epoch 969: val_loss did not improve from 28.31873
196/196 - 42s - loss: 27.8689 - MinusLogProbMetric: 27.8689 - val_loss: 28.3293 - val_MinusLogProbMetric: 28.3293 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 970/1000
2023-10-24 15:20:33.810 
Epoch 970/1000 
	 loss: 27.8707, MinusLogProbMetric: 27.8707, val_loss: 28.3463, val_MinusLogProbMetric: 28.3463

Epoch 970: val_loss did not improve from 28.31873
196/196 - 41s - loss: 27.8707 - MinusLogProbMetric: 27.8707 - val_loss: 28.3463 - val_MinusLogProbMetric: 28.3463 - lr: 4.1667e-05 - 41s/epoch - 212ms/step
Epoch 971/1000
2023-10-24 15:21:15.398 
Epoch 971/1000 
	 loss: 27.8659, MinusLogProbMetric: 27.8659, val_loss: 28.3298, val_MinusLogProbMetric: 28.3298

Epoch 971: val_loss did not improve from 28.31873
196/196 - 42s - loss: 27.8659 - MinusLogProbMetric: 27.8659 - val_loss: 28.3298 - val_MinusLogProbMetric: 28.3298 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 972/1000
2023-10-24 15:21:56.789 
Epoch 972/1000 
	 loss: 27.8702, MinusLogProbMetric: 27.8702, val_loss: 28.3676, val_MinusLogProbMetric: 28.3676

Epoch 972: val_loss did not improve from 28.31873
196/196 - 41s - loss: 27.8702 - MinusLogProbMetric: 27.8702 - val_loss: 28.3676 - val_MinusLogProbMetric: 28.3676 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 973/1000
2023-10-24 15:22:38.456 
Epoch 973/1000 
	 loss: 27.8688, MinusLogProbMetric: 27.8688, val_loss: 28.3385, val_MinusLogProbMetric: 28.3385

Epoch 973: val_loss did not improve from 28.31873
196/196 - 42s - loss: 27.8688 - MinusLogProbMetric: 27.8688 - val_loss: 28.3385 - val_MinusLogProbMetric: 28.3385 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 974/1000
2023-10-24 15:23:20.034 
Epoch 974/1000 
	 loss: 27.8647, MinusLogProbMetric: 27.8647, val_loss: 28.3579, val_MinusLogProbMetric: 28.3579

Epoch 974: val_loss did not improve from 28.31873
196/196 - 42s - loss: 27.8647 - MinusLogProbMetric: 27.8647 - val_loss: 28.3579 - val_MinusLogProbMetric: 28.3579 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 975/1000
2023-10-24 15:24:01.794 
Epoch 975/1000 
	 loss: 27.8635, MinusLogProbMetric: 27.8635, val_loss: 28.3519, val_MinusLogProbMetric: 28.3519

Epoch 975: val_loss did not improve from 28.31873
196/196 - 42s - loss: 27.8635 - MinusLogProbMetric: 27.8635 - val_loss: 28.3519 - val_MinusLogProbMetric: 28.3519 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 976/1000
2023-10-24 15:24:42.915 
Epoch 976/1000 
	 loss: 27.8684, MinusLogProbMetric: 27.8684, val_loss: 28.3501, val_MinusLogProbMetric: 28.3501

Epoch 976: val_loss did not improve from 28.31873
196/196 - 41s - loss: 27.8684 - MinusLogProbMetric: 27.8684 - val_loss: 28.3501 - val_MinusLogProbMetric: 28.3501 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 977/1000
2023-10-24 15:25:24.939 
Epoch 977/1000 
	 loss: 27.8656, MinusLogProbMetric: 27.8656, val_loss: 28.3630, val_MinusLogProbMetric: 28.3630

Epoch 977: val_loss did not improve from 28.31873
196/196 - 42s - loss: 27.8656 - MinusLogProbMetric: 27.8656 - val_loss: 28.3630 - val_MinusLogProbMetric: 28.3630 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 978/1000
2023-10-24 15:26:06.305 
Epoch 978/1000 
	 loss: 27.8681, MinusLogProbMetric: 27.8681, val_loss: 28.3351, val_MinusLogProbMetric: 28.3351

Epoch 978: val_loss did not improve from 28.31873
196/196 - 41s - loss: 27.8681 - MinusLogProbMetric: 27.8681 - val_loss: 28.3351 - val_MinusLogProbMetric: 28.3351 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 979/1000
2023-10-24 15:26:48.247 
Epoch 979/1000 
	 loss: 27.8642, MinusLogProbMetric: 27.8642, val_loss: 28.3895, val_MinusLogProbMetric: 28.3895

Epoch 979: val_loss did not improve from 28.31873
196/196 - 42s - loss: 27.8642 - MinusLogProbMetric: 27.8642 - val_loss: 28.3895 - val_MinusLogProbMetric: 28.3895 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 980/1000
2023-10-24 15:27:30.022 
Epoch 980/1000 
	 loss: 27.8675, MinusLogProbMetric: 27.8675, val_loss: 28.3378, val_MinusLogProbMetric: 28.3378

Epoch 980: val_loss did not improve from 28.31873
196/196 - 42s - loss: 27.8675 - MinusLogProbMetric: 27.8675 - val_loss: 28.3378 - val_MinusLogProbMetric: 28.3378 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 981/1000
2023-10-24 15:28:11.358 
Epoch 981/1000 
	 loss: 27.8702, MinusLogProbMetric: 27.8702, val_loss: 28.3382, val_MinusLogProbMetric: 28.3382

Epoch 981: val_loss did not improve from 28.31873
196/196 - 41s - loss: 27.8702 - MinusLogProbMetric: 27.8702 - val_loss: 28.3382 - val_MinusLogProbMetric: 28.3382 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 982/1000
2023-10-24 15:28:52.506 
Epoch 982/1000 
	 loss: 27.8724, MinusLogProbMetric: 27.8724, val_loss: 28.3144, val_MinusLogProbMetric: 28.3144

Epoch 982: val_loss improved from 28.31873 to 28.31439, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_339/weights/best_weights.h5
196/196 - 42s - loss: 27.8724 - MinusLogProbMetric: 27.8724 - val_loss: 28.3144 - val_MinusLogProbMetric: 28.3144 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 983/1000
2023-10-24 15:29:34.084 
Epoch 983/1000 
	 loss: 27.8616, MinusLogProbMetric: 27.8616, val_loss: 28.3252, val_MinusLogProbMetric: 28.3252

Epoch 983: val_loss did not improve from 28.31439
196/196 - 41s - loss: 27.8616 - MinusLogProbMetric: 27.8616 - val_loss: 28.3252 - val_MinusLogProbMetric: 28.3252 - lr: 4.1667e-05 - 41s/epoch - 209ms/step
Epoch 984/1000
2023-10-24 15:30:15.140 
Epoch 984/1000 
	 loss: 27.8661, MinusLogProbMetric: 27.8661, val_loss: 28.3270, val_MinusLogProbMetric: 28.3270

Epoch 984: val_loss did not improve from 28.31439
196/196 - 41s - loss: 27.8661 - MinusLogProbMetric: 27.8661 - val_loss: 28.3270 - val_MinusLogProbMetric: 28.3270 - lr: 4.1667e-05 - 41s/epoch - 209ms/step
Epoch 985/1000
2023-10-24 15:30:56.497 
Epoch 985/1000 
	 loss: 27.8671, MinusLogProbMetric: 27.8671, val_loss: 28.3427, val_MinusLogProbMetric: 28.3427

Epoch 985: val_loss did not improve from 28.31439
196/196 - 41s - loss: 27.8671 - MinusLogProbMetric: 27.8671 - val_loss: 28.3427 - val_MinusLogProbMetric: 28.3427 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 986/1000
2023-10-24 15:31:34.199 
Epoch 986/1000 
	 loss: 27.8687, MinusLogProbMetric: 27.8687, val_loss: 28.3339, val_MinusLogProbMetric: 28.3339

Epoch 986: val_loss did not improve from 28.31439
196/196 - 38s - loss: 27.8687 - MinusLogProbMetric: 27.8687 - val_loss: 28.3339 - val_MinusLogProbMetric: 28.3339 - lr: 4.1667e-05 - 38s/epoch - 192ms/step
Epoch 987/1000
2023-10-24 15:32:14.795 
Epoch 987/1000 
	 loss: 27.8626, MinusLogProbMetric: 27.8626, val_loss: 28.3838, val_MinusLogProbMetric: 28.3838

Epoch 987: val_loss did not improve from 28.31439
196/196 - 41s - loss: 27.8626 - MinusLogProbMetric: 27.8626 - val_loss: 28.3838 - val_MinusLogProbMetric: 28.3838 - lr: 4.1667e-05 - 41s/epoch - 207ms/step
Epoch 988/1000
2023-10-24 15:32:56.989 
Epoch 988/1000 
	 loss: 27.8717, MinusLogProbMetric: 27.8717, val_loss: 28.3586, val_MinusLogProbMetric: 28.3586

Epoch 988: val_loss did not improve from 28.31439
196/196 - 42s - loss: 27.8717 - MinusLogProbMetric: 27.8717 - val_loss: 28.3586 - val_MinusLogProbMetric: 28.3586 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 989/1000
2023-10-24 15:33:38.365 
Epoch 989/1000 
	 loss: 27.8662, MinusLogProbMetric: 27.8662, val_loss: 28.3680, val_MinusLogProbMetric: 28.3680

Epoch 989: val_loss did not improve from 28.31439
196/196 - 41s - loss: 27.8662 - MinusLogProbMetric: 27.8662 - val_loss: 28.3680 - val_MinusLogProbMetric: 28.3680 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 990/1000
2023-10-24 15:34:20.055 
Epoch 990/1000 
	 loss: 27.8619, MinusLogProbMetric: 27.8619, val_loss: 28.3456, val_MinusLogProbMetric: 28.3456

Epoch 990: val_loss did not improve from 28.31439
196/196 - 42s - loss: 27.8619 - MinusLogProbMetric: 27.8619 - val_loss: 28.3456 - val_MinusLogProbMetric: 28.3456 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 991/1000
2023-10-24 15:35:02.001 
Epoch 991/1000 
	 loss: 27.8647, MinusLogProbMetric: 27.8647, val_loss: 28.3461, val_MinusLogProbMetric: 28.3461

Epoch 991: val_loss did not improve from 28.31439
196/196 - 42s - loss: 27.8647 - MinusLogProbMetric: 27.8647 - val_loss: 28.3461 - val_MinusLogProbMetric: 28.3461 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 992/1000
2023-10-24 15:35:43.804 
Epoch 992/1000 
	 loss: 27.8630, MinusLogProbMetric: 27.8630, val_loss: 28.3309, val_MinusLogProbMetric: 28.3309

Epoch 992: val_loss did not improve from 28.31439
196/196 - 42s - loss: 27.8630 - MinusLogProbMetric: 27.8630 - val_loss: 28.3309 - val_MinusLogProbMetric: 28.3309 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 993/1000
2023-10-24 15:36:25.121 
Epoch 993/1000 
	 loss: 27.8650, MinusLogProbMetric: 27.8650, val_loss: 28.3468, val_MinusLogProbMetric: 28.3468

Epoch 993: val_loss did not improve from 28.31439
196/196 - 41s - loss: 27.8650 - MinusLogProbMetric: 27.8650 - val_loss: 28.3468 - val_MinusLogProbMetric: 28.3468 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 994/1000
2023-10-24 15:37:07.164 
Epoch 994/1000 
	 loss: 27.8572, MinusLogProbMetric: 27.8572, val_loss: 28.3493, val_MinusLogProbMetric: 28.3493

Epoch 994: val_loss did not improve from 28.31439
196/196 - 42s - loss: 27.8572 - MinusLogProbMetric: 27.8572 - val_loss: 28.3493 - val_MinusLogProbMetric: 28.3493 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 995/1000
2023-10-24 15:37:48.545 
Epoch 995/1000 
	 loss: 27.8612, MinusLogProbMetric: 27.8612, val_loss: 28.3444, val_MinusLogProbMetric: 28.3444

Epoch 995: val_loss did not improve from 28.31439
196/196 - 41s - loss: 27.8612 - MinusLogProbMetric: 27.8612 - val_loss: 28.3444 - val_MinusLogProbMetric: 28.3444 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 996/1000
2023-10-24 15:38:30.121 
Epoch 996/1000 
	 loss: 27.8621, MinusLogProbMetric: 27.8621, val_loss: 28.3263, val_MinusLogProbMetric: 28.3263

Epoch 996: val_loss did not improve from 28.31439
196/196 - 42s - loss: 27.8621 - MinusLogProbMetric: 27.8621 - val_loss: 28.3263 - val_MinusLogProbMetric: 28.3263 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 997/1000
2023-10-24 15:39:12.436 
Epoch 997/1000 
	 loss: 27.8634, MinusLogProbMetric: 27.8634, val_loss: 28.3294, val_MinusLogProbMetric: 28.3294

Epoch 997: val_loss did not improve from 28.31439
196/196 - 42s - loss: 27.8634 - MinusLogProbMetric: 27.8634 - val_loss: 28.3294 - val_MinusLogProbMetric: 28.3294 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 998/1000
2023-10-24 15:39:53.746 
Epoch 998/1000 
	 loss: 27.8616, MinusLogProbMetric: 27.8616, val_loss: 28.3859, val_MinusLogProbMetric: 28.3859

Epoch 998: val_loss did not improve from 28.31439
196/196 - 41s - loss: 27.8616 - MinusLogProbMetric: 27.8616 - val_loss: 28.3859 - val_MinusLogProbMetric: 28.3859 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 999/1000
2023-10-24 15:40:35.450 
Epoch 999/1000 
	 loss: 27.8666, MinusLogProbMetric: 27.8666, val_loss: 28.3323, val_MinusLogProbMetric: 28.3323

Epoch 999: val_loss did not improve from 28.31439
196/196 - 42s - loss: 27.8666 - MinusLogProbMetric: 27.8666 - val_loss: 28.3323 - val_MinusLogProbMetric: 28.3323 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 1000/1000
2023-10-24 15:41:17.111 
Epoch 1000/1000 
	 loss: 27.8609, MinusLogProbMetric: 27.8609, val_loss: 28.3229, val_MinusLogProbMetric: 28.3229

Epoch 1000: val_loss did not improve from 28.31439
196/196 - 42s - loss: 27.8609 - MinusLogProbMetric: 27.8609 - val_loss: 28.3229 - val_MinusLogProbMetric: 28.3229 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 377.
Model trained in 40143.35 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.72 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.01 s.
===========
Run 339/720 done in 40220.76 s.
===========

Directory ../../results/CsplineN_new/run_340/ already exists.
Skipping it.
===========
Run 340/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_341/ already exists.
Skipping it.
===========
Run 341/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_342/ already exists.
Skipping it.
===========
Run 342/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_343/ already exists.
Skipping it.
===========
Run 343/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_344/ already exists.
Skipping it.
===========
Run 344/720 already exists. Skipping it.
===========

===========
Generating train data for run 345.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_345/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_345/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_345/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_345
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_50"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_51 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  660960    
 yer)                                                            
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7f17d03d0a90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f17d0363460>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f17d0363460>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1e909676a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f17d0265030>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f17d02655a0>, <keras.callbacks.ModelCheckpoint object at 0x7f17d0265660>, <keras.callbacks.EarlyStopping object at 0x7f17d02658d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f17d0265900>, <keras.callbacks.TerminateOnNaN object at 0x7f17d0265540>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_345/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 345/720 with hyperparameters:
timestamp = 2023-10-24 15:41:22.164872
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-10-24 15:42:40.503 
Epoch 1/1000 
	 loss: 817.4555, MinusLogProbMetric: 817.4555, val_loss: 205.9654, val_MinusLogProbMetric: 205.9654

Epoch 1: val_loss improved from inf to 205.96544, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 79s - loss: 817.4555 - MinusLogProbMetric: 817.4555 - val_loss: 205.9654 - val_MinusLogProbMetric: 205.9654 - lr: 0.0010 - 79s/epoch - 401ms/step
Epoch 2/1000
2023-10-24 15:43:13.993 
Epoch 2/1000 
	 loss: 161.7097, MinusLogProbMetric: 161.7097, val_loss: 135.5985, val_MinusLogProbMetric: 135.5985

Epoch 2: val_loss improved from 205.96544 to 135.59846, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 161.7097 - MinusLogProbMetric: 161.7097 - val_loss: 135.5985 - val_MinusLogProbMetric: 135.5985 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 3/1000
2023-10-24 15:43:48.156 
Epoch 3/1000 
	 loss: 111.4740, MinusLogProbMetric: 111.4740, val_loss: 96.9418, val_MinusLogProbMetric: 96.9418

Epoch 3: val_loss improved from 135.59846 to 96.94176, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 111.4740 - MinusLogProbMetric: 111.4740 - val_loss: 96.9418 - val_MinusLogProbMetric: 96.9418 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 4/1000
2023-10-24 15:44:22.967 
Epoch 4/1000 
	 loss: 90.3730, MinusLogProbMetric: 90.3730, val_loss: 88.5987, val_MinusLogProbMetric: 88.5987

Epoch 4: val_loss improved from 96.94176 to 88.59873, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 90.3730 - MinusLogProbMetric: 90.3730 - val_loss: 88.5987 - val_MinusLogProbMetric: 88.5987 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 5/1000
2023-10-24 15:44:57.421 
Epoch 5/1000 
	 loss: 76.5887, MinusLogProbMetric: 76.5887, val_loss: 71.6618, val_MinusLogProbMetric: 71.6618

Epoch 5: val_loss improved from 88.59873 to 71.66181, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 76.5887 - MinusLogProbMetric: 76.5887 - val_loss: 71.6618 - val_MinusLogProbMetric: 71.6618 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 6/1000
2023-10-24 15:45:32.663 
Epoch 6/1000 
	 loss: 67.8506, MinusLogProbMetric: 67.8506, val_loss: 65.2584, val_MinusLogProbMetric: 65.2584

Epoch 6: val_loss improved from 71.66181 to 65.25843, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 67.8506 - MinusLogProbMetric: 67.8506 - val_loss: 65.2584 - val_MinusLogProbMetric: 65.2584 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 7/1000
2023-10-24 15:46:08.286 
Epoch 7/1000 
	 loss: 61.3523, MinusLogProbMetric: 61.3523, val_loss: 59.9706, val_MinusLogProbMetric: 59.9706

Epoch 7: val_loss improved from 65.25843 to 59.97058, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 36s - loss: 61.3523 - MinusLogProbMetric: 61.3523 - val_loss: 59.9706 - val_MinusLogProbMetric: 59.9706 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 8/1000
2023-10-24 15:46:43.891 
Epoch 8/1000 
	 loss: 57.0915, MinusLogProbMetric: 57.0915, val_loss: 56.9855, val_MinusLogProbMetric: 56.9855

Epoch 8: val_loss improved from 59.97058 to 56.98545, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 36s - loss: 57.0915 - MinusLogProbMetric: 57.0915 - val_loss: 56.9855 - val_MinusLogProbMetric: 56.9855 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 9/1000
2023-10-24 15:47:17.551 
Epoch 9/1000 
	 loss: 54.1681, MinusLogProbMetric: 54.1681, val_loss: 52.8964, val_MinusLogProbMetric: 52.8964

Epoch 9: val_loss improved from 56.98545 to 52.89639, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 54.1681 - MinusLogProbMetric: 54.1681 - val_loss: 52.8964 - val_MinusLogProbMetric: 52.8964 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 10/1000
2023-10-24 15:47:51.968 
Epoch 10/1000 
	 loss: 51.4746, MinusLogProbMetric: 51.4746, val_loss: 51.4578, val_MinusLogProbMetric: 51.4578

Epoch 10: val_loss improved from 52.89639 to 51.45778, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 51.4746 - MinusLogProbMetric: 51.4746 - val_loss: 51.4578 - val_MinusLogProbMetric: 51.4578 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 11/1000
2023-10-24 15:48:26.277 
Epoch 11/1000 
	 loss: 48.8738, MinusLogProbMetric: 48.8738, val_loss: 48.4311, val_MinusLogProbMetric: 48.4311

Epoch 11: val_loss improved from 51.45778 to 48.43107, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 48.8738 - MinusLogProbMetric: 48.8738 - val_loss: 48.4311 - val_MinusLogProbMetric: 48.4311 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 12/1000
2023-10-24 15:49:01.480 
Epoch 12/1000 
	 loss: 46.9692, MinusLogProbMetric: 46.9692, val_loss: 45.9972, val_MinusLogProbMetric: 45.9972

Epoch 12: val_loss improved from 48.43107 to 45.99718, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 46.9692 - MinusLogProbMetric: 46.9692 - val_loss: 45.9972 - val_MinusLogProbMetric: 45.9972 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 13/1000
2023-10-24 15:49:36.084 
Epoch 13/1000 
	 loss: 45.6076, MinusLogProbMetric: 45.6076, val_loss: 45.5392, val_MinusLogProbMetric: 45.5392

Epoch 13: val_loss improved from 45.99718 to 45.53916, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 45.6076 - MinusLogProbMetric: 45.6076 - val_loss: 45.5392 - val_MinusLogProbMetric: 45.5392 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 14/1000
2023-10-24 15:50:10.739 
Epoch 14/1000 
	 loss: 44.1213, MinusLogProbMetric: 44.1213, val_loss: 44.9323, val_MinusLogProbMetric: 44.9323

Epoch 14: val_loss improved from 45.53916 to 44.93233, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 44.1213 - MinusLogProbMetric: 44.1213 - val_loss: 44.9323 - val_MinusLogProbMetric: 44.9323 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 15/1000
2023-10-24 15:50:45.435 
Epoch 15/1000 
	 loss: 43.4356, MinusLogProbMetric: 43.4356, val_loss: 43.8263, val_MinusLogProbMetric: 43.8263

Epoch 15: val_loss improved from 44.93233 to 43.82626, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 43.4356 - MinusLogProbMetric: 43.4356 - val_loss: 43.8263 - val_MinusLogProbMetric: 43.8263 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 16/1000
2023-10-24 15:51:20.360 
Epoch 16/1000 
	 loss: 42.3714, MinusLogProbMetric: 42.3714, val_loss: 42.2866, val_MinusLogProbMetric: 42.2866

Epoch 16: val_loss improved from 43.82626 to 42.28661, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 42.3714 - MinusLogProbMetric: 42.3714 - val_loss: 42.2866 - val_MinusLogProbMetric: 42.2866 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 17/1000
2023-10-24 15:51:53.455 
Epoch 17/1000 
	 loss: 41.4222, MinusLogProbMetric: 41.4222, val_loss: 41.2982, val_MinusLogProbMetric: 41.2982

Epoch 17: val_loss improved from 42.28661 to 41.29819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 41.4222 - MinusLogProbMetric: 41.4222 - val_loss: 41.2982 - val_MinusLogProbMetric: 41.2982 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 18/1000
2023-10-24 15:52:27.972 
Epoch 18/1000 
	 loss: 40.7409, MinusLogProbMetric: 40.7409, val_loss: 41.8441, val_MinusLogProbMetric: 41.8441

Epoch 18: val_loss did not improve from 41.29819
196/196 - 34s - loss: 40.7409 - MinusLogProbMetric: 40.7409 - val_loss: 41.8441 - val_MinusLogProbMetric: 41.8441 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 19/1000
2023-10-24 15:53:02.796 
Epoch 19/1000 
	 loss: 40.5513, MinusLogProbMetric: 40.5513, val_loss: 41.4965, val_MinusLogProbMetric: 41.4965

Epoch 19: val_loss did not improve from 41.29819
196/196 - 35s - loss: 40.5513 - MinusLogProbMetric: 40.5513 - val_loss: 41.4965 - val_MinusLogProbMetric: 41.4965 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 20/1000
2023-10-24 15:53:37.394 
Epoch 20/1000 
	 loss: 39.7473, MinusLogProbMetric: 39.7473, val_loss: 39.8808, val_MinusLogProbMetric: 39.8808

Epoch 20: val_loss improved from 41.29819 to 39.88079, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 39.7473 - MinusLogProbMetric: 39.7473 - val_loss: 39.8808 - val_MinusLogProbMetric: 39.8808 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 21/1000
2023-10-24 15:54:10.650 
Epoch 21/1000 
	 loss: 39.1681, MinusLogProbMetric: 39.1681, val_loss: 38.5693, val_MinusLogProbMetric: 38.5693

Epoch 21: val_loss improved from 39.88079 to 38.56926, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 39.1681 - MinusLogProbMetric: 39.1681 - val_loss: 38.5693 - val_MinusLogProbMetric: 38.5693 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 22/1000
2023-10-24 15:54:45.334 
Epoch 22/1000 
	 loss: 38.9990, MinusLogProbMetric: 38.9990, val_loss: 41.0189, val_MinusLogProbMetric: 41.0189

Epoch 22: val_loss did not improve from 38.56926
196/196 - 34s - loss: 38.9990 - MinusLogProbMetric: 38.9990 - val_loss: 41.0189 - val_MinusLogProbMetric: 41.0189 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 23/1000
2023-10-24 15:55:19.680 
Epoch 23/1000 
	 loss: 38.6866, MinusLogProbMetric: 38.6866, val_loss: 39.0130, val_MinusLogProbMetric: 39.0130

Epoch 23: val_loss did not improve from 38.56926
196/196 - 34s - loss: 38.6866 - MinusLogProbMetric: 38.6866 - val_loss: 39.0130 - val_MinusLogProbMetric: 39.0130 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 24/1000
2023-10-24 15:55:54.029 
Epoch 24/1000 
	 loss: 37.9427, MinusLogProbMetric: 37.9427, val_loss: 37.5257, val_MinusLogProbMetric: 37.5257

Epoch 24: val_loss improved from 38.56926 to 37.52565, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 37.9427 - MinusLogProbMetric: 37.9427 - val_loss: 37.5257 - val_MinusLogProbMetric: 37.5257 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 25/1000
2023-10-24 15:56:29.165 
Epoch 25/1000 
	 loss: 37.6997, MinusLogProbMetric: 37.6997, val_loss: 36.9943, val_MinusLogProbMetric: 36.9943

Epoch 25: val_loss improved from 37.52565 to 36.99432, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 37.6997 - MinusLogProbMetric: 37.6997 - val_loss: 36.9943 - val_MinusLogProbMetric: 36.9943 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 26/1000
2023-10-24 15:57:04.621 
Epoch 26/1000 
	 loss: 37.2833, MinusLogProbMetric: 37.2833, val_loss: 39.0734, val_MinusLogProbMetric: 39.0734

Epoch 26: val_loss did not improve from 36.99432
196/196 - 35s - loss: 37.2833 - MinusLogProbMetric: 37.2833 - val_loss: 39.0734 - val_MinusLogProbMetric: 39.0734 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 27/1000
2023-10-24 15:57:37.861 
Epoch 27/1000 
	 loss: 37.0157, MinusLogProbMetric: 37.0157, val_loss: 36.9090, val_MinusLogProbMetric: 36.9090

Epoch 27: val_loss improved from 36.99432 to 36.90900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 37.0157 - MinusLogProbMetric: 37.0157 - val_loss: 36.9090 - val_MinusLogProbMetric: 36.9090 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 28/1000
2023-10-24 15:58:11.001 
Epoch 28/1000 
	 loss: 36.6886, MinusLogProbMetric: 36.6886, val_loss: 37.1388, val_MinusLogProbMetric: 37.1388

Epoch 28: val_loss did not improve from 36.90900
196/196 - 33s - loss: 36.6886 - MinusLogProbMetric: 36.6886 - val_loss: 37.1388 - val_MinusLogProbMetric: 37.1388 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 29/1000
2023-10-24 15:58:45.637 
Epoch 29/1000 
	 loss: 36.5445, MinusLogProbMetric: 36.5445, val_loss: 36.1296, val_MinusLogProbMetric: 36.1296

Epoch 29: val_loss improved from 36.90900 to 36.12955, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 36.5445 - MinusLogProbMetric: 36.5445 - val_loss: 36.1296 - val_MinusLogProbMetric: 36.1296 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 30/1000
2023-10-24 15:59:20.220 
Epoch 30/1000 
	 loss: 36.2037, MinusLogProbMetric: 36.2037, val_loss: 36.2350, val_MinusLogProbMetric: 36.2350

Epoch 30: val_loss did not improve from 36.12955
196/196 - 34s - loss: 36.2037 - MinusLogProbMetric: 36.2037 - val_loss: 36.2350 - val_MinusLogProbMetric: 36.2350 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 31/1000
2023-10-24 15:59:54.609 
Epoch 31/1000 
	 loss: 36.1996, MinusLogProbMetric: 36.1996, val_loss: 36.5620, val_MinusLogProbMetric: 36.5620

Epoch 31: val_loss did not improve from 36.12955
196/196 - 34s - loss: 36.1996 - MinusLogProbMetric: 36.1996 - val_loss: 36.5620 - val_MinusLogProbMetric: 36.5620 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 32/1000
2023-10-24 16:00:29.062 
Epoch 32/1000 
	 loss: 36.1652, MinusLogProbMetric: 36.1652, val_loss: 36.6928, val_MinusLogProbMetric: 36.6928

Epoch 32: val_loss did not improve from 36.12955
196/196 - 34s - loss: 36.1652 - MinusLogProbMetric: 36.1652 - val_loss: 36.6928 - val_MinusLogProbMetric: 36.6928 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 33/1000
2023-10-24 16:01:03.194 
Epoch 33/1000 
	 loss: 35.4685, MinusLogProbMetric: 35.4685, val_loss: 36.0588, val_MinusLogProbMetric: 36.0588

Epoch 33: val_loss improved from 36.12955 to 36.05876, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 35.4685 - MinusLogProbMetric: 35.4685 - val_loss: 36.0588 - val_MinusLogProbMetric: 36.0588 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 34/1000
2023-10-24 16:01:37.517 
Epoch 34/1000 
	 loss: 35.5526, MinusLogProbMetric: 35.5526, val_loss: 37.1850, val_MinusLogProbMetric: 37.1850

Epoch 34: val_loss did not improve from 36.05876
196/196 - 34s - loss: 35.5526 - MinusLogProbMetric: 35.5526 - val_loss: 37.1850 - val_MinusLogProbMetric: 37.1850 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 35/1000
2023-10-24 16:02:11.696 
Epoch 35/1000 
	 loss: 35.4008, MinusLogProbMetric: 35.4008, val_loss: 36.6346, val_MinusLogProbMetric: 36.6346

Epoch 35: val_loss did not improve from 36.05876
196/196 - 34s - loss: 35.4008 - MinusLogProbMetric: 35.4008 - val_loss: 36.6346 - val_MinusLogProbMetric: 36.6346 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 36/1000
2023-10-24 16:02:46.190 
Epoch 36/1000 
	 loss: 35.2393, MinusLogProbMetric: 35.2393, val_loss: 35.1426, val_MinusLogProbMetric: 35.1426

Epoch 36: val_loss improved from 36.05876 to 35.14256, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 35.2393 - MinusLogProbMetric: 35.2393 - val_loss: 35.1426 - val_MinusLogProbMetric: 35.1426 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 37/1000
2023-10-24 16:03:20.883 
Epoch 37/1000 
	 loss: 34.9665, MinusLogProbMetric: 34.9665, val_loss: 36.0507, val_MinusLogProbMetric: 36.0507

Epoch 37: val_loss did not improve from 35.14256
196/196 - 34s - loss: 34.9665 - MinusLogProbMetric: 34.9665 - val_loss: 36.0507 - val_MinusLogProbMetric: 36.0507 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 38/1000
2023-10-24 16:03:55.224 
Epoch 38/1000 
	 loss: 34.9059, MinusLogProbMetric: 34.9059, val_loss: 34.1275, val_MinusLogProbMetric: 34.1275

Epoch 38: val_loss improved from 35.14256 to 34.12753, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 34.9059 - MinusLogProbMetric: 34.9059 - val_loss: 34.1275 - val_MinusLogProbMetric: 34.1275 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 39/1000
2023-10-24 16:04:30.012 
Epoch 39/1000 
	 loss: 34.5348, MinusLogProbMetric: 34.5348, val_loss: 36.6724, val_MinusLogProbMetric: 36.6724

Epoch 39: val_loss did not improve from 34.12753
196/196 - 34s - loss: 34.5348 - MinusLogProbMetric: 34.5348 - val_loss: 36.6724 - val_MinusLogProbMetric: 36.6724 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 40/1000
2023-10-24 16:05:04.517 
Epoch 40/1000 
	 loss: 34.5756, MinusLogProbMetric: 34.5756, val_loss: 34.1154, val_MinusLogProbMetric: 34.1154

Epoch 40: val_loss improved from 34.12753 to 34.11541, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 34.5756 - MinusLogProbMetric: 34.5756 - val_loss: 34.1154 - val_MinusLogProbMetric: 34.1154 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 41/1000
2023-10-24 16:05:39.374 
Epoch 41/1000 
	 loss: 34.5262, MinusLogProbMetric: 34.5262, val_loss: 35.0631, val_MinusLogProbMetric: 35.0631

Epoch 41: val_loss did not improve from 34.11541
196/196 - 34s - loss: 34.5262 - MinusLogProbMetric: 34.5262 - val_loss: 35.0631 - val_MinusLogProbMetric: 35.0631 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 42/1000
2023-10-24 16:06:14.025 
Epoch 42/1000 
	 loss: 34.2166, MinusLogProbMetric: 34.2166, val_loss: 34.4332, val_MinusLogProbMetric: 34.4332

Epoch 42: val_loss did not improve from 34.11541
196/196 - 35s - loss: 34.2166 - MinusLogProbMetric: 34.2166 - val_loss: 34.4332 - val_MinusLogProbMetric: 34.4332 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 43/1000
2023-10-24 16:06:48.861 
Epoch 43/1000 
	 loss: 34.2668, MinusLogProbMetric: 34.2668, val_loss: 34.4092, val_MinusLogProbMetric: 34.4092

Epoch 43: val_loss did not improve from 34.11541
196/196 - 35s - loss: 34.2668 - MinusLogProbMetric: 34.2668 - val_loss: 34.4092 - val_MinusLogProbMetric: 34.4092 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 44/1000
2023-10-24 16:07:23.089 
Epoch 44/1000 
	 loss: 33.8196, MinusLogProbMetric: 33.8196, val_loss: 34.1642, val_MinusLogProbMetric: 34.1642

Epoch 44: val_loss did not improve from 34.11541
196/196 - 34s - loss: 33.8196 - MinusLogProbMetric: 33.8196 - val_loss: 34.1642 - val_MinusLogProbMetric: 34.1642 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 45/1000
2023-10-24 16:07:57.847 
Epoch 45/1000 
	 loss: 34.0282, MinusLogProbMetric: 34.0282, val_loss: 33.7412, val_MinusLogProbMetric: 33.7412

Epoch 45: val_loss improved from 34.11541 to 33.74121, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 34.0282 - MinusLogProbMetric: 34.0282 - val_loss: 33.7412 - val_MinusLogProbMetric: 33.7412 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 46/1000
2023-10-24 16:08:32.758 
Epoch 46/1000 
	 loss: 33.8627, MinusLogProbMetric: 33.8627, val_loss: 33.2344, val_MinusLogProbMetric: 33.2344

Epoch 46: val_loss improved from 33.74121 to 33.23443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 33.8627 - MinusLogProbMetric: 33.8627 - val_loss: 33.2344 - val_MinusLogProbMetric: 33.2344 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 47/1000
2023-10-24 16:09:07.720 
Epoch 47/1000 
	 loss: 33.7056, MinusLogProbMetric: 33.7056, val_loss: 33.4164, val_MinusLogProbMetric: 33.4164

Epoch 47: val_loss did not improve from 33.23443
196/196 - 34s - loss: 33.7056 - MinusLogProbMetric: 33.7056 - val_loss: 33.4164 - val_MinusLogProbMetric: 33.4164 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 48/1000
2023-10-24 16:09:42.472 
Epoch 48/1000 
	 loss: 33.7369, MinusLogProbMetric: 33.7369, val_loss: 34.1801, val_MinusLogProbMetric: 34.1801

Epoch 48: val_loss did not improve from 33.23443
196/196 - 35s - loss: 33.7369 - MinusLogProbMetric: 33.7369 - val_loss: 34.1801 - val_MinusLogProbMetric: 34.1801 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 49/1000
2023-10-24 16:10:16.763 
Epoch 49/1000 
	 loss: 33.3721, MinusLogProbMetric: 33.3721, val_loss: 34.0759, val_MinusLogProbMetric: 34.0759

Epoch 49: val_loss did not improve from 33.23443
196/196 - 34s - loss: 33.3721 - MinusLogProbMetric: 33.3721 - val_loss: 34.0759 - val_MinusLogProbMetric: 34.0759 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 50/1000
2023-10-24 16:10:50.512 
Epoch 50/1000 
	 loss: 33.4990, MinusLogProbMetric: 33.4990, val_loss: 33.4602, val_MinusLogProbMetric: 33.4602

Epoch 50: val_loss did not improve from 33.23443
196/196 - 34s - loss: 33.4990 - MinusLogProbMetric: 33.4990 - val_loss: 33.4602 - val_MinusLogProbMetric: 33.4602 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 51/1000
2023-10-24 16:11:24.769 
Epoch 51/1000 
	 loss: 33.3063, MinusLogProbMetric: 33.3063, val_loss: 33.6921, val_MinusLogProbMetric: 33.6921

Epoch 51: val_loss did not improve from 33.23443
196/196 - 34s - loss: 33.3063 - MinusLogProbMetric: 33.3063 - val_loss: 33.6921 - val_MinusLogProbMetric: 33.6921 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 52/1000
2023-10-24 16:11:59.469 
Epoch 52/1000 
	 loss: 33.1894, MinusLogProbMetric: 33.1894, val_loss: 33.3886, val_MinusLogProbMetric: 33.3886

Epoch 52: val_loss did not improve from 33.23443
196/196 - 35s - loss: 33.1894 - MinusLogProbMetric: 33.1894 - val_loss: 33.3886 - val_MinusLogProbMetric: 33.3886 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 53/1000
2023-10-24 16:12:33.901 
Epoch 53/1000 
	 loss: 33.1903, MinusLogProbMetric: 33.1903, val_loss: 33.4028, val_MinusLogProbMetric: 33.4028

Epoch 53: val_loss did not improve from 33.23443
196/196 - 34s - loss: 33.1903 - MinusLogProbMetric: 33.1903 - val_loss: 33.4028 - val_MinusLogProbMetric: 33.4028 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 54/1000
2023-10-24 16:13:08.196 
Epoch 54/1000 
	 loss: 33.1743, MinusLogProbMetric: 33.1743, val_loss: 32.8043, val_MinusLogProbMetric: 32.8043

Epoch 54: val_loss improved from 33.23443 to 32.80434, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 33.1743 - MinusLogProbMetric: 33.1743 - val_loss: 32.8043 - val_MinusLogProbMetric: 32.8043 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 55/1000
2023-10-24 16:13:43.240 
Epoch 55/1000 
	 loss: 32.9161, MinusLogProbMetric: 32.9161, val_loss: 33.2858, val_MinusLogProbMetric: 33.2858

Epoch 55: val_loss did not improve from 32.80434
196/196 - 35s - loss: 32.9161 - MinusLogProbMetric: 32.9161 - val_loss: 33.2858 - val_MinusLogProbMetric: 33.2858 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 56/1000
2023-10-24 16:14:17.285 
Epoch 56/1000 
	 loss: 33.0691, MinusLogProbMetric: 33.0691, val_loss: 33.9627, val_MinusLogProbMetric: 33.9627

Epoch 56: val_loss did not improve from 32.80434
196/196 - 34s - loss: 33.0691 - MinusLogProbMetric: 33.0691 - val_loss: 33.9627 - val_MinusLogProbMetric: 33.9627 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 57/1000
2023-10-24 16:14:51.411 
Epoch 57/1000 
	 loss: 32.8656, MinusLogProbMetric: 32.8656, val_loss: 34.9114, val_MinusLogProbMetric: 34.9114

Epoch 57: val_loss did not improve from 32.80434
196/196 - 34s - loss: 32.8656 - MinusLogProbMetric: 32.8656 - val_loss: 34.9114 - val_MinusLogProbMetric: 34.9114 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 58/1000
2023-10-24 16:15:25.724 
Epoch 58/1000 
	 loss: 32.8886, MinusLogProbMetric: 32.8886, val_loss: 32.6070, val_MinusLogProbMetric: 32.6070

Epoch 58: val_loss improved from 32.80434 to 32.60698, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 32.8886 - MinusLogProbMetric: 32.8886 - val_loss: 32.6070 - val_MinusLogProbMetric: 32.6070 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 59/1000
2023-10-24 16:16:00.832 
Epoch 59/1000 
	 loss: 32.6269, MinusLogProbMetric: 32.6269, val_loss: 34.0536, val_MinusLogProbMetric: 34.0536

Epoch 59: val_loss did not improve from 32.60698
196/196 - 35s - loss: 32.6269 - MinusLogProbMetric: 32.6269 - val_loss: 34.0536 - val_MinusLogProbMetric: 34.0536 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 60/1000
2023-10-24 16:16:35.006 
Epoch 60/1000 
	 loss: 32.8564, MinusLogProbMetric: 32.8564, val_loss: 32.7394, val_MinusLogProbMetric: 32.7394

Epoch 60: val_loss did not improve from 32.60698
196/196 - 34s - loss: 32.8564 - MinusLogProbMetric: 32.8564 - val_loss: 32.7394 - val_MinusLogProbMetric: 32.7394 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 61/1000
2023-10-24 16:17:09.040 
Epoch 61/1000 
	 loss: 32.5946, MinusLogProbMetric: 32.5946, val_loss: 32.3927, val_MinusLogProbMetric: 32.3927

Epoch 61: val_loss improved from 32.60698 to 32.39272, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 32.5946 - MinusLogProbMetric: 32.5946 - val_loss: 32.3927 - val_MinusLogProbMetric: 32.3927 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 62/1000
2023-10-24 16:17:42.006 
Epoch 62/1000 
	 loss: 32.6507, MinusLogProbMetric: 32.6507, val_loss: 32.3217, val_MinusLogProbMetric: 32.3217

Epoch 62: val_loss improved from 32.39272 to 32.32169, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 32.6507 - MinusLogProbMetric: 32.6507 - val_loss: 32.3217 - val_MinusLogProbMetric: 32.3217 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 63/1000
2023-10-24 16:18:16.910 
Epoch 63/1000 
	 loss: 32.4421, MinusLogProbMetric: 32.4421, val_loss: 32.9798, val_MinusLogProbMetric: 32.9798

Epoch 63: val_loss did not improve from 32.32169
196/196 - 34s - loss: 32.4421 - MinusLogProbMetric: 32.4421 - val_loss: 32.9798 - val_MinusLogProbMetric: 32.9798 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 64/1000
2023-10-24 16:18:50.498 
Epoch 64/1000 
	 loss: 32.3663, MinusLogProbMetric: 32.3663, val_loss: 33.0563, val_MinusLogProbMetric: 33.0563

Epoch 64: val_loss did not improve from 32.32169
196/196 - 34s - loss: 32.3663 - MinusLogProbMetric: 32.3663 - val_loss: 33.0563 - val_MinusLogProbMetric: 33.0563 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 65/1000
2023-10-24 16:19:25.248 
Epoch 65/1000 
	 loss: 32.4007, MinusLogProbMetric: 32.4007, val_loss: 32.5384, val_MinusLogProbMetric: 32.5384

Epoch 65: val_loss did not improve from 32.32169
196/196 - 35s - loss: 32.4007 - MinusLogProbMetric: 32.4007 - val_loss: 32.5384 - val_MinusLogProbMetric: 32.5384 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 66/1000
2023-10-24 16:19:59.684 
Epoch 66/1000 
	 loss: 32.1901, MinusLogProbMetric: 32.1901, val_loss: 32.4724, val_MinusLogProbMetric: 32.4724

Epoch 66: val_loss did not improve from 32.32169
196/196 - 34s - loss: 32.1901 - MinusLogProbMetric: 32.1901 - val_loss: 32.4724 - val_MinusLogProbMetric: 32.4724 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 67/1000
2023-10-24 16:20:33.160 
Epoch 67/1000 
	 loss: 32.2405, MinusLogProbMetric: 32.2405, val_loss: 31.8607, val_MinusLogProbMetric: 31.8607

Epoch 67: val_loss improved from 32.32169 to 31.86071, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 32.2405 - MinusLogProbMetric: 32.2405 - val_loss: 31.8607 - val_MinusLogProbMetric: 31.8607 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 68/1000
2023-10-24 16:21:06.980 
Epoch 68/1000 
	 loss: 32.2051, MinusLogProbMetric: 32.2051, val_loss: 31.8365, val_MinusLogProbMetric: 31.8365

Epoch 68: val_loss improved from 31.86071 to 31.83645, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 32.2051 - MinusLogProbMetric: 32.2051 - val_loss: 31.8365 - val_MinusLogProbMetric: 31.8365 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 69/1000
2023-10-24 16:21:41.670 
Epoch 69/1000 
	 loss: 32.0714, MinusLogProbMetric: 32.0714, val_loss: 32.1587, val_MinusLogProbMetric: 32.1587

Epoch 69: val_loss did not improve from 31.83645
196/196 - 34s - loss: 32.0714 - MinusLogProbMetric: 32.0714 - val_loss: 32.1587 - val_MinusLogProbMetric: 32.1587 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 70/1000
2023-10-24 16:22:15.897 
Epoch 70/1000 
	 loss: 32.1536, MinusLogProbMetric: 32.1536, val_loss: 31.8954, val_MinusLogProbMetric: 31.8954

Epoch 70: val_loss did not improve from 31.83645
196/196 - 34s - loss: 32.1536 - MinusLogProbMetric: 32.1536 - val_loss: 31.8954 - val_MinusLogProbMetric: 31.8954 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 71/1000
2023-10-24 16:22:49.963 
Epoch 71/1000 
	 loss: 31.9981, MinusLogProbMetric: 31.9981, val_loss: 32.6090, val_MinusLogProbMetric: 32.6090

Epoch 71: val_loss did not improve from 31.83645
196/196 - 34s - loss: 31.9981 - MinusLogProbMetric: 31.9981 - val_loss: 32.6090 - val_MinusLogProbMetric: 32.6090 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 72/1000
2023-10-24 16:23:24.682 
Epoch 72/1000 
	 loss: 32.2493, MinusLogProbMetric: 32.2493, val_loss: 32.8205, val_MinusLogProbMetric: 32.8205

Epoch 72: val_loss did not improve from 31.83645
196/196 - 35s - loss: 32.2493 - MinusLogProbMetric: 32.2493 - val_loss: 32.8205 - val_MinusLogProbMetric: 32.8205 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 73/1000
2023-10-24 16:23:59.305 
Epoch 73/1000 
	 loss: 32.0236, MinusLogProbMetric: 32.0236, val_loss: 32.3116, val_MinusLogProbMetric: 32.3116

Epoch 73: val_loss did not improve from 31.83645
196/196 - 35s - loss: 32.0236 - MinusLogProbMetric: 32.0236 - val_loss: 32.3116 - val_MinusLogProbMetric: 32.3116 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 74/1000
2023-10-24 16:24:33.225 
Epoch 74/1000 
	 loss: 31.9164, MinusLogProbMetric: 31.9164, val_loss: 32.0452, val_MinusLogProbMetric: 32.0452

Epoch 74: val_loss did not improve from 31.83645
196/196 - 34s - loss: 31.9164 - MinusLogProbMetric: 31.9164 - val_loss: 32.0452 - val_MinusLogProbMetric: 32.0452 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 75/1000
2023-10-24 16:25:07.079 
Epoch 75/1000 
	 loss: 31.8556, MinusLogProbMetric: 31.8556, val_loss: 32.0336, val_MinusLogProbMetric: 32.0336

Epoch 75: val_loss did not improve from 31.83645
196/196 - 34s - loss: 31.8556 - MinusLogProbMetric: 31.8556 - val_loss: 32.0336 - val_MinusLogProbMetric: 32.0336 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 76/1000
2023-10-24 16:25:41.082 
Epoch 76/1000 
	 loss: 32.0059, MinusLogProbMetric: 32.0059, val_loss: 32.8166, val_MinusLogProbMetric: 32.8166

Epoch 76: val_loss did not improve from 31.83645
196/196 - 34s - loss: 32.0059 - MinusLogProbMetric: 32.0059 - val_loss: 32.8166 - val_MinusLogProbMetric: 32.8166 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 77/1000
2023-10-24 16:26:15.451 
Epoch 77/1000 
	 loss: 31.8196, MinusLogProbMetric: 31.8196, val_loss: 32.0078, val_MinusLogProbMetric: 32.0078

Epoch 77: val_loss did not improve from 31.83645
196/196 - 34s - loss: 31.8196 - MinusLogProbMetric: 31.8196 - val_loss: 32.0078 - val_MinusLogProbMetric: 32.0078 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 78/1000
2023-10-24 16:26:50.012 
Epoch 78/1000 
	 loss: 31.7331, MinusLogProbMetric: 31.7331, val_loss: 32.8532, val_MinusLogProbMetric: 32.8532

Epoch 78: val_loss did not improve from 31.83645
196/196 - 35s - loss: 31.7331 - MinusLogProbMetric: 31.7331 - val_loss: 32.8532 - val_MinusLogProbMetric: 32.8532 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 79/1000
2023-10-24 16:27:24.453 
Epoch 79/1000 
	 loss: 31.7630, MinusLogProbMetric: 31.7630, val_loss: 31.5691, val_MinusLogProbMetric: 31.5691

Epoch 79: val_loss improved from 31.83645 to 31.56914, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 31.7630 - MinusLogProbMetric: 31.7630 - val_loss: 31.5691 - val_MinusLogProbMetric: 31.5691 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 80/1000
2023-10-24 16:27:59.134 
Epoch 80/1000 
	 loss: 31.9404, MinusLogProbMetric: 31.9404, val_loss: 32.3785, val_MinusLogProbMetric: 32.3785

Epoch 80: val_loss did not improve from 31.56914
196/196 - 34s - loss: 31.9404 - MinusLogProbMetric: 31.9404 - val_loss: 32.3785 - val_MinusLogProbMetric: 32.3785 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 81/1000
2023-10-24 16:28:33.008 
Epoch 81/1000 
	 loss: 31.6140, MinusLogProbMetric: 31.6140, val_loss: 31.9906, val_MinusLogProbMetric: 31.9906

Epoch 81: val_loss did not improve from 31.56914
196/196 - 34s - loss: 31.6140 - MinusLogProbMetric: 31.6140 - val_loss: 31.9906 - val_MinusLogProbMetric: 31.9906 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 82/1000
2023-10-24 16:29:07.346 
Epoch 82/1000 
	 loss: 31.7145, MinusLogProbMetric: 31.7145, val_loss: 31.3918, val_MinusLogProbMetric: 31.3918

Epoch 82: val_loss improved from 31.56914 to 31.39183, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 31.7145 - MinusLogProbMetric: 31.7145 - val_loss: 31.3918 - val_MinusLogProbMetric: 31.3918 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 83/1000
2023-10-24 16:29:41.900 
Epoch 83/1000 
	 loss: 31.4425, MinusLogProbMetric: 31.4425, val_loss: 32.5206, val_MinusLogProbMetric: 32.5206

Epoch 83: val_loss did not improve from 31.39183
196/196 - 34s - loss: 31.4425 - MinusLogProbMetric: 31.4425 - val_loss: 32.5206 - val_MinusLogProbMetric: 32.5206 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 84/1000
2023-10-24 16:30:16.420 
Epoch 84/1000 
	 loss: 31.7817, MinusLogProbMetric: 31.7817, val_loss: 31.3735, val_MinusLogProbMetric: 31.3735

Epoch 84: val_loss improved from 31.39183 to 31.37354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 31.7817 - MinusLogProbMetric: 31.7817 - val_loss: 31.3735 - val_MinusLogProbMetric: 31.3735 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 85/1000
2023-10-24 16:30:51.110 
Epoch 85/1000 
	 loss: 31.5092, MinusLogProbMetric: 31.5092, val_loss: 31.4570, val_MinusLogProbMetric: 31.4570

Epoch 85: val_loss did not improve from 31.37354
196/196 - 34s - loss: 31.5092 - MinusLogProbMetric: 31.5092 - val_loss: 31.4570 - val_MinusLogProbMetric: 31.4570 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 86/1000
2023-10-24 16:31:25.436 
Epoch 86/1000 
	 loss: 31.3589, MinusLogProbMetric: 31.3589, val_loss: 32.4596, val_MinusLogProbMetric: 32.4596

Epoch 86: val_loss did not improve from 31.37354
196/196 - 34s - loss: 31.3589 - MinusLogProbMetric: 31.3589 - val_loss: 32.4596 - val_MinusLogProbMetric: 32.4596 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 87/1000
2023-10-24 16:31:59.892 
Epoch 87/1000 
	 loss: 31.7037, MinusLogProbMetric: 31.7037, val_loss: 32.0671, val_MinusLogProbMetric: 32.0671

Epoch 87: val_loss did not improve from 31.37354
196/196 - 34s - loss: 31.7037 - MinusLogProbMetric: 31.7037 - val_loss: 32.0671 - val_MinusLogProbMetric: 32.0671 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 88/1000
2023-10-24 16:32:33.932 
Epoch 88/1000 
	 loss: 31.4232, MinusLogProbMetric: 31.4232, val_loss: 32.4267, val_MinusLogProbMetric: 32.4267

Epoch 88: val_loss did not improve from 31.37354
196/196 - 34s - loss: 31.4232 - MinusLogProbMetric: 31.4232 - val_loss: 32.4267 - val_MinusLogProbMetric: 32.4267 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 89/1000
2023-10-24 16:33:08.195 
Epoch 89/1000 
	 loss: 31.4356, MinusLogProbMetric: 31.4356, val_loss: 30.7873, val_MinusLogProbMetric: 30.7873

Epoch 89: val_loss improved from 31.37354 to 30.78725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 31.4356 - MinusLogProbMetric: 31.4356 - val_loss: 30.7873 - val_MinusLogProbMetric: 30.7873 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 90/1000
2023-10-24 16:33:42.816 
Epoch 90/1000 
	 loss: 31.5090, MinusLogProbMetric: 31.5090, val_loss: 31.1116, val_MinusLogProbMetric: 31.1116

Epoch 90: val_loss did not improve from 30.78725
196/196 - 34s - loss: 31.5090 - MinusLogProbMetric: 31.5090 - val_loss: 31.1116 - val_MinusLogProbMetric: 31.1116 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 91/1000
2023-10-24 16:34:17.256 
Epoch 91/1000 
	 loss: 31.3947, MinusLogProbMetric: 31.3947, val_loss: 31.4081, val_MinusLogProbMetric: 31.4081

Epoch 91: val_loss did not improve from 30.78725
196/196 - 34s - loss: 31.3947 - MinusLogProbMetric: 31.3947 - val_loss: 31.4081 - val_MinusLogProbMetric: 31.4081 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 92/1000
2023-10-24 16:34:50.784 
Epoch 92/1000 
	 loss: 31.2984, MinusLogProbMetric: 31.2984, val_loss: 31.1267, val_MinusLogProbMetric: 31.1267

Epoch 92: val_loss did not improve from 30.78725
196/196 - 34s - loss: 31.2984 - MinusLogProbMetric: 31.2984 - val_loss: 31.1267 - val_MinusLogProbMetric: 31.1267 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 93/1000
2023-10-24 16:35:25.230 
Epoch 93/1000 
	 loss: 31.2425, MinusLogProbMetric: 31.2425, val_loss: 31.7421, val_MinusLogProbMetric: 31.7421

Epoch 93: val_loss did not improve from 30.78725
196/196 - 34s - loss: 31.2425 - MinusLogProbMetric: 31.2425 - val_loss: 31.7421 - val_MinusLogProbMetric: 31.7421 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 94/1000
2023-10-24 16:35:59.713 
Epoch 94/1000 
	 loss: 31.2574, MinusLogProbMetric: 31.2574, val_loss: 31.3361, val_MinusLogProbMetric: 31.3361

Epoch 94: val_loss did not improve from 30.78725
196/196 - 34s - loss: 31.2574 - MinusLogProbMetric: 31.2574 - val_loss: 31.3361 - val_MinusLogProbMetric: 31.3361 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 95/1000
2023-10-24 16:36:34.317 
Epoch 95/1000 
	 loss: 31.2549, MinusLogProbMetric: 31.2549, val_loss: 31.2859, val_MinusLogProbMetric: 31.2859

Epoch 95: val_loss did not improve from 30.78725
196/196 - 35s - loss: 31.2549 - MinusLogProbMetric: 31.2549 - val_loss: 31.2859 - val_MinusLogProbMetric: 31.2859 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 96/1000
2023-10-24 16:37:08.021 
Epoch 96/1000 
	 loss: 31.3182, MinusLogProbMetric: 31.3182, val_loss: 32.6665, val_MinusLogProbMetric: 32.6665

Epoch 96: val_loss did not improve from 30.78725
196/196 - 34s - loss: 31.3182 - MinusLogProbMetric: 31.3182 - val_loss: 32.6665 - val_MinusLogProbMetric: 32.6665 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 97/1000
2023-10-24 16:37:42.398 
Epoch 97/1000 
	 loss: 31.2509, MinusLogProbMetric: 31.2509, val_loss: 31.3471, val_MinusLogProbMetric: 31.3471

Epoch 97: val_loss did not improve from 30.78725
196/196 - 34s - loss: 31.2509 - MinusLogProbMetric: 31.2509 - val_loss: 31.3471 - val_MinusLogProbMetric: 31.3471 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 98/1000
2023-10-24 16:38:15.956 
Epoch 98/1000 
	 loss: 31.2907, MinusLogProbMetric: 31.2907, val_loss: 32.0653, val_MinusLogProbMetric: 32.0653

Epoch 98: val_loss did not improve from 30.78725
196/196 - 34s - loss: 31.2907 - MinusLogProbMetric: 31.2907 - val_loss: 32.0653 - val_MinusLogProbMetric: 32.0653 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 99/1000
2023-10-24 16:38:50.545 
Epoch 99/1000 
	 loss: 31.1669, MinusLogProbMetric: 31.1669, val_loss: 33.4405, val_MinusLogProbMetric: 33.4405

Epoch 99: val_loss did not improve from 30.78725
196/196 - 35s - loss: 31.1669 - MinusLogProbMetric: 31.1669 - val_loss: 33.4405 - val_MinusLogProbMetric: 33.4405 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 100/1000
2023-10-24 16:39:23.097 
Epoch 100/1000 
	 loss: 31.2860, MinusLogProbMetric: 31.2860, val_loss: 30.4569, val_MinusLogProbMetric: 30.4569

Epoch 100: val_loss improved from 30.78725 to 30.45685, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 31.2860 - MinusLogProbMetric: 31.2860 - val_loss: 30.4569 - val_MinusLogProbMetric: 30.4569 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 101/1000
2023-10-24 16:39:57.653 
Epoch 101/1000 
	 loss: 30.9889, MinusLogProbMetric: 30.9889, val_loss: 31.3566, val_MinusLogProbMetric: 31.3566

Epoch 101: val_loss did not improve from 30.45685
196/196 - 34s - loss: 30.9889 - MinusLogProbMetric: 30.9889 - val_loss: 31.3566 - val_MinusLogProbMetric: 31.3566 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 102/1000
2023-10-24 16:40:32.062 
Epoch 102/1000 
	 loss: 30.9642, MinusLogProbMetric: 30.9642, val_loss: 31.3622, val_MinusLogProbMetric: 31.3622

Epoch 102: val_loss did not improve from 30.45685
196/196 - 34s - loss: 30.9642 - MinusLogProbMetric: 30.9642 - val_loss: 31.3622 - val_MinusLogProbMetric: 31.3622 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 103/1000
2023-10-24 16:41:05.996 
Epoch 103/1000 
	 loss: 31.0853, MinusLogProbMetric: 31.0853, val_loss: 31.7143, val_MinusLogProbMetric: 31.7143

Epoch 103: val_loss did not improve from 30.45685
196/196 - 34s - loss: 31.0853 - MinusLogProbMetric: 31.0853 - val_loss: 31.7143 - val_MinusLogProbMetric: 31.7143 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 104/1000
2023-10-24 16:41:40.208 
Epoch 104/1000 
	 loss: 31.2095, MinusLogProbMetric: 31.2095, val_loss: 31.6720, val_MinusLogProbMetric: 31.6720

Epoch 104: val_loss did not improve from 30.45685
196/196 - 34s - loss: 31.2095 - MinusLogProbMetric: 31.2095 - val_loss: 31.6720 - val_MinusLogProbMetric: 31.6720 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 105/1000
2023-10-24 16:42:14.347 
Epoch 105/1000 
	 loss: 31.0855, MinusLogProbMetric: 31.0855, val_loss: 31.1139, val_MinusLogProbMetric: 31.1139

Epoch 105: val_loss did not improve from 30.45685
196/196 - 34s - loss: 31.0855 - MinusLogProbMetric: 31.0855 - val_loss: 31.1139 - val_MinusLogProbMetric: 31.1139 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 106/1000
2023-10-24 16:42:47.504 
Epoch 106/1000 
	 loss: 30.8540, MinusLogProbMetric: 30.8540, val_loss: 31.2620, val_MinusLogProbMetric: 31.2620

Epoch 106: val_loss did not improve from 30.45685
196/196 - 33s - loss: 30.8540 - MinusLogProbMetric: 30.8540 - val_loss: 31.2620 - val_MinusLogProbMetric: 31.2620 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 107/1000
2023-10-24 16:43:22.192 
Epoch 107/1000 
	 loss: 30.9790, MinusLogProbMetric: 30.9790, val_loss: 31.7037, val_MinusLogProbMetric: 31.7037

Epoch 107: val_loss did not improve from 30.45685
196/196 - 35s - loss: 30.9790 - MinusLogProbMetric: 30.9790 - val_loss: 31.7037 - val_MinusLogProbMetric: 31.7037 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 108/1000
2023-10-24 16:43:56.318 
Epoch 108/1000 
	 loss: 30.8604, MinusLogProbMetric: 30.8604, val_loss: 31.3408, val_MinusLogProbMetric: 31.3408

Epoch 108: val_loss did not improve from 30.45685
196/196 - 34s - loss: 30.8604 - MinusLogProbMetric: 30.8604 - val_loss: 31.3408 - val_MinusLogProbMetric: 31.3408 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 109/1000
2023-10-24 16:44:30.568 
Epoch 109/1000 
	 loss: 30.7358, MinusLogProbMetric: 30.7358, val_loss: 31.8203, val_MinusLogProbMetric: 31.8203

Epoch 109: val_loss did not improve from 30.45685
196/196 - 34s - loss: 30.7358 - MinusLogProbMetric: 30.7358 - val_loss: 31.8203 - val_MinusLogProbMetric: 31.8203 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 110/1000
2023-10-24 16:45:04.986 
Epoch 110/1000 
	 loss: 30.9231, MinusLogProbMetric: 30.9231, val_loss: 30.7648, val_MinusLogProbMetric: 30.7648

Epoch 110: val_loss did not improve from 30.45685
196/196 - 34s - loss: 30.9231 - MinusLogProbMetric: 30.9231 - val_loss: 30.7648 - val_MinusLogProbMetric: 30.7648 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 111/1000
2023-10-24 16:45:38.339 
Epoch 111/1000 
	 loss: 30.8615, MinusLogProbMetric: 30.8615, val_loss: 30.6524, val_MinusLogProbMetric: 30.6524

Epoch 111: val_loss did not improve from 30.45685
196/196 - 33s - loss: 30.8615 - MinusLogProbMetric: 30.8615 - val_loss: 30.6524 - val_MinusLogProbMetric: 30.6524 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 112/1000
2023-10-24 16:46:12.333 
Epoch 112/1000 
	 loss: 30.7977, MinusLogProbMetric: 30.7977, val_loss: 32.4018, val_MinusLogProbMetric: 32.4018

Epoch 112: val_loss did not improve from 30.45685
196/196 - 34s - loss: 30.7977 - MinusLogProbMetric: 30.7977 - val_loss: 32.4018 - val_MinusLogProbMetric: 32.4018 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 113/1000
2023-10-24 16:46:46.687 
Epoch 113/1000 
	 loss: 30.8229, MinusLogProbMetric: 30.8229, val_loss: 31.7010, val_MinusLogProbMetric: 31.7010

Epoch 113: val_loss did not improve from 30.45685
196/196 - 34s - loss: 30.8229 - MinusLogProbMetric: 30.8229 - val_loss: 31.7010 - val_MinusLogProbMetric: 31.7010 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 114/1000
2023-10-24 16:47:20.614 
Epoch 114/1000 
	 loss: 30.8519, MinusLogProbMetric: 30.8519, val_loss: 31.8105, val_MinusLogProbMetric: 31.8105

Epoch 114: val_loss did not improve from 30.45685
196/196 - 34s - loss: 30.8519 - MinusLogProbMetric: 30.8519 - val_loss: 31.8105 - val_MinusLogProbMetric: 31.8105 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 115/1000
2023-10-24 16:47:55.460 
Epoch 115/1000 
	 loss: 30.8750, MinusLogProbMetric: 30.8750, val_loss: 31.1582, val_MinusLogProbMetric: 31.1582

Epoch 115: val_loss did not improve from 30.45685
196/196 - 35s - loss: 30.8750 - MinusLogProbMetric: 30.8750 - val_loss: 31.1582 - val_MinusLogProbMetric: 31.1582 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 116/1000
2023-10-24 16:48:29.338 
Epoch 116/1000 
	 loss: 31.6697, MinusLogProbMetric: 31.6697, val_loss: 32.7903, val_MinusLogProbMetric: 32.7903

Epoch 116: val_loss did not improve from 30.45685
196/196 - 34s - loss: 31.6697 - MinusLogProbMetric: 31.6697 - val_loss: 32.7903 - val_MinusLogProbMetric: 32.7903 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 117/1000
2023-10-24 16:49:03.668 
Epoch 117/1000 
	 loss: 31.6613, MinusLogProbMetric: 31.6613, val_loss: 32.2771, val_MinusLogProbMetric: 32.2771

Epoch 117: val_loss did not improve from 30.45685
196/196 - 34s - loss: 31.6613 - MinusLogProbMetric: 31.6613 - val_loss: 32.2771 - val_MinusLogProbMetric: 32.2771 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 118/1000
2023-10-24 16:49:38.137 
Epoch 118/1000 
	 loss: 31.6507, MinusLogProbMetric: 31.6507, val_loss: 31.3129, val_MinusLogProbMetric: 31.3129

Epoch 118: val_loss did not improve from 30.45685
196/196 - 34s - loss: 31.6507 - MinusLogProbMetric: 31.6507 - val_loss: 31.3129 - val_MinusLogProbMetric: 31.3129 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 119/1000
2023-10-24 16:50:12.865 
Epoch 119/1000 
	 loss: 30.8645, MinusLogProbMetric: 30.8645, val_loss: 31.7716, val_MinusLogProbMetric: 31.7716

Epoch 119: val_loss did not improve from 30.45685
196/196 - 35s - loss: 30.8645 - MinusLogProbMetric: 30.8645 - val_loss: 31.7716 - val_MinusLogProbMetric: 31.7716 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 120/1000
2023-10-24 16:50:47.412 
Epoch 120/1000 
	 loss: 30.6385, MinusLogProbMetric: 30.6385, val_loss: 30.4597, val_MinusLogProbMetric: 30.4597

Epoch 120: val_loss did not improve from 30.45685
196/196 - 35s - loss: 30.6385 - MinusLogProbMetric: 30.6385 - val_loss: 30.4597 - val_MinusLogProbMetric: 30.4597 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 121/1000
2023-10-24 16:51:21.885 
Epoch 121/1000 
	 loss: 30.6015, MinusLogProbMetric: 30.6015, val_loss: 31.6333, val_MinusLogProbMetric: 31.6333

Epoch 121: val_loss did not improve from 30.45685
196/196 - 34s - loss: 30.6015 - MinusLogProbMetric: 30.6015 - val_loss: 31.6333 - val_MinusLogProbMetric: 31.6333 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 122/1000
2023-10-24 16:51:56.432 
Epoch 122/1000 
	 loss: 30.5874, MinusLogProbMetric: 30.5874, val_loss: 31.2571, val_MinusLogProbMetric: 31.2571

Epoch 122: val_loss did not improve from 30.45685
196/196 - 35s - loss: 30.5874 - MinusLogProbMetric: 30.5874 - val_loss: 31.2571 - val_MinusLogProbMetric: 31.2571 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 123/1000
2023-10-24 16:52:30.890 
Epoch 123/1000 
	 loss: 30.4966, MinusLogProbMetric: 30.4966, val_loss: 31.7923, val_MinusLogProbMetric: 31.7923

Epoch 123: val_loss did not improve from 30.45685
196/196 - 34s - loss: 30.4966 - MinusLogProbMetric: 30.4966 - val_loss: 31.7923 - val_MinusLogProbMetric: 31.7923 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 124/1000
2023-10-24 16:53:05.254 
Epoch 124/1000 
	 loss: 30.6130, MinusLogProbMetric: 30.6130, val_loss: 31.0569, val_MinusLogProbMetric: 31.0569

Epoch 124: val_loss did not improve from 30.45685
196/196 - 34s - loss: 30.6130 - MinusLogProbMetric: 30.6130 - val_loss: 31.0569 - val_MinusLogProbMetric: 31.0569 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 125/1000
2023-10-24 16:53:39.596 
Epoch 125/1000 
	 loss: 30.4378, MinusLogProbMetric: 30.4378, val_loss: 30.3239, val_MinusLogProbMetric: 30.3239

Epoch 125: val_loss improved from 30.45685 to 30.32390, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 30.4378 - MinusLogProbMetric: 30.4378 - val_loss: 30.3239 - val_MinusLogProbMetric: 30.3239 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 126/1000
2023-10-24 16:54:13.775 
Epoch 126/1000 
	 loss: 30.4203, MinusLogProbMetric: 30.4203, val_loss: 30.7088, val_MinusLogProbMetric: 30.7088

Epoch 126: val_loss did not improve from 30.32390
196/196 - 34s - loss: 30.4203 - MinusLogProbMetric: 30.4203 - val_loss: 30.7088 - val_MinusLogProbMetric: 30.7088 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 127/1000
2023-10-24 16:54:48.103 
Epoch 127/1000 
	 loss: 30.5879, MinusLogProbMetric: 30.5879, val_loss: 30.7323, val_MinusLogProbMetric: 30.7323

Epoch 127: val_loss did not improve from 30.32390
196/196 - 34s - loss: 30.5879 - MinusLogProbMetric: 30.5879 - val_loss: 30.7323 - val_MinusLogProbMetric: 30.7323 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 128/1000
2023-10-24 16:55:22.090 
Epoch 128/1000 
	 loss: 30.4852, MinusLogProbMetric: 30.4852, val_loss: 30.4482, val_MinusLogProbMetric: 30.4482

Epoch 128: val_loss did not improve from 30.32390
196/196 - 34s - loss: 30.4852 - MinusLogProbMetric: 30.4852 - val_loss: 30.4482 - val_MinusLogProbMetric: 30.4482 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 129/1000
2023-10-24 16:55:56.754 
Epoch 129/1000 
	 loss: 30.4609, MinusLogProbMetric: 30.4609, val_loss: 30.6046, val_MinusLogProbMetric: 30.6046

Epoch 129: val_loss did not improve from 30.32390
196/196 - 35s - loss: 30.4609 - MinusLogProbMetric: 30.4609 - val_loss: 30.6046 - val_MinusLogProbMetric: 30.6046 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 130/1000
2023-10-24 16:56:30.704 
Epoch 130/1000 
	 loss: 30.3964, MinusLogProbMetric: 30.3964, val_loss: 30.4311, val_MinusLogProbMetric: 30.4311

Epoch 130: val_loss did not improve from 30.32390
196/196 - 34s - loss: 30.3964 - MinusLogProbMetric: 30.3964 - val_loss: 30.4311 - val_MinusLogProbMetric: 30.4311 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 131/1000
2023-10-24 16:57:05.309 
Epoch 131/1000 
	 loss: 30.3947, MinusLogProbMetric: 30.3947, val_loss: 30.6573, val_MinusLogProbMetric: 30.6573

Epoch 131: val_loss did not improve from 30.32390
196/196 - 35s - loss: 30.3947 - MinusLogProbMetric: 30.3947 - val_loss: 30.6573 - val_MinusLogProbMetric: 30.6573 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 132/1000
2023-10-24 16:57:39.402 
Epoch 132/1000 
	 loss: 30.5593, MinusLogProbMetric: 30.5593, val_loss: 30.4496, val_MinusLogProbMetric: 30.4496

Epoch 132: val_loss did not improve from 30.32390
196/196 - 34s - loss: 30.5593 - MinusLogProbMetric: 30.5593 - val_loss: 30.4496 - val_MinusLogProbMetric: 30.4496 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 133/1000
2023-10-24 16:58:13.678 
Epoch 133/1000 
	 loss: 30.4441, MinusLogProbMetric: 30.4441, val_loss: 30.6083, val_MinusLogProbMetric: 30.6083

Epoch 133: val_loss did not improve from 30.32390
196/196 - 34s - loss: 30.4441 - MinusLogProbMetric: 30.4441 - val_loss: 30.6083 - val_MinusLogProbMetric: 30.6083 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 134/1000
2023-10-24 16:58:48.233 
Epoch 134/1000 
	 loss: 30.3853, MinusLogProbMetric: 30.3853, val_loss: 30.4031, val_MinusLogProbMetric: 30.4031

Epoch 134: val_loss did not improve from 30.32390
196/196 - 35s - loss: 30.3853 - MinusLogProbMetric: 30.3853 - val_loss: 30.4031 - val_MinusLogProbMetric: 30.4031 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 135/1000
2023-10-24 16:59:22.838 
Epoch 135/1000 
	 loss: 30.3752, MinusLogProbMetric: 30.3752, val_loss: 30.4608, val_MinusLogProbMetric: 30.4608

Epoch 135: val_loss did not improve from 30.32390
196/196 - 35s - loss: 30.3752 - MinusLogProbMetric: 30.3752 - val_loss: 30.4608 - val_MinusLogProbMetric: 30.4608 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 136/1000
2023-10-24 16:59:57.438 
Epoch 136/1000 
	 loss: 30.4284, MinusLogProbMetric: 30.4284, val_loss: 30.8708, val_MinusLogProbMetric: 30.8708

Epoch 136: val_loss did not improve from 30.32390
196/196 - 35s - loss: 30.4284 - MinusLogProbMetric: 30.4284 - val_loss: 30.8708 - val_MinusLogProbMetric: 30.8708 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 137/1000
2023-10-24 17:00:31.420 
Epoch 137/1000 
	 loss: 30.2653, MinusLogProbMetric: 30.2653, val_loss: 31.7413, val_MinusLogProbMetric: 31.7413

Epoch 137: val_loss did not improve from 30.32390
196/196 - 34s - loss: 30.2653 - MinusLogProbMetric: 30.2653 - val_loss: 31.7413 - val_MinusLogProbMetric: 31.7413 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 138/1000
2023-10-24 17:01:05.486 
Epoch 138/1000 
	 loss: 30.3712, MinusLogProbMetric: 30.3712, val_loss: 29.9039, val_MinusLogProbMetric: 29.9039

Epoch 138: val_loss improved from 30.32390 to 29.90388, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 30.3712 - MinusLogProbMetric: 30.3712 - val_loss: 29.9039 - val_MinusLogProbMetric: 29.9039 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 139/1000
2023-10-24 17:01:40.380 
Epoch 139/1000 
	 loss: 30.3504, MinusLogProbMetric: 30.3504, val_loss: 30.2616, val_MinusLogProbMetric: 30.2616

Epoch 139: val_loss did not improve from 29.90388
196/196 - 34s - loss: 30.3504 - MinusLogProbMetric: 30.3504 - val_loss: 30.2616 - val_MinusLogProbMetric: 30.2616 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 140/1000
2023-10-24 17:02:14.135 
Epoch 140/1000 
	 loss: 30.2588, MinusLogProbMetric: 30.2588, val_loss: 31.6108, val_MinusLogProbMetric: 31.6108

Epoch 140: val_loss did not improve from 29.90388
196/196 - 34s - loss: 30.2588 - MinusLogProbMetric: 30.2588 - val_loss: 31.6108 - val_MinusLogProbMetric: 31.6108 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 141/1000
2023-10-24 17:02:48.623 
Epoch 141/1000 
	 loss: 30.5539, MinusLogProbMetric: 30.5539, val_loss: 30.4163, val_MinusLogProbMetric: 30.4163

Epoch 141: val_loss did not improve from 29.90388
196/196 - 34s - loss: 30.5539 - MinusLogProbMetric: 30.5539 - val_loss: 30.4163 - val_MinusLogProbMetric: 30.4163 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 142/1000
2023-10-24 17:03:22.892 
Epoch 142/1000 
	 loss: 30.1181, MinusLogProbMetric: 30.1181, val_loss: 30.6981, val_MinusLogProbMetric: 30.6981

Epoch 142: val_loss did not improve from 29.90388
196/196 - 34s - loss: 30.1181 - MinusLogProbMetric: 30.1181 - val_loss: 30.6981 - val_MinusLogProbMetric: 30.6981 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 143/1000
2023-10-24 17:03:56.583 
Epoch 143/1000 
	 loss: 30.5407, MinusLogProbMetric: 30.5407, val_loss: 31.2744, val_MinusLogProbMetric: 31.2744

Epoch 143: val_loss did not improve from 29.90388
196/196 - 34s - loss: 30.5407 - MinusLogProbMetric: 30.5407 - val_loss: 31.2744 - val_MinusLogProbMetric: 31.2744 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 144/1000
2023-10-24 17:04:30.937 
Epoch 144/1000 
	 loss: 30.2549, MinusLogProbMetric: 30.2549, val_loss: 30.0908, val_MinusLogProbMetric: 30.0908

Epoch 144: val_loss did not improve from 29.90388
196/196 - 34s - loss: 30.2549 - MinusLogProbMetric: 30.2549 - val_loss: 30.0908 - val_MinusLogProbMetric: 30.0908 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 145/1000
2023-10-24 17:05:04.853 
Epoch 145/1000 
	 loss: 30.2958, MinusLogProbMetric: 30.2958, val_loss: 30.8828, val_MinusLogProbMetric: 30.8828

Epoch 145: val_loss did not improve from 29.90388
196/196 - 34s - loss: 30.2958 - MinusLogProbMetric: 30.2958 - val_loss: 30.8828 - val_MinusLogProbMetric: 30.8828 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 146/1000
2023-10-24 17:05:39.450 
Epoch 146/1000 
	 loss: 30.1057, MinusLogProbMetric: 30.1057, val_loss: 30.4394, val_MinusLogProbMetric: 30.4394

Epoch 146: val_loss did not improve from 29.90388
196/196 - 35s - loss: 30.1057 - MinusLogProbMetric: 30.1057 - val_loss: 30.4394 - val_MinusLogProbMetric: 30.4394 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 147/1000
2023-10-24 17:06:13.483 
Epoch 147/1000 
	 loss: 30.2575, MinusLogProbMetric: 30.2575, val_loss: 30.1276, val_MinusLogProbMetric: 30.1276

Epoch 147: val_loss did not improve from 29.90388
196/196 - 34s - loss: 30.2575 - MinusLogProbMetric: 30.2575 - val_loss: 30.1276 - val_MinusLogProbMetric: 30.1276 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 148/1000
2023-10-24 17:06:47.825 
Epoch 148/1000 
	 loss: 30.1020, MinusLogProbMetric: 30.1020, val_loss: 30.6089, val_MinusLogProbMetric: 30.6089

Epoch 148: val_loss did not improve from 29.90388
196/196 - 34s - loss: 30.1020 - MinusLogProbMetric: 30.1020 - val_loss: 30.6089 - val_MinusLogProbMetric: 30.6089 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 149/1000
2023-10-24 17:07:21.493 
Epoch 149/1000 
	 loss: 30.2115, MinusLogProbMetric: 30.2115, val_loss: 29.8585, val_MinusLogProbMetric: 29.8585

Epoch 149: val_loss improved from 29.90388 to 29.85847, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 30.2115 - MinusLogProbMetric: 30.2115 - val_loss: 29.8585 - val_MinusLogProbMetric: 29.8585 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 150/1000
2023-10-24 17:07:55.987 
Epoch 150/1000 
	 loss: 30.1557, MinusLogProbMetric: 30.1557, val_loss: 30.7572, val_MinusLogProbMetric: 30.7572

Epoch 150: val_loss did not improve from 29.85847
196/196 - 34s - loss: 30.1557 - MinusLogProbMetric: 30.1557 - val_loss: 30.7572 - val_MinusLogProbMetric: 30.7572 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 151/1000
2023-10-24 17:08:30.328 
Epoch 151/1000 
	 loss: 30.0981, MinusLogProbMetric: 30.0981, val_loss: 30.9840, val_MinusLogProbMetric: 30.9840

Epoch 151: val_loss did not improve from 29.85847
196/196 - 34s - loss: 30.0981 - MinusLogProbMetric: 30.0981 - val_loss: 30.9840 - val_MinusLogProbMetric: 30.9840 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 152/1000
2023-10-24 17:09:04.405 
Epoch 152/1000 
	 loss: 30.1455, MinusLogProbMetric: 30.1455, val_loss: 30.3959, val_MinusLogProbMetric: 30.3959

Epoch 152: val_loss did not improve from 29.85847
196/196 - 34s - loss: 30.1455 - MinusLogProbMetric: 30.1455 - val_loss: 30.3959 - val_MinusLogProbMetric: 30.3959 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 153/1000
2023-10-24 17:09:38.073 
Epoch 153/1000 
	 loss: 30.1787, MinusLogProbMetric: 30.1787, val_loss: 30.5157, val_MinusLogProbMetric: 30.5157

Epoch 153: val_loss did not improve from 29.85847
196/196 - 34s - loss: 30.1787 - MinusLogProbMetric: 30.1787 - val_loss: 30.5157 - val_MinusLogProbMetric: 30.5157 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 154/1000
2023-10-24 17:10:12.091 
Epoch 154/1000 
	 loss: 30.1342, MinusLogProbMetric: 30.1342, val_loss: 30.6515, val_MinusLogProbMetric: 30.6515

Epoch 154: val_loss did not improve from 29.85847
196/196 - 34s - loss: 30.1342 - MinusLogProbMetric: 30.1342 - val_loss: 30.6515 - val_MinusLogProbMetric: 30.6515 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 155/1000
2023-10-24 17:10:46.478 
Epoch 155/1000 
	 loss: 30.0112, MinusLogProbMetric: 30.0112, val_loss: 30.6299, val_MinusLogProbMetric: 30.6299

Epoch 155: val_loss did not improve from 29.85847
196/196 - 34s - loss: 30.0112 - MinusLogProbMetric: 30.0112 - val_loss: 30.6299 - val_MinusLogProbMetric: 30.6299 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 156/1000
2023-10-24 17:11:20.528 
Epoch 156/1000 
	 loss: 30.2057, MinusLogProbMetric: 30.2057, val_loss: 30.2897, val_MinusLogProbMetric: 30.2897

Epoch 156: val_loss did not improve from 29.85847
196/196 - 34s - loss: 30.2057 - MinusLogProbMetric: 30.2057 - val_loss: 30.2897 - val_MinusLogProbMetric: 30.2897 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 157/1000
2023-10-24 17:11:55.196 
Epoch 157/1000 
	 loss: 30.1687, MinusLogProbMetric: 30.1687, val_loss: 30.0956, val_MinusLogProbMetric: 30.0956

Epoch 157: val_loss did not improve from 29.85847
196/196 - 35s - loss: 30.1687 - MinusLogProbMetric: 30.1687 - val_loss: 30.0956 - val_MinusLogProbMetric: 30.0956 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 158/1000
2023-10-24 17:12:29.495 
Epoch 158/1000 
	 loss: 30.1459, MinusLogProbMetric: 30.1459, val_loss: 30.9555, val_MinusLogProbMetric: 30.9555

Epoch 158: val_loss did not improve from 29.85847
196/196 - 34s - loss: 30.1459 - MinusLogProbMetric: 30.1459 - val_loss: 30.9555 - val_MinusLogProbMetric: 30.9555 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 159/1000
2023-10-24 17:13:03.766 
Epoch 159/1000 
	 loss: 30.1643, MinusLogProbMetric: 30.1643, val_loss: 29.8321, val_MinusLogProbMetric: 29.8321

Epoch 159: val_loss improved from 29.85847 to 29.83212, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 30.1643 - MinusLogProbMetric: 30.1643 - val_loss: 29.8321 - val_MinusLogProbMetric: 29.8321 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 160/1000
2023-10-24 17:13:38.475 
Epoch 160/1000 
	 loss: 30.0346, MinusLogProbMetric: 30.0346, val_loss: 30.6649, val_MinusLogProbMetric: 30.6649

Epoch 160: val_loss did not improve from 29.83212
196/196 - 34s - loss: 30.0346 - MinusLogProbMetric: 30.0346 - val_loss: 30.6649 - val_MinusLogProbMetric: 30.6649 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 161/1000
2023-10-24 17:14:12.702 
Epoch 161/1000 
	 loss: 29.9176, MinusLogProbMetric: 29.9176, val_loss: 29.7012, val_MinusLogProbMetric: 29.7012

Epoch 161: val_loss improved from 29.83212 to 29.70118, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 29.9176 - MinusLogProbMetric: 29.9176 - val_loss: 29.7012 - val_MinusLogProbMetric: 29.7012 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 162/1000
2023-10-24 17:14:47.370 
Epoch 162/1000 
	 loss: 30.0310, MinusLogProbMetric: 30.0310, val_loss: 33.2632, val_MinusLogProbMetric: 33.2632

Epoch 162: val_loss did not improve from 29.70118
196/196 - 34s - loss: 30.0310 - MinusLogProbMetric: 30.0310 - val_loss: 33.2632 - val_MinusLogProbMetric: 33.2632 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 163/1000
2023-10-24 17:15:21.067 
Epoch 163/1000 
	 loss: 31.8392, MinusLogProbMetric: 31.8392, val_loss: 32.4560, val_MinusLogProbMetric: 32.4560

Epoch 163: val_loss did not improve from 29.70118
196/196 - 34s - loss: 31.8392 - MinusLogProbMetric: 31.8392 - val_loss: 32.4560 - val_MinusLogProbMetric: 32.4560 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 164/1000
2023-10-24 17:15:55.710 
Epoch 164/1000 
	 loss: 30.7321, MinusLogProbMetric: 30.7321, val_loss: 31.8585, val_MinusLogProbMetric: 31.8585

Epoch 164: val_loss did not improve from 29.70118
196/196 - 35s - loss: 30.7321 - MinusLogProbMetric: 30.7321 - val_loss: 31.8585 - val_MinusLogProbMetric: 31.8585 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 165/1000
2023-10-24 17:16:29.851 
Epoch 165/1000 
	 loss: 30.9656, MinusLogProbMetric: 30.9656, val_loss: 32.6086, val_MinusLogProbMetric: 32.6086

Epoch 165: val_loss did not improve from 29.70118
196/196 - 34s - loss: 30.9656 - MinusLogProbMetric: 30.9656 - val_loss: 32.6086 - val_MinusLogProbMetric: 32.6086 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 166/1000
2023-10-24 17:17:03.968 
Epoch 166/1000 
	 loss: 31.1637, MinusLogProbMetric: 31.1637, val_loss: 31.0079, val_MinusLogProbMetric: 31.0079

Epoch 166: val_loss did not improve from 29.70118
196/196 - 34s - loss: 31.1637 - MinusLogProbMetric: 31.1637 - val_loss: 31.0079 - val_MinusLogProbMetric: 31.0079 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 167/1000
2023-10-24 17:17:38.445 
Epoch 167/1000 
	 loss: 30.4425, MinusLogProbMetric: 30.4425, val_loss: 30.6958, val_MinusLogProbMetric: 30.6958

Epoch 167: val_loss did not improve from 29.70118
196/196 - 34s - loss: 30.4425 - MinusLogProbMetric: 30.4425 - val_loss: 30.6958 - val_MinusLogProbMetric: 30.6958 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 168/1000
2023-10-24 17:18:12.956 
Epoch 168/1000 
	 loss: 29.8831, MinusLogProbMetric: 29.8831, val_loss: 29.7990, val_MinusLogProbMetric: 29.7990

Epoch 168: val_loss did not improve from 29.70118
196/196 - 35s - loss: 29.8831 - MinusLogProbMetric: 29.8831 - val_loss: 29.7990 - val_MinusLogProbMetric: 29.7990 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 169/1000
2023-10-24 17:18:47.769 
Epoch 169/1000 
	 loss: 29.8597, MinusLogProbMetric: 29.8597, val_loss: 30.7396, val_MinusLogProbMetric: 30.7396

Epoch 169: val_loss did not improve from 29.70118
196/196 - 35s - loss: 29.8597 - MinusLogProbMetric: 29.8597 - val_loss: 30.7396 - val_MinusLogProbMetric: 30.7396 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 170/1000
2023-10-24 17:19:22.802 
Epoch 170/1000 
	 loss: 29.7232, MinusLogProbMetric: 29.7232, val_loss: 30.0438, val_MinusLogProbMetric: 30.0438

Epoch 170: val_loss did not improve from 29.70118
196/196 - 35s - loss: 29.7232 - MinusLogProbMetric: 29.7232 - val_loss: 30.0438 - val_MinusLogProbMetric: 30.0438 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 171/1000
2023-10-24 17:19:56.960 
Epoch 171/1000 
	 loss: 29.7417, MinusLogProbMetric: 29.7417, val_loss: 30.8624, val_MinusLogProbMetric: 30.8624

Epoch 171: val_loss did not improve from 29.70118
196/196 - 34s - loss: 29.7417 - MinusLogProbMetric: 29.7417 - val_loss: 30.8624 - val_MinusLogProbMetric: 30.8624 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 172/1000
2023-10-24 17:20:30.176 
Epoch 172/1000 
	 loss: 29.8898, MinusLogProbMetric: 29.8898, val_loss: 30.2437, val_MinusLogProbMetric: 30.2437

Epoch 172: val_loss did not improve from 29.70118
196/196 - 33s - loss: 29.8898 - MinusLogProbMetric: 29.8898 - val_loss: 30.2437 - val_MinusLogProbMetric: 30.2437 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 173/1000
2023-10-24 17:21:04.248 
Epoch 173/1000 
	 loss: 29.7973, MinusLogProbMetric: 29.7973, val_loss: 29.6771, val_MinusLogProbMetric: 29.6771

Epoch 173: val_loss improved from 29.70118 to 29.67711, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 29.7973 - MinusLogProbMetric: 29.7973 - val_loss: 29.6771 - val_MinusLogProbMetric: 29.6771 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 174/1000
2023-10-24 17:21:39.173 
Epoch 174/1000 
	 loss: 29.7698, MinusLogProbMetric: 29.7698, val_loss: 31.0295, val_MinusLogProbMetric: 31.0295

Epoch 174: val_loss did not improve from 29.67711
196/196 - 34s - loss: 29.7698 - MinusLogProbMetric: 29.7698 - val_loss: 31.0295 - val_MinusLogProbMetric: 31.0295 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 175/1000
2023-10-24 17:22:13.384 
Epoch 175/1000 
	 loss: 29.8279, MinusLogProbMetric: 29.8279, val_loss: 31.8212, val_MinusLogProbMetric: 31.8212

Epoch 175: val_loss did not improve from 29.67711
196/196 - 34s - loss: 29.8279 - MinusLogProbMetric: 29.8279 - val_loss: 31.8212 - val_MinusLogProbMetric: 31.8212 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 176/1000
2023-10-24 17:22:47.820 
Epoch 176/1000 
	 loss: 29.8525, MinusLogProbMetric: 29.8525, val_loss: 29.9699, val_MinusLogProbMetric: 29.9699

Epoch 176: val_loss did not improve from 29.67711
196/196 - 34s - loss: 29.8525 - MinusLogProbMetric: 29.8525 - val_loss: 29.9699 - val_MinusLogProbMetric: 29.9699 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 177/1000
2023-10-24 17:23:21.778 
Epoch 177/1000 
	 loss: 29.8048, MinusLogProbMetric: 29.8048, val_loss: 30.1346, val_MinusLogProbMetric: 30.1346

Epoch 177: val_loss did not improve from 29.67711
196/196 - 34s - loss: 29.8048 - MinusLogProbMetric: 29.8048 - val_loss: 30.1346 - val_MinusLogProbMetric: 30.1346 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 178/1000
2023-10-24 17:23:56.075 
Epoch 178/1000 
	 loss: 29.8243, MinusLogProbMetric: 29.8243, val_loss: 29.9030, val_MinusLogProbMetric: 29.9030

Epoch 178: val_loss did not improve from 29.67711
196/196 - 34s - loss: 29.8243 - MinusLogProbMetric: 29.8243 - val_loss: 29.9030 - val_MinusLogProbMetric: 29.9030 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 179/1000
2023-10-24 17:24:29.778 
Epoch 179/1000 
	 loss: 29.7478, MinusLogProbMetric: 29.7478, val_loss: 30.2026, val_MinusLogProbMetric: 30.2026

Epoch 179: val_loss did not improve from 29.67711
196/196 - 34s - loss: 29.7478 - MinusLogProbMetric: 29.7478 - val_loss: 30.2026 - val_MinusLogProbMetric: 30.2026 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 180/1000
2023-10-24 17:25:04.162 
Epoch 180/1000 
	 loss: 29.7224, MinusLogProbMetric: 29.7224, val_loss: 30.1485, val_MinusLogProbMetric: 30.1485

Epoch 180: val_loss did not improve from 29.67711
196/196 - 34s - loss: 29.7224 - MinusLogProbMetric: 29.7224 - val_loss: 30.1485 - val_MinusLogProbMetric: 30.1485 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 181/1000
2023-10-24 17:25:38.489 
Epoch 181/1000 
	 loss: 29.8758, MinusLogProbMetric: 29.8758, val_loss: 29.9083, val_MinusLogProbMetric: 29.9083

Epoch 181: val_loss did not improve from 29.67711
196/196 - 34s - loss: 29.8758 - MinusLogProbMetric: 29.8758 - val_loss: 29.9083 - val_MinusLogProbMetric: 29.9083 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 182/1000
2023-10-24 17:26:12.783 
Epoch 182/1000 
	 loss: 29.6804, MinusLogProbMetric: 29.6804, val_loss: 30.0375, val_MinusLogProbMetric: 30.0375

Epoch 182: val_loss did not improve from 29.67711
196/196 - 34s - loss: 29.6804 - MinusLogProbMetric: 29.6804 - val_loss: 30.0375 - val_MinusLogProbMetric: 30.0375 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 183/1000
2023-10-24 17:26:47.096 
Epoch 183/1000 
	 loss: 30.0721, MinusLogProbMetric: 30.0721, val_loss: 30.1074, val_MinusLogProbMetric: 30.1074

Epoch 183: val_loss did not improve from 29.67711
196/196 - 34s - loss: 30.0721 - MinusLogProbMetric: 30.0721 - val_loss: 30.1074 - val_MinusLogProbMetric: 30.1074 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 184/1000
2023-10-24 17:27:21.218 
Epoch 184/1000 
	 loss: 31.6175, MinusLogProbMetric: 31.6175, val_loss: 31.8741, val_MinusLogProbMetric: 31.8741

Epoch 184: val_loss did not improve from 29.67711
196/196 - 34s - loss: 31.6175 - MinusLogProbMetric: 31.6175 - val_loss: 31.8741 - val_MinusLogProbMetric: 31.8741 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 185/1000
2023-10-24 17:27:54.252 
Epoch 185/1000 
	 loss: 30.9945, MinusLogProbMetric: 30.9945, val_loss: 32.1486, val_MinusLogProbMetric: 32.1486

Epoch 185: val_loss did not improve from 29.67711
196/196 - 33s - loss: 30.9945 - MinusLogProbMetric: 30.9945 - val_loss: 32.1486 - val_MinusLogProbMetric: 32.1486 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 186/1000
2023-10-24 17:28:29.367 
Epoch 186/1000 
	 loss: 31.4664, MinusLogProbMetric: 31.4664, val_loss: 31.5017, val_MinusLogProbMetric: 31.5017

Epoch 186: val_loss did not improve from 29.67711
196/196 - 35s - loss: 31.4664 - MinusLogProbMetric: 31.4664 - val_loss: 31.5017 - val_MinusLogProbMetric: 31.5017 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 187/1000
2023-10-24 17:29:03.887 
Epoch 187/1000 
	 loss: 30.9661, MinusLogProbMetric: 30.9661, val_loss: 30.9906, val_MinusLogProbMetric: 30.9906

Epoch 187: val_loss did not improve from 29.67711
196/196 - 35s - loss: 30.9661 - MinusLogProbMetric: 30.9661 - val_loss: 30.9906 - val_MinusLogProbMetric: 30.9906 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 188/1000
2023-10-24 17:29:37.963 
Epoch 188/1000 
	 loss: 30.5039, MinusLogProbMetric: 30.5039, val_loss: 30.8225, val_MinusLogProbMetric: 30.8225

Epoch 188: val_loss did not improve from 29.67711
196/196 - 34s - loss: 30.5039 - MinusLogProbMetric: 30.5039 - val_loss: 30.8225 - val_MinusLogProbMetric: 30.8225 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 189/1000
2023-10-24 17:30:11.983 
Epoch 189/1000 
	 loss: 29.7338, MinusLogProbMetric: 29.7338, val_loss: 30.9909, val_MinusLogProbMetric: 30.9909

Epoch 189: val_loss did not improve from 29.67711
196/196 - 34s - loss: 29.7338 - MinusLogProbMetric: 29.7338 - val_loss: 30.9909 - val_MinusLogProbMetric: 30.9909 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 190/1000
2023-10-24 17:30:46.489 
Epoch 190/1000 
	 loss: 29.7406, MinusLogProbMetric: 29.7406, val_loss: 32.2484, val_MinusLogProbMetric: 32.2484

Epoch 190: val_loss did not improve from 29.67711
196/196 - 35s - loss: 29.7406 - MinusLogProbMetric: 29.7406 - val_loss: 32.2484 - val_MinusLogProbMetric: 32.2484 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 191/1000
2023-10-24 17:31:21.237 
Epoch 191/1000 
	 loss: 29.6100, MinusLogProbMetric: 29.6100, val_loss: 29.8855, val_MinusLogProbMetric: 29.8855

Epoch 191: val_loss did not improve from 29.67711
196/196 - 35s - loss: 29.6100 - MinusLogProbMetric: 29.6100 - val_loss: 29.8855 - val_MinusLogProbMetric: 29.8855 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 192/1000
2023-10-24 17:31:55.801 
Epoch 192/1000 
	 loss: 29.8265, MinusLogProbMetric: 29.8265, val_loss: 30.9247, val_MinusLogProbMetric: 30.9247

Epoch 192: val_loss did not improve from 29.67711
196/196 - 35s - loss: 29.8265 - MinusLogProbMetric: 29.8265 - val_loss: 30.9247 - val_MinusLogProbMetric: 30.9247 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 193/1000
2023-10-24 17:32:30.448 
Epoch 193/1000 
	 loss: 29.6322, MinusLogProbMetric: 29.6322, val_loss: 30.1375, val_MinusLogProbMetric: 30.1375

Epoch 193: val_loss did not improve from 29.67711
196/196 - 35s - loss: 29.6322 - MinusLogProbMetric: 29.6322 - val_loss: 30.1375 - val_MinusLogProbMetric: 30.1375 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 194/1000
2023-10-24 17:33:04.701 
Epoch 194/1000 
	 loss: 29.5488, MinusLogProbMetric: 29.5488, val_loss: 29.6528, val_MinusLogProbMetric: 29.6528

Epoch 194: val_loss improved from 29.67711 to 29.65284, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 29.5488 - MinusLogProbMetric: 29.5488 - val_loss: 29.6528 - val_MinusLogProbMetric: 29.6528 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 195/1000
2023-10-24 17:33:39.591 
Epoch 195/1000 
	 loss: 29.7250, MinusLogProbMetric: 29.7250, val_loss: 31.0349, val_MinusLogProbMetric: 31.0349

Epoch 195: val_loss did not improve from 29.65284
196/196 - 34s - loss: 29.7250 - MinusLogProbMetric: 29.7250 - val_loss: 31.0349 - val_MinusLogProbMetric: 31.0349 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 196/1000
2023-10-24 17:34:13.509 
Epoch 196/1000 
	 loss: 29.5612, MinusLogProbMetric: 29.5612, val_loss: 29.8413, val_MinusLogProbMetric: 29.8413

Epoch 196: val_loss did not improve from 29.65284
196/196 - 34s - loss: 29.5612 - MinusLogProbMetric: 29.5612 - val_loss: 29.8413 - val_MinusLogProbMetric: 29.8413 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 197/1000
2023-10-24 17:34:48.147 
Epoch 197/1000 
	 loss: 29.7305, MinusLogProbMetric: 29.7305, val_loss: 31.2311, val_MinusLogProbMetric: 31.2311

Epoch 197: val_loss did not improve from 29.65284
196/196 - 35s - loss: 29.7305 - MinusLogProbMetric: 29.7305 - val_loss: 31.2311 - val_MinusLogProbMetric: 31.2311 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 198/1000
2023-10-24 17:35:21.767 
Epoch 198/1000 
	 loss: 29.6176, MinusLogProbMetric: 29.6176, val_loss: 29.7341, val_MinusLogProbMetric: 29.7341

Epoch 198: val_loss did not improve from 29.65284
196/196 - 34s - loss: 29.6176 - MinusLogProbMetric: 29.6176 - val_loss: 29.7341 - val_MinusLogProbMetric: 29.7341 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 199/1000
2023-10-24 17:35:55.526 
Epoch 199/1000 
	 loss: 29.5893, MinusLogProbMetric: 29.5893, val_loss: 29.6327, val_MinusLogProbMetric: 29.6327

Epoch 199: val_loss improved from 29.65284 to 29.63270, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 29.5893 - MinusLogProbMetric: 29.5893 - val_loss: 29.6327 - val_MinusLogProbMetric: 29.6327 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 200/1000
2023-10-24 17:36:30.422 
Epoch 200/1000 
	 loss: 29.6322, MinusLogProbMetric: 29.6322, val_loss: 29.8534, val_MinusLogProbMetric: 29.8534

Epoch 200: val_loss did not improve from 29.63270
196/196 - 34s - loss: 29.6322 - MinusLogProbMetric: 29.6322 - val_loss: 29.8534 - val_MinusLogProbMetric: 29.8534 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 201/1000
2023-10-24 17:37:04.933 
Epoch 201/1000 
	 loss: 29.5192, MinusLogProbMetric: 29.5192, val_loss: 29.9140, val_MinusLogProbMetric: 29.9140

Epoch 201: val_loss did not improve from 29.63270
196/196 - 35s - loss: 29.5192 - MinusLogProbMetric: 29.5192 - val_loss: 29.9140 - val_MinusLogProbMetric: 29.9140 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 202/1000
2023-10-24 17:37:39.336 
Epoch 202/1000 
	 loss: 29.5813, MinusLogProbMetric: 29.5813, val_loss: 29.8800, val_MinusLogProbMetric: 29.8800

Epoch 202: val_loss did not improve from 29.63270
196/196 - 34s - loss: 29.5813 - MinusLogProbMetric: 29.5813 - val_loss: 29.8800 - val_MinusLogProbMetric: 29.8800 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 203/1000
2023-10-24 17:38:13.382 
Epoch 203/1000 
	 loss: 29.6280, MinusLogProbMetric: 29.6280, val_loss: 31.2465, val_MinusLogProbMetric: 31.2465

Epoch 203: val_loss did not improve from 29.63270
196/196 - 34s - loss: 29.6280 - MinusLogProbMetric: 29.6280 - val_loss: 31.2465 - val_MinusLogProbMetric: 31.2465 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 204/1000
2023-10-24 17:38:47.665 
Epoch 204/1000 
	 loss: 29.4873, MinusLogProbMetric: 29.4873, val_loss: 29.2684, val_MinusLogProbMetric: 29.2684

Epoch 204: val_loss improved from 29.63270 to 29.26843, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 29.4873 - MinusLogProbMetric: 29.4873 - val_loss: 29.2684 - val_MinusLogProbMetric: 29.2684 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 205/1000
2023-10-24 17:39:21.084 
Epoch 205/1000 
	 loss: 29.4723, MinusLogProbMetric: 29.4723, val_loss: 29.9069, val_MinusLogProbMetric: 29.9069

Epoch 205: val_loss did not improve from 29.26843
196/196 - 33s - loss: 29.4723 - MinusLogProbMetric: 29.4723 - val_loss: 29.9069 - val_MinusLogProbMetric: 29.9069 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 206/1000
2023-10-24 17:39:54.647 
Epoch 206/1000 
	 loss: 29.4690, MinusLogProbMetric: 29.4690, val_loss: 29.5975, val_MinusLogProbMetric: 29.5975

Epoch 206: val_loss did not improve from 29.26843
196/196 - 34s - loss: 29.4690 - MinusLogProbMetric: 29.4690 - val_loss: 29.5975 - val_MinusLogProbMetric: 29.5975 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 207/1000
2023-10-24 17:40:29.096 
Epoch 207/1000 
	 loss: 29.4750, MinusLogProbMetric: 29.4750, val_loss: 30.2571, val_MinusLogProbMetric: 30.2571

Epoch 207: val_loss did not improve from 29.26843
196/196 - 34s - loss: 29.4750 - MinusLogProbMetric: 29.4750 - val_loss: 30.2571 - val_MinusLogProbMetric: 30.2571 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 208/1000
2023-10-24 17:41:03.442 
Epoch 208/1000 
	 loss: 29.6496, MinusLogProbMetric: 29.6496, val_loss: 30.0916, val_MinusLogProbMetric: 30.0916

Epoch 208: val_loss did not improve from 29.26843
196/196 - 34s - loss: 29.6496 - MinusLogProbMetric: 29.6496 - val_loss: 30.0916 - val_MinusLogProbMetric: 30.0916 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 209/1000
2023-10-24 17:41:36.927 
Epoch 209/1000 
	 loss: 29.4424, MinusLogProbMetric: 29.4424, val_loss: 29.2191, val_MinusLogProbMetric: 29.2191

Epoch 209: val_loss improved from 29.26843 to 29.21908, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 29.4424 - MinusLogProbMetric: 29.4424 - val_loss: 29.2191 - val_MinusLogProbMetric: 29.2191 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 210/1000
2023-10-24 17:42:11.304 
Epoch 210/1000 
	 loss: 29.5813, MinusLogProbMetric: 29.5813, val_loss: 29.7108, val_MinusLogProbMetric: 29.7108

Epoch 210: val_loss did not improve from 29.21908
196/196 - 34s - loss: 29.5813 - MinusLogProbMetric: 29.5813 - val_loss: 29.7108 - val_MinusLogProbMetric: 29.7108 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 211/1000
2023-10-24 17:42:44.687 
Epoch 211/1000 
	 loss: 29.6098, MinusLogProbMetric: 29.6098, val_loss: 29.7803, val_MinusLogProbMetric: 29.7803

Epoch 211: val_loss did not improve from 29.21908
196/196 - 33s - loss: 29.6098 - MinusLogProbMetric: 29.6098 - val_loss: 29.7803 - val_MinusLogProbMetric: 29.7803 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 212/1000
2023-10-24 17:43:18.619 
Epoch 212/1000 
	 loss: 29.4081, MinusLogProbMetric: 29.4081, val_loss: 30.0403, val_MinusLogProbMetric: 30.0403

Epoch 212: val_loss did not improve from 29.21908
196/196 - 34s - loss: 29.4081 - MinusLogProbMetric: 29.4081 - val_loss: 30.0403 - val_MinusLogProbMetric: 30.0403 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 213/1000
2023-10-24 17:43:53.536 
Epoch 213/1000 
	 loss: 29.3756, MinusLogProbMetric: 29.3756, val_loss: 29.7826, val_MinusLogProbMetric: 29.7826

Epoch 213: val_loss did not improve from 29.21908
196/196 - 35s - loss: 29.3756 - MinusLogProbMetric: 29.3756 - val_loss: 29.7826 - val_MinusLogProbMetric: 29.7826 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 214/1000
2023-10-24 17:44:27.869 
Epoch 214/1000 
	 loss: 29.4917, MinusLogProbMetric: 29.4917, val_loss: 29.5192, val_MinusLogProbMetric: 29.5192

Epoch 214: val_loss did not improve from 29.21908
196/196 - 34s - loss: 29.4917 - MinusLogProbMetric: 29.4917 - val_loss: 29.5192 - val_MinusLogProbMetric: 29.5192 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 215/1000
2023-10-24 17:45:02.554 
Epoch 215/1000 
	 loss: 29.4581, MinusLogProbMetric: 29.4581, val_loss: 30.4716, val_MinusLogProbMetric: 30.4716

Epoch 215: val_loss did not improve from 29.21908
196/196 - 35s - loss: 29.4581 - MinusLogProbMetric: 29.4581 - val_loss: 30.4716 - val_MinusLogProbMetric: 30.4716 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 216/1000
2023-10-24 17:45:36.464 
Epoch 216/1000 
	 loss: 29.4537, MinusLogProbMetric: 29.4537, val_loss: 30.6312, val_MinusLogProbMetric: 30.6312

Epoch 216: val_loss did not improve from 29.21908
196/196 - 34s - loss: 29.4537 - MinusLogProbMetric: 29.4537 - val_loss: 30.6312 - val_MinusLogProbMetric: 30.6312 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 217/1000
2023-10-24 17:46:10.845 
Epoch 217/1000 
	 loss: 29.4876, MinusLogProbMetric: 29.4876, val_loss: 30.2748, val_MinusLogProbMetric: 30.2748

Epoch 217: val_loss did not improve from 29.21908
196/196 - 34s - loss: 29.4876 - MinusLogProbMetric: 29.4876 - val_loss: 30.2748 - val_MinusLogProbMetric: 30.2748 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 218/1000
2023-10-24 17:46:44.551 
Epoch 218/1000 
	 loss: 29.4083, MinusLogProbMetric: 29.4083, val_loss: 29.7555, val_MinusLogProbMetric: 29.7555

Epoch 218: val_loss did not improve from 29.21908
196/196 - 34s - loss: 29.4083 - MinusLogProbMetric: 29.4083 - val_loss: 29.7555 - val_MinusLogProbMetric: 29.7555 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 219/1000
2023-10-24 17:47:18.381 
Epoch 219/1000 
	 loss: 29.4542, MinusLogProbMetric: 29.4542, val_loss: 29.7975, val_MinusLogProbMetric: 29.7975

Epoch 219: val_loss did not improve from 29.21908
196/196 - 34s - loss: 29.4542 - MinusLogProbMetric: 29.4542 - val_loss: 29.7975 - val_MinusLogProbMetric: 29.7975 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 220/1000
2023-10-24 17:47:52.818 
Epoch 220/1000 
	 loss: 29.4737, MinusLogProbMetric: 29.4737, val_loss: 30.0888, val_MinusLogProbMetric: 30.0888

Epoch 220: val_loss did not improve from 29.21908
196/196 - 34s - loss: 29.4737 - MinusLogProbMetric: 29.4737 - val_loss: 30.0888 - val_MinusLogProbMetric: 30.0888 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 221/1000
2023-10-24 17:48:26.690 
Epoch 221/1000 
	 loss: 29.5184, MinusLogProbMetric: 29.5184, val_loss: 29.6824, val_MinusLogProbMetric: 29.6824

Epoch 221: val_loss did not improve from 29.21908
196/196 - 34s - loss: 29.5184 - MinusLogProbMetric: 29.5184 - val_loss: 29.6824 - val_MinusLogProbMetric: 29.6824 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 222/1000
2023-10-24 17:49:01.079 
Epoch 222/1000 
	 loss: 29.3402, MinusLogProbMetric: 29.3402, val_loss: 29.4324, val_MinusLogProbMetric: 29.4324

Epoch 222: val_loss did not improve from 29.21908
196/196 - 34s - loss: 29.3402 - MinusLogProbMetric: 29.3402 - val_loss: 29.4324 - val_MinusLogProbMetric: 29.4324 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 223/1000
2023-10-24 17:49:34.617 
Epoch 223/1000 
	 loss: 29.4876, MinusLogProbMetric: 29.4876, val_loss: 30.0365, val_MinusLogProbMetric: 30.0365

Epoch 223: val_loss did not improve from 29.21908
196/196 - 34s - loss: 29.4876 - MinusLogProbMetric: 29.4876 - val_loss: 30.0365 - val_MinusLogProbMetric: 30.0365 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 224/1000
2023-10-24 17:50:09.406 
Epoch 224/1000 
	 loss: 29.4738, MinusLogProbMetric: 29.4738, val_loss: 30.2126, val_MinusLogProbMetric: 30.2126

Epoch 224: val_loss did not improve from 29.21908
196/196 - 35s - loss: 29.4738 - MinusLogProbMetric: 29.4738 - val_loss: 30.2126 - val_MinusLogProbMetric: 30.2126 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 225/1000
2023-10-24 17:50:44.019 
Epoch 225/1000 
	 loss: 29.4168, MinusLogProbMetric: 29.4168, val_loss: 29.5944, val_MinusLogProbMetric: 29.5944

Epoch 225: val_loss did not improve from 29.21908
196/196 - 35s - loss: 29.4168 - MinusLogProbMetric: 29.4168 - val_loss: 29.5944 - val_MinusLogProbMetric: 29.5944 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 226/1000
2023-10-24 17:51:18.199 
Epoch 226/1000 
	 loss: 29.4280, MinusLogProbMetric: 29.4280, val_loss: 29.9830, val_MinusLogProbMetric: 29.9830

Epoch 226: val_loss did not improve from 29.21908
196/196 - 34s - loss: 29.4280 - MinusLogProbMetric: 29.4280 - val_loss: 29.9830 - val_MinusLogProbMetric: 29.9830 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 227/1000
2023-10-24 17:51:52.365 
Epoch 227/1000 
	 loss: 30.2710, MinusLogProbMetric: 30.2710, val_loss: 31.6956, val_MinusLogProbMetric: 31.6956

Epoch 227: val_loss did not improve from 29.21908
196/196 - 34s - loss: 30.2710 - MinusLogProbMetric: 30.2710 - val_loss: 31.6956 - val_MinusLogProbMetric: 31.6956 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 228/1000
2023-10-24 17:52:26.962 
Epoch 228/1000 
	 loss: 31.2175, MinusLogProbMetric: 31.2175, val_loss: 31.4732, val_MinusLogProbMetric: 31.4732

Epoch 228: val_loss did not improve from 29.21908
196/196 - 35s - loss: 31.2175 - MinusLogProbMetric: 31.2175 - val_loss: 31.4732 - val_MinusLogProbMetric: 31.4732 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 229/1000
2023-10-24 17:53:01.011 
Epoch 229/1000 
	 loss: 30.9116, MinusLogProbMetric: 30.9116, val_loss: 30.8385, val_MinusLogProbMetric: 30.8385

Epoch 229: val_loss did not improve from 29.21908
196/196 - 34s - loss: 30.9116 - MinusLogProbMetric: 30.9116 - val_loss: 30.8385 - val_MinusLogProbMetric: 30.8385 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 230/1000
2023-10-24 17:53:35.569 
Epoch 230/1000 
	 loss: 30.3560, MinusLogProbMetric: 30.3560, val_loss: 33.6619, val_MinusLogProbMetric: 33.6619

Epoch 230: val_loss did not improve from 29.21908
196/196 - 35s - loss: 30.3560 - MinusLogProbMetric: 30.3560 - val_loss: 33.6619 - val_MinusLogProbMetric: 33.6619 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 231/1000
2023-10-24 17:54:07.799 
Epoch 231/1000 
	 loss: 30.8997, MinusLogProbMetric: 30.8997, val_loss: 30.9025, val_MinusLogProbMetric: 30.9025

Epoch 231: val_loss did not improve from 29.21908
196/196 - 32s - loss: 30.8997 - MinusLogProbMetric: 30.8997 - val_loss: 30.9025 - val_MinusLogProbMetric: 30.9025 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 232/1000
2023-10-24 17:54:40.168 
Epoch 232/1000 
	 loss: 30.3859, MinusLogProbMetric: 30.3859, val_loss: 29.9648, val_MinusLogProbMetric: 29.9648

Epoch 232: val_loss did not improve from 29.21908
196/196 - 32s - loss: 30.3859 - MinusLogProbMetric: 30.3859 - val_loss: 29.9648 - val_MinusLogProbMetric: 29.9648 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 233/1000
2023-10-24 17:55:14.032 
Epoch 233/1000 
	 loss: 30.2862, MinusLogProbMetric: 30.2862, val_loss: 31.2247, val_MinusLogProbMetric: 31.2247

Epoch 233: val_loss did not improve from 29.21908
196/196 - 34s - loss: 30.2862 - MinusLogProbMetric: 30.2862 - val_loss: 31.2247 - val_MinusLogProbMetric: 31.2247 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 234/1000
2023-10-24 17:55:47.951 
Epoch 234/1000 
	 loss: 29.9597, MinusLogProbMetric: 29.9597, val_loss: 29.5446, val_MinusLogProbMetric: 29.5446

Epoch 234: val_loss did not improve from 29.21908
196/196 - 34s - loss: 29.9597 - MinusLogProbMetric: 29.9597 - val_loss: 29.5446 - val_MinusLogProbMetric: 29.5446 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 235/1000
2023-10-24 17:56:22.262 
Epoch 235/1000 
	 loss: 29.3009, MinusLogProbMetric: 29.3009, val_loss: 29.2243, val_MinusLogProbMetric: 29.2243

Epoch 235: val_loss did not improve from 29.21908
196/196 - 34s - loss: 29.3009 - MinusLogProbMetric: 29.3009 - val_loss: 29.2243 - val_MinusLogProbMetric: 29.2243 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 236/1000
2023-10-24 17:56:56.357 
Epoch 236/1000 
	 loss: 29.2939, MinusLogProbMetric: 29.2939, val_loss: 29.7395, val_MinusLogProbMetric: 29.7395

Epoch 236: val_loss did not improve from 29.21908
196/196 - 34s - loss: 29.2939 - MinusLogProbMetric: 29.2939 - val_loss: 29.7395 - val_MinusLogProbMetric: 29.7395 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 237/1000
2023-10-24 17:57:30.621 
Epoch 237/1000 
	 loss: 29.3271, MinusLogProbMetric: 29.3271, val_loss: 29.6540, val_MinusLogProbMetric: 29.6540

Epoch 237: val_loss did not improve from 29.21908
196/196 - 34s - loss: 29.3271 - MinusLogProbMetric: 29.3271 - val_loss: 29.6540 - val_MinusLogProbMetric: 29.6540 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 238/1000
2023-10-24 17:58:04.964 
Epoch 238/1000 
	 loss: 29.2887, MinusLogProbMetric: 29.2887, val_loss: 29.4338, val_MinusLogProbMetric: 29.4338

Epoch 238: val_loss did not improve from 29.21908
196/196 - 34s - loss: 29.2887 - MinusLogProbMetric: 29.2887 - val_loss: 29.4338 - val_MinusLogProbMetric: 29.4338 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 239/1000
2023-10-24 17:58:39.305 
Epoch 239/1000 
	 loss: 29.3673, MinusLogProbMetric: 29.3673, val_loss: 29.2439, val_MinusLogProbMetric: 29.2439

Epoch 239: val_loss did not improve from 29.21908
196/196 - 34s - loss: 29.3673 - MinusLogProbMetric: 29.3673 - val_loss: 29.2439 - val_MinusLogProbMetric: 29.2439 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 240/1000
2023-10-24 17:59:14.092 
Epoch 240/1000 
	 loss: 29.3040, MinusLogProbMetric: 29.3040, val_loss: 29.1839, val_MinusLogProbMetric: 29.1839

Epoch 240: val_loss improved from 29.21908 to 29.18386, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 29.3040 - MinusLogProbMetric: 29.3040 - val_loss: 29.1839 - val_MinusLogProbMetric: 29.1839 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 241/1000
2023-10-24 17:59:48.874 
Epoch 241/1000 
	 loss: 29.2581, MinusLogProbMetric: 29.2581, val_loss: 29.7901, val_MinusLogProbMetric: 29.7901

Epoch 241: val_loss did not improve from 29.18386
196/196 - 34s - loss: 29.2581 - MinusLogProbMetric: 29.2581 - val_loss: 29.7901 - val_MinusLogProbMetric: 29.7901 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 242/1000
2023-10-24 18:00:22.672 
Epoch 242/1000 
	 loss: 29.3611, MinusLogProbMetric: 29.3611, val_loss: 29.6542, val_MinusLogProbMetric: 29.6542

Epoch 242: val_loss did not improve from 29.18386
196/196 - 34s - loss: 29.3611 - MinusLogProbMetric: 29.3611 - val_loss: 29.6542 - val_MinusLogProbMetric: 29.6542 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 243/1000
2023-10-24 18:00:56.966 
Epoch 243/1000 
	 loss: 29.2283, MinusLogProbMetric: 29.2283, val_loss: 29.2605, val_MinusLogProbMetric: 29.2605

Epoch 243: val_loss did not improve from 29.18386
196/196 - 34s - loss: 29.2283 - MinusLogProbMetric: 29.2283 - val_loss: 29.2605 - val_MinusLogProbMetric: 29.2605 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 244/1000
2023-10-24 18:01:30.751 
Epoch 244/1000 
	 loss: 29.2847, MinusLogProbMetric: 29.2847, val_loss: 30.8981, val_MinusLogProbMetric: 30.8981

Epoch 244: val_loss did not improve from 29.18386
196/196 - 34s - loss: 29.2847 - MinusLogProbMetric: 29.2847 - val_loss: 30.8981 - val_MinusLogProbMetric: 30.8981 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 245/1000
2023-10-24 18:02:04.246 
Epoch 245/1000 
	 loss: 29.2667, MinusLogProbMetric: 29.2667, val_loss: 30.1005, val_MinusLogProbMetric: 30.1005

Epoch 245: val_loss did not improve from 29.18386
196/196 - 33s - loss: 29.2667 - MinusLogProbMetric: 29.2667 - val_loss: 30.1005 - val_MinusLogProbMetric: 30.1005 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 246/1000
2023-10-24 18:02:39.205 
Epoch 246/1000 
	 loss: 29.3382, MinusLogProbMetric: 29.3382, val_loss: 29.6261, val_MinusLogProbMetric: 29.6261

Epoch 246: val_loss did not improve from 29.18386
196/196 - 35s - loss: 29.3382 - MinusLogProbMetric: 29.3382 - val_loss: 29.6261 - val_MinusLogProbMetric: 29.6261 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 247/1000
2023-10-24 18:03:13.081 
Epoch 247/1000 
	 loss: 29.2861, MinusLogProbMetric: 29.2861, val_loss: 29.1422, val_MinusLogProbMetric: 29.1422

Epoch 247: val_loss improved from 29.18386 to 29.14218, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 29.2861 - MinusLogProbMetric: 29.2861 - val_loss: 29.1422 - val_MinusLogProbMetric: 29.1422 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 248/1000
2023-10-24 18:03:47.886 
Epoch 248/1000 
	 loss: 29.2465, MinusLogProbMetric: 29.2465, val_loss: 29.2082, val_MinusLogProbMetric: 29.2082

Epoch 248: val_loss did not improve from 29.14218
196/196 - 34s - loss: 29.2465 - MinusLogProbMetric: 29.2465 - val_loss: 29.2082 - val_MinusLogProbMetric: 29.2082 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 249/1000
2023-10-24 18:04:21.891 
Epoch 249/1000 
	 loss: 29.2153, MinusLogProbMetric: 29.2153, val_loss: 29.3058, val_MinusLogProbMetric: 29.3058

Epoch 249: val_loss did not improve from 29.14218
196/196 - 34s - loss: 29.2153 - MinusLogProbMetric: 29.2153 - val_loss: 29.3058 - val_MinusLogProbMetric: 29.3058 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 250/1000
2023-10-24 18:04:56.445 
Epoch 250/1000 
	 loss: 29.2750, MinusLogProbMetric: 29.2750, val_loss: 29.0804, val_MinusLogProbMetric: 29.0804

Epoch 250: val_loss improved from 29.14218 to 29.08042, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 29.2750 - MinusLogProbMetric: 29.2750 - val_loss: 29.0804 - val_MinusLogProbMetric: 29.0804 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 251/1000
2023-10-24 18:05:30.455 
Epoch 251/1000 
	 loss: 29.1769, MinusLogProbMetric: 29.1769, val_loss: 29.1479, val_MinusLogProbMetric: 29.1479

Epoch 251: val_loss did not improve from 29.08042
196/196 - 34s - loss: 29.1769 - MinusLogProbMetric: 29.1769 - val_loss: 29.1479 - val_MinusLogProbMetric: 29.1479 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 252/1000
2023-10-24 18:06:04.674 
Epoch 252/1000 
	 loss: 29.3360, MinusLogProbMetric: 29.3360, val_loss: 29.5403, val_MinusLogProbMetric: 29.5403

Epoch 252: val_loss did not improve from 29.08042
196/196 - 34s - loss: 29.3360 - MinusLogProbMetric: 29.3360 - val_loss: 29.5403 - val_MinusLogProbMetric: 29.5403 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 253/1000
2023-10-24 18:06:39.297 
Epoch 253/1000 
	 loss: 29.2659, MinusLogProbMetric: 29.2659, val_loss: 30.5114, val_MinusLogProbMetric: 30.5114

Epoch 253: val_loss did not improve from 29.08042
196/196 - 35s - loss: 29.2659 - MinusLogProbMetric: 29.2659 - val_loss: 30.5114 - val_MinusLogProbMetric: 30.5114 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 254/1000
2023-10-24 18:07:13.839 
Epoch 254/1000 
	 loss: 29.2505, MinusLogProbMetric: 29.2505, val_loss: 29.6973, val_MinusLogProbMetric: 29.6973

Epoch 254: val_loss did not improve from 29.08042
196/196 - 35s - loss: 29.2505 - MinusLogProbMetric: 29.2505 - val_loss: 29.6973 - val_MinusLogProbMetric: 29.6973 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 255/1000
2023-10-24 18:07:47.896 
Epoch 255/1000 
	 loss: 29.2769, MinusLogProbMetric: 29.2769, val_loss: 29.5322, val_MinusLogProbMetric: 29.5322

Epoch 255: val_loss did not improve from 29.08042
196/196 - 34s - loss: 29.2769 - MinusLogProbMetric: 29.2769 - val_loss: 29.5322 - val_MinusLogProbMetric: 29.5322 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 256/1000
2023-10-24 18:08:22.236 
Epoch 256/1000 
	 loss: 29.2358, MinusLogProbMetric: 29.2358, val_loss: 29.9652, val_MinusLogProbMetric: 29.9652

Epoch 256: val_loss did not improve from 29.08042
196/196 - 34s - loss: 29.2358 - MinusLogProbMetric: 29.2358 - val_loss: 29.9652 - val_MinusLogProbMetric: 29.9652 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 257/1000
2023-10-24 18:08:56.567 
Epoch 257/1000 
	 loss: 29.2673, MinusLogProbMetric: 29.2673, val_loss: 29.2293, val_MinusLogProbMetric: 29.2293

Epoch 257: val_loss did not improve from 29.08042
196/196 - 34s - loss: 29.2673 - MinusLogProbMetric: 29.2673 - val_loss: 29.2293 - val_MinusLogProbMetric: 29.2293 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 258/1000
2023-10-24 18:09:30.143 
Epoch 258/1000 
	 loss: 29.3148, MinusLogProbMetric: 29.3148, val_loss: 29.1625, val_MinusLogProbMetric: 29.1625

Epoch 258: val_loss did not improve from 29.08042
196/196 - 34s - loss: 29.3148 - MinusLogProbMetric: 29.3148 - val_loss: 29.1625 - val_MinusLogProbMetric: 29.1625 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 259/1000
2023-10-24 18:10:04.357 
Epoch 259/1000 
	 loss: 29.2886, MinusLogProbMetric: 29.2886, val_loss: 29.0454, val_MinusLogProbMetric: 29.0454

Epoch 259: val_loss improved from 29.08042 to 29.04538, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 29.2886 - MinusLogProbMetric: 29.2886 - val_loss: 29.0454 - val_MinusLogProbMetric: 29.0454 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 260/1000
2023-10-24 18:10:39.302 
Epoch 260/1000 
	 loss: 29.2183, MinusLogProbMetric: 29.2183, val_loss: 29.8236, val_MinusLogProbMetric: 29.8236

Epoch 260: val_loss did not improve from 29.04538
196/196 - 35s - loss: 29.2183 - MinusLogProbMetric: 29.2183 - val_loss: 29.8236 - val_MinusLogProbMetric: 29.8236 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 261/1000
2023-10-24 18:11:11.796 
Epoch 261/1000 
	 loss: 29.2256, MinusLogProbMetric: 29.2256, val_loss: 30.2336, val_MinusLogProbMetric: 30.2336

Epoch 261: val_loss did not improve from 29.04538
196/196 - 32s - loss: 29.2256 - MinusLogProbMetric: 29.2256 - val_loss: 30.2336 - val_MinusLogProbMetric: 30.2336 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 262/1000
2023-10-24 18:11:46.155 
Epoch 262/1000 
	 loss: 29.2192, MinusLogProbMetric: 29.2192, val_loss: 29.6783, val_MinusLogProbMetric: 29.6783

Epoch 262: val_loss did not improve from 29.04538
196/196 - 34s - loss: 29.2192 - MinusLogProbMetric: 29.2192 - val_loss: 29.6783 - val_MinusLogProbMetric: 29.6783 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 263/1000
2023-10-24 18:12:19.891 
Epoch 263/1000 
	 loss: 29.1663, MinusLogProbMetric: 29.1663, val_loss: 29.4632, val_MinusLogProbMetric: 29.4632

Epoch 263: val_loss did not improve from 29.04538
196/196 - 34s - loss: 29.1663 - MinusLogProbMetric: 29.1663 - val_loss: 29.4632 - val_MinusLogProbMetric: 29.4632 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 264/1000
2023-10-24 18:12:53.840 
Epoch 264/1000 
	 loss: 29.2000, MinusLogProbMetric: 29.2000, val_loss: 29.2937, val_MinusLogProbMetric: 29.2937

Epoch 264: val_loss did not improve from 29.04538
196/196 - 34s - loss: 29.2000 - MinusLogProbMetric: 29.2000 - val_loss: 29.2937 - val_MinusLogProbMetric: 29.2937 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 265/1000
2023-10-24 18:13:28.351 
Epoch 265/1000 
	 loss: 29.1758, MinusLogProbMetric: 29.1758, val_loss: 30.4942, val_MinusLogProbMetric: 30.4942

Epoch 265: val_loss did not improve from 29.04538
196/196 - 35s - loss: 29.1758 - MinusLogProbMetric: 29.1758 - val_loss: 30.4942 - val_MinusLogProbMetric: 30.4942 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 266/1000
2023-10-24 18:14:03.062 
Epoch 266/1000 
	 loss: 29.3169, MinusLogProbMetric: 29.3169, val_loss: 29.4986, val_MinusLogProbMetric: 29.4986

Epoch 266: val_loss did not improve from 29.04538
196/196 - 35s - loss: 29.3169 - MinusLogProbMetric: 29.3169 - val_loss: 29.4986 - val_MinusLogProbMetric: 29.4986 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 267/1000
2023-10-24 18:14:37.172 
Epoch 267/1000 
	 loss: 29.1778, MinusLogProbMetric: 29.1778, val_loss: 29.1488, val_MinusLogProbMetric: 29.1488

Epoch 267: val_loss did not improve from 29.04538
196/196 - 34s - loss: 29.1778 - MinusLogProbMetric: 29.1778 - val_loss: 29.1488 - val_MinusLogProbMetric: 29.1488 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 268/1000
2023-10-24 18:15:12.113 
Epoch 268/1000 
	 loss: 29.1847, MinusLogProbMetric: 29.1847, val_loss: 29.7409, val_MinusLogProbMetric: 29.7409

Epoch 268: val_loss did not improve from 29.04538
196/196 - 35s - loss: 29.1847 - MinusLogProbMetric: 29.1847 - val_loss: 29.7409 - val_MinusLogProbMetric: 29.7409 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 269/1000
2023-10-24 18:15:45.895 
Epoch 269/1000 
	 loss: 29.2377, MinusLogProbMetric: 29.2377, val_loss: 31.3324, val_MinusLogProbMetric: 31.3324

Epoch 269: val_loss did not improve from 29.04538
196/196 - 34s - loss: 29.2377 - MinusLogProbMetric: 29.2377 - val_loss: 31.3324 - val_MinusLogProbMetric: 31.3324 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 270/1000
2023-10-24 18:16:19.718 
Epoch 270/1000 
	 loss: 29.1681, MinusLogProbMetric: 29.1681, val_loss: 29.8897, val_MinusLogProbMetric: 29.8897

Epoch 270: val_loss did not improve from 29.04538
196/196 - 34s - loss: 29.1681 - MinusLogProbMetric: 29.1681 - val_loss: 29.8897 - val_MinusLogProbMetric: 29.8897 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 271/1000
2023-10-24 18:16:53.928 
Epoch 271/1000 
	 loss: 29.2161, MinusLogProbMetric: 29.2161, val_loss: 29.7526, val_MinusLogProbMetric: 29.7526

Epoch 271: val_loss did not improve from 29.04538
196/196 - 34s - loss: 29.2161 - MinusLogProbMetric: 29.2161 - val_loss: 29.7526 - val_MinusLogProbMetric: 29.7526 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 272/1000
2023-10-24 18:17:28.722 
Epoch 272/1000 
	 loss: 29.1358, MinusLogProbMetric: 29.1358, val_loss: 29.4309, val_MinusLogProbMetric: 29.4309

Epoch 272: val_loss did not improve from 29.04538
196/196 - 35s - loss: 29.1358 - MinusLogProbMetric: 29.1358 - val_loss: 29.4309 - val_MinusLogProbMetric: 29.4309 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 273/1000
2023-10-24 18:18:03.070 
Epoch 273/1000 
	 loss: 29.2777, MinusLogProbMetric: 29.2777, val_loss: 29.5859, val_MinusLogProbMetric: 29.5859

Epoch 273: val_loss did not improve from 29.04538
196/196 - 34s - loss: 29.2777 - MinusLogProbMetric: 29.2777 - val_loss: 29.5859 - val_MinusLogProbMetric: 29.5859 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 274/1000
2023-10-24 18:18:37.459 
Epoch 274/1000 
	 loss: 29.2682, MinusLogProbMetric: 29.2682, val_loss: 29.6505, val_MinusLogProbMetric: 29.6505

Epoch 274: val_loss did not improve from 29.04538
196/196 - 34s - loss: 29.2682 - MinusLogProbMetric: 29.2682 - val_loss: 29.6505 - val_MinusLogProbMetric: 29.6505 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 275/1000
2023-10-24 18:19:11.827 
Epoch 275/1000 
	 loss: 29.0997, MinusLogProbMetric: 29.0997, val_loss: 29.4867, val_MinusLogProbMetric: 29.4867

Epoch 275: val_loss did not improve from 29.04538
196/196 - 34s - loss: 29.0997 - MinusLogProbMetric: 29.0997 - val_loss: 29.4867 - val_MinusLogProbMetric: 29.4867 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 276/1000
2023-10-24 18:19:46.333 
Epoch 276/1000 
	 loss: 29.2129, MinusLogProbMetric: 29.2129, val_loss: 29.2798, val_MinusLogProbMetric: 29.2798

Epoch 276: val_loss did not improve from 29.04538
196/196 - 35s - loss: 29.2129 - MinusLogProbMetric: 29.2129 - val_loss: 29.2798 - val_MinusLogProbMetric: 29.2798 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 277/1000
2023-10-24 18:20:20.516 
Epoch 277/1000 
	 loss: 29.2236, MinusLogProbMetric: 29.2236, val_loss: 29.3117, val_MinusLogProbMetric: 29.3117

Epoch 277: val_loss did not improve from 29.04538
196/196 - 34s - loss: 29.2236 - MinusLogProbMetric: 29.2236 - val_loss: 29.3117 - val_MinusLogProbMetric: 29.3117 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 278/1000
2023-10-24 18:20:55.124 
Epoch 278/1000 
	 loss: 28.9947, MinusLogProbMetric: 28.9947, val_loss: 30.5210, val_MinusLogProbMetric: 30.5210

Epoch 278: val_loss did not improve from 29.04538
196/196 - 35s - loss: 28.9947 - MinusLogProbMetric: 28.9947 - val_loss: 30.5210 - val_MinusLogProbMetric: 30.5210 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 279/1000
2023-10-24 18:21:29.732 
Epoch 279/1000 
	 loss: 29.0886, MinusLogProbMetric: 29.0886, val_loss: 29.2529, val_MinusLogProbMetric: 29.2529

Epoch 279: val_loss did not improve from 29.04538
196/196 - 35s - loss: 29.0886 - MinusLogProbMetric: 29.0886 - val_loss: 29.2529 - val_MinusLogProbMetric: 29.2529 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 280/1000
2023-10-24 18:22:04.094 
Epoch 280/1000 
	 loss: 29.1386, MinusLogProbMetric: 29.1386, val_loss: 29.0074, val_MinusLogProbMetric: 29.0074

Epoch 280: val_loss improved from 29.04538 to 29.00741, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 29.1386 - MinusLogProbMetric: 29.1386 - val_loss: 29.0074 - val_MinusLogProbMetric: 29.0074 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 281/1000
2023-10-24 18:22:38.850 
Epoch 281/1000 
	 loss: 30.5473, MinusLogProbMetric: 30.5473, val_loss: 30.6316, val_MinusLogProbMetric: 30.6316

Epoch 281: val_loss did not improve from 29.00741
196/196 - 34s - loss: 30.5473 - MinusLogProbMetric: 30.5473 - val_loss: 30.6316 - val_MinusLogProbMetric: 30.6316 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 282/1000
2023-10-24 18:23:13.144 
Epoch 282/1000 
	 loss: 31.1836, MinusLogProbMetric: 31.1836, val_loss: 30.9868, val_MinusLogProbMetric: 30.9868

Epoch 282: val_loss did not improve from 29.00741
196/196 - 34s - loss: 31.1836 - MinusLogProbMetric: 31.1836 - val_loss: 30.9868 - val_MinusLogProbMetric: 30.9868 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 283/1000
2023-10-24 18:23:46.774 
Epoch 283/1000 
	 loss: 30.6598, MinusLogProbMetric: 30.6598, val_loss: 30.8852, val_MinusLogProbMetric: 30.8852

Epoch 283: val_loss did not improve from 29.00741
196/196 - 34s - loss: 30.6598 - MinusLogProbMetric: 30.6598 - val_loss: 30.8852 - val_MinusLogProbMetric: 30.8852 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 284/1000
2023-10-24 18:24:20.962 
Epoch 284/1000 
	 loss: 30.0799, MinusLogProbMetric: 30.0799, val_loss: 30.6199, val_MinusLogProbMetric: 30.6199

Epoch 284: val_loss did not improve from 29.00741
196/196 - 34s - loss: 30.0799 - MinusLogProbMetric: 30.0799 - val_loss: 30.6199 - val_MinusLogProbMetric: 30.6199 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 285/1000
2023-10-24 18:24:55.382 
Epoch 285/1000 
	 loss: 30.2372, MinusLogProbMetric: 30.2372, val_loss: 30.8435, val_MinusLogProbMetric: 30.8435

Epoch 285: val_loss did not improve from 29.00741
196/196 - 34s - loss: 30.2372 - MinusLogProbMetric: 30.2372 - val_loss: 30.8435 - val_MinusLogProbMetric: 30.8435 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 286/1000
2023-10-24 18:25:28.974 
Epoch 286/1000 
	 loss: 30.0015, MinusLogProbMetric: 30.0015, val_loss: 30.5364, val_MinusLogProbMetric: 30.5364

Epoch 286: val_loss did not improve from 29.00741
196/196 - 34s - loss: 30.0015 - MinusLogProbMetric: 30.0015 - val_loss: 30.5364 - val_MinusLogProbMetric: 30.5364 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 287/1000
2023-10-24 18:26:03.361 
Epoch 287/1000 
	 loss: 30.0269, MinusLogProbMetric: 30.0269, val_loss: 30.2778, val_MinusLogProbMetric: 30.2778

Epoch 287: val_loss did not improve from 29.00741
196/196 - 34s - loss: 30.0269 - MinusLogProbMetric: 30.0269 - val_loss: 30.2778 - val_MinusLogProbMetric: 30.2778 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 288/1000
2023-10-24 18:26:37.870 
Epoch 288/1000 
	 loss: 29.1846, MinusLogProbMetric: 29.1846, val_loss: 29.1034, val_MinusLogProbMetric: 29.1034

Epoch 288: val_loss did not improve from 29.00741
196/196 - 35s - loss: 29.1846 - MinusLogProbMetric: 29.1846 - val_loss: 29.1034 - val_MinusLogProbMetric: 29.1034 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 289/1000
2023-10-24 18:27:11.829 
Epoch 289/1000 
	 loss: 29.1076, MinusLogProbMetric: 29.1076, val_loss: 29.2332, val_MinusLogProbMetric: 29.2332

Epoch 289: val_loss did not improve from 29.00741
196/196 - 34s - loss: 29.1076 - MinusLogProbMetric: 29.1076 - val_loss: 29.2332 - val_MinusLogProbMetric: 29.2332 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 290/1000
2023-10-24 18:27:46.534 
Epoch 290/1000 
	 loss: 29.0506, MinusLogProbMetric: 29.0506, val_loss: 29.0830, val_MinusLogProbMetric: 29.0830

Epoch 290: val_loss did not improve from 29.00741
196/196 - 35s - loss: 29.0506 - MinusLogProbMetric: 29.0506 - val_loss: 29.0830 - val_MinusLogProbMetric: 29.0830 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 291/1000
2023-10-24 18:28:20.228 
Epoch 291/1000 
	 loss: 29.0846, MinusLogProbMetric: 29.0846, val_loss: 29.3271, val_MinusLogProbMetric: 29.3271

Epoch 291: val_loss did not improve from 29.00741
196/196 - 34s - loss: 29.0846 - MinusLogProbMetric: 29.0846 - val_loss: 29.3271 - val_MinusLogProbMetric: 29.3271 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 292/1000
2023-10-24 18:28:53.703 
Epoch 292/1000 
	 loss: 29.0721, MinusLogProbMetric: 29.0721, val_loss: 29.4793, val_MinusLogProbMetric: 29.4793

Epoch 292: val_loss did not improve from 29.00741
196/196 - 33s - loss: 29.0721 - MinusLogProbMetric: 29.0721 - val_loss: 29.4793 - val_MinusLogProbMetric: 29.4793 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 293/1000
2023-10-24 18:29:27.981 
Epoch 293/1000 
	 loss: 29.0104, MinusLogProbMetric: 29.0104, val_loss: 28.9854, val_MinusLogProbMetric: 28.9854

Epoch 293: val_loss improved from 29.00741 to 28.98538, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 29.0104 - MinusLogProbMetric: 29.0104 - val_loss: 28.9854 - val_MinusLogProbMetric: 28.9854 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 294/1000
2023-10-24 18:30:02.816 
Epoch 294/1000 
	 loss: 28.9491, MinusLogProbMetric: 28.9491, val_loss: 30.1367, val_MinusLogProbMetric: 30.1367

Epoch 294: val_loss did not improve from 28.98538
196/196 - 34s - loss: 28.9491 - MinusLogProbMetric: 28.9491 - val_loss: 30.1367 - val_MinusLogProbMetric: 30.1367 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 295/1000
2023-10-24 18:30:36.803 
Epoch 295/1000 
	 loss: 29.1365, MinusLogProbMetric: 29.1365, val_loss: 29.9961, val_MinusLogProbMetric: 29.9961

Epoch 295: val_loss did not improve from 28.98538
196/196 - 34s - loss: 29.1365 - MinusLogProbMetric: 29.1365 - val_loss: 29.9961 - val_MinusLogProbMetric: 29.9961 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 296/1000
2023-10-24 18:31:11.031 
Epoch 296/1000 
	 loss: 29.1320, MinusLogProbMetric: 29.1320, val_loss: 29.5035, val_MinusLogProbMetric: 29.5035

Epoch 296: val_loss did not improve from 28.98538
196/196 - 34s - loss: 29.1320 - MinusLogProbMetric: 29.1320 - val_loss: 29.5035 - val_MinusLogProbMetric: 29.5035 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 297/1000
2023-10-24 18:31:45.447 
Epoch 297/1000 
	 loss: 29.1116, MinusLogProbMetric: 29.1116, val_loss: 29.3996, val_MinusLogProbMetric: 29.3996

Epoch 297: val_loss did not improve from 28.98538
196/196 - 34s - loss: 29.1116 - MinusLogProbMetric: 29.1116 - val_loss: 29.3996 - val_MinusLogProbMetric: 29.3996 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 298/1000
2023-10-24 18:32:20.153 
Epoch 298/1000 
	 loss: 29.0227, MinusLogProbMetric: 29.0227, val_loss: 29.8188, val_MinusLogProbMetric: 29.8188

Epoch 298: val_loss did not improve from 28.98538
196/196 - 35s - loss: 29.0227 - MinusLogProbMetric: 29.0227 - val_loss: 29.8188 - val_MinusLogProbMetric: 29.8188 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 299/1000
2023-10-24 18:32:54.848 
Epoch 299/1000 
	 loss: 29.0387, MinusLogProbMetric: 29.0387, val_loss: 29.6129, val_MinusLogProbMetric: 29.6129

Epoch 299: val_loss did not improve from 28.98538
196/196 - 35s - loss: 29.0387 - MinusLogProbMetric: 29.0387 - val_loss: 29.6129 - val_MinusLogProbMetric: 29.6129 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 300/1000
2023-10-24 18:33:29.045 
Epoch 300/1000 
	 loss: 29.0366, MinusLogProbMetric: 29.0366, val_loss: 28.9701, val_MinusLogProbMetric: 28.9701

Epoch 300: val_loss improved from 28.98538 to 28.97007, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 29.0366 - MinusLogProbMetric: 29.0366 - val_loss: 28.9701 - val_MinusLogProbMetric: 28.9701 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 301/1000
2023-10-24 18:34:03.986 
Epoch 301/1000 
	 loss: 28.9211, MinusLogProbMetric: 28.9211, val_loss: 29.3964, val_MinusLogProbMetric: 29.3964

Epoch 301: val_loss did not improve from 28.97007
196/196 - 34s - loss: 28.9211 - MinusLogProbMetric: 28.9211 - val_loss: 29.3964 - val_MinusLogProbMetric: 29.3964 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 302/1000
2023-10-24 18:34:38.189 
Epoch 302/1000 
	 loss: 28.9700, MinusLogProbMetric: 28.9700, val_loss: 29.4920, val_MinusLogProbMetric: 29.4920

Epoch 302: val_loss did not improve from 28.97007
196/196 - 34s - loss: 28.9700 - MinusLogProbMetric: 28.9700 - val_loss: 29.4920 - val_MinusLogProbMetric: 29.4920 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 303/1000
2023-10-24 18:35:12.419 
Epoch 303/1000 
	 loss: 29.0795, MinusLogProbMetric: 29.0795, val_loss: 29.0146, val_MinusLogProbMetric: 29.0146

Epoch 303: val_loss did not improve from 28.97007
196/196 - 34s - loss: 29.0795 - MinusLogProbMetric: 29.0795 - val_loss: 29.0146 - val_MinusLogProbMetric: 29.0146 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 304/1000
2023-10-24 18:35:47.336 
Epoch 304/1000 
	 loss: 28.9732, MinusLogProbMetric: 28.9732, val_loss: 29.5160, val_MinusLogProbMetric: 29.5160

Epoch 304: val_loss did not improve from 28.97007
196/196 - 35s - loss: 28.9732 - MinusLogProbMetric: 28.9732 - val_loss: 29.5160 - val_MinusLogProbMetric: 29.5160 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 305/1000
2023-10-24 18:36:21.401 
Epoch 305/1000 
	 loss: 29.0428, MinusLogProbMetric: 29.0428, val_loss: 28.9412, val_MinusLogProbMetric: 28.9412

Epoch 305: val_loss improved from 28.97007 to 28.94117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 29.0428 - MinusLogProbMetric: 29.0428 - val_loss: 28.9412 - val_MinusLogProbMetric: 28.9412 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 306/1000
2023-10-24 18:36:56.716 
Epoch 306/1000 
	 loss: 28.9744, MinusLogProbMetric: 28.9744, val_loss: 29.6433, val_MinusLogProbMetric: 29.6433

Epoch 306: val_loss did not improve from 28.94117
196/196 - 35s - loss: 28.9744 - MinusLogProbMetric: 28.9744 - val_loss: 29.6433 - val_MinusLogProbMetric: 29.6433 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 307/1000
2023-10-24 18:37:31.350 
Epoch 307/1000 
	 loss: 28.9661, MinusLogProbMetric: 28.9661, val_loss: 30.0003, val_MinusLogProbMetric: 30.0003

Epoch 307: val_loss did not improve from 28.94117
196/196 - 35s - loss: 28.9661 - MinusLogProbMetric: 28.9661 - val_loss: 30.0003 - val_MinusLogProbMetric: 30.0003 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 308/1000
2023-10-24 18:38:05.857 
Epoch 308/1000 
	 loss: 28.9586, MinusLogProbMetric: 28.9586, val_loss: 30.0091, val_MinusLogProbMetric: 30.0091

Epoch 308: val_loss did not improve from 28.94117
196/196 - 35s - loss: 28.9586 - MinusLogProbMetric: 28.9586 - val_loss: 30.0091 - val_MinusLogProbMetric: 30.0091 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 309/1000
2023-10-24 18:38:40.682 
Epoch 309/1000 
	 loss: 29.1283, MinusLogProbMetric: 29.1283, val_loss: 29.4036, val_MinusLogProbMetric: 29.4036

Epoch 309: val_loss did not improve from 28.94117
196/196 - 35s - loss: 29.1283 - MinusLogProbMetric: 29.1283 - val_loss: 29.4036 - val_MinusLogProbMetric: 29.4036 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 310/1000
2023-10-24 18:39:15.584 
Epoch 310/1000 
	 loss: 28.9428, MinusLogProbMetric: 28.9428, val_loss: 28.9819, val_MinusLogProbMetric: 28.9819

Epoch 310: val_loss did not improve from 28.94117
196/196 - 35s - loss: 28.9428 - MinusLogProbMetric: 28.9428 - val_loss: 28.9819 - val_MinusLogProbMetric: 28.9819 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 311/1000
2023-10-24 18:39:50.069 
Epoch 311/1000 
	 loss: 29.1525, MinusLogProbMetric: 29.1525, val_loss: 29.1907, val_MinusLogProbMetric: 29.1907

Epoch 311: val_loss did not improve from 28.94117
196/196 - 34s - loss: 29.1525 - MinusLogProbMetric: 29.1525 - val_loss: 29.1907 - val_MinusLogProbMetric: 29.1907 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 312/1000
2023-10-24 18:40:23.835 
Epoch 312/1000 
	 loss: 29.0426, MinusLogProbMetric: 29.0426, val_loss: 29.4111, val_MinusLogProbMetric: 29.4111

Epoch 312: val_loss did not improve from 28.94117
196/196 - 34s - loss: 29.0426 - MinusLogProbMetric: 29.0426 - val_loss: 29.4111 - val_MinusLogProbMetric: 29.4111 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 313/1000
2023-10-24 18:40:58.526 
Epoch 313/1000 
	 loss: 28.9670, MinusLogProbMetric: 28.9670, val_loss: 29.7654, val_MinusLogProbMetric: 29.7654

Epoch 313: val_loss did not improve from 28.94117
196/196 - 35s - loss: 28.9670 - MinusLogProbMetric: 28.9670 - val_loss: 29.7654 - val_MinusLogProbMetric: 29.7654 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 314/1000
2023-10-24 18:41:32.413 
Epoch 314/1000 
	 loss: 29.0740, MinusLogProbMetric: 29.0740, val_loss: 29.3735, val_MinusLogProbMetric: 29.3735

Epoch 314: val_loss did not improve from 28.94117
196/196 - 34s - loss: 29.0740 - MinusLogProbMetric: 29.0740 - val_loss: 29.3735 - val_MinusLogProbMetric: 29.3735 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 315/1000
2023-10-24 18:42:07.249 
Epoch 315/1000 
	 loss: 29.0676, MinusLogProbMetric: 29.0676, val_loss: 30.2149, val_MinusLogProbMetric: 30.2149

Epoch 315: val_loss did not improve from 28.94117
196/196 - 35s - loss: 29.0676 - MinusLogProbMetric: 29.0676 - val_loss: 30.2149 - val_MinusLogProbMetric: 30.2149 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 316/1000
2023-10-24 18:42:41.497 
Epoch 316/1000 
	 loss: 29.0815, MinusLogProbMetric: 29.0815, val_loss: 29.1535, val_MinusLogProbMetric: 29.1535

Epoch 316: val_loss did not improve from 28.94117
196/196 - 34s - loss: 29.0815 - MinusLogProbMetric: 29.0815 - val_loss: 29.1535 - val_MinusLogProbMetric: 29.1535 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 317/1000
2023-10-24 18:43:15.752 
Epoch 317/1000 
	 loss: 29.0654, MinusLogProbMetric: 29.0654, val_loss: 30.2113, val_MinusLogProbMetric: 30.2113

Epoch 317: val_loss did not improve from 28.94117
196/196 - 34s - loss: 29.0654 - MinusLogProbMetric: 29.0654 - val_loss: 30.2113 - val_MinusLogProbMetric: 30.2113 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 318/1000
2023-10-24 18:43:50.318 
Epoch 318/1000 
	 loss: 28.9747, MinusLogProbMetric: 28.9747, val_loss: 29.7053, val_MinusLogProbMetric: 29.7053

Epoch 318: val_loss did not improve from 28.94117
196/196 - 35s - loss: 28.9747 - MinusLogProbMetric: 28.9747 - val_loss: 29.7053 - val_MinusLogProbMetric: 29.7053 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 319/1000
2023-10-24 18:44:24.758 
Epoch 319/1000 
	 loss: 29.1112, MinusLogProbMetric: 29.1112, val_loss: 29.1844, val_MinusLogProbMetric: 29.1844

Epoch 319: val_loss did not improve from 28.94117
196/196 - 34s - loss: 29.1112 - MinusLogProbMetric: 29.1112 - val_loss: 29.1844 - val_MinusLogProbMetric: 29.1844 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 320/1000
2023-10-24 18:44:59.462 
Epoch 320/1000 
	 loss: 28.8469, MinusLogProbMetric: 28.8469, val_loss: 29.9075, val_MinusLogProbMetric: 29.9075

Epoch 320: val_loss did not improve from 28.94117
196/196 - 35s - loss: 28.8469 - MinusLogProbMetric: 28.8469 - val_loss: 29.9075 - val_MinusLogProbMetric: 29.9075 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 321/1000
2023-10-24 18:45:33.873 
Epoch 321/1000 
	 loss: 28.9598, MinusLogProbMetric: 28.9598, val_loss: 29.0916, val_MinusLogProbMetric: 29.0916

Epoch 321: val_loss did not improve from 28.94117
196/196 - 34s - loss: 28.9598 - MinusLogProbMetric: 28.9598 - val_loss: 29.0916 - val_MinusLogProbMetric: 29.0916 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 322/1000
2023-10-24 18:46:08.379 
Epoch 322/1000 
	 loss: 28.9550, MinusLogProbMetric: 28.9550, val_loss: 28.8786, val_MinusLogProbMetric: 28.8786

Epoch 322: val_loss improved from 28.94117 to 28.87862, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 28.9550 - MinusLogProbMetric: 28.9550 - val_loss: 28.8786 - val_MinusLogProbMetric: 28.8786 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 323/1000
2023-10-24 18:46:43.431 
Epoch 323/1000 
	 loss: 28.8935, MinusLogProbMetric: 28.8935, val_loss: 28.8002, val_MinusLogProbMetric: 28.8002

Epoch 323: val_loss improved from 28.87862 to 28.80017, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 28.8935 - MinusLogProbMetric: 28.8935 - val_loss: 28.8002 - val_MinusLogProbMetric: 28.8002 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 324/1000
2023-10-24 18:47:17.680 
Epoch 324/1000 
	 loss: 28.8796, MinusLogProbMetric: 28.8796, val_loss: 29.1512, val_MinusLogProbMetric: 29.1512

Epoch 324: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.8796 - MinusLogProbMetric: 28.8796 - val_loss: 29.1512 - val_MinusLogProbMetric: 29.1512 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 325/1000
2023-10-24 18:47:51.897 
Epoch 325/1000 
	 loss: 29.0262, MinusLogProbMetric: 29.0262, val_loss: 29.5260, val_MinusLogProbMetric: 29.5260

Epoch 325: val_loss did not improve from 28.80017
196/196 - 34s - loss: 29.0262 - MinusLogProbMetric: 29.0262 - val_loss: 29.5260 - val_MinusLogProbMetric: 29.5260 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 326/1000
2023-10-24 18:48:23.459 
Epoch 326/1000 
	 loss: 28.8580, MinusLogProbMetric: 28.8580, val_loss: 29.4476, val_MinusLogProbMetric: 29.4476

Epoch 326: val_loss did not improve from 28.80017
196/196 - 32s - loss: 28.8580 - MinusLogProbMetric: 28.8580 - val_loss: 29.4476 - val_MinusLogProbMetric: 29.4476 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 327/1000
2023-10-24 18:48:52.858 
Epoch 327/1000 
	 loss: 29.0021, MinusLogProbMetric: 29.0021, val_loss: 29.4556, val_MinusLogProbMetric: 29.4556

Epoch 327: val_loss did not improve from 28.80017
196/196 - 29s - loss: 29.0021 - MinusLogProbMetric: 29.0021 - val_loss: 29.4556 - val_MinusLogProbMetric: 29.4556 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 328/1000
2023-10-24 18:49:22.805 
Epoch 328/1000 
	 loss: 28.8712, MinusLogProbMetric: 28.8712, val_loss: 30.0934, val_MinusLogProbMetric: 30.0934

Epoch 328: val_loss did not improve from 28.80017
196/196 - 30s - loss: 28.8712 - MinusLogProbMetric: 28.8712 - val_loss: 30.0934 - val_MinusLogProbMetric: 30.0934 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 329/1000
2023-10-24 18:49:52.782 
Epoch 329/1000 
	 loss: 28.9906, MinusLogProbMetric: 28.9906, val_loss: 29.2199, val_MinusLogProbMetric: 29.2199

Epoch 329: val_loss did not improve from 28.80017
196/196 - 30s - loss: 28.9906 - MinusLogProbMetric: 28.9906 - val_loss: 29.2199 - val_MinusLogProbMetric: 29.2199 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 330/1000
2023-10-24 18:50:24.851 
Epoch 330/1000 
	 loss: 28.8948, MinusLogProbMetric: 28.8948, val_loss: 30.2023, val_MinusLogProbMetric: 30.2023

Epoch 330: val_loss did not improve from 28.80017
196/196 - 32s - loss: 28.8948 - MinusLogProbMetric: 28.8948 - val_loss: 30.2023 - val_MinusLogProbMetric: 30.2023 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 331/1000
2023-10-24 18:50:59.559 
Epoch 331/1000 
	 loss: 28.8776, MinusLogProbMetric: 28.8776, val_loss: 28.9936, val_MinusLogProbMetric: 28.9936

Epoch 331: val_loss did not improve from 28.80017
196/196 - 35s - loss: 28.8776 - MinusLogProbMetric: 28.8776 - val_loss: 28.9936 - val_MinusLogProbMetric: 28.9936 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 332/1000
2023-10-24 18:51:33.847 
Epoch 332/1000 
	 loss: 28.9724, MinusLogProbMetric: 28.9724, val_loss: 29.1504, val_MinusLogProbMetric: 29.1504

Epoch 332: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.9724 - MinusLogProbMetric: 28.9724 - val_loss: 29.1504 - val_MinusLogProbMetric: 29.1504 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 333/1000
2023-10-24 18:52:06.700 
Epoch 333/1000 
	 loss: 28.9265, MinusLogProbMetric: 28.9265, val_loss: 29.1664, val_MinusLogProbMetric: 29.1664

Epoch 333: val_loss did not improve from 28.80017
196/196 - 33s - loss: 28.9265 - MinusLogProbMetric: 28.9265 - val_loss: 29.1664 - val_MinusLogProbMetric: 29.1664 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 334/1000
2023-10-24 18:52:40.900 
Epoch 334/1000 
	 loss: 28.9845, MinusLogProbMetric: 28.9845, val_loss: 29.0103, val_MinusLogProbMetric: 29.0103

Epoch 334: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.9845 - MinusLogProbMetric: 28.9845 - val_loss: 29.0103 - val_MinusLogProbMetric: 29.0103 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 335/1000
2023-10-24 18:53:15.155 
Epoch 335/1000 
	 loss: 28.9544, MinusLogProbMetric: 28.9544, val_loss: 29.4142, val_MinusLogProbMetric: 29.4142

Epoch 335: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.9544 - MinusLogProbMetric: 28.9544 - val_loss: 29.4142 - val_MinusLogProbMetric: 29.4142 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 336/1000
2023-10-24 18:53:49.712 
Epoch 336/1000 
	 loss: 28.9313, MinusLogProbMetric: 28.9313, val_loss: 29.3417, val_MinusLogProbMetric: 29.3417

Epoch 336: val_loss did not improve from 28.80017
196/196 - 35s - loss: 28.9313 - MinusLogProbMetric: 28.9313 - val_loss: 29.3417 - val_MinusLogProbMetric: 29.3417 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 337/1000
2023-10-24 18:54:22.765 
Epoch 337/1000 
	 loss: 28.9201, MinusLogProbMetric: 28.9201, val_loss: 29.8858, val_MinusLogProbMetric: 29.8858

Epoch 337: val_loss did not improve from 28.80017
196/196 - 33s - loss: 28.9201 - MinusLogProbMetric: 28.9201 - val_loss: 29.8858 - val_MinusLogProbMetric: 29.8858 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 338/1000
2023-10-24 18:54:56.275 
Epoch 338/1000 
	 loss: 28.9744, MinusLogProbMetric: 28.9744, val_loss: 29.5638, val_MinusLogProbMetric: 29.5638

Epoch 338: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.9744 - MinusLogProbMetric: 28.9744 - val_loss: 29.5638 - val_MinusLogProbMetric: 29.5638 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 339/1000
2023-10-24 18:55:30.164 
Epoch 339/1000 
	 loss: 29.0360, MinusLogProbMetric: 29.0360, val_loss: 29.9163, val_MinusLogProbMetric: 29.9163

Epoch 339: val_loss did not improve from 28.80017
196/196 - 34s - loss: 29.0360 - MinusLogProbMetric: 29.0360 - val_loss: 29.9163 - val_MinusLogProbMetric: 29.9163 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 340/1000
2023-10-24 18:56:04.041 
Epoch 340/1000 
	 loss: 30.7101, MinusLogProbMetric: 30.7101, val_loss: 29.7010, val_MinusLogProbMetric: 29.7010

Epoch 340: val_loss did not improve from 28.80017
196/196 - 34s - loss: 30.7101 - MinusLogProbMetric: 30.7101 - val_loss: 29.7010 - val_MinusLogProbMetric: 29.7010 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 341/1000
2023-10-24 18:56:38.049 
Epoch 341/1000 
	 loss: 30.2560, MinusLogProbMetric: 30.2560, val_loss: 29.6317, val_MinusLogProbMetric: 29.6317

Epoch 341: val_loss did not improve from 28.80017
196/196 - 34s - loss: 30.2560 - MinusLogProbMetric: 30.2560 - val_loss: 29.6317 - val_MinusLogProbMetric: 29.6317 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 342/1000
2023-10-24 18:57:12.459 
Epoch 342/1000 
	 loss: 29.7778, MinusLogProbMetric: 29.7778, val_loss: 30.4621, val_MinusLogProbMetric: 30.4621

Epoch 342: val_loss did not improve from 28.80017
196/196 - 34s - loss: 29.7778 - MinusLogProbMetric: 29.7778 - val_loss: 30.4621 - val_MinusLogProbMetric: 30.4621 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 343/1000
2023-10-24 18:57:46.422 
Epoch 343/1000 
	 loss: 29.9020, MinusLogProbMetric: 29.9020, val_loss: 30.9411, val_MinusLogProbMetric: 30.9411

Epoch 343: val_loss did not improve from 28.80017
196/196 - 34s - loss: 29.9020 - MinusLogProbMetric: 29.9020 - val_loss: 30.9411 - val_MinusLogProbMetric: 30.9411 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 344/1000
2023-10-24 18:58:20.929 
Epoch 344/1000 
	 loss: 30.0631, MinusLogProbMetric: 30.0631, val_loss: 29.9517, val_MinusLogProbMetric: 29.9517

Epoch 344: val_loss did not improve from 28.80017
196/196 - 35s - loss: 30.0631 - MinusLogProbMetric: 30.0631 - val_loss: 29.9517 - val_MinusLogProbMetric: 29.9517 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 345/1000
2023-10-24 18:58:55.432 
Epoch 345/1000 
	 loss: 29.6276, MinusLogProbMetric: 29.6276, val_loss: 29.5573, val_MinusLogProbMetric: 29.5573

Epoch 345: val_loss did not improve from 28.80017
196/196 - 34s - loss: 29.6276 - MinusLogProbMetric: 29.6276 - val_loss: 29.5573 - val_MinusLogProbMetric: 29.5573 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 346/1000
2023-10-24 18:59:30.095 
Epoch 346/1000 
	 loss: 29.7674, MinusLogProbMetric: 29.7674, val_loss: 29.6475, val_MinusLogProbMetric: 29.6475

Epoch 346: val_loss did not improve from 28.80017
196/196 - 35s - loss: 29.7674 - MinusLogProbMetric: 29.7674 - val_loss: 29.6475 - val_MinusLogProbMetric: 29.6475 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 347/1000
2023-10-24 19:00:04.452 
Epoch 347/1000 
	 loss: 29.7892, MinusLogProbMetric: 29.7892, val_loss: 29.2974, val_MinusLogProbMetric: 29.2974

Epoch 347: val_loss did not improve from 28.80017
196/196 - 34s - loss: 29.7892 - MinusLogProbMetric: 29.7892 - val_loss: 29.2974 - val_MinusLogProbMetric: 29.2974 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 348/1000
2023-10-24 19:00:38.898 
Epoch 348/1000 
	 loss: 28.9660, MinusLogProbMetric: 28.9660, val_loss: 29.3672, val_MinusLogProbMetric: 29.3672

Epoch 348: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.9660 - MinusLogProbMetric: 28.9660 - val_loss: 29.3672 - val_MinusLogProbMetric: 29.3672 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 349/1000
2023-10-24 19:01:13.269 
Epoch 349/1000 
	 loss: 28.8946, MinusLogProbMetric: 28.8946, val_loss: 29.1842, val_MinusLogProbMetric: 29.1842

Epoch 349: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.8946 - MinusLogProbMetric: 28.8946 - val_loss: 29.1842 - val_MinusLogProbMetric: 29.1842 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 350/1000
2023-10-24 19:01:47.102 
Epoch 350/1000 
	 loss: 28.8139, MinusLogProbMetric: 28.8139, val_loss: 28.9996, val_MinusLogProbMetric: 28.9996

Epoch 350: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.8139 - MinusLogProbMetric: 28.8139 - val_loss: 28.9996 - val_MinusLogProbMetric: 28.9996 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 351/1000
2023-10-24 19:02:21.856 
Epoch 351/1000 
	 loss: 28.8655, MinusLogProbMetric: 28.8655, val_loss: 29.5858, val_MinusLogProbMetric: 29.5858

Epoch 351: val_loss did not improve from 28.80017
196/196 - 35s - loss: 28.8655 - MinusLogProbMetric: 28.8655 - val_loss: 29.5858 - val_MinusLogProbMetric: 29.5858 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 352/1000
2023-10-24 19:02:56.740 
Epoch 352/1000 
	 loss: 28.8000, MinusLogProbMetric: 28.8000, val_loss: 29.2278, val_MinusLogProbMetric: 29.2278

Epoch 352: val_loss did not improve from 28.80017
196/196 - 35s - loss: 28.8000 - MinusLogProbMetric: 28.8000 - val_loss: 29.2278 - val_MinusLogProbMetric: 29.2278 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 353/1000
2023-10-24 19:03:30.962 
Epoch 353/1000 
	 loss: 28.7916, MinusLogProbMetric: 28.7916, val_loss: 28.9352, val_MinusLogProbMetric: 28.9352

Epoch 353: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.7916 - MinusLogProbMetric: 28.7916 - val_loss: 28.9352 - val_MinusLogProbMetric: 28.9352 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 354/1000
2023-10-24 19:04:05.269 
Epoch 354/1000 
	 loss: 28.7840, MinusLogProbMetric: 28.7840, val_loss: 29.5178, val_MinusLogProbMetric: 29.5178

Epoch 354: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.7840 - MinusLogProbMetric: 28.7840 - val_loss: 29.5178 - val_MinusLogProbMetric: 29.5178 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 355/1000
2023-10-24 19:04:39.810 
Epoch 355/1000 
	 loss: 28.8065, MinusLogProbMetric: 28.8065, val_loss: 29.4188, val_MinusLogProbMetric: 29.4188

Epoch 355: val_loss did not improve from 28.80017
196/196 - 35s - loss: 28.8065 - MinusLogProbMetric: 28.8065 - val_loss: 29.4188 - val_MinusLogProbMetric: 29.4188 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 356/1000
2023-10-24 19:05:14.374 
Epoch 356/1000 
	 loss: 28.7862, MinusLogProbMetric: 28.7862, val_loss: 29.5981, val_MinusLogProbMetric: 29.5981

Epoch 356: val_loss did not improve from 28.80017
196/196 - 35s - loss: 28.7862 - MinusLogProbMetric: 28.7862 - val_loss: 29.5981 - val_MinusLogProbMetric: 29.5981 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 357/1000
2023-10-24 19:05:48.754 
Epoch 357/1000 
	 loss: 28.8739, MinusLogProbMetric: 28.8739, val_loss: 29.0339, val_MinusLogProbMetric: 29.0339

Epoch 357: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.8739 - MinusLogProbMetric: 28.8739 - val_loss: 29.0339 - val_MinusLogProbMetric: 29.0339 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 358/1000
2023-10-24 19:06:22.840 
Epoch 358/1000 
	 loss: 28.7996, MinusLogProbMetric: 28.7996, val_loss: 29.1741, val_MinusLogProbMetric: 29.1741

Epoch 358: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.7996 - MinusLogProbMetric: 28.7996 - val_loss: 29.1741 - val_MinusLogProbMetric: 29.1741 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 359/1000
2023-10-24 19:06:57.600 
Epoch 359/1000 
	 loss: 28.7760, MinusLogProbMetric: 28.7760, val_loss: 28.9213, val_MinusLogProbMetric: 28.9213

Epoch 359: val_loss did not improve from 28.80017
196/196 - 35s - loss: 28.7760 - MinusLogProbMetric: 28.7760 - val_loss: 28.9213 - val_MinusLogProbMetric: 28.9213 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 360/1000
2023-10-24 19:07:32.305 
Epoch 360/1000 
	 loss: 28.7528, MinusLogProbMetric: 28.7528, val_loss: 29.0851, val_MinusLogProbMetric: 29.0851

Epoch 360: val_loss did not improve from 28.80017
196/196 - 35s - loss: 28.7528 - MinusLogProbMetric: 28.7528 - val_loss: 29.0851 - val_MinusLogProbMetric: 29.0851 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 361/1000
2023-10-24 19:08:07.144 
Epoch 361/1000 
	 loss: 28.8559, MinusLogProbMetric: 28.8559, val_loss: 29.3982, val_MinusLogProbMetric: 29.3982

Epoch 361: val_loss did not improve from 28.80017
196/196 - 35s - loss: 28.8559 - MinusLogProbMetric: 28.8559 - val_loss: 29.3982 - val_MinusLogProbMetric: 29.3982 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 362/1000
2023-10-24 19:08:41.950 
Epoch 362/1000 
	 loss: 28.8177, MinusLogProbMetric: 28.8177, val_loss: 29.3168, val_MinusLogProbMetric: 29.3168

Epoch 362: val_loss did not improve from 28.80017
196/196 - 35s - loss: 28.8177 - MinusLogProbMetric: 28.8177 - val_loss: 29.3168 - val_MinusLogProbMetric: 29.3168 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 363/1000
2023-10-24 19:09:16.223 
Epoch 363/1000 
	 loss: 28.7844, MinusLogProbMetric: 28.7844, val_loss: 29.8581, val_MinusLogProbMetric: 29.8581

Epoch 363: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.7844 - MinusLogProbMetric: 28.7844 - val_loss: 29.8581 - val_MinusLogProbMetric: 29.8581 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 364/1000
2023-10-24 19:09:50.334 
Epoch 364/1000 
	 loss: 28.7923, MinusLogProbMetric: 28.7923, val_loss: 29.5444, val_MinusLogProbMetric: 29.5444

Epoch 364: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.7923 - MinusLogProbMetric: 28.7923 - val_loss: 29.5444 - val_MinusLogProbMetric: 29.5444 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 365/1000
2023-10-24 19:10:24.927 
Epoch 365/1000 
	 loss: 28.8668, MinusLogProbMetric: 28.8668, val_loss: 29.2759, val_MinusLogProbMetric: 29.2759

Epoch 365: val_loss did not improve from 28.80017
196/196 - 35s - loss: 28.8668 - MinusLogProbMetric: 28.8668 - val_loss: 29.2759 - val_MinusLogProbMetric: 29.2759 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 366/1000
2023-10-24 19:10:59.094 
Epoch 366/1000 
	 loss: 28.7826, MinusLogProbMetric: 28.7826, val_loss: 29.0070, val_MinusLogProbMetric: 29.0070

Epoch 366: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.7826 - MinusLogProbMetric: 28.7826 - val_loss: 29.0070 - val_MinusLogProbMetric: 29.0070 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 367/1000
2023-10-24 19:11:33.911 
Epoch 367/1000 
	 loss: 28.8589, MinusLogProbMetric: 28.8589, val_loss: 29.7805, val_MinusLogProbMetric: 29.7805

Epoch 367: val_loss did not improve from 28.80017
196/196 - 35s - loss: 28.8589 - MinusLogProbMetric: 28.8589 - val_loss: 29.7805 - val_MinusLogProbMetric: 29.7805 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 368/1000
2023-10-24 19:12:07.782 
Epoch 368/1000 
	 loss: 28.7459, MinusLogProbMetric: 28.7459, val_loss: 29.3237, val_MinusLogProbMetric: 29.3237

Epoch 368: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.7459 - MinusLogProbMetric: 28.7459 - val_loss: 29.3237 - val_MinusLogProbMetric: 29.3237 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 369/1000
2023-10-24 19:12:42.104 
Epoch 369/1000 
	 loss: 28.7661, MinusLogProbMetric: 28.7661, val_loss: 28.9657, val_MinusLogProbMetric: 28.9657

Epoch 369: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.7661 - MinusLogProbMetric: 28.7661 - val_loss: 28.9657 - val_MinusLogProbMetric: 28.9657 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 370/1000
2023-10-24 19:13:16.343 
Epoch 370/1000 
	 loss: 28.7315, MinusLogProbMetric: 28.7315, val_loss: 29.0245, val_MinusLogProbMetric: 29.0245

Epoch 370: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.7315 - MinusLogProbMetric: 28.7315 - val_loss: 29.0245 - val_MinusLogProbMetric: 29.0245 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 371/1000
2023-10-24 19:13:50.832 
Epoch 371/1000 
	 loss: 28.8660, MinusLogProbMetric: 28.8660, val_loss: 28.8979, val_MinusLogProbMetric: 28.8979

Epoch 371: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.8660 - MinusLogProbMetric: 28.8660 - val_loss: 28.8979 - val_MinusLogProbMetric: 28.8979 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 372/1000
2023-10-24 19:14:25.694 
Epoch 372/1000 
	 loss: 28.7853, MinusLogProbMetric: 28.7853, val_loss: 29.0871, val_MinusLogProbMetric: 29.0871

Epoch 372: val_loss did not improve from 28.80017
196/196 - 35s - loss: 28.7853 - MinusLogProbMetric: 28.7853 - val_loss: 29.0871 - val_MinusLogProbMetric: 29.0871 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 373/1000
2023-10-24 19:14:59.876 
Epoch 373/1000 
	 loss: 28.8205, MinusLogProbMetric: 28.8205, val_loss: 29.3083, val_MinusLogProbMetric: 29.3083

Epoch 373: val_loss did not improve from 28.80017
196/196 - 34s - loss: 28.8205 - MinusLogProbMetric: 28.8205 - val_loss: 29.3083 - val_MinusLogProbMetric: 29.3083 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 374/1000
2023-10-24 19:15:34.248 
Epoch 374/1000 
	 loss: 28.2172, MinusLogProbMetric: 28.2172, val_loss: 28.4262, val_MinusLogProbMetric: 28.4262

Epoch 374: val_loss improved from 28.80017 to 28.42622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 28.2172 - MinusLogProbMetric: 28.2172 - val_loss: 28.4262 - val_MinusLogProbMetric: 28.4262 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 375/1000
2023-10-24 19:16:09.020 
Epoch 375/1000 
	 loss: 28.1984, MinusLogProbMetric: 28.1984, val_loss: 28.3630, val_MinusLogProbMetric: 28.3630

Epoch 375: val_loss improved from 28.42622 to 28.36301, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 28.1984 - MinusLogProbMetric: 28.1984 - val_loss: 28.3630 - val_MinusLogProbMetric: 28.3630 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 376/1000
2023-10-24 19:16:44.207 
Epoch 376/1000 
	 loss: 28.1564, MinusLogProbMetric: 28.1564, val_loss: 28.4932, val_MinusLogProbMetric: 28.4932

Epoch 376: val_loss did not improve from 28.36301
196/196 - 35s - loss: 28.1564 - MinusLogProbMetric: 28.1564 - val_loss: 28.4932 - val_MinusLogProbMetric: 28.4932 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 377/1000
2023-10-24 19:17:18.249 
Epoch 377/1000 
	 loss: 28.1790, MinusLogProbMetric: 28.1790, val_loss: 28.5115, val_MinusLogProbMetric: 28.5115

Epoch 377: val_loss did not improve from 28.36301
196/196 - 34s - loss: 28.1790 - MinusLogProbMetric: 28.1790 - val_loss: 28.5115 - val_MinusLogProbMetric: 28.5115 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 378/1000
2023-10-24 19:17:52.530 
Epoch 378/1000 
	 loss: 28.1700, MinusLogProbMetric: 28.1700, val_loss: 28.5371, val_MinusLogProbMetric: 28.5371

Epoch 378: val_loss did not improve from 28.36301
196/196 - 34s - loss: 28.1700 - MinusLogProbMetric: 28.1700 - val_loss: 28.5371 - val_MinusLogProbMetric: 28.5371 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 379/1000
2023-10-24 19:18:26.763 
Epoch 379/1000 
	 loss: 28.1670, MinusLogProbMetric: 28.1670, val_loss: 28.4913, val_MinusLogProbMetric: 28.4913

Epoch 379: val_loss did not improve from 28.36301
196/196 - 34s - loss: 28.1670 - MinusLogProbMetric: 28.1670 - val_loss: 28.4913 - val_MinusLogProbMetric: 28.4913 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 380/1000
2023-10-24 19:19:01.275 
Epoch 380/1000 
	 loss: 28.1857, MinusLogProbMetric: 28.1857, val_loss: 28.4403, val_MinusLogProbMetric: 28.4403

Epoch 380: val_loss did not improve from 28.36301
196/196 - 35s - loss: 28.1857 - MinusLogProbMetric: 28.1857 - val_loss: 28.4403 - val_MinusLogProbMetric: 28.4403 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 381/1000
2023-10-24 19:19:35.277 
Epoch 381/1000 
	 loss: 28.1798, MinusLogProbMetric: 28.1798, val_loss: 28.3879, val_MinusLogProbMetric: 28.3879

Epoch 381: val_loss did not improve from 28.36301
196/196 - 34s - loss: 28.1798 - MinusLogProbMetric: 28.1798 - val_loss: 28.3879 - val_MinusLogProbMetric: 28.3879 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 382/1000
2023-10-24 19:20:10.261 
Epoch 382/1000 
	 loss: 28.1746, MinusLogProbMetric: 28.1746, val_loss: 28.4998, val_MinusLogProbMetric: 28.4998

Epoch 382: val_loss did not improve from 28.36301
196/196 - 35s - loss: 28.1746 - MinusLogProbMetric: 28.1746 - val_loss: 28.4998 - val_MinusLogProbMetric: 28.4998 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 383/1000
2023-10-24 19:20:44.686 
Epoch 383/1000 
	 loss: 28.1664, MinusLogProbMetric: 28.1664, val_loss: 28.4433, val_MinusLogProbMetric: 28.4433

Epoch 383: val_loss did not improve from 28.36301
196/196 - 34s - loss: 28.1664 - MinusLogProbMetric: 28.1664 - val_loss: 28.4433 - val_MinusLogProbMetric: 28.4433 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 384/1000
2023-10-24 19:21:19.501 
Epoch 384/1000 
	 loss: 28.1833, MinusLogProbMetric: 28.1833, val_loss: 28.6959, val_MinusLogProbMetric: 28.6959

Epoch 384: val_loss did not improve from 28.36301
196/196 - 35s - loss: 28.1833 - MinusLogProbMetric: 28.1833 - val_loss: 28.6959 - val_MinusLogProbMetric: 28.6959 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 385/1000
2023-10-24 19:21:53.666 
Epoch 385/1000 
	 loss: 28.1729, MinusLogProbMetric: 28.1729, val_loss: 28.5474, val_MinusLogProbMetric: 28.5474

Epoch 385: val_loss did not improve from 28.36301
196/196 - 34s - loss: 28.1729 - MinusLogProbMetric: 28.1729 - val_loss: 28.5474 - val_MinusLogProbMetric: 28.5474 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 386/1000
2023-10-24 19:22:27.459 
Epoch 386/1000 
	 loss: 28.2002, MinusLogProbMetric: 28.2002, val_loss: 28.3375, val_MinusLogProbMetric: 28.3375

Epoch 386: val_loss improved from 28.36301 to 28.33751, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 28.2002 - MinusLogProbMetric: 28.2002 - val_loss: 28.3375 - val_MinusLogProbMetric: 28.3375 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 387/1000
2023-10-24 19:23:02.436 
Epoch 387/1000 
	 loss: 28.1751, MinusLogProbMetric: 28.1751, val_loss: 28.4608, val_MinusLogProbMetric: 28.4608

Epoch 387: val_loss did not improve from 28.33751
196/196 - 34s - loss: 28.1751 - MinusLogProbMetric: 28.1751 - val_loss: 28.4608 - val_MinusLogProbMetric: 28.4608 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 388/1000
2023-10-24 19:23:36.776 
Epoch 388/1000 
	 loss: 28.1583, MinusLogProbMetric: 28.1583, val_loss: 28.3260, val_MinusLogProbMetric: 28.3260

Epoch 388: val_loss improved from 28.33751 to 28.32603, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 28.1583 - MinusLogProbMetric: 28.1583 - val_loss: 28.3260 - val_MinusLogProbMetric: 28.3260 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 389/1000
2023-10-24 19:24:11.552 
Epoch 389/1000 
	 loss: 28.1509, MinusLogProbMetric: 28.1509, val_loss: 28.6062, val_MinusLogProbMetric: 28.6062

Epoch 389: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1509 - MinusLogProbMetric: 28.1509 - val_loss: 28.6062 - val_MinusLogProbMetric: 28.6062 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 390/1000
2023-10-24 19:24:45.804 
Epoch 390/1000 
	 loss: 28.1538, MinusLogProbMetric: 28.1538, val_loss: 28.5261, val_MinusLogProbMetric: 28.5261

Epoch 390: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1538 - MinusLogProbMetric: 28.1538 - val_loss: 28.5261 - val_MinusLogProbMetric: 28.5261 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 391/1000
2023-10-24 19:25:20.595 
Epoch 391/1000 
	 loss: 28.1531, MinusLogProbMetric: 28.1531, val_loss: 28.4280, val_MinusLogProbMetric: 28.4280

Epoch 391: val_loss did not improve from 28.32603
196/196 - 35s - loss: 28.1531 - MinusLogProbMetric: 28.1531 - val_loss: 28.4280 - val_MinusLogProbMetric: 28.4280 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 392/1000
2023-10-24 19:25:53.958 
Epoch 392/1000 
	 loss: 28.1716, MinusLogProbMetric: 28.1716, val_loss: 28.3872, val_MinusLogProbMetric: 28.3872

Epoch 392: val_loss did not improve from 28.32603
196/196 - 33s - loss: 28.1716 - MinusLogProbMetric: 28.1716 - val_loss: 28.3872 - val_MinusLogProbMetric: 28.3872 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 393/1000
2023-10-24 19:26:27.514 
Epoch 393/1000 
	 loss: 28.1419, MinusLogProbMetric: 28.1419, val_loss: 28.7740, val_MinusLogProbMetric: 28.7740

Epoch 393: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1419 - MinusLogProbMetric: 28.1419 - val_loss: 28.7740 - val_MinusLogProbMetric: 28.7740 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 394/1000
2023-10-24 19:27:01.669 
Epoch 394/1000 
	 loss: 28.2028, MinusLogProbMetric: 28.2028, val_loss: 28.5296, val_MinusLogProbMetric: 28.5296

Epoch 394: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.2028 - MinusLogProbMetric: 28.2028 - val_loss: 28.5296 - val_MinusLogProbMetric: 28.5296 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 395/1000
2023-10-24 19:27:35.911 
Epoch 395/1000 
	 loss: 28.1625, MinusLogProbMetric: 28.1625, val_loss: 28.4476, val_MinusLogProbMetric: 28.4476

Epoch 395: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1625 - MinusLogProbMetric: 28.1625 - val_loss: 28.4476 - val_MinusLogProbMetric: 28.4476 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 396/1000
2023-10-24 19:28:09.292 
Epoch 396/1000 
	 loss: 28.1957, MinusLogProbMetric: 28.1957, val_loss: 28.5464, val_MinusLogProbMetric: 28.5464

Epoch 396: val_loss did not improve from 28.32603
196/196 - 33s - loss: 28.1957 - MinusLogProbMetric: 28.1957 - val_loss: 28.5464 - val_MinusLogProbMetric: 28.5464 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 397/1000
2023-10-24 19:28:43.323 
Epoch 397/1000 
	 loss: 28.1611, MinusLogProbMetric: 28.1611, val_loss: 28.5360, val_MinusLogProbMetric: 28.5360

Epoch 397: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1611 - MinusLogProbMetric: 28.1611 - val_loss: 28.5360 - val_MinusLogProbMetric: 28.5360 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 398/1000
2023-10-24 19:29:17.360 
Epoch 398/1000 
	 loss: 28.1982, MinusLogProbMetric: 28.1982, val_loss: 28.5349, val_MinusLogProbMetric: 28.5349

Epoch 398: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1982 - MinusLogProbMetric: 28.1982 - val_loss: 28.5349 - val_MinusLogProbMetric: 28.5349 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 399/1000
2023-10-24 19:29:51.328 
Epoch 399/1000 
	 loss: 28.1846, MinusLogProbMetric: 28.1846, val_loss: 28.5651, val_MinusLogProbMetric: 28.5651

Epoch 399: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1846 - MinusLogProbMetric: 28.1846 - val_loss: 28.5651 - val_MinusLogProbMetric: 28.5651 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 400/1000
2023-10-24 19:30:25.931 
Epoch 400/1000 
	 loss: 28.1609, MinusLogProbMetric: 28.1609, val_loss: 28.4137, val_MinusLogProbMetric: 28.4137

Epoch 400: val_loss did not improve from 28.32603
196/196 - 35s - loss: 28.1609 - MinusLogProbMetric: 28.1609 - val_loss: 28.4137 - val_MinusLogProbMetric: 28.4137 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 401/1000
2023-10-24 19:30:59.918 
Epoch 401/1000 
	 loss: 28.1571, MinusLogProbMetric: 28.1571, val_loss: 28.6625, val_MinusLogProbMetric: 28.6625

Epoch 401: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1571 - MinusLogProbMetric: 28.1571 - val_loss: 28.6625 - val_MinusLogProbMetric: 28.6625 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 402/1000
2023-10-24 19:31:34.301 
Epoch 402/1000 
	 loss: 28.1516, MinusLogProbMetric: 28.1516, val_loss: 28.8048, val_MinusLogProbMetric: 28.8048

Epoch 402: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1516 - MinusLogProbMetric: 28.1516 - val_loss: 28.8048 - val_MinusLogProbMetric: 28.8048 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 403/1000
2023-10-24 19:32:08.731 
Epoch 403/1000 
	 loss: 28.1895, MinusLogProbMetric: 28.1895, val_loss: 28.3618, val_MinusLogProbMetric: 28.3618

Epoch 403: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1895 - MinusLogProbMetric: 28.1895 - val_loss: 28.3618 - val_MinusLogProbMetric: 28.3618 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 404/1000
2023-10-24 19:32:43.261 
Epoch 404/1000 
	 loss: 28.1632, MinusLogProbMetric: 28.1632, val_loss: 28.3910, val_MinusLogProbMetric: 28.3910

Epoch 404: val_loss did not improve from 28.32603
196/196 - 35s - loss: 28.1632 - MinusLogProbMetric: 28.1632 - val_loss: 28.3910 - val_MinusLogProbMetric: 28.3910 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 405/1000
2023-10-24 19:33:16.860 
Epoch 405/1000 
	 loss: 28.1325, MinusLogProbMetric: 28.1325, val_loss: 28.6146, val_MinusLogProbMetric: 28.6146

Epoch 405: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1325 - MinusLogProbMetric: 28.1325 - val_loss: 28.6146 - val_MinusLogProbMetric: 28.6146 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 406/1000
2023-10-24 19:33:51.295 
Epoch 406/1000 
	 loss: 28.1752, MinusLogProbMetric: 28.1752, val_loss: 28.5384, val_MinusLogProbMetric: 28.5384

Epoch 406: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1752 - MinusLogProbMetric: 28.1752 - val_loss: 28.5384 - val_MinusLogProbMetric: 28.5384 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 407/1000
2023-10-24 19:34:24.381 
Epoch 407/1000 
	 loss: 28.1745, MinusLogProbMetric: 28.1745, val_loss: 28.4542, val_MinusLogProbMetric: 28.4542

Epoch 407: val_loss did not improve from 28.32603
196/196 - 33s - loss: 28.1745 - MinusLogProbMetric: 28.1745 - val_loss: 28.4542 - val_MinusLogProbMetric: 28.4542 - lr: 5.0000e-04 - 33s/epoch - 169ms/step
Epoch 408/1000
2023-10-24 19:34:59.227 
Epoch 408/1000 
	 loss: 28.1860, MinusLogProbMetric: 28.1860, val_loss: 28.4321, val_MinusLogProbMetric: 28.4321

Epoch 408: val_loss did not improve from 28.32603
196/196 - 35s - loss: 28.1860 - MinusLogProbMetric: 28.1860 - val_loss: 28.4321 - val_MinusLogProbMetric: 28.4321 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 409/1000
2023-10-24 19:35:32.779 
Epoch 409/1000 
	 loss: 28.1531, MinusLogProbMetric: 28.1531, val_loss: 28.5271, val_MinusLogProbMetric: 28.5271

Epoch 409: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1531 - MinusLogProbMetric: 28.1531 - val_loss: 28.5271 - val_MinusLogProbMetric: 28.5271 - lr: 5.0000e-04 - 34s/epoch - 171ms/step
Epoch 410/1000
2023-10-24 19:36:07.349 
Epoch 410/1000 
	 loss: 28.1895, MinusLogProbMetric: 28.1895, val_loss: 28.5234, val_MinusLogProbMetric: 28.5234

Epoch 410: val_loss did not improve from 28.32603
196/196 - 35s - loss: 28.1895 - MinusLogProbMetric: 28.1895 - val_loss: 28.5234 - val_MinusLogProbMetric: 28.5234 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 411/1000
2023-10-24 19:36:41.050 
Epoch 411/1000 
	 loss: 28.1634, MinusLogProbMetric: 28.1634, val_loss: 28.3515, val_MinusLogProbMetric: 28.3515

Epoch 411: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1634 - MinusLogProbMetric: 28.1634 - val_loss: 28.3515 - val_MinusLogProbMetric: 28.3515 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 412/1000
2023-10-24 19:37:14.928 
Epoch 412/1000 
	 loss: 28.1696, MinusLogProbMetric: 28.1696, val_loss: 28.6790, val_MinusLogProbMetric: 28.6790

Epoch 412: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1696 - MinusLogProbMetric: 28.1696 - val_loss: 28.6790 - val_MinusLogProbMetric: 28.6790 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 413/1000
2023-10-24 19:37:49.503 
Epoch 413/1000 
	 loss: 28.1567, MinusLogProbMetric: 28.1567, val_loss: 28.5428, val_MinusLogProbMetric: 28.5428

Epoch 413: val_loss did not improve from 28.32603
196/196 - 35s - loss: 28.1567 - MinusLogProbMetric: 28.1567 - val_loss: 28.5428 - val_MinusLogProbMetric: 28.5428 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 414/1000
2023-10-24 19:38:23.556 
Epoch 414/1000 
	 loss: 28.1362, MinusLogProbMetric: 28.1362, val_loss: 28.5652, val_MinusLogProbMetric: 28.5652

Epoch 414: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1362 - MinusLogProbMetric: 28.1362 - val_loss: 28.5652 - val_MinusLogProbMetric: 28.5652 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 415/1000
2023-10-24 19:38:58.044 
Epoch 415/1000 
	 loss: 28.1590, MinusLogProbMetric: 28.1590, val_loss: 28.3454, val_MinusLogProbMetric: 28.3454

Epoch 415: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1590 - MinusLogProbMetric: 28.1590 - val_loss: 28.3454 - val_MinusLogProbMetric: 28.3454 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 416/1000
2023-10-24 19:39:32.071 
Epoch 416/1000 
	 loss: 28.1465, MinusLogProbMetric: 28.1465, val_loss: 28.3853, val_MinusLogProbMetric: 28.3853

Epoch 416: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1465 - MinusLogProbMetric: 28.1465 - val_loss: 28.3853 - val_MinusLogProbMetric: 28.3853 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 417/1000
2023-10-24 19:40:06.512 
Epoch 417/1000 
	 loss: 28.1723, MinusLogProbMetric: 28.1723, val_loss: 28.4449, val_MinusLogProbMetric: 28.4449

Epoch 417: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1723 - MinusLogProbMetric: 28.1723 - val_loss: 28.4449 - val_MinusLogProbMetric: 28.4449 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 418/1000
2023-10-24 19:40:40.970 
Epoch 418/1000 
	 loss: 28.1294, MinusLogProbMetric: 28.1294, val_loss: 28.5386, val_MinusLogProbMetric: 28.5386

Epoch 418: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1294 - MinusLogProbMetric: 28.1294 - val_loss: 28.5386 - val_MinusLogProbMetric: 28.5386 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 419/1000
2023-10-24 19:41:15.169 
Epoch 419/1000 
	 loss: 28.1432, MinusLogProbMetric: 28.1432, val_loss: 28.5002, val_MinusLogProbMetric: 28.5002

Epoch 419: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1432 - MinusLogProbMetric: 28.1432 - val_loss: 28.5002 - val_MinusLogProbMetric: 28.5002 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 420/1000
2023-10-24 19:41:49.887 
Epoch 420/1000 
	 loss: 28.1472, MinusLogProbMetric: 28.1472, val_loss: 28.5197, val_MinusLogProbMetric: 28.5197

Epoch 420: val_loss did not improve from 28.32603
196/196 - 35s - loss: 28.1472 - MinusLogProbMetric: 28.1472 - val_loss: 28.5197 - val_MinusLogProbMetric: 28.5197 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 421/1000
2023-10-24 19:42:23.899 
Epoch 421/1000 
	 loss: 28.1784, MinusLogProbMetric: 28.1784, val_loss: 28.3779, val_MinusLogProbMetric: 28.3779

Epoch 421: val_loss did not improve from 28.32603
196/196 - 34s - loss: 28.1784 - MinusLogProbMetric: 28.1784 - val_loss: 28.3779 - val_MinusLogProbMetric: 28.3779 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 422/1000
2023-10-24 19:42:57.157 
Epoch 422/1000 
	 loss: 28.1474, MinusLogProbMetric: 28.1474, val_loss: 28.4536, val_MinusLogProbMetric: 28.4536

Epoch 422: val_loss did not improve from 28.32603
196/196 - 33s - loss: 28.1474 - MinusLogProbMetric: 28.1474 - val_loss: 28.4536 - val_MinusLogProbMetric: 28.4536 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 423/1000
2023-10-24 19:43:28.375 
Epoch 423/1000 
	 loss: 28.1557, MinusLogProbMetric: 28.1557, val_loss: 28.3043, val_MinusLogProbMetric: 28.3043

Epoch 423: val_loss improved from 28.32603 to 28.30430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 32s - loss: 28.1557 - MinusLogProbMetric: 28.1557 - val_loss: 28.3043 - val_MinusLogProbMetric: 28.3043 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 424/1000
2023-10-24 19:43:59.775 
Epoch 424/1000 
	 loss: 28.1590, MinusLogProbMetric: 28.1590, val_loss: 28.5658, val_MinusLogProbMetric: 28.5658

Epoch 424: val_loss did not improve from 28.30430
196/196 - 31s - loss: 28.1590 - MinusLogProbMetric: 28.1590 - val_loss: 28.5658 - val_MinusLogProbMetric: 28.5658 - lr: 5.0000e-04 - 31s/epoch - 158ms/step
Epoch 425/1000
2023-10-24 19:44:33.932 
Epoch 425/1000 
	 loss: 28.1479, MinusLogProbMetric: 28.1479, val_loss: 28.5727, val_MinusLogProbMetric: 28.5727

Epoch 425: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1479 - MinusLogProbMetric: 28.1479 - val_loss: 28.5727 - val_MinusLogProbMetric: 28.5727 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 426/1000
2023-10-24 19:45:08.419 
Epoch 426/1000 
	 loss: 28.1542, MinusLogProbMetric: 28.1542, val_loss: 28.4941, val_MinusLogProbMetric: 28.4941

Epoch 426: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1542 - MinusLogProbMetric: 28.1542 - val_loss: 28.4941 - val_MinusLogProbMetric: 28.4941 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 427/1000
2023-10-24 19:45:43.258 
Epoch 427/1000 
	 loss: 28.1326, MinusLogProbMetric: 28.1326, val_loss: 28.3636, val_MinusLogProbMetric: 28.3636

Epoch 427: val_loss did not improve from 28.30430
196/196 - 35s - loss: 28.1326 - MinusLogProbMetric: 28.1326 - val_loss: 28.3636 - val_MinusLogProbMetric: 28.3636 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 428/1000
2023-10-24 19:46:17.416 
Epoch 428/1000 
	 loss: 28.1542, MinusLogProbMetric: 28.1542, val_loss: 28.4396, val_MinusLogProbMetric: 28.4396

Epoch 428: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1542 - MinusLogProbMetric: 28.1542 - val_loss: 28.4396 - val_MinusLogProbMetric: 28.4396 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 429/1000
2023-10-24 19:46:51.346 
Epoch 429/1000 
	 loss: 28.1473, MinusLogProbMetric: 28.1473, val_loss: 28.3886, val_MinusLogProbMetric: 28.3886

Epoch 429: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1473 - MinusLogProbMetric: 28.1473 - val_loss: 28.3886 - val_MinusLogProbMetric: 28.3886 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 430/1000
2023-10-24 19:47:25.822 
Epoch 430/1000 
	 loss: 28.1437, MinusLogProbMetric: 28.1437, val_loss: 28.4905, val_MinusLogProbMetric: 28.4905

Epoch 430: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1437 - MinusLogProbMetric: 28.1437 - val_loss: 28.4905 - val_MinusLogProbMetric: 28.4905 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 431/1000
2023-10-24 19:47:59.317 
Epoch 431/1000 
	 loss: 28.1441, MinusLogProbMetric: 28.1441, val_loss: 28.4022, val_MinusLogProbMetric: 28.4022

Epoch 431: val_loss did not improve from 28.30430
196/196 - 33s - loss: 28.1441 - MinusLogProbMetric: 28.1441 - val_loss: 28.4022 - val_MinusLogProbMetric: 28.4022 - lr: 5.0000e-04 - 33s/epoch - 171ms/step
Epoch 432/1000
2023-10-24 19:48:33.899 
Epoch 432/1000 
	 loss: 28.1426, MinusLogProbMetric: 28.1426, val_loss: 28.4264, val_MinusLogProbMetric: 28.4264

Epoch 432: val_loss did not improve from 28.30430
196/196 - 35s - loss: 28.1426 - MinusLogProbMetric: 28.1426 - val_loss: 28.4264 - val_MinusLogProbMetric: 28.4264 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 433/1000
2023-10-24 19:49:07.965 
Epoch 433/1000 
	 loss: 28.1860, MinusLogProbMetric: 28.1860, val_loss: 28.3694, val_MinusLogProbMetric: 28.3694

Epoch 433: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1860 - MinusLogProbMetric: 28.1860 - val_loss: 28.3694 - val_MinusLogProbMetric: 28.3694 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 434/1000
2023-10-24 19:49:42.435 
Epoch 434/1000 
	 loss: 28.1582, MinusLogProbMetric: 28.1582, val_loss: 28.4918, val_MinusLogProbMetric: 28.4918

Epoch 434: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1582 - MinusLogProbMetric: 28.1582 - val_loss: 28.4918 - val_MinusLogProbMetric: 28.4918 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 435/1000
2023-10-24 19:50:16.982 
Epoch 435/1000 
	 loss: 28.1548, MinusLogProbMetric: 28.1548, val_loss: 28.7101, val_MinusLogProbMetric: 28.7101

Epoch 435: val_loss did not improve from 28.30430
196/196 - 35s - loss: 28.1548 - MinusLogProbMetric: 28.1548 - val_loss: 28.7101 - val_MinusLogProbMetric: 28.7101 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 436/1000
2023-10-24 19:50:51.284 
Epoch 436/1000 
	 loss: 28.1351, MinusLogProbMetric: 28.1351, val_loss: 28.4289, val_MinusLogProbMetric: 28.4289

Epoch 436: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1351 - MinusLogProbMetric: 28.1351 - val_loss: 28.4289 - val_MinusLogProbMetric: 28.4289 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 437/1000
2023-10-24 19:51:25.227 
Epoch 437/1000 
	 loss: 28.1397, MinusLogProbMetric: 28.1397, val_loss: 28.4599, val_MinusLogProbMetric: 28.4599

Epoch 437: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1397 - MinusLogProbMetric: 28.1397 - val_loss: 28.4599 - val_MinusLogProbMetric: 28.4599 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 438/1000
2023-10-24 19:51:59.506 
Epoch 438/1000 
	 loss: 28.1568, MinusLogProbMetric: 28.1568, val_loss: 28.5224, val_MinusLogProbMetric: 28.5224

Epoch 438: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1568 - MinusLogProbMetric: 28.1568 - val_loss: 28.5224 - val_MinusLogProbMetric: 28.5224 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 439/1000
2023-10-24 19:52:33.243 
Epoch 439/1000 
	 loss: 28.1498, MinusLogProbMetric: 28.1498, val_loss: 28.4120, val_MinusLogProbMetric: 28.4120

Epoch 439: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1498 - MinusLogProbMetric: 28.1498 - val_loss: 28.4120 - val_MinusLogProbMetric: 28.4120 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 440/1000
2023-10-24 19:53:07.229 
Epoch 440/1000 
	 loss: 28.1293, MinusLogProbMetric: 28.1293, val_loss: 28.3913, val_MinusLogProbMetric: 28.3913

Epoch 440: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1293 - MinusLogProbMetric: 28.1293 - val_loss: 28.3913 - val_MinusLogProbMetric: 28.3913 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 441/1000
2023-10-24 19:53:41.722 
Epoch 441/1000 
	 loss: 28.1389, MinusLogProbMetric: 28.1389, val_loss: 28.4569, val_MinusLogProbMetric: 28.4569

Epoch 441: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1389 - MinusLogProbMetric: 28.1389 - val_loss: 28.4569 - val_MinusLogProbMetric: 28.4569 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 442/1000
2023-10-24 19:54:16.540 
Epoch 442/1000 
	 loss: 28.1331, MinusLogProbMetric: 28.1331, val_loss: 28.7401, val_MinusLogProbMetric: 28.7401

Epoch 442: val_loss did not improve from 28.30430
196/196 - 35s - loss: 28.1331 - MinusLogProbMetric: 28.1331 - val_loss: 28.7401 - val_MinusLogProbMetric: 28.7401 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 443/1000
2023-10-24 19:54:50.719 
Epoch 443/1000 
	 loss: 28.1718, MinusLogProbMetric: 28.1718, val_loss: 28.4937, val_MinusLogProbMetric: 28.4937

Epoch 443: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1718 - MinusLogProbMetric: 28.1718 - val_loss: 28.4937 - val_MinusLogProbMetric: 28.4937 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 444/1000
2023-10-24 19:55:25.066 
Epoch 444/1000 
	 loss: 28.1340, MinusLogProbMetric: 28.1340, val_loss: 28.3851, val_MinusLogProbMetric: 28.3851

Epoch 444: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1340 - MinusLogProbMetric: 28.1340 - val_loss: 28.3851 - val_MinusLogProbMetric: 28.3851 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 445/1000
2023-10-24 19:55:58.226 
Epoch 445/1000 
	 loss: 28.1462, MinusLogProbMetric: 28.1462, val_loss: 28.4593, val_MinusLogProbMetric: 28.4593

Epoch 445: val_loss did not improve from 28.30430
196/196 - 33s - loss: 28.1462 - MinusLogProbMetric: 28.1462 - val_loss: 28.4593 - val_MinusLogProbMetric: 28.4593 - lr: 5.0000e-04 - 33s/epoch - 169ms/step
Epoch 446/1000
2023-10-24 19:56:32.561 
Epoch 446/1000 
	 loss: 28.1204, MinusLogProbMetric: 28.1204, val_loss: 28.4714, val_MinusLogProbMetric: 28.4714

Epoch 446: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1204 - MinusLogProbMetric: 28.1204 - val_loss: 28.4714 - val_MinusLogProbMetric: 28.4714 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 447/1000
2023-10-24 19:57:06.747 
Epoch 447/1000 
	 loss: 28.1020, MinusLogProbMetric: 28.1020, val_loss: 28.5495, val_MinusLogProbMetric: 28.5495

Epoch 447: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1020 - MinusLogProbMetric: 28.1020 - val_loss: 28.5495 - val_MinusLogProbMetric: 28.5495 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 448/1000
2023-10-24 19:57:41.526 
Epoch 448/1000 
	 loss: 28.1658, MinusLogProbMetric: 28.1658, val_loss: 28.6816, val_MinusLogProbMetric: 28.6816

Epoch 448: val_loss did not improve from 28.30430
196/196 - 35s - loss: 28.1658 - MinusLogProbMetric: 28.1658 - val_loss: 28.6816 - val_MinusLogProbMetric: 28.6816 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 449/1000
2023-10-24 19:58:16.125 
Epoch 449/1000 
	 loss: 28.1348, MinusLogProbMetric: 28.1348, val_loss: 28.5716, val_MinusLogProbMetric: 28.5716

Epoch 449: val_loss did not improve from 28.30430
196/196 - 35s - loss: 28.1348 - MinusLogProbMetric: 28.1348 - val_loss: 28.5716 - val_MinusLogProbMetric: 28.5716 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 450/1000
2023-10-24 19:58:50.432 
Epoch 450/1000 
	 loss: 28.1149, MinusLogProbMetric: 28.1149, val_loss: 28.5591, val_MinusLogProbMetric: 28.5591

Epoch 450: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1149 - MinusLogProbMetric: 28.1149 - val_loss: 28.5591 - val_MinusLogProbMetric: 28.5591 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 451/1000
2023-10-24 19:59:24.315 
Epoch 451/1000 
	 loss: 28.1303, MinusLogProbMetric: 28.1303, val_loss: 28.6883, val_MinusLogProbMetric: 28.6883

Epoch 451: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1303 - MinusLogProbMetric: 28.1303 - val_loss: 28.6883 - val_MinusLogProbMetric: 28.6883 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 452/1000
2023-10-24 19:59:58.782 
Epoch 452/1000 
	 loss: 28.1187, MinusLogProbMetric: 28.1187, val_loss: 28.4216, val_MinusLogProbMetric: 28.4216

Epoch 452: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1187 - MinusLogProbMetric: 28.1187 - val_loss: 28.4216 - val_MinusLogProbMetric: 28.4216 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 453/1000
2023-10-24 20:00:33.189 
Epoch 453/1000 
	 loss: 28.1065, MinusLogProbMetric: 28.1065, val_loss: 28.5064, val_MinusLogProbMetric: 28.5064

Epoch 453: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1065 - MinusLogProbMetric: 28.1065 - val_loss: 28.5064 - val_MinusLogProbMetric: 28.5064 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 454/1000
2023-10-24 20:01:07.222 
Epoch 454/1000 
	 loss: 28.1367, MinusLogProbMetric: 28.1367, val_loss: 28.4246, val_MinusLogProbMetric: 28.4246

Epoch 454: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1367 - MinusLogProbMetric: 28.1367 - val_loss: 28.4246 - val_MinusLogProbMetric: 28.4246 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 455/1000
2023-10-24 20:01:40.991 
Epoch 455/1000 
	 loss: 28.1297, MinusLogProbMetric: 28.1297, val_loss: 28.5453, val_MinusLogProbMetric: 28.5453

Epoch 455: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1297 - MinusLogProbMetric: 28.1297 - val_loss: 28.5453 - val_MinusLogProbMetric: 28.5453 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 456/1000
2023-10-24 20:02:15.055 
Epoch 456/1000 
	 loss: 28.0962, MinusLogProbMetric: 28.0962, val_loss: 28.4077, val_MinusLogProbMetric: 28.4077

Epoch 456: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.0962 - MinusLogProbMetric: 28.0962 - val_loss: 28.4077 - val_MinusLogProbMetric: 28.4077 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 457/1000
2023-10-24 20:02:48.111 
Epoch 457/1000 
	 loss: 28.1468, MinusLogProbMetric: 28.1468, val_loss: 28.5098, val_MinusLogProbMetric: 28.5098

Epoch 457: val_loss did not improve from 28.30430
196/196 - 33s - loss: 28.1468 - MinusLogProbMetric: 28.1468 - val_loss: 28.5098 - val_MinusLogProbMetric: 28.5098 - lr: 5.0000e-04 - 33s/epoch - 169ms/step
Epoch 458/1000
2023-10-24 20:03:22.379 
Epoch 458/1000 
	 loss: 28.1568, MinusLogProbMetric: 28.1568, val_loss: 28.5910, val_MinusLogProbMetric: 28.5910

Epoch 458: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1568 - MinusLogProbMetric: 28.1568 - val_loss: 28.5910 - val_MinusLogProbMetric: 28.5910 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 459/1000
2023-10-24 20:03:55.837 
Epoch 459/1000 
	 loss: 28.1048, MinusLogProbMetric: 28.1048, val_loss: 28.3674, val_MinusLogProbMetric: 28.3674

Epoch 459: val_loss did not improve from 28.30430
196/196 - 33s - loss: 28.1048 - MinusLogProbMetric: 28.1048 - val_loss: 28.3674 - val_MinusLogProbMetric: 28.3674 - lr: 5.0000e-04 - 33s/epoch - 171ms/step
Epoch 460/1000
2023-10-24 20:04:30.283 
Epoch 460/1000 
	 loss: 28.1248, MinusLogProbMetric: 28.1248, val_loss: 28.6037, val_MinusLogProbMetric: 28.6037

Epoch 460: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1248 - MinusLogProbMetric: 28.1248 - val_loss: 28.6037 - val_MinusLogProbMetric: 28.6037 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 461/1000
2023-10-24 20:05:04.544 
Epoch 461/1000 
	 loss: 28.1247, MinusLogProbMetric: 28.1247, val_loss: 28.3810, val_MinusLogProbMetric: 28.3810

Epoch 461: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1247 - MinusLogProbMetric: 28.1247 - val_loss: 28.3810 - val_MinusLogProbMetric: 28.3810 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 462/1000
2023-10-24 20:05:38.999 
Epoch 462/1000 
	 loss: 28.1131, MinusLogProbMetric: 28.1131, val_loss: 28.3940, val_MinusLogProbMetric: 28.3940

Epoch 462: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1131 - MinusLogProbMetric: 28.1131 - val_loss: 28.3940 - val_MinusLogProbMetric: 28.3940 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 463/1000
2023-10-24 20:06:13.160 
Epoch 463/1000 
	 loss: 28.1611, MinusLogProbMetric: 28.1611, val_loss: 28.4007, val_MinusLogProbMetric: 28.4007

Epoch 463: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1611 - MinusLogProbMetric: 28.1611 - val_loss: 28.4007 - val_MinusLogProbMetric: 28.4007 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 464/1000
2023-10-24 20:06:47.440 
Epoch 464/1000 
	 loss: 28.1410, MinusLogProbMetric: 28.1410, val_loss: 28.5027, val_MinusLogProbMetric: 28.5027

Epoch 464: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1410 - MinusLogProbMetric: 28.1410 - val_loss: 28.5027 - val_MinusLogProbMetric: 28.5027 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 465/1000
2023-10-24 20:07:21.814 
Epoch 465/1000 
	 loss: 28.1289, MinusLogProbMetric: 28.1289, val_loss: 28.5430, val_MinusLogProbMetric: 28.5430

Epoch 465: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1289 - MinusLogProbMetric: 28.1289 - val_loss: 28.5430 - val_MinusLogProbMetric: 28.5430 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 466/1000
2023-10-24 20:07:55.550 
Epoch 466/1000 
	 loss: 28.1374, MinusLogProbMetric: 28.1374, val_loss: 28.4550, val_MinusLogProbMetric: 28.4550

Epoch 466: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1374 - MinusLogProbMetric: 28.1374 - val_loss: 28.4550 - val_MinusLogProbMetric: 28.4550 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 467/1000
2023-10-24 20:08:30.062 
Epoch 467/1000 
	 loss: 28.1465, MinusLogProbMetric: 28.1465, val_loss: 28.3599, val_MinusLogProbMetric: 28.3599

Epoch 467: val_loss did not improve from 28.30430
196/196 - 35s - loss: 28.1465 - MinusLogProbMetric: 28.1465 - val_loss: 28.3599 - val_MinusLogProbMetric: 28.3599 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 468/1000
2023-10-24 20:09:04.208 
Epoch 468/1000 
	 loss: 28.1352, MinusLogProbMetric: 28.1352, val_loss: 28.4988, val_MinusLogProbMetric: 28.4988

Epoch 468: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1352 - MinusLogProbMetric: 28.1352 - val_loss: 28.4988 - val_MinusLogProbMetric: 28.4988 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 469/1000
2023-10-24 20:09:38.320 
Epoch 469/1000 
	 loss: 28.1123, MinusLogProbMetric: 28.1123, val_loss: 28.4897, val_MinusLogProbMetric: 28.4897

Epoch 469: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1123 - MinusLogProbMetric: 28.1123 - val_loss: 28.4897 - val_MinusLogProbMetric: 28.4897 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 470/1000
2023-10-24 20:10:12.669 
Epoch 470/1000 
	 loss: 28.1356, MinusLogProbMetric: 28.1356, val_loss: 28.3793, val_MinusLogProbMetric: 28.3793

Epoch 470: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1356 - MinusLogProbMetric: 28.1356 - val_loss: 28.3793 - val_MinusLogProbMetric: 28.3793 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 471/1000
2023-10-24 20:10:47.172 
Epoch 471/1000 
	 loss: 28.0813, MinusLogProbMetric: 28.0813, val_loss: 28.4719, val_MinusLogProbMetric: 28.4719

Epoch 471: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.0813 - MinusLogProbMetric: 28.0813 - val_loss: 28.4719 - val_MinusLogProbMetric: 28.4719 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 472/1000
2023-10-24 20:11:21.365 
Epoch 472/1000 
	 loss: 28.1172, MinusLogProbMetric: 28.1172, val_loss: 28.4196, val_MinusLogProbMetric: 28.4196

Epoch 472: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1172 - MinusLogProbMetric: 28.1172 - val_loss: 28.4196 - val_MinusLogProbMetric: 28.4196 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 473/1000
2023-10-24 20:11:55.106 
Epoch 473/1000 
	 loss: 28.1647, MinusLogProbMetric: 28.1647, val_loss: 28.3734, val_MinusLogProbMetric: 28.3734

Epoch 473: val_loss did not improve from 28.30430
196/196 - 34s - loss: 28.1647 - MinusLogProbMetric: 28.1647 - val_loss: 28.3734 - val_MinusLogProbMetric: 28.3734 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 474/1000
2023-10-24 20:12:29.472 
Epoch 474/1000 
	 loss: 27.9276, MinusLogProbMetric: 27.9276, val_loss: 28.3647, val_MinusLogProbMetric: 28.3647

Epoch 474: val_loss did not improve from 28.30430
196/196 - 34s - loss: 27.9276 - MinusLogProbMetric: 27.9276 - val_loss: 28.3647 - val_MinusLogProbMetric: 28.3647 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 475/1000
2023-10-24 20:13:04.025 
Epoch 475/1000 
	 loss: 27.9212, MinusLogProbMetric: 27.9212, val_loss: 28.2501, val_MinusLogProbMetric: 28.2501

Epoch 475: val_loss improved from 28.30430 to 28.25007, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.9212 - MinusLogProbMetric: 27.9212 - val_loss: 28.2501 - val_MinusLogProbMetric: 28.2501 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 476/1000
2023-10-24 20:13:38.534 
Epoch 476/1000 
	 loss: 27.9176, MinusLogProbMetric: 27.9176, val_loss: 28.2574, val_MinusLogProbMetric: 28.2574

Epoch 476: val_loss did not improve from 28.25007
196/196 - 34s - loss: 27.9176 - MinusLogProbMetric: 27.9176 - val_loss: 28.2574 - val_MinusLogProbMetric: 28.2574 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 477/1000
2023-10-24 20:14:11.297 
Epoch 477/1000 
	 loss: 27.9115, MinusLogProbMetric: 27.9115, val_loss: 28.2271, val_MinusLogProbMetric: 28.2271

Epoch 477: val_loss improved from 28.25007 to 28.22709, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 27.9115 - MinusLogProbMetric: 27.9115 - val_loss: 28.2271 - val_MinusLogProbMetric: 28.2271 - lr: 2.5000e-04 - 33s/epoch - 169ms/step
Epoch 478/1000
2023-10-24 20:14:46.032 
Epoch 478/1000 
	 loss: 27.9294, MinusLogProbMetric: 27.9294, val_loss: 28.2438, val_MinusLogProbMetric: 28.2438

Epoch 478: val_loss did not improve from 28.22709
196/196 - 34s - loss: 27.9294 - MinusLogProbMetric: 27.9294 - val_loss: 28.2438 - val_MinusLogProbMetric: 28.2438 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 479/1000
2023-10-24 20:15:20.585 
Epoch 479/1000 
	 loss: 27.9349, MinusLogProbMetric: 27.9349, val_loss: 28.3217, val_MinusLogProbMetric: 28.3217

Epoch 479: val_loss did not improve from 28.22709
196/196 - 35s - loss: 27.9349 - MinusLogProbMetric: 27.9349 - val_loss: 28.3217 - val_MinusLogProbMetric: 28.3217 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 480/1000
2023-10-24 20:15:54.314 
Epoch 480/1000 
	 loss: 27.9279, MinusLogProbMetric: 27.9279, val_loss: 28.2488, val_MinusLogProbMetric: 28.2488

Epoch 480: val_loss did not improve from 28.22709
196/196 - 34s - loss: 27.9279 - MinusLogProbMetric: 27.9279 - val_loss: 28.2488 - val_MinusLogProbMetric: 28.2488 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 481/1000
2023-10-24 20:16:28.607 
Epoch 481/1000 
	 loss: 27.9355, MinusLogProbMetric: 27.9355, val_loss: 28.2382, val_MinusLogProbMetric: 28.2382

Epoch 481: val_loss did not improve from 28.22709
196/196 - 34s - loss: 27.9355 - MinusLogProbMetric: 27.9355 - val_loss: 28.2382 - val_MinusLogProbMetric: 28.2382 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 482/1000
2023-10-24 20:17:03.097 
Epoch 482/1000 
	 loss: 27.9224, MinusLogProbMetric: 27.9224, val_loss: 28.2331, val_MinusLogProbMetric: 28.2331

Epoch 482: val_loss did not improve from 28.22709
196/196 - 34s - loss: 27.9224 - MinusLogProbMetric: 27.9224 - val_loss: 28.2331 - val_MinusLogProbMetric: 28.2331 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 483/1000
2023-10-24 20:17:36.751 
Epoch 483/1000 
	 loss: 27.9276, MinusLogProbMetric: 27.9276, val_loss: 28.2297, val_MinusLogProbMetric: 28.2297

Epoch 483: val_loss did not improve from 28.22709
196/196 - 34s - loss: 27.9276 - MinusLogProbMetric: 27.9276 - val_loss: 28.2297 - val_MinusLogProbMetric: 28.2297 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 484/1000
2023-10-24 20:18:11.356 
Epoch 484/1000 
	 loss: 27.9227, MinusLogProbMetric: 27.9227, val_loss: 28.1894, val_MinusLogProbMetric: 28.1894

Epoch 484: val_loss improved from 28.22709 to 28.18937, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.9227 - MinusLogProbMetric: 27.9227 - val_loss: 28.1894 - val_MinusLogProbMetric: 28.1894 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 485/1000
2023-10-24 20:18:45.655 
Epoch 485/1000 
	 loss: 27.9180, MinusLogProbMetric: 27.9180, val_loss: 28.2365, val_MinusLogProbMetric: 28.2365

Epoch 485: val_loss did not improve from 28.18937
196/196 - 34s - loss: 27.9180 - MinusLogProbMetric: 27.9180 - val_loss: 28.2365 - val_MinusLogProbMetric: 28.2365 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 486/1000
2023-10-24 20:19:19.985 
Epoch 486/1000 
	 loss: 27.9174, MinusLogProbMetric: 27.9174, val_loss: 28.3175, val_MinusLogProbMetric: 28.3175

Epoch 486: val_loss did not improve from 28.18937
196/196 - 34s - loss: 27.9174 - MinusLogProbMetric: 27.9174 - val_loss: 28.3175 - val_MinusLogProbMetric: 28.3175 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 487/1000
2023-10-24 20:19:53.992 
Epoch 487/1000 
	 loss: 27.9256, MinusLogProbMetric: 27.9256, val_loss: 28.2215, val_MinusLogProbMetric: 28.2215

Epoch 487: val_loss did not improve from 28.18937
196/196 - 34s - loss: 27.9256 - MinusLogProbMetric: 27.9256 - val_loss: 28.2215 - val_MinusLogProbMetric: 28.2215 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 488/1000
2023-10-24 20:20:28.800 
Epoch 488/1000 
	 loss: 27.9270, MinusLogProbMetric: 27.9270, val_loss: 28.2418, val_MinusLogProbMetric: 28.2418

Epoch 488: val_loss did not improve from 28.18937
196/196 - 35s - loss: 27.9270 - MinusLogProbMetric: 27.9270 - val_loss: 28.2418 - val_MinusLogProbMetric: 28.2418 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 489/1000
2023-10-24 20:21:02.706 
Epoch 489/1000 
	 loss: 27.9223, MinusLogProbMetric: 27.9223, val_loss: 28.3059, val_MinusLogProbMetric: 28.3059

Epoch 489: val_loss did not improve from 28.18937
196/196 - 34s - loss: 27.9223 - MinusLogProbMetric: 27.9223 - val_loss: 28.3059 - val_MinusLogProbMetric: 28.3059 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 490/1000
2023-10-24 20:21:37.384 
Epoch 490/1000 
	 loss: 27.9363, MinusLogProbMetric: 27.9363, val_loss: 28.2679, val_MinusLogProbMetric: 28.2679

Epoch 490: val_loss did not improve from 28.18937
196/196 - 35s - loss: 27.9363 - MinusLogProbMetric: 27.9363 - val_loss: 28.2679 - val_MinusLogProbMetric: 28.2679 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 491/1000
2023-10-24 20:22:11.540 
Epoch 491/1000 
	 loss: 27.9174, MinusLogProbMetric: 27.9174, val_loss: 28.2288, val_MinusLogProbMetric: 28.2288

Epoch 491: val_loss did not improve from 28.18937
196/196 - 34s - loss: 27.9174 - MinusLogProbMetric: 27.9174 - val_loss: 28.2288 - val_MinusLogProbMetric: 28.2288 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 492/1000
2023-10-24 20:22:46.053 
Epoch 492/1000 
	 loss: 27.9187, MinusLogProbMetric: 27.9187, val_loss: 28.2048, val_MinusLogProbMetric: 28.2048

Epoch 492: val_loss did not improve from 28.18937
196/196 - 35s - loss: 27.9187 - MinusLogProbMetric: 27.9187 - val_loss: 28.2048 - val_MinusLogProbMetric: 28.2048 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 493/1000
2023-10-24 20:23:19.942 
Epoch 493/1000 
	 loss: 27.9070, MinusLogProbMetric: 27.9070, val_loss: 28.2163, val_MinusLogProbMetric: 28.2163

Epoch 493: val_loss did not improve from 28.18937
196/196 - 34s - loss: 27.9070 - MinusLogProbMetric: 27.9070 - val_loss: 28.2163 - val_MinusLogProbMetric: 28.2163 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 494/1000
2023-10-24 20:23:54.338 
Epoch 494/1000 
	 loss: 27.9225, MinusLogProbMetric: 27.9225, val_loss: 28.2323, val_MinusLogProbMetric: 28.2323

Epoch 494: val_loss did not improve from 28.18937
196/196 - 34s - loss: 27.9225 - MinusLogProbMetric: 27.9225 - val_loss: 28.2323 - val_MinusLogProbMetric: 28.2323 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 495/1000
2023-10-24 20:24:29.065 
Epoch 495/1000 
	 loss: 27.9133, MinusLogProbMetric: 27.9133, val_loss: 28.1805, val_MinusLogProbMetric: 28.1805

Epoch 495: val_loss improved from 28.18937 to 28.18046, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.9133 - MinusLogProbMetric: 27.9133 - val_loss: 28.1805 - val_MinusLogProbMetric: 28.1805 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 496/1000
2023-10-24 20:25:03.645 
Epoch 496/1000 
	 loss: 27.9106, MinusLogProbMetric: 27.9106, val_loss: 28.2565, val_MinusLogProbMetric: 28.2565

Epoch 496: val_loss did not improve from 28.18046
196/196 - 34s - loss: 27.9106 - MinusLogProbMetric: 27.9106 - val_loss: 28.2565 - val_MinusLogProbMetric: 28.2565 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 497/1000
2023-10-24 20:25:38.155 
Epoch 497/1000 
	 loss: 27.9212, MinusLogProbMetric: 27.9212, val_loss: 28.1826, val_MinusLogProbMetric: 28.1826

Epoch 497: val_loss did not improve from 28.18046
196/196 - 35s - loss: 27.9212 - MinusLogProbMetric: 27.9212 - val_loss: 28.1826 - val_MinusLogProbMetric: 28.1826 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 498/1000
2023-10-24 20:26:12.502 
Epoch 498/1000 
	 loss: 27.9082, MinusLogProbMetric: 27.9082, val_loss: 28.1580, val_MinusLogProbMetric: 28.1580

Epoch 498: val_loss improved from 28.18046 to 28.15803, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.9082 - MinusLogProbMetric: 27.9082 - val_loss: 28.1580 - val_MinusLogProbMetric: 28.1580 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 499/1000
2023-10-24 20:26:46.717 
Epoch 499/1000 
	 loss: 27.8999, MinusLogProbMetric: 27.8999, val_loss: 28.2451, val_MinusLogProbMetric: 28.2451

Epoch 499: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.8999 - MinusLogProbMetric: 27.8999 - val_loss: 28.2451 - val_MinusLogProbMetric: 28.2451 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 500/1000
2023-10-24 20:27:20.904 
Epoch 500/1000 
	 loss: 27.9051, MinusLogProbMetric: 27.9051, val_loss: 28.2783, val_MinusLogProbMetric: 28.2783

Epoch 500: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9051 - MinusLogProbMetric: 27.9051 - val_loss: 28.2783 - val_MinusLogProbMetric: 28.2783 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 501/1000
2023-10-24 20:27:55.100 
Epoch 501/1000 
	 loss: 27.9159, MinusLogProbMetric: 27.9159, val_loss: 28.2677, val_MinusLogProbMetric: 28.2677

Epoch 501: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9159 - MinusLogProbMetric: 27.9159 - val_loss: 28.2677 - val_MinusLogProbMetric: 28.2677 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 502/1000
2023-10-24 20:28:29.714 
Epoch 502/1000 
	 loss: 27.9011, MinusLogProbMetric: 27.9011, val_loss: 28.2330, val_MinusLogProbMetric: 28.2330

Epoch 502: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.9011 - MinusLogProbMetric: 27.9011 - val_loss: 28.2330 - val_MinusLogProbMetric: 28.2330 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 503/1000
2023-10-24 20:29:03.861 
Epoch 503/1000 
	 loss: 27.9117, MinusLogProbMetric: 27.9117, val_loss: 28.2650, val_MinusLogProbMetric: 28.2650

Epoch 503: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9117 - MinusLogProbMetric: 27.9117 - val_loss: 28.2650 - val_MinusLogProbMetric: 28.2650 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 504/1000
2023-10-24 20:29:37.802 
Epoch 504/1000 
	 loss: 27.9217, MinusLogProbMetric: 27.9217, val_loss: 28.2535, val_MinusLogProbMetric: 28.2535

Epoch 504: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9217 - MinusLogProbMetric: 27.9217 - val_loss: 28.2535 - val_MinusLogProbMetric: 28.2535 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 505/1000
2023-10-24 20:30:12.254 
Epoch 505/1000 
	 loss: 27.9174, MinusLogProbMetric: 27.9174, val_loss: 28.2004, val_MinusLogProbMetric: 28.2004

Epoch 505: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9174 - MinusLogProbMetric: 27.9174 - val_loss: 28.2004 - val_MinusLogProbMetric: 28.2004 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 506/1000
2023-10-24 20:30:46.774 
Epoch 506/1000 
	 loss: 27.9098, MinusLogProbMetric: 27.9098, val_loss: 28.2503, val_MinusLogProbMetric: 28.2503

Epoch 506: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.9098 - MinusLogProbMetric: 27.9098 - val_loss: 28.2503 - val_MinusLogProbMetric: 28.2503 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 507/1000
2023-10-24 20:31:19.659 
Epoch 507/1000 
	 loss: 27.9056, MinusLogProbMetric: 27.9056, val_loss: 28.2401, val_MinusLogProbMetric: 28.2401

Epoch 507: val_loss did not improve from 28.15803
196/196 - 33s - loss: 27.9056 - MinusLogProbMetric: 27.9056 - val_loss: 28.2401 - val_MinusLogProbMetric: 28.2401 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 508/1000
2023-10-24 20:31:53.814 
Epoch 508/1000 
	 loss: 27.9075, MinusLogProbMetric: 27.9075, val_loss: 28.3677, val_MinusLogProbMetric: 28.3677

Epoch 508: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9075 - MinusLogProbMetric: 27.9075 - val_loss: 28.3677 - val_MinusLogProbMetric: 28.3677 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 509/1000
2023-10-24 20:32:26.276 
Epoch 509/1000 
	 loss: 27.9337, MinusLogProbMetric: 27.9337, val_loss: 28.3513, val_MinusLogProbMetric: 28.3513

Epoch 509: val_loss did not improve from 28.15803
196/196 - 32s - loss: 27.9337 - MinusLogProbMetric: 27.9337 - val_loss: 28.3513 - val_MinusLogProbMetric: 28.3513 - lr: 2.5000e-04 - 32s/epoch - 166ms/step
Epoch 510/1000
2023-10-24 20:33:00.755 
Epoch 510/1000 
	 loss: 27.9129, MinusLogProbMetric: 27.9129, val_loss: 28.2046, val_MinusLogProbMetric: 28.2046

Epoch 510: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9129 - MinusLogProbMetric: 27.9129 - val_loss: 28.2046 - val_MinusLogProbMetric: 28.2046 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 511/1000
2023-10-24 20:33:34.481 
Epoch 511/1000 
	 loss: 27.9103, MinusLogProbMetric: 27.9103, val_loss: 28.2380, val_MinusLogProbMetric: 28.2380

Epoch 511: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9103 - MinusLogProbMetric: 27.9103 - val_loss: 28.2380 - val_MinusLogProbMetric: 28.2380 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 512/1000
2023-10-24 20:34:07.155 
Epoch 512/1000 
	 loss: 27.9163, MinusLogProbMetric: 27.9163, val_loss: 28.2160, val_MinusLogProbMetric: 28.2160

Epoch 512: val_loss did not improve from 28.15803
196/196 - 33s - loss: 27.9163 - MinusLogProbMetric: 27.9163 - val_loss: 28.2160 - val_MinusLogProbMetric: 28.2160 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 513/1000
2023-10-24 20:34:38.909 
Epoch 513/1000 
	 loss: 27.9041, MinusLogProbMetric: 27.9041, val_loss: 28.1877, val_MinusLogProbMetric: 28.1877

Epoch 513: val_loss did not improve from 28.15803
196/196 - 32s - loss: 27.9041 - MinusLogProbMetric: 27.9041 - val_loss: 28.1877 - val_MinusLogProbMetric: 28.1877 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 514/1000
2023-10-24 20:35:13.502 
Epoch 514/1000 
	 loss: 27.9022, MinusLogProbMetric: 27.9022, val_loss: 28.3320, val_MinusLogProbMetric: 28.3320

Epoch 514: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.9022 - MinusLogProbMetric: 27.9022 - val_loss: 28.3320 - val_MinusLogProbMetric: 28.3320 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 515/1000
2023-10-24 20:35:47.659 
Epoch 515/1000 
	 loss: 27.9057, MinusLogProbMetric: 27.9057, val_loss: 28.1910, val_MinusLogProbMetric: 28.1910

Epoch 515: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9057 - MinusLogProbMetric: 27.9057 - val_loss: 28.1910 - val_MinusLogProbMetric: 28.1910 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 516/1000
2023-10-24 20:36:22.348 
Epoch 516/1000 
	 loss: 27.9200, MinusLogProbMetric: 27.9200, val_loss: 28.2094, val_MinusLogProbMetric: 28.2094

Epoch 516: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.9200 - MinusLogProbMetric: 27.9200 - val_loss: 28.2094 - val_MinusLogProbMetric: 28.2094 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 517/1000
2023-10-24 20:36:56.264 
Epoch 517/1000 
	 loss: 27.9123, MinusLogProbMetric: 27.9123, val_loss: 28.1860, val_MinusLogProbMetric: 28.1860

Epoch 517: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9123 - MinusLogProbMetric: 27.9123 - val_loss: 28.1860 - val_MinusLogProbMetric: 28.1860 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 518/1000
2023-10-24 20:37:31.008 
Epoch 518/1000 
	 loss: 27.9196, MinusLogProbMetric: 27.9196, val_loss: 28.2360, val_MinusLogProbMetric: 28.2360

Epoch 518: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.9196 - MinusLogProbMetric: 27.9196 - val_loss: 28.2360 - val_MinusLogProbMetric: 28.2360 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 519/1000
2023-10-24 20:38:05.237 
Epoch 519/1000 
	 loss: 27.9072, MinusLogProbMetric: 27.9072, val_loss: 28.2930, val_MinusLogProbMetric: 28.2930

Epoch 519: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9072 - MinusLogProbMetric: 27.9072 - val_loss: 28.2930 - val_MinusLogProbMetric: 28.2930 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 520/1000
2023-10-24 20:38:40.104 
Epoch 520/1000 
	 loss: 27.8993, MinusLogProbMetric: 27.8993, val_loss: 28.2763, val_MinusLogProbMetric: 28.2763

Epoch 520: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.8993 - MinusLogProbMetric: 27.8993 - val_loss: 28.2763 - val_MinusLogProbMetric: 28.2763 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 521/1000
2023-10-24 20:39:14.618 
Epoch 521/1000 
	 loss: 27.9113, MinusLogProbMetric: 27.9113, val_loss: 28.1905, val_MinusLogProbMetric: 28.1905

Epoch 521: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.9113 - MinusLogProbMetric: 27.9113 - val_loss: 28.1905 - val_MinusLogProbMetric: 28.1905 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 522/1000
2023-10-24 20:39:49.421 
Epoch 522/1000 
	 loss: 27.9061, MinusLogProbMetric: 27.9061, val_loss: 28.3253, val_MinusLogProbMetric: 28.3253

Epoch 522: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.9061 - MinusLogProbMetric: 27.9061 - val_loss: 28.3253 - val_MinusLogProbMetric: 28.3253 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 523/1000
2023-10-24 20:40:23.848 
Epoch 523/1000 
	 loss: 27.9008, MinusLogProbMetric: 27.9008, val_loss: 28.1913, val_MinusLogProbMetric: 28.1913

Epoch 523: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9008 - MinusLogProbMetric: 27.9008 - val_loss: 28.1913 - val_MinusLogProbMetric: 28.1913 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 524/1000
2023-10-24 20:40:58.010 
Epoch 524/1000 
	 loss: 27.9025, MinusLogProbMetric: 27.9025, val_loss: 28.2950, val_MinusLogProbMetric: 28.2950

Epoch 524: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9025 - MinusLogProbMetric: 27.9025 - val_loss: 28.2950 - val_MinusLogProbMetric: 28.2950 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 525/1000
2023-10-24 20:41:32.981 
Epoch 525/1000 
	 loss: 27.9038, MinusLogProbMetric: 27.9038, val_loss: 28.3135, val_MinusLogProbMetric: 28.3135

Epoch 525: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.9038 - MinusLogProbMetric: 27.9038 - val_loss: 28.3135 - val_MinusLogProbMetric: 28.3135 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 526/1000
2023-10-24 20:42:07.858 
Epoch 526/1000 
	 loss: 27.9082, MinusLogProbMetric: 27.9082, val_loss: 28.2208, val_MinusLogProbMetric: 28.2208

Epoch 526: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.9082 - MinusLogProbMetric: 27.9082 - val_loss: 28.2208 - val_MinusLogProbMetric: 28.2208 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 527/1000
2023-10-24 20:42:41.297 
Epoch 527/1000 
	 loss: 27.9141, MinusLogProbMetric: 27.9141, val_loss: 28.2103, val_MinusLogProbMetric: 28.2103

Epoch 527: val_loss did not improve from 28.15803
196/196 - 33s - loss: 27.9141 - MinusLogProbMetric: 27.9141 - val_loss: 28.2103 - val_MinusLogProbMetric: 28.2103 - lr: 2.5000e-04 - 33s/epoch - 171ms/step
Epoch 528/1000
2023-10-24 20:43:15.532 
Epoch 528/1000 
	 loss: 27.9080, MinusLogProbMetric: 27.9080, val_loss: 28.1920, val_MinusLogProbMetric: 28.1920

Epoch 528: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9080 - MinusLogProbMetric: 27.9080 - val_loss: 28.1920 - val_MinusLogProbMetric: 28.1920 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 529/1000
2023-10-24 20:43:50.517 
Epoch 529/1000 
	 loss: 27.9205, MinusLogProbMetric: 27.9205, val_loss: 28.2830, val_MinusLogProbMetric: 28.2830

Epoch 529: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.9205 - MinusLogProbMetric: 27.9205 - val_loss: 28.2830 - val_MinusLogProbMetric: 28.2830 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 530/1000
2023-10-24 20:44:25.080 
Epoch 530/1000 
	 loss: 27.9026, MinusLogProbMetric: 27.9026, val_loss: 28.2802, val_MinusLogProbMetric: 28.2802

Epoch 530: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.9026 - MinusLogProbMetric: 27.9026 - val_loss: 28.2802 - val_MinusLogProbMetric: 28.2802 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 531/1000
2023-10-24 20:44:59.509 
Epoch 531/1000 
	 loss: 27.9103, MinusLogProbMetric: 27.9103, val_loss: 28.1952, val_MinusLogProbMetric: 28.1952

Epoch 531: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9103 - MinusLogProbMetric: 27.9103 - val_loss: 28.1952 - val_MinusLogProbMetric: 28.1952 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 532/1000
2023-10-24 20:45:34.238 
Epoch 532/1000 
	 loss: 27.9026, MinusLogProbMetric: 27.9026, val_loss: 28.2121, val_MinusLogProbMetric: 28.2121

Epoch 532: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.9026 - MinusLogProbMetric: 27.9026 - val_loss: 28.2121 - val_MinusLogProbMetric: 28.2121 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 533/1000
2023-10-24 20:46:08.663 
Epoch 533/1000 
	 loss: 27.9066, MinusLogProbMetric: 27.9066, val_loss: 28.3283, val_MinusLogProbMetric: 28.3283

Epoch 533: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9066 - MinusLogProbMetric: 27.9066 - val_loss: 28.3283 - val_MinusLogProbMetric: 28.3283 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 534/1000
2023-10-24 20:46:43.143 
Epoch 534/1000 
	 loss: 27.9163, MinusLogProbMetric: 27.9163, val_loss: 28.3433, val_MinusLogProbMetric: 28.3433

Epoch 534: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9163 - MinusLogProbMetric: 27.9163 - val_loss: 28.3433 - val_MinusLogProbMetric: 28.3433 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 535/1000
2023-10-24 20:47:17.919 
Epoch 535/1000 
	 loss: 27.8955, MinusLogProbMetric: 27.8955, val_loss: 28.2209, val_MinusLogProbMetric: 28.2209

Epoch 535: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.8955 - MinusLogProbMetric: 27.8955 - val_loss: 28.2209 - val_MinusLogProbMetric: 28.2209 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 536/1000
2023-10-24 20:47:52.215 
Epoch 536/1000 
	 loss: 27.8948, MinusLogProbMetric: 27.8948, val_loss: 28.2069, val_MinusLogProbMetric: 28.2069

Epoch 536: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.8948 - MinusLogProbMetric: 27.8948 - val_loss: 28.2069 - val_MinusLogProbMetric: 28.2069 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 537/1000
2023-10-24 20:48:25.090 
Epoch 537/1000 
	 loss: 27.9167, MinusLogProbMetric: 27.9167, val_loss: 28.2892, val_MinusLogProbMetric: 28.2892

Epoch 537: val_loss did not improve from 28.15803
196/196 - 33s - loss: 27.9167 - MinusLogProbMetric: 27.9167 - val_loss: 28.2892 - val_MinusLogProbMetric: 28.2892 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 538/1000
2023-10-24 20:48:55.271 
Epoch 538/1000 
	 loss: 27.9071, MinusLogProbMetric: 27.9071, val_loss: 28.3017, val_MinusLogProbMetric: 28.3017

Epoch 538: val_loss did not improve from 28.15803
196/196 - 30s - loss: 27.9071 - MinusLogProbMetric: 27.9071 - val_loss: 28.3017 - val_MinusLogProbMetric: 28.3017 - lr: 2.5000e-04 - 30s/epoch - 154ms/step
Epoch 539/1000
2023-10-24 20:49:29.700 
Epoch 539/1000 
	 loss: 27.9054, MinusLogProbMetric: 27.9054, val_loss: 28.2445, val_MinusLogProbMetric: 28.2445

Epoch 539: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9054 - MinusLogProbMetric: 27.9054 - val_loss: 28.2445 - val_MinusLogProbMetric: 28.2445 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 540/1000
2023-10-24 20:50:04.248 
Epoch 540/1000 
	 loss: 27.9002, MinusLogProbMetric: 27.9002, val_loss: 28.2159, val_MinusLogProbMetric: 28.2159

Epoch 540: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.9002 - MinusLogProbMetric: 27.9002 - val_loss: 28.2159 - val_MinusLogProbMetric: 28.2159 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 541/1000
2023-10-24 20:50:39.244 
Epoch 541/1000 
	 loss: 27.8970, MinusLogProbMetric: 27.8970, val_loss: 28.2388, val_MinusLogProbMetric: 28.2388

Epoch 541: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.8970 - MinusLogProbMetric: 27.8970 - val_loss: 28.2388 - val_MinusLogProbMetric: 28.2388 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 542/1000
2023-10-24 20:51:13.744 
Epoch 542/1000 
	 loss: 27.9090, MinusLogProbMetric: 27.9090, val_loss: 28.3823, val_MinusLogProbMetric: 28.3823

Epoch 542: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9090 - MinusLogProbMetric: 27.9090 - val_loss: 28.3823 - val_MinusLogProbMetric: 28.3823 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 543/1000
2023-10-24 20:51:47.958 
Epoch 543/1000 
	 loss: 27.9124, MinusLogProbMetric: 27.9124, val_loss: 28.2465, val_MinusLogProbMetric: 28.2465

Epoch 543: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.9124 - MinusLogProbMetric: 27.9124 - val_loss: 28.2465 - val_MinusLogProbMetric: 28.2465 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 544/1000
2023-10-24 20:52:22.814 
Epoch 544/1000 
	 loss: 27.8932, MinusLogProbMetric: 27.8932, val_loss: 28.2003, val_MinusLogProbMetric: 28.2003

Epoch 544: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.8932 - MinusLogProbMetric: 27.8932 - val_loss: 28.2003 - val_MinusLogProbMetric: 28.2003 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 545/1000
2023-10-24 20:52:57.214 
Epoch 545/1000 
	 loss: 27.8957, MinusLogProbMetric: 27.8957, val_loss: 28.2458, val_MinusLogProbMetric: 28.2458

Epoch 545: val_loss did not improve from 28.15803
196/196 - 34s - loss: 27.8957 - MinusLogProbMetric: 27.8957 - val_loss: 28.2458 - val_MinusLogProbMetric: 28.2458 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 546/1000
2023-10-24 20:53:32.000 
Epoch 546/1000 
	 loss: 27.9011, MinusLogProbMetric: 27.9011, val_loss: 28.1972, val_MinusLogProbMetric: 28.1972

Epoch 546: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.9011 - MinusLogProbMetric: 27.9011 - val_loss: 28.1972 - val_MinusLogProbMetric: 28.1972 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 547/1000
2023-10-24 20:54:06.757 
Epoch 547/1000 
	 loss: 27.9057, MinusLogProbMetric: 27.9057, val_loss: 28.2873, val_MinusLogProbMetric: 28.2873

Epoch 547: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.9057 - MinusLogProbMetric: 27.9057 - val_loss: 28.2873 - val_MinusLogProbMetric: 28.2873 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 548/1000
2023-10-24 20:54:41.594 
Epoch 548/1000 
	 loss: 27.9009, MinusLogProbMetric: 27.9009, val_loss: 28.2089, val_MinusLogProbMetric: 28.2089

Epoch 548: val_loss did not improve from 28.15803
196/196 - 35s - loss: 27.9009 - MinusLogProbMetric: 27.9009 - val_loss: 28.2089 - val_MinusLogProbMetric: 28.2089 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 549/1000
2023-10-24 20:55:16.594 
Epoch 549/1000 
	 loss: 27.8189, MinusLogProbMetric: 27.8189, val_loss: 28.1227, val_MinusLogProbMetric: 28.1227

Epoch 549: val_loss improved from 28.15803 to 28.12271, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 36s - loss: 27.8189 - MinusLogProbMetric: 27.8189 - val_loss: 28.1227 - val_MinusLogProbMetric: 28.1227 - lr: 1.2500e-04 - 36s/epoch - 181ms/step
Epoch 550/1000
2023-10-24 20:55:51.722 
Epoch 550/1000 
	 loss: 27.8242, MinusLogProbMetric: 27.8242, val_loss: 28.1566, val_MinusLogProbMetric: 28.1566

Epoch 550: val_loss did not improve from 28.12271
196/196 - 35s - loss: 27.8242 - MinusLogProbMetric: 27.8242 - val_loss: 28.1566 - val_MinusLogProbMetric: 28.1566 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 551/1000
2023-10-24 20:56:26.082 
Epoch 551/1000 
	 loss: 27.8239, MinusLogProbMetric: 27.8239, val_loss: 28.1402, val_MinusLogProbMetric: 28.1402

Epoch 551: val_loss did not improve from 28.12271
196/196 - 34s - loss: 27.8239 - MinusLogProbMetric: 27.8239 - val_loss: 28.1402 - val_MinusLogProbMetric: 28.1402 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 552/1000
2023-10-24 20:57:00.738 
Epoch 552/1000 
	 loss: 27.8189, MinusLogProbMetric: 27.8189, val_loss: 28.1276, val_MinusLogProbMetric: 28.1276

Epoch 552: val_loss did not improve from 28.12271
196/196 - 35s - loss: 27.8189 - MinusLogProbMetric: 27.8189 - val_loss: 28.1276 - val_MinusLogProbMetric: 28.1276 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 553/1000
2023-10-24 20:57:35.264 
Epoch 553/1000 
	 loss: 27.8196, MinusLogProbMetric: 27.8196, val_loss: 28.1196, val_MinusLogProbMetric: 28.1196

Epoch 553: val_loss improved from 28.12271 to 28.11958, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.8196 - MinusLogProbMetric: 27.8196 - val_loss: 28.1196 - val_MinusLogProbMetric: 28.1196 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 554/1000
2023-10-24 20:58:09.849 
Epoch 554/1000 
	 loss: 27.8165, MinusLogProbMetric: 27.8165, val_loss: 28.1195, val_MinusLogProbMetric: 28.1195

Epoch 554: val_loss improved from 28.11958 to 28.11946, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.8165 - MinusLogProbMetric: 27.8165 - val_loss: 28.1195 - val_MinusLogProbMetric: 28.1195 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 555/1000
2023-10-24 20:58:45.059 
Epoch 555/1000 
	 loss: 27.8180, MinusLogProbMetric: 27.8180, val_loss: 28.1269, val_MinusLogProbMetric: 28.1269

Epoch 555: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8180 - MinusLogProbMetric: 27.8180 - val_loss: 28.1269 - val_MinusLogProbMetric: 28.1269 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 556/1000
2023-10-24 20:59:19.532 
Epoch 556/1000 
	 loss: 27.8127, MinusLogProbMetric: 27.8127, val_loss: 28.1567, val_MinusLogProbMetric: 28.1567

Epoch 556: val_loss did not improve from 28.11946
196/196 - 34s - loss: 27.8127 - MinusLogProbMetric: 27.8127 - val_loss: 28.1567 - val_MinusLogProbMetric: 28.1567 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 557/1000
2023-10-24 20:59:54.220 
Epoch 557/1000 
	 loss: 27.8170, MinusLogProbMetric: 27.8170, val_loss: 28.1269, val_MinusLogProbMetric: 28.1269

Epoch 557: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8170 - MinusLogProbMetric: 27.8170 - val_loss: 28.1269 - val_MinusLogProbMetric: 28.1269 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 558/1000
2023-10-24 21:00:28.876 
Epoch 558/1000 
	 loss: 27.8201, MinusLogProbMetric: 27.8201, val_loss: 28.1212, val_MinusLogProbMetric: 28.1212

Epoch 558: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8201 - MinusLogProbMetric: 27.8201 - val_loss: 28.1212 - val_MinusLogProbMetric: 28.1212 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 559/1000
2023-10-24 21:01:03.692 
Epoch 559/1000 
	 loss: 27.8173, MinusLogProbMetric: 27.8173, val_loss: 28.1328, val_MinusLogProbMetric: 28.1328

Epoch 559: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8173 - MinusLogProbMetric: 27.8173 - val_loss: 28.1328 - val_MinusLogProbMetric: 28.1328 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 560/1000
2023-10-24 21:01:38.804 
Epoch 560/1000 
	 loss: 27.8185, MinusLogProbMetric: 27.8185, val_loss: 28.1204, val_MinusLogProbMetric: 28.1204

Epoch 560: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8185 - MinusLogProbMetric: 27.8185 - val_loss: 28.1204 - val_MinusLogProbMetric: 28.1204 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 561/1000
2023-10-24 21:02:13.032 
Epoch 561/1000 
	 loss: 27.8168, MinusLogProbMetric: 27.8168, val_loss: 28.1771, val_MinusLogProbMetric: 28.1771

Epoch 561: val_loss did not improve from 28.11946
196/196 - 34s - loss: 27.8168 - MinusLogProbMetric: 27.8168 - val_loss: 28.1771 - val_MinusLogProbMetric: 28.1771 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 562/1000
2023-10-24 21:02:47.757 
Epoch 562/1000 
	 loss: 27.8203, MinusLogProbMetric: 27.8203, val_loss: 28.1527, val_MinusLogProbMetric: 28.1527

Epoch 562: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8203 - MinusLogProbMetric: 27.8203 - val_loss: 28.1527 - val_MinusLogProbMetric: 28.1527 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 563/1000
2023-10-24 21:03:22.074 
Epoch 563/1000 
	 loss: 27.8175, MinusLogProbMetric: 27.8175, val_loss: 28.1286, val_MinusLogProbMetric: 28.1286

Epoch 563: val_loss did not improve from 28.11946
196/196 - 34s - loss: 27.8175 - MinusLogProbMetric: 27.8175 - val_loss: 28.1286 - val_MinusLogProbMetric: 28.1286 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 564/1000
2023-10-24 21:03:57.148 
Epoch 564/1000 
	 loss: 27.8108, MinusLogProbMetric: 27.8108, val_loss: 28.1556, val_MinusLogProbMetric: 28.1556

Epoch 564: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8108 - MinusLogProbMetric: 27.8108 - val_loss: 28.1556 - val_MinusLogProbMetric: 28.1556 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 565/1000
2023-10-24 21:04:31.995 
Epoch 565/1000 
	 loss: 27.8212, MinusLogProbMetric: 27.8212, val_loss: 28.1335, val_MinusLogProbMetric: 28.1335

Epoch 565: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8212 - MinusLogProbMetric: 27.8212 - val_loss: 28.1335 - val_MinusLogProbMetric: 28.1335 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 566/1000
2023-10-24 21:05:06.609 
Epoch 566/1000 
	 loss: 27.8146, MinusLogProbMetric: 27.8146, val_loss: 28.1564, val_MinusLogProbMetric: 28.1564

Epoch 566: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8146 - MinusLogProbMetric: 27.8146 - val_loss: 28.1564 - val_MinusLogProbMetric: 28.1564 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 567/1000
2023-10-24 21:05:41.390 
Epoch 567/1000 
	 loss: 27.8182, MinusLogProbMetric: 27.8182, val_loss: 28.1486, val_MinusLogProbMetric: 28.1486

Epoch 567: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8182 - MinusLogProbMetric: 27.8182 - val_loss: 28.1486 - val_MinusLogProbMetric: 28.1486 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 568/1000
2023-10-24 21:06:16.052 
Epoch 568/1000 
	 loss: 27.8150, MinusLogProbMetric: 27.8150, val_loss: 28.1455, val_MinusLogProbMetric: 28.1455

Epoch 568: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8150 - MinusLogProbMetric: 27.8150 - val_loss: 28.1455 - val_MinusLogProbMetric: 28.1455 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 569/1000
2023-10-24 21:06:50.516 
Epoch 569/1000 
	 loss: 27.8158, MinusLogProbMetric: 27.8158, val_loss: 28.1314, val_MinusLogProbMetric: 28.1314

Epoch 569: val_loss did not improve from 28.11946
196/196 - 34s - loss: 27.8158 - MinusLogProbMetric: 27.8158 - val_loss: 28.1314 - val_MinusLogProbMetric: 28.1314 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 570/1000
2023-10-24 21:07:25.201 
Epoch 570/1000 
	 loss: 27.8172, MinusLogProbMetric: 27.8172, val_loss: 28.1297, val_MinusLogProbMetric: 28.1297

Epoch 570: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8172 - MinusLogProbMetric: 27.8172 - val_loss: 28.1297 - val_MinusLogProbMetric: 28.1297 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 571/1000
2023-10-24 21:07:59.866 
Epoch 571/1000 
	 loss: 27.8224, MinusLogProbMetric: 27.8224, val_loss: 28.1614, val_MinusLogProbMetric: 28.1614

Epoch 571: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8224 - MinusLogProbMetric: 27.8224 - val_loss: 28.1614 - val_MinusLogProbMetric: 28.1614 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 572/1000
2023-10-24 21:08:34.848 
Epoch 572/1000 
	 loss: 27.8215, MinusLogProbMetric: 27.8215, val_loss: 28.1282, val_MinusLogProbMetric: 28.1282

Epoch 572: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8215 - MinusLogProbMetric: 27.8215 - val_loss: 28.1282 - val_MinusLogProbMetric: 28.1282 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 573/1000
2023-10-24 21:09:09.789 
Epoch 573/1000 
	 loss: 27.8223, MinusLogProbMetric: 27.8223, val_loss: 28.1556, val_MinusLogProbMetric: 28.1556

Epoch 573: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8223 - MinusLogProbMetric: 27.8223 - val_loss: 28.1556 - val_MinusLogProbMetric: 28.1556 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 574/1000
2023-10-24 21:09:44.374 
Epoch 574/1000 
	 loss: 27.8172, MinusLogProbMetric: 27.8172, val_loss: 28.1387, val_MinusLogProbMetric: 28.1387

Epoch 574: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8172 - MinusLogProbMetric: 27.8172 - val_loss: 28.1387 - val_MinusLogProbMetric: 28.1387 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 575/1000
2023-10-24 21:10:19.230 
Epoch 575/1000 
	 loss: 27.8226, MinusLogProbMetric: 27.8226, val_loss: 28.1514, val_MinusLogProbMetric: 28.1514

Epoch 575: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8226 - MinusLogProbMetric: 27.8226 - val_loss: 28.1514 - val_MinusLogProbMetric: 28.1514 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 576/1000
2023-10-24 21:10:53.564 
Epoch 576/1000 
	 loss: 27.8235, MinusLogProbMetric: 27.8235, val_loss: 28.1252, val_MinusLogProbMetric: 28.1252

Epoch 576: val_loss did not improve from 28.11946
196/196 - 34s - loss: 27.8235 - MinusLogProbMetric: 27.8235 - val_loss: 28.1252 - val_MinusLogProbMetric: 28.1252 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 577/1000
2023-10-24 21:11:28.215 
Epoch 577/1000 
	 loss: 27.8204, MinusLogProbMetric: 27.8204, val_loss: 28.1390, val_MinusLogProbMetric: 28.1390

Epoch 577: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8204 - MinusLogProbMetric: 27.8204 - val_loss: 28.1390 - val_MinusLogProbMetric: 28.1390 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 578/1000
2023-10-24 21:12:03.199 
Epoch 578/1000 
	 loss: 27.8155, MinusLogProbMetric: 27.8155, val_loss: 28.1745, val_MinusLogProbMetric: 28.1745

Epoch 578: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8155 - MinusLogProbMetric: 27.8155 - val_loss: 28.1745 - val_MinusLogProbMetric: 28.1745 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 579/1000
2023-10-24 21:12:38.023 
Epoch 579/1000 
	 loss: 27.8145, MinusLogProbMetric: 27.8145, val_loss: 28.1229, val_MinusLogProbMetric: 28.1229

Epoch 579: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8145 - MinusLogProbMetric: 27.8145 - val_loss: 28.1229 - val_MinusLogProbMetric: 28.1229 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 580/1000
2023-10-24 21:13:12.457 
Epoch 580/1000 
	 loss: 27.8123, MinusLogProbMetric: 27.8123, val_loss: 28.1455, val_MinusLogProbMetric: 28.1455

Epoch 580: val_loss did not improve from 28.11946
196/196 - 34s - loss: 27.8123 - MinusLogProbMetric: 27.8123 - val_loss: 28.1455 - val_MinusLogProbMetric: 28.1455 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 581/1000
2023-10-24 21:13:45.810 
Epoch 581/1000 
	 loss: 27.8153, MinusLogProbMetric: 27.8153, val_loss: 28.1238, val_MinusLogProbMetric: 28.1238

Epoch 581: val_loss did not improve from 28.11946
196/196 - 33s - loss: 27.8153 - MinusLogProbMetric: 27.8153 - val_loss: 28.1238 - val_MinusLogProbMetric: 28.1238 - lr: 1.2500e-04 - 33s/epoch - 170ms/step
Epoch 582/1000
2023-10-24 21:14:20.686 
Epoch 582/1000 
	 loss: 27.8145, MinusLogProbMetric: 27.8145, val_loss: 28.1221, val_MinusLogProbMetric: 28.1221

Epoch 582: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8145 - MinusLogProbMetric: 27.8145 - val_loss: 28.1221 - val_MinusLogProbMetric: 28.1221 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 583/1000
2023-10-24 21:14:55.761 
Epoch 583/1000 
	 loss: 27.8150, MinusLogProbMetric: 27.8150, val_loss: 28.1256, val_MinusLogProbMetric: 28.1256

Epoch 583: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8150 - MinusLogProbMetric: 27.8150 - val_loss: 28.1256 - val_MinusLogProbMetric: 28.1256 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 584/1000
2023-10-24 21:15:30.636 
Epoch 584/1000 
	 loss: 27.8218, MinusLogProbMetric: 27.8218, val_loss: 28.1324, val_MinusLogProbMetric: 28.1324

Epoch 584: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8218 - MinusLogProbMetric: 27.8218 - val_loss: 28.1324 - val_MinusLogProbMetric: 28.1324 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 585/1000
2023-10-24 21:16:05.518 
Epoch 585/1000 
	 loss: 27.8153, MinusLogProbMetric: 27.8153, val_loss: 28.1377, val_MinusLogProbMetric: 28.1377

Epoch 585: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8153 - MinusLogProbMetric: 27.8153 - val_loss: 28.1377 - val_MinusLogProbMetric: 28.1377 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 586/1000
2023-10-24 21:16:40.276 
Epoch 586/1000 
	 loss: 27.8117, MinusLogProbMetric: 27.8117, val_loss: 28.1506, val_MinusLogProbMetric: 28.1506

Epoch 586: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8117 - MinusLogProbMetric: 27.8117 - val_loss: 28.1506 - val_MinusLogProbMetric: 28.1506 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 587/1000
2023-10-24 21:17:14.985 
Epoch 587/1000 
	 loss: 27.8132, MinusLogProbMetric: 27.8132, val_loss: 28.1391, val_MinusLogProbMetric: 28.1391

Epoch 587: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8132 - MinusLogProbMetric: 27.8132 - val_loss: 28.1391 - val_MinusLogProbMetric: 28.1391 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 588/1000
2023-10-24 21:17:49.217 
Epoch 588/1000 
	 loss: 27.8131, MinusLogProbMetric: 27.8131, val_loss: 28.1272, val_MinusLogProbMetric: 28.1272

Epoch 588: val_loss did not improve from 28.11946
196/196 - 34s - loss: 27.8131 - MinusLogProbMetric: 27.8131 - val_loss: 28.1272 - val_MinusLogProbMetric: 28.1272 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 589/1000
2023-10-24 21:18:23.879 
Epoch 589/1000 
	 loss: 27.8174, MinusLogProbMetric: 27.8174, val_loss: 28.1326, val_MinusLogProbMetric: 28.1326

Epoch 589: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8174 - MinusLogProbMetric: 27.8174 - val_loss: 28.1326 - val_MinusLogProbMetric: 28.1326 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 590/1000
2023-10-24 21:18:58.626 
Epoch 590/1000 
	 loss: 27.8206, MinusLogProbMetric: 27.8206, val_loss: 28.1675, val_MinusLogProbMetric: 28.1675

Epoch 590: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8206 - MinusLogProbMetric: 27.8206 - val_loss: 28.1675 - val_MinusLogProbMetric: 28.1675 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 591/1000
2023-10-24 21:19:32.713 
Epoch 591/1000 
	 loss: 27.8128, MinusLogProbMetric: 27.8128, val_loss: 28.1372, val_MinusLogProbMetric: 28.1372

Epoch 591: val_loss did not improve from 28.11946
196/196 - 34s - loss: 27.8128 - MinusLogProbMetric: 27.8128 - val_loss: 28.1372 - val_MinusLogProbMetric: 28.1372 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 592/1000
2023-10-24 21:20:07.376 
Epoch 592/1000 
	 loss: 27.8179, MinusLogProbMetric: 27.8179, val_loss: 28.1395, val_MinusLogProbMetric: 28.1395

Epoch 592: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8179 - MinusLogProbMetric: 27.8179 - val_loss: 28.1395 - val_MinusLogProbMetric: 28.1395 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 593/1000
2023-10-24 21:20:41.960 
Epoch 593/1000 
	 loss: 27.8093, MinusLogProbMetric: 27.8093, val_loss: 28.1372, val_MinusLogProbMetric: 28.1372

Epoch 593: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8093 - MinusLogProbMetric: 27.8093 - val_loss: 28.1372 - val_MinusLogProbMetric: 28.1372 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 594/1000
2023-10-24 21:21:17.118 
Epoch 594/1000 
	 loss: 27.8178, MinusLogProbMetric: 27.8178, val_loss: 28.1560, val_MinusLogProbMetric: 28.1560

Epoch 594: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8178 - MinusLogProbMetric: 27.8178 - val_loss: 28.1560 - val_MinusLogProbMetric: 28.1560 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 595/1000
2023-10-24 21:21:52.193 
Epoch 595/1000 
	 loss: 27.8147, MinusLogProbMetric: 27.8147, val_loss: 28.1740, val_MinusLogProbMetric: 28.1740

Epoch 595: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8147 - MinusLogProbMetric: 27.8147 - val_loss: 28.1740 - val_MinusLogProbMetric: 28.1740 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 596/1000
2023-10-24 21:22:26.765 
Epoch 596/1000 
	 loss: 27.8108, MinusLogProbMetric: 27.8108, val_loss: 28.1423, val_MinusLogProbMetric: 28.1423

Epoch 596: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8108 - MinusLogProbMetric: 27.8108 - val_loss: 28.1423 - val_MinusLogProbMetric: 28.1423 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 597/1000
2023-10-24 21:23:01.149 
Epoch 597/1000 
	 loss: 27.8092, MinusLogProbMetric: 27.8092, val_loss: 28.1399, val_MinusLogProbMetric: 28.1399

Epoch 597: val_loss did not improve from 28.11946
196/196 - 34s - loss: 27.8092 - MinusLogProbMetric: 27.8092 - val_loss: 28.1399 - val_MinusLogProbMetric: 28.1399 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 598/1000
2023-10-24 21:23:35.750 
Epoch 598/1000 
	 loss: 27.8117, MinusLogProbMetric: 27.8117, val_loss: 28.1978, val_MinusLogProbMetric: 28.1978

Epoch 598: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8117 - MinusLogProbMetric: 27.8117 - val_loss: 28.1978 - val_MinusLogProbMetric: 28.1978 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 599/1000
2023-10-24 21:24:10.303 
Epoch 599/1000 
	 loss: 27.8147, MinusLogProbMetric: 27.8147, val_loss: 28.1509, val_MinusLogProbMetric: 28.1509

Epoch 599: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8147 - MinusLogProbMetric: 27.8147 - val_loss: 28.1509 - val_MinusLogProbMetric: 28.1509 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 600/1000
2023-10-24 21:24:44.894 
Epoch 600/1000 
	 loss: 27.8131, MinusLogProbMetric: 27.8131, val_loss: 28.1375, val_MinusLogProbMetric: 28.1375

Epoch 600: val_loss did not improve from 28.11946
196/196 - 35s - loss: 27.8131 - MinusLogProbMetric: 27.8131 - val_loss: 28.1375 - val_MinusLogProbMetric: 28.1375 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 601/1000
2023-10-24 21:25:19.614 
Epoch 601/1000 
	 loss: 27.8080, MinusLogProbMetric: 27.8080, val_loss: 28.1126, val_MinusLogProbMetric: 28.1126

Epoch 601: val_loss improved from 28.11946 to 28.11262, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.8080 - MinusLogProbMetric: 27.8080 - val_loss: 28.1126 - val_MinusLogProbMetric: 28.1126 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 602/1000
2023-10-24 21:25:54.815 
Epoch 602/1000 
	 loss: 27.8068, MinusLogProbMetric: 27.8068, val_loss: 28.1114, val_MinusLogProbMetric: 28.1114

Epoch 602: val_loss improved from 28.11262 to 28.11138, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.8068 - MinusLogProbMetric: 27.8068 - val_loss: 28.1114 - val_MinusLogProbMetric: 28.1114 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 603/1000
2023-10-24 21:26:30.074 
Epoch 603/1000 
	 loss: 27.8175, MinusLogProbMetric: 27.8175, val_loss: 28.1350, val_MinusLogProbMetric: 28.1350

Epoch 603: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8175 - MinusLogProbMetric: 27.8175 - val_loss: 28.1350 - val_MinusLogProbMetric: 28.1350 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 604/1000
2023-10-24 21:27:04.618 
Epoch 604/1000 
	 loss: 27.8123, MinusLogProbMetric: 27.8123, val_loss: 28.1386, val_MinusLogProbMetric: 28.1386

Epoch 604: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8123 - MinusLogProbMetric: 27.8123 - val_loss: 28.1386 - val_MinusLogProbMetric: 28.1386 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 605/1000
2023-10-24 21:27:39.463 
Epoch 605/1000 
	 loss: 27.8113, MinusLogProbMetric: 27.8113, val_loss: 28.1185, val_MinusLogProbMetric: 28.1185

Epoch 605: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8113 - MinusLogProbMetric: 27.8113 - val_loss: 28.1185 - val_MinusLogProbMetric: 28.1185 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 606/1000
2023-10-24 21:28:13.685 
Epoch 606/1000 
	 loss: 27.8074, MinusLogProbMetric: 27.8074, val_loss: 28.1252, val_MinusLogProbMetric: 28.1252

Epoch 606: val_loss did not improve from 28.11138
196/196 - 34s - loss: 27.8074 - MinusLogProbMetric: 27.8074 - val_loss: 28.1252 - val_MinusLogProbMetric: 28.1252 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 607/1000
2023-10-24 21:28:48.327 
Epoch 607/1000 
	 loss: 27.8137, MinusLogProbMetric: 27.8137, val_loss: 28.1255, val_MinusLogProbMetric: 28.1255

Epoch 607: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8137 - MinusLogProbMetric: 27.8137 - val_loss: 28.1255 - val_MinusLogProbMetric: 28.1255 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 608/1000
2023-10-24 21:29:23.105 
Epoch 608/1000 
	 loss: 27.8127, MinusLogProbMetric: 27.8127, val_loss: 28.1635, val_MinusLogProbMetric: 28.1635

Epoch 608: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8127 - MinusLogProbMetric: 27.8127 - val_loss: 28.1635 - val_MinusLogProbMetric: 28.1635 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 609/1000
2023-10-24 21:29:58.135 
Epoch 609/1000 
	 loss: 27.8063, MinusLogProbMetric: 27.8063, val_loss: 28.1629, val_MinusLogProbMetric: 28.1629

Epoch 609: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8063 - MinusLogProbMetric: 27.8063 - val_loss: 28.1629 - val_MinusLogProbMetric: 28.1629 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 610/1000
2023-10-24 21:30:32.742 
Epoch 610/1000 
	 loss: 27.8181, MinusLogProbMetric: 27.8181, val_loss: 28.1524, val_MinusLogProbMetric: 28.1524

Epoch 610: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8181 - MinusLogProbMetric: 27.8181 - val_loss: 28.1524 - val_MinusLogProbMetric: 28.1524 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 611/1000
2023-10-24 21:31:07.385 
Epoch 611/1000 
	 loss: 27.8139, MinusLogProbMetric: 27.8139, val_loss: 28.1272, val_MinusLogProbMetric: 28.1272

Epoch 611: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8139 - MinusLogProbMetric: 27.8139 - val_loss: 28.1272 - val_MinusLogProbMetric: 28.1272 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 612/1000
2023-10-24 21:31:41.775 
Epoch 612/1000 
	 loss: 27.8069, MinusLogProbMetric: 27.8069, val_loss: 28.1477, val_MinusLogProbMetric: 28.1477

Epoch 612: val_loss did not improve from 28.11138
196/196 - 34s - loss: 27.8069 - MinusLogProbMetric: 27.8069 - val_loss: 28.1477 - val_MinusLogProbMetric: 28.1477 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 613/1000
2023-10-24 21:32:16.588 
Epoch 613/1000 
	 loss: 27.8113, MinusLogProbMetric: 27.8113, val_loss: 28.1248, val_MinusLogProbMetric: 28.1248

Epoch 613: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8113 - MinusLogProbMetric: 27.8113 - val_loss: 28.1248 - val_MinusLogProbMetric: 28.1248 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 614/1000
2023-10-24 21:32:50.817 
Epoch 614/1000 
	 loss: 27.8113, MinusLogProbMetric: 27.8113, val_loss: 28.1563, val_MinusLogProbMetric: 28.1563

Epoch 614: val_loss did not improve from 28.11138
196/196 - 34s - loss: 27.8113 - MinusLogProbMetric: 27.8113 - val_loss: 28.1563 - val_MinusLogProbMetric: 28.1563 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 615/1000
2023-10-24 21:33:25.485 
Epoch 615/1000 
	 loss: 27.8054, MinusLogProbMetric: 27.8054, val_loss: 28.1665, val_MinusLogProbMetric: 28.1665

Epoch 615: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8054 - MinusLogProbMetric: 27.8054 - val_loss: 28.1665 - val_MinusLogProbMetric: 28.1665 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 616/1000
2023-10-24 21:34:00.598 
Epoch 616/1000 
	 loss: 27.8072, MinusLogProbMetric: 27.8072, val_loss: 28.1494, val_MinusLogProbMetric: 28.1494

Epoch 616: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8072 - MinusLogProbMetric: 27.8072 - val_loss: 28.1494 - val_MinusLogProbMetric: 28.1494 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 617/1000
2023-10-24 21:34:35.329 
Epoch 617/1000 
	 loss: 27.8139, MinusLogProbMetric: 27.8139, val_loss: 28.1604, val_MinusLogProbMetric: 28.1604

Epoch 617: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8139 - MinusLogProbMetric: 27.8139 - val_loss: 28.1604 - val_MinusLogProbMetric: 28.1604 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 618/1000
2023-10-24 21:35:10.118 
Epoch 618/1000 
	 loss: 27.8004, MinusLogProbMetric: 27.8004, val_loss: 28.1182, val_MinusLogProbMetric: 28.1182

Epoch 618: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8004 - MinusLogProbMetric: 27.8004 - val_loss: 28.1182 - val_MinusLogProbMetric: 28.1182 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 619/1000
2023-10-24 21:35:44.960 
Epoch 619/1000 
	 loss: 27.8126, MinusLogProbMetric: 27.8126, val_loss: 28.1408, val_MinusLogProbMetric: 28.1408

Epoch 619: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8126 - MinusLogProbMetric: 27.8126 - val_loss: 28.1408 - val_MinusLogProbMetric: 28.1408 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 620/1000
2023-10-24 21:36:19.707 
Epoch 620/1000 
	 loss: 27.8070, MinusLogProbMetric: 27.8070, val_loss: 28.1287, val_MinusLogProbMetric: 28.1287

Epoch 620: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8070 - MinusLogProbMetric: 27.8070 - val_loss: 28.1287 - val_MinusLogProbMetric: 28.1287 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 621/1000
2023-10-24 21:36:54.365 
Epoch 621/1000 
	 loss: 27.8063, MinusLogProbMetric: 27.8063, val_loss: 28.1326, val_MinusLogProbMetric: 28.1326

Epoch 621: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8063 - MinusLogProbMetric: 27.8063 - val_loss: 28.1326 - val_MinusLogProbMetric: 28.1326 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 622/1000
2023-10-24 21:37:28.555 
Epoch 622/1000 
	 loss: 27.8046, MinusLogProbMetric: 27.8046, val_loss: 28.1257, val_MinusLogProbMetric: 28.1257

Epoch 622: val_loss did not improve from 28.11138
196/196 - 34s - loss: 27.8046 - MinusLogProbMetric: 27.8046 - val_loss: 28.1257 - val_MinusLogProbMetric: 28.1257 - lr: 1.2500e-04 - 34s/epoch - 174ms/step
Epoch 623/1000
2023-10-24 21:38:03.042 
Epoch 623/1000 
	 loss: 27.8087, MinusLogProbMetric: 27.8087, val_loss: 28.1577, val_MinusLogProbMetric: 28.1577

Epoch 623: val_loss did not improve from 28.11138
196/196 - 34s - loss: 27.8087 - MinusLogProbMetric: 27.8087 - val_loss: 28.1577 - val_MinusLogProbMetric: 28.1577 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 624/1000
2023-10-24 21:38:37.507 
Epoch 624/1000 
	 loss: 27.8112, MinusLogProbMetric: 27.8112, val_loss: 28.1188, val_MinusLogProbMetric: 28.1188

Epoch 624: val_loss did not improve from 28.11138
196/196 - 34s - loss: 27.8112 - MinusLogProbMetric: 27.8112 - val_loss: 28.1188 - val_MinusLogProbMetric: 28.1188 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 625/1000
2023-10-24 21:39:12.163 
Epoch 625/1000 
	 loss: 27.8006, MinusLogProbMetric: 27.8006, val_loss: 28.1424, val_MinusLogProbMetric: 28.1424

Epoch 625: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8006 - MinusLogProbMetric: 27.8006 - val_loss: 28.1424 - val_MinusLogProbMetric: 28.1424 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 626/1000
2023-10-24 21:39:47.134 
Epoch 626/1000 
	 loss: 27.8047, MinusLogProbMetric: 27.8047, val_loss: 28.1668, val_MinusLogProbMetric: 28.1668

Epoch 626: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8047 - MinusLogProbMetric: 27.8047 - val_loss: 28.1668 - val_MinusLogProbMetric: 28.1668 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 627/1000
2023-10-24 21:40:21.862 
Epoch 627/1000 
	 loss: 27.8098, MinusLogProbMetric: 27.8098, val_loss: 28.1200, val_MinusLogProbMetric: 28.1200

Epoch 627: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8098 - MinusLogProbMetric: 27.8098 - val_loss: 28.1200 - val_MinusLogProbMetric: 28.1200 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 628/1000
2023-10-24 21:40:54.461 
Epoch 628/1000 
	 loss: 27.8069, MinusLogProbMetric: 27.8069, val_loss: 28.1291, val_MinusLogProbMetric: 28.1291

Epoch 628: val_loss did not improve from 28.11138
196/196 - 33s - loss: 27.8069 - MinusLogProbMetric: 27.8069 - val_loss: 28.1291 - val_MinusLogProbMetric: 28.1291 - lr: 1.2500e-04 - 33s/epoch - 166ms/step
Epoch 629/1000
2023-10-24 21:41:28.683 
Epoch 629/1000 
	 loss: 27.8059, MinusLogProbMetric: 27.8059, val_loss: 28.1567, val_MinusLogProbMetric: 28.1567

Epoch 629: val_loss did not improve from 28.11138
196/196 - 34s - loss: 27.8059 - MinusLogProbMetric: 27.8059 - val_loss: 28.1567 - val_MinusLogProbMetric: 28.1567 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 630/1000
2023-10-24 21:42:02.171 
Epoch 630/1000 
	 loss: 27.8044, MinusLogProbMetric: 27.8044, val_loss: 28.1202, val_MinusLogProbMetric: 28.1202

Epoch 630: val_loss did not improve from 28.11138
196/196 - 33s - loss: 27.8044 - MinusLogProbMetric: 27.8044 - val_loss: 28.1202 - val_MinusLogProbMetric: 28.1202 - lr: 1.2500e-04 - 33s/epoch - 171ms/step
Epoch 631/1000
2023-10-24 21:42:36.488 
Epoch 631/1000 
	 loss: 27.8077, MinusLogProbMetric: 27.8077, val_loss: 28.1390, val_MinusLogProbMetric: 28.1390

Epoch 631: val_loss did not improve from 28.11138
196/196 - 34s - loss: 27.8077 - MinusLogProbMetric: 27.8077 - val_loss: 28.1390 - val_MinusLogProbMetric: 28.1390 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 632/1000
2023-10-24 21:43:11.342 
Epoch 632/1000 
	 loss: 27.8035, MinusLogProbMetric: 27.8035, val_loss: 28.1520, val_MinusLogProbMetric: 28.1520

Epoch 632: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8035 - MinusLogProbMetric: 27.8035 - val_loss: 28.1520 - val_MinusLogProbMetric: 28.1520 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 633/1000
2023-10-24 21:43:45.855 
Epoch 633/1000 
	 loss: 27.8064, MinusLogProbMetric: 27.8064, val_loss: 28.1387, val_MinusLogProbMetric: 28.1387

Epoch 633: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8064 - MinusLogProbMetric: 27.8064 - val_loss: 28.1387 - val_MinusLogProbMetric: 28.1387 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 634/1000
2023-10-24 21:44:20.632 
Epoch 634/1000 
	 loss: 27.8060, MinusLogProbMetric: 27.8060, val_loss: 28.1168, val_MinusLogProbMetric: 28.1168

Epoch 634: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8060 - MinusLogProbMetric: 27.8060 - val_loss: 28.1168 - val_MinusLogProbMetric: 28.1168 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 635/1000
2023-10-24 21:44:55.396 
Epoch 635/1000 
	 loss: 27.8077, MinusLogProbMetric: 27.8077, val_loss: 28.1441, val_MinusLogProbMetric: 28.1441

Epoch 635: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8077 - MinusLogProbMetric: 27.8077 - val_loss: 28.1441 - val_MinusLogProbMetric: 28.1441 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 636/1000
2023-10-24 21:45:30.107 
Epoch 636/1000 
	 loss: 27.8041, MinusLogProbMetric: 27.8041, val_loss: 28.1470, val_MinusLogProbMetric: 28.1470

Epoch 636: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8041 - MinusLogProbMetric: 27.8041 - val_loss: 28.1470 - val_MinusLogProbMetric: 28.1470 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 637/1000
2023-10-24 21:46:05.214 
Epoch 637/1000 
	 loss: 27.8106, MinusLogProbMetric: 27.8106, val_loss: 28.1653, val_MinusLogProbMetric: 28.1653

Epoch 637: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8106 - MinusLogProbMetric: 27.8106 - val_loss: 28.1653 - val_MinusLogProbMetric: 28.1653 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 638/1000
2023-10-24 21:46:40.058 
Epoch 638/1000 
	 loss: 27.8083, MinusLogProbMetric: 27.8083, val_loss: 28.1307, val_MinusLogProbMetric: 28.1307

Epoch 638: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8083 - MinusLogProbMetric: 27.8083 - val_loss: 28.1307 - val_MinusLogProbMetric: 28.1307 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 639/1000
2023-10-24 21:47:15.023 
Epoch 639/1000 
	 loss: 27.8055, MinusLogProbMetric: 27.8055, val_loss: 28.1265, val_MinusLogProbMetric: 28.1265

Epoch 639: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8055 - MinusLogProbMetric: 27.8055 - val_loss: 28.1265 - val_MinusLogProbMetric: 28.1265 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 640/1000
2023-10-24 21:47:49.836 
Epoch 640/1000 
	 loss: 27.8030, MinusLogProbMetric: 27.8030, val_loss: 28.1280, val_MinusLogProbMetric: 28.1280

Epoch 640: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8030 - MinusLogProbMetric: 27.8030 - val_loss: 28.1280 - val_MinusLogProbMetric: 28.1280 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 641/1000
2023-10-24 21:48:24.610 
Epoch 641/1000 
	 loss: 27.8092, MinusLogProbMetric: 27.8092, val_loss: 28.1469, val_MinusLogProbMetric: 28.1469

Epoch 641: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8092 - MinusLogProbMetric: 27.8092 - val_loss: 28.1469 - val_MinusLogProbMetric: 28.1469 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 642/1000
2023-10-24 21:48:59.042 
Epoch 642/1000 
	 loss: 27.8023, MinusLogProbMetric: 27.8023, val_loss: 28.1576, val_MinusLogProbMetric: 28.1576

Epoch 642: val_loss did not improve from 28.11138
196/196 - 34s - loss: 27.8023 - MinusLogProbMetric: 27.8023 - val_loss: 28.1576 - val_MinusLogProbMetric: 28.1576 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 643/1000
2023-10-24 21:49:34.179 
Epoch 643/1000 
	 loss: 27.8106, MinusLogProbMetric: 27.8106, val_loss: 28.1135, val_MinusLogProbMetric: 28.1135

Epoch 643: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.8106 - MinusLogProbMetric: 27.8106 - val_loss: 28.1135 - val_MinusLogProbMetric: 28.1135 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 644/1000
2023-10-24 21:50:08.748 
Epoch 644/1000 
	 loss: 27.7984, MinusLogProbMetric: 27.7984, val_loss: 28.1127, val_MinusLogProbMetric: 28.1127

Epoch 644: val_loss did not improve from 28.11138
196/196 - 35s - loss: 27.7984 - MinusLogProbMetric: 27.7984 - val_loss: 28.1127 - val_MinusLogProbMetric: 28.1127 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 645/1000
2023-10-24 21:50:43.757 
Epoch 645/1000 
	 loss: 27.8103, MinusLogProbMetric: 27.8103, val_loss: 28.1002, val_MinusLogProbMetric: 28.1002

Epoch 645: val_loss improved from 28.11138 to 28.10023, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.8103 - MinusLogProbMetric: 27.8103 - val_loss: 28.1002 - val_MinusLogProbMetric: 28.1002 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 646/1000
2023-10-24 21:51:19.169 
Epoch 646/1000 
	 loss: 27.8014, MinusLogProbMetric: 27.8014, val_loss: 28.1422, val_MinusLogProbMetric: 28.1422

Epoch 646: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8014 - MinusLogProbMetric: 27.8014 - val_loss: 28.1422 - val_MinusLogProbMetric: 28.1422 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 647/1000
2023-10-24 21:51:53.168 
Epoch 647/1000 
	 loss: 27.8014, MinusLogProbMetric: 27.8014, val_loss: 28.1253, val_MinusLogProbMetric: 28.1253

Epoch 647: val_loss did not improve from 28.10023
196/196 - 34s - loss: 27.8014 - MinusLogProbMetric: 27.8014 - val_loss: 28.1253 - val_MinusLogProbMetric: 28.1253 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 648/1000
2023-10-24 21:52:27.802 
Epoch 648/1000 
	 loss: 27.8085, MinusLogProbMetric: 27.8085, val_loss: 28.1320, val_MinusLogProbMetric: 28.1320

Epoch 648: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8085 - MinusLogProbMetric: 27.8085 - val_loss: 28.1320 - val_MinusLogProbMetric: 28.1320 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 649/1000
2023-10-24 21:53:02.749 
Epoch 649/1000 
	 loss: 27.8062, MinusLogProbMetric: 27.8062, val_loss: 28.1213, val_MinusLogProbMetric: 28.1213

Epoch 649: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8062 - MinusLogProbMetric: 27.8062 - val_loss: 28.1213 - val_MinusLogProbMetric: 28.1213 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 650/1000
2023-10-24 21:53:37.820 
Epoch 650/1000 
	 loss: 27.8061, MinusLogProbMetric: 27.8061, val_loss: 28.1210, val_MinusLogProbMetric: 28.1210

Epoch 650: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8061 - MinusLogProbMetric: 27.8061 - val_loss: 28.1210 - val_MinusLogProbMetric: 28.1210 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 651/1000
2023-10-24 21:54:12.802 
Epoch 651/1000 
	 loss: 27.8055, MinusLogProbMetric: 27.8055, val_loss: 28.1255, val_MinusLogProbMetric: 28.1255

Epoch 651: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8055 - MinusLogProbMetric: 27.8055 - val_loss: 28.1255 - val_MinusLogProbMetric: 28.1255 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 652/1000
2023-10-24 21:54:47.841 
Epoch 652/1000 
	 loss: 27.8022, MinusLogProbMetric: 27.8022, val_loss: 28.1297, val_MinusLogProbMetric: 28.1297

Epoch 652: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8022 - MinusLogProbMetric: 27.8022 - val_loss: 28.1297 - val_MinusLogProbMetric: 28.1297 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 653/1000
2023-10-24 21:55:22.718 
Epoch 653/1000 
	 loss: 27.8057, MinusLogProbMetric: 27.8057, val_loss: 28.1419, val_MinusLogProbMetric: 28.1419

Epoch 653: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8057 - MinusLogProbMetric: 27.8057 - val_loss: 28.1419 - val_MinusLogProbMetric: 28.1419 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 654/1000
2023-10-24 21:55:57.404 
Epoch 654/1000 
	 loss: 27.8015, MinusLogProbMetric: 27.8015, val_loss: 28.1826, val_MinusLogProbMetric: 28.1826

Epoch 654: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8015 - MinusLogProbMetric: 27.8015 - val_loss: 28.1826 - val_MinusLogProbMetric: 28.1826 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 655/1000
2023-10-24 21:56:32.244 
Epoch 655/1000 
	 loss: 27.8023, MinusLogProbMetric: 27.8023, val_loss: 28.1702, val_MinusLogProbMetric: 28.1702

Epoch 655: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8023 - MinusLogProbMetric: 27.8023 - val_loss: 28.1702 - val_MinusLogProbMetric: 28.1702 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 656/1000
2023-10-24 21:57:06.589 
Epoch 656/1000 
	 loss: 27.8018, MinusLogProbMetric: 27.8018, val_loss: 28.1276, val_MinusLogProbMetric: 28.1276

Epoch 656: val_loss did not improve from 28.10023
196/196 - 34s - loss: 27.8018 - MinusLogProbMetric: 27.8018 - val_loss: 28.1276 - val_MinusLogProbMetric: 28.1276 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 657/1000
2023-10-24 21:57:41.542 
Epoch 657/1000 
	 loss: 27.8027, MinusLogProbMetric: 27.8027, val_loss: 28.1874, val_MinusLogProbMetric: 28.1874

Epoch 657: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8027 - MinusLogProbMetric: 27.8027 - val_loss: 28.1874 - val_MinusLogProbMetric: 28.1874 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 658/1000
2023-10-24 21:58:16.234 
Epoch 658/1000 
	 loss: 27.8034, MinusLogProbMetric: 27.8034, val_loss: 28.1226, val_MinusLogProbMetric: 28.1226

Epoch 658: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8034 - MinusLogProbMetric: 27.8034 - val_loss: 28.1226 - val_MinusLogProbMetric: 28.1226 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 659/1000
2023-10-24 21:58:50.949 
Epoch 659/1000 
	 loss: 27.8035, MinusLogProbMetric: 27.8035, val_loss: 28.1397, val_MinusLogProbMetric: 28.1397

Epoch 659: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8035 - MinusLogProbMetric: 27.8035 - val_loss: 28.1397 - val_MinusLogProbMetric: 28.1397 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 660/1000
2023-10-24 21:59:25.371 
Epoch 660/1000 
	 loss: 27.8033, MinusLogProbMetric: 27.8033, val_loss: 28.1529, val_MinusLogProbMetric: 28.1529

Epoch 660: val_loss did not improve from 28.10023
196/196 - 34s - loss: 27.8033 - MinusLogProbMetric: 27.8033 - val_loss: 28.1529 - val_MinusLogProbMetric: 28.1529 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 661/1000
2023-10-24 22:00:00.020 
Epoch 661/1000 
	 loss: 27.8010, MinusLogProbMetric: 27.8010, val_loss: 28.1457, val_MinusLogProbMetric: 28.1457

Epoch 661: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8010 - MinusLogProbMetric: 27.8010 - val_loss: 28.1457 - val_MinusLogProbMetric: 28.1457 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 662/1000
2023-10-24 22:00:34.903 
Epoch 662/1000 
	 loss: 27.8007, MinusLogProbMetric: 27.8007, val_loss: 28.1630, val_MinusLogProbMetric: 28.1630

Epoch 662: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8007 - MinusLogProbMetric: 27.8007 - val_loss: 28.1630 - val_MinusLogProbMetric: 28.1630 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 663/1000
2023-10-24 22:01:09.620 
Epoch 663/1000 
	 loss: 27.8065, MinusLogProbMetric: 27.8065, val_loss: 28.1839, val_MinusLogProbMetric: 28.1839

Epoch 663: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8065 - MinusLogProbMetric: 27.8065 - val_loss: 28.1839 - val_MinusLogProbMetric: 28.1839 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 664/1000
2023-10-24 22:01:44.281 
Epoch 664/1000 
	 loss: 27.8106, MinusLogProbMetric: 27.8106, val_loss: 28.1441, val_MinusLogProbMetric: 28.1441

Epoch 664: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8106 - MinusLogProbMetric: 27.8106 - val_loss: 28.1441 - val_MinusLogProbMetric: 28.1441 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 665/1000
2023-10-24 22:02:18.856 
Epoch 665/1000 
	 loss: 27.7995, MinusLogProbMetric: 27.7995, val_loss: 28.1412, val_MinusLogProbMetric: 28.1412

Epoch 665: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.7995 - MinusLogProbMetric: 27.7995 - val_loss: 28.1412 - val_MinusLogProbMetric: 28.1412 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 666/1000
2023-10-24 22:02:53.406 
Epoch 666/1000 
	 loss: 27.7972, MinusLogProbMetric: 27.7972, val_loss: 28.1484, val_MinusLogProbMetric: 28.1484

Epoch 666: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.7972 - MinusLogProbMetric: 27.7972 - val_loss: 28.1484 - val_MinusLogProbMetric: 28.1484 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 667/1000
2023-10-24 22:03:28.144 
Epoch 667/1000 
	 loss: 27.7955, MinusLogProbMetric: 27.7955, val_loss: 28.1762, val_MinusLogProbMetric: 28.1762

Epoch 667: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.7955 - MinusLogProbMetric: 27.7955 - val_loss: 28.1762 - val_MinusLogProbMetric: 28.1762 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 668/1000
2023-10-24 22:04:02.587 
Epoch 668/1000 
	 loss: 27.8062, MinusLogProbMetric: 27.8062, val_loss: 28.1206, val_MinusLogProbMetric: 28.1206

Epoch 668: val_loss did not improve from 28.10023
196/196 - 34s - loss: 27.8062 - MinusLogProbMetric: 27.8062 - val_loss: 28.1206 - val_MinusLogProbMetric: 28.1206 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 669/1000
2023-10-24 22:04:36.968 
Epoch 669/1000 
	 loss: 27.8058, MinusLogProbMetric: 27.8058, val_loss: 28.1444, val_MinusLogProbMetric: 28.1444

Epoch 669: val_loss did not improve from 28.10023
196/196 - 34s - loss: 27.8058 - MinusLogProbMetric: 27.8058 - val_loss: 28.1444 - val_MinusLogProbMetric: 28.1444 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 670/1000
2023-10-24 22:05:11.661 
Epoch 670/1000 
	 loss: 27.8010, MinusLogProbMetric: 27.8010, val_loss: 28.1755, val_MinusLogProbMetric: 28.1755

Epoch 670: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8010 - MinusLogProbMetric: 27.8010 - val_loss: 28.1755 - val_MinusLogProbMetric: 28.1755 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 671/1000
2023-10-24 22:05:46.161 
Epoch 671/1000 
	 loss: 27.8021, MinusLogProbMetric: 27.8021, val_loss: 28.1519, val_MinusLogProbMetric: 28.1519

Epoch 671: val_loss did not improve from 28.10023
196/196 - 34s - loss: 27.8021 - MinusLogProbMetric: 27.8021 - val_loss: 28.1519 - val_MinusLogProbMetric: 28.1519 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 672/1000
2023-10-24 22:06:20.755 
Epoch 672/1000 
	 loss: 27.8024, MinusLogProbMetric: 27.8024, val_loss: 28.1334, val_MinusLogProbMetric: 28.1334

Epoch 672: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8024 - MinusLogProbMetric: 27.8024 - val_loss: 28.1334 - val_MinusLogProbMetric: 28.1334 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 673/1000
2023-10-24 22:06:55.289 
Epoch 673/1000 
	 loss: 27.8002, MinusLogProbMetric: 27.8002, val_loss: 28.1470, val_MinusLogProbMetric: 28.1470

Epoch 673: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8002 - MinusLogProbMetric: 27.8002 - val_loss: 28.1470 - val_MinusLogProbMetric: 28.1470 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 674/1000
2023-10-24 22:07:29.732 
Epoch 674/1000 
	 loss: 27.8086, MinusLogProbMetric: 27.8086, val_loss: 28.1531, val_MinusLogProbMetric: 28.1531

Epoch 674: val_loss did not improve from 28.10023
196/196 - 34s - loss: 27.8086 - MinusLogProbMetric: 27.8086 - val_loss: 28.1531 - val_MinusLogProbMetric: 28.1531 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 675/1000
2023-10-24 22:08:03.562 
Epoch 675/1000 
	 loss: 27.8044, MinusLogProbMetric: 27.8044, val_loss: 28.1453, val_MinusLogProbMetric: 28.1453

Epoch 675: val_loss did not improve from 28.10023
196/196 - 34s - loss: 27.8044 - MinusLogProbMetric: 27.8044 - val_loss: 28.1453 - val_MinusLogProbMetric: 28.1453 - lr: 1.2500e-04 - 34s/epoch - 173ms/step
Epoch 676/1000
2023-10-24 22:08:34.905 
Epoch 676/1000 
	 loss: 27.8007, MinusLogProbMetric: 27.8007, val_loss: 28.1280, val_MinusLogProbMetric: 28.1280

Epoch 676: val_loss did not improve from 28.10023
196/196 - 31s - loss: 27.8007 - MinusLogProbMetric: 27.8007 - val_loss: 28.1280 - val_MinusLogProbMetric: 28.1280 - lr: 1.2500e-04 - 31s/epoch - 160ms/step
Epoch 677/1000
2023-10-24 22:09:07.834 
Epoch 677/1000 
	 loss: 27.8037, MinusLogProbMetric: 27.8037, val_loss: 28.1149, val_MinusLogProbMetric: 28.1149

Epoch 677: val_loss did not improve from 28.10023
196/196 - 33s - loss: 27.8037 - MinusLogProbMetric: 27.8037 - val_loss: 28.1149 - val_MinusLogProbMetric: 28.1149 - lr: 1.2500e-04 - 33s/epoch - 168ms/step
Epoch 678/1000
2023-10-24 22:09:42.686 
Epoch 678/1000 
	 loss: 27.7986, MinusLogProbMetric: 27.7986, val_loss: 28.1300, val_MinusLogProbMetric: 28.1300

Epoch 678: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.7986 - MinusLogProbMetric: 27.7986 - val_loss: 28.1300 - val_MinusLogProbMetric: 28.1300 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 679/1000
2023-10-24 22:10:17.326 
Epoch 679/1000 
	 loss: 27.8061, MinusLogProbMetric: 27.8061, val_loss: 28.1044, val_MinusLogProbMetric: 28.1044

Epoch 679: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8061 - MinusLogProbMetric: 27.8061 - val_loss: 28.1044 - val_MinusLogProbMetric: 28.1044 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 680/1000
2023-10-24 22:10:51.092 
Epoch 680/1000 
	 loss: 27.7993, MinusLogProbMetric: 27.7993, val_loss: 28.1564, val_MinusLogProbMetric: 28.1564

Epoch 680: val_loss did not improve from 28.10023
196/196 - 34s - loss: 27.7993 - MinusLogProbMetric: 27.7993 - val_loss: 28.1564 - val_MinusLogProbMetric: 28.1564 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 681/1000
2023-10-24 22:11:26.146 
Epoch 681/1000 
	 loss: 27.8021, MinusLogProbMetric: 27.8021, val_loss: 28.1222, val_MinusLogProbMetric: 28.1222

Epoch 681: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8021 - MinusLogProbMetric: 27.8021 - val_loss: 28.1222 - val_MinusLogProbMetric: 28.1222 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 682/1000
2023-10-24 22:12:01.191 
Epoch 682/1000 
	 loss: 27.8004, MinusLogProbMetric: 27.8004, val_loss: 28.1051, val_MinusLogProbMetric: 28.1051

Epoch 682: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8004 - MinusLogProbMetric: 27.8004 - val_loss: 28.1051 - val_MinusLogProbMetric: 28.1051 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 683/1000
2023-10-24 22:12:35.834 
Epoch 683/1000 
	 loss: 27.8005, MinusLogProbMetric: 27.8005, val_loss: 28.1420, val_MinusLogProbMetric: 28.1420

Epoch 683: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8005 - MinusLogProbMetric: 27.8005 - val_loss: 28.1420 - val_MinusLogProbMetric: 28.1420 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 684/1000
2023-10-24 22:13:10.322 
Epoch 684/1000 
	 loss: 27.8020, MinusLogProbMetric: 27.8020, val_loss: 28.1964, val_MinusLogProbMetric: 28.1964

Epoch 684: val_loss did not improve from 28.10023
196/196 - 34s - loss: 27.8020 - MinusLogProbMetric: 27.8020 - val_loss: 28.1964 - val_MinusLogProbMetric: 28.1964 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 685/1000
2023-10-24 22:13:44.768 
Epoch 685/1000 
	 loss: 27.8034, MinusLogProbMetric: 27.8034, val_loss: 28.1785, val_MinusLogProbMetric: 28.1785

Epoch 685: val_loss did not improve from 28.10023
196/196 - 34s - loss: 27.8034 - MinusLogProbMetric: 27.8034 - val_loss: 28.1785 - val_MinusLogProbMetric: 28.1785 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 686/1000
2023-10-24 22:14:19.383 
Epoch 686/1000 
	 loss: 27.7978, MinusLogProbMetric: 27.7978, val_loss: 28.1079, val_MinusLogProbMetric: 28.1079

Epoch 686: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.7978 - MinusLogProbMetric: 27.7978 - val_loss: 28.1079 - val_MinusLogProbMetric: 28.1079 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 687/1000
2023-10-24 22:14:54.217 
Epoch 687/1000 
	 loss: 27.8065, MinusLogProbMetric: 27.8065, val_loss: 28.1416, val_MinusLogProbMetric: 28.1416

Epoch 687: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8065 - MinusLogProbMetric: 27.8065 - val_loss: 28.1416 - val_MinusLogProbMetric: 28.1416 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 688/1000
2023-10-24 22:15:28.795 
Epoch 688/1000 
	 loss: 27.8001, MinusLogProbMetric: 27.8001, val_loss: 28.1822, val_MinusLogProbMetric: 28.1822

Epoch 688: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.8001 - MinusLogProbMetric: 27.8001 - val_loss: 28.1822 - val_MinusLogProbMetric: 28.1822 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 689/1000
2023-10-24 22:16:03.235 
Epoch 689/1000 
	 loss: 27.8013, MinusLogProbMetric: 27.8013, val_loss: 28.1299, val_MinusLogProbMetric: 28.1299

Epoch 689: val_loss did not improve from 28.10023
196/196 - 34s - loss: 27.8013 - MinusLogProbMetric: 27.8013 - val_loss: 28.1299 - val_MinusLogProbMetric: 28.1299 - lr: 1.2500e-04 - 34s/epoch - 176ms/step
Epoch 690/1000
2023-10-24 22:16:38.046 
Epoch 690/1000 
	 loss: 27.7970, MinusLogProbMetric: 27.7970, val_loss: 28.1495, val_MinusLogProbMetric: 28.1495

Epoch 690: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.7970 - MinusLogProbMetric: 27.7970 - val_loss: 28.1495 - val_MinusLogProbMetric: 28.1495 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 691/1000
2023-10-24 22:17:12.942 
Epoch 691/1000 
	 loss: 27.7902, MinusLogProbMetric: 27.7902, val_loss: 28.1021, val_MinusLogProbMetric: 28.1021

Epoch 691: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.7902 - MinusLogProbMetric: 27.7902 - val_loss: 28.1021 - val_MinusLogProbMetric: 28.1021 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 692/1000
2023-10-24 22:17:47.286 
Epoch 692/1000 
	 loss: 27.7861, MinusLogProbMetric: 27.7861, val_loss: 28.1292, val_MinusLogProbMetric: 28.1292

Epoch 692: val_loss did not improve from 28.10023
196/196 - 34s - loss: 27.7861 - MinusLogProbMetric: 27.7861 - val_loss: 28.1292 - val_MinusLogProbMetric: 28.1292 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 693/1000
2023-10-24 22:18:21.941 
Epoch 693/1000 
	 loss: 27.7970, MinusLogProbMetric: 27.7970, val_loss: 28.1233, val_MinusLogProbMetric: 28.1233

Epoch 693: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.7970 - MinusLogProbMetric: 27.7970 - val_loss: 28.1233 - val_MinusLogProbMetric: 28.1233 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 694/1000
2023-10-24 22:18:56.704 
Epoch 694/1000 
	 loss: 27.7892, MinusLogProbMetric: 27.7892, val_loss: 28.1154, val_MinusLogProbMetric: 28.1154

Epoch 694: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.7892 - MinusLogProbMetric: 27.7892 - val_loss: 28.1154 - val_MinusLogProbMetric: 28.1154 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 695/1000
2023-10-24 22:19:31.355 
Epoch 695/1000 
	 loss: 27.7892, MinusLogProbMetric: 27.7892, val_loss: 28.1348, val_MinusLogProbMetric: 28.1348

Epoch 695: val_loss did not improve from 28.10023
196/196 - 35s - loss: 27.7892 - MinusLogProbMetric: 27.7892 - val_loss: 28.1348 - val_MinusLogProbMetric: 28.1348 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 696/1000
2023-10-24 22:20:06.173 
Epoch 696/1000 
	 loss: 27.7561, MinusLogProbMetric: 27.7561, val_loss: 28.0961, val_MinusLogProbMetric: 28.0961

Epoch 696: val_loss improved from 28.10023 to 28.09608, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.7561 - MinusLogProbMetric: 27.7561 - val_loss: 28.0961 - val_MinusLogProbMetric: 28.0961 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 697/1000
2023-10-24 22:20:41.073 
Epoch 697/1000 
	 loss: 27.7544, MinusLogProbMetric: 27.7544, val_loss: 28.0834, val_MinusLogProbMetric: 28.0834

Epoch 697: val_loss improved from 28.09608 to 28.08337, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.7544 - MinusLogProbMetric: 27.7544 - val_loss: 28.0834 - val_MinusLogProbMetric: 28.0834 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 698/1000
2023-10-24 22:21:16.254 
Epoch 698/1000 
	 loss: 27.7532, MinusLogProbMetric: 27.7532, val_loss: 28.0978, val_MinusLogProbMetric: 28.0978

Epoch 698: val_loss did not improve from 28.08337
196/196 - 35s - loss: 27.7532 - MinusLogProbMetric: 27.7532 - val_loss: 28.0978 - val_MinusLogProbMetric: 28.0978 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 699/1000
2023-10-24 22:21:51.230 
Epoch 699/1000 
	 loss: 27.7545, MinusLogProbMetric: 27.7545, val_loss: 28.0863, val_MinusLogProbMetric: 28.0863

Epoch 699: val_loss did not improve from 28.08337
196/196 - 35s - loss: 27.7545 - MinusLogProbMetric: 27.7545 - val_loss: 28.0863 - val_MinusLogProbMetric: 28.0863 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 700/1000
2023-10-24 22:22:26.210 
Epoch 700/1000 
	 loss: 27.7555, MinusLogProbMetric: 27.7555, val_loss: 28.0910, val_MinusLogProbMetric: 28.0910

Epoch 700: val_loss did not improve from 28.08337
196/196 - 35s - loss: 27.7555 - MinusLogProbMetric: 27.7555 - val_loss: 28.0910 - val_MinusLogProbMetric: 28.0910 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 701/1000
2023-10-24 22:23:00.536 
Epoch 701/1000 
	 loss: 27.7554, MinusLogProbMetric: 27.7554, val_loss: 28.0823, val_MinusLogProbMetric: 28.0823

Epoch 701: val_loss improved from 28.08337 to 28.08234, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.7554 - MinusLogProbMetric: 27.7554 - val_loss: 28.0823 - val_MinusLogProbMetric: 28.0823 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 702/1000
2023-10-24 22:23:35.781 
Epoch 702/1000 
	 loss: 27.7534, MinusLogProbMetric: 27.7534, val_loss: 28.1068, val_MinusLogProbMetric: 28.1068

Epoch 702: val_loss did not improve from 28.08234
196/196 - 35s - loss: 27.7534 - MinusLogProbMetric: 27.7534 - val_loss: 28.1068 - val_MinusLogProbMetric: 28.1068 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 703/1000
2023-10-24 22:24:09.785 
Epoch 703/1000 
	 loss: 27.7537, MinusLogProbMetric: 27.7537, val_loss: 28.0737, val_MinusLogProbMetric: 28.0737

Epoch 703: val_loss improved from 28.08234 to 28.07373, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 34s - loss: 27.7537 - MinusLogProbMetric: 27.7537 - val_loss: 28.0737 - val_MinusLogProbMetric: 28.0737 - lr: 6.2500e-05 - 34s/epoch - 176ms/step
Epoch 704/1000
2023-10-24 22:24:44.757 
Epoch 704/1000 
	 loss: 27.7541, MinusLogProbMetric: 27.7541, val_loss: 28.0986, val_MinusLogProbMetric: 28.0986

Epoch 704: val_loss did not improve from 28.07373
196/196 - 35s - loss: 27.7541 - MinusLogProbMetric: 27.7541 - val_loss: 28.0986 - val_MinusLogProbMetric: 28.0986 - lr: 6.2500e-05 - 35s/epoch - 176ms/step
Epoch 705/1000
2023-10-24 22:25:19.133 
Epoch 705/1000 
	 loss: 27.7553, MinusLogProbMetric: 27.7553, val_loss: 28.0924, val_MinusLogProbMetric: 28.0924

Epoch 705: val_loss did not improve from 28.07373
196/196 - 34s - loss: 27.7553 - MinusLogProbMetric: 27.7553 - val_loss: 28.0924 - val_MinusLogProbMetric: 28.0924 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 706/1000
2023-10-24 22:25:53.934 
Epoch 706/1000 
	 loss: 27.7554, MinusLogProbMetric: 27.7554, val_loss: 28.0963, val_MinusLogProbMetric: 28.0963

Epoch 706: val_loss did not improve from 28.07373
196/196 - 35s - loss: 27.7554 - MinusLogProbMetric: 27.7554 - val_loss: 28.0963 - val_MinusLogProbMetric: 28.0963 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 707/1000
2023-10-24 22:26:28.493 
Epoch 707/1000 
	 loss: 27.7557, MinusLogProbMetric: 27.7557, val_loss: 28.0943, val_MinusLogProbMetric: 28.0943

Epoch 707: val_loss did not improve from 28.07373
196/196 - 35s - loss: 27.7557 - MinusLogProbMetric: 27.7557 - val_loss: 28.0943 - val_MinusLogProbMetric: 28.0943 - lr: 6.2500e-05 - 35s/epoch - 176ms/step
Epoch 708/1000
2023-10-24 22:27:02.667 
Epoch 708/1000 
	 loss: 27.7535, MinusLogProbMetric: 27.7535, val_loss: 28.0821, val_MinusLogProbMetric: 28.0821

Epoch 708: val_loss did not improve from 28.07373
196/196 - 34s - loss: 27.7535 - MinusLogProbMetric: 27.7535 - val_loss: 28.0821 - val_MinusLogProbMetric: 28.0821 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 709/1000
2023-10-24 22:27:37.434 
Epoch 709/1000 
	 loss: 27.7548, MinusLogProbMetric: 27.7548, val_loss: 28.1068, val_MinusLogProbMetric: 28.1068

Epoch 709: val_loss did not improve from 28.07373
196/196 - 35s - loss: 27.7548 - MinusLogProbMetric: 27.7548 - val_loss: 28.1068 - val_MinusLogProbMetric: 28.1068 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 710/1000
2023-10-24 22:28:11.773 
Epoch 710/1000 
	 loss: 27.7548, MinusLogProbMetric: 27.7548, val_loss: 28.0978, val_MinusLogProbMetric: 28.0978

Epoch 710: val_loss did not improve from 28.07373
196/196 - 34s - loss: 27.7548 - MinusLogProbMetric: 27.7548 - val_loss: 28.0978 - val_MinusLogProbMetric: 28.0978 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 711/1000
2023-10-24 22:28:44.567 
Epoch 711/1000 
	 loss: 27.7539, MinusLogProbMetric: 27.7539, val_loss: 28.0932, val_MinusLogProbMetric: 28.0932

Epoch 711: val_loss did not improve from 28.07373
196/196 - 33s - loss: 27.7539 - MinusLogProbMetric: 27.7539 - val_loss: 28.0932 - val_MinusLogProbMetric: 28.0932 - lr: 6.2500e-05 - 33s/epoch - 167ms/step
Epoch 712/1000
2023-10-24 22:29:19.086 
Epoch 712/1000 
	 loss: 27.7534, MinusLogProbMetric: 27.7534, val_loss: 28.0994, val_MinusLogProbMetric: 28.0994

Epoch 712: val_loss did not improve from 28.07373
196/196 - 35s - loss: 27.7534 - MinusLogProbMetric: 27.7534 - val_loss: 28.0994 - val_MinusLogProbMetric: 28.0994 - lr: 6.2500e-05 - 35s/epoch - 176ms/step
Epoch 713/1000
2023-10-24 22:29:53.529 
Epoch 713/1000 
	 loss: 27.7536, MinusLogProbMetric: 27.7536, val_loss: 28.0857, val_MinusLogProbMetric: 28.0857

Epoch 713: val_loss did not improve from 28.07373
196/196 - 34s - loss: 27.7536 - MinusLogProbMetric: 27.7536 - val_loss: 28.0857 - val_MinusLogProbMetric: 28.0857 - lr: 6.2500e-05 - 34s/epoch - 176ms/step
Epoch 714/1000
2023-10-24 22:30:27.825 
Epoch 714/1000 
	 loss: 27.7570, MinusLogProbMetric: 27.7570, val_loss: 28.0816, val_MinusLogProbMetric: 28.0816

Epoch 714: val_loss did not improve from 28.07373
196/196 - 34s - loss: 27.7570 - MinusLogProbMetric: 27.7570 - val_loss: 28.0816 - val_MinusLogProbMetric: 28.0816 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 715/1000
2023-10-24 22:31:02.462 
Epoch 715/1000 
	 loss: 27.7536, MinusLogProbMetric: 27.7536, val_loss: 28.0873, val_MinusLogProbMetric: 28.0873

Epoch 715: val_loss did not improve from 28.07373
196/196 - 35s - loss: 27.7536 - MinusLogProbMetric: 27.7536 - val_loss: 28.0873 - val_MinusLogProbMetric: 28.0873 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 716/1000
2023-10-24 22:31:37.320 
Epoch 716/1000 
	 loss: 27.7558, MinusLogProbMetric: 27.7558, val_loss: 28.0997, val_MinusLogProbMetric: 28.0997

Epoch 716: val_loss did not improve from 28.07373
196/196 - 35s - loss: 27.7558 - MinusLogProbMetric: 27.7558 - val_loss: 28.0997 - val_MinusLogProbMetric: 28.0997 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 717/1000
2023-10-24 22:32:11.996 
Epoch 717/1000 
	 loss: 27.7524, MinusLogProbMetric: 27.7524, val_loss: 28.0995, val_MinusLogProbMetric: 28.0995

Epoch 717: val_loss did not improve from 28.07373
196/196 - 35s - loss: 27.7524 - MinusLogProbMetric: 27.7524 - val_loss: 28.0995 - val_MinusLogProbMetric: 28.0995 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 718/1000
2023-10-24 22:32:45.903 
Epoch 718/1000 
	 loss: 27.7522, MinusLogProbMetric: 27.7522, val_loss: 28.0920, val_MinusLogProbMetric: 28.0920

Epoch 718: val_loss did not improve from 28.07373
196/196 - 34s - loss: 27.7522 - MinusLogProbMetric: 27.7522 - val_loss: 28.0920 - val_MinusLogProbMetric: 28.0920 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 719/1000
2023-10-24 22:33:20.449 
Epoch 719/1000 
	 loss: 27.7533, MinusLogProbMetric: 27.7533, val_loss: 28.0779, val_MinusLogProbMetric: 28.0779

Epoch 719: val_loss did not improve from 28.07373
196/196 - 35s - loss: 27.7533 - MinusLogProbMetric: 27.7533 - val_loss: 28.0779 - val_MinusLogProbMetric: 28.0779 - lr: 6.2500e-05 - 35s/epoch - 176ms/step
Epoch 720/1000
2023-10-24 22:33:54.762 
Epoch 720/1000 
	 loss: 27.7545, MinusLogProbMetric: 27.7545, val_loss: 28.1057, val_MinusLogProbMetric: 28.1057

Epoch 720: val_loss did not improve from 28.07373
196/196 - 34s - loss: 27.7545 - MinusLogProbMetric: 27.7545 - val_loss: 28.1057 - val_MinusLogProbMetric: 28.1057 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 721/1000
2023-10-24 22:34:29.058 
Epoch 721/1000 
	 loss: 27.7570, MinusLogProbMetric: 27.7570, val_loss: 28.0849, val_MinusLogProbMetric: 28.0849

Epoch 721: val_loss did not improve from 28.07373
196/196 - 34s - loss: 27.7570 - MinusLogProbMetric: 27.7570 - val_loss: 28.0849 - val_MinusLogProbMetric: 28.0849 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 722/1000
2023-10-24 22:35:04.090 
Epoch 722/1000 
	 loss: 27.7520, MinusLogProbMetric: 27.7520, val_loss: 28.0993, val_MinusLogProbMetric: 28.0993

Epoch 722: val_loss did not improve from 28.07373
196/196 - 35s - loss: 27.7520 - MinusLogProbMetric: 27.7520 - val_loss: 28.0993 - val_MinusLogProbMetric: 28.0993 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 723/1000
2023-10-24 22:35:38.712 
Epoch 723/1000 
	 loss: 27.7535, MinusLogProbMetric: 27.7535, val_loss: 28.0747, val_MinusLogProbMetric: 28.0747

Epoch 723: val_loss did not improve from 28.07373
196/196 - 35s - loss: 27.7535 - MinusLogProbMetric: 27.7535 - val_loss: 28.0747 - val_MinusLogProbMetric: 28.0747 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 724/1000
2023-10-24 22:36:13.339 
Epoch 724/1000 
	 loss: 27.7555, MinusLogProbMetric: 27.7555, val_loss: 28.1193, val_MinusLogProbMetric: 28.1193

Epoch 724: val_loss did not improve from 28.07373
196/196 - 35s - loss: 27.7555 - MinusLogProbMetric: 27.7555 - val_loss: 28.1193 - val_MinusLogProbMetric: 28.1193 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 725/1000
2023-10-24 22:36:47.760 
Epoch 725/1000 
	 loss: 27.7524, MinusLogProbMetric: 27.7524, val_loss: 28.0839, val_MinusLogProbMetric: 28.0839

Epoch 725: val_loss did not improve from 28.07373
196/196 - 34s - loss: 27.7524 - MinusLogProbMetric: 27.7524 - val_loss: 28.0839 - val_MinusLogProbMetric: 28.0839 - lr: 6.2500e-05 - 34s/epoch - 176ms/step
Epoch 726/1000
2023-10-24 22:37:22.354 
Epoch 726/1000 
	 loss: 27.7550, MinusLogProbMetric: 27.7550, val_loss: 28.1080, val_MinusLogProbMetric: 28.1080

Epoch 726: val_loss did not improve from 28.07373
196/196 - 35s - loss: 27.7550 - MinusLogProbMetric: 27.7550 - val_loss: 28.1080 - val_MinusLogProbMetric: 28.1080 - lr: 6.2500e-05 - 35s/epoch - 176ms/step
Epoch 727/1000
2023-10-24 22:37:56.152 
Epoch 727/1000 
	 loss: 27.7525, MinusLogProbMetric: 27.7525, val_loss: 28.0919, val_MinusLogProbMetric: 28.0919

Epoch 727: val_loss did not improve from 28.07373
196/196 - 34s - loss: 27.7525 - MinusLogProbMetric: 27.7525 - val_loss: 28.0919 - val_MinusLogProbMetric: 28.0919 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 728/1000
2023-10-24 22:38:30.343 
Epoch 728/1000 
	 loss: 27.7526, MinusLogProbMetric: 27.7526, val_loss: 28.0880, val_MinusLogProbMetric: 28.0880

Epoch 728: val_loss did not improve from 28.07373
196/196 - 34s - loss: 27.7526 - MinusLogProbMetric: 27.7526 - val_loss: 28.0880 - val_MinusLogProbMetric: 28.0880 - lr: 6.2500e-05 - 34s/epoch - 174ms/step
Epoch 729/1000
2023-10-24 22:39:04.762 
Epoch 729/1000 
	 loss: 27.7557, MinusLogProbMetric: 27.7557, val_loss: 28.0747, val_MinusLogProbMetric: 28.0747

Epoch 729: val_loss did not improve from 28.07373
196/196 - 34s - loss: 27.7557 - MinusLogProbMetric: 27.7557 - val_loss: 28.0747 - val_MinusLogProbMetric: 28.0747 - lr: 6.2500e-05 - 34s/epoch - 176ms/step
Epoch 730/1000
2023-10-24 22:39:38.250 
Epoch 730/1000 
	 loss: 27.7567, MinusLogProbMetric: 27.7567, val_loss: 28.0920, val_MinusLogProbMetric: 28.0920

Epoch 730: val_loss did not improve from 28.07373
196/196 - 33s - loss: 27.7567 - MinusLogProbMetric: 27.7567 - val_loss: 28.0920 - val_MinusLogProbMetric: 28.0920 - lr: 6.2500e-05 - 33s/epoch - 171ms/step
Epoch 731/1000
2023-10-24 22:40:12.956 
Epoch 731/1000 
	 loss: 27.7499, MinusLogProbMetric: 27.7499, val_loss: 28.0863, val_MinusLogProbMetric: 28.0863

Epoch 731: val_loss did not improve from 28.07373
196/196 - 35s - loss: 27.7499 - MinusLogProbMetric: 27.7499 - val_loss: 28.0863 - val_MinusLogProbMetric: 28.0863 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 732/1000
2023-10-24 22:40:47.473 
Epoch 732/1000 
	 loss: 27.7523, MinusLogProbMetric: 27.7523, val_loss: 28.0771, val_MinusLogProbMetric: 28.0771

Epoch 732: val_loss did not improve from 28.07373
196/196 - 35s - loss: 27.7523 - MinusLogProbMetric: 27.7523 - val_loss: 28.0771 - val_MinusLogProbMetric: 28.0771 - lr: 6.2500e-05 - 35s/epoch - 176ms/step
Epoch 733/1000
2023-10-24 22:41:21.374 
Epoch 733/1000 
	 loss: 27.7579, MinusLogProbMetric: 27.7579, val_loss: 28.0930, val_MinusLogProbMetric: 28.0930

Epoch 733: val_loss did not improve from 28.07373
196/196 - 34s - loss: 27.7579 - MinusLogProbMetric: 27.7579 - val_loss: 28.0930 - val_MinusLogProbMetric: 28.0930 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 734/1000
2023-10-24 22:41:56.203 
Epoch 734/1000 
	 loss: 27.7520, MinusLogProbMetric: 27.7520, val_loss: 28.1003, val_MinusLogProbMetric: 28.1003

Epoch 734: val_loss did not improve from 28.07373
196/196 - 35s - loss: 27.7520 - MinusLogProbMetric: 27.7520 - val_loss: 28.1003 - val_MinusLogProbMetric: 28.1003 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 735/1000
2023-10-24 22:42:30.545 
Epoch 735/1000 
	 loss: 27.7526, MinusLogProbMetric: 27.7526, val_loss: 28.0973, val_MinusLogProbMetric: 28.0973

Epoch 735: val_loss did not improve from 28.07373
196/196 - 34s - loss: 27.7526 - MinusLogProbMetric: 27.7526 - val_loss: 28.0973 - val_MinusLogProbMetric: 28.0973 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 736/1000
2023-10-24 22:43:04.950 
Epoch 736/1000 
	 loss: 27.7545, MinusLogProbMetric: 27.7545, val_loss: 28.0994, val_MinusLogProbMetric: 28.0994

Epoch 736: val_loss did not improve from 28.07373
196/196 - 34s - loss: 27.7545 - MinusLogProbMetric: 27.7545 - val_loss: 28.0994 - val_MinusLogProbMetric: 28.0994 - lr: 6.2500e-05 - 34s/epoch - 176ms/step
Epoch 737/1000
2023-10-24 22:43:38.880 
Epoch 737/1000 
	 loss: 27.7537, MinusLogProbMetric: 27.7537, val_loss: 28.0956, val_MinusLogProbMetric: 28.0956

Epoch 737: val_loss did not improve from 28.07373
196/196 - 34s - loss: 27.7537 - MinusLogProbMetric: 27.7537 - val_loss: 28.0956 - val_MinusLogProbMetric: 28.0956 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 738/1000
2023-10-24 22:44:11.123 
Epoch 738/1000 
	 loss: 27.7531, MinusLogProbMetric: 27.7531, val_loss: 28.0804, val_MinusLogProbMetric: 28.0804

Epoch 738: val_loss did not improve from 28.07373
196/196 - 32s - loss: 27.7531 - MinusLogProbMetric: 27.7531 - val_loss: 28.0804 - val_MinusLogProbMetric: 28.0804 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 739/1000
2023-10-24 22:44:45.664 
Epoch 739/1000 
	 loss: 27.7494, MinusLogProbMetric: 27.7494, val_loss: 28.0871, val_MinusLogProbMetric: 28.0871

Epoch 739: val_loss did not improve from 28.07373
196/196 - 35s - loss: 27.7494 - MinusLogProbMetric: 27.7494 - val_loss: 28.0871 - val_MinusLogProbMetric: 28.0871 - lr: 6.2500e-05 - 35s/epoch - 176ms/step
Epoch 740/1000
2023-10-24 22:45:17.540 
Epoch 740/1000 
	 loss: 27.7536, MinusLogProbMetric: 27.7536, val_loss: 28.0968, val_MinusLogProbMetric: 28.0968

Epoch 740: val_loss did not improve from 28.07373
196/196 - 32s - loss: 27.7536 - MinusLogProbMetric: 27.7536 - val_loss: 28.0968 - val_MinusLogProbMetric: 28.0968 - lr: 6.2500e-05 - 32s/epoch - 163ms/step
Epoch 741/1000
2023-10-24 22:45:51.353 
Epoch 741/1000 
	 loss: 27.7499, MinusLogProbMetric: 27.7499, val_loss: 28.0815, val_MinusLogProbMetric: 28.0815

Epoch 741: val_loss did not improve from 28.07373
196/196 - 34s - loss: 27.7499 - MinusLogProbMetric: 27.7499 - val_loss: 28.0815 - val_MinusLogProbMetric: 28.0815 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 742/1000
2023-10-24 22:46:23.417 
Epoch 742/1000 
	 loss: 27.7530, MinusLogProbMetric: 27.7530, val_loss: 28.0679, val_MinusLogProbMetric: 28.0679

Epoch 742: val_loss improved from 28.07373 to 28.06794, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 33s - loss: 27.7530 - MinusLogProbMetric: 27.7530 - val_loss: 28.0679 - val_MinusLogProbMetric: 28.0679 - lr: 6.2500e-05 - 33s/epoch - 166ms/step
Epoch 743/1000
2023-10-24 22:46:58.107 
Epoch 743/1000 
	 loss: 27.7529, MinusLogProbMetric: 27.7529, val_loss: 28.0819, val_MinusLogProbMetric: 28.0819

Epoch 743: val_loss did not improve from 28.06794
196/196 - 34s - loss: 27.7529 - MinusLogProbMetric: 27.7529 - val_loss: 28.0819 - val_MinusLogProbMetric: 28.0819 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 744/1000
2023-10-24 22:47:31.166 
Epoch 744/1000 
	 loss: 27.7534, MinusLogProbMetric: 27.7534, val_loss: 28.0842, val_MinusLogProbMetric: 28.0842

Epoch 744: val_loss did not improve from 28.06794
196/196 - 33s - loss: 27.7534 - MinusLogProbMetric: 27.7534 - val_loss: 28.0842 - val_MinusLogProbMetric: 28.0842 - lr: 6.2500e-05 - 33s/epoch - 169ms/step
Epoch 745/1000
2023-10-24 22:48:02.577 
Epoch 745/1000 
	 loss: 27.7538, MinusLogProbMetric: 27.7538, val_loss: 28.1066, val_MinusLogProbMetric: 28.1066

Epoch 745: val_loss did not improve from 28.06794
196/196 - 31s - loss: 27.7538 - MinusLogProbMetric: 27.7538 - val_loss: 28.1066 - val_MinusLogProbMetric: 28.1066 - lr: 6.2500e-05 - 31s/epoch - 160ms/step
Epoch 746/1000
2023-10-24 22:48:36.379 
Epoch 746/1000 
	 loss: 27.7495, MinusLogProbMetric: 27.7495, val_loss: 28.0925, val_MinusLogProbMetric: 28.0925

Epoch 746: val_loss did not improve from 28.06794
196/196 - 34s - loss: 27.7495 - MinusLogProbMetric: 27.7495 - val_loss: 28.0925 - val_MinusLogProbMetric: 28.0925 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 747/1000
2023-10-24 22:49:11.070 
Epoch 747/1000 
	 loss: 27.7518, MinusLogProbMetric: 27.7518, val_loss: 28.0931, val_MinusLogProbMetric: 28.0931

Epoch 747: val_loss did not improve from 28.06794
196/196 - 35s - loss: 27.7518 - MinusLogProbMetric: 27.7518 - val_loss: 28.0931 - val_MinusLogProbMetric: 28.0931 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 748/1000
2023-10-24 22:49:43.431 
Epoch 748/1000 
	 loss: 27.7506, MinusLogProbMetric: 27.7506, val_loss: 28.0960, val_MinusLogProbMetric: 28.0960

Epoch 748: val_loss did not improve from 28.06794
196/196 - 32s - loss: 27.7506 - MinusLogProbMetric: 27.7506 - val_loss: 28.0960 - val_MinusLogProbMetric: 28.0960 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 749/1000
2023-10-24 22:50:18.037 
Epoch 749/1000 
	 loss: 27.7547, MinusLogProbMetric: 27.7547, val_loss: 28.0922, val_MinusLogProbMetric: 28.0922

Epoch 749: val_loss did not improve from 28.06794
196/196 - 35s - loss: 27.7547 - MinusLogProbMetric: 27.7547 - val_loss: 28.0922 - val_MinusLogProbMetric: 28.0922 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 750/1000
2023-10-24 22:50:52.702 
Epoch 750/1000 
	 loss: 27.7483, MinusLogProbMetric: 27.7483, val_loss: 28.0953, val_MinusLogProbMetric: 28.0953

Epoch 750: val_loss did not improve from 28.06794
196/196 - 35s - loss: 27.7483 - MinusLogProbMetric: 27.7483 - val_loss: 28.0953 - val_MinusLogProbMetric: 28.0953 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 751/1000
2023-10-24 22:51:27.389 
Epoch 751/1000 
	 loss: 27.7514, MinusLogProbMetric: 27.7514, val_loss: 28.0744, val_MinusLogProbMetric: 28.0744

Epoch 751: val_loss did not improve from 28.06794
196/196 - 35s - loss: 27.7514 - MinusLogProbMetric: 27.7514 - val_loss: 28.0744 - val_MinusLogProbMetric: 28.0744 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 752/1000
2023-10-24 22:52:00.879 
Epoch 752/1000 
	 loss: 27.7525, MinusLogProbMetric: 27.7525, val_loss: 28.0852, val_MinusLogProbMetric: 28.0852

Epoch 752: val_loss did not improve from 28.06794
196/196 - 33s - loss: 27.7525 - MinusLogProbMetric: 27.7525 - val_loss: 28.0852 - val_MinusLogProbMetric: 28.0852 - lr: 6.2500e-05 - 33s/epoch - 171ms/step
Epoch 753/1000
2023-10-24 22:52:32.353 
Epoch 753/1000 
	 loss: 27.7493, MinusLogProbMetric: 27.7493, val_loss: 28.1048, val_MinusLogProbMetric: 28.1048

Epoch 753: val_loss did not improve from 28.06794
196/196 - 31s - loss: 27.7493 - MinusLogProbMetric: 27.7493 - val_loss: 28.1048 - val_MinusLogProbMetric: 28.1048 - lr: 6.2500e-05 - 31s/epoch - 161ms/step
Epoch 754/1000
2023-10-24 22:53:03.850 
Epoch 754/1000 
	 loss: 27.7496, MinusLogProbMetric: 27.7496, val_loss: 28.0885, val_MinusLogProbMetric: 28.0885

Epoch 754: val_loss did not improve from 28.06794
196/196 - 31s - loss: 27.7496 - MinusLogProbMetric: 27.7496 - val_loss: 28.0885 - val_MinusLogProbMetric: 28.0885 - lr: 6.2500e-05 - 31s/epoch - 161ms/step
Epoch 755/1000
2023-10-24 22:53:37.507 
Epoch 755/1000 
	 loss: 27.7481, MinusLogProbMetric: 27.7481, val_loss: 28.0888, val_MinusLogProbMetric: 28.0888

Epoch 755: val_loss did not improve from 28.06794
196/196 - 34s - loss: 27.7481 - MinusLogProbMetric: 27.7481 - val_loss: 28.0888 - val_MinusLogProbMetric: 28.0888 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 756/1000
2023-10-24 22:54:12.509 
Epoch 756/1000 
	 loss: 27.7512, MinusLogProbMetric: 27.7512, val_loss: 28.0783, val_MinusLogProbMetric: 28.0783

Epoch 756: val_loss did not improve from 28.06794
196/196 - 35s - loss: 27.7512 - MinusLogProbMetric: 27.7512 - val_loss: 28.0783 - val_MinusLogProbMetric: 28.0783 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 757/1000
2023-10-24 22:54:46.273 
Epoch 757/1000 
	 loss: 27.7520, MinusLogProbMetric: 27.7520, val_loss: 28.0874, val_MinusLogProbMetric: 28.0874

Epoch 757: val_loss did not improve from 28.06794
196/196 - 34s - loss: 27.7520 - MinusLogProbMetric: 27.7520 - val_loss: 28.0874 - val_MinusLogProbMetric: 28.0874 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 758/1000
2023-10-24 22:55:20.066 
Epoch 758/1000 
	 loss: 27.7507, MinusLogProbMetric: 27.7507, val_loss: 28.0973, val_MinusLogProbMetric: 28.0973

Epoch 758: val_loss did not improve from 28.06794
196/196 - 34s - loss: 27.7507 - MinusLogProbMetric: 27.7507 - val_loss: 28.0973 - val_MinusLogProbMetric: 28.0973 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 759/1000
2023-10-24 22:55:53.669 
Epoch 759/1000 
	 loss: 27.7468, MinusLogProbMetric: 27.7468, val_loss: 28.0952, val_MinusLogProbMetric: 28.0952

Epoch 759: val_loss did not improve from 28.06794
196/196 - 34s - loss: 27.7468 - MinusLogProbMetric: 27.7468 - val_loss: 28.0952 - val_MinusLogProbMetric: 28.0952 - lr: 6.2500e-05 - 34s/epoch - 171ms/step
Epoch 760/1000
2023-10-24 22:56:26.956 
Epoch 760/1000 
	 loss: 27.7508, MinusLogProbMetric: 27.7508, val_loss: 28.0936, val_MinusLogProbMetric: 28.0936

Epoch 760: val_loss did not improve from 28.06794
196/196 - 33s - loss: 27.7508 - MinusLogProbMetric: 27.7508 - val_loss: 28.0936 - val_MinusLogProbMetric: 28.0936 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 761/1000
2023-10-24 22:57:01.221 
Epoch 761/1000 
	 loss: 27.7483, MinusLogProbMetric: 27.7483, val_loss: 28.0942, val_MinusLogProbMetric: 28.0942

Epoch 761: val_loss did not improve from 28.06794
196/196 - 34s - loss: 27.7483 - MinusLogProbMetric: 27.7483 - val_loss: 28.0942 - val_MinusLogProbMetric: 28.0942 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 762/1000
2023-10-24 22:57:32.691 
Epoch 762/1000 
	 loss: 27.7500, MinusLogProbMetric: 27.7500, val_loss: 28.1029, val_MinusLogProbMetric: 28.1029

Epoch 762: val_loss did not improve from 28.06794
196/196 - 31s - loss: 27.7500 - MinusLogProbMetric: 27.7500 - val_loss: 28.1029 - val_MinusLogProbMetric: 28.1029 - lr: 6.2500e-05 - 31s/epoch - 161ms/step
Epoch 763/1000
2023-10-24 22:58:05.984 
Epoch 763/1000 
	 loss: 27.7525, MinusLogProbMetric: 27.7525, val_loss: 28.1607, val_MinusLogProbMetric: 28.1607

Epoch 763: val_loss did not improve from 28.06794
196/196 - 33s - loss: 27.7525 - MinusLogProbMetric: 27.7525 - val_loss: 28.1607 - val_MinusLogProbMetric: 28.1607 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 764/1000
2023-10-24 22:58:38.042 
Epoch 764/1000 
	 loss: 27.7489, MinusLogProbMetric: 27.7489, val_loss: 28.0935, val_MinusLogProbMetric: 28.0935

Epoch 764: val_loss did not improve from 28.06794
196/196 - 32s - loss: 27.7489 - MinusLogProbMetric: 27.7489 - val_loss: 28.0935 - val_MinusLogProbMetric: 28.0935 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 765/1000
2023-10-24 22:59:11.641 
Epoch 765/1000 
	 loss: 27.7491, MinusLogProbMetric: 27.7491, val_loss: 28.0879, val_MinusLogProbMetric: 28.0879

Epoch 765: val_loss did not improve from 28.06794
196/196 - 34s - loss: 27.7491 - MinusLogProbMetric: 27.7491 - val_loss: 28.0879 - val_MinusLogProbMetric: 28.0879 - lr: 6.2500e-05 - 34s/epoch - 171ms/step
Epoch 766/1000
2023-10-24 22:59:43.622 
Epoch 766/1000 
	 loss: 27.7509, MinusLogProbMetric: 27.7509, val_loss: 28.0875, val_MinusLogProbMetric: 28.0875

Epoch 766: val_loss did not improve from 28.06794
196/196 - 32s - loss: 27.7509 - MinusLogProbMetric: 27.7509 - val_loss: 28.0875 - val_MinusLogProbMetric: 28.0875 - lr: 6.2500e-05 - 32s/epoch - 163ms/step
Epoch 767/1000
2023-10-24 23:00:18.242 
Epoch 767/1000 
	 loss: 27.7479, MinusLogProbMetric: 27.7479, val_loss: 28.0877, val_MinusLogProbMetric: 28.0877

Epoch 767: val_loss did not improve from 28.06794
196/196 - 35s - loss: 27.7479 - MinusLogProbMetric: 27.7479 - val_loss: 28.0877 - val_MinusLogProbMetric: 28.0877 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 768/1000
2023-10-24 23:00:49.974 
Epoch 768/1000 
	 loss: 27.7490, MinusLogProbMetric: 27.7490, val_loss: 28.0824, val_MinusLogProbMetric: 28.0824

Epoch 768: val_loss did not improve from 28.06794
196/196 - 32s - loss: 27.7490 - MinusLogProbMetric: 27.7490 - val_loss: 28.0824 - val_MinusLogProbMetric: 28.0824 - lr: 6.2500e-05 - 32s/epoch - 162ms/step
Epoch 769/1000
2023-10-24 23:01:24.554 
Epoch 769/1000 
	 loss: 27.7468, MinusLogProbMetric: 27.7468, val_loss: 28.0981, val_MinusLogProbMetric: 28.0981

Epoch 769: val_loss did not improve from 28.06794
196/196 - 35s - loss: 27.7468 - MinusLogProbMetric: 27.7468 - val_loss: 28.0981 - val_MinusLogProbMetric: 28.0981 - lr: 6.2500e-05 - 35s/epoch - 176ms/step
Epoch 770/1000
2023-10-24 23:01:57.013 
Epoch 770/1000 
	 loss: 27.7522, MinusLogProbMetric: 27.7522, val_loss: 28.0920, val_MinusLogProbMetric: 28.0920

Epoch 770: val_loss did not improve from 28.06794
196/196 - 32s - loss: 27.7522 - MinusLogProbMetric: 27.7522 - val_loss: 28.0920 - val_MinusLogProbMetric: 28.0920 - lr: 6.2500e-05 - 32s/epoch - 166ms/step
Epoch 771/1000
2023-10-24 23:02:27.406 
Epoch 771/1000 
	 loss: 27.7504, MinusLogProbMetric: 27.7504, val_loss: 28.0863, val_MinusLogProbMetric: 28.0863

Epoch 771: val_loss did not improve from 28.06794
196/196 - 30s - loss: 27.7504 - MinusLogProbMetric: 27.7504 - val_loss: 28.0863 - val_MinusLogProbMetric: 28.0863 - lr: 6.2500e-05 - 30s/epoch - 155ms/step
Epoch 772/1000
2023-10-24 23:03:01.072 
Epoch 772/1000 
	 loss: 27.7490, MinusLogProbMetric: 27.7490, val_loss: 28.0906, val_MinusLogProbMetric: 28.0906

Epoch 772: val_loss did not improve from 28.06794
196/196 - 34s - loss: 27.7490 - MinusLogProbMetric: 27.7490 - val_loss: 28.0906 - val_MinusLogProbMetric: 28.0906 - lr: 6.2500e-05 - 34s/epoch - 172ms/step
Epoch 773/1000
2023-10-24 23:03:35.806 
Epoch 773/1000 
	 loss: 27.7486, MinusLogProbMetric: 27.7486, val_loss: 28.0946, val_MinusLogProbMetric: 28.0946

Epoch 773: val_loss did not improve from 28.06794
196/196 - 35s - loss: 27.7486 - MinusLogProbMetric: 27.7486 - val_loss: 28.0946 - val_MinusLogProbMetric: 28.0946 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 774/1000
2023-10-24 23:04:08.771 
Epoch 774/1000 
	 loss: 27.7514, MinusLogProbMetric: 27.7514, val_loss: 28.1000, val_MinusLogProbMetric: 28.1000

Epoch 774: val_loss did not improve from 28.06794
196/196 - 33s - loss: 27.7514 - MinusLogProbMetric: 27.7514 - val_loss: 28.1000 - val_MinusLogProbMetric: 28.1000 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 775/1000
2023-10-24 23:04:41.074 
Epoch 775/1000 
	 loss: 27.7476, MinusLogProbMetric: 27.7476, val_loss: 28.0855, val_MinusLogProbMetric: 28.0855

Epoch 775: val_loss did not improve from 28.06794
196/196 - 32s - loss: 27.7476 - MinusLogProbMetric: 27.7476 - val_loss: 28.0855 - val_MinusLogProbMetric: 28.0855 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 776/1000
2023-10-24 23:05:13.326 
Epoch 776/1000 
	 loss: 27.7489, MinusLogProbMetric: 27.7489, val_loss: 28.0898, val_MinusLogProbMetric: 28.0898

Epoch 776: val_loss did not improve from 28.06794
196/196 - 32s - loss: 27.7489 - MinusLogProbMetric: 27.7489 - val_loss: 28.0898 - val_MinusLogProbMetric: 28.0898 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 777/1000
2023-10-24 23:05:47.975 
Epoch 777/1000 
	 loss: 27.7470, MinusLogProbMetric: 27.7470, val_loss: 28.0802, val_MinusLogProbMetric: 28.0802

Epoch 777: val_loss did not improve from 28.06794
196/196 - 35s - loss: 27.7470 - MinusLogProbMetric: 27.7470 - val_loss: 28.0802 - val_MinusLogProbMetric: 28.0802 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 778/1000
2023-10-24 23:06:22.356 
Epoch 778/1000 
	 loss: 27.7476, MinusLogProbMetric: 27.7476, val_loss: 28.0912, val_MinusLogProbMetric: 28.0912

Epoch 778: val_loss did not improve from 28.06794
196/196 - 34s - loss: 27.7476 - MinusLogProbMetric: 27.7476 - val_loss: 28.0912 - val_MinusLogProbMetric: 28.0912 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 779/1000
2023-10-24 23:06:56.879 
Epoch 779/1000 
	 loss: 27.7489, MinusLogProbMetric: 27.7489, val_loss: 28.0821, val_MinusLogProbMetric: 28.0821

Epoch 779: val_loss did not improve from 28.06794
196/196 - 35s - loss: 27.7489 - MinusLogProbMetric: 27.7489 - val_loss: 28.0821 - val_MinusLogProbMetric: 28.0821 - lr: 6.2500e-05 - 35s/epoch - 176ms/step
Epoch 780/1000
2023-10-24 23:07:31.178 
Epoch 780/1000 
	 loss: 27.7519, MinusLogProbMetric: 27.7519, val_loss: 28.0953, val_MinusLogProbMetric: 28.0953

Epoch 780: val_loss did not improve from 28.06794
196/196 - 34s - loss: 27.7519 - MinusLogProbMetric: 27.7519 - val_loss: 28.0953 - val_MinusLogProbMetric: 28.0953 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 781/1000
2023-10-24 23:08:03.480 
Epoch 781/1000 
	 loss: 27.7472, MinusLogProbMetric: 27.7472, val_loss: 28.0964, val_MinusLogProbMetric: 28.0964

Epoch 781: val_loss did not improve from 28.06794
196/196 - 32s - loss: 27.7472 - MinusLogProbMetric: 27.7472 - val_loss: 28.0964 - val_MinusLogProbMetric: 28.0964 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 782/1000
2023-10-24 23:08:36.330 
Epoch 782/1000 
	 loss: 27.7454, MinusLogProbMetric: 27.7454, val_loss: 28.0888, val_MinusLogProbMetric: 28.0888

Epoch 782: val_loss did not improve from 28.06794
196/196 - 33s - loss: 27.7454 - MinusLogProbMetric: 27.7454 - val_loss: 28.0888 - val_MinusLogProbMetric: 28.0888 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 783/1000
2023-10-24 23:09:09.558 
Epoch 783/1000 
	 loss: 27.7477, MinusLogProbMetric: 27.7477, val_loss: 28.0920, val_MinusLogProbMetric: 28.0920

Epoch 783: val_loss did not improve from 28.06794
196/196 - 33s - loss: 27.7477 - MinusLogProbMetric: 27.7477 - val_loss: 28.0920 - val_MinusLogProbMetric: 28.0920 - lr: 6.2500e-05 - 33s/epoch - 170ms/step
Epoch 784/1000
2023-10-24 23:09:41.962 
Epoch 784/1000 
	 loss: 27.7455, MinusLogProbMetric: 27.7455, val_loss: 28.0739, val_MinusLogProbMetric: 28.0739

Epoch 784: val_loss did not improve from 28.06794
196/196 - 32s - loss: 27.7455 - MinusLogProbMetric: 27.7455 - val_loss: 28.0739 - val_MinusLogProbMetric: 28.0739 - lr: 6.2500e-05 - 32s/epoch - 165ms/step
Epoch 785/1000
2023-10-24 23:10:14.819 
Epoch 785/1000 
	 loss: 27.7504, MinusLogProbMetric: 27.7504, val_loss: 28.1190, val_MinusLogProbMetric: 28.1190

Epoch 785: val_loss did not improve from 28.06794
196/196 - 33s - loss: 27.7504 - MinusLogProbMetric: 27.7504 - val_loss: 28.1190 - val_MinusLogProbMetric: 28.1190 - lr: 6.2500e-05 - 33s/epoch - 168ms/step
Epoch 786/1000
2023-10-24 23:10:48.728 
Epoch 786/1000 
	 loss: 27.7499, MinusLogProbMetric: 27.7499, val_loss: 28.0757, val_MinusLogProbMetric: 28.0757

Epoch 786: val_loss did not improve from 28.06794
196/196 - 34s - loss: 27.7499 - MinusLogProbMetric: 27.7499 - val_loss: 28.0757 - val_MinusLogProbMetric: 28.0757 - lr: 6.2500e-05 - 34s/epoch - 173ms/step
Epoch 787/1000
2023-10-24 23:11:23.087 
Epoch 787/1000 
	 loss: 27.7482, MinusLogProbMetric: 27.7482, val_loss: 28.0888, val_MinusLogProbMetric: 28.0888

Epoch 787: val_loss did not improve from 28.06794
196/196 - 34s - loss: 27.7482 - MinusLogProbMetric: 27.7482 - val_loss: 28.0888 - val_MinusLogProbMetric: 28.0888 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 788/1000
2023-10-24 23:11:57.772 
Epoch 788/1000 
	 loss: 27.7474, MinusLogProbMetric: 27.7474, val_loss: 28.0886, val_MinusLogProbMetric: 28.0886

Epoch 788: val_loss did not improve from 28.06794
196/196 - 35s - loss: 27.7474 - MinusLogProbMetric: 27.7474 - val_loss: 28.0886 - val_MinusLogProbMetric: 28.0886 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 789/1000
2023-10-24 23:12:32.284 
Epoch 789/1000 
	 loss: 27.7465, MinusLogProbMetric: 27.7465, val_loss: 28.0891, val_MinusLogProbMetric: 28.0891

Epoch 789: val_loss did not improve from 28.06794
196/196 - 35s - loss: 27.7465 - MinusLogProbMetric: 27.7465 - val_loss: 28.0891 - val_MinusLogProbMetric: 28.0891 - lr: 6.2500e-05 - 35s/epoch - 176ms/step
Epoch 790/1000
2023-10-24 23:13:06.622 
Epoch 790/1000 
	 loss: 27.7470, MinusLogProbMetric: 27.7470, val_loss: 28.0875, val_MinusLogProbMetric: 28.0875

Epoch 790: val_loss did not improve from 28.06794
196/196 - 34s - loss: 27.7470 - MinusLogProbMetric: 27.7470 - val_loss: 28.0875 - val_MinusLogProbMetric: 28.0875 - lr: 6.2500e-05 - 34s/epoch - 175ms/step
Epoch 791/1000
2023-10-24 23:13:41.222 
Epoch 791/1000 
	 loss: 27.7467, MinusLogProbMetric: 27.7467, val_loss: 28.0940, val_MinusLogProbMetric: 28.0940

Epoch 791: val_loss did not improve from 28.06794
196/196 - 35s - loss: 27.7467 - MinusLogProbMetric: 27.7467 - val_loss: 28.0940 - val_MinusLogProbMetric: 28.0940 - lr: 6.2500e-05 - 35s/epoch - 177ms/step
Epoch 792/1000
2023-10-24 23:14:13.284 
Epoch 792/1000 
	 loss: 27.7448, MinusLogProbMetric: 27.7448, val_loss: 28.0825, val_MinusLogProbMetric: 28.0825

Epoch 792: val_loss did not improve from 28.06794
196/196 - 32s - loss: 27.7448 - MinusLogProbMetric: 27.7448 - val_loss: 28.0825 - val_MinusLogProbMetric: 28.0825 - lr: 6.2500e-05 - 32s/epoch - 164ms/step
Epoch 793/1000
2023-10-24 23:14:47.651 
Epoch 793/1000 
	 loss: 27.7310, MinusLogProbMetric: 27.7310, val_loss: 28.0724, val_MinusLogProbMetric: 28.0724

Epoch 793: val_loss did not improve from 28.06794
196/196 - 34s - loss: 27.7310 - MinusLogProbMetric: 27.7310 - val_loss: 28.0724 - val_MinusLogProbMetric: 28.0724 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 794/1000
2023-10-24 23:15:22.207 
Epoch 794/1000 
	 loss: 27.7295, MinusLogProbMetric: 27.7295, val_loss: 28.0843, val_MinusLogProbMetric: 28.0843

Epoch 794: val_loss did not improve from 28.06794
196/196 - 35s - loss: 27.7295 - MinusLogProbMetric: 27.7295 - val_loss: 28.0843 - val_MinusLogProbMetric: 28.0843 - lr: 3.1250e-05 - 35s/epoch - 176ms/step
Epoch 795/1000
2023-10-24 23:15:57.431 
Epoch 795/1000 
	 loss: 27.7310, MinusLogProbMetric: 27.7310, val_loss: 28.0655, val_MinusLogProbMetric: 28.0655

Epoch 795: val_loss improved from 28.06794 to 28.06551, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 36s - loss: 27.7310 - MinusLogProbMetric: 27.7310 - val_loss: 28.0655 - val_MinusLogProbMetric: 28.0655 - lr: 3.1250e-05 - 36s/epoch - 182ms/step
Epoch 796/1000
2023-10-24 23:16:32.527 
Epoch 796/1000 
	 loss: 27.7284, MinusLogProbMetric: 27.7284, val_loss: 28.0690, val_MinusLogProbMetric: 28.0690

Epoch 796: val_loss did not improve from 28.06551
196/196 - 35s - loss: 27.7284 - MinusLogProbMetric: 27.7284 - val_loss: 28.0690 - val_MinusLogProbMetric: 28.0690 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 797/1000
2023-10-24 23:17:06.700 
Epoch 797/1000 
	 loss: 27.7296, MinusLogProbMetric: 27.7296, val_loss: 28.0686, val_MinusLogProbMetric: 28.0686

Epoch 797: val_loss did not improve from 28.06551
196/196 - 34s - loss: 27.7296 - MinusLogProbMetric: 27.7296 - val_loss: 28.0686 - val_MinusLogProbMetric: 28.0686 - lr: 3.1250e-05 - 34s/epoch - 174ms/step
Epoch 798/1000
2023-10-24 23:17:40.826 
Epoch 798/1000 
	 loss: 27.7289, MinusLogProbMetric: 27.7289, val_loss: 28.0755, val_MinusLogProbMetric: 28.0755

Epoch 798: val_loss did not improve from 28.06551
196/196 - 34s - loss: 27.7289 - MinusLogProbMetric: 27.7289 - val_loss: 28.0755 - val_MinusLogProbMetric: 28.0755 - lr: 3.1250e-05 - 34s/epoch - 174ms/step
Epoch 799/1000
2023-10-24 23:18:15.527 
Epoch 799/1000 
	 loss: 27.7273, MinusLogProbMetric: 27.7273, val_loss: 28.0678, val_MinusLogProbMetric: 28.0678

Epoch 799: val_loss did not improve from 28.06551
196/196 - 35s - loss: 27.7273 - MinusLogProbMetric: 27.7273 - val_loss: 28.0678 - val_MinusLogProbMetric: 28.0678 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 800/1000
2023-10-24 23:18:46.801 
Epoch 800/1000 
	 loss: 27.7275, MinusLogProbMetric: 27.7275, val_loss: 28.0724, val_MinusLogProbMetric: 28.0724

Epoch 800: val_loss did not improve from 28.06551
196/196 - 31s - loss: 27.7275 - MinusLogProbMetric: 27.7275 - val_loss: 28.0724 - val_MinusLogProbMetric: 28.0724 - lr: 3.1250e-05 - 31s/epoch - 160ms/step
Epoch 801/1000
2023-10-24 23:19:21.218 
Epoch 801/1000 
	 loss: 27.7292, MinusLogProbMetric: 27.7292, val_loss: 28.0836, val_MinusLogProbMetric: 28.0836

Epoch 801: val_loss did not improve from 28.06551
196/196 - 34s - loss: 27.7292 - MinusLogProbMetric: 27.7292 - val_loss: 28.0836 - val_MinusLogProbMetric: 28.0836 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 802/1000
2023-10-24 23:19:55.759 
Epoch 802/1000 
	 loss: 27.7280, MinusLogProbMetric: 27.7280, val_loss: 28.0794, val_MinusLogProbMetric: 28.0794

Epoch 802: val_loss did not improve from 28.06551
196/196 - 35s - loss: 27.7280 - MinusLogProbMetric: 27.7280 - val_loss: 28.0794 - val_MinusLogProbMetric: 28.0794 - lr: 3.1250e-05 - 35s/epoch - 176ms/step
Epoch 803/1000
2023-10-24 23:20:30.458 
Epoch 803/1000 
	 loss: 27.7295, MinusLogProbMetric: 27.7295, val_loss: 28.0771, val_MinusLogProbMetric: 28.0771

Epoch 803: val_loss did not improve from 28.06551
196/196 - 35s - loss: 27.7295 - MinusLogProbMetric: 27.7295 - val_loss: 28.0771 - val_MinusLogProbMetric: 28.0771 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 804/1000
2023-10-24 23:21:04.664 
Epoch 804/1000 
	 loss: 27.7272, MinusLogProbMetric: 27.7272, val_loss: 28.0666, val_MinusLogProbMetric: 28.0666

Epoch 804: val_loss did not improve from 28.06551
196/196 - 34s - loss: 27.7272 - MinusLogProbMetric: 27.7272 - val_loss: 28.0666 - val_MinusLogProbMetric: 28.0666 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 805/1000
2023-10-24 23:21:39.296 
Epoch 805/1000 
	 loss: 27.7272, MinusLogProbMetric: 27.7272, val_loss: 28.0734, val_MinusLogProbMetric: 28.0734

Epoch 805: val_loss did not improve from 28.06551
196/196 - 35s - loss: 27.7272 - MinusLogProbMetric: 27.7272 - val_loss: 28.0734 - val_MinusLogProbMetric: 28.0734 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 806/1000
2023-10-24 23:22:14.140 
Epoch 806/1000 
	 loss: 27.7285, MinusLogProbMetric: 27.7285, val_loss: 28.0745, val_MinusLogProbMetric: 28.0745

Epoch 806: val_loss did not improve from 28.06551
196/196 - 35s - loss: 27.7285 - MinusLogProbMetric: 27.7285 - val_loss: 28.0745 - val_MinusLogProbMetric: 28.0745 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 807/1000
2023-10-24 23:22:49.244 
Epoch 807/1000 
	 loss: 27.7294, MinusLogProbMetric: 27.7294, val_loss: 28.0715, val_MinusLogProbMetric: 28.0715

Epoch 807: val_loss did not improve from 28.06551
196/196 - 35s - loss: 27.7294 - MinusLogProbMetric: 27.7294 - val_loss: 28.0715 - val_MinusLogProbMetric: 28.0715 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 808/1000
2023-10-24 23:23:24.030 
Epoch 808/1000 
	 loss: 27.7301, MinusLogProbMetric: 27.7301, val_loss: 28.0740, val_MinusLogProbMetric: 28.0740

Epoch 808: val_loss did not improve from 28.06551
196/196 - 35s - loss: 27.7301 - MinusLogProbMetric: 27.7301 - val_loss: 28.0740 - val_MinusLogProbMetric: 28.0740 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 809/1000
2023-10-24 23:23:58.181 
Epoch 809/1000 
	 loss: 27.7292, MinusLogProbMetric: 27.7292, val_loss: 28.0740, val_MinusLogProbMetric: 28.0740

Epoch 809: val_loss did not improve from 28.06551
196/196 - 34s - loss: 27.7292 - MinusLogProbMetric: 27.7292 - val_loss: 28.0740 - val_MinusLogProbMetric: 28.0740 - lr: 3.1250e-05 - 34s/epoch - 174ms/step
Epoch 810/1000
2023-10-24 23:24:32.946 
Epoch 810/1000 
	 loss: 27.7292, MinusLogProbMetric: 27.7292, val_loss: 28.0677, val_MinusLogProbMetric: 28.0677

Epoch 810: val_loss did not improve from 28.06551
196/196 - 35s - loss: 27.7292 - MinusLogProbMetric: 27.7292 - val_loss: 28.0677 - val_MinusLogProbMetric: 28.0677 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 811/1000
2023-10-24 23:25:06.398 
Epoch 811/1000 
	 loss: 27.7282, MinusLogProbMetric: 27.7282, val_loss: 28.0765, val_MinusLogProbMetric: 28.0765

Epoch 811: val_loss did not improve from 28.06551
196/196 - 33s - loss: 27.7282 - MinusLogProbMetric: 27.7282 - val_loss: 28.0765 - val_MinusLogProbMetric: 28.0765 - lr: 3.1250e-05 - 33s/epoch - 171ms/step
Epoch 812/1000
2023-10-24 23:25:40.860 
Epoch 812/1000 
	 loss: 27.7300, MinusLogProbMetric: 27.7300, val_loss: 28.0649, val_MinusLogProbMetric: 28.0649

Epoch 812: val_loss improved from 28.06551 to 28.06489, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.7300 - MinusLogProbMetric: 27.7300 - val_loss: 28.0649 - val_MinusLogProbMetric: 28.0649 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 813/1000
2023-10-24 23:26:15.562 
Epoch 813/1000 
	 loss: 27.7261, MinusLogProbMetric: 27.7261, val_loss: 28.0696, val_MinusLogProbMetric: 28.0696

Epoch 813: val_loss did not improve from 28.06489
196/196 - 34s - loss: 27.7261 - MinusLogProbMetric: 27.7261 - val_loss: 28.0696 - val_MinusLogProbMetric: 28.0696 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 814/1000
2023-10-24 23:26:49.627 
Epoch 814/1000 
	 loss: 27.7291, MinusLogProbMetric: 27.7291, val_loss: 28.0750, val_MinusLogProbMetric: 28.0750

Epoch 814: val_loss did not improve from 28.06489
196/196 - 34s - loss: 27.7291 - MinusLogProbMetric: 27.7291 - val_loss: 28.0750 - val_MinusLogProbMetric: 28.0750 - lr: 3.1250e-05 - 34s/epoch - 174ms/step
Epoch 815/1000
2023-10-24 23:27:20.385 
Epoch 815/1000 
	 loss: 27.7294, MinusLogProbMetric: 27.7294, val_loss: 28.0749, val_MinusLogProbMetric: 28.0749

Epoch 815: val_loss did not improve from 28.06489
196/196 - 31s - loss: 27.7294 - MinusLogProbMetric: 27.7294 - val_loss: 28.0749 - val_MinusLogProbMetric: 28.0749 - lr: 3.1250e-05 - 31s/epoch - 157ms/step
Epoch 816/1000
2023-10-24 23:27:53.810 
Epoch 816/1000 
	 loss: 27.7279, MinusLogProbMetric: 27.7279, val_loss: 28.0673, val_MinusLogProbMetric: 28.0673

Epoch 816: val_loss did not improve from 28.06489
196/196 - 33s - loss: 27.7279 - MinusLogProbMetric: 27.7279 - val_loss: 28.0673 - val_MinusLogProbMetric: 28.0673 - lr: 3.1250e-05 - 33s/epoch - 171ms/step
Epoch 817/1000
2023-10-24 23:28:28.687 
Epoch 817/1000 
	 loss: 27.7276, MinusLogProbMetric: 27.7276, val_loss: 28.0718, val_MinusLogProbMetric: 28.0718

Epoch 817: val_loss did not improve from 28.06489
196/196 - 35s - loss: 27.7276 - MinusLogProbMetric: 27.7276 - val_loss: 28.0718 - val_MinusLogProbMetric: 28.0718 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 818/1000
2023-10-24 23:29:03.496 
Epoch 818/1000 
	 loss: 27.7281, MinusLogProbMetric: 27.7281, val_loss: 28.0785, val_MinusLogProbMetric: 28.0785

Epoch 818: val_loss did not improve from 28.06489
196/196 - 35s - loss: 27.7281 - MinusLogProbMetric: 27.7281 - val_loss: 28.0785 - val_MinusLogProbMetric: 28.0785 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 819/1000
2023-10-24 23:29:38.103 
Epoch 819/1000 
	 loss: 27.7298, MinusLogProbMetric: 27.7298, val_loss: 28.0723, val_MinusLogProbMetric: 28.0723

Epoch 819: val_loss did not improve from 28.06489
196/196 - 35s - loss: 27.7298 - MinusLogProbMetric: 27.7298 - val_loss: 28.0723 - val_MinusLogProbMetric: 28.0723 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 820/1000
2023-10-24 23:30:12.353 
Epoch 820/1000 
	 loss: 27.7298, MinusLogProbMetric: 27.7298, val_loss: 28.0616, val_MinusLogProbMetric: 28.0616

Epoch 820: val_loss improved from 28.06489 to 28.06161, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.7298 - MinusLogProbMetric: 27.7298 - val_loss: 28.0616 - val_MinusLogProbMetric: 28.0616 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 821/1000
2023-10-24 23:30:47.352 
Epoch 821/1000 
	 loss: 27.7271, MinusLogProbMetric: 27.7271, val_loss: 28.0667, val_MinusLogProbMetric: 28.0667

Epoch 821: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7271 - MinusLogProbMetric: 27.7271 - val_loss: 28.0667 - val_MinusLogProbMetric: 28.0667 - lr: 3.1250e-05 - 35s/epoch - 176ms/step
Epoch 822/1000
2023-10-24 23:31:22.096 
Epoch 822/1000 
	 loss: 27.7276, MinusLogProbMetric: 27.7276, val_loss: 28.0758, val_MinusLogProbMetric: 28.0758

Epoch 822: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7276 - MinusLogProbMetric: 27.7276 - val_loss: 28.0758 - val_MinusLogProbMetric: 28.0758 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 823/1000
2023-10-24 23:31:56.381 
Epoch 823/1000 
	 loss: 27.7269, MinusLogProbMetric: 27.7269, val_loss: 28.0795, val_MinusLogProbMetric: 28.0795

Epoch 823: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7269 - MinusLogProbMetric: 27.7269 - val_loss: 28.0795 - val_MinusLogProbMetric: 28.0795 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 824/1000
2023-10-24 23:32:30.490 
Epoch 824/1000 
	 loss: 27.7288, MinusLogProbMetric: 27.7288, val_loss: 28.0709, val_MinusLogProbMetric: 28.0709

Epoch 824: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7288 - MinusLogProbMetric: 27.7288 - val_loss: 28.0709 - val_MinusLogProbMetric: 28.0709 - lr: 3.1250e-05 - 34s/epoch - 174ms/step
Epoch 825/1000
2023-10-24 23:33:05.175 
Epoch 825/1000 
	 loss: 27.7269, MinusLogProbMetric: 27.7269, val_loss: 28.0748, val_MinusLogProbMetric: 28.0748

Epoch 825: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7269 - MinusLogProbMetric: 27.7269 - val_loss: 28.0748 - val_MinusLogProbMetric: 28.0748 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 826/1000
2023-10-24 23:33:39.963 
Epoch 826/1000 
	 loss: 27.7274, MinusLogProbMetric: 27.7274, val_loss: 28.0806, val_MinusLogProbMetric: 28.0806

Epoch 826: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7274 - MinusLogProbMetric: 27.7274 - val_loss: 28.0806 - val_MinusLogProbMetric: 28.0806 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 827/1000
2023-10-24 23:34:13.360 
Epoch 827/1000 
	 loss: 27.7279, MinusLogProbMetric: 27.7279, val_loss: 28.0649, val_MinusLogProbMetric: 28.0649

Epoch 827: val_loss did not improve from 28.06161
196/196 - 33s - loss: 27.7279 - MinusLogProbMetric: 27.7279 - val_loss: 28.0649 - val_MinusLogProbMetric: 28.0649 - lr: 3.1250e-05 - 33s/epoch - 170ms/step
Epoch 828/1000
2023-10-24 23:34:47.774 
Epoch 828/1000 
	 loss: 27.7275, MinusLogProbMetric: 27.7275, val_loss: 28.0632, val_MinusLogProbMetric: 28.0632

Epoch 828: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7275 - MinusLogProbMetric: 27.7275 - val_loss: 28.0632 - val_MinusLogProbMetric: 28.0632 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 829/1000
2023-10-24 23:35:22.139 
Epoch 829/1000 
	 loss: 27.7278, MinusLogProbMetric: 27.7278, val_loss: 28.0723, val_MinusLogProbMetric: 28.0723

Epoch 829: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7278 - MinusLogProbMetric: 27.7278 - val_loss: 28.0723 - val_MinusLogProbMetric: 28.0723 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 830/1000
2023-10-24 23:35:56.099 
Epoch 830/1000 
	 loss: 27.7280, MinusLogProbMetric: 27.7280, val_loss: 28.0705, val_MinusLogProbMetric: 28.0705

Epoch 830: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7280 - MinusLogProbMetric: 27.7280 - val_loss: 28.0705 - val_MinusLogProbMetric: 28.0705 - lr: 3.1250e-05 - 34s/epoch - 173ms/step
Epoch 831/1000
2023-10-24 23:36:30.585 
Epoch 831/1000 
	 loss: 27.7270, MinusLogProbMetric: 27.7270, val_loss: 28.0777, val_MinusLogProbMetric: 28.0777

Epoch 831: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7270 - MinusLogProbMetric: 27.7270 - val_loss: 28.0777 - val_MinusLogProbMetric: 28.0777 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 832/1000
2023-10-24 23:37:05.077 
Epoch 832/1000 
	 loss: 27.7298, MinusLogProbMetric: 27.7298, val_loss: 28.0807, val_MinusLogProbMetric: 28.0807

Epoch 832: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7298 - MinusLogProbMetric: 27.7298 - val_loss: 28.0807 - val_MinusLogProbMetric: 28.0807 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 833/1000
2023-10-24 23:37:39.823 
Epoch 833/1000 
	 loss: 27.7280, MinusLogProbMetric: 27.7280, val_loss: 28.0712, val_MinusLogProbMetric: 28.0712

Epoch 833: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7280 - MinusLogProbMetric: 27.7280 - val_loss: 28.0712 - val_MinusLogProbMetric: 28.0712 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 834/1000
2023-10-24 23:38:14.677 
Epoch 834/1000 
	 loss: 27.7273, MinusLogProbMetric: 27.7273, val_loss: 28.0817, val_MinusLogProbMetric: 28.0817

Epoch 834: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7273 - MinusLogProbMetric: 27.7273 - val_loss: 28.0817 - val_MinusLogProbMetric: 28.0817 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 835/1000
2023-10-24 23:38:49.462 
Epoch 835/1000 
	 loss: 27.7278, MinusLogProbMetric: 27.7278, val_loss: 28.0671, val_MinusLogProbMetric: 28.0671

Epoch 835: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7278 - MinusLogProbMetric: 27.7278 - val_loss: 28.0671 - val_MinusLogProbMetric: 28.0671 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 836/1000
2023-10-24 23:39:24.139 
Epoch 836/1000 
	 loss: 27.7279, MinusLogProbMetric: 27.7279, val_loss: 28.0665, val_MinusLogProbMetric: 28.0665

Epoch 836: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7279 - MinusLogProbMetric: 27.7279 - val_loss: 28.0665 - val_MinusLogProbMetric: 28.0665 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 837/1000
2023-10-24 23:39:59.176 
Epoch 837/1000 
	 loss: 27.7282, MinusLogProbMetric: 27.7282, val_loss: 28.0871, val_MinusLogProbMetric: 28.0871

Epoch 837: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7282 - MinusLogProbMetric: 27.7282 - val_loss: 28.0871 - val_MinusLogProbMetric: 28.0871 - lr: 3.1250e-05 - 35s/epoch - 179ms/step
Epoch 838/1000
2023-10-24 23:40:33.696 
Epoch 838/1000 
	 loss: 27.7275, MinusLogProbMetric: 27.7275, val_loss: 28.0685, val_MinusLogProbMetric: 28.0685

Epoch 838: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7275 - MinusLogProbMetric: 27.7275 - val_loss: 28.0685 - val_MinusLogProbMetric: 28.0685 - lr: 3.1250e-05 - 35s/epoch - 176ms/step
Epoch 839/1000
2023-10-24 23:41:08.227 
Epoch 839/1000 
	 loss: 27.7273, MinusLogProbMetric: 27.7273, val_loss: 28.0765, val_MinusLogProbMetric: 28.0765

Epoch 839: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7273 - MinusLogProbMetric: 27.7273 - val_loss: 28.0765 - val_MinusLogProbMetric: 28.0765 - lr: 3.1250e-05 - 35s/epoch - 176ms/step
Epoch 840/1000
2023-10-24 23:41:42.691 
Epoch 840/1000 
	 loss: 27.7267, MinusLogProbMetric: 27.7267, val_loss: 28.0638, val_MinusLogProbMetric: 28.0638

Epoch 840: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7267 - MinusLogProbMetric: 27.7267 - val_loss: 28.0638 - val_MinusLogProbMetric: 28.0638 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 841/1000
2023-10-24 23:42:17.293 
Epoch 841/1000 
	 loss: 27.7274, MinusLogProbMetric: 27.7274, val_loss: 28.0679, val_MinusLogProbMetric: 28.0679

Epoch 841: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7274 - MinusLogProbMetric: 27.7274 - val_loss: 28.0679 - val_MinusLogProbMetric: 28.0679 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 842/1000
2023-10-24 23:42:51.779 
Epoch 842/1000 
	 loss: 27.7261, MinusLogProbMetric: 27.7261, val_loss: 28.0731, val_MinusLogProbMetric: 28.0731

Epoch 842: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7261 - MinusLogProbMetric: 27.7261 - val_loss: 28.0731 - val_MinusLogProbMetric: 28.0731 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 843/1000
2023-10-24 23:43:26.191 
Epoch 843/1000 
	 loss: 27.7264, MinusLogProbMetric: 27.7264, val_loss: 28.0777, val_MinusLogProbMetric: 28.0777

Epoch 843: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7264 - MinusLogProbMetric: 27.7264 - val_loss: 28.0777 - val_MinusLogProbMetric: 28.0777 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 844/1000
2023-10-24 23:44:00.644 
Epoch 844/1000 
	 loss: 27.7270, MinusLogProbMetric: 27.7270, val_loss: 28.0672, val_MinusLogProbMetric: 28.0672

Epoch 844: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7270 - MinusLogProbMetric: 27.7270 - val_loss: 28.0672 - val_MinusLogProbMetric: 28.0672 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 845/1000
2023-10-24 23:44:35.299 
Epoch 845/1000 
	 loss: 27.7272, MinusLogProbMetric: 27.7272, val_loss: 28.0842, val_MinusLogProbMetric: 28.0842

Epoch 845: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7272 - MinusLogProbMetric: 27.7272 - val_loss: 28.0842 - val_MinusLogProbMetric: 28.0842 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 846/1000
2023-10-24 23:45:09.987 
Epoch 846/1000 
	 loss: 27.7270, MinusLogProbMetric: 27.7270, val_loss: 28.0646, val_MinusLogProbMetric: 28.0646

Epoch 846: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7270 - MinusLogProbMetric: 27.7270 - val_loss: 28.0646 - val_MinusLogProbMetric: 28.0646 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 847/1000
2023-10-24 23:45:44.520 
Epoch 847/1000 
	 loss: 27.7281, MinusLogProbMetric: 27.7281, val_loss: 28.0801, val_MinusLogProbMetric: 28.0801

Epoch 847: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7281 - MinusLogProbMetric: 27.7281 - val_loss: 28.0801 - val_MinusLogProbMetric: 28.0801 - lr: 3.1250e-05 - 35s/epoch - 176ms/step
Epoch 848/1000
2023-10-24 23:46:19.362 
Epoch 848/1000 
	 loss: 27.7274, MinusLogProbMetric: 27.7274, val_loss: 28.0618, val_MinusLogProbMetric: 28.0618

Epoch 848: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7274 - MinusLogProbMetric: 27.7274 - val_loss: 28.0618 - val_MinusLogProbMetric: 28.0618 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 849/1000
2023-10-24 23:46:53.668 
Epoch 849/1000 
	 loss: 27.7263, MinusLogProbMetric: 27.7263, val_loss: 28.0699, val_MinusLogProbMetric: 28.0699

Epoch 849: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7263 - MinusLogProbMetric: 27.7263 - val_loss: 28.0699 - val_MinusLogProbMetric: 28.0699 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 850/1000
2023-10-24 23:47:28.327 
Epoch 850/1000 
	 loss: 27.7264, MinusLogProbMetric: 27.7264, val_loss: 28.0632, val_MinusLogProbMetric: 28.0632

Epoch 850: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7264 - MinusLogProbMetric: 27.7264 - val_loss: 28.0632 - val_MinusLogProbMetric: 28.0632 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 851/1000
2023-10-24 23:48:03.092 
Epoch 851/1000 
	 loss: 27.7252, MinusLogProbMetric: 27.7252, val_loss: 28.0624, val_MinusLogProbMetric: 28.0624

Epoch 851: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7252 - MinusLogProbMetric: 27.7252 - val_loss: 28.0624 - val_MinusLogProbMetric: 28.0624 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 852/1000
2023-10-24 23:48:37.799 
Epoch 852/1000 
	 loss: 27.7251, MinusLogProbMetric: 27.7251, val_loss: 28.0751, val_MinusLogProbMetric: 28.0751

Epoch 852: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7251 - MinusLogProbMetric: 27.7251 - val_loss: 28.0751 - val_MinusLogProbMetric: 28.0751 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 853/1000
2023-10-24 23:49:12.235 
Epoch 853/1000 
	 loss: 27.7275, MinusLogProbMetric: 27.7275, val_loss: 28.0768, val_MinusLogProbMetric: 28.0768

Epoch 853: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7275 - MinusLogProbMetric: 27.7275 - val_loss: 28.0768 - val_MinusLogProbMetric: 28.0768 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 854/1000
2023-10-24 23:49:46.782 
Epoch 854/1000 
	 loss: 27.7257, MinusLogProbMetric: 27.7257, val_loss: 28.0722, val_MinusLogProbMetric: 28.0722

Epoch 854: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7257 - MinusLogProbMetric: 27.7257 - val_loss: 28.0722 - val_MinusLogProbMetric: 28.0722 - lr: 3.1250e-05 - 35s/epoch - 176ms/step
Epoch 855/1000
2023-10-24 23:50:21.165 
Epoch 855/1000 
	 loss: 27.7278, MinusLogProbMetric: 27.7278, val_loss: 28.0717, val_MinusLogProbMetric: 28.0717

Epoch 855: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7278 - MinusLogProbMetric: 27.7278 - val_loss: 28.0717 - val_MinusLogProbMetric: 28.0717 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 856/1000
2023-10-24 23:50:56.005 
Epoch 856/1000 
	 loss: 27.7268, MinusLogProbMetric: 27.7268, val_loss: 28.0703, val_MinusLogProbMetric: 28.0703

Epoch 856: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7268 - MinusLogProbMetric: 27.7268 - val_loss: 28.0703 - val_MinusLogProbMetric: 28.0703 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 857/1000
2023-10-24 23:51:30.521 
Epoch 857/1000 
	 loss: 27.7267, MinusLogProbMetric: 27.7267, val_loss: 28.0756, val_MinusLogProbMetric: 28.0756

Epoch 857: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7267 - MinusLogProbMetric: 27.7267 - val_loss: 28.0756 - val_MinusLogProbMetric: 28.0756 - lr: 3.1250e-05 - 35s/epoch - 176ms/step
Epoch 858/1000
2023-10-24 23:52:05.413 
Epoch 858/1000 
	 loss: 27.7262, MinusLogProbMetric: 27.7262, val_loss: 28.0676, val_MinusLogProbMetric: 28.0676

Epoch 858: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7262 - MinusLogProbMetric: 27.7262 - val_loss: 28.0676 - val_MinusLogProbMetric: 28.0676 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 859/1000
2023-10-24 23:52:40.053 
Epoch 859/1000 
	 loss: 27.7279, MinusLogProbMetric: 27.7279, val_loss: 28.0725, val_MinusLogProbMetric: 28.0725

Epoch 859: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7279 - MinusLogProbMetric: 27.7279 - val_loss: 28.0725 - val_MinusLogProbMetric: 28.0725 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 860/1000
2023-10-24 23:53:14.631 
Epoch 860/1000 
	 loss: 27.7263, MinusLogProbMetric: 27.7263, val_loss: 28.0825, val_MinusLogProbMetric: 28.0825

Epoch 860: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7263 - MinusLogProbMetric: 27.7263 - val_loss: 28.0825 - val_MinusLogProbMetric: 28.0825 - lr: 3.1250e-05 - 35s/epoch - 176ms/step
Epoch 861/1000
2023-10-24 23:53:49.199 
Epoch 861/1000 
	 loss: 27.7292, MinusLogProbMetric: 27.7292, val_loss: 28.0723, val_MinusLogProbMetric: 28.0723

Epoch 861: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7292 - MinusLogProbMetric: 27.7292 - val_loss: 28.0723 - val_MinusLogProbMetric: 28.0723 - lr: 3.1250e-05 - 35s/epoch - 176ms/step
Epoch 862/1000
2023-10-24 23:54:23.602 
Epoch 862/1000 
	 loss: 27.7261, MinusLogProbMetric: 27.7261, val_loss: 28.0765, val_MinusLogProbMetric: 28.0765

Epoch 862: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7261 - MinusLogProbMetric: 27.7261 - val_loss: 28.0765 - val_MinusLogProbMetric: 28.0765 - lr: 3.1250e-05 - 34s/epoch - 176ms/step
Epoch 863/1000
2023-10-24 23:54:58.235 
Epoch 863/1000 
	 loss: 27.7265, MinusLogProbMetric: 27.7265, val_loss: 28.0754, val_MinusLogProbMetric: 28.0754

Epoch 863: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7265 - MinusLogProbMetric: 27.7265 - val_loss: 28.0754 - val_MinusLogProbMetric: 28.0754 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 864/1000
2023-10-24 23:55:32.614 
Epoch 864/1000 
	 loss: 27.7261, MinusLogProbMetric: 27.7261, val_loss: 28.0795, val_MinusLogProbMetric: 28.0795

Epoch 864: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7261 - MinusLogProbMetric: 27.7261 - val_loss: 28.0795 - val_MinusLogProbMetric: 28.0795 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 865/1000
2023-10-24 23:56:07.493 
Epoch 865/1000 
	 loss: 27.7269, MinusLogProbMetric: 27.7269, val_loss: 28.0882, val_MinusLogProbMetric: 28.0882

Epoch 865: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7269 - MinusLogProbMetric: 27.7269 - val_loss: 28.0882 - val_MinusLogProbMetric: 28.0882 - lr: 3.1250e-05 - 35s/epoch - 178ms/step
Epoch 866/1000
2023-10-24 23:56:42.262 
Epoch 866/1000 
	 loss: 27.7271, MinusLogProbMetric: 27.7271, val_loss: 28.0702, val_MinusLogProbMetric: 28.0702

Epoch 866: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7271 - MinusLogProbMetric: 27.7271 - val_loss: 28.0702 - val_MinusLogProbMetric: 28.0702 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 867/1000
2023-10-24 23:57:16.323 
Epoch 867/1000 
	 loss: 27.7250, MinusLogProbMetric: 27.7250, val_loss: 28.0743, val_MinusLogProbMetric: 28.0743

Epoch 867: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7250 - MinusLogProbMetric: 27.7250 - val_loss: 28.0743 - val_MinusLogProbMetric: 28.0743 - lr: 3.1250e-05 - 34s/epoch - 174ms/step
Epoch 868/1000
2023-10-24 23:57:50.542 
Epoch 868/1000 
	 loss: 27.7259, MinusLogProbMetric: 27.7259, val_loss: 28.0657, val_MinusLogProbMetric: 28.0657

Epoch 868: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7259 - MinusLogProbMetric: 27.7259 - val_loss: 28.0657 - val_MinusLogProbMetric: 28.0657 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 869/1000
2023-10-24 23:58:25.302 
Epoch 869/1000 
	 loss: 27.7260, MinusLogProbMetric: 27.7260, val_loss: 28.0633, val_MinusLogProbMetric: 28.0633

Epoch 869: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7260 - MinusLogProbMetric: 27.7260 - val_loss: 28.0633 - val_MinusLogProbMetric: 28.0633 - lr: 3.1250e-05 - 35s/epoch - 177ms/step
Epoch 870/1000
2023-10-24 23:58:59.657 
Epoch 870/1000 
	 loss: 27.7261, MinusLogProbMetric: 27.7261, val_loss: 28.0703, val_MinusLogProbMetric: 28.0703

Epoch 870: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7261 - MinusLogProbMetric: 27.7261 - val_loss: 28.0703 - val_MinusLogProbMetric: 28.0703 - lr: 3.1250e-05 - 34s/epoch - 175ms/step
Epoch 871/1000
2023-10-24 23:59:33.886 
Epoch 871/1000 
	 loss: 27.7169, MinusLogProbMetric: 27.7169, val_loss: 28.0636, val_MinusLogProbMetric: 28.0636

Epoch 871: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7169 - MinusLogProbMetric: 27.7169 - val_loss: 28.0636 - val_MinusLogProbMetric: 28.0636 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 872/1000
2023-10-25 00:00:08.297 
Epoch 872/1000 
	 loss: 27.7174, MinusLogProbMetric: 27.7174, val_loss: 28.0625, val_MinusLogProbMetric: 28.0625

Epoch 872: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7174 - MinusLogProbMetric: 27.7174 - val_loss: 28.0625 - val_MinusLogProbMetric: 28.0625 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 873/1000
2023-10-25 00:00:43.030 
Epoch 873/1000 
	 loss: 27.7169, MinusLogProbMetric: 27.7169, val_loss: 28.0665, val_MinusLogProbMetric: 28.0665

Epoch 873: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7169 - MinusLogProbMetric: 27.7169 - val_loss: 28.0665 - val_MinusLogProbMetric: 28.0665 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 874/1000
2023-10-25 00:01:17.882 
Epoch 874/1000 
	 loss: 27.7169, MinusLogProbMetric: 27.7169, val_loss: 28.0668, val_MinusLogProbMetric: 28.0668

Epoch 874: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7169 - MinusLogProbMetric: 27.7169 - val_loss: 28.0668 - val_MinusLogProbMetric: 28.0668 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 875/1000
2023-10-25 00:01:52.359 
Epoch 875/1000 
	 loss: 27.7164, MinusLogProbMetric: 27.7164, val_loss: 28.0652, val_MinusLogProbMetric: 28.0652

Epoch 875: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7164 - MinusLogProbMetric: 27.7164 - val_loss: 28.0652 - val_MinusLogProbMetric: 28.0652 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 876/1000
2023-10-25 00:02:26.938 
Epoch 876/1000 
	 loss: 27.7163, MinusLogProbMetric: 27.7163, val_loss: 28.0630, val_MinusLogProbMetric: 28.0630

Epoch 876: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7163 - MinusLogProbMetric: 27.7163 - val_loss: 28.0630 - val_MinusLogProbMetric: 28.0630 - lr: 1.5625e-05 - 35s/epoch - 176ms/step
Epoch 877/1000
2023-10-25 00:03:01.287 
Epoch 877/1000 
	 loss: 27.7172, MinusLogProbMetric: 27.7172, val_loss: 28.0661, val_MinusLogProbMetric: 28.0661

Epoch 877: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7172 - MinusLogProbMetric: 27.7172 - val_loss: 28.0661 - val_MinusLogProbMetric: 28.0661 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 878/1000
2023-10-25 00:03:35.632 
Epoch 878/1000 
	 loss: 27.7172, MinusLogProbMetric: 27.7172, val_loss: 28.0674, val_MinusLogProbMetric: 28.0674

Epoch 878: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7172 - MinusLogProbMetric: 27.7172 - val_loss: 28.0674 - val_MinusLogProbMetric: 28.0674 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 879/1000
2023-10-25 00:04:10.448 
Epoch 879/1000 
	 loss: 27.7172, MinusLogProbMetric: 27.7172, val_loss: 28.0625, val_MinusLogProbMetric: 28.0625

Epoch 879: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7172 - MinusLogProbMetric: 27.7172 - val_loss: 28.0625 - val_MinusLogProbMetric: 28.0625 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 880/1000
2023-10-25 00:04:44.661 
Epoch 880/1000 
	 loss: 27.7169, MinusLogProbMetric: 27.7169, val_loss: 28.0671, val_MinusLogProbMetric: 28.0671

Epoch 880: val_loss did not improve from 28.06161
196/196 - 34s - loss: 27.7169 - MinusLogProbMetric: 27.7169 - val_loss: 28.0671 - val_MinusLogProbMetric: 28.0671 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 881/1000
2023-10-25 00:05:19.313 
Epoch 881/1000 
	 loss: 27.7177, MinusLogProbMetric: 27.7177, val_loss: 28.0649, val_MinusLogProbMetric: 28.0649

Epoch 881: val_loss did not improve from 28.06161
196/196 - 35s - loss: 27.7177 - MinusLogProbMetric: 27.7177 - val_loss: 28.0649 - val_MinusLogProbMetric: 28.0649 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 882/1000
2023-10-25 00:05:53.481 
Epoch 882/1000 
	 loss: 27.7165, MinusLogProbMetric: 27.7165, val_loss: 28.0602, val_MinusLogProbMetric: 28.0602

Epoch 882: val_loss improved from 28.06161 to 28.06022, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.7165 - MinusLogProbMetric: 27.7165 - val_loss: 28.0602 - val_MinusLogProbMetric: 28.0602 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 883/1000
2023-10-25 00:06:28.226 
Epoch 883/1000 
	 loss: 27.7174, MinusLogProbMetric: 27.7174, val_loss: 28.0647, val_MinusLogProbMetric: 28.0647

Epoch 883: val_loss did not improve from 28.06022
196/196 - 34s - loss: 27.7174 - MinusLogProbMetric: 27.7174 - val_loss: 28.0647 - val_MinusLogProbMetric: 28.0647 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 884/1000
2023-10-25 00:07:02.950 
Epoch 884/1000 
	 loss: 27.7169, MinusLogProbMetric: 27.7169, val_loss: 28.0656, val_MinusLogProbMetric: 28.0656

Epoch 884: val_loss did not improve from 28.06022
196/196 - 35s - loss: 27.7169 - MinusLogProbMetric: 27.7169 - val_loss: 28.0656 - val_MinusLogProbMetric: 28.0656 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 885/1000
2023-10-25 00:07:37.659 
Epoch 885/1000 
	 loss: 27.7163, MinusLogProbMetric: 27.7163, val_loss: 28.0632, val_MinusLogProbMetric: 28.0632

Epoch 885: val_loss did not improve from 28.06022
196/196 - 35s - loss: 27.7163 - MinusLogProbMetric: 27.7163 - val_loss: 28.0632 - val_MinusLogProbMetric: 28.0632 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 886/1000
2023-10-25 00:08:12.352 
Epoch 886/1000 
	 loss: 27.7169, MinusLogProbMetric: 27.7169, val_loss: 28.0597, val_MinusLogProbMetric: 28.0597

Epoch 886: val_loss improved from 28.06022 to 28.05969, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.7169 - MinusLogProbMetric: 27.7169 - val_loss: 28.0597 - val_MinusLogProbMetric: 28.0597 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 887/1000
2023-10-25 00:08:47.262 
Epoch 887/1000 
	 loss: 27.7176, MinusLogProbMetric: 27.7176, val_loss: 28.0610, val_MinusLogProbMetric: 28.0610

Epoch 887: val_loss did not improve from 28.05969
196/196 - 34s - loss: 27.7176 - MinusLogProbMetric: 27.7176 - val_loss: 28.0610 - val_MinusLogProbMetric: 28.0610 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 888/1000
2023-10-25 00:09:21.115 
Epoch 888/1000 
	 loss: 27.7167, MinusLogProbMetric: 27.7167, val_loss: 28.0608, val_MinusLogProbMetric: 28.0608

Epoch 888: val_loss did not improve from 28.05969
196/196 - 34s - loss: 27.7167 - MinusLogProbMetric: 27.7167 - val_loss: 28.0608 - val_MinusLogProbMetric: 28.0608 - lr: 1.5625e-05 - 34s/epoch - 173ms/step
Epoch 889/1000
2023-10-25 00:09:55.926 
Epoch 889/1000 
	 loss: 27.7173, MinusLogProbMetric: 27.7173, val_loss: 28.0679, val_MinusLogProbMetric: 28.0679

Epoch 889: val_loss did not improve from 28.05969
196/196 - 35s - loss: 27.7173 - MinusLogProbMetric: 27.7173 - val_loss: 28.0679 - val_MinusLogProbMetric: 28.0679 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 890/1000
2023-10-25 00:10:30.363 
Epoch 890/1000 
	 loss: 27.7171, MinusLogProbMetric: 27.7171, val_loss: 28.0774, val_MinusLogProbMetric: 28.0774

Epoch 890: val_loss did not improve from 28.05969
196/196 - 34s - loss: 27.7171 - MinusLogProbMetric: 27.7171 - val_loss: 28.0774 - val_MinusLogProbMetric: 28.0774 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 891/1000
2023-10-25 00:11:05.072 
Epoch 891/1000 
	 loss: 27.7170, MinusLogProbMetric: 27.7170, val_loss: 28.0683, val_MinusLogProbMetric: 28.0683

Epoch 891: val_loss did not improve from 28.05969
196/196 - 35s - loss: 27.7170 - MinusLogProbMetric: 27.7170 - val_loss: 28.0683 - val_MinusLogProbMetric: 28.0683 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 892/1000
2023-10-25 00:11:39.732 
Epoch 892/1000 
	 loss: 27.7159, MinusLogProbMetric: 27.7159, val_loss: 28.0644, val_MinusLogProbMetric: 28.0644

Epoch 892: val_loss did not improve from 28.05969
196/196 - 35s - loss: 27.7159 - MinusLogProbMetric: 27.7159 - val_loss: 28.0644 - val_MinusLogProbMetric: 28.0644 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 893/1000
2023-10-25 00:12:14.507 
Epoch 893/1000 
	 loss: 27.7176, MinusLogProbMetric: 27.7176, val_loss: 28.0629, val_MinusLogProbMetric: 28.0629

Epoch 893: val_loss did not improve from 28.05969
196/196 - 35s - loss: 27.7176 - MinusLogProbMetric: 27.7176 - val_loss: 28.0629 - val_MinusLogProbMetric: 28.0629 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 894/1000
2023-10-25 00:12:49.396 
Epoch 894/1000 
	 loss: 27.7171, MinusLogProbMetric: 27.7171, val_loss: 28.0702, val_MinusLogProbMetric: 28.0702

Epoch 894: val_loss did not improve from 28.05969
196/196 - 35s - loss: 27.7171 - MinusLogProbMetric: 27.7171 - val_loss: 28.0702 - val_MinusLogProbMetric: 28.0702 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 895/1000
2023-10-25 00:13:24.350 
Epoch 895/1000 
	 loss: 27.7175, MinusLogProbMetric: 27.7175, val_loss: 28.0582, val_MinusLogProbMetric: 28.0582

Epoch 895: val_loss improved from 28.05969 to 28.05819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.7175 - MinusLogProbMetric: 27.7175 - val_loss: 28.0582 - val_MinusLogProbMetric: 28.0582 - lr: 1.5625e-05 - 35s/epoch - 181ms/step
Epoch 896/1000
2023-10-25 00:13:59.589 
Epoch 896/1000 
	 loss: 27.7165, MinusLogProbMetric: 27.7165, val_loss: 28.0594, val_MinusLogProbMetric: 28.0594

Epoch 896: val_loss did not improve from 28.05819
196/196 - 35s - loss: 27.7165 - MinusLogProbMetric: 27.7165 - val_loss: 28.0594 - val_MinusLogProbMetric: 28.0594 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 897/1000
2023-10-25 00:14:34.191 
Epoch 897/1000 
	 loss: 27.7173, MinusLogProbMetric: 27.7173, val_loss: 28.0624, val_MinusLogProbMetric: 28.0624

Epoch 897: val_loss did not improve from 28.05819
196/196 - 35s - loss: 27.7173 - MinusLogProbMetric: 27.7173 - val_loss: 28.0624 - val_MinusLogProbMetric: 28.0624 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 898/1000
2023-10-25 00:15:08.147 
Epoch 898/1000 
	 loss: 27.7174, MinusLogProbMetric: 27.7174, val_loss: 28.0642, val_MinusLogProbMetric: 28.0642

Epoch 898: val_loss did not improve from 28.05819
196/196 - 34s - loss: 27.7174 - MinusLogProbMetric: 27.7174 - val_loss: 28.0642 - val_MinusLogProbMetric: 28.0642 - lr: 1.5625e-05 - 34s/epoch - 173ms/step
Epoch 899/1000
2023-10-25 00:15:42.434 
Epoch 899/1000 
	 loss: 27.7173, MinusLogProbMetric: 27.7173, val_loss: 28.0645, val_MinusLogProbMetric: 28.0645

Epoch 899: val_loss did not improve from 28.05819
196/196 - 34s - loss: 27.7173 - MinusLogProbMetric: 27.7173 - val_loss: 28.0645 - val_MinusLogProbMetric: 28.0645 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 900/1000
2023-10-25 00:16:16.916 
Epoch 900/1000 
	 loss: 27.7174, MinusLogProbMetric: 27.7174, val_loss: 28.0661, val_MinusLogProbMetric: 28.0661

Epoch 900: val_loss did not improve from 28.05819
196/196 - 34s - loss: 27.7174 - MinusLogProbMetric: 27.7174 - val_loss: 28.0661 - val_MinusLogProbMetric: 28.0661 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 901/1000
2023-10-25 00:16:51.664 
Epoch 901/1000 
	 loss: 27.7161, MinusLogProbMetric: 27.7161, val_loss: 28.0598, val_MinusLogProbMetric: 28.0598

Epoch 901: val_loss did not improve from 28.05819
196/196 - 35s - loss: 27.7161 - MinusLogProbMetric: 27.7161 - val_loss: 28.0598 - val_MinusLogProbMetric: 28.0598 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 902/1000
2023-10-25 00:17:26.265 
Epoch 902/1000 
	 loss: 27.7180, MinusLogProbMetric: 27.7180, val_loss: 28.0682, val_MinusLogProbMetric: 28.0682

Epoch 902: val_loss did not improve from 28.05819
196/196 - 35s - loss: 27.7180 - MinusLogProbMetric: 27.7180 - val_loss: 28.0682 - val_MinusLogProbMetric: 28.0682 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 903/1000
2023-10-25 00:18:01.047 
Epoch 903/1000 
	 loss: 27.7172, MinusLogProbMetric: 27.7172, val_loss: 28.0641, val_MinusLogProbMetric: 28.0641

Epoch 903: val_loss did not improve from 28.05819
196/196 - 35s - loss: 27.7172 - MinusLogProbMetric: 27.7172 - val_loss: 28.0641 - val_MinusLogProbMetric: 28.0641 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 904/1000
2023-10-25 00:18:35.631 
Epoch 904/1000 
	 loss: 27.7151, MinusLogProbMetric: 27.7151, val_loss: 28.0709, val_MinusLogProbMetric: 28.0709

Epoch 904: val_loss did not improve from 28.05819
196/196 - 35s - loss: 27.7151 - MinusLogProbMetric: 27.7151 - val_loss: 28.0709 - val_MinusLogProbMetric: 28.0709 - lr: 1.5625e-05 - 35s/epoch - 176ms/step
Epoch 905/1000
2023-10-25 00:19:10.205 
Epoch 905/1000 
	 loss: 27.7173, MinusLogProbMetric: 27.7173, val_loss: 28.0723, val_MinusLogProbMetric: 28.0723

Epoch 905: val_loss did not improve from 28.05819
196/196 - 35s - loss: 27.7173 - MinusLogProbMetric: 27.7173 - val_loss: 28.0723 - val_MinusLogProbMetric: 28.0723 - lr: 1.5625e-05 - 35s/epoch - 176ms/step
Epoch 906/1000
2023-10-25 00:19:44.729 
Epoch 906/1000 
	 loss: 27.7171, MinusLogProbMetric: 27.7171, val_loss: 28.0569, val_MinusLogProbMetric: 28.0569

Epoch 906: val_loss improved from 28.05819 to 28.05692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.7171 - MinusLogProbMetric: 27.7171 - val_loss: 28.0569 - val_MinusLogProbMetric: 28.0569 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 907/1000
2023-10-25 00:20:19.578 
Epoch 907/1000 
	 loss: 27.7176, MinusLogProbMetric: 27.7176, val_loss: 28.0678, val_MinusLogProbMetric: 28.0678

Epoch 907: val_loss did not improve from 28.05692
196/196 - 34s - loss: 27.7176 - MinusLogProbMetric: 27.7176 - val_loss: 28.0678 - val_MinusLogProbMetric: 28.0678 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 908/1000
2023-10-25 00:20:53.895 
Epoch 908/1000 
	 loss: 27.7173, MinusLogProbMetric: 27.7173, val_loss: 28.0694, val_MinusLogProbMetric: 28.0694

Epoch 908: val_loss did not improve from 28.05692
196/196 - 34s - loss: 27.7173 - MinusLogProbMetric: 27.7173 - val_loss: 28.0694 - val_MinusLogProbMetric: 28.0694 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 909/1000
2023-10-25 00:21:27.900 
Epoch 909/1000 
	 loss: 27.7173, MinusLogProbMetric: 27.7173, val_loss: 28.0671, val_MinusLogProbMetric: 28.0671

Epoch 909: val_loss did not improve from 28.05692
196/196 - 34s - loss: 27.7173 - MinusLogProbMetric: 27.7173 - val_loss: 28.0671 - val_MinusLogProbMetric: 28.0671 - lr: 1.5625e-05 - 34s/epoch - 173ms/step
Epoch 910/1000
2023-10-25 00:22:02.601 
Epoch 910/1000 
	 loss: 27.7158, MinusLogProbMetric: 27.7158, val_loss: 28.0648, val_MinusLogProbMetric: 28.0648

Epoch 910: val_loss did not improve from 28.05692
196/196 - 35s - loss: 27.7158 - MinusLogProbMetric: 27.7158 - val_loss: 28.0648 - val_MinusLogProbMetric: 28.0648 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 911/1000
2023-10-25 00:22:37.048 
Epoch 911/1000 
	 loss: 27.7173, MinusLogProbMetric: 27.7173, val_loss: 28.0651, val_MinusLogProbMetric: 28.0651

Epoch 911: val_loss did not improve from 28.05692
196/196 - 34s - loss: 27.7173 - MinusLogProbMetric: 27.7173 - val_loss: 28.0651 - val_MinusLogProbMetric: 28.0651 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 912/1000
2023-10-25 00:23:11.489 
Epoch 912/1000 
	 loss: 27.7173, MinusLogProbMetric: 27.7173, val_loss: 28.0683, val_MinusLogProbMetric: 28.0683

Epoch 912: val_loss did not improve from 28.05692
196/196 - 34s - loss: 27.7173 - MinusLogProbMetric: 27.7173 - val_loss: 28.0683 - val_MinusLogProbMetric: 28.0683 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 913/1000
2023-10-25 00:23:44.027 
Epoch 913/1000 
	 loss: 27.7172, MinusLogProbMetric: 27.7172, val_loss: 28.0653, val_MinusLogProbMetric: 28.0653

Epoch 913: val_loss did not improve from 28.05692
196/196 - 33s - loss: 27.7172 - MinusLogProbMetric: 27.7172 - val_loss: 28.0653 - val_MinusLogProbMetric: 28.0653 - lr: 1.5625e-05 - 33s/epoch - 166ms/step
Epoch 914/1000
2023-10-25 00:24:16.499 
Epoch 914/1000 
	 loss: 27.7166, MinusLogProbMetric: 27.7166, val_loss: 28.0703, val_MinusLogProbMetric: 28.0703

Epoch 914: val_loss did not improve from 28.05692
196/196 - 32s - loss: 27.7166 - MinusLogProbMetric: 27.7166 - val_loss: 28.0703 - val_MinusLogProbMetric: 28.0703 - lr: 1.5625e-05 - 32s/epoch - 166ms/step
Epoch 915/1000
2023-10-25 00:24:51.000 
Epoch 915/1000 
	 loss: 27.7168, MinusLogProbMetric: 27.7168, val_loss: 28.0648, val_MinusLogProbMetric: 28.0648

Epoch 915: val_loss did not improve from 28.05692
196/196 - 34s - loss: 27.7168 - MinusLogProbMetric: 27.7168 - val_loss: 28.0648 - val_MinusLogProbMetric: 28.0648 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 916/1000
2023-10-25 00:25:25.687 
Epoch 916/1000 
	 loss: 27.7162, MinusLogProbMetric: 27.7162, val_loss: 28.0645, val_MinusLogProbMetric: 28.0645

Epoch 916: val_loss did not improve from 28.05692
196/196 - 35s - loss: 27.7162 - MinusLogProbMetric: 27.7162 - val_loss: 28.0645 - val_MinusLogProbMetric: 28.0645 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 917/1000
2023-10-25 00:26:00.464 
Epoch 917/1000 
	 loss: 27.7172, MinusLogProbMetric: 27.7172, val_loss: 28.0774, val_MinusLogProbMetric: 28.0774

Epoch 917: val_loss did not improve from 28.05692
196/196 - 35s - loss: 27.7172 - MinusLogProbMetric: 27.7172 - val_loss: 28.0774 - val_MinusLogProbMetric: 28.0774 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 918/1000
2023-10-25 00:26:35.016 
Epoch 918/1000 
	 loss: 27.7166, MinusLogProbMetric: 27.7166, val_loss: 28.0668, val_MinusLogProbMetric: 28.0668

Epoch 918: val_loss did not improve from 28.05692
196/196 - 35s - loss: 27.7166 - MinusLogProbMetric: 27.7166 - val_loss: 28.0668 - val_MinusLogProbMetric: 28.0668 - lr: 1.5625e-05 - 35s/epoch - 176ms/step
Epoch 919/1000
2023-10-25 00:27:09.679 
Epoch 919/1000 
	 loss: 27.7156, MinusLogProbMetric: 27.7156, val_loss: 28.0638, val_MinusLogProbMetric: 28.0638

Epoch 919: val_loss did not improve from 28.05692
196/196 - 35s - loss: 27.7156 - MinusLogProbMetric: 27.7156 - val_loss: 28.0638 - val_MinusLogProbMetric: 28.0638 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 920/1000
2023-10-25 00:27:44.363 
Epoch 920/1000 
	 loss: 27.7169, MinusLogProbMetric: 27.7169, val_loss: 28.0587, val_MinusLogProbMetric: 28.0587

Epoch 920: val_loss did not improve from 28.05692
196/196 - 35s - loss: 27.7169 - MinusLogProbMetric: 27.7169 - val_loss: 28.0587 - val_MinusLogProbMetric: 28.0587 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 921/1000
2023-10-25 00:28:19.490 
Epoch 921/1000 
	 loss: 27.7162, MinusLogProbMetric: 27.7162, val_loss: 28.0707, val_MinusLogProbMetric: 28.0707

Epoch 921: val_loss did not improve from 28.05692
196/196 - 35s - loss: 27.7162 - MinusLogProbMetric: 27.7162 - val_loss: 28.0707 - val_MinusLogProbMetric: 28.0707 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 922/1000
2023-10-25 00:28:54.533 
Epoch 922/1000 
	 loss: 27.7158, MinusLogProbMetric: 27.7158, val_loss: 28.0642, val_MinusLogProbMetric: 28.0642

Epoch 922: val_loss did not improve from 28.05692
196/196 - 35s - loss: 27.7158 - MinusLogProbMetric: 27.7158 - val_loss: 28.0642 - val_MinusLogProbMetric: 28.0642 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 923/1000
2023-10-25 00:29:29.294 
Epoch 923/1000 
	 loss: 27.7165, MinusLogProbMetric: 27.7165, val_loss: 28.0556, val_MinusLogProbMetric: 28.0556

Epoch 923: val_loss improved from 28.05692 to 28.05557, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.7165 - MinusLogProbMetric: 27.7165 - val_loss: 28.0556 - val_MinusLogProbMetric: 28.0556 - lr: 1.5625e-05 - 35s/epoch - 180ms/step
Epoch 924/1000
2023-10-25 00:30:04.126 
Epoch 924/1000 
	 loss: 27.7153, MinusLogProbMetric: 27.7153, val_loss: 28.0628, val_MinusLogProbMetric: 28.0628

Epoch 924: val_loss did not improve from 28.05557
196/196 - 34s - loss: 27.7153 - MinusLogProbMetric: 27.7153 - val_loss: 28.0628 - val_MinusLogProbMetric: 28.0628 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 925/1000
2023-10-25 00:30:38.773 
Epoch 925/1000 
	 loss: 27.7158, MinusLogProbMetric: 27.7158, val_loss: 28.0596, val_MinusLogProbMetric: 28.0596

Epoch 925: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7158 - MinusLogProbMetric: 27.7158 - val_loss: 28.0596 - val_MinusLogProbMetric: 28.0596 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 926/1000
2023-10-25 00:31:12.932 
Epoch 926/1000 
	 loss: 27.7156, MinusLogProbMetric: 27.7156, val_loss: 28.0678, val_MinusLogProbMetric: 28.0678

Epoch 926: val_loss did not improve from 28.05557
196/196 - 34s - loss: 27.7156 - MinusLogProbMetric: 27.7156 - val_loss: 28.0678 - val_MinusLogProbMetric: 28.0678 - lr: 1.5625e-05 - 34s/epoch - 174ms/step
Epoch 927/1000
2023-10-25 00:31:47.712 
Epoch 927/1000 
	 loss: 27.7163, MinusLogProbMetric: 27.7163, val_loss: 28.0648, val_MinusLogProbMetric: 28.0648

Epoch 927: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7163 - MinusLogProbMetric: 27.7163 - val_loss: 28.0648 - val_MinusLogProbMetric: 28.0648 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 928/1000
2023-10-25 00:32:22.242 
Epoch 928/1000 
	 loss: 27.7157, MinusLogProbMetric: 27.7157, val_loss: 28.0562, val_MinusLogProbMetric: 28.0562

Epoch 928: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7157 - MinusLogProbMetric: 27.7157 - val_loss: 28.0562 - val_MinusLogProbMetric: 28.0562 - lr: 1.5625e-05 - 35s/epoch - 176ms/step
Epoch 929/1000
2023-10-25 00:32:56.692 
Epoch 929/1000 
	 loss: 27.7152, MinusLogProbMetric: 27.7152, val_loss: 28.0648, val_MinusLogProbMetric: 28.0648

Epoch 929: val_loss did not improve from 28.05557
196/196 - 34s - loss: 27.7152 - MinusLogProbMetric: 27.7152 - val_loss: 28.0648 - val_MinusLogProbMetric: 28.0648 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 930/1000
2023-10-25 00:33:31.170 
Epoch 930/1000 
	 loss: 27.7152, MinusLogProbMetric: 27.7152, val_loss: 28.0622, val_MinusLogProbMetric: 28.0622

Epoch 930: val_loss did not improve from 28.05557
196/196 - 34s - loss: 27.7152 - MinusLogProbMetric: 27.7152 - val_loss: 28.0622 - val_MinusLogProbMetric: 28.0622 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 931/1000
2023-10-25 00:34:05.522 
Epoch 931/1000 
	 loss: 27.7156, MinusLogProbMetric: 27.7156, val_loss: 28.0610, val_MinusLogProbMetric: 28.0610

Epoch 931: val_loss did not improve from 28.05557
196/196 - 34s - loss: 27.7156 - MinusLogProbMetric: 27.7156 - val_loss: 28.0610 - val_MinusLogProbMetric: 28.0610 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 932/1000
2023-10-25 00:34:40.176 
Epoch 932/1000 
	 loss: 27.7152, MinusLogProbMetric: 27.7152, val_loss: 28.0629, val_MinusLogProbMetric: 28.0629

Epoch 932: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7152 - MinusLogProbMetric: 27.7152 - val_loss: 28.0629 - val_MinusLogProbMetric: 28.0629 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 933/1000
2023-10-25 00:35:15.242 
Epoch 933/1000 
	 loss: 27.7150, MinusLogProbMetric: 27.7150, val_loss: 28.0598, val_MinusLogProbMetric: 28.0598

Epoch 933: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7150 - MinusLogProbMetric: 27.7150 - val_loss: 28.0598 - val_MinusLogProbMetric: 28.0598 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 934/1000
2023-10-25 00:35:50.162 
Epoch 934/1000 
	 loss: 27.7155, MinusLogProbMetric: 27.7155, val_loss: 28.0650, val_MinusLogProbMetric: 28.0650

Epoch 934: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7155 - MinusLogProbMetric: 27.7155 - val_loss: 28.0650 - val_MinusLogProbMetric: 28.0650 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 935/1000
2023-10-25 00:36:25.146 
Epoch 935/1000 
	 loss: 27.7155, MinusLogProbMetric: 27.7155, val_loss: 28.0611, val_MinusLogProbMetric: 28.0611

Epoch 935: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7155 - MinusLogProbMetric: 27.7155 - val_loss: 28.0611 - val_MinusLogProbMetric: 28.0611 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 936/1000
2023-10-25 00:36:59.884 
Epoch 936/1000 
	 loss: 27.7157, MinusLogProbMetric: 27.7157, val_loss: 28.0642, val_MinusLogProbMetric: 28.0642

Epoch 936: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7157 - MinusLogProbMetric: 27.7157 - val_loss: 28.0642 - val_MinusLogProbMetric: 28.0642 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 937/1000
2023-10-25 00:37:34.943 
Epoch 937/1000 
	 loss: 27.7156, MinusLogProbMetric: 27.7156, val_loss: 28.0663, val_MinusLogProbMetric: 28.0663

Epoch 937: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7156 - MinusLogProbMetric: 27.7156 - val_loss: 28.0663 - val_MinusLogProbMetric: 28.0663 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 938/1000
2023-10-25 00:38:09.390 
Epoch 938/1000 
	 loss: 27.7152, MinusLogProbMetric: 27.7152, val_loss: 28.0613, val_MinusLogProbMetric: 28.0613

Epoch 938: val_loss did not improve from 28.05557
196/196 - 34s - loss: 27.7152 - MinusLogProbMetric: 27.7152 - val_loss: 28.0613 - val_MinusLogProbMetric: 28.0613 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 939/1000
2023-10-25 00:38:44.145 
Epoch 939/1000 
	 loss: 27.7157, MinusLogProbMetric: 27.7157, val_loss: 28.0588, val_MinusLogProbMetric: 28.0588

Epoch 939: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7157 - MinusLogProbMetric: 27.7157 - val_loss: 28.0588 - val_MinusLogProbMetric: 28.0588 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 940/1000
2023-10-25 00:39:18.607 
Epoch 940/1000 
	 loss: 27.7153, MinusLogProbMetric: 27.7153, val_loss: 28.0585, val_MinusLogProbMetric: 28.0585

Epoch 940: val_loss did not improve from 28.05557
196/196 - 34s - loss: 27.7153 - MinusLogProbMetric: 27.7153 - val_loss: 28.0585 - val_MinusLogProbMetric: 28.0585 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 941/1000
2023-10-25 00:39:53.275 
Epoch 941/1000 
	 loss: 27.7156, MinusLogProbMetric: 27.7156, val_loss: 28.0643, val_MinusLogProbMetric: 28.0643

Epoch 941: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7156 - MinusLogProbMetric: 27.7156 - val_loss: 28.0643 - val_MinusLogProbMetric: 28.0643 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 942/1000
2023-10-25 00:40:28.134 
Epoch 942/1000 
	 loss: 27.7148, MinusLogProbMetric: 27.7148, val_loss: 28.0641, val_MinusLogProbMetric: 28.0641

Epoch 942: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7148 - MinusLogProbMetric: 27.7148 - val_loss: 28.0641 - val_MinusLogProbMetric: 28.0641 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 943/1000
2023-10-25 00:41:02.298 
Epoch 943/1000 
	 loss: 27.7166, MinusLogProbMetric: 27.7166, val_loss: 28.0585, val_MinusLogProbMetric: 28.0585

Epoch 943: val_loss did not improve from 28.05557
196/196 - 34s - loss: 27.7166 - MinusLogProbMetric: 27.7166 - val_loss: 28.0585 - val_MinusLogProbMetric: 28.0585 - lr: 1.5625e-05 - 34s/epoch - 174ms/step
Epoch 944/1000
2023-10-25 00:41:33.716 
Epoch 944/1000 
	 loss: 27.7156, MinusLogProbMetric: 27.7156, val_loss: 28.0698, val_MinusLogProbMetric: 28.0698

Epoch 944: val_loss did not improve from 28.05557
196/196 - 31s - loss: 27.7156 - MinusLogProbMetric: 27.7156 - val_loss: 28.0698 - val_MinusLogProbMetric: 28.0698 - lr: 1.5625e-05 - 31s/epoch - 160ms/step
Epoch 945/1000
2023-10-25 00:42:08.398 
Epoch 945/1000 
	 loss: 27.7160, MinusLogProbMetric: 27.7160, val_loss: 28.0695, val_MinusLogProbMetric: 28.0695

Epoch 945: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7160 - MinusLogProbMetric: 27.7160 - val_loss: 28.0695 - val_MinusLogProbMetric: 28.0695 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 946/1000
2023-10-25 00:42:42.660 
Epoch 946/1000 
	 loss: 27.7154, MinusLogProbMetric: 27.7154, val_loss: 28.0621, val_MinusLogProbMetric: 28.0621

Epoch 946: val_loss did not improve from 28.05557
196/196 - 34s - loss: 27.7154 - MinusLogProbMetric: 27.7154 - val_loss: 28.0621 - val_MinusLogProbMetric: 28.0621 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 947/1000
2023-10-25 00:43:17.142 
Epoch 947/1000 
	 loss: 27.7157, MinusLogProbMetric: 27.7157, val_loss: 28.0687, val_MinusLogProbMetric: 28.0687

Epoch 947: val_loss did not improve from 28.05557
196/196 - 34s - loss: 27.7157 - MinusLogProbMetric: 27.7157 - val_loss: 28.0687 - val_MinusLogProbMetric: 28.0687 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 948/1000
2023-10-25 00:43:51.580 
Epoch 948/1000 
	 loss: 27.7164, MinusLogProbMetric: 27.7164, val_loss: 28.0636, val_MinusLogProbMetric: 28.0636

Epoch 948: val_loss did not improve from 28.05557
196/196 - 34s - loss: 27.7164 - MinusLogProbMetric: 27.7164 - val_loss: 28.0636 - val_MinusLogProbMetric: 28.0636 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 949/1000
2023-10-25 00:44:26.459 
Epoch 949/1000 
	 loss: 27.7149, MinusLogProbMetric: 27.7149, val_loss: 28.0647, val_MinusLogProbMetric: 28.0647

Epoch 949: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7149 - MinusLogProbMetric: 27.7149 - val_loss: 28.0647 - val_MinusLogProbMetric: 28.0647 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 950/1000
2023-10-25 00:45:01.239 
Epoch 950/1000 
	 loss: 27.7158, MinusLogProbMetric: 27.7158, val_loss: 28.0603, val_MinusLogProbMetric: 28.0603

Epoch 950: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7158 - MinusLogProbMetric: 27.7158 - val_loss: 28.0603 - val_MinusLogProbMetric: 28.0603 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 951/1000
2023-10-25 00:45:35.897 
Epoch 951/1000 
	 loss: 27.7169, MinusLogProbMetric: 27.7169, val_loss: 28.0621, val_MinusLogProbMetric: 28.0621

Epoch 951: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7169 - MinusLogProbMetric: 27.7169 - val_loss: 28.0621 - val_MinusLogProbMetric: 28.0621 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 952/1000
2023-10-25 00:46:10.717 
Epoch 952/1000 
	 loss: 27.7159, MinusLogProbMetric: 27.7159, val_loss: 28.0584, val_MinusLogProbMetric: 28.0584

Epoch 952: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7159 - MinusLogProbMetric: 27.7159 - val_loss: 28.0584 - val_MinusLogProbMetric: 28.0584 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 953/1000
2023-10-25 00:46:45.357 
Epoch 953/1000 
	 loss: 27.7146, MinusLogProbMetric: 27.7146, val_loss: 28.0598, val_MinusLogProbMetric: 28.0598

Epoch 953: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7146 - MinusLogProbMetric: 27.7146 - val_loss: 28.0598 - val_MinusLogProbMetric: 28.0598 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 954/1000
2023-10-25 00:47:20.473 
Epoch 954/1000 
	 loss: 27.7162, MinusLogProbMetric: 27.7162, val_loss: 28.0589, val_MinusLogProbMetric: 28.0589

Epoch 954: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7162 - MinusLogProbMetric: 27.7162 - val_loss: 28.0589 - val_MinusLogProbMetric: 28.0589 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 955/1000
2023-10-25 00:47:55.137 
Epoch 955/1000 
	 loss: 27.7154, MinusLogProbMetric: 27.7154, val_loss: 28.0647, val_MinusLogProbMetric: 28.0647

Epoch 955: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7154 - MinusLogProbMetric: 27.7154 - val_loss: 28.0647 - val_MinusLogProbMetric: 28.0647 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 956/1000
2023-10-25 00:48:29.987 
Epoch 956/1000 
	 loss: 27.7152, MinusLogProbMetric: 27.7152, val_loss: 28.0630, val_MinusLogProbMetric: 28.0630

Epoch 956: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7152 - MinusLogProbMetric: 27.7152 - val_loss: 28.0630 - val_MinusLogProbMetric: 28.0630 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 957/1000
2023-10-25 00:49:04.488 
Epoch 957/1000 
	 loss: 27.7169, MinusLogProbMetric: 27.7169, val_loss: 28.0616, val_MinusLogProbMetric: 28.0616

Epoch 957: val_loss did not improve from 28.05557
196/196 - 34s - loss: 27.7169 - MinusLogProbMetric: 27.7169 - val_loss: 28.0616 - val_MinusLogProbMetric: 28.0616 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 958/1000
2023-10-25 00:49:39.434 
Epoch 958/1000 
	 loss: 27.7155, MinusLogProbMetric: 27.7155, val_loss: 28.0576, val_MinusLogProbMetric: 28.0576

Epoch 958: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7155 - MinusLogProbMetric: 27.7155 - val_loss: 28.0576 - val_MinusLogProbMetric: 28.0576 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 959/1000
2023-10-25 00:50:13.847 
Epoch 959/1000 
	 loss: 27.7163, MinusLogProbMetric: 27.7163, val_loss: 28.0632, val_MinusLogProbMetric: 28.0632

Epoch 959: val_loss did not improve from 28.05557
196/196 - 34s - loss: 27.7163 - MinusLogProbMetric: 27.7163 - val_loss: 28.0632 - val_MinusLogProbMetric: 28.0632 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 960/1000
2023-10-25 00:50:48.574 
Epoch 960/1000 
	 loss: 27.7162, MinusLogProbMetric: 27.7162, val_loss: 28.0623, val_MinusLogProbMetric: 28.0623

Epoch 960: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7162 - MinusLogProbMetric: 27.7162 - val_loss: 28.0623 - val_MinusLogProbMetric: 28.0623 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 961/1000
2023-10-25 00:51:22.922 
Epoch 961/1000 
	 loss: 27.7145, MinusLogProbMetric: 27.7145, val_loss: 28.0624, val_MinusLogProbMetric: 28.0624

Epoch 961: val_loss did not improve from 28.05557
196/196 - 34s - loss: 27.7145 - MinusLogProbMetric: 27.7145 - val_loss: 28.0624 - val_MinusLogProbMetric: 28.0624 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 962/1000
2023-10-25 00:51:57.527 
Epoch 962/1000 
	 loss: 27.7163, MinusLogProbMetric: 27.7163, val_loss: 28.0665, val_MinusLogProbMetric: 28.0665

Epoch 962: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7163 - MinusLogProbMetric: 27.7163 - val_loss: 28.0665 - val_MinusLogProbMetric: 28.0665 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 963/1000
2023-10-25 00:52:32.241 
Epoch 963/1000 
	 loss: 27.7145, MinusLogProbMetric: 27.7145, val_loss: 28.0625, val_MinusLogProbMetric: 28.0625

Epoch 963: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7145 - MinusLogProbMetric: 27.7145 - val_loss: 28.0625 - val_MinusLogProbMetric: 28.0625 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 964/1000
2023-10-25 00:53:06.936 
Epoch 964/1000 
	 loss: 27.7146, MinusLogProbMetric: 27.7146, val_loss: 28.0727, val_MinusLogProbMetric: 28.0727

Epoch 964: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7146 - MinusLogProbMetric: 27.7146 - val_loss: 28.0727 - val_MinusLogProbMetric: 28.0727 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 965/1000
2023-10-25 00:53:41.203 
Epoch 965/1000 
	 loss: 27.7149, MinusLogProbMetric: 27.7149, val_loss: 28.0576, val_MinusLogProbMetric: 28.0576

Epoch 965: val_loss did not improve from 28.05557
196/196 - 34s - loss: 27.7149 - MinusLogProbMetric: 27.7149 - val_loss: 28.0576 - val_MinusLogProbMetric: 28.0576 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 966/1000
2023-10-25 00:54:15.831 
Epoch 966/1000 
	 loss: 27.7141, MinusLogProbMetric: 27.7141, val_loss: 28.0576, val_MinusLogProbMetric: 28.0576

Epoch 966: val_loss did not improve from 28.05557
196/196 - 35s - loss: 27.7141 - MinusLogProbMetric: 27.7141 - val_loss: 28.0576 - val_MinusLogProbMetric: 28.0576 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 967/1000
2023-10-25 00:54:50.646 
Epoch 967/1000 
	 loss: 27.7151, MinusLogProbMetric: 27.7151, val_loss: 28.0531, val_MinusLogProbMetric: 28.0531

Epoch 967: val_loss improved from 28.05557 to 28.05307, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_345/weights/best_weights.h5
196/196 - 35s - loss: 27.7151 - MinusLogProbMetric: 27.7151 - val_loss: 28.0531 - val_MinusLogProbMetric: 28.0531 - lr: 1.5625e-05 - 35s/epoch - 180ms/step
Epoch 968/1000
2023-10-25 00:55:25.545 
Epoch 968/1000 
	 loss: 27.7147, MinusLogProbMetric: 27.7147, val_loss: 28.0576, val_MinusLogProbMetric: 28.0576

Epoch 968: val_loss did not improve from 28.05307
196/196 - 34s - loss: 27.7147 - MinusLogProbMetric: 27.7147 - val_loss: 28.0576 - val_MinusLogProbMetric: 28.0576 - lr: 1.5625e-05 - 34s/epoch - 175ms/step
Epoch 969/1000
2023-10-25 00:55:59.960 
Epoch 969/1000 
	 loss: 27.7157, MinusLogProbMetric: 27.7157, val_loss: 28.0617, val_MinusLogProbMetric: 28.0617

Epoch 969: val_loss did not improve from 28.05307
196/196 - 34s - loss: 27.7157 - MinusLogProbMetric: 27.7157 - val_loss: 28.0617 - val_MinusLogProbMetric: 28.0617 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 970/1000
2023-10-25 00:56:34.901 
Epoch 970/1000 
	 loss: 27.7140, MinusLogProbMetric: 27.7140, val_loss: 28.0631, val_MinusLogProbMetric: 28.0631

Epoch 970: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7140 - MinusLogProbMetric: 27.7140 - val_loss: 28.0631 - val_MinusLogProbMetric: 28.0631 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 971/1000
2023-10-25 00:57:10.020 
Epoch 971/1000 
	 loss: 27.7142, MinusLogProbMetric: 27.7142, val_loss: 28.0677, val_MinusLogProbMetric: 28.0677

Epoch 971: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7142 - MinusLogProbMetric: 27.7142 - val_loss: 28.0677 - val_MinusLogProbMetric: 28.0677 - lr: 1.5625e-05 - 35s/epoch - 179ms/step
Epoch 972/1000
2023-10-25 00:57:44.694 
Epoch 972/1000 
	 loss: 27.7147, MinusLogProbMetric: 27.7147, val_loss: 28.0615, val_MinusLogProbMetric: 28.0615

Epoch 972: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7147 - MinusLogProbMetric: 27.7147 - val_loss: 28.0615 - val_MinusLogProbMetric: 28.0615 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 973/1000
2023-10-25 00:58:18.624 
Epoch 973/1000 
	 loss: 27.7146, MinusLogProbMetric: 27.7146, val_loss: 28.0544, val_MinusLogProbMetric: 28.0544

Epoch 973: val_loss did not improve from 28.05307
196/196 - 34s - loss: 27.7146 - MinusLogProbMetric: 27.7146 - val_loss: 28.0544 - val_MinusLogProbMetric: 28.0544 - lr: 1.5625e-05 - 34s/epoch - 173ms/step
Epoch 974/1000
2023-10-25 00:58:53.330 
Epoch 974/1000 
	 loss: 27.7147, MinusLogProbMetric: 27.7147, val_loss: 28.0558, val_MinusLogProbMetric: 28.0558

Epoch 974: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7147 - MinusLogProbMetric: 27.7147 - val_loss: 28.0558 - val_MinusLogProbMetric: 28.0558 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 975/1000
2023-10-25 00:59:27.946 
Epoch 975/1000 
	 loss: 27.7139, MinusLogProbMetric: 27.7139, val_loss: 28.0553, val_MinusLogProbMetric: 28.0553

Epoch 975: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7139 - MinusLogProbMetric: 27.7139 - val_loss: 28.0553 - val_MinusLogProbMetric: 28.0553 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 976/1000
2023-10-25 01:00:01.950 
Epoch 976/1000 
	 loss: 27.7146, MinusLogProbMetric: 27.7146, val_loss: 28.0631, val_MinusLogProbMetric: 28.0631

Epoch 976: val_loss did not improve from 28.05307
196/196 - 34s - loss: 27.7146 - MinusLogProbMetric: 27.7146 - val_loss: 28.0631 - val_MinusLogProbMetric: 28.0631 - lr: 1.5625e-05 - 34s/epoch - 173ms/step
Epoch 977/1000
2023-10-25 01:00:36.578 
Epoch 977/1000 
	 loss: 27.7156, MinusLogProbMetric: 27.7156, val_loss: 28.0682, val_MinusLogProbMetric: 28.0682

Epoch 977: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7156 - MinusLogProbMetric: 27.7156 - val_loss: 28.0682 - val_MinusLogProbMetric: 28.0682 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 978/1000
2023-10-25 01:01:11.250 
Epoch 978/1000 
	 loss: 27.7160, MinusLogProbMetric: 27.7160, val_loss: 28.0644, val_MinusLogProbMetric: 28.0644

Epoch 978: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7160 - MinusLogProbMetric: 27.7160 - val_loss: 28.0644 - val_MinusLogProbMetric: 28.0644 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 979/1000
2023-10-25 01:01:45.900 
Epoch 979/1000 
	 loss: 27.7147, MinusLogProbMetric: 27.7147, val_loss: 28.0686, val_MinusLogProbMetric: 28.0686

Epoch 979: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7147 - MinusLogProbMetric: 27.7147 - val_loss: 28.0686 - val_MinusLogProbMetric: 28.0686 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 980/1000
2023-10-25 01:02:20.580 
Epoch 980/1000 
	 loss: 27.7153, MinusLogProbMetric: 27.7153, val_loss: 28.0596, val_MinusLogProbMetric: 28.0596

Epoch 980: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7153 - MinusLogProbMetric: 27.7153 - val_loss: 28.0596 - val_MinusLogProbMetric: 28.0596 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 981/1000
2023-10-25 01:02:55.357 
Epoch 981/1000 
	 loss: 27.7151, MinusLogProbMetric: 27.7151, val_loss: 28.0644, val_MinusLogProbMetric: 28.0644

Epoch 981: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7151 - MinusLogProbMetric: 27.7151 - val_loss: 28.0644 - val_MinusLogProbMetric: 28.0644 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 982/1000
2023-10-25 01:03:30.105 
Epoch 982/1000 
	 loss: 27.7141, MinusLogProbMetric: 27.7141, val_loss: 28.0605, val_MinusLogProbMetric: 28.0605

Epoch 982: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7141 - MinusLogProbMetric: 27.7141 - val_loss: 28.0605 - val_MinusLogProbMetric: 28.0605 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 983/1000
2023-10-25 01:04:04.787 
Epoch 983/1000 
	 loss: 27.7141, MinusLogProbMetric: 27.7141, val_loss: 28.0582, val_MinusLogProbMetric: 28.0582

Epoch 983: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7141 - MinusLogProbMetric: 27.7141 - val_loss: 28.0582 - val_MinusLogProbMetric: 28.0582 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 984/1000
2023-10-25 01:04:39.412 
Epoch 984/1000 
	 loss: 27.7157, MinusLogProbMetric: 27.7157, val_loss: 28.0654, val_MinusLogProbMetric: 28.0654

Epoch 984: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7157 - MinusLogProbMetric: 27.7157 - val_loss: 28.0654 - val_MinusLogProbMetric: 28.0654 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 985/1000
2023-10-25 01:05:13.900 
Epoch 985/1000 
	 loss: 27.7142, MinusLogProbMetric: 27.7142, val_loss: 28.0619, val_MinusLogProbMetric: 28.0619

Epoch 985: val_loss did not improve from 28.05307
196/196 - 34s - loss: 27.7142 - MinusLogProbMetric: 27.7142 - val_loss: 28.0619 - val_MinusLogProbMetric: 28.0619 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 986/1000
2023-10-25 01:05:48.315 
Epoch 986/1000 
	 loss: 27.7149, MinusLogProbMetric: 27.7149, val_loss: 28.0599, val_MinusLogProbMetric: 28.0599

Epoch 986: val_loss did not improve from 28.05307
196/196 - 34s - loss: 27.7149 - MinusLogProbMetric: 27.7149 - val_loss: 28.0599 - val_MinusLogProbMetric: 28.0599 - lr: 1.5625e-05 - 34s/epoch - 176ms/step
Epoch 987/1000
2023-10-25 01:06:22.476 
Epoch 987/1000 
	 loss: 27.7138, MinusLogProbMetric: 27.7138, val_loss: 28.0663, val_MinusLogProbMetric: 28.0663

Epoch 987: val_loss did not improve from 28.05307
196/196 - 34s - loss: 27.7138 - MinusLogProbMetric: 27.7138 - val_loss: 28.0663 - val_MinusLogProbMetric: 28.0663 - lr: 1.5625e-05 - 34s/epoch - 174ms/step
Epoch 988/1000
2023-10-25 01:06:57.174 
Epoch 988/1000 
	 loss: 27.7144, MinusLogProbMetric: 27.7144, val_loss: 28.0604, val_MinusLogProbMetric: 28.0604

Epoch 988: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7144 - MinusLogProbMetric: 27.7144 - val_loss: 28.0604 - val_MinusLogProbMetric: 28.0604 - lr: 1.5625e-05 - 35s/epoch - 177ms/step
Epoch 989/1000
2023-10-25 01:07:32.139 
Epoch 989/1000 
	 loss: 27.7141, MinusLogProbMetric: 27.7141, val_loss: 28.0552, val_MinusLogProbMetric: 28.0552

Epoch 989: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7141 - MinusLogProbMetric: 27.7141 - val_loss: 28.0552 - val_MinusLogProbMetric: 28.0552 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 990/1000
2023-10-25 01:08:07.090 
Epoch 990/1000 
	 loss: 27.7146, MinusLogProbMetric: 27.7146, val_loss: 28.0577, val_MinusLogProbMetric: 28.0577

Epoch 990: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7146 - MinusLogProbMetric: 27.7146 - val_loss: 28.0577 - val_MinusLogProbMetric: 28.0577 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 991/1000
2023-10-25 01:08:41.936 
Epoch 991/1000 
	 loss: 27.7151, MinusLogProbMetric: 27.7151, val_loss: 28.0591, val_MinusLogProbMetric: 28.0591

Epoch 991: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7151 - MinusLogProbMetric: 27.7151 - val_loss: 28.0591 - val_MinusLogProbMetric: 28.0591 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 992/1000
2023-10-25 01:09:15.868 
Epoch 992/1000 
	 loss: 27.7140, MinusLogProbMetric: 27.7140, val_loss: 28.0557, val_MinusLogProbMetric: 28.0557

Epoch 992: val_loss did not improve from 28.05307
196/196 - 34s - loss: 27.7140 - MinusLogProbMetric: 27.7140 - val_loss: 28.0557 - val_MinusLogProbMetric: 28.0557 - lr: 1.5625e-05 - 34s/epoch - 173ms/step
Epoch 993/1000
2023-10-25 01:09:50.021 
Epoch 993/1000 
	 loss: 27.7142, MinusLogProbMetric: 27.7142, val_loss: 28.0627, val_MinusLogProbMetric: 28.0627

Epoch 993: val_loss did not improve from 28.05307
196/196 - 34s - loss: 27.7142 - MinusLogProbMetric: 27.7142 - val_loss: 28.0627 - val_MinusLogProbMetric: 28.0627 - lr: 1.5625e-05 - 34s/epoch - 174ms/step
Epoch 994/1000
2023-10-25 01:10:24.926 
Epoch 994/1000 
	 loss: 27.7145, MinusLogProbMetric: 27.7145, val_loss: 28.0657, val_MinusLogProbMetric: 28.0657

Epoch 994: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7145 - MinusLogProbMetric: 27.7145 - val_loss: 28.0657 - val_MinusLogProbMetric: 28.0657 - lr: 1.5625e-05 - 35s/epoch - 178ms/step
Epoch 995/1000
2023-10-25 01:10:58.390 
Epoch 995/1000 
	 loss: 27.7155, MinusLogProbMetric: 27.7155, val_loss: 28.0611, val_MinusLogProbMetric: 28.0611

Epoch 995: val_loss did not improve from 28.05307
196/196 - 33s - loss: 27.7155 - MinusLogProbMetric: 27.7155 - val_loss: 28.0611 - val_MinusLogProbMetric: 28.0611 - lr: 1.5625e-05 - 33s/epoch - 171ms/step
Epoch 996/1000
2023-10-25 01:11:32.939 
Epoch 996/1000 
	 loss: 27.7146, MinusLogProbMetric: 27.7146, val_loss: 28.0596, val_MinusLogProbMetric: 28.0596

Epoch 996: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7146 - MinusLogProbMetric: 27.7146 - val_loss: 28.0596 - val_MinusLogProbMetric: 28.0596 - lr: 1.5625e-05 - 35s/epoch - 176ms/step
Epoch 997/1000
2023-10-25 01:12:05.683 
Epoch 997/1000 
	 loss: 27.7148, MinusLogProbMetric: 27.7148, val_loss: 28.0653, val_MinusLogProbMetric: 28.0653

Epoch 997: val_loss did not improve from 28.05307
196/196 - 33s - loss: 27.7148 - MinusLogProbMetric: 27.7148 - val_loss: 28.0653 - val_MinusLogProbMetric: 28.0653 - lr: 1.5625e-05 - 33s/epoch - 167ms/step
Epoch 998/1000
2023-10-25 01:12:39.562 
Epoch 998/1000 
	 loss: 27.7149, MinusLogProbMetric: 27.7149, val_loss: 28.0659, val_MinusLogProbMetric: 28.0659

Epoch 998: val_loss did not improve from 28.05307
196/196 - 34s - loss: 27.7149 - MinusLogProbMetric: 27.7149 - val_loss: 28.0659 - val_MinusLogProbMetric: 28.0659 - lr: 1.5625e-05 - 34s/epoch - 173ms/step
Epoch 999/1000
2023-10-25 01:13:14.762 
Epoch 999/1000 
	 loss: 27.7153, MinusLogProbMetric: 27.7153, val_loss: 28.0601, val_MinusLogProbMetric: 28.0601

Epoch 999: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7153 - MinusLogProbMetric: 27.7153 - val_loss: 28.0601 - val_MinusLogProbMetric: 28.0601 - lr: 1.5625e-05 - 35s/epoch - 180ms/step
Epoch 1000/1000
2023-10-25 01:13:49.349 
Epoch 1000/1000 
	 loss: 27.7141, MinusLogProbMetric: 27.7141, val_loss: 28.0730, val_MinusLogProbMetric: 28.0730

Epoch 1000: val_loss did not improve from 28.05307
196/196 - 35s - loss: 27.7141 - MinusLogProbMetric: 27.7141 - val_loss: 28.0730 - val_MinusLogProbMetric: 28.0730 - lr: 1.5625e-05 - 35s/epoch - 176ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 440.
Model trained in 34347.28 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.64 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 0.89 s.
===========
Run 345/720 done in 34352.09 s.
===========

Directory ../../results/CsplineN_new/run_346/ already exists.
Skipping it.
===========
Run 346/720 already exists. Skipping it.
===========

===========
Generating train data for run 347.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_347/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_347/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_347/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_347
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_56"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_57 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  908640    
 yer)                                                            
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7f18010ac610>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1e8810bdc0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1e8810bdc0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f18010acbe0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f16c83117b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f16c83121d0>, <keras.callbacks.ModelCheckpoint object at 0x7f16c83122c0>, <keras.callbacks.EarlyStopping object at 0x7f16c8310490>, <keras.callbacks.ReduceLROnPlateau object at 0x7f16c8310790>, <keras.callbacks.TerminateOnNaN object at 0x7f16c83123b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_347/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 347/720 with hyperparameters:
timestamp = 2023-10-25 01:14:00.114740
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-10-25 01:15:44.637 
Epoch 1/1000 
	 loss: 1161.8937, MinusLogProbMetric: 1161.8937, val_loss: 458.4743, val_MinusLogProbMetric: 458.4743

Epoch 1: val_loss improved from inf to 458.47427, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 105s - loss: 1161.8937 - MinusLogProbMetric: 1161.8937 - val_loss: 458.4743 - val_MinusLogProbMetric: 458.4743 - lr: 0.0010 - 105s/epoch - 535ms/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 164: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 01:16:22.017 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 359.6231, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 458.47427
196/196 - 37s - loss: nan - MinusLogProbMetric: 359.6231 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 37s/epoch - 186ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 347.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_347/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_347/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_347/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_347
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_62"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_63 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  908640    
 yer)                                                            
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7f19706a6740>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f164c29b580>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f164c29b580>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f164c270c10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f164c272da0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f164c273250>, <keras.callbacks.ModelCheckpoint object at 0x7f164c270190>, <keras.callbacks.EarlyStopping object at 0x7f164c271270>, <keras.callbacks.ReduceLROnPlateau object at 0x7f164c271510>, <keras.callbacks.TerminateOnNaN object at 0x7f164c271630>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 347/720 with hyperparameters:
timestamp = 2023-10-25 01:16:28.984563
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-10-25 01:18:15.666 
Epoch 1/1000 
	 loss: 209.7841, MinusLogProbMetric: 209.7841, val_loss: 126.3560, val_MinusLogProbMetric: 126.3560

Epoch 1: val_loss improved from inf to 126.35604, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 107s - loss: 209.7841 - MinusLogProbMetric: 209.7841 - val_loss: 126.3560 - val_MinusLogProbMetric: 126.3560 - lr: 3.3333e-04 - 107s/epoch - 547ms/step
Epoch 2/1000
2023-10-25 01:18:58.472 
Epoch 2/1000 
	 loss: 108.3018, MinusLogProbMetric: 108.3018, val_loss: 122.1459, val_MinusLogProbMetric: 122.1459

Epoch 2: val_loss improved from 126.35604 to 122.14594, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 108.3018 - MinusLogProbMetric: 108.3018 - val_loss: 122.1459 - val_MinusLogProbMetric: 122.1459 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 3/1000
2023-10-25 01:19:41.383 
Epoch 3/1000 
	 loss: 83.2891, MinusLogProbMetric: 83.2891, val_loss: 72.1223, val_MinusLogProbMetric: 72.1223

Epoch 3: val_loss improved from 122.14594 to 72.12231, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 83.2891 - MinusLogProbMetric: 83.2891 - val_loss: 72.1223 - val_MinusLogProbMetric: 72.1223 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 4/1000
2023-10-25 01:20:24.132 
Epoch 4/1000 
	 loss: 67.1151, MinusLogProbMetric: 67.1151, val_loss: 63.8004, val_MinusLogProbMetric: 63.8004

Epoch 4: val_loss improved from 72.12231 to 63.80043, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 67.1151 - MinusLogProbMetric: 67.1151 - val_loss: 63.8004 - val_MinusLogProbMetric: 63.8004 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 5/1000
2023-10-25 01:21:06.853 
Epoch 5/1000 
	 loss: 59.0688, MinusLogProbMetric: 59.0688, val_loss: 56.2816, val_MinusLogProbMetric: 56.2816

Epoch 5: val_loss improved from 63.80043 to 56.28158, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 59.0688 - MinusLogProbMetric: 59.0688 - val_loss: 56.2816 - val_MinusLogProbMetric: 56.2816 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 6/1000
2023-10-25 01:21:50.082 
Epoch 6/1000 
	 loss: 53.9917, MinusLogProbMetric: 53.9917, val_loss: 54.1339, val_MinusLogProbMetric: 54.1339

Epoch 6: val_loss improved from 56.28158 to 54.13389, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 53.9917 - MinusLogProbMetric: 53.9917 - val_loss: 54.1339 - val_MinusLogProbMetric: 54.1339 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 7/1000
2023-10-25 01:22:33.267 
Epoch 7/1000 
	 loss: 50.7230, MinusLogProbMetric: 50.7230, val_loss: 49.0049, val_MinusLogProbMetric: 49.0049

Epoch 7: val_loss improved from 54.13389 to 49.00490, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 50.7230 - MinusLogProbMetric: 50.7230 - val_loss: 49.0049 - val_MinusLogProbMetric: 49.0049 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 8/1000
2023-10-25 01:23:15.684 
Epoch 8/1000 
	 loss: 47.4707, MinusLogProbMetric: 47.4707, val_loss: 45.8360, val_MinusLogProbMetric: 45.8360

Epoch 8: val_loss improved from 49.00490 to 45.83604, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 47.4707 - MinusLogProbMetric: 47.4707 - val_loss: 45.8360 - val_MinusLogProbMetric: 45.8360 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 9/1000
2023-10-25 01:23:57.393 
Epoch 9/1000 
	 loss: 45.8343, MinusLogProbMetric: 45.8343, val_loss: 43.1278, val_MinusLogProbMetric: 43.1278

Epoch 9: val_loss improved from 45.83604 to 43.12780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 45.8343 - MinusLogProbMetric: 45.8343 - val_loss: 43.1278 - val_MinusLogProbMetric: 43.1278 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 10/1000
2023-10-25 01:24:39.236 
Epoch 10/1000 
	 loss: 44.1222, MinusLogProbMetric: 44.1222, val_loss: 43.8073, val_MinusLogProbMetric: 43.8073

Epoch 10: val_loss did not improve from 43.12780
196/196 - 41s - loss: 44.1222 - MinusLogProbMetric: 44.1222 - val_loss: 43.8073 - val_MinusLogProbMetric: 43.8073 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 11/1000
2023-10-25 01:25:20.865 
Epoch 11/1000 
	 loss: 42.4979, MinusLogProbMetric: 42.4979, val_loss: 44.9580, val_MinusLogProbMetric: 44.9580

Epoch 11: val_loss did not improve from 43.12780
196/196 - 42s - loss: 42.4979 - MinusLogProbMetric: 42.4979 - val_loss: 44.9580 - val_MinusLogProbMetric: 44.9580 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 12/1000
2023-10-25 01:26:03.068 
Epoch 12/1000 
	 loss: 42.1271, MinusLogProbMetric: 42.1271, val_loss: 42.9998, val_MinusLogProbMetric: 42.9998

Epoch 12: val_loss improved from 43.12780 to 42.99978, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 42.1271 - MinusLogProbMetric: 42.1271 - val_loss: 42.9998 - val_MinusLogProbMetric: 42.9998 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 13/1000
2023-10-25 01:26:45.943 
Epoch 13/1000 
	 loss: 40.4464, MinusLogProbMetric: 40.4464, val_loss: 40.0436, val_MinusLogProbMetric: 40.0436

Epoch 13: val_loss improved from 42.99978 to 40.04356, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 40.4464 - MinusLogProbMetric: 40.4464 - val_loss: 40.0436 - val_MinusLogProbMetric: 40.0436 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 14/1000
2023-10-25 01:27:29.749 
Epoch 14/1000 
	 loss: 40.9933, MinusLogProbMetric: 40.9933, val_loss: 40.7281, val_MinusLogProbMetric: 40.7281

Epoch 14: val_loss did not improve from 40.04356
196/196 - 43s - loss: 40.9933 - MinusLogProbMetric: 40.9933 - val_loss: 40.7281 - val_MinusLogProbMetric: 40.7281 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 15/1000
2023-10-25 01:28:12.869 
Epoch 15/1000 
	 loss: 38.9987, MinusLogProbMetric: 38.9987, val_loss: 39.5378, val_MinusLogProbMetric: 39.5378

Epoch 15: val_loss improved from 40.04356 to 39.53784, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 38.9987 - MinusLogProbMetric: 38.9987 - val_loss: 39.5378 - val_MinusLogProbMetric: 39.5378 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 16/1000
2023-10-25 01:28:55.623 
Epoch 16/1000 
	 loss: 38.3572, MinusLogProbMetric: 38.3572, val_loss: 38.7881, val_MinusLogProbMetric: 38.7881

Epoch 16: val_loss improved from 39.53784 to 38.78811, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 38.3572 - MinusLogProbMetric: 38.3572 - val_loss: 38.7881 - val_MinusLogProbMetric: 38.7881 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 17/1000
2023-10-25 01:29:37.920 
Epoch 17/1000 
	 loss: 37.8511, MinusLogProbMetric: 37.8511, val_loss: 39.1677, val_MinusLogProbMetric: 39.1677

Epoch 17: val_loss did not improve from 38.78811
196/196 - 41s - loss: 37.8511 - MinusLogProbMetric: 37.8511 - val_loss: 39.1677 - val_MinusLogProbMetric: 39.1677 - lr: 3.3333e-04 - 41s/epoch - 212ms/step
Epoch 18/1000
2023-10-25 01:30:20.156 
Epoch 18/1000 
	 loss: 37.4290, MinusLogProbMetric: 37.4290, val_loss: 37.2311, val_MinusLogProbMetric: 37.2311

Epoch 18: val_loss improved from 38.78811 to 37.23108, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 37.4290 - MinusLogProbMetric: 37.4290 - val_loss: 37.2311 - val_MinusLogProbMetric: 37.2311 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 19/1000
2023-10-25 01:31:03.298 
Epoch 19/1000 
	 loss: 36.9834, MinusLogProbMetric: 36.9834, val_loss: 37.8785, val_MinusLogProbMetric: 37.8785

Epoch 19: val_loss did not improve from 37.23108
196/196 - 42s - loss: 36.9834 - MinusLogProbMetric: 36.9834 - val_loss: 37.8785 - val_MinusLogProbMetric: 37.8785 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 20/1000
2023-10-25 01:31:45.448 
Epoch 20/1000 
	 loss: 36.5593, MinusLogProbMetric: 36.5593, val_loss: 37.0335, val_MinusLogProbMetric: 37.0335

Epoch 20: val_loss improved from 37.23108 to 37.03349, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 36.5593 - MinusLogProbMetric: 36.5593 - val_loss: 37.0335 - val_MinusLogProbMetric: 37.0335 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 21/1000
2023-10-25 01:32:28.007 
Epoch 21/1000 
	 loss: 36.0247, MinusLogProbMetric: 36.0247, val_loss: 36.5056, val_MinusLogProbMetric: 36.5056

Epoch 21: val_loss improved from 37.03349 to 36.50557, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 36.0247 - MinusLogProbMetric: 36.0247 - val_loss: 36.5056 - val_MinusLogProbMetric: 36.5056 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 22/1000
2023-10-25 01:33:10.708 
Epoch 22/1000 
	 loss: 36.0723, MinusLogProbMetric: 36.0723, val_loss: 36.2928, val_MinusLogProbMetric: 36.2928

Epoch 22: val_loss improved from 36.50557 to 36.29284, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 36.0723 - MinusLogProbMetric: 36.0723 - val_loss: 36.2928 - val_MinusLogProbMetric: 36.2928 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 23/1000
2023-10-25 01:33:51.973 
Epoch 23/1000 
	 loss: 35.2979, MinusLogProbMetric: 35.2979, val_loss: 36.2438, val_MinusLogProbMetric: 36.2438

Epoch 23: val_loss improved from 36.29284 to 36.24382, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 41s - loss: 35.2979 - MinusLogProbMetric: 35.2979 - val_loss: 36.2438 - val_MinusLogProbMetric: 36.2438 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 24/1000
2023-10-25 01:34:31.215 
Epoch 24/1000 
	 loss: 35.8953, MinusLogProbMetric: 35.8953, val_loss: 36.5127, val_MinusLogProbMetric: 36.5127

Epoch 24: val_loss did not improve from 36.24382
196/196 - 39s - loss: 35.8953 - MinusLogProbMetric: 35.8953 - val_loss: 36.5127 - val_MinusLogProbMetric: 36.5127 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 25/1000
2023-10-25 01:35:09.278 
Epoch 25/1000 
	 loss: 35.2912, MinusLogProbMetric: 35.2912, val_loss: 34.6127, val_MinusLogProbMetric: 34.6127

Epoch 25: val_loss improved from 36.24382 to 34.61273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 39s - loss: 35.2912 - MinusLogProbMetric: 35.2912 - val_loss: 34.6127 - val_MinusLogProbMetric: 34.6127 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 26/1000
2023-10-25 01:35:51.993 
Epoch 26/1000 
	 loss: 34.9430, MinusLogProbMetric: 34.9430, val_loss: 34.8504, val_MinusLogProbMetric: 34.8504

Epoch 26: val_loss did not improve from 34.61273
196/196 - 42s - loss: 34.9430 - MinusLogProbMetric: 34.9430 - val_loss: 34.8504 - val_MinusLogProbMetric: 34.8504 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 27/1000
2023-10-25 01:36:34.547 
Epoch 27/1000 
	 loss: 34.5432, MinusLogProbMetric: 34.5432, val_loss: 35.2306, val_MinusLogProbMetric: 35.2306

Epoch 27: val_loss did not improve from 34.61273
196/196 - 43s - loss: 34.5432 - MinusLogProbMetric: 34.5432 - val_loss: 35.2306 - val_MinusLogProbMetric: 35.2306 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 28/1000
2023-10-25 01:37:14.703 
Epoch 28/1000 
	 loss: 34.4791, MinusLogProbMetric: 34.4791, val_loss: 34.2666, val_MinusLogProbMetric: 34.2666

Epoch 28: val_loss improved from 34.61273 to 34.26662, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 41s - loss: 34.4791 - MinusLogProbMetric: 34.4791 - val_loss: 34.2666 - val_MinusLogProbMetric: 34.2666 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 29/1000
2023-10-25 01:37:56.209 
Epoch 29/1000 
	 loss: 34.2104, MinusLogProbMetric: 34.2104, val_loss: 34.6681, val_MinusLogProbMetric: 34.6681

Epoch 29: val_loss did not improve from 34.26662
196/196 - 41s - loss: 34.2104 - MinusLogProbMetric: 34.2104 - val_loss: 34.6681 - val_MinusLogProbMetric: 34.6681 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 30/1000
2023-10-25 01:38:38.267 
Epoch 30/1000 
	 loss: 34.2563, MinusLogProbMetric: 34.2563, val_loss: 34.4038, val_MinusLogProbMetric: 34.4038

Epoch 30: val_loss did not improve from 34.26662
196/196 - 42s - loss: 34.2563 - MinusLogProbMetric: 34.2563 - val_loss: 34.4038 - val_MinusLogProbMetric: 34.4038 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 31/1000
2023-10-25 01:39:20.674 
Epoch 31/1000 
	 loss: 33.7199, MinusLogProbMetric: 33.7199, val_loss: 34.3981, val_MinusLogProbMetric: 34.3981

Epoch 31: val_loss did not improve from 34.26662
196/196 - 42s - loss: 33.7199 - MinusLogProbMetric: 33.7199 - val_loss: 34.3981 - val_MinusLogProbMetric: 34.3981 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 32/1000
2023-10-25 01:40:03.169 
Epoch 32/1000 
	 loss: 33.7102, MinusLogProbMetric: 33.7102, val_loss: 33.4792, val_MinusLogProbMetric: 33.4792

Epoch 32: val_loss improved from 34.26662 to 33.47924, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 33.7102 - MinusLogProbMetric: 33.7102 - val_loss: 33.4792 - val_MinusLogProbMetric: 33.4792 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 33/1000
2023-10-25 01:40:46.642 
Epoch 33/1000 
	 loss: 33.2974, MinusLogProbMetric: 33.2974, val_loss: 34.0684, val_MinusLogProbMetric: 34.0684

Epoch 33: val_loss did not improve from 33.47924
196/196 - 43s - loss: 33.2974 - MinusLogProbMetric: 33.2974 - val_loss: 34.0684 - val_MinusLogProbMetric: 34.0684 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 34/1000
2023-10-25 01:41:28.941 
Epoch 34/1000 
	 loss: 33.5137, MinusLogProbMetric: 33.5137, val_loss: 35.8670, val_MinusLogProbMetric: 35.8670

Epoch 34: val_loss did not improve from 33.47924
196/196 - 42s - loss: 33.5137 - MinusLogProbMetric: 33.5137 - val_loss: 35.8670 - val_MinusLogProbMetric: 35.8670 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 35/1000
2023-10-25 01:42:11.240 
Epoch 35/1000 
	 loss: 33.1386, MinusLogProbMetric: 33.1386, val_loss: 33.4738, val_MinusLogProbMetric: 33.4738

Epoch 35: val_loss improved from 33.47924 to 33.47380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 33.1386 - MinusLogProbMetric: 33.1386 - val_loss: 33.4738 - val_MinusLogProbMetric: 33.4738 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 36/1000
2023-10-25 01:42:54.045 
Epoch 36/1000 
	 loss: 33.1552, MinusLogProbMetric: 33.1552, val_loss: 33.5220, val_MinusLogProbMetric: 33.5220

Epoch 36: val_loss did not improve from 33.47380
196/196 - 42s - loss: 33.1552 - MinusLogProbMetric: 33.1552 - val_loss: 33.5220 - val_MinusLogProbMetric: 33.5220 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 37/1000
2023-10-25 01:43:36.342 
Epoch 37/1000 
	 loss: 32.7363, MinusLogProbMetric: 32.7363, val_loss: 32.3993, val_MinusLogProbMetric: 32.3993

Epoch 37: val_loss improved from 33.47380 to 32.39927, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 32.7363 - MinusLogProbMetric: 32.7363 - val_loss: 32.3993 - val_MinusLogProbMetric: 32.3993 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 38/1000
2023-10-25 01:44:20.072 
Epoch 38/1000 
	 loss: 32.6902, MinusLogProbMetric: 32.6902, val_loss: 33.1759, val_MinusLogProbMetric: 33.1759

Epoch 38: val_loss did not improve from 32.39927
196/196 - 43s - loss: 32.6902 - MinusLogProbMetric: 32.6902 - val_loss: 33.1759 - val_MinusLogProbMetric: 33.1759 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 39/1000
2023-10-25 01:45:02.360 
Epoch 39/1000 
	 loss: 32.6704, MinusLogProbMetric: 32.6704, val_loss: 33.0706, val_MinusLogProbMetric: 33.0706

Epoch 39: val_loss did not improve from 32.39927
196/196 - 42s - loss: 32.6704 - MinusLogProbMetric: 32.6704 - val_loss: 33.0706 - val_MinusLogProbMetric: 33.0706 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 40/1000
2023-10-25 01:45:44.882 
Epoch 40/1000 
	 loss: 32.4837, MinusLogProbMetric: 32.4837, val_loss: 32.2761, val_MinusLogProbMetric: 32.2761

Epoch 40: val_loss improved from 32.39927 to 32.27608, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 32.4837 - MinusLogProbMetric: 32.4837 - val_loss: 32.2761 - val_MinusLogProbMetric: 32.2761 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 41/1000
2023-10-25 01:46:27.809 
Epoch 41/1000 
	 loss: 32.4444, MinusLogProbMetric: 32.4444, val_loss: 33.7858, val_MinusLogProbMetric: 33.7858

Epoch 41: val_loss did not improve from 32.27608
196/196 - 42s - loss: 32.4444 - MinusLogProbMetric: 32.4444 - val_loss: 33.7858 - val_MinusLogProbMetric: 33.7858 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 42/1000
2023-10-25 01:47:09.868 
Epoch 42/1000 
	 loss: 32.2950, MinusLogProbMetric: 32.2950, val_loss: 33.2782, val_MinusLogProbMetric: 33.2782

Epoch 42: val_loss did not improve from 32.27608
196/196 - 42s - loss: 32.2950 - MinusLogProbMetric: 32.2950 - val_loss: 33.2782 - val_MinusLogProbMetric: 33.2782 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 43/1000
2023-10-25 01:47:52.302 
Epoch 43/1000 
	 loss: 32.5299, MinusLogProbMetric: 32.5299, val_loss: 32.6182, val_MinusLogProbMetric: 32.6182

Epoch 43: val_loss did not improve from 32.27608
196/196 - 42s - loss: 32.5299 - MinusLogProbMetric: 32.5299 - val_loss: 32.6182 - val_MinusLogProbMetric: 32.6182 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 44/1000
2023-10-25 01:48:34.376 
Epoch 44/1000 
	 loss: 32.1741, MinusLogProbMetric: 32.1741, val_loss: 31.9838, val_MinusLogProbMetric: 31.9838

Epoch 44: val_loss improved from 32.27608 to 31.98383, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 32.1741 - MinusLogProbMetric: 32.1741 - val_loss: 31.9838 - val_MinusLogProbMetric: 31.9838 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 45/1000
2023-10-25 01:49:17.330 
Epoch 45/1000 
	 loss: 32.0032, MinusLogProbMetric: 32.0032, val_loss: 33.3385, val_MinusLogProbMetric: 33.3385

Epoch 45: val_loss did not improve from 31.98383
196/196 - 42s - loss: 32.0032 - MinusLogProbMetric: 32.0032 - val_loss: 33.3385 - val_MinusLogProbMetric: 33.3385 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 46/1000
2023-10-25 01:50:00.466 
Epoch 46/1000 
	 loss: 32.0216, MinusLogProbMetric: 32.0216, val_loss: 31.5376, val_MinusLogProbMetric: 31.5376

Epoch 46: val_loss improved from 31.98383 to 31.53758, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 32.0216 - MinusLogProbMetric: 32.0216 - val_loss: 31.5376 - val_MinusLogProbMetric: 31.5376 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 47/1000
2023-10-25 01:50:43.761 
Epoch 47/1000 
	 loss: 32.2763, MinusLogProbMetric: 32.2763, val_loss: 33.2960, val_MinusLogProbMetric: 33.2960

Epoch 47: val_loss did not improve from 31.53758
196/196 - 43s - loss: 32.2763 - MinusLogProbMetric: 32.2763 - val_loss: 33.2960 - val_MinusLogProbMetric: 33.2960 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 48/1000
2023-10-25 01:51:25.950 
Epoch 48/1000 
	 loss: 31.8146, MinusLogProbMetric: 31.8146, val_loss: 32.5222, val_MinusLogProbMetric: 32.5222

Epoch 48: val_loss did not improve from 31.53758
196/196 - 42s - loss: 31.8146 - MinusLogProbMetric: 31.8146 - val_loss: 32.5222 - val_MinusLogProbMetric: 32.5222 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 49/1000
2023-10-25 01:52:08.985 
Epoch 49/1000 
	 loss: 31.6733, MinusLogProbMetric: 31.6733, val_loss: 32.5370, val_MinusLogProbMetric: 32.5370

Epoch 49: val_loss did not improve from 31.53758
196/196 - 43s - loss: 31.6733 - MinusLogProbMetric: 31.6733 - val_loss: 32.5370 - val_MinusLogProbMetric: 32.5370 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 50/1000
2023-10-25 01:52:51.516 
Epoch 50/1000 
	 loss: 31.6471, MinusLogProbMetric: 31.6471, val_loss: 31.6205, val_MinusLogProbMetric: 31.6205

Epoch 50: val_loss did not improve from 31.53758
196/196 - 43s - loss: 31.6471 - MinusLogProbMetric: 31.6471 - val_loss: 31.6205 - val_MinusLogProbMetric: 31.6205 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 51/1000
2023-10-25 01:53:33.648 
Epoch 51/1000 
	 loss: 31.7466, MinusLogProbMetric: 31.7466, val_loss: 30.8820, val_MinusLogProbMetric: 30.8820

Epoch 51: val_loss improved from 31.53758 to 30.88201, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 31.7466 - MinusLogProbMetric: 31.7466 - val_loss: 30.8820 - val_MinusLogProbMetric: 30.8820 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 52/1000
2023-10-25 01:54:15.764 
Epoch 52/1000 
	 loss: 31.6508, MinusLogProbMetric: 31.6508, val_loss: 32.7236, val_MinusLogProbMetric: 32.7236

Epoch 52: val_loss did not improve from 30.88201
196/196 - 41s - loss: 31.6508 - MinusLogProbMetric: 31.6508 - val_loss: 32.7236 - val_MinusLogProbMetric: 32.7236 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 53/1000
2023-10-25 01:54:57.869 
Epoch 53/1000 
	 loss: 31.3453, MinusLogProbMetric: 31.3453, val_loss: 31.1560, val_MinusLogProbMetric: 31.1560

Epoch 53: val_loss did not improve from 30.88201
196/196 - 42s - loss: 31.3453 - MinusLogProbMetric: 31.3453 - val_loss: 31.1560 - val_MinusLogProbMetric: 31.1560 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 54/1000
2023-10-25 01:55:39.402 
Epoch 54/1000 
	 loss: 31.5843, MinusLogProbMetric: 31.5843, val_loss: 31.6045, val_MinusLogProbMetric: 31.6045

Epoch 54: val_loss did not improve from 30.88201
196/196 - 42s - loss: 31.5843 - MinusLogProbMetric: 31.5843 - val_loss: 31.6045 - val_MinusLogProbMetric: 31.6045 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 55/1000
2023-10-25 01:56:19.923 
Epoch 55/1000 
	 loss: 31.3455, MinusLogProbMetric: 31.3455, val_loss: 30.8530, val_MinusLogProbMetric: 30.8530

Epoch 55: val_loss improved from 30.88201 to 30.85301, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 41s - loss: 31.3455 - MinusLogProbMetric: 31.3455 - val_loss: 30.8530 - val_MinusLogProbMetric: 30.8530 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 56/1000
2023-10-25 01:57:03.166 
Epoch 56/1000 
	 loss: 31.2771, MinusLogProbMetric: 31.2771, val_loss: 30.6266, val_MinusLogProbMetric: 30.6266

Epoch 56: val_loss improved from 30.85301 to 30.62663, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 31.2771 - MinusLogProbMetric: 31.2771 - val_loss: 30.6266 - val_MinusLogProbMetric: 30.6266 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 57/1000
2023-10-25 01:57:46.377 
Epoch 57/1000 
	 loss: 31.5177, MinusLogProbMetric: 31.5177, val_loss: 32.3224, val_MinusLogProbMetric: 32.3224

Epoch 57: val_loss did not improve from 30.62663
196/196 - 42s - loss: 31.5177 - MinusLogProbMetric: 31.5177 - val_loss: 32.3224 - val_MinusLogProbMetric: 32.3224 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 58/1000
2023-10-25 01:58:28.132 
Epoch 58/1000 
	 loss: 31.1618, MinusLogProbMetric: 31.1618, val_loss: 30.7103, val_MinusLogProbMetric: 30.7103

Epoch 58: val_loss did not improve from 30.62663
196/196 - 42s - loss: 31.1618 - MinusLogProbMetric: 31.1618 - val_loss: 30.7103 - val_MinusLogProbMetric: 30.7103 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 59/1000
2023-10-25 01:59:10.106 
Epoch 59/1000 
	 loss: 31.1843, MinusLogProbMetric: 31.1843, val_loss: 31.7637, val_MinusLogProbMetric: 31.7637

Epoch 59: val_loss did not improve from 30.62663
196/196 - 42s - loss: 31.1843 - MinusLogProbMetric: 31.1843 - val_loss: 31.7637 - val_MinusLogProbMetric: 31.7637 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 60/1000
2023-10-25 01:59:50.623 
Epoch 60/1000 
	 loss: 31.1438, MinusLogProbMetric: 31.1438, val_loss: 30.9598, val_MinusLogProbMetric: 30.9598

Epoch 60: val_loss did not improve from 30.62663
196/196 - 41s - loss: 31.1438 - MinusLogProbMetric: 31.1438 - val_loss: 30.9598 - val_MinusLogProbMetric: 30.9598 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 61/1000
2023-10-25 02:00:32.516 
Epoch 61/1000 
	 loss: 31.0775, MinusLogProbMetric: 31.0775, val_loss: 32.3547, val_MinusLogProbMetric: 32.3547

Epoch 61: val_loss did not improve from 30.62663
196/196 - 42s - loss: 31.0775 - MinusLogProbMetric: 31.0775 - val_loss: 32.3547 - val_MinusLogProbMetric: 32.3547 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 62/1000
2023-10-25 02:01:13.980 
Epoch 62/1000 
	 loss: 31.2054, MinusLogProbMetric: 31.2054, val_loss: 30.7801, val_MinusLogProbMetric: 30.7801

Epoch 62: val_loss did not improve from 30.62663
196/196 - 41s - loss: 31.2054 - MinusLogProbMetric: 31.2054 - val_loss: 30.7801 - val_MinusLogProbMetric: 30.7801 - lr: 3.3333e-04 - 41s/epoch - 212ms/step
Epoch 63/1000
2023-10-25 02:01:54.898 
Epoch 63/1000 
	 loss: 30.8613, MinusLogProbMetric: 30.8613, val_loss: 30.8152, val_MinusLogProbMetric: 30.8152

Epoch 63: val_loss did not improve from 30.62663
196/196 - 41s - loss: 30.8613 - MinusLogProbMetric: 30.8613 - val_loss: 30.8152 - val_MinusLogProbMetric: 30.8152 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 64/1000
2023-10-25 02:02:37.034 
Epoch 64/1000 
	 loss: 31.1954, MinusLogProbMetric: 31.1954, val_loss: 31.3064, val_MinusLogProbMetric: 31.3064

Epoch 64: val_loss did not improve from 30.62663
196/196 - 42s - loss: 31.1954 - MinusLogProbMetric: 31.1954 - val_loss: 31.3064 - val_MinusLogProbMetric: 31.3064 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 65/1000
2023-10-25 02:03:19.997 
Epoch 65/1000 
	 loss: 30.9487, MinusLogProbMetric: 30.9487, val_loss: 30.4173, val_MinusLogProbMetric: 30.4173

Epoch 65: val_loss improved from 30.62663 to 30.41727, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 30.9487 - MinusLogProbMetric: 30.9487 - val_loss: 30.4173 - val_MinusLogProbMetric: 30.4173 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 66/1000
2023-10-25 02:04:03.071 
Epoch 66/1000 
	 loss: 30.6119, MinusLogProbMetric: 30.6119, val_loss: 31.0949, val_MinusLogProbMetric: 31.0949

Epoch 66: val_loss did not improve from 30.41727
196/196 - 42s - loss: 30.6119 - MinusLogProbMetric: 30.6119 - val_loss: 31.0949 - val_MinusLogProbMetric: 31.0949 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 67/1000
2023-10-25 02:04:43.153 
Epoch 67/1000 
	 loss: 30.8578, MinusLogProbMetric: 30.8578, val_loss: 30.6379, val_MinusLogProbMetric: 30.6379

Epoch 67: val_loss did not improve from 30.41727
196/196 - 40s - loss: 30.8578 - MinusLogProbMetric: 30.8578 - val_loss: 30.6379 - val_MinusLogProbMetric: 30.6379 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 68/1000
2023-10-25 02:05:21.477 
Epoch 68/1000 
	 loss: 30.7400, MinusLogProbMetric: 30.7400, val_loss: 31.5730, val_MinusLogProbMetric: 31.5730

Epoch 68: val_loss did not improve from 30.41727
196/196 - 38s - loss: 30.7400 - MinusLogProbMetric: 30.7400 - val_loss: 31.5730 - val_MinusLogProbMetric: 31.5730 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 69/1000
2023-10-25 02:06:02.097 
Epoch 69/1000 
	 loss: 30.6710, MinusLogProbMetric: 30.6710, val_loss: 30.9108, val_MinusLogProbMetric: 30.9108

Epoch 69: val_loss did not improve from 30.41727
196/196 - 41s - loss: 30.6710 - MinusLogProbMetric: 30.6710 - val_loss: 30.9108 - val_MinusLogProbMetric: 30.9108 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 70/1000
2023-10-25 02:06:42.315 
Epoch 70/1000 
	 loss: 30.6608, MinusLogProbMetric: 30.6608, val_loss: 32.6661, val_MinusLogProbMetric: 32.6661

Epoch 70: val_loss did not improve from 30.41727
196/196 - 40s - loss: 30.6608 - MinusLogProbMetric: 30.6608 - val_loss: 32.6661 - val_MinusLogProbMetric: 32.6661 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 71/1000
2023-10-25 02:07:25.015 
Epoch 71/1000 
	 loss: 30.6242, MinusLogProbMetric: 30.6242, val_loss: 30.1671, val_MinusLogProbMetric: 30.1671

Epoch 71: val_loss improved from 30.41727 to 30.16706, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 30.6242 - MinusLogProbMetric: 30.6242 - val_loss: 30.1671 - val_MinusLogProbMetric: 30.1671 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 72/1000
2023-10-25 02:08:06.036 
Epoch 72/1000 
	 loss: 30.5853, MinusLogProbMetric: 30.5853, val_loss: 30.7583, val_MinusLogProbMetric: 30.7583

Epoch 72: val_loss did not improve from 30.16706
196/196 - 40s - loss: 30.5853 - MinusLogProbMetric: 30.5853 - val_loss: 30.7583 - val_MinusLogProbMetric: 30.7583 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 73/1000
2023-10-25 02:08:47.973 
Epoch 73/1000 
	 loss: 30.4832, MinusLogProbMetric: 30.4832, val_loss: 30.5268, val_MinusLogProbMetric: 30.5268

Epoch 73: val_loss did not improve from 30.16706
196/196 - 42s - loss: 30.4832 - MinusLogProbMetric: 30.4832 - val_loss: 30.5268 - val_MinusLogProbMetric: 30.5268 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 74/1000
2023-10-25 02:09:29.721 
Epoch 74/1000 
	 loss: 30.4698, MinusLogProbMetric: 30.4698, val_loss: 30.0855, val_MinusLogProbMetric: 30.0855

Epoch 74: val_loss improved from 30.16706 to 30.08549, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 30.4698 - MinusLogProbMetric: 30.4698 - val_loss: 30.0855 - val_MinusLogProbMetric: 30.0855 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 75/1000
2023-10-25 02:10:09.637 
Epoch 75/1000 
	 loss: 30.4573, MinusLogProbMetric: 30.4573, val_loss: 31.7582, val_MinusLogProbMetric: 31.7582

Epoch 75: val_loss did not improve from 30.08549
196/196 - 39s - loss: 30.4573 - MinusLogProbMetric: 30.4573 - val_loss: 31.7582 - val_MinusLogProbMetric: 31.7582 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 76/1000
2023-10-25 02:10:52.181 
Epoch 76/1000 
	 loss: 30.4396, MinusLogProbMetric: 30.4396, val_loss: 29.7868, val_MinusLogProbMetric: 29.7868

Epoch 76: val_loss improved from 30.08549 to 29.78679, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 30.4396 - MinusLogProbMetric: 30.4396 - val_loss: 29.7868 - val_MinusLogProbMetric: 29.7868 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 77/1000
2023-10-25 02:11:35.911 
Epoch 77/1000 
	 loss: 30.3564, MinusLogProbMetric: 30.3564, val_loss: 30.2412, val_MinusLogProbMetric: 30.2412

Epoch 77: val_loss did not improve from 29.78679
196/196 - 43s - loss: 30.3564 - MinusLogProbMetric: 30.3564 - val_loss: 30.2412 - val_MinusLogProbMetric: 30.2412 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 78/1000
2023-10-25 02:12:18.400 
Epoch 78/1000 
	 loss: 30.5003, MinusLogProbMetric: 30.5003, val_loss: 30.6637, val_MinusLogProbMetric: 30.6637

Epoch 78: val_loss did not improve from 29.78679
196/196 - 42s - loss: 30.5003 - MinusLogProbMetric: 30.5003 - val_loss: 30.6637 - val_MinusLogProbMetric: 30.6637 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 79/1000
2023-10-25 02:13:00.236 
Epoch 79/1000 
	 loss: 30.5263, MinusLogProbMetric: 30.5263, val_loss: 30.9599, val_MinusLogProbMetric: 30.9599

Epoch 79: val_loss did not improve from 29.78679
196/196 - 42s - loss: 30.5263 - MinusLogProbMetric: 30.5263 - val_loss: 30.9599 - val_MinusLogProbMetric: 30.9599 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 80/1000
2023-10-25 02:13:43.004 
Epoch 80/1000 
	 loss: 30.3482, MinusLogProbMetric: 30.3482, val_loss: 31.7352, val_MinusLogProbMetric: 31.7352

Epoch 80: val_loss did not improve from 29.78679
196/196 - 43s - loss: 30.3482 - MinusLogProbMetric: 30.3482 - val_loss: 31.7352 - val_MinusLogProbMetric: 31.7352 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 81/1000
2023-10-25 02:14:24.209 
Epoch 81/1000 
	 loss: 30.3209, MinusLogProbMetric: 30.3209, val_loss: 31.5861, val_MinusLogProbMetric: 31.5861

Epoch 81: val_loss did not improve from 29.78679
196/196 - 41s - loss: 30.3209 - MinusLogProbMetric: 30.3209 - val_loss: 31.5861 - val_MinusLogProbMetric: 31.5861 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 82/1000
2023-10-25 02:15:06.238 
Epoch 82/1000 
	 loss: 30.2246, MinusLogProbMetric: 30.2246, val_loss: 30.9250, val_MinusLogProbMetric: 30.9250

Epoch 82: val_loss did not improve from 29.78679
196/196 - 42s - loss: 30.2246 - MinusLogProbMetric: 30.2246 - val_loss: 30.9250 - val_MinusLogProbMetric: 30.9250 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 83/1000
2023-10-25 02:15:45.028 
Epoch 83/1000 
	 loss: 30.2880, MinusLogProbMetric: 30.2880, val_loss: 30.3102, val_MinusLogProbMetric: 30.3102

Epoch 83: val_loss did not improve from 29.78679
196/196 - 39s - loss: 30.2880 - MinusLogProbMetric: 30.2880 - val_loss: 30.3102 - val_MinusLogProbMetric: 30.3102 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 84/1000
2023-10-25 02:16:24.535 
Epoch 84/1000 
	 loss: 30.2383, MinusLogProbMetric: 30.2383, val_loss: 31.0282, val_MinusLogProbMetric: 31.0282

Epoch 84: val_loss did not improve from 29.78679
196/196 - 40s - loss: 30.2383 - MinusLogProbMetric: 30.2383 - val_loss: 31.0282 - val_MinusLogProbMetric: 31.0282 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 85/1000
2023-10-25 02:17:05.925 
Epoch 85/1000 
	 loss: 30.5393, MinusLogProbMetric: 30.5393, val_loss: 30.5830, val_MinusLogProbMetric: 30.5830

Epoch 85: val_loss did not improve from 29.78679
196/196 - 41s - loss: 30.5393 - MinusLogProbMetric: 30.5393 - val_loss: 30.5830 - val_MinusLogProbMetric: 30.5830 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 86/1000
2023-10-25 02:17:45.761 
Epoch 86/1000 
	 loss: 30.3528, MinusLogProbMetric: 30.3528, val_loss: 30.3181, val_MinusLogProbMetric: 30.3181

Epoch 86: val_loss did not improve from 29.78679
196/196 - 40s - loss: 30.3528 - MinusLogProbMetric: 30.3528 - val_loss: 30.3181 - val_MinusLogProbMetric: 30.3181 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 87/1000
2023-10-25 02:18:27.933 
Epoch 87/1000 
	 loss: 29.9765, MinusLogProbMetric: 29.9765, val_loss: 29.9274, val_MinusLogProbMetric: 29.9274

Epoch 87: val_loss did not improve from 29.78679
196/196 - 42s - loss: 29.9765 - MinusLogProbMetric: 29.9765 - val_loss: 29.9274 - val_MinusLogProbMetric: 29.9274 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 88/1000
2023-10-25 02:19:09.600 
Epoch 88/1000 
	 loss: 30.3653, MinusLogProbMetric: 30.3653, val_loss: 30.9054, val_MinusLogProbMetric: 30.9054

Epoch 88: val_loss did not improve from 29.78679
196/196 - 42s - loss: 30.3653 - MinusLogProbMetric: 30.3653 - val_loss: 30.9054 - val_MinusLogProbMetric: 30.9054 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 89/1000
2023-10-25 02:19:50.640 
Epoch 89/1000 
	 loss: 30.0778, MinusLogProbMetric: 30.0778, val_loss: 31.4884, val_MinusLogProbMetric: 31.4884

Epoch 89: val_loss did not improve from 29.78679
196/196 - 41s - loss: 30.0778 - MinusLogProbMetric: 30.0778 - val_loss: 31.4884 - val_MinusLogProbMetric: 31.4884 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 90/1000
2023-10-25 02:20:31.616 
Epoch 90/1000 
	 loss: 30.3198, MinusLogProbMetric: 30.3198, val_loss: 30.2958, val_MinusLogProbMetric: 30.2958

Epoch 90: val_loss did not improve from 29.78679
196/196 - 41s - loss: 30.3198 - MinusLogProbMetric: 30.3198 - val_loss: 30.2958 - val_MinusLogProbMetric: 30.2958 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 91/1000
2023-10-25 02:21:14.615 
Epoch 91/1000 
	 loss: 31.0791, MinusLogProbMetric: 31.0791, val_loss: 30.1888, val_MinusLogProbMetric: 30.1888

Epoch 91: val_loss did not improve from 29.78679
196/196 - 43s - loss: 31.0791 - MinusLogProbMetric: 31.0791 - val_loss: 30.1888 - val_MinusLogProbMetric: 30.1888 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 92/1000
2023-10-25 02:21:55.991 
Epoch 92/1000 
	 loss: 29.9595, MinusLogProbMetric: 29.9595, val_loss: 29.6446, val_MinusLogProbMetric: 29.6446

Epoch 92: val_loss improved from 29.78679 to 29.64456, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 29.9595 - MinusLogProbMetric: 29.9595 - val_loss: 29.6446 - val_MinusLogProbMetric: 29.6446 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 93/1000
2023-10-25 02:22:38.061 
Epoch 93/1000 
	 loss: 29.9162, MinusLogProbMetric: 29.9162, val_loss: 29.6797, val_MinusLogProbMetric: 29.6797

Epoch 93: val_loss did not improve from 29.64456
196/196 - 41s - loss: 29.9162 - MinusLogProbMetric: 29.9162 - val_loss: 29.6797 - val_MinusLogProbMetric: 29.6797 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 94/1000
2023-10-25 02:23:18.519 
Epoch 94/1000 
	 loss: 30.1201, MinusLogProbMetric: 30.1201, val_loss: 30.6856, val_MinusLogProbMetric: 30.6856

Epoch 94: val_loss did not improve from 29.64456
196/196 - 40s - loss: 30.1201 - MinusLogProbMetric: 30.1201 - val_loss: 30.6856 - val_MinusLogProbMetric: 30.6856 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 95/1000
2023-10-25 02:24:00.054 
Epoch 95/1000 
	 loss: 29.9792, MinusLogProbMetric: 29.9792, val_loss: 31.0013, val_MinusLogProbMetric: 31.0013

Epoch 95: val_loss did not improve from 29.64456
196/196 - 42s - loss: 29.9792 - MinusLogProbMetric: 29.9792 - val_loss: 31.0013 - val_MinusLogProbMetric: 31.0013 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 96/1000
2023-10-25 02:24:41.964 
Epoch 96/1000 
	 loss: 30.0796, MinusLogProbMetric: 30.0796, val_loss: 29.9011, val_MinusLogProbMetric: 29.9011

Epoch 96: val_loss did not improve from 29.64456
196/196 - 42s - loss: 30.0796 - MinusLogProbMetric: 30.0796 - val_loss: 29.9011 - val_MinusLogProbMetric: 29.9011 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 97/1000
2023-10-25 02:25:23.654 
Epoch 97/1000 
	 loss: 30.0078, MinusLogProbMetric: 30.0078, val_loss: 29.8315, val_MinusLogProbMetric: 29.8315

Epoch 97: val_loss did not improve from 29.64456
196/196 - 42s - loss: 30.0078 - MinusLogProbMetric: 30.0078 - val_loss: 29.8315 - val_MinusLogProbMetric: 29.8315 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 98/1000
2023-10-25 02:26:05.523 
Epoch 98/1000 
	 loss: 29.9580, MinusLogProbMetric: 29.9580, val_loss: 29.8551, val_MinusLogProbMetric: 29.8551

Epoch 98: val_loss did not improve from 29.64456
196/196 - 42s - loss: 29.9580 - MinusLogProbMetric: 29.9580 - val_loss: 29.8551 - val_MinusLogProbMetric: 29.8551 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 99/1000
2023-10-25 02:26:47.131 
Epoch 99/1000 
	 loss: 30.0182, MinusLogProbMetric: 30.0182, val_loss: 29.3806, val_MinusLogProbMetric: 29.3806

Epoch 99: val_loss improved from 29.64456 to 29.38060, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 30.0182 - MinusLogProbMetric: 30.0182 - val_loss: 29.3806 - val_MinusLogProbMetric: 29.3806 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 100/1000
2023-10-25 02:27:29.040 
Epoch 100/1000 
	 loss: 30.0770, MinusLogProbMetric: 30.0770, val_loss: 30.8623, val_MinusLogProbMetric: 30.8623

Epoch 100: val_loss did not improve from 29.38060
196/196 - 41s - loss: 30.0770 - MinusLogProbMetric: 30.0770 - val_loss: 30.8623 - val_MinusLogProbMetric: 30.8623 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 101/1000
2023-10-25 02:28:11.837 
Epoch 101/1000 
	 loss: 29.9725, MinusLogProbMetric: 29.9725, val_loss: 29.7856, val_MinusLogProbMetric: 29.7856

Epoch 101: val_loss did not improve from 29.38060
196/196 - 43s - loss: 29.9725 - MinusLogProbMetric: 29.9725 - val_loss: 29.7856 - val_MinusLogProbMetric: 29.7856 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 102/1000
2023-10-25 02:28:54.651 
Epoch 102/1000 
	 loss: 30.2221, MinusLogProbMetric: 30.2221, val_loss: 29.4277, val_MinusLogProbMetric: 29.4277

Epoch 102: val_loss did not improve from 29.38060
196/196 - 43s - loss: 30.2221 - MinusLogProbMetric: 30.2221 - val_loss: 29.4277 - val_MinusLogProbMetric: 29.4277 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 103/1000
2023-10-25 02:29:37.811 
Epoch 103/1000 
	 loss: 29.8793, MinusLogProbMetric: 29.8793, val_loss: 30.0208, val_MinusLogProbMetric: 30.0208

Epoch 103: val_loss did not improve from 29.38060
196/196 - 43s - loss: 29.8793 - MinusLogProbMetric: 29.8793 - val_loss: 30.0208 - val_MinusLogProbMetric: 30.0208 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 104/1000
2023-10-25 02:30:20.128 
Epoch 104/1000 
	 loss: 30.0499, MinusLogProbMetric: 30.0499, val_loss: 30.1507, val_MinusLogProbMetric: 30.1507

Epoch 104: val_loss did not improve from 29.38060
196/196 - 42s - loss: 30.0499 - MinusLogProbMetric: 30.0499 - val_loss: 30.1507 - val_MinusLogProbMetric: 30.1507 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 105/1000
2023-10-25 02:31:02.691 
Epoch 105/1000 
	 loss: 29.6734, MinusLogProbMetric: 29.6734, val_loss: 30.4027, val_MinusLogProbMetric: 30.4027

Epoch 105: val_loss did not improve from 29.38060
196/196 - 43s - loss: 29.6734 - MinusLogProbMetric: 29.6734 - val_loss: 30.4027 - val_MinusLogProbMetric: 30.4027 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 106/1000
2023-10-25 02:31:45.174 
Epoch 106/1000 
	 loss: 29.8002, MinusLogProbMetric: 29.8002, val_loss: 29.4908, val_MinusLogProbMetric: 29.4908

Epoch 106: val_loss did not improve from 29.38060
196/196 - 42s - loss: 29.8002 - MinusLogProbMetric: 29.8002 - val_loss: 29.4908 - val_MinusLogProbMetric: 29.4908 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 107/1000
2023-10-25 02:32:27.575 
Epoch 107/1000 
	 loss: 30.1384, MinusLogProbMetric: 30.1384, val_loss: 30.7390, val_MinusLogProbMetric: 30.7390

Epoch 107: val_loss did not improve from 29.38060
196/196 - 42s - loss: 30.1384 - MinusLogProbMetric: 30.1384 - val_loss: 30.7390 - val_MinusLogProbMetric: 30.7390 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 108/1000
2023-10-25 02:33:09.707 
Epoch 108/1000 
	 loss: 29.7717, MinusLogProbMetric: 29.7717, val_loss: 31.0129, val_MinusLogProbMetric: 31.0129

Epoch 108: val_loss did not improve from 29.38060
196/196 - 42s - loss: 29.7717 - MinusLogProbMetric: 29.7717 - val_loss: 31.0129 - val_MinusLogProbMetric: 31.0129 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 109/1000
2023-10-25 02:33:52.798 
Epoch 109/1000 
	 loss: 29.9368, MinusLogProbMetric: 29.9368, val_loss: 29.6917, val_MinusLogProbMetric: 29.6917

Epoch 109: val_loss did not improve from 29.38060
196/196 - 43s - loss: 29.9368 - MinusLogProbMetric: 29.9368 - val_loss: 29.6917 - val_MinusLogProbMetric: 29.6917 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 110/1000
2023-10-25 02:34:35.297 
Epoch 110/1000 
	 loss: 29.7822, MinusLogProbMetric: 29.7822, val_loss: 32.6134, val_MinusLogProbMetric: 32.6134

Epoch 110: val_loss did not improve from 29.38060
196/196 - 42s - loss: 29.7822 - MinusLogProbMetric: 29.7822 - val_loss: 32.6134 - val_MinusLogProbMetric: 32.6134 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 111/1000
2023-10-25 02:35:17.727 
Epoch 111/1000 
	 loss: 29.8845, MinusLogProbMetric: 29.8845, val_loss: 29.6276, val_MinusLogProbMetric: 29.6276

Epoch 111: val_loss did not improve from 29.38060
196/196 - 42s - loss: 29.8845 - MinusLogProbMetric: 29.8845 - val_loss: 29.6276 - val_MinusLogProbMetric: 29.6276 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 112/1000
2023-10-25 02:36:00.442 
Epoch 112/1000 
	 loss: 29.8981, MinusLogProbMetric: 29.8981, val_loss: 29.9838, val_MinusLogProbMetric: 29.9838

Epoch 112: val_loss did not improve from 29.38060
196/196 - 43s - loss: 29.8981 - MinusLogProbMetric: 29.8981 - val_loss: 29.9838 - val_MinusLogProbMetric: 29.9838 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 113/1000
2023-10-25 02:36:43.319 
Epoch 113/1000 
	 loss: 29.6985, MinusLogProbMetric: 29.6985, val_loss: 31.0982, val_MinusLogProbMetric: 31.0982

Epoch 113: val_loss did not improve from 29.38060
196/196 - 43s - loss: 29.6985 - MinusLogProbMetric: 29.6985 - val_loss: 31.0982 - val_MinusLogProbMetric: 31.0982 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 114/1000
2023-10-25 02:37:24.992 
Epoch 114/1000 
	 loss: 29.7551, MinusLogProbMetric: 29.7551, val_loss: 29.5649, val_MinusLogProbMetric: 29.5649

Epoch 114: val_loss did not improve from 29.38060
196/196 - 42s - loss: 29.7551 - MinusLogProbMetric: 29.7551 - val_loss: 29.5649 - val_MinusLogProbMetric: 29.5649 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 115/1000
2023-10-25 02:38:07.869 
Epoch 115/1000 
	 loss: 29.7162, MinusLogProbMetric: 29.7162, val_loss: 31.0720, val_MinusLogProbMetric: 31.0720

Epoch 115: val_loss did not improve from 29.38060
196/196 - 43s - loss: 29.7162 - MinusLogProbMetric: 29.7162 - val_loss: 31.0720 - val_MinusLogProbMetric: 31.0720 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 116/1000
2023-10-25 02:38:50.232 
Epoch 116/1000 
	 loss: 29.7540, MinusLogProbMetric: 29.7540, val_loss: 29.4770, val_MinusLogProbMetric: 29.4770

Epoch 116: val_loss did not improve from 29.38060
196/196 - 42s - loss: 29.7540 - MinusLogProbMetric: 29.7540 - val_loss: 29.4770 - val_MinusLogProbMetric: 29.4770 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 117/1000
2023-10-25 02:39:31.091 
Epoch 117/1000 
	 loss: 29.5762, MinusLogProbMetric: 29.5762, val_loss: 29.4639, val_MinusLogProbMetric: 29.4639

Epoch 117: val_loss did not improve from 29.38060
196/196 - 41s - loss: 29.5762 - MinusLogProbMetric: 29.5762 - val_loss: 29.4639 - val_MinusLogProbMetric: 29.4639 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 118/1000
2023-10-25 02:40:13.687 
Epoch 118/1000 
	 loss: 29.6783, MinusLogProbMetric: 29.6783, val_loss: 30.8631, val_MinusLogProbMetric: 30.8631

Epoch 118: val_loss did not improve from 29.38060
196/196 - 43s - loss: 29.6783 - MinusLogProbMetric: 29.6783 - val_loss: 30.8631 - val_MinusLogProbMetric: 30.8631 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 119/1000
2023-10-25 02:40:56.503 
Epoch 119/1000 
	 loss: 29.6598, MinusLogProbMetric: 29.6598, val_loss: 32.7821, val_MinusLogProbMetric: 32.7821

Epoch 119: val_loss did not improve from 29.38060
196/196 - 43s - loss: 29.6598 - MinusLogProbMetric: 29.6598 - val_loss: 32.7821 - val_MinusLogProbMetric: 32.7821 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 120/1000
2023-10-25 02:41:39.356 
Epoch 120/1000 
	 loss: 29.6318, MinusLogProbMetric: 29.6318, val_loss: 30.2555, val_MinusLogProbMetric: 30.2555

Epoch 120: val_loss did not improve from 29.38060
196/196 - 43s - loss: 29.6318 - MinusLogProbMetric: 29.6318 - val_loss: 30.2555 - val_MinusLogProbMetric: 30.2555 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 121/1000
2023-10-25 02:42:22.081 
Epoch 121/1000 
	 loss: 29.8507, MinusLogProbMetric: 29.8507, val_loss: 29.3351, val_MinusLogProbMetric: 29.3351

Epoch 121: val_loss improved from 29.38060 to 29.33512, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 29.8507 - MinusLogProbMetric: 29.8507 - val_loss: 29.3351 - val_MinusLogProbMetric: 29.3351 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 122/1000
2023-10-25 02:43:04.832 
Epoch 122/1000 
	 loss: 29.6208, MinusLogProbMetric: 29.6208, val_loss: 29.5639, val_MinusLogProbMetric: 29.5639

Epoch 122: val_loss did not improve from 29.33512
196/196 - 42s - loss: 29.6208 - MinusLogProbMetric: 29.6208 - val_loss: 29.5639 - val_MinusLogProbMetric: 29.5639 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 123/1000
2023-10-25 02:43:47.781 
Epoch 123/1000 
	 loss: 29.5869, MinusLogProbMetric: 29.5869, val_loss: 29.5509, val_MinusLogProbMetric: 29.5509

Epoch 123: val_loss did not improve from 29.33512
196/196 - 43s - loss: 29.5869 - MinusLogProbMetric: 29.5869 - val_loss: 29.5509 - val_MinusLogProbMetric: 29.5509 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 124/1000
2023-10-25 02:44:29.185 
Epoch 124/1000 
	 loss: 29.6885, MinusLogProbMetric: 29.6885, val_loss: 29.7145, val_MinusLogProbMetric: 29.7145

Epoch 124: val_loss did not improve from 29.33512
196/196 - 41s - loss: 29.6885 - MinusLogProbMetric: 29.6885 - val_loss: 29.7145 - val_MinusLogProbMetric: 29.7145 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 125/1000
2023-10-25 02:45:10.217 
Epoch 125/1000 
	 loss: 29.7005, MinusLogProbMetric: 29.7005, val_loss: 29.1694, val_MinusLogProbMetric: 29.1694

Epoch 125: val_loss improved from 29.33512 to 29.16943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 29.7005 - MinusLogProbMetric: 29.7005 - val_loss: 29.1694 - val_MinusLogProbMetric: 29.1694 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 126/1000
2023-10-25 02:45:53.065 
Epoch 126/1000 
	 loss: 29.4471, MinusLogProbMetric: 29.4471, val_loss: 30.1922, val_MinusLogProbMetric: 30.1922

Epoch 126: val_loss did not improve from 29.16943
196/196 - 42s - loss: 29.4471 - MinusLogProbMetric: 29.4471 - val_loss: 30.1922 - val_MinusLogProbMetric: 30.1922 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 127/1000
2023-10-25 02:46:35.900 
Epoch 127/1000 
	 loss: 29.6117, MinusLogProbMetric: 29.6117, val_loss: 29.2390, val_MinusLogProbMetric: 29.2390

Epoch 127: val_loss did not improve from 29.16943
196/196 - 43s - loss: 29.6117 - MinusLogProbMetric: 29.6117 - val_loss: 29.2390 - val_MinusLogProbMetric: 29.2390 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 128/1000
2023-10-25 02:47:18.031 
Epoch 128/1000 
	 loss: 29.5233, MinusLogProbMetric: 29.5233, val_loss: 29.5741, val_MinusLogProbMetric: 29.5741

Epoch 128: val_loss did not improve from 29.16943
196/196 - 42s - loss: 29.5233 - MinusLogProbMetric: 29.5233 - val_loss: 29.5741 - val_MinusLogProbMetric: 29.5741 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 129/1000
2023-10-25 02:48:00.229 
Epoch 129/1000 
	 loss: 29.7932, MinusLogProbMetric: 29.7932, val_loss: 29.4551, val_MinusLogProbMetric: 29.4551

Epoch 129: val_loss did not improve from 29.16943
196/196 - 42s - loss: 29.7932 - MinusLogProbMetric: 29.7932 - val_loss: 29.4551 - val_MinusLogProbMetric: 29.4551 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 130/1000
2023-10-25 02:48:42.719 
Epoch 130/1000 
	 loss: 29.5051, MinusLogProbMetric: 29.5051, val_loss: 29.6697, val_MinusLogProbMetric: 29.6697

Epoch 130: val_loss did not improve from 29.16943
196/196 - 42s - loss: 29.5051 - MinusLogProbMetric: 29.5051 - val_loss: 29.6697 - val_MinusLogProbMetric: 29.6697 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 131/1000
2023-10-25 02:49:25.099 
Epoch 131/1000 
	 loss: 29.5765, MinusLogProbMetric: 29.5765, val_loss: 30.2202, val_MinusLogProbMetric: 30.2202

Epoch 131: val_loss did not improve from 29.16943
196/196 - 42s - loss: 29.5765 - MinusLogProbMetric: 29.5765 - val_loss: 30.2202 - val_MinusLogProbMetric: 30.2202 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 132/1000
2023-10-25 02:50:08.050 
Epoch 132/1000 
	 loss: 29.2967, MinusLogProbMetric: 29.2967, val_loss: 29.3330, val_MinusLogProbMetric: 29.3330

Epoch 132: val_loss did not improve from 29.16943
196/196 - 43s - loss: 29.2967 - MinusLogProbMetric: 29.2967 - val_loss: 29.3330 - val_MinusLogProbMetric: 29.3330 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 133/1000
2023-10-25 02:50:50.313 
Epoch 133/1000 
	 loss: 30.2886, MinusLogProbMetric: 30.2886, val_loss: 29.6892, val_MinusLogProbMetric: 29.6892

Epoch 133: val_loss did not improve from 29.16943
196/196 - 42s - loss: 30.2886 - MinusLogProbMetric: 30.2886 - val_loss: 29.6892 - val_MinusLogProbMetric: 29.6892 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 134/1000
2023-10-25 02:51:32.907 
Epoch 134/1000 
	 loss: 29.4435, MinusLogProbMetric: 29.4435, val_loss: 29.7422, val_MinusLogProbMetric: 29.7422

Epoch 134: val_loss did not improve from 29.16943
196/196 - 43s - loss: 29.4435 - MinusLogProbMetric: 29.4435 - val_loss: 29.7422 - val_MinusLogProbMetric: 29.7422 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 135/1000
2023-10-25 02:52:14.143 
Epoch 135/1000 
	 loss: 29.6004, MinusLogProbMetric: 29.6004, val_loss: 29.2144, val_MinusLogProbMetric: 29.2144

Epoch 135: val_loss did not improve from 29.16943
196/196 - 41s - loss: 29.6004 - MinusLogProbMetric: 29.6004 - val_loss: 29.2144 - val_MinusLogProbMetric: 29.2144 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 136/1000
2023-10-25 02:52:56.918 
Epoch 136/1000 
	 loss: 29.5581, MinusLogProbMetric: 29.5581, val_loss: 29.5065, val_MinusLogProbMetric: 29.5065

Epoch 136: val_loss did not improve from 29.16943
196/196 - 43s - loss: 29.5581 - MinusLogProbMetric: 29.5581 - val_loss: 29.5065 - val_MinusLogProbMetric: 29.5065 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 137/1000
2023-10-25 02:53:39.255 
Epoch 137/1000 
	 loss: 29.2704, MinusLogProbMetric: 29.2704, val_loss: 29.7339, val_MinusLogProbMetric: 29.7339

Epoch 137: val_loss did not improve from 29.16943
196/196 - 42s - loss: 29.2704 - MinusLogProbMetric: 29.2704 - val_loss: 29.7339 - val_MinusLogProbMetric: 29.7339 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 138/1000
2023-10-25 02:54:22.106 
Epoch 138/1000 
	 loss: 29.6596, MinusLogProbMetric: 29.6596, val_loss: 29.4752, val_MinusLogProbMetric: 29.4752

Epoch 138: val_loss did not improve from 29.16943
196/196 - 43s - loss: 29.6596 - MinusLogProbMetric: 29.6596 - val_loss: 29.4752 - val_MinusLogProbMetric: 29.4752 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 139/1000
2023-10-25 02:55:03.506 
Epoch 139/1000 
	 loss: 29.3679, MinusLogProbMetric: 29.3679, val_loss: 29.4337, val_MinusLogProbMetric: 29.4337

Epoch 139: val_loss did not improve from 29.16943
196/196 - 41s - loss: 29.3679 - MinusLogProbMetric: 29.3679 - val_loss: 29.4337 - val_MinusLogProbMetric: 29.4337 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 140/1000
2023-10-25 02:55:45.705 
Epoch 140/1000 
	 loss: 29.3772, MinusLogProbMetric: 29.3772, val_loss: 30.2130, val_MinusLogProbMetric: 30.2130

Epoch 140: val_loss did not improve from 29.16943
196/196 - 42s - loss: 29.3772 - MinusLogProbMetric: 29.3772 - val_loss: 30.2130 - val_MinusLogProbMetric: 30.2130 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 141/1000
2023-10-25 02:56:26.558 
Epoch 141/1000 
	 loss: 29.4468, MinusLogProbMetric: 29.4468, val_loss: 29.1683, val_MinusLogProbMetric: 29.1683

Epoch 141: val_loss improved from 29.16943 to 29.16832, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 29.4468 - MinusLogProbMetric: 29.4468 - val_loss: 29.1683 - val_MinusLogProbMetric: 29.1683 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 142/1000
2023-10-25 02:57:10.163 
Epoch 142/1000 
	 loss: 29.4783, MinusLogProbMetric: 29.4783, val_loss: 29.1530, val_MinusLogProbMetric: 29.1530

Epoch 142: val_loss improved from 29.16832 to 29.15296, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 29.4783 - MinusLogProbMetric: 29.4783 - val_loss: 29.1530 - val_MinusLogProbMetric: 29.1530 - lr: 3.3333e-04 - 44s/epoch - 223ms/step
Epoch 143/1000
2023-10-25 02:57:53.051 
Epoch 143/1000 
	 loss: 29.6507, MinusLogProbMetric: 29.6507, val_loss: 29.8374, val_MinusLogProbMetric: 29.8374

Epoch 143: val_loss did not improve from 29.15296
196/196 - 42s - loss: 29.6507 - MinusLogProbMetric: 29.6507 - val_loss: 29.8374 - val_MinusLogProbMetric: 29.8374 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 144/1000
2023-10-25 02:58:34.582 
Epoch 144/1000 
	 loss: 29.7098, MinusLogProbMetric: 29.7098, val_loss: 29.3134, val_MinusLogProbMetric: 29.3134

Epoch 144: val_loss did not improve from 29.15296
196/196 - 42s - loss: 29.7098 - MinusLogProbMetric: 29.7098 - val_loss: 29.3134 - val_MinusLogProbMetric: 29.3134 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 145/1000
2023-10-25 02:59:17.334 
Epoch 145/1000 
	 loss: 29.3998, MinusLogProbMetric: 29.3998, val_loss: 29.0516, val_MinusLogProbMetric: 29.0516

Epoch 145: val_loss improved from 29.15296 to 29.05165, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 29.3998 - MinusLogProbMetric: 29.3998 - val_loss: 29.0516 - val_MinusLogProbMetric: 29.0516 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 146/1000
2023-10-25 03:00:00.133 
Epoch 146/1000 
	 loss: 29.4965, MinusLogProbMetric: 29.4965, val_loss: 29.6277, val_MinusLogProbMetric: 29.6277

Epoch 146: val_loss did not improve from 29.05165
196/196 - 42s - loss: 29.4965 - MinusLogProbMetric: 29.4965 - val_loss: 29.6277 - val_MinusLogProbMetric: 29.6277 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 147/1000
2023-10-25 03:00:41.590 
Epoch 147/1000 
	 loss: 29.5410, MinusLogProbMetric: 29.5410, val_loss: 29.6493, val_MinusLogProbMetric: 29.6493

Epoch 147: val_loss did not improve from 29.05165
196/196 - 41s - loss: 29.5410 - MinusLogProbMetric: 29.5410 - val_loss: 29.6493 - val_MinusLogProbMetric: 29.6493 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 148/1000
2023-10-25 03:01:22.245 
Epoch 148/1000 
	 loss: 29.3623, MinusLogProbMetric: 29.3623, val_loss: 29.3384, val_MinusLogProbMetric: 29.3384

Epoch 148: val_loss did not improve from 29.05165
196/196 - 41s - loss: 29.3623 - MinusLogProbMetric: 29.3623 - val_loss: 29.3384 - val_MinusLogProbMetric: 29.3384 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 149/1000
2023-10-25 03:02:03.414 
Epoch 149/1000 
	 loss: 29.6663, MinusLogProbMetric: 29.6663, val_loss: 38.9625, val_MinusLogProbMetric: 38.9625

Epoch 149: val_loss did not improve from 29.05165
196/196 - 41s - loss: 29.6663 - MinusLogProbMetric: 29.6663 - val_loss: 38.9625 - val_MinusLogProbMetric: 38.9625 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 150/1000
2023-10-25 03:02:43.398 
Epoch 150/1000 
	 loss: 29.8336, MinusLogProbMetric: 29.8336, val_loss: 29.6365, val_MinusLogProbMetric: 29.6365

Epoch 150: val_loss did not improve from 29.05165
196/196 - 40s - loss: 29.8336 - MinusLogProbMetric: 29.8336 - val_loss: 29.6365 - val_MinusLogProbMetric: 29.6365 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 151/1000
2023-10-25 03:03:25.881 
Epoch 151/1000 
	 loss: 29.3019, MinusLogProbMetric: 29.3019, val_loss: 29.0730, val_MinusLogProbMetric: 29.0730

Epoch 151: val_loss did not improve from 29.05165
196/196 - 42s - loss: 29.3019 - MinusLogProbMetric: 29.3019 - val_loss: 29.0730 - val_MinusLogProbMetric: 29.0730 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 152/1000
2023-10-25 03:04:08.947 
Epoch 152/1000 
	 loss: 29.4413, MinusLogProbMetric: 29.4413, val_loss: 29.3469, val_MinusLogProbMetric: 29.3469

Epoch 152: val_loss did not improve from 29.05165
196/196 - 43s - loss: 29.4413 - MinusLogProbMetric: 29.4413 - val_loss: 29.3469 - val_MinusLogProbMetric: 29.3469 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 153/1000
2023-10-25 03:04:51.335 
Epoch 153/1000 
	 loss: 29.3155, MinusLogProbMetric: 29.3155, val_loss: 29.4145, val_MinusLogProbMetric: 29.4145

Epoch 153: val_loss did not improve from 29.05165
196/196 - 42s - loss: 29.3155 - MinusLogProbMetric: 29.3155 - val_loss: 29.4145 - val_MinusLogProbMetric: 29.4145 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 154/1000
2023-10-25 03:05:33.584 
Epoch 154/1000 
	 loss: 29.5694, MinusLogProbMetric: 29.5694, val_loss: 29.3651, val_MinusLogProbMetric: 29.3651

Epoch 154: val_loss did not improve from 29.05165
196/196 - 42s - loss: 29.5694 - MinusLogProbMetric: 29.5694 - val_loss: 29.3651 - val_MinusLogProbMetric: 29.3651 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 155/1000
2023-10-25 03:06:14.803 
Epoch 155/1000 
	 loss: 29.1778, MinusLogProbMetric: 29.1778, val_loss: 29.3775, val_MinusLogProbMetric: 29.3775

Epoch 155: val_loss did not improve from 29.05165
196/196 - 41s - loss: 29.1778 - MinusLogProbMetric: 29.1778 - val_loss: 29.3775 - val_MinusLogProbMetric: 29.3775 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 156/1000
2023-10-25 03:06:57.057 
Epoch 156/1000 
	 loss: 29.4516, MinusLogProbMetric: 29.4516, val_loss: 29.8083, val_MinusLogProbMetric: 29.8083

Epoch 156: val_loss did not improve from 29.05165
196/196 - 42s - loss: 29.4516 - MinusLogProbMetric: 29.4516 - val_loss: 29.8083 - val_MinusLogProbMetric: 29.8083 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 157/1000
2023-10-25 03:07:39.833 
Epoch 157/1000 
	 loss: 29.1695, MinusLogProbMetric: 29.1695, val_loss: 29.2172, val_MinusLogProbMetric: 29.2172

Epoch 157: val_loss did not improve from 29.05165
196/196 - 43s - loss: 29.1695 - MinusLogProbMetric: 29.1695 - val_loss: 29.2172 - val_MinusLogProbMetric: 29.2172 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 158/1000
2023-10-25 03:08:21.970 
Epoch 158/1000 
	 loss: 29.6010, MinusLogProbMetric: 29.6010, val_loss: 29.2795, val_MinusLogProbMetric: 29.2795

Epoch 158: val_loss did not improve from 29.05165
196/196 - 42s - loss: 29.6010 - MinusLogProbMetric: 29.6010 - val_loss: 29.2795 - val_MinusLogProbMetric: 29.2795 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 159/1000
2023-10-25 03:09:04.352 
Epoch 159/1000 
	 loss: 29.1936, MinusLogProbMetric: 29.1936, val_loss: 29.4102, val_MinusLogProbMetric: 29.4102

Epoch 159: val_loss did not improve from 29.05165
196/196 - 42s - loss: 29.1936 - MinusLogProbMetric: 29.1936 - val_loss: 29.4102 - val_MinusLogProbMetric: 29.4102 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 160/1000
2023-10-25 03:09:43.479 
Epoch 160/1000 
	 loss: 29.3358, MinusLogProbMetric: 29.3358, val_loss: 29.4643, val_MinusLogProbMetric: 29.4643

Epoch 160: val_loss did not improve from 29.05165
196/196 - 39s - loss: 29.3358 - MinusLogProbMetric: 29.3358 - val_loss: 29.4643 - val_MinusLogProbMetric: 29.4643 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 161/1000
2023-10-25 03:10:20.386 
Epoch 161/1000 
	 loss: 29.3788, MinusLogProbMetric: 29.3788, val_loss: 29.7749, val_MinusLogProbMetric: 29.7749

Epoch 161: val_loss did not improve from 29.05165
196/196 - 37s - loss: 29.3788 - MinusLogProbMetric: 29.3788 - val_loss: 29.7749 - val_MinusLogProbMetric: 29.7749 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 162/1000
2023-10-25 03:11:02.324 
Epoch 162/1000 
	 loss: 29.3131, MinusLogProbMetric: 29.3131, val_loss: 30.4395, val_MinusLogProbMetric: 30.4395

Epoch 162: val_loss did not improve from 29.05165
196/196 - 42s - loss: 29.3131 - MinusLogProbMetric: 29.3131 - val_loss: 30.4395 - val_MinusLogProbMetric: 30.4395 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 163/1000
2023-10-25 03:11:43.587 
Epoch 163/1000 
	 loss: 29.2419, MinusLogProbMetric: 29.2419, val_loss: 29.2871, val_MinusLogProbMetric: 29.2871

Epoch 163: val_loss did not improve from 29.05165
196/196 - 41s - loss: 29.2419 - MinusLogProbMetric: 29.2419 - val_loss: 29.2871 - val_MinusLogProbMetric: 29.2871 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 164/1000
2023-10-25 03:12:26.239 
Epoch 164/1000 
	 loss: 29.0833, MinusLogProbMetric: 29.0833, val_loss: 29.6200, val_MinusLogProbMetric: 29.6200

Epoch 164: val_loss did not improve from 29.05165
196/196 - 43s - loss: 29.0833 - MinusLogProbMetric: 29.0833 - val_loss: 29.6200 - val_MinusLogProbMetric: 29.6200 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 165/1000
2023-10-25 03:13:08.686 
Epoch 165/1000 
	 loss: 29.2231, MinusLogProbMetric: 29.2231, val_loss: 30.9954, val_MinusLogProbMetric: 30.9954

Epoch 165: val_loss did not improve from 29.05165
196/196 - 42s - loss: 29.2231 - MinusLogProbMetric: 29.2231 - val_loss: 30.9954 - val_MinusLogProbMetric: 30.9954 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 166/1000
2023-10-25 03:13:50.307 
Epoch 166/1000 
	 loss: 29.5588, MinusLogProbMetric: 29.5588, val_loss: 29.5508, val_MinusLogProbMetric: 29.5508

Epoch 166: val_loss did not improve from 29.05165
196/196 - 42s - loss: 29.5588 - MinusLogProbMetric: 29.5588 - val_loss: 29.5508 - val_MinusLogProbMetric: 29.5508 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 167/1000
2023-10-25 03:14:32.206 
Epoch 167/1000 
	 loss: 29.4289, MinusLogProbMetric: 29.4289, val_loss: 29.0962, val_MinusLogProbMetric: 29.0962

Epoch 167: val_loss did not improve from 29.05165
196/196 - 42s - loss: 29.4289 - MinusLogProbMetric: 29.4289 - val_loss: 29.0962 - val_MinusLogProbMetric: 29.0962 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 168/1000
2023-10-25 03:15:14.878 
Epoch 168/1000 
	 loss: 29.1835, MinusLogProbMetric: 29.1835, val_loss: 29.1911, val_MinusLogProbMetric: 29.1911

Epoch 168: val_loss did not improve from 29.05165
196/196 - 43s - loss: 29.1835 - MinusLogProbMetric: 29.1835 - val_loss: 29.1911 - val_MinusLogProbMetric: 29.1911 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 169/1000
2023-10-25 03:15:57.098 
Epoch 169/1000 
	 loss: 29.3115, MinusLogProbMetric: 29.3115, val_loss: 29.1063, val_MinusLogProbMetric: 29.1063

Epoch 169: val_loss did not improve from 29.05165
196/196 - 42s - loss: 29.3115 - MinusLogProbMetric: 29.3115 - val_loss: 29.1063 - val_MinusLogProbMetric: 29.1063 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 170/1000
2023-10-25 03:16:39.138 
Epoch 170/1000 
	 loss: 29.3426, MinusLogProbMetric: 29.3426, val_loss: 29.5564, val_MinusLogProbMetric: 29.5564

Epoch 170: val_loss did not improve from 29.05165
196/196 - 42s - loss: 29.3426 - MinusLogProbMetric: 29.3426 - val_loss: 29.5564 - val_MinusLogProbMetric: 29.5564 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 171/1000
2023-10-25 03:17:21.184 
Epoch 171/1000 
	 loss: 29.3516, MinusLogProbMetric: 29.3516, val_loss: 31.1587, val_MinusLogProbMetric: 31.1587

Epoch 171: val_loss did not improve from 29.05165
196/196 - 42s - loss: 29.3516 - MinusLogProbMetric: 29.3516 - val_loss: 31.1587 - val_MinusLogProbMetric: 31.1587 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 172/1000
2023-10-25 03:18:03.758 
Epoch 172/1000 
	 loss: 29.1224, MinusLogProbMetric: 29.1224, val_loss: 29.2448, val_MinusLogProbMetric: 29.2448

Epoch 172: val_loss did not improve from 29.05165
196/196 - 43s - loss: 29.1224 - MinusLogProbMetric: 29.1224 - val_loss: 29.2448 - val_MinusLogProbMetric: 29.2448 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 173/1000
2023-10-25 03:18:45.911 
Epoch 173/1000 
	 loss: 29.2775, MinusLogProbMetric: 29.2775, val_loss: 29.2586, val_MinusLogProbMetric: 29.2586

Epoch 173: val_loss did not improve from 29.05165
196/196 - 42s - loss: 29.2775 - MinusLogProbMetric: 29.2775 - val_loss: 29.2586 - val_MinusLogProbMetric: 29.2586 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 174/1000
2023-10-25 03:19:27.674 
Epoch 174/1000 
	 loss: 29.1054, MinusLogProbMetric: 29.1054, val_loss: 29.7233, val_MinusLogProbMetric: 29.7233

Epoch 174: val_loss did not improve from 29.05165
196/196 - 42s - loss: 29.1054 - MinusLogProbMetric: 29.1054 - val_loss: 29.7233 - val_MinusLogProbMetric: 29.7233 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 175/1000
2023-10-25 03:20:10.643 
Epoch 175/1000 
	 loss: 29.5326, MinusLogProbMetric: 29.5326, val_loss: 34.6154, val_MinusLogProbMetric: 34.6154

Epoch 175: val_loss did not improve from 29.05165
196/196 - 43s - loss: 29.5326 - MinusLogProbMetric: 29.5326 - val_loss: 34.6154 - val_MinusLogProbMetric: 34.6154 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 176/1000
2023-10-25 03:20:53.228 
Epoch 176/1000 
	 loss: 29.3095, MinusLogProbMetric: 29.3095, val_loss: 29.3159, val_MinusLogProbMetric: 29.3159

Epoch 176: val_loss did not improve from 29.05165
196/196 - 43s - loss: 29.3095 - MinusLogProbMetric: 29.3095 - val_loss: 29.3159 - val_MinusLogProbMetric: 29.3159 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 177/1000
2023-10-25 03:21:35.367 
Epoch 177/1000 
	 loss: 29.4042, MinusLogProbMetric: 29.4042, val_loss: 29.3941, val_MinusLogProbMetric: 29.3941

Epoch 177: val_loss did not improve from 29.05165
196/196 - 42s - loss: 29.4042 - MinusLogProbMetric: 29.4042 - val_loss: 29.3941 - val_MinusLogProbMetric: 29.3941 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 178/1000
2023-10-25 03:22:18.185 
Epoch 178/1000 
	 loss: 29.1218, MinusLogProbMetric: 29.1218, val_loss: 28.9586, val_MinusLogProbMetric: 28.9586

Epoch 178: val_loss improved from 29.05165 to 28.95856, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 44s - loss: 29.1218 - MinusLogProbMetric: 29.1218 - val_loss: 28.9586 - val_MinusLogProbMetric: 28.9586 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 179/1000
2023-10-25 03:23:01.045 
Epoch 179/1000 
	 loss: 29.1102, MinusLogProbMetric: 29.1102, val_loss: 30.0059, val_MinusLogProbMetric: 30.0059

Epoch 179: val_loss did not improve from 28.95856
196/196 - 42s - loss: 29.1102 - MinusLogProbMetric: 29.1102 - val_loss: 30.0059 - val_MinusLogProbMetric: 30.0059 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 180/1000
2023-10-25 03:23:43.522 
Epoch 180/1000 
	 loss: 29.1115, MinusLogProbMetric: 29.1115, val_loss: 29.1099, val_MinusLogProbMetric: 29.1099

Epoch 180: val_loss did not improve from 28.95856
196/196 - 42s - loss: 29.1115 - MinusLogProbMetric: 29.1115 - val_loss: 29.1099 - val_MinusLogProbMetric: 29.1099 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 181/1000
2023-10-25 03:24:23.811 
Epoch 181/1000 
	 loss: 29.1057, MinusLogProbMetric: 29.1057, val_loss: 29.7129, val_MinusLogProbMetric: 29.7129

Epoch 181: val_loss did not improve from 28.95856
196/196 - 40s - loss: 29.1057 - MinusLogProbMetric: 29.1057 - val_loss: 29.7129 - val_MinusLogProbMetric: 29.7129 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 182/1000
2023-10-25 03:25:04.186 
Epoch 182/1000 
	 loss: 29.2118, MinusLogProbMetric: 29.2118, val_loss: 29.7756, val_MinusLogProbMetric: 29.7756

Epoch 182: val_loss did not improve from 28.95856
196/196 - 40s - loss: 29.2118 - MinusLogProbMetric: 29.2118 - val_loss: 29.7756 - val_MinusLogProbMetric: 29.7756 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 183/1000
2023-10-25 03:25:46.438 
Epoch 183/1000 
	 loss: 29.2044, MinusLogProbMetric: 29.2044, val_loss: 29.1983, val_MinusLogProbMetric: 29.1983

Epoch 183: val_loss did not improve from 28.95856
196/196 - 42s - loss: 29.2044 - MinusLogProbMetric: 29.2044 - val_loss: 29.1983 - val_MinusLogProbMetric: 29.1983 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 184/1000
2023-10-25 03:26:28.197 
Epoch 184/1000 
	 loss: 29.0187, MinusLogProbMetric: 29.0187, val_loss: 29.5120, val_MinusLogProbMetric: 29.5120

Epoch 184: val_loss did not improve from 28.95856
196/196 - 42s - loss: 29.0187 - MinusLogProbMetric: 29.0187 - val_loss: 29.5120 - val_MinusLogProbMetric: 29.5120 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 185/1000
2023-10-25 03:27:10.742 
Epoch 185/1000 
	 loss: 29.2499, MinusLogProbMetric: 29.2499, val_loss: 30.5150, val_MinusLogProbMetric: 30.5150

Epoch 185: val_loss did not improve from 28.95856
196/196 - 43s - loss: 29.2499 - MinusLogProbMetric: 29.2499 - val_loss: 30.5150 - val_MinusLogProbMetric: 30.5150 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 186/1000
2023-10-25 03:27:52.328 
Epoch 186/1000 
	 loss: 29.3205, MinusLogProbMetric: 29.3205, val_loss: 29.4267, val_MinusLogProbMetric: 29.4267

Epoch 186: val_loss did not improve from 28.95856
196/196 - 42s - loss: 29.3205 - MinusLogProbMetric: 29.3205 - val_loss: 29.4267 - val_MinusLogProbMetric: 29.4267 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 187/1000
2023-10-25 03:28:33.894 
Epoch 187/1000 
	 loss: 29.0925, MinusLogProbMetric: 29.0925, val_loss: 29.1367, val_MinusLogProbMetric: 29.1367

Epoch 187: val_loss did not improve from 28.95856
196/196 - 42s - loss: 29.0925 - MinusLogProbMetric: 29.0925 - val_loss: 29.1367 - val_MinusLogProbMetric: 29.1367 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 188/1000
2023-10-25 03:29:15.178 
Epoch 188/1000 
	 loss: 29.0086, MinusLogProbMetric: 29.0086, val_loss: 29.6357, val_MinusLogProbMetric: 29.6357

Epoch 188: val_loss did not improve from 28.95856
196/196 - 41s - loss: 29.0086 - MinusLogProbMetric: 29.0086 - val_loss: 29.6357 - val_MinusLogProbMetric: 29.6357 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 189/1000
2023-10-25 03:29:56.881 
Epoch 189/1000 
	 loss: 29.1412, MinusLogProbMetric: 29.1412, val_loss: 29.3096, val_MinusLogProbMetric: 29.3096

Epoch 189: val_loss did not improve from 28.95856
196/196 - 42s - loss: 29.1412 - MinusLogProbMetric: 29.1412 - val_loss: 29.3096 - val_MinusLogProbMetric: 29.3096 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 190/1000
2023-10-25 03:30:40.162 
Epoch 190/1000 
	 loss: 29.2162, MinusLogProbMetric: 29.2162, val_loss: 29.0124, val_MinusLogProbMetric: 29.0124

Epoch 190: val_loss did not improve from 28.95856
196/196 - 43s - loss: 29.2162 - MinusLogProbMetric: 29.2162 - val_loss: 29.0124 - val_MinusLogProbMetric: 29.0124 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 191/1000
2023-10-25 03:31:23.345 
Epoch 191/1000 
	 loss: 29.0205, MinusLogProbMetric: 29.0205, val_loss: 29.0426, val_MinusLogProbMetric: 29.0426

Epoch 191: val_loss did not improve from 28.95856
196/196 - 43s - loss: 29.0205 - MinusLogProbMetric: 29.0205 - val_loss: 29.0426 - val_MinusLogProbMetric: 29.0426 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 192/1000
2023-10-25 03:32:06.541 
Epoch 192/1000 
	 loss: 29.2946, MinusLogProbMetric: 29.2946, val_loss: 29.0113, val_MinusLogProbMetric: 29.0113

Epoch 192: val_loss did not improve from 28.95856
196/196 - 43s - loss: 29.2946 - MinusLogProbMetric: 29.2946 - val_loss: 29.0113 - val_MinusLogProbMetric: 29.0113 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 193/1000
2023-10-25 03:32:49.627 
Epoch 193/1000 
	 loss: 29.0018, MinusLogProbMetric: 29.0018, val_loss: 29.5821, val_MinusLogProbMetric: 29.5821

Epoch 193: val_loss did not improve from 28.95856
196/196 - 43s - loss: 29.0018 - MinusLogProbMetric: 29.0018 - val_loss: 29.5821 - val_MinusLogProbMetric: 29.5821 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 194/1000
2023-10-25 03:33:32.390 
Epoch 194/1000 
	 loss: 29.1817, MinusLogProbMetric: 29.1817, val_loss: 29.1311, val_MinusLogProbMetric: 29.1311

Epoch 194: val_loss did not improve from 28.95856
196/196 - 43s - loss: 29.1817 - MinusLogProbMetric: 29.1817 - val_loss: 29.1311 - val_MinusLogProbMetric: 29.1311 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 195/1000
2023-10-25 03:34:15.237 
Epoch 195/1000 
	 loss: 29.1986, MinusLogProbMetric: 29.1986, val_loss: 29.5124, val_MinusLogProbMetric: 29.5124

Epoch 195: val_loss did not improve from 28.95856
196/196 - 43s - loss: 29.1986 - MinusLogProbMetric: 29.1986 - val_loss: 29.5124 - val_MinusLogProbMetric: 29.5124 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 196/1000
2023-10-25 03:34:56.902 
Epoch 196/1000 
	 loss: 29.0219, MinusLogProbMetric: 29.0219, val_loss: 29.4447, val_MinusLogProbMetric: 29.4447

Epoch 196: val_loss did not improve from 28.95856
196/196 - 42s - loss: 29.0219 - MinusLogProbMetric: 29.0219 - val_loss: 29.4447 - val_MinusLogProbMetric: 29.4447 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 197/1000
2023-10-25 03:35:39.174 
Epoch 197/1000 
	 loss: 29.1244, MinusLogProbMetric: 29.1244, val_loss: 29.6110, val_MinusLogProbMetric: 29.6110

Epoch 197: val_loss did not improve from 28.95856
196/196 - 42s - loss: 29.1244 - MinusLogProbMetric: 29.1244 - val_loss: 29.6110 - val_MinusLogProbMetric: 29.6110 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 198/1000
2023-10-25 03:36:20.841 
Epoch 198/1000 
	 loss: 28.9231, MinusLogProbMetric: 28.9231, val_loss: 29.0020, val_MinusLogProbMetric: 29.0020

Epoch 198: val_loss did not improve from 28.95856
196/196 - 42s - loss: 28.9231 - MinusLogProbMetric: 28.9231 - val_loss: 29.0020 - val_MinusLogProbMetric: 29.0020 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 199/1000
2023-10-25 03:37:01.562 
Epoch 199/1000 
	 loss: 29.0127, MinusLogProbMetric: 29.0127, val_loss: 30.3574, val_MinusLogProbMetric: 30.3574

Epoch 199: val_loss did not improve from 28.95856
196/196 - 41s - loss: 29.0127 - MinusLogProbMetric: 29.0127 - val_loss: 30.3574 - val_MinusLogProbMetric: 30.3574 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 200/1000
2023-10-25 03:37:43.424 
Epoch 200/1000 
	 loss: 28.9765, MinusLogProbMetric: 28.9765, val_loss: 28.9320, val_MinusLogProbMetric: 28.9320

Epoch 200: val_loss improved from 28.95856 to 28.93199, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 28.9765 - MinusLogProbMetric: 28.9765 - val_loss: 28.9320 - val_MinusLogProbMetric: 28.9320 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 201/1000
2023-10-25 03:38:26.831 
Epoch 201/1000 
	 loss: 29.1187, MinusLogProbMetric: 29.1187, val_loss: 29.6938, val_MinusLogProbMetric: 29.6938

Epoch 201: val_loss did not improve from 28.93199
196/196 - 43s - loss: 29.1187 - MinusLogProbMetric: 29.1187 - val_loss: 29.6938 - val_MinusLogProbMetric: 29.6938 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 202/1000
2023-10-25 03:39:08.165 
Epoch 202/1000 
	 loss: 29.2571, MinusLogProbMetric: 29.2571, val_loss: 29.4280, val_MinusLogProbMetric: 29.4280

Epoch 202: val_loss did not improve from 28.93199
196/196 - 41s - loss: 29.2571 - MinusLogProbMetric: 29.2571 - val_loss: 29.4280 - val_MinusLogProbMetric: 29.4280 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 203/1000
2023-10-25 03:39:50.637 
Epoch 203/1000 
	 loss: 28.9258, MinusLogProbMetric: 28.9258, val_loss: 28.9566, val_MinusLogProbMetric: 28.9566

Epoch 203: val_loss did not improve from 28.93199
196/196 - 42s - loss: 28.9258 - MinusLogProbMetric: 28.9258 - val_loss: 28.9566 - val_MinusLogProbMetric: 28.9566 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 204/1000
2023-10-25 03:40:33.106 
Epoch 204/1000 
	 loss: 29.2546, MinusLogProbMetric: 29.2546, val_loss: 29.2577, val_MinusLogProbMetric: 29.2577

Epoch 204: val_loss did not improve from 28.93199
196/196 - 42s - loss: 29.2546 - MinusLogProbMetric: 29.2546 - val_loss: 29.2577 - val_MinusLogProbMetric: 29.2577 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 205/1000
2023-10-25 03:41:10.509 
Epoch 205/1000 
	 loss: 29.0415, MinusLogProbMetric: 29.0415, val_loss: 31.5275, val_MinusLogProbMetric: 31.5275

Epoch 205: val_loss did not improve from 28.93199
196/196 - 37s - loss: 29.0415 - MinusLogProbMetric: 29.0415 - val_loss: 31.5275 - val_MinusLogProbMetric: 31.5275 - lr: 3.3333e-04 - 37s/epoch - 191ms/step
Epoch 206/1000
2023-10-25 03:41:53.169 
Epoch 206/1000 
	 loss: 29.0165, MinusLogProbMetric: 29.0165, val_loss: 29.1049, val_MinusLogProbMetric: 29.1049

Epoch 206: val_loss did not improve from 28.93199
196/196 - 43s - loss: 29.0165 - MinusLogProbMetric: 29.0165 - val_loss: 29.1049 - val_MinusLogProbMetric: 29.1049 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 207/1000
2023-10-25 03:42:35.514 
Epoch 207/1000 
	 loss: 29.0594, MinusLogProbMetric: 29.0594, val_loss: 29.0922, val_MinusLogProbMetric: 29.0922

Epoch 207: val_loss did not improve from 28.93199
196/196 - 42s - loss: 29.0594 - MinusLogProbMetric: 29.0594 - val_loss: 29.0922 - val_MinusLogProbMetric: 29.0922 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 208/1000
2023-10-25 03:43:17.085 
Epoch 208/1000 
	 loss: 29.4468, MinusLogProbMetric: 29.4468, val_loss: 29.4215, val_MinusLogProbMetric: 29.4215

Epoch 208: val_loss did not improve from 28.93199
196/196 - 42s - loss: 29.4468 - MinusLogProbMetric: 29.4468 - val_loss: 29.4215 - val_MinusLogProbMetric: 29.4215 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 209/1000
2023-10-25 03:43:59.427 
Epoch 209/1000 
	 loss: 28.9903, MinusLogProbMetric: 28.9903, val_loss: 29.7782, val_MinusLogProbMetric: 29.7782

Epoch 209: val_loss did not improve from 28.93199
196/196 - 42s - loss: 28.9903 - MinusLogProbMetric: 28.9903 - val_loss: 29.7782 - val_MinusLogProbMetric: 29.7782 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 210/1000
2023-10-25 03:44:41.263 
Epoch 210/1000 
	 loss: 28.8661, MinusLogProbMetric: 28.8661, val_loss: 28.9122, val_MinusLogProbMetric: 28.9122

Epoch 210: val_loss improved from 28.93199 to 28.91216, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 28.8661 - MinusLogProbMetric: 28.8661 - val_loss: 28.9122 - val_MinusLogProbMetric: 28.9122 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 211/1000
2023-10-25 03:45:24.714 
Epoch 211/1000 
	 loss: 29.0287, MinusLogProbMetric: 29.0287, val_loss: 28.9201, val_MinusLogProbMetric: 28.9201

Epoch 211: val_loss did not improve from 28.91216
196/196 - 43s - loss: 29.0287 - MinusLogProbMetric: 29.0287 - val_loss: 28.9201 - val_MinusLogProbMetric: 28.9201 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 212/1000
2023-10-25 03:46:06.530 
Epoch 212/1000 
	 loss: 29.0418, MinusLogProbMetric: 29.0418, val_loss: 29.0062, val_MinusLogProbMetric: 29.0062

Epoch 212: val_loss did not improve from 28.91216
196/196 - 42s - loss: 29.0418 - MinusLogProbMetric: 29.0418 - val_loss: 29.0062 - val_MinusLogProbMetric: 29.0062 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 213/1000
2023-10-25 03:46:49.274 
Epoch 213/1000 
	 loss: 28.8968, MinusLogProbMetric: 28.8968, val_loss: 31.3305, val_MinusLogProbMetric: 31.3305

Epoch 213: val_loss did not improve from 28.91216
196/196 - 43s - loss: 28.8968 - MinusLogProbMetric: 28.8968 - val_loss: 31.3305 - val_MinusLogProbMetric: 31.3305 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 214/1000
2023-10-25 03:47:30.753 
Epoch 214/1000 
	 loss: 28.9959, MinusLogProbMetric: 28.9959, val_loss: 29.0811, val_MinusLogProbMetric: 29.0811

Epoch 214: val_loss did not improve from 28.91216
196/196 - 41s - loss: 28.9959 - MinusLogProbMetric: 28.9959 - val_loss: 29.0811 - val_MinusLogProbMetric: 29.0811 - lr: 3.3333e-04 - 41s/epoch - 212ms/step
Epoch 215/1000
2023-10-25 03:48:12.176 
Epoch 215/1000 
	 loss: 28.8893, MinusLogProbMetric: 28.8893, val_loss: 29.1247, val_MinusLogProbMetric: 29.1247

Epoch 215: val_loss did not improve from 28.91216
196/196 - 41s - loss: 28.8893 - MinusLogProbMetric: 28.8893 - val_loss: 29.1247 - val_MinusLogProbMetric: 29.1247 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 216/1000
2023-10-25 03:48:54.406 
Epoch 216/1000 
	 loss: 29.0844, MinusLogProbMetric: 29.0844, val_loss: 31.6189, val_MinusLogProbMetric: 31.6189

Epoch 216: val_loss did not improve from 28.91216
196/196 - 42s - loss: 29.0844 - MinusLogProbMetric: 29.0844 - val_loss: 31.6189 - val_MinusLogProbMetric: 31.6189 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 217/1000
2023-10-25 03:49:36.380 
Epoch 217/1000 
	 loss: 28.9901, MinusLogProbMetric: 28.9901, val_loss: 28.7915, val_MinusLogProbMetric: 28.7915

Epoch 217: val_loss improved from 28.91216 to 28.79146, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 28.9901 - MinusLogProbMetric: 28.9901 - val_loss: 28.7915 - val_MinusLogProbMetric: 28.7915 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 218/1000
2023-10-25 03:50:18.604 
Epoch 218/1000 
	 loss: 28.9983, MinusLogProbMetric: 28.9983, val_loss: 29.5376, val_MinusLogProbMetric: 29.5376

Epoch 218: val_loss did not improve from 28.79146
196/196 - 41s - loss: 28.9983 - MinusLogProbMetric: 28.9983 - val_loss: 29.5376 - val_MinusLogProbMetric: 29.5376 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 219/1000
2023-10-25 03:50:57.114 
Epoch 219/1000 
	 loss: 28.8419, MinusLogProbMetric: 28.8419, val_loss: 28.8504, val_MinusLogProbMetric: 28.8504

Epoch 219: val_loss did not improve from 28.79146
196/196 - 39s - loss: 28.8419 - MinusLogProbMetric: 28.8419 - val_loss: 28.8504 - val_MinusLogProbMetric: 28.8504 - lr: 3.3333e-04 - 39s/epoch - 196ms/step
Epoch 220/1000
2023-10-25 03:51:37.817 
Epoch 220/1000 
	 loss: 29.0392, MinusLogProbMetric: 29.0392, val_loss: 29.4962, val_MinusLogProbMetric: 29.4962

Epoch 220: val_loss did not improve from 28.79146
196/196 - 41s - loss: 29.0392 - MinusLogProbMetric: 29.0392 - val_loss: 29.4962 - val_MinusLogProbMetric: 29.4962 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 221/1000
2023-10-25 03:52:20.138 
Epoch 221/1000 
	 loss: 28.8020, MinusLogProbMetric: 28.8020, val_loss: 28.8339, val_MinusLogProbMetric: 28.8339

Epoch 221: val_loss did not improve from 28.79146
196/196 - 42s - loss: 28.8020 - MinusLogProbMetric: 28.8020 - val_loss: 28.8339 - val_MinusLogProbMetric: 28.8339 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 222/1000
2023-10-25 03:53:02.749 
Epoch 222/1000 
	 loss: 28.8531, MinusLogProbMetric: 28.8531, val_loss: 29.0407, val_MinusLogProbMetric: 29.0407

Epoch 222: val_loss did not improve from 28.79146
196/196 - 43s - loss: 28.8531 - MinusLogProbMetric: 28.8531 - val_loss: 29.0407 - val_MinusLogProbMetric: 29.0407 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 223/1000
2023-10-25 03:53:44.165 
Epoch 223/1000 
	 loss: 28.8211, MinusLogProbMetric: 28.8211, val_loss: 28.8546, val_MinusLogProbMetric: 28.8546

Epoch 223: val_loss did not improve from 28.79146
196/196 - 41s - loss: 28.8211 - MinusLogProbMetric: 28.8211 - val_loss: 28.8546 - val_MinusLogProbMetric: 28.8546 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 224/1000
2023-10-25 03:54:25.238 
Epoch 224/1000 
	 loss: 29.0274, MinusLogProbMetric: 29.0274, val_loss: 28.9932, val_MinusLogProbMetric: 28.9932

Epoch 224: val_loss did not improve from 28.79146
196/196 - 41s - loss: 29.0274 - MinusLogProbMetric: 29.0274 - val_loss: 28.9932 - val_MinusLogProbMetric: 28.9932 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 225/1000
2023-10-25 03:55:07.569 
Epoch 225/1000 
	 loss: 28.8388, MinusLogProbMetric: 28.8388, val_loss: 28.8479, val_MinusLogProbMetric: 28.8479

Epoch 225: val_loss did not improve from 28.79146
196/196 - 42s - loss: 28.8388 - MinusLogProbMetric: 28.8388 - val_loss: 28.8479 - val_MinusLogProbMetric: 28.8479 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 226/1000
2023-10-25 03:55:49.557 
Epoch 226/1000 
	 loss: 28.7672, MinusLogProbMetric: 28.7672, val_loss: 29.7826, val_MinusLogProbMetric: 29.7826

Epoch 226: val_loss did not improve from 28.79146
196/196 - 42s - loss: 28.7672 - MinusLogProbMetric: 28.7672 - val_loss: 29.7826 - val_MinusLogProbMetric: 29.7826 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 227/1000
2023-10-25 03:56:31.584 
Epoch 227/1000 
	 loss: 29.2086, MinusLogProbMetric: 29.2086, val_loss: 29.1705, val_MinusLogProbMetric: 29.1705

Epoch 227: val_loss did not improve from 28.79146
196/196 - 42s - loss: 29.2086 - MinusLogProbMetric: 29.2086 - val_loss: 29.1705 - val_MinusLogProbMetric: 29.1705 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 228/1000
2023-10-25 03:57:14.270 
Epoch 228/1000 
	 loss: 28.8712, MinusLogProbMetric: 28.8712, val_loss: 29.3395, val_MinusLogProbMetric: 29.3395

Epoch 228: val_loss did not improve from 28.79146
196/196 - 43s - loss: 28.8712 - MinusLogProbMetric: 28.8712 - val_loss: 29.3395 - val_MinusLogProbMetric: 29.3395 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 229/1000
2023-10-25 03:57:56.683 
Epoch 229/1000 
	 loss: 28.7635, MinusLogProbMetric: 28.7635, val_loss: 30.8460, val_MinusLogProbMetric: 30.8460

Epoch 229: val_loss did not improve from 28.79146
196/196 - 42s - loss: 28.7635 - MinusLogProbMetric: 28.7635 - val_loss: 30.8460 - val_MinusLogProbMetric: 30.8460 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 230/1000
2023-10-25 03:58:38.980 
Epoch 230/1000 
	 loss: 28.8331, MinusLogProbMetric: 28.8331, val_loss: 30.2568, val_MinusLogProbMetric: 30.2568

Epoch 230: val_loss did not improve from 28.79146
196/196 - 42s - loss: 28.8331 - MinusLogProbMetric: 28.8331 - val_loss: 30.2568 - val_MinusLogProbMetric: 30.2568 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 231/1000
2023-10-25 03:59:21.235 
Epoch 231/1000 
	 loss: 28.8432, MinusLogProbMetric: 28.8432, val_loss: 29.8632, val_MinusLogProbMetric: 29.8632

Epoch 231: val_loss did not improve from 28.79146
196/196 - 42s - loss: 28.8432 - MinusLogProbMetric: 28.8432 - val_loss: 29.8632 - val_MinusLogProbMetric: 29.8632 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 232/1000
2023-10-25 04:00:02.906 
Epoch 232/1000 
	 loss: 29.3507, MinusLogProbMetric: 29.3507, val_loss: 28.9304, val_MinusLogProbMetric: 28.9304

Epoch 232: val_loss did not improve from 28.79146
196/196 - 42s - loss: 29.3507 - MinusLogProbMetric: 29.3507 - val_loss: 28.9304 - val_MinusLogProbMetric: 28.9304 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 233/1000
2023-10-25 04:00:44.468 
Epoch 233/1000 
	 loss: 28.7866, MinusLogProbMetric: 28.7866, val_loss: 29.4010, val_MinusLogProbMetric: 29.4010

Epoch 233: val_loss did not improve from 28.79146
196/196 - 42s - loss: 28.7866 - MinusLogProbMetric: 28.7866 - val_loss: 29.4010 - val_MinusLogProbMetric: 29.4010 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 234/1000
2023-10-25 04:01:25.739 
Epoch 234/1000 
	 loss: 28.9204, MinusLogProbMetric: 28.9204, val_loss: 28.6714, val_MinusLogProbMetric: 28.6714

Epoch 234: val_loss improved from 28.79146 to 28.67137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 28.9204 - MinusLogProbMetric: 28.9204 - val_loss: 28.6714 - val_MinusLogProbMetric: 28.6714 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 235/1000
2023-10-25 04:02:08.425 
Epoch 235/1000 
	 loss: 29.0365, MinusLogProbMetric: 29.0365, val_loss: 29.0046, val_MinusLogProbMetric: 29.0046

Epoch 235: val_loss did not improve from 28.67137
196/196 - 42s - loss: 29.0365 - MinusLogProbMetric: 29.0365 - val_loss: 29.0046 - val_MinusLogProbMetric: 29.0046 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 236/1000
2023-10-25 04:02:51.431 
Epoch 236/1000 
	 loss: 28.9169, MinusLogProbMetric: 28.9169, val_loss: 29.1499, val_MinusLogProbMetric: 29.1499

Epoch 236: val_loss did not improve from 28.67137
196/196 - 43s - loss: 28.9169 - MinusLogProbMetric: 28.9169 - val_loss: 29.1499 - val_MinusLogProbMetric: 29.1499 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 237/1000
2023-10-25 04:03:34.000 
Epoch 237/1000 
	 loss: 28.7335, MinusLogProbMetric: 28.7335, val_loss: 29.1029, val_MinusLogProbMetric: 29.1029

Epoch 237: val_loss did not improve from 28.67137
196/196 - 43s - loss: 28.7335 - MinusLogProbMetric: 28.7335 - val_loss: 29.1029 - val_MinusLogProbMetric: 29.1029 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 238/1000
2023-10-25 04:04:15.070 
Epoch 238/1000 
	 loss: 28.8193, MinusLogProbMetric: 28.8193, val_loss: 29.0040, val_MinusLogProbMetric: 29.0040

Epoch 238: val_loss did not improve from 28.67137
196/196 - 41s - loss: 28.8193 - MinusLogProbMetric: 28.8193 - val_loss: 29.0040 - val_MinusLogProbMetric: 29.0040 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 239/1000
2023-10-25 04:04:57.582 
Epoch 239/1000 
	 loss: 28.9818, MinusLogProbMetric: 28.9818, val_loss: 28.8695, val_MinusLogProbMetric: 28.8695

Epoch 239: val_loss did not improve from 28.67137
196/196 - 43s - loss: 28.9818 - MinusLogProbMetric: 28.9818 - val_loss: 28.8695 - val_MinusLogProbMetric: 28.8695 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 240/1000
2023-10-25 04:05:40.382 
Epoch 240/1000 
	 loss: 28.9592, MinusLogProbMetric: 28.9592, val_loss: 28.9534, val_MinusLogProbMetric: 28.9534

Epoch 240: val_loss did not improve from 28.67137
196/196 - 43s - loss: 28.9592 - MinusLogProbMetric: 28.9592 - val_loss: 28.9534 - val_MinusLogProbMetric: 28.9534 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 241/1000
2023-10-25 04:06:23.387 
Epoch 241/1000 
	 loss: 28.7568, MinusLogProbMetric: 28.7568, val_loss: 28.8678, val_MinusLogProbMetric: 28.8678

Epoch 241: val_loss did not improve from 28.67137
196/196 - 43s - loss: 28.7568 - MinusLogProbMetric: 28.7568 - val_loss: 28.8678 - val_MinusLogProbMetric: 28.8678 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 242/1000
2023-10-25 04:07:02.701 
Epoch 242/1000 
	 loss: 28.9184, MinusLogProbMetric: 28.9184, val_loss: 28.9048, val_MinusLogProbMetric: 28.9048

Epoch 242: val_loss did not improve from 28.67137
196/196 - 39s - loss: 28.9184 - MinusLogProbMetric: 28.9184 - val_loss: 28.9048 - val_MinusLogProbMetric: 28.9048 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 243/1000
2023-10-25 04:07:44.726 
Epoch 243/1000 
	 loss: 28.7675, MinusLogProbMetric: 28.7675, val_loss: 29.0800, val_MinusLogProbMetric: 29.0800

Epoch 243: val_loss did not improve from 28.67137
196/196 - 42s - loss: 28.7675 - MinusLogProbMetric: 28.7675 - val_loss: 29.0800 - val_MinusLogProbMetric: 29.0800 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 244/1000
2023-10-25 04:08:26.577 
Epoch 244/1000 
	 loss: 28.8964, MinusLogProbMetric: 28.8964, val_loss: 29.3696, val_MinusLogProbMetric: 29.3696

Epoch 244: val_loss did not improve from 28.67137
196/196 - 42s - loss: 28.8964 - MinusLogProbMetric: 28.8964 - val_loss: 29.3696 - val_MinusLogProbMetric: 29.3696 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 245/1000
2023-10-25 04:09:05.813 
Epoch 245/1000 
	 loss: 28.9352, MinusLogProbMetric: 28.9352, val_loss: 28.7484, val_MinusLogProbMetric: 28.7484

Epoch 245: val_loss did not improve from 28.67137
196/196 - 39s - loss: 28.9352 - MinusLogProbMetric: 28.9352 - val_loss: 28.7484 - val_MinusLogProbMetric: 28.7484 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 246/1000
2023-10-25 04:09:44.052 
Epoch 246/1000 
	 loss: 28.7746, MinusLogProbMetric: 28.7746, val_loss: 30.3836, val_MinusLogProbMetric: 30.3836

Epoch 246: val_loss did not improve from 28.67137
196/196 - 38s - loss: 28.7746 - MinusLogProbMetric: 28.7746 - val_loss: 30.3836 - val_MinusLogProbMetric: 30.3836 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 247/1000
2023-10-25 04:10:25.423 
Epoch 247/1000 
	 loss: 28.8667, MinusLogProbMetric: 28.8667, val_loss: 28.9283, val_MinusLogProbMetric: 28.9283

Epoch 247: val_loss did not improve from 28.67137
196/196 - 41s - loss: 28.8667 - MinusLogProbMetric: 28.8667 - val_loss: 28.9283 - val_MinusLogProbMetric: 28.9283 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 248/1000
2023-10-25 04:11:07.652 
Epoch 248/1000 
	 loss: 28.7398, MinusLogProbMetric: 28.7398, val_loss: 32.1839, val_MinusLogProbMetric: 32.1839

Epoch 248: val_loss did not improve from 28.67137
196/196 - 42s - loss: 28.7398 - MinusLogProbMetric: 28.7398 - val_loss: 32.1839 - val_MinusLogProbMetric: 32.1839 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 249/1000
2023-10-25 04:11:50.383 
Epoch 249/1000 
	 loss: 29.2845, MinusLogProbMetric: 29.2845, val_loss: 28.8868, val_MinusLogProbMetric: 28.8868

Epoch 249: val_loss did not improve from 28.67137
196/196 - 43s - loss: 29.2845 - MinusLogProbMetric: 29.2845 - val_loss: 28.8868 - val_MinusLogProbMetric: 28.8868 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 250/1000
2023-10-25 04:12:32.833 
Epoch 250/1000 
	 loss: 28.9449, MinusLogProbMetric: 28.9449, val_loss: 29.1238, val_MinusLogProbMetric: 29.1238

Epoch 250: val_loss did not improve from 28.67137
196/196 - 42s - loss: 28.9449 - MinusLogProbMetric: 28.9449 - val_loss: 29.1238 - val_MinusLogProbMetric: 29.1238 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 251/1000
2023-10-25 04:13:15.530 
Epoch 251/1000 
	 loss: 28.6005, MinusLogProbMetric: 28.6005, val_loss: 29.1065, val_MinusLogProbMetric: 29.1065

Epoch 251: val_loss did not improve from 28.67137
196/196 - 43s - loss: 28.6005 - MinusLogProbMetric: 28.6005 - val_loss: 29.1065 - val_MinusLogProbMetric: 29.1065 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 252/1000
2023-10-25 04:13:57.577 
Epoch 252/1000 
	 loss: 28.7320, MinusLogProbMetric: 28.7320, val_loss: 28.7383, val_MinusLogProbMetric: 28.7383

Epoch 252: val_loss did not improve from 28.67137
196/196 - 42s - loss: 28.7320 - MinusLogProbMetric: 28.7320 - val_loss: 28.7383 - val_MinusLogProbMetric: 28.7383 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 253/1000
2023-10-25 04:14:39.589 
Epoch 253/1000 
	 loss: 28.7952, MinusLogProbMetric: 28.7952, val_loss: 29.2347, val_MinusLogProbMetric: 29.2347

Epoch 253: val_loss did not improve from 28.67137
196/196 - 42s - loss: 28.7952 - MinusLogProbMetric: 28.7952 - val_loss: 29.2347 - val_MinusLogProbMetric: 29.2347 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 254/1000
2023-10-25 04:15:22.076 
Epoch 254/1000 
	 loss: 28.8784, MinusLogProbMetric: 28.8784, val_loss: 28.7226, val_MinusLogProbMetric: 28.7226

Epoch 254: val_loss did not improve from 28.67137
196/196 - 42s - loss: 28.8784 - MinusLogProbMetric: 28.8784 - val_loss: 28.7226 - val_MinusLogProbMetric: 28.7226 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 255/1000
2023-10-25 04:16:04.584 
Epoch 255/1000 
	 loss: 28.6689, MinusLogProbMetric: 28.6689, val_loss: 28.9800, val_MinusLogProbMetric: 28.9800

Epoch 255: val_loss did not improve from 28.67137
196/196 - 43s - loss: 28.6689 - MinusLogProbMetric: 28.6689 - val_loss: 28.9800 - val_MinusLogProbMetric: 28.9800 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 256/1000
2023-10-25 04:16:47.065 
Epoch 256/1000 
	 loss: 28.7351, MinusLogProbMetric: 28.7351, val_loss: 28.6666, val_MinusLogProbMetric: 28.6666

Epoch 256: val_loss improved from 28.67137 to 28.66664, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 28.7351 - MinusLogProbMetric: 28.7351 - val_loss: 28.6666 - val_MinusLogProbMetric: 28.6666 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 257/1000
2023-10-25 04:17:30.100 
Epoch 257/1000 
	 loss: 29.0322, MinusLogProbMetric: 29.0322, val_loss: 28.9238, val_MinusLogProbMetric: 28.9238

Epoch 257: val_loss did not improve from 28.66664
196/196 - 42s - loss: 29.0322 - MinusLogProbMetric: 29.0322 - val_loss: 28.9238 - val_MinusLogProbMetric: 28.9238 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 258/1000
2023-10-25 04:18:11.284 
Epoch 258/1000 
	 loss: 28.6784, MinusLogProbMetric: 28.6784, val_loss: 28.7017, val_MinusLogProbMetric: 28.7017

Epoch 258: val_loss did not improve from 28.66664
196/196 - 41s - loss: 28.6784 - MinusLogProbMetric: 28.6784 - val_loss: 28.7017 - val_MinusLogProbMetric: 28.7017 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 259/1000
2023-10-25 04:18:53.700 
Epoch 259/1000 
	 loss: 28.7187, MinusLogProbMetric: 28.7187, val_loss: 29.0121, val_MinusLogProbMetric: 29.0121

Epoch 259: val_loss did not improve from 28.66664
196/196 - 42s - loss: 28.7187 - MinusLogProbMetric: 28.7187 - val_loss: 29.0121 - val_MinusLogProbMetric: 29.0121 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 260/1000
2023-10-25 04:19:36.881 
Epoch 260/1000 
	 loss: 28.9001, MinusLogProbMetric: 28.9001, val_loss: 29.6656, val_MinusLogProbMetric: 29.6656

Epoch 260: val_loss did not improve from 28.66664
196/196 - 43s - loss: 28.9001 - MinusLogProbMetric: 28.9001 - val_loss: 29.6656 - val_MinusLogProbMetric: 29.6656 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 261/1000
2023-10-25 04:20:19.558 
Epoch 261/1000 
	 loss: 28.8313, MinusLogProbMetric: 28.8313, val_loss: 28.9876, val_MinusLogProbMetric: 28.9876

Epoch 261: val_loss did not improve from 28.66664
196/196 - 43s - loss: 28.8313 - MinusLogProbMetric: 28.8313 - val_loss: 28.9876 - val_MinusLogProbMetric: 28.9876 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 262/1000
2023-10-25 04:21:02.167 
Epoch 262/1000 
	 loss: 28.5911, MinusLogProbMetric: 28.5911, val_loss: 28.9855, val_MinusLogProbMetric: 28.9855

Epoch 262: val_loss did not improve from 28.66664
196/196 - 43s - loss: 28.5911 - MinusLogProbMetric: 28.5911 - val_loss: 28.9855 - val_MinusLogProbMetric: 28.9855 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 263/1000
2023-10-25 04:21:40.674 
Epoch 263/1000 
	 loss: 29.0275, MinusLogProbMetric: 29.0275, val_loss: 29.3729, val_MinusLogProbMetric: 29.3729

Epoch 263: val_loss did not improve from 28.66664
196/196 - 39s - loss: 29.0275 - MinusLogProbMetric: 29.0275 - val_loss: 29.3729 - val_MinusLogProbMetric: 29.3729 - lr: 3.3333e-04 - 39s/epoch - 196ms/step
Epoch 264/1000
2023-10-25 04:22:23.188 
Epoch 264/1000 
	 loss: 28.6916, MinusLogProbMetric: 28.6916, val_loss: 28.8208, val_MinusLogProbMetric: 28.8208

Epoch 264: val_loss did not improve from 28.66664
196/196 - 43s - loss: 28.6916 - MinusLogProbMetric: 28.6916 - val_loss: 28.8208 - val_MinusLogProbMetric: 28.8208 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 265/1000
2023-10-25 04:23:05.991 
Epoch 265/1000 
	 loss: 28.7969, MinusLogProbMetric: 28.7969, val_loss: 28.7671, val_MinusLogProbMetric: 28.7671

Epoch 265: val_loss did not improve from 28.66664
196/196 - 43s - loss: 28.7969 - MinusLogProbMetric: 28.7969 - val_loss: 28.7671 - val_MinusLogProbMetric: 28.7671 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 266/1000
2023-10-25 04:23:48.490 
Epoch 266/1000 
	 loss: 28.6223, MinusLogProbMetric: 28.6223, val_loss: 29.2687, val_MinusLogProbMetric: 29.2687

Epoch 266: val_loss did not improve from 28.66664
196/196 - 42s - loss: 28.6223 - MinusLogProbMetric: 28.6223 - val_loss: 29.2687 - val_MinusLogProbMetric: 29.2687 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 267/1000
2023-10-25 04:24:30.726 
Epoch 267/1000 
	 loss: 28.7645, MinusLogProbMetric: 28.7645, val_loss: 28.9688, val_MinusLogProbMetric: 28.9688

Epoch 267: val_loss did not improve from 28.66664
196/196 - 42s - loss: 28.7645 - MinusLogProbMetric: 28.7645 - val_loss: 28.9688 - val_MinusLogProbMetric: 28.9688 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 268/1000
2023-10-25 04:25:09.937 
Epoch 268/1000 
	 loss: 28.7238, MinusLogProbMetric: 28.7238, val_loss: 29.1492, val_MinusLogProbMetric: 29.1492

Epoch 268: val_loss did not improve from 28.66664
196/196 - 39s - loss: 28.7238 - MinusLogProbMetric: 28.7238 - val_loss: 29.1492 - val_MinusLogProbMetric: 29.1492 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 269/1000
2023-10-25 04:25:51.821 
Epoch 269/1000 
	 loss: 28.8748, MinusLogProbMetric: 28.8748, val_loss: 28.9345, val_MinusLogProbMetric: 28.9345

Epoch 269: val_loss did not improve from 28.66664
196/196 - 42s - loss: 28.8748 - MinusLogProbMetric: 28.8748 - val_loss: 28.9345 - val_MinusLogProbMetric: 28.9345 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 270/1000
2023-10-25 04:26:34.079 
Epoch 270/1000 
	 loss: 28.7478, MinusLogProbMetric: 28.7478, val_loss: 28.8055, val_MinusLogProbMetric: 28.8055

Epoch 270: val_loss did not improve from 28.66664
196/196 - 42s - loss: 28.7478 - MinusLogProbMetric: 28.7478 - val_loss: 28.8055 - val_MinusLogProbMetric: 28.8055 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 271/1000
2023-10-25 04:27:16.702 
Epoch 271/1000 
	 loss: 28.8847, MinusLogProbMetric: 28.8847, val_loss: 28.7415, val_MinusLogProbMetric: 28.7415

Epoch 271: val_loss did not improve from 28.66664
196/196 - 43s - loss: 28.8847 - MinusLogProbMetric: 28.8847 - val_loss: 28.7415 - val_MinusLogProbMetric: 28.7415 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 272/1000
2023-10-25 04:27:57.791 
Epoch 272/1000 
	 loss: 28.7560, MinusLogProbMetric: 28.7560, val_loss: 29.1363, val_MinusLogProbMetric: 29.1363

Epoch 272: val_loss did not improve from 28.66664
196/196 - 41s - loss: 28.7560 - MinusLogProbMetric: 28.7560 - val_loss: 29.1363 - val_MinusLogProbMetric: 29.1363 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 273/1000
2023-10-25 04:28:40.565 
Epoch 273/1000 
	 loss: 28.6987, MinusLogProbMetric: 28.6987, val_loss: 30.0728, val_MinusLogProbMetric: 30.0728

Epoch 273: val_loss did not improve from 28.66664
196/196 - 43s - loss: 28.6987 - MinusLogProbMetric: 28.6987 - val_loss: 30.0728 - val_MinusLogProbMetric: 30.0728 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 274/1000
2023-10-25 04:29:21.004 
Epoch 274/1000 
	 loss: 28.6078, MinusLogProbMetric: 28.6078, val_loss: 29.2612, val_MinusLogProbMetric: 29.2612

Epoch 274: val_loss did not improve from 28.66664
196/196 - 40s - loss: 28.6078 - MinusLogProbMetric: 28.6078 - val_loss: 29.2612 - val_MinusLogProbMetric: 29.2612 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 275/1000
2023-10-25 04:30:02.008 
Epoch 275/1000 
	 loss: 28.8901, MinusLogProbMetric: 28.8901, val_loss: 29.2846, val_MinusLogProbMetric: 29.2846

Epoch 275: val_loss did not improve from 28.66664
196/196 - 41s - loss: 28.8901 - MinusLogProbMetric: 28.8901 - val_loss: 29.2846 - val_MinusLogProbMetric: 29.2846 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 276/1000
2023-10-25 04:30:41.885 
Epoch 276/1000 
	 loss: 28.6961, MinusLogProbMetric: 28.6961, val_loss: 29.3071, val_MinusLogProbMetric: 29.3071

Epoch 276: val_loss did not improve from 28.66664
196/196 - 40s - loss: 28.6961 - MinusLogProbMetric: 28.6961 - val_loss: 29.3071 - val_MinusLogProbMetric: 29.3071 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 277/1000
2023-10-25 04:31:19.820 
Epoch 277/1000 
	 loss: 28.8696, MinusLogProbMetric: 28.8696, val_loss: 29.4511, val_MinusLogProbMetric: 29.4511

Epoch 277: val_loss did not improve from 28.66664
196/196 - 38s - loss: 28.8696 - MinusLogProbMetric: 28.8696 - val_loss: 29.4511 - val_MinusLogProbMetric: 29.4511 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 278/1000
2023-10-25 04:31:57.667 
Epoch 278/1000 
	 loss: 28.6596, MinusLogProbMetric: 28.6596, val_loss: 28.7768, val_MinusLogProbMetric: 28.7768

Epoch 278: val_loss did not improve from 28.66664
196/196 - 38s - loss: 28.6596 - MinusLogProbMetric: 28.6596 - val_loss: 28.7768 - val_MinusLogProbMetric: 28.7768 - lr: 3.3333e-04 - 38s/epoch - 193ms/step
Epoch 279/1000
2023-10-25 04:32:35.269 
Epoch 279/1000 
	 loss: 28.6831, MinusLogProbMetric: 28.6831, val_loss: 28.7540, val_MinusLogProbMetric: 28.7540

Epoch 279: val_loss did not improve from 28.66664
196/196 - 38s - loss: 28.6831 - MinusLogProbMetric: 28.6831 - val_loss: 28.7540 - val_MinusLogProbMetric: 28.7540 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 280/1000
2023-10-25 04:33:16.963 
Epoch 280/1000 
	 loss: 28.8649, MinusLogProbMetric: 28.8649, val_loss: 28.6268, val_MinusLogProbMetric: 28.6268

Epoch 280: val_loss improved from 28.66664 to 28.62680, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 28.8649 - MinusLogProbMetric: 28.8649 - val_loss: 28.6268 - val_MinusLogProbMetric: 28.6268 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 281/1000
2023-10-25 04:33:59.755 
Epoch 281/1000 
	 loss: 28.6654, MinusLogProbMetric: 28.6654, val_loss: 28.5442, val_MinusLogProbMetric: 28.5442

Epoch 281: val_loss improved from 28.62680 to 28.54420, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 28.6654 - MinusLogProbMetric: 28.6654 - val_loss: 28.5442 - val_MinusLogProbMetric: 28.5442 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 282/1000
2023-10-25 04:34:42.706 
Epoch 282/1000 
	 loss: 28.6577, MinusLogProbMetric: 28.6577, val_loss: 28.5448, val_MinusLogProbMetric: 28.5448

Epoch 282: val_loss did not improve from 28.54420
196/196 - 42s - loss: 28.6577 - MinusLogProbMetric: 28.6577 - val_loss: 28.5448 - val_MinusLogProbMetric: 28.5448 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 283/1000
2023-10-25 04:35:24.959 
Epoch 283/1000 
	 loss: 28.7408, MinusLogProbMetric: 28.7408, val_loss: 29.2141, val_MinusLogProbMetric: 29.2141

Epoch 283: val_loss did not improve from 28.54420
196/196 - 42s - loss: 28.7408 - MinusLogProbMetric: 28.7408 - val_loss: 29.2141 - val_MinusLogProbMetric: 29.2141 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 284/1000
2023-10-25 04:36:07.429 
Epoch 284/1000 
	 loss: 28.5343, MinusLogProbMetric: 28.5343, val_loss: 29.4436, val_MinusLogProbMetric: 29.4436

Epoch 284: val_loss did not improve from 28.54420
196/196 - 42s - loss: 28.5343 - MinusLogProbMetric: 28.5343 - val_loss: 29.4436 - val_MinusLogProbMetric: 29.4436 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 285/1000
2023-10-25 04:36:48.213 
Epoch 285/1000 
	 loss: 28.6637, MinusLogProbMetric: 28.6637, val_loss: 28.6925, val_MinusLogProbMetric: 28.6925

Epoch 285: val_loss did not improve from 28.54420
196/196 - 41s - loss: 28.6637 - MinusLogProbMetric: 28.6637 - val_loss: 28.6925 - val_MinusLogProbMetric: 28.6925 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 286/1000
2023-10-25 04:37:30.439 
Epoch 286/1000 
	 loss: 28.8424, MinusLogProbMetric: 28.8424, val_loss: 29.6558, val_MinusLogProbMetric: 29.6558

Epoch 286: val_loss did not improve from 28.54420
196/196 - 42s - loss: 28.8424 - MinusLogProbMetric: 28.8424 - val_loss: 29.6558 - val_MinusLogProbMetric: 29.6558 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 287/1000
2023-10-25 04:38:10.866 
Epoch 287/1000 
	 loss: 28.6411, MinusLogProbMetric: 28.6411, val_loss: 28.9418, val_MinusLogProbMetric: 28.9418

Epoch 287: val_loss did not improve from 28.54420
196/196 - 40s - loss: 28.6411 - MinusLogProbMetric: 28.6411 - val_loss: 28.9418 - val_MinusLogProbMetric: 28.9418 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 288/1000
2023-10-25 04:38:51.784 
Epoch 288/1000 
	 loss: 28.7267, MinusLogProbMetric: 28.7267, val_loss: 29.1066, val_MinusLogProbMetric: 29.1066

Epoch 288: val_loss did not improve from 28.54420
196/196 - 41s - loss: 28.7267 - MinusLogProbMetric: 28.7267 - val_loss: 29.1066 - val_MinusLogProbMetric: 29.1066 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 289/1000
2023-10-25 04:39:32.263 
Epoch 289/1000 
	 loss: 28.5572, MinusLogProbMetric: 28.5572, val_loss: 29.0975, val_MinusLogProbMetric: 29.0975

Epoch 289: val_loss did not improve from 28.54420
196/196 - 40s - loss: 28.5572 - MinusLogProbMetric: 28.5572 - val_loss: 29.0975 - val_MinusLogProbMetric: 29.0975 - lr: 3.3333e-04 - 40s/epoch - 207ms/step
Epoch 290/1000
2023-10-25 04:40:12.739 
Epoch 290/1000 
	 loss: 28.7462, MinusLogProbMetric: 28.7462, val_loss: 28.6561, val_MinusLogProbMetric: 28.6561

Epoch 290: val_loss did not improve from 28.54420
196/196 - 40s - loss: 28.7462 - MinusLogProbMetric: 28.7462 - val_loss: 28.6561 - val_MinusLogProbMetric: 28.6561 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 291/1000
2023-10-25 04:40:54.460 
Epoch 291/1000 
	 loss: 28.7024, MinusLogProbMetric: 28.7024, val_loss: 28.9925, val_MinusLogProbMetric: 28.9925

Epoch 291: val_loss did not improve from 28.54420
196/196 - 42s - loss: 28.7024 - MinusLogProbMetric: 28.7024 - val_loss: 28.9925 - val_MinusLogProbMetric: 28.9925 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 292/1000
2023-10-25 04:41:35.783 
Epoch 292/1000 
	 loss: 28.7297, MinusLogProbMetric: 28.7297, val_loss: 31.5253, val_MinusLogProbMetric: 31.5253

Epoch 292: val_loss did not improve from 28.54420
196/196 - 41s - loss: 28.7297 - MinusLogProbMetric: 28.7297 - val_loss: 31.5253 - val_MinusLogProbMetric: 31.5253 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 293/1000
2023-10-25 04:42:17.715 
Epoch 293/1000 
	 loss: 28.7062, MinusLogProbMetric: 28.7062, val_loss: 28.6530, val_MinusLogProbMetric: 28.6530

Epoch 293: val_loss did not improve from 28.54420
196/196 - 42s - loss: 28.7062 - MinusLogProbMetric: 28.7062 - val_loss: 28.6530 - val_MinusLogProbMetric: 28.6530 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 294/1000
2023-10-25 04:42:59.118 
Epoch 294/1000 
	 loss: 28.5805, MinusLogProbMetric: 28.5805, val_loss: 29.0470, val_MinusLogProbMetric: 29.0470

Epoch 294: val_loss did not improve from 28.54420
196/196 - 41s - loss: 28.5805 - MinusLogProbMetric: 28.5805 - val_loss: 29.0470 - val_MinusLogProbMetric: 29.0470 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 295/1000
2023-10-25 04:43:41.807 
Epoch 295/1000 
	 loss: 28.6504, MinusLogProbMetric: 28.6504, val_loss: 28.6799, val_MinusLogProbMetric: 28.6799

Epoch 295: val_loss did not improve from 28.54420
196/196 - 43s - loss: 28.6504 - MinusLogProbMetric: 28.6504 - val_loss: 28.6799 - val_MinusLogProbMetric: 28.6799 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 296/1000
2023-10-25 04:44:24.466 
Epoch 296/1000 
	 loss: 28.8678, MinusLogProbMetric: 28.8678, val_loss: 29.3106, val_MinusLogProbMetric: 29.3106

Epoch 296: val_loss did not improve from 28.54420
196/196 - 43s - loss: 28.8678 - MinusLogProbMetric: 28.8678 - val_loss: 29.3106 - val_MinusLogProbMetric: 29.3106 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 297/1000
2023-10-25 04:45:06.333 
Epoch 297/1000 
	 loss: 28.7662, MinusLogProbMetric: 28.7662, val_loss: 28.8084, val_MinusLogProbMetric: 28.8084

Epoch 297: val_loss did not improve from 28.54420
196/196 - 42s - loss: 28.7662 - MinusLogProbMetric: 28.7662 - val_loss: 28.8084 - val_MinusLogProbMetric: 28.8084 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 298/1000
2023-10-25 04:45:46.216 
Epoch 298/1000 
	 loss: 28.5988, MinusLogProbMetric: 28.5988, val_loss: 29.9126, val_MinusLogProbMetric: 29.9126

Epoch 298: val_loss did not improve from 28.54420
196/196 - 40s - loss: 28.5988 - MinusLogProbMetric: 28.5988 - val_loss: 29.9126 - val_MinusLogProbMetric: 29.9126 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 299/1000
2023-10-25 04:46:27.770 
Epoch 299/1000 
	 loss: 28.8184, MinusLogProbMetric: 28.8184, val_loss: 29.6867, val_MinusLogProbMetric: 29.6867

Epoch 299: val_loss did not improve from 28.54420
196/196 - 42s - loss: 28.8184 - MinusLogProbMetric: 28.8184 - val_loss: 29.6867 - val_MinusLogProbMetric: 29.6867 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 300/1000
2023-10-25 04:47:10.422 
Epoch 300/1000 
	 loss: 28.6451, MinusLogProbMetric: 28.6451, val_loss: 28.8081, val_MinusLogProbMetric: 28.8081

Epoch 300: val_loss did not improve from 28.54420
196/196 - 43s - loss: 28.6451 - MinusLogProbMetric: 28.6451 - val_loss: 28.8081 - val_MinusLogProbMetric: 28.8081 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 301/1000
2023-10-25 04:47:52.287 
Epoch 301/1000 
	 loss: 28.6629, MinusLogProbMetric: 28.6629, val_loss: 28.9175, val_MinusLogProbMetric: 28.9175

Epoch 301: val_loss did not improve from 28.54420
196/196 - 42s - loss: 28.6629 - MinusLogProbMetric: 28.6629 - val_loss: 28.9175 - val_MinusLogProbMetric: 28.9175 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 302/1000
2023-10-25 04:48:35.246 
Epoch 302/1000 
	 loss: 28.7388, MinusLogProbMetric: 28.7388, val_loss: 28.8628, val_MinusLogProbMetric: 28.8628

Epoch 302: val_loss did not improve from 28.54420
196/196 - 43s - loss: 28.7388 - MinusLogProbMetric: 28.7388 - val_loss: 28.8628 - val_MinusLogProbMetric: 28.8628 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 303/1000
2023-10-25 04:49:18.146 
Epoch 303/1000 
	 loss: 28.5103, MinusLogProbMetric: 28.5103, val_loss: 28.7555, val_MinusLogProbMetric: 28.7555

Epoch 303: val_loss did not improve from 28.54420
196/196 - 43s - loss: 28.5103 - MinusLogProbMetric: 28.5103 - val_loss: 28.7555 - val_MinusLogProbMetric: 28.7555 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 304/1000
2023-10-25 04:50:00.885 
Epoch 304/1000 
	 loss: 28.5963, MinusLogProbMetric: 28.5963, val_loss: 28.8234, val_MinusLogProbMetric: 28.8234

Epoch 304: val_loss did not improve from 28.54420
196/196 - 43s - loss: 28.5963 - MinusLogProbMetric: 28.5963 - val_loss: 28.8234 - val_MinusLogProbMetric: 28.8234 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 305/1000
2023-10-25 04:50:43.789 
Epoch 305/1000 
	 loss: 28.5797, MinusLogProbMetric: 28.5797, val_loss: 29.1880, val_MinusLogProbMetric: 29.1880

Epoch 305: val_loss did not improve from 28.54420
196/196 - 43s - loss: 28.5797 - MinusLogProbMetric: 28.5797 - val_loss: 29.1880 - val_MinusLogProbMetric: 29.1880 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 306/1000
2023-10-25 04:51:24.685 
Epoch 306/1000 
	 loss: 28.6554, MinusLogProbMetric: 28.6554, val_loss: 28.5704, val_MinusLogProbMetric: 28.5704

Epoch 306: val_loss did not improve from 28.54420
196/196 - 41s - loss: 28.6554 - MinusLogProbMetric: 28.6554 - val_loss: 28.5704 - val_MinusLogProbMetric: 28.5704 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 307/1000
2023-10-25 04:52:07.286 
Epoch 307/1000 
	 loss: 28.6806, MinusLogProbMetric: 28.6806, val_loss: 29.1528, val_MinusLogProbMetric: 29.1528

Epoch 307: val_loss did not improve from 28.54420
196/196 - 43s - loss: 28.6806 - MinusLogProbMetric: 28.6806 - val_loss: 29.1528 - val_MinusLogProbMetric: 29.1528 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 308/1000
2023-10-25 04:52:47.001 
Epoch 308/1000 
	 loss: 28.6759, MinusLogProbMetric: 28.6759, val_loss: 29.2060, val_MinusLogProbMetric: 29.2060

Epoch 308: val_loss did not improve from 28.54420
196/196 - 40s - loss: 28.6759 - MinusLogProbMetric: 28.6759 - val_loss: 29.2060 - val_MinusLogProbMetric: 29.2060 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 309/1000
2023-10-25 04:53:28.755 
Epoch 309/1000 
	 loss: 28.6370, MinusLogProbMetric: 28.6370, val_loss: 28.7738, val_MinusLogProbMetric: 28.7738

Epoch 309: val_loss did not improve from 28.54420
196/196 - 42s - loss: 28.6370 - MinusLogProbMetric: 28.6370 - val_loss: 28.7738 - val_MinusLogProbMetric: 28.7738 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 310/1000
2023-10-25 04:54:11.147 
Epoch 310/1000 
	 loss: 28.5614, MinusLogProbMetric: 28.5614, val_loss: 30.9245, val_MinusLogProbMetric: 30.9245

Epoch 310: val_loss did not improve from 28.54420
196/196 - 42s - loss: 28.5614 - MinusLogProbMetric: 28.5614 - val_loss: 30.9245 - val_MinusLogProbMetric: 30.9245 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 311/1000
2023-10-25 04:54:53.918 
Epoch 311/1000 
	 loss: 28.6959, MinusLogProbMetric: 28.6959, val_loss: 28.7035, val_MinusLogProbMetric: 28.7035

Epoch 311: val_loss did not improve from 28.54420
196/196 - 43s - loss: 28.6959 - MinusLogProbMetric: 28.6959 - val_loss: 28.7035 - val_MinusLogProbMetric: 28.7035 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 312/1000
2023-10-25 04:55:36.809 
Epoch 312/1000 
	 loss: 28.4932, MinusLogProbMetric: 28.4932, val_loss: 29.9308, val_MinusLogProbMetric: 29.9308

Epoch 312: val_loss did not improve from 28.54420
196/196 - 43s - loss: 28.4932 - MinusLogProbMetric: 28.4932 - val_loss: 29.9308 - val_MinusLogProbMetric: 29.9308 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 313/1000
2023-10-25 04:56:17.928 
Epoch 313/1000 
	 loss: 28.6024, MinusLogProbMetric: 28.6024, val_loss: 28.7375, val_MinusLogProbMetric: 28.7375

Epoch 313: val_loss did not improve from 28.54420
196/196 - 41s - loss: 28.6024 - MinusLogProbMetric: 28.6024 - val_loss: 28.7375 - val_MinusLogProbMetric: 28.7375 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 314/1000
2023-10-25 04:56:59.792 
Epoch 314/1000 
	 loss: 28.4854, MinusLogProbMetric: 28.4854, val_loss: 28.6987, val_MinusLogProbMetric: 28.6987

Epoch 314: val_loss did not improve from 28.54420
196/196 - 42s - loss: 28.4854 - MinusLogProbMetric: 28.4854 - val_loss: 28.6987 - val_MinusLogProbMetric: 28.6987 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 315/1000
2023-10-25 04:57:41.683 
Epoch 315/1000 
	 loss: 28.7910, MinusLogProbMetric: 28.7910, val_loss: 28.4797, val_MinusLogProbMetric: 28.4797

Epoch 315: val_loss improved from 28.54420 to 28.47973, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 28.7910 - MinusLogProbMetric: 28.7910 - val_loss: 28.4797 - val_MinusLogProbMetric: 28.4797 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 316/1000
2023-10-25 04:58:24.842 
Epoch 316/1000 
	 loss: 28.5925, MinusLogProbMetric: 28.5925, val_loss: 28.6029, val_MinusLogProbMetric: 28.6029

Epoch 316: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.5925 - MinusLogProbMetric: 28.5925 - val_loss: 28.6029 - val_MinusLogProbMetric: 28.6029 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 317/1000
2023-10-25 04:59:06.986 
Epoch 317/1000 
	 loss: 28.5623, MinusLogProbMetric: 28.5623, val_loss: 28.7052, val_MinusLogProbMetric: 28.7052

Epoch 317: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.5623 - MinusLogProbMetric: 28.5623 - val_loss: 28.7052 - val_MinusLogProbMetric: 28.7052 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 318/1000
2023-10-25 04:59:48.273 
Epoch 318/1000 
	 loss: 28.7631, MinusLogProbMetric: 28.7631, val_loss: 28.8982, val_MinusLogProbMetric: 28.8982

Epoch 318: val_loss did not improve from 28.47973
196/196 - 41s - loss: 28.7631 - MinusLogProbMetric: 28.7631 - val_loss: 28.8982 - val_MinusLogProbMetric: 28.8982 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 319/1000
2023-10-25 05:00:27.945 
Epoch 319/1000 
	 loss: 28.5949, MinusLogProbMetric: 28.5949, val_loss: 28.7664, val_MinusLogProbMetric: 28.7664

Epoch 319: val_loss did not improve from 28.47973
196/196 - 40s - loss: 28.5949 - MinusLogProbMetric: 28.5949 - val_loss: 28.7664 - val_MinusLogProbMetric: 28.7664 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 320/1000
2023-10-25 05:01:08.983 
Epoch 320/1000 
	 loss: 28.7258, MinusLogProbMetric: 28.7258, val_loss: 28.9159, val_MinusLogProbMetric: 28.9159

Epoch 320: val_loss did not improve from 28.47973
196/196 - 41s - loss: 28.7258 - MinusLogProbMetric: 28.7258 - val_loss: 28.9159 - val_MinusLogProbMetric: 28.9159 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 321/1000
2023-10-25 05:01:51.792 
Epoch 321/1000 
	 loss: 28.4610, MinusLogProbMetric: 28.4610, val_loss: 28.6305, val_MinusLogProbMetric: 28.6305

Epoch 321: val_loss did not improve from 28.47973
196/196 - 43s - loss: 28.4610 - MinusLogProbMetric: 28.4610 - val_loss: 28.6305 - val_MinusLogProbMetric: 28.6305 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 322/1000
2023-10-25 05:02:33.505 
Epoch 322/1000 
	 loss: 28.7205, MinusLogProbMetric: 28.7205, val_loss: 29.8273, val_MinusLogProbMetric: 29.8273

Epoch 322: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.7205 - MinusLogProbMetric: 28.7205 - val_loss: 29.8273 - val_MinusLogProbMetric: 29.8273 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 323/1000
2023-10-25 05:03:16.594 
Epoch 323/1000 
	 loss: 28.5511, MinusLogProbMetric: 28.5511, val_loss: 28.4914, val_MinusLogProbMetric: 28.4914

Epoch 323: val_loss did not improve from 28.47973
196/196 - 43s - loss: 28.5511 - MinusLogProbMetric: 28.5511 - val_loss: 28.4914 - val_MinusLogProbMetric: 28.4914 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 324/1000
2023-10-25 05:03:57.148 
Epoch 324/1000 
	 loss: 28.4832, MinusLogProbMetric: 28.4832, val_loss: 29.2968, val_MinusLogProbMetric: 29.2968

Epoch 324: val_loss did not improve from 28.47973
196/196 - 41s - loss: 28.4832 - MinusLogProbMetric: 28.4832 - val_loss: 29.2968 - val_MinusLogProbMetric: 29.2968 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 325/1000
2023-10-25 05:04:38.752 
Epoch 325/1000 
	 loss: 28.5146, MinusLogProbMetric: 28.5146, val_loss: 29.3034, val_MinusLogProbMetric: 29.3034

Epoch 325: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.5146 - MinusLogProbMetric: 28.5146 - val_loss: 29.3034 - val_MinusLogProbMetric: 29.3034 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 326/1000
2023-10-25 05:05:20.121 
Epoch 326/1000 
	 loss: 28.5099, MinusLogProbMetric: 28.5099, val_loss: 28.9107, val_MinusLogProbMetric: 28.9107

Epoch 326: val_loss did not improve from 28.47973
196/196 - 41s - loss: 28.5099 - MinusLogProbMetric: 28.5099 - val_loss: 28.9107 - val_MinusLogProbMetric: 28.9107 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 327/1000
2023-10-25 05:06:01.725 
Epoch 327/1000 
	 loss: 28.6231, MinusLogProbMetric: 28.6231, val_loss: 29.0375, val_MinusLogProbMetric: 29.0375

Epoch 327: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.6231 - MinusLogProbMetric: 28.6231 - val_loss: 29.0375 - val_MinusLogProbMetric: 29.0375 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 328/1000
2023-10-25 05:06:43.472 
Epoch 328/1000 
	 loss: 28.5841, MinusLogProbMetric: 28.5841, val_loss: 29.0426, val_MinusLogProbMetric: 29.0426

Epoch 328: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.5841 - MinusLogProbMetric: 28.5841 - val_loss: 29.0426 - val_MinusLogProbMetric: 29.0426 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 329/1000
2023-10-25 05:07:25.676 
Epoch 329/1000 
	 loss: 28.4926, MinusLogProbMetric: 28.4926, val_loss: 28.5658, val_MinusLogProbMetric: 28.5658

Epoch 329: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.4926 - MinusLogProbMetric: 28.4926 - val_loss: 28.5658 - val_MinusLogProbMetric: 28.5658 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 330/1000
2023-10-25 05:08:08.126 
Epoch 330/1000 
	 loss: 28.7396, MinusLogProbMetric: 28.7396, val_loss: 28.9786, val_MinusLogProbMetric: 28.9786

Epoch 330: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.7396 - MinusLogProbMetric: 28.7396 - val_loss: 28.9786 - val_MinusLogProbMetric: 28.9786 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 331/1000
2023-10-25 05:08:50.918 
Epoch 331/1000 
	 loss: 28.8188, MinusLogProbMetric: 28.8188, val_loss: 28.7743, val_MinusLogProbMetric: 28.7743

Epoch 331: val_loss did not improve from 28.47973
196/196 - 43s - loss: 28.8188 - MinusLogProbMetric: 28.8188 - val_loss: 28.7743 - val_MinusLogProbMetric: 28.7743 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 332/1000
2023-10-25 05:09:33.410 
Epoch 332/1000 
	 loss: 28.6199, MinusLogProbMetric: 28.6199, val_loss: 28.7671, val_MinusLogProbMetric: 28.7671

Epoch 332: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.6199 - MinusLogProbMetric: 28.6199 - val_loss: 28.7671 - val_MinusLogProbMetric: 28.7671 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 333/1000
2023-10-25 05:10:14.490 
Epoch 333/1000 
	 loss: 28.3634, MinusLogProbMetric: 28.3634, val_loss: 29.2610, val_MinusLogProbMetric: 29.2610

Epoch 333: val_loss did not improve from 28.47973
196/196 - 41s - loss: 28.3634 - MinusLogProbMetric: 28.3634 - val_loss: 29.2610 - val_MinusLogProbMetric: 29.2610 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 334/1000
2023-10-25 05:10:57.457 
Epoch 334/1000 
	 loss: 28.8581, MinusLogProbMetric: 28.8581, val_loss: 29.4372, val_MinusLogProbMetric: 29.4372

Epoch 334: val_loss did not improve from 28.47973
196/196 - 43s - loss: 28.8581 - MinusLogProbMetric: 28.8581 - val_loss: 29.4372 - val_MinusLogProbMetric: 29.4372 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 335/1000
2023-10-25 05:11:40.316 
Epoch 335/1000 
	 loss: 28.5425, MinusLogProbMetric: 28.5425, val_loss: 30.1661, val_MinusLogProbMetric: 30.1661

Epoch 335: val_loss did not improve from 28.47973
196/196 - 43s - loss: 28.5425 - MinusLogProbMetric: 28.5425 - val_loss: 30.1661 - val_MinusLogProbMetric: 30.1661 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 336/1000
2023-10-25 05:12:20.324 
Epoch 336/1000 
	 loss: 28.8642, MinusLogProbMetric: 28.8642, val_loss: 29.9402, val_MinusLogProbMetric: 29.9402

Epoch 336: val_loss did not improve from 28.47973
196/196 - 40s - loss: 28.8642 - MinusLogProbMetric: 28.8642 - val_loss: 29.9402 - val_MinusLogProbMetric: 29.9402 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 337/1000
2023-10-25 05:13:02.186 
Epoch 337/1000 
	 loss: 28.5184, MinusLogProbMetric: 28.5184, val_loss: 28.8944, val_MinusLogProbMetric: 28.8944

Epoch 337: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.5184 - MinusLogProbMetric: 28.5184 - val_loss: 28.8944 - val_MinusLogProbMetric: 28.8944 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 338/1000
2023-10-25 05:13:43.762 
Epoch 338/1000 
	 loss: 28.5796, MinusLogProbMetric: 28.5796, val_loss: 28.6365, val_MinusLogProbMetric: 28.6365

Epoch 338: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.5796 - MinusLogProbMetric: 28.5796 - val_loss: 28.6365 - val_MinusLogProbMetric: 28.6365 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 339/1000
2023-10-25 05:14:26.402 
Epoch 339/1000 
	 loss: 28.6502, MinusLogProbMetric: 28.6502, val_loss: 28.8050, val_MinusLogProbMetric: 28.8050

Epoch 339: val_loss did not improve from 28.47973
196/196 - 43s - loss: 28.6502 - MinusLogProbMetric: 28.6502 - val_loss: 28.8050 - val_MinusLogProbMetric: 28.8050 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 340/1000
2023-10-25 05:15:08.439 
Epoch 340/1000 
	 loss: 28.4947, MinusLogProbMetric: 28.4947, val_loss: 28.7150, val_MinusLogProbMetric: 28.7150

Epoch 340: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.4947 - MinusLogProbMetric: 28.4947 - val_loss: 28.7150 - val_MinusLogProbMetric: 28.7150 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 341/1000
2023-10-25 05:15:50.121 
Epoch 341/1000 
	 loss: 28.4501, MinusLogProbMetric: 28.4501, val_loss: 28.7200, val_MinusLogProbMetric: 28.7200

Epoch 341: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.4501 - MinusLogProbMetric: 28.4501 - val_loss: 28.7200 - val_MinusLogProbMetric: 28.7200 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 342/1000
2023-10-25 05:16:30.675 
Epoch 342/1000 
	 loss: 28.4686, MinusLogProbMetric: 28.4686, val_loss: 28.6104, val_MinusLogProbMetric: 28.6104

Epoch 342: val_loss did not improve from 28.47973
196/196 - 41s - loss: 28.4686 - MinusLogProbMetric: 28.4686 - val_loss: 28.6104 - val_MinusLogProbMetric: 28.6104 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 343/1000
2023-10-25 05:17:13.640 
Epoch 343/1000 
	 loss: 28.6097, MinusLogProbMetric: 28.6097, val_loss: 32.7420, val_MinusLogProbMetric: 32.7420

Epoch 343: val_loss did not improve from 28.47973
196/196 - 43s - loss: 28.6097 - MinusLogProbMetric: 28.6097 - val_loss: 32.7420 - val_MinusLogProbMetric: 32.7420 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 344/1000
2023-10-25 05:17:56.030 
Epoch 344/1000 
	 loss: 28.5160, MinusLogProbMetric: 28.5160, val_loss: 29.6096, val_MinusLogProbMetric: 29.6096

Epoch 344: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.5160 - MinusLogProbMetric: 28.5160 - val_loss: 29.6096 - val_MinusLogProbMetric: 29.6096 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 345/1000
2023-10-25 05:18:37.904 
Epoch 345/1000 
	 loss: 28.8087, MinusLogProbMetric: 28.8087, val_loss: 29.6497, val_MinusLogProbMetric: 29.6497

Epoch 345: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.8087 - MinusLogProbMetric: 28.8087 - val_loss: 29.6497 - val_MinusLogProbMetric: 29.6497 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 346/1000
2023-10-25 05:19:19.201 
Epoch 346/1000 
	 loss: 28.6509, MinusLogProbMetric: 28.6509, val_loss: 28.5793, val_MinusLogProbMetric: 28.5793

Epoch 346: val_loss did not improve from 28.47973
196/196 - 41s - loss: 28.6509 - MinusLogProbMetric: 28.6509 - val_loss: 28.5793 - val_MinusLogProbMetric: 28.5793 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 347/1000
2023-10-25 05:20:01.904 
Epoch 347/1000 
	 loss: 28.3022, MinusLogProbMetric: 28.3022, val_loss: 28.9316, val_MinusLogProbMetric: 28.9316

Epoch 347: val_loss did not improve from 28.47973
196/196 - 43s - loss: 28.3022 - MinusLogProbMetric: 28.3022 - val_loss: 28.9316 - val_MinusLogProbMetric: 28.9316 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 348/1000
2023-10-25 05:20:44.659 
Epoch 348/1000 
	 loss: 28.4771, MinusLogProbMetric: 28.4771, val_loss: 29.6333, val_MinusLogProbMetric: 29.6333

Epoch 348: val_loss did not improve from 28.47973
196/196 - 43s - loss: 28.4771 - MinusLogProbMetric: 28.4771 - val_loss: 29.6333 - val_MinusLogProbMetric: 29.6333 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 349/1000
2023-10-25 05:21:25.957 
Epoch 349/1000 
	 loss: 28.4974, MinusLogProbMetric: 28.4974, val_loss: 28.8925, val_MinusLogProbMetric: 28.8925

Epoch 349: val_loss did not improve from 28.47973
196/196 - 41s - loss: 28.4974 - MinusLogProbMetric: 28.4974 - val_loss: 28.8925 - val_MinusLogProbMetric: 28.8925 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 350/1000
2023-10-25 05:22:05.401 
Epoch 350/1000 
	 loss: 28.5956, MinusLogProbMetric: 28.5956, val_loss: 28.8149, val_MinusLogProbMetric: 28.8149

Epoch 350: val_loss did not improve from 28.47973
196/196 - 39s - loss: 28.5956 - MinusLogProbMetric: 28.5956 - val_loss: 28.8149 - val_MinusLogProbMetric: 28.8149 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 351/1000
2023-10-25 05:22:47.952 
Epoch 351/1000 
	 loss: 28.5102, MinusLogProbMetric: 28.5102, val_loss: 28.8367, val_MinusLogProbMetric: 28.8367

Epoch 351: val_loss did not improve from 28.47973
196/196 - 43s - loss: 28.5102 - MinusLogProbMetric: 28.5102 - val_loss: 28.8367 - val_MinusLogProbMetric: 28.8367 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 352/1000
2023-10-25 05:23:30.677 
Epoch 352/1000 
	 loss: 28.7545, MinusLogProbMetric: 28.7545, val_loss: 28.6798, val_MinusLogProbMetric: 28.6798

Epoch 352: val_loss did not improve from 28.47973
196/196 - 43s - loss: 28.7545 - MinusLogProbMetric: 28.7545 - val_loss: 28.6798 - val_MinusLogProbMetric: 28.6798 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 353/1000
2023-10-25 05:24:12.458 
Epoch 353/1000 
	 loss: 28.4099, MinusLogProbMetric: 28.4099, val_loss: 29.0829, val_MinusLogProbMetric: 29.0829

Epoch 353: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.4099 - MinusLogProbMetric: 28.4099 - val_loss: 29.0829 - val_MinusLogProbMetric: 29.0829 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 354/1000
2023-10-25 05:24:52.528 
Epoch 354/1000 
	 loss: 28.5847, MinusLogProbMetric: 28.5847, val_loss: 29.1299, val_MinusLogProbMetric: 29.1299

Epoch 354: val_loss did not improve from 28.47973
196/196 - 40s - loss: 28.5847 - MinusLogProbMetric: 28.5847 - val_loss: 29.1299 - val_MinusLogProbMetric: 29.1299 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 355/1000
2023-10-25 05:25:35.207 
Epoch 355/1000 
	 loss: 28.3544, MinusLogProbMetric: 28.3544, val_loss: 28.5095, val_MinusLogProbMetric: 28.5095

Epoch 355: val_loss did not improve from 28.47973
196/196 - 43s - loss: 28.3544 - MinusLogProbMetric: 28.3544 - val_loss: 28.5095 - val_MinusLogProbMetric: 28.5095 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 356/1000
2023-10-25 05:26:17.410 
Epoch 356/1000 
	 loss: 28.4693, MinusLogProbMetric: 28.4693, val_loss: 29.4451, val_MinusLogProbMetric: 29.4451

Epoch 356: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.4693 - MinusLogProbMetric: 28.4693 - val_loss: 29.4451 - val_MinusLogProbMetric: 29.4451 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 357/1000
2023-10-25 05:26:59.821 
Epoch 357/1000 
	 loss: 28.4848, MinusLogProbMetric: 28.4848, val_loss: 28.7357, val_MinusLogProbMetric: 28.7357

Epoch 357: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.4848 - MinusLogProbMetric: 28.4848 - val_loss: 28.7357 - val_MinusLogProbMetric: 28.7357 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 358/1000
2023-10-25 05:27:39.816 
Epoch 358/1000 
	 loss: 28.5207, MinusLogProbMetric: 28.5207, val_loss: 33.2472, val_MinusLogProbMetric: 33.2472

Epoch 358: val_loss did not improve from 28.47973
196/196 - 40s - loss: 28.5207 - MinusLogProbMetric: 28.5207 - val_loss: 33.2472 - val_MinusLogProbMetric: 33.2472 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 359/1000
2023-10-25 05:28:21.882 
Epoch 359/1000 
	 loss: 28.5987, MinusLogProbMetric: 28.5987, val_loss: 28.5029, val_MinusLogProbMetric: 28.5029

Epoch 359: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.5987 - MinusLogProbMetric: 28.5987 - val_loss: 28.5029 - val_MinusLogProbMetric: 28.5029 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 360/1000
2023-10-25 05:29:04.384 
Epoch 360/1000 
	 loss: 28.7073, MinusLogProbMetric: 28.7073, val_loss: 29.5998, val_MinusLogProbMetric: 29.5998

Epoch 360: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.7073 - MinusLogProbMetric: 28.7073 - val_loss: 29.5998 - val_MinusLogProbMetric: 29.5998 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 361/1000
2023-10-25 05:29:46.660 
Epoch 361/1000 
	 loss: 28.4995, MinusLogProbMetric: 28.4995, val_loss: 28.6058, val_MinusLogProbMetric: 28.6058

Epoch 361: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.4995 - MinusLogProbMetric: 28.4995 - val_loss: 28.6058 - val_MinusLogProbMetric: 28.6058 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 362/1000
2023-10-25 05:30:29.699 
Epoch 362/1000 
	 loss: 28.5103, MinusLogProbMetric: 28.5103, val_loss: 28.5578, val_MinusLogProbMetric: 28.5578

Epoch 362: val_loss did not improve from 28.47973
196/196 - 43s - loss: 28.5103 - MinusLogProbMetric: 28.5103 - val_loss: 28.5578 - val_MinusLogProbMetric: 28.5578 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 363/1000
2023-10-25 05:31:12.134 
Epoch 363/1000 
	 loss: 28.4977, MinusLogProbMetric: 28.4977, val_loss: 29.1700, val_MinusLogProbMetric: 29.1700

Epoch 363: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.4977 - MinusLogProbMetric: 28.4977 - val_loss: 29.1700 - val_MinusLogProbMetric: 29.1700 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 364/1000
2023-10-25 05:31:54.219 
Epoch 364/1000 
	 loss: 28.3538, MinusLogProbMetric: 28.3538, val_loss: 28.8404, val_MinusLogProbMetric: 28.8404

Epoch 364: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.3538 - MinusLogProbMetric: 28.3538 - val_loss: 28.8404 - val_MinusLogProbMetric: 28.8404 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 365/1000
2023-10-25 05:32:36.063 
Epoch 365/1000 
	 loss: 28.5885, MinusLogProbMetric: 28.5885, val_loss: 28.7091, val_MinusLogProbMetric: 28.7091

Epoch 365: val_loss did not improve from 28.47973
196/196 - 42s - loss: 28.5885 - MinusLogProbMetric: 28.5885 - val_loss: 28.7091 - val_MinusLogProbMetric: 28.7091 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 366/1000
2023-10-25 05:33:17.987 
Epoch 366/1000 
	 loss: 27.8576, MinusLogProbMetric: 27.8576, val_loss: 28.1004, val_MinusLogProbMetric: 28.1004

Epoch 366: val_loss improved from 28.47973 to 28.10043, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 27.8576 - MinusLogProbMetric: 27.8576 - val_loss: 28.1004 - val_MinusLogProbMetric: 28.1004 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 367/1000
2023-10-25 05:34:00.944 
Epoch 367/1000 
	 loss: 27.8212, MinusLogProbMetric: 27.8212, val_loss: 28.1632, val_MinusLogProbMetric: 28.1632

Epoch 367: val_loss did not improve from 28.10043
196/196 - 42s - loss: 27.8212 - MinusLogProbMetric: 27.8212 - val_loss: 28.1632 - val_MinusLogProbMetric: 28.1632 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 368/1000
2023-10-25 05:34:43.441 
Epoch 368/1000 
	 loss: 27.8873, MinusLogProbMetric: 27.8873, val_loss: 28.2041, val_MinusLogProbMetric: 28.2041

Epoch 368: val_loss did not improve from 28.10043
196/196 - 42s - loss: 27.8873 - MinusLogProbMetric: 27.8873 - val_loss: 28.2041 - val_MinusLogProbMetric: 28.2041 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 369/1000
2023-10-25 05:35:25.699 
Epoch 369/1000 
	 loss: 27.8640, MinusLogProbMetric: 27.8640, val_loss: 28.1447, val_MinusLogProbMetric: 28.1447

Epoch 369: val_loss did not improve from 28.10043
196/196 - 42s - loss: 27.8640 - MinusLogProbMetric: 27.8640 - val_loss: 28.1447 - val_MinusLogProbMetric: 28.1447 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 370/1000
2023-10-25 05:36:07.799 
Epoch 370/1000 
	 loss: 27.8776, MinusLogProbMetric: 27.8776, val_loss: 28.3629, val_MinusLogProbMetric: 28.3629

Epoch 370: val_loss did not improve from 28.10043
196/196 - 42s - loss: 27.8776 - MinusLogProbMetric: 27.8776 - val_loss: 28.3629 - val_MinusLogProbMetric: 28.3629 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 371/1000
2023-10-25 05:36:51.263 
Epoch 371/1000 
	 loss: 27.8935, MinusLogProbMetric: 27.8935, val_loss: 28.3401, val_MinusLogProbMetric: 28.3401

Epoch 371: val_loss did not improve from 28.10043
196/196 - 43s - loss: 27.8935 - MinusLogProbMetric: 27.8935 - val_loss: 28.3401 - val_MinusLogProbMetric: 28.3401 - lr: 1.6667e-04 - 43s/epoch - 222ms/step
Epoch 372/1000
2023-10-25 05:37:33.253 
Epoch 372/1000 
	 loss: 27.9042, MinusLogProbMetric: 27.9042, val_loss: 28.6082, val_MinusLogProbMetric: 28.6082

Epoch 372: val_loss did not improve from 28.10043
196/196 - 42s - loss: 27.9042 - MinusLogProbMetric: 27.9042 - val_loss: 28.6082 - val_MinusLogProbMetric: 28.6082 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 373/1000
2023-10-25 05:38:16.062 
Epoch 373/1000 
	 loss: 27.8928, MinusLogProbMetric: 27.8928, val_loss: 28.4235, val_MinusLogProbMetric: 28.4235

Epoch 373: val_loss did not improve from 28.10043
196/196 - 43s - loss: 27.8928 - MinusLogProbMetric: 27.8928 - val_loss: 28.4235 - val_MinusLogProbMetric: 28.4235 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 374/1000
2023-10-25 05:38:57.420 
Epoch 374/1000 
	 loss: 27.9139, MinusLogProbMetric: 27.9139, val_loss: 28.1508, val_MinusLogProbMetric: 28.1508

Epoch 374: val_loss did not improve from 28.10043
196/196 - 41s - loss: 27.9139 - MinusLogProbMetric: 27.9139 - val_loss: 28.1508 - val_MinusLogProbMetric: 28.1508 - lr: 1.6667e-04 - 41s/epoch - 211ms/step
Epoch 375/1000
2023-10-25 05:39:39.890 
Epoch 375/1000 
	 loss: 27.9306, MinusLogProbMetric: 27.9306, val_loss: 28.2065, val_MinusLogProbMetric: 28.2065

Epoch 375: val_loss did not improve from 28.10043
196/196 - 42s - loss: 27.9306 - MinusLogProbMetric: 27.9306 - val_loss: 28.2065 - val_MinusLogProbMetric: 28.2065 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 376/1000
2023-10-25 05:40:22.068 
Epoch 376/1000 
	 loss: 27.8622, MinusLogProbMetric: 27.8622, val_loss: 28.2633, val_MinusLogProbMetric: 28.2633

Epoch 376: val_loss did not improve from 28.10043
196/196 - 42s - loss: 27.8622 - MinusLogProbMetric: 27.8622 - val_loss: 28.2633 - val_MinusLogProbMetric: 28.2633 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 377/1000
2023-10-25 05:41:03.896 
Epoch 377/1000 
	 loss: 27.9191, MinusLogProbMetric: 27.9191, val_loss: 28.2241, val_MinusLogProbMetric: 28.2241

Epoch 377: val_loss did not improve from 28.10043
196/196 - 42s - loss: 27.9191 - MinusLogProbMetric: 27.9191 - val_loss: 28.2241 - val_MinusLogProbMetric: 28.2241 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 378/1000
2023-10-25 05:41:45.950 
Epoch 378/1000 
	 loss: 27.9293, MinusLogProbMetric: 27.9293, val_loss: 28.9499, val_MinusLogProbMetric: 28.9499

Epoch 378: val_loss did not improve from 28.10043
196/196 - 42s - loss: 27.9293 - MinusLogProbMetric: 27.9293 - val_loss: 28.9499 - val_MinusLogProbMetric: 28.9499 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 379/1000
2023-10-25 05:42:27.644 
Epoch 379/1000 
	 loss: 27.8707, MinusLogProbMetric: 27.8707, val_loss: 28.1293, val_MinusLogProbMetric: 28.1293

Epoch 379: val_loss did not improve from 28.10043
196/196 - 42s - loss: 27.8707 - MinusLogProbMetric: 27.8707 - val_loss: 28.1293 - val_MinusLogProbMetric: 28.1293 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 380/1000
2023-10-25 05:43:09.820 
Epoch 380/1000 
	 loss: 27.8823, MinusLogProbMetric: 27.8823, val_loss: 28.1172, val_MinusLogProbMetric: 28.1172

Epoch 380: val_loss did not improve from 28.10043
196/196 - 42s - loss: 27.8823 - MinusLogProbMetric: 27.8823 - val_loss: 28.1172 - val_MinusLogProbMetric: 28.1172 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 381/1000
2023-10-25 05:43:51.343 
Epoch 381/1000 
	 loss: 27.9101, MinusLogProbMetric: 27.9101, val_loss: 28.3438, val_MinusLogProbMetric: 28.3438

Epoch 381: val_loss did not improve from 28.10043
196/196 - 42s - loss: 27.9101 - MinusLogProbMetric: 27.9101 - val_loss: 28.3438 - val_MinusLogProbMetric: 28.3438 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 382/1000
2023-10-25 05:44:30.963 
Epoch 382/1000 
	 loss: 27.8682, MinusLogProbMetric: 27.8682, val_loss: 28.5360, val_MinusLogProbMetric: 28.5360

Epoch 382: val_loss did not improve from 28.10043
196/196 - 40s - loss: 27.8682 - MinusLogProbMetric: 27.8682 - val_loss: 28.5360 - val_MinusLogProbMetric: 28.5360 - lr: 1.6667e-04 - 40s/epoch - 202ms/step
Epoch 383/1000
2023-10-25 05:45:11.566 
Epoch 383/1000 
	 loss: 27.8954, MinusLogProbMetric: 27.8954, val_loss: 28.1880, val_MinusLogProbMetric: 28.1880

Epoch 383: val_loss did not improve from 28.10043
196/196 - 41s - loss: 27.8954 - MinusLogProbMetric: 27.8954 - val_loss: 28.1880 - val_MinusLogProbMetric: 28.1880 - lr: 1.6667e-04 - 41s/epoch - 207ms/step
Epoch 384/1000
2023-10-25 05:45:54.212 
Epoch 384/1000 
	 loss: 27.8527, MinusLogProbMetric: 27.8527, val_loss: 28.1494, val_MinusLogProbMetric: 28.1494

Epoch 384: val_loss did not improve from 28.10043
196/196 - 43s - loss: 27.8527 - MinusLogProbMetric: 27.8527 - val_loss: 28.1494 - val_MinusLogProbMetric: 28.1494 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 385/1000
2023-10-25 05:46:36.414 
Epoch 385/1000 
	 loss: 27.9340, MinusLogProbMetric: 27.9340, val_loss: 28.5372, val_MinusLogProbMetric: 28.5372

Epoch 385: val_loss did not improve from 28.10043
196/196 - 42s - loss: 27.9340 - MinusLogProbMetric: 27.9340 - val_loss: 28.5372 - val_MinusLogProbMetric: 28.5372 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 386/1000
2023-10-25 05:47:17.232 
Epoch 386/1000 
	 loss: 27.8855, MinusLogProbMetric: 27.8855, val_loss: 28.2859, val_MinusLogProbMetric: 28.2859

Epoch 386: val_loss did not improve from 28.10043
196/196 - 41s - loss: 27.8855 - MinusLogProbMetric: 27.8855 - val_loss: 28.2859 - val_MinusLogProbMetric: 28.2859 - lr: 1.6667e-04 - 41s/epoch - 208ms/step
Epoch 387/1000
2023-10-25 05:47:59.457 
Epoch 387/1000 
	 loss: 27.9065, MinusLogProbMetric: 27.9065, val_loss: 28.2031, val_MinusLogProbMetric: 28.2031

Epoch 387: val_loss did not improve from 28.10043
196/196 - 42s - loss: 27.9065 - MinusLogProbMetric: 27.9065 - val_loss: 28.2031 - val_MinusLogProbMetric: 28.2031 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 388/1000
2023-10-25 05:48:40.952 
Epoch 388/1000 
	 loss: 27.8922, MinusLogProbMetric: 27.8922, val_loss: 28.2667, val_MinusLogProbMetric: 28.2667

Epoch 388: val_loss did not improve from 28.10043
196/196 - 41s - loss: 27.8922 - MinusLogProbMetric: 27.8922 - val_loss: 28.2667 - val_MinusLogProbMetric: 28.2667 - lr: 1.6667e-04 - 41s/epoch - 212ms/step
Epoch 389/1000
2023-10-25 05:49:22.269 
Epoch 389/1000 
	 loss: 27.8152, MinusLogProbMetric: 27.8152, val_loss: 28.5444, val_MinusLogProbMetric: 28.5444

Epoch 389: val_loss did not improve from 28.10043
196/196 - 41s - loss: 27.8152 - MinusLogProbMetric: 27.8152 - val_loss: 28.5444 - val_MinusLogProbMetric: 28.5444 - lr: 1.6667e-04 - 41s/epoch - 211ms/step
Epoch 390/1000
2023-10-25 05:50:04.648 
Epoch 390/1000 
	 loss: 27.8580, MinusLogProbMetric: 27.8580, val_loss: 28.0821, val_MinusLogProbMetric: 28.0821

Epoch 390: val_loss improved from 28.10043 to 28.08212, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 27.8580 - MinusLogProbMetric: 27.8580 - val_loss: 28.0821 - val_MinusLogProbMetric: 28.0821 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 391/1000
2023-10-25 05:50:43.174 
Epoch 391/1000 
	 loss: 27.9200, MinusLogProbMetric: 27.9200, val_loss: 28.2041, val_MinusLogProbMetric: 28.2041

Epoch 391: val_loss did not improve from 28.08212
196/196 - 38s - loss: 27.9200 - MinusLogProbMetric: 27.9200 - val_loss: 28.2041 - val_MinusLogProbMetric: 28.2041 - lr: 1.6667e-04 - 38s/epoch - 193ms/step
Epoch 392/1000
2023-10-25 05:51:23.609 
Epoch 392/1000 
	 loss: 27.8519, MinusLogProbMetric: 27.8519, val_loss: 28.3260, val_MinusLogProbMetric: 28.3260

Epoch 392: val_loss did not improve from 28.08212
196/196 - 40s - loss: 27.8519 - MinusLogProbMetric: 27.8519 - val_loss: 28.3260 - val_MinusLogProbMetric: 28.3260 - lr: 1.6667e-04 - 40s/epoch - 206ms/step
Epoch 393/1000
2023-10-25 05:52:05.847 
Epoch 393/1000 
	 loss: 27.9122, MinusLogProbMetric: 27.9122, val_loss: 28.0982, val_MinusLogProbMetric: 28.0982

Epoch 393: val_loss did not improve from 28.08212
196/196 - 42s - loss: 27.9122 - MinusLogProbMetric: 27.9122 - val_loss: 28.0982 - val_MinusLogProbMetric: 28.0982 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 394/1000
2023-10-25 05:52:47.123 
Epoch 394/1000 
	 loss: 27.8383, MinusLogProbMetric: 27.8383, val_loss: 28.1813, val_MinusLogProbMetric: 28.1813

Epoch 394: val_loss did not improve from 28.08212
196/196 - 41s - loss: 27.8383 - MinusLogProbMetric: 27.8383 - val_loss: 28.1813 - val_MinusLogProbMetric: 28.1813 - lr: 1.6667e-04 - 41s/epoch - 211ms/step
Epoch 395/1000
2023-10-25 05:53:29.470 
Epoch 395/1000 
	 loss: 27.8444, MinusLogProbMetric: 27.8444, val_loss: 28.1687, val_MinusLogProbMetric: 28.1687

Epoch 395: val_loss did not improve from 28.08212
196/196 - 42s - loss: 27.8444 - MinusLogProbMetric: 27.8444 - val_loss: 28.1687 - val_MinusLogProbMetric: 28.1687 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 396/1000
2023-10-25 05:54:11.626 
Epoch 396/1000 
	 loss: 27.8690, MinusLogProbMetric: 27.8690, val_loss: 28.2452, val_MinusLogProbMetric: 28.2452

Epoch 396: val_loss did not improve from 28.08212
196/196 - 42s - loss: 27.8690 - MinusLogProbMetric: 27.8690 - val_loss: 28.2452 - val_MinusLogProbMetric: 28.2452 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 397/1000
2023-10-25 05:54:54.175 
Epoch 397/1000 
	 loss: 27.8707, MinusLogProbMetric: 27.8707, val_loss: 28.1282, val_MinusLogProbMetric: 28.1282

Epoch 397: val_loss did not improve from 28.08212
196/196 - 43s - loss: 27.8707 - MinusLogProbMetric: 27.8707 - val_loss: 28.1282 - val_MinusLogProbMetric: 28.1282 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 398/1000
2023-10-25 05:55:37.163 
Epoch 398/1000 
	 loss: 27.8385, MinusLogProbMetric: 27.8385, val_loss: 28.1335, val_MinusLogProbMetric: 28.1335

Epoch 398: val_loss did not improve from 28.08212
196/196 - 43s - loss: 27.8385 - MinusLogProbMetric: 27.8385 - val_loss: 28.1335 - val_MinusLogProbMetric: 28.1335 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 399/1000
2023-10-25 05:56:18.753 
Epoch 399/1000 
	 loss: 27.8873, MinusLogProbMetric: 27.8873, val_loss: 28.1385, val_MinusLogProbMetric: 28.1385

Epoch 399: val_loss did not improve from 28.08212
196/196 - 42s - loss: 27.8873 - MinusLogProbMetric: 27.8873 - val_loss: 28.1385 - val_MinusLogProbMetric: 28.1385 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 400/1000
2023-10-25 05:57:01.338 
Epoch 400/1000 
	 loss: 27.8612, MinusLogProbMetric: 27.8612, val_loss: 28.2363, val_MinusLogProbMetric: 28.2363

Epoch 400: val_loss did not improve from 28.08212
196/196 - 43s - loss: 27.8612 - MinusLogProbMetric: 27.8612 - val_loss: 28.2363 - val_MinusLogProbMetric: 28.2363 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 401/1000
2023-10-25 05:57:43.513 
Epoch 401/1000 
	 loss: 27.8642, MinusLogProbMetric: 27.8642, val_loss: 28.2590, val_MinusLogProbMetric: 28.2590

Epoch 401: val_loss did not improve from 28.08212
196/196 - 42s - loss: 27.8642 - MinusLogProbMetric: 27.8642 - val_loss: 28.2590 - val_MinusLogProbMetric: 28.2590 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 402/1000
2023-10-25 05:58:26.744 
Epoch 402/1000 
	 loss: 27.8820, MinusLogProbMetric: 27.8820, val_loss: 28.2666, val_MinusLogProbMetric: 28.2666

Epoch 402: val_loss did not improve from 28.08212
196/196 - 43s - loss: 27.8820 - MinusLogProbMetric: 27.8820 - val_loss: 28.2666 - val_MinusLogProbMetric: 28.2666 - lr: 1.6667e-04 - 43s/epoch - 221ms/step
Epoch 403/1000
2023-10-25 05:59:09.300 
Epoch 403/1000 
	 loss: 27.8899, MinusLogProbMetric: 27.8899, val_loss: 28.3169, val_MinusLogProbMetric: 28.3169

Epoch 403: val_loss did not improve from 28.08212
196/196 - 43s - loss: 27.8899 - MinusLogProbMetric: 27.8899 - val_loss: 28.3169 - val_MinusLogProbMetric: 28.3169 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 404/1000
2023-10-25 05:59:52.052 
Epoch 404/1000 
	 loss: 27.8337, MinusLogProbMetric: 27.8337, val_loss: 28.1913, val_MinusLogProbMetric: 28.1913

Epoch 404: val_loss did not improve from 28.08212
196/196 - 43s - loss: 27.8337 - MinusLogProbMetric: 27.8337 - val_loss: 28.1913 - val_MinusLogProbMetric: 28.1913 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 405/1000
2023-10-25 06:00:34.651 
Epoch 405/1000 
	 loss: 27.8738, MinusLogProbMetric: 27.8738, val_loss: 28.9491, val_MinusLogProbMetric: 28.9491

Epoch 405: val_loss did not improve from 28.08212
196/196 - 43s - loss: 27.8738 - MinusLogProbMetric: 27.8738 - val_loss: 28.9491 - val_MinusLogProbMetric: 28.9491 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 406/1000
2023-10-25 06:01:16.757 
Epoch 406/1000 
	 loss: 27.9222, MinusLogProbMetric: 27.9222, val_loss: 28.3324, val_MinusLogProbMetric: 28.3324

Epoch 406: val_loss did not improve from 28.08212
196/196 - 42s - loss: 27.9222 - MinusLogProbMetric: 27.9222 - val_loss: 28.3324 - val_MinusLogProbMetric: 28.3324 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 407/1000
2023-10-25 06:01:57.963 
Epoch 407/1000 
	 loss: 27.8409, MinusLogProbMetric: 27.8409, val_loss: 28.1374, val_MinusLogProbMetric: 28.1374

Epoch 407: val_loss did not improve from 28.08212
196/196 - 41s - loss: 27.8409 - MinusLogProbMetric: 27.8409 - val_loss: 28.1374 - val_MinusLogProbMetric: 28.1374 - lr: 1.6667e-04 - 41s/epoch - 210ms/step
Epoch 408/1000
2023-10-25 06:02:40.119 
Epoch 408/1000 
	 loss: 27.8267, MinusLogProbMetric: 27.8267, val_loss: 28.1723, val_MinusLogProbMetric: 28.1723

Epoch 408: val_loss did not improve from 28.08212
196/196 - 42s - loss: 27.8267 - MinusLogProbMetric: 27.8267 - val_loss: 28.1723 - val_MinusLogProbMetric: 28.1723 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 409/1000
2023-10-25 06:03:22.104 
Epoch 409/1000 
	 loss: 27.8678, MinusLogProbMetric: 27.8678, val_loss: 28.2445, val_MinusLogProbMetric: 28.2445

Epoch 409: val_loss did not improve from 28.08212
196/196 - 42s - loss: 27.8678 - MinusLogProbMetric: 27.8678 - val_loss: 28.2445 - val_MinusLogProbMetric: 28.2445 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 410/1000
2023-10-25 06:04:03.697 
Epoch 410/1000 
	 loss: 27.8555, MinusLogProbMetric: 27.8555, val_loss: 28.3625, val_MinusLogProbMetric: 28.3625

Epoch 410: val_loss did not improve from 28.08212
196/196 - 42s - loss: 27.8555 - MinusLogProbMetric: 27.8555 - val_loss: 28.3625 - val_MinusLogProbMetric: 28.3625 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 411/1000
2023-10-25 06:04:46.326 
Epoch 411/1000 
	 loss: 27.8710, MinusLogProbMetric: 27.8710, val_loss: 28.3341, val_MinusLogProbMetric: 28.3341

Epoch 411: val_loss did not improve from 28.08212
196/196 - 43s - loss: 27.8710 - MinusLogProbMetric: 27.8710 - val_loss: 28.3341 - val_MinusLogProbMetric: 28.3341 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 412/1000
2023-10-25 06:05:26.646 
Epoch 412/1000 
	 loss: 27.8714, MinusLogProbMetric: 27.8714, val_loss: 28.0817, val_MinusLogProbMetric: 28.0817

Epoch 412: val_loss improved from 28.08212 to 28.08174, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 41s - loss: 27.8714 - MinusLogProbMetric: 27.8714 - val_loss: 28.0817 - val_MinusLogProbMetric: 28.0817 - lr: 1.6667e-04 - 41s/epoch - 209ms/step
Epoch 413/1000
2023-10-25 06:06:09.603 
Epoch 413/1000 
	 loss: 27.8661, MinusLogProbMetric: 27.8661, val_loss: 28.0850, val_MinusLogProbMetric: 28.0850

Epoch 413: val_loss did not improve from 28.08174
196/196 - 42s - loss: 27.8661 - MinusLogProbMetric: 27.8661 - val_loss: 28.0850 - val_MinusLogProbMetric: 28.0850 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 414/1000
2023-10-25 06:06:51.389 
Epoch 414/1000 
	 loss: 27.7987, MinusLogProbMetric: 27.7987, val_loss: 28.3266, val_MinusLogProbMetric: 28.3266

Epoch 414: val_loss did not improve from 28.08174
196/196 - 42s - loss: 27.7987 - MinusLogProbMetric: 27.7987 - val_loss: 28.3266 - val_MinusLogProbMetric: 28.3266 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 415/1000
2023-10-25 06:07:33.804 
Epoch 415/1000 
	 loss: 27.9024, MinusLogProbMetric: 27.9024, val_loss: 28.2014, val_MinusLogProbMetric: 28.2014

Epoch 415: val_loss did not improve from 28.08174
196/196 - 42s - loss: 27.9024 - MinusLogProbMetric: 27.9024 - val_loss: 28.2014 - val_MinusLogProbMetric: 28.2014 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 416/1000
2023-10-25 06:08:15.884 
Epoch 416/1000 
	 loss: 27.8200, MinusLogProbMetric: 27.8200, val_loss: 28.3631, val_MinusLogProbMetric: 28.3631

Epoch 416: val_loss did not improve from 28.08174
196/196 - 42s - loss: 27.8200 - MinusLogProbMetric: 27.8200 - val_loss: 28.3631 - val_MinusLogProbMetric: 28.3631 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 417/1000
2023-10-25 06:08:55.400 
Epoch 417/1000 
	 loss: 27.8369, MinusLogProbMetric: 27.8369, val_loss: 28.0880, val_MinusLogProbMetric: 28.0880

Epoch 417: val_loss did not improve from 28.08174
196/196 - 40s - loss: 27.8369 - MinusLogProbMetric: 27.8369 - val_loss: 28.0880 - val_MinusLogProbMetric: 28.0880 - lr: 1.6667e-04 - 40s/epoch - 202ms/step
Epoch 418/1000
2023-10-25 06:09:37.301 
Epoch 418/1000 
	 loss: 27.8417, MinusLogProbMetric: 27.8417, val_loss: 28.2837, val_MinusLogProbMetric: 28.2837

Epoch 418: val_loss did not improve from 28.08174
196/196 - 42s - loss: 27.8417 - MinusLogProbMetric: 27.8417 - val_loss: 28.2837 - val_MinusLogProbMetric: 28.2837 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 419/1000
2023-10-25 06:10:19.566 
Epoch 419/1000 
	 loss: 27.8483, MinusLogProbMetric: 27.8483, val_loss: 28.3667, val_MinusLogProbMetric: 28.3667

Epoch 419: val_loss did not improve from 28.08174
196/196 - 42s - loss: 27.8483 - MinusLogProbMetric: 27.8483 - val_loss: 28.3667 - val_MinusLogProbMetric: 28.3667 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 420/1000
2023-10-25 06:11:01.767 
Epoch 420/1000 
	 loss: 27.8883, MinusLogProbMetric: 27.8883, val_loss: 28.2211, val_MinusLogProbMetric: 28.2211

Epoch 420: val_loss did not improve from 28.08174
196/196 - 42s - loss: 27.8883 - MinusLogProbMetric: 27.8883 - val_loss: 28.2211 - val_MinusLogProbMetric: 28.2211 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 421/1000
2023-10-25 06:11:43.338 
Epoch 421/1000 
	 loss: 27.8436, MinusLogProbMetric: 27.8436, val_loss: 28.2955, val_MinusLogProbMetric: 28.2955

Epoch 421: val_loss did not improve from 28.08174
196/196 - 42s - loss: 27.8436 - MinusLogProbMetric: 27.8436 - val_loss: 28.2955 - val_MinusLogProbMetric: 28.2955 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 422/1000
2023-10-25 06:12:24.019 
Epoch 422/1000 
	 loss: 27.8882, MinusLogProbMetric: 27.8882, val_loss: 28.1806, val_MinusLogProbMetric: 28.1806

Epoch 422: val_loss did not improve from 28.08174
196/196 - 41s - loss: 27.8882 - MinusLogProbMetric: 27.8882 - val_loss: 28.1806 - val_MinusLogProbMetric: 28.1806 - lr: 1.6667e-04 - 41s/epoch - 208ms/step
Epoch 423/1000
2023-10-25 06:13:06.192 
Epoch 423/1000 
	 loss: 27.8964, MinusLogProbMetric: 27.8964, val_loss: 28.1699, val_MinusLogProbMetric: 28.1699

Epoch 423: val_loss did not improve from 28.08174
196/196 - 42s - loss: 27.8964 - MinusLogProbMetric: 27.8964 - val_loss: 28.1699 - val_MinusLogProbMetric: 28.1699 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 424/1000
2023-10-25 06:13:48.821 
Epoch 424/1000 
	 loss: 27.8208, MinusLogProbMetric: 27.8208, val_loss: 28.3982, val_MinusLogProbMetric: 28.3982

Epoch 424: val_loss did not improve from 28.08174
196/196 - 43s - loss: 27.8208 - MinusLogProbMetric: 27.8208 - val_loss: 28.3982 - val_MinusLogProbMetric: 28.3982 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 425/1000
2023-10-25 06:14:30.835 
Epoch 425/1000 
	 loss: 27.8531, MinusLogProbMetric: 27.8531, val_loss: 28.0614, val_MinusLogProbMetric: 28.0614

Epoch 425: val_loss improved from 28.08174 to 28.06136, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 27.8531 - MinusLogProbMetric: 27.8531 - val_loss: 28.0614 - val_MinusLogProbMetric: 28.0614 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 426/1000
2023-10-25 06:15:13.609 
Epoch 426/1000 
	 loss: 27.8298, MinusLogProbMetric: 27.8298, val_loss: 28.0605, val_MinusLogProbMetric: 28.0605

Epoch 426: val_loss improved from 28.06136 to 28.06052, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 27.8298 - MinusLogProbMetric: 27.8298 - val_loss: 28.0605 - val_MinusLogProbMetric: 28.0605 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 427/1000
2023-10-25 06:15:56.695 
Epoch 427/1000 
	 loss: 27.8331, MinusLogProbMetric: 27.8331, val_loss: 28.1229, val_MinusLogProbMetric: 28.1229

Epoch 427: val_loss did not improve from 28.06052
196/196 - 42s - loss: 27.8331 - MinusLogProbMetric: 27.8331 - val_loss: 28.1229 - val_MinusLogProbMetric: 28.1229 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 428/1000
2023-10-25 06:16:38.449 
Epoch 428/1000 
	 loss: 27.8128, MinusLogProbMetric: 27.8128, val_loss: 28.1541, val_MinusLogProbMetric: 28.1541

Epoch 428: val_loss did not improve from 28.06052
196/196 - 42s - loss: 27.8128 - MinusLogProbMetric: 27.8128 - val_loss: 28.1541 - val_MinusLogProbMetric: 28.1541 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 429/1000
2023-10-25 06:17:21.141 
Epoch 429/1000 
	 loss: 27.8749, MinusLogProbMetric: 27.8749, val_loss: 28.2939, val_MinusLogProbMetric: 28.2939

Epoch 429: val_loss did not improve from 28.06052
196/196 - 43s - loss: 27.8749 - MinusLogProbMetric: 27.8749 - val_loss: 28.2939 - val_MinusLogProbMetric: 28.2939 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 430/1000
2023-10-25 06:18:03.173 
Epoch 430/1000 
	 loss: 27.7747, MinusLogProbMetric: 27.7747, val_loss: 28.5051, val_MinusLogProbMetric: 28.5051

Epoch 430: val_loss did not improve from 28.06052
196/196 - 42s - loss: 27.7747 - MinusLogProbMetric: 27.7747 - val_loss: 28.5051 - val_MinusLogProbMetric: 28.5051 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 431/1000
2023-10-25 06:18:45.424 
Epoch 431/1000 
	 loss: 27.8801, MinusLogProbMetric: 27.8801, val_loss: 28.3214, val_MinusLogProbMetric: 28.3214

Epoch 431: val_loss did not improve from 28.06052
196/196 - 42s - loss: 27.8801 - MinusLogProbMetric: 27.8801 - val_loss: 28.3214 - val_MinusLogProbMetric: 28.3214 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 432/1000
2023-10-25 06:19:27.189 
Epoch 432/1000 
	 loss: 27.8631, MinusLogProbMetric: 27.8631, val_loss: 28.1882, val_MinusLogProbMetric: 28.1882

Epoch 432: val_loss did not improve from 28.06052
196/196 - 42s - loss: 27.8631 - MinusLogProbMetric: 27.8631 - val_loss: 28.1882 - val_MinusLogProbMetric: 28.1882 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 433/1000
2023-10-25 06:20:09.586 
Epoch 433/1000 
	 loss: 27.8119, MinusLogProbMetric: 27.8119, val_loss: 28.3168, val_MinusLogProbMetric: 28.3168

Epoch 433: val_loss did not improve from 28.06052
196/196 - 42s - loss: 27.8119 - MinusLogProbMetric: 27.8119 - val_loss: 28.3168 - val_MinusLogProbMetric: 28.3168 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 434/1000
2023-10-25 06:20:51.956 
Epoch 434/1000 
	 loss: 27.8374, MinusLogProbMetric: 27.8374, val_loss: 28.1152, val_MinusLogProbMetric: 28.1152

Epoch 434: val_loss did not improve from 28.06052
196/196 - 42s - loss: 27.8374 - MinusLogProbMetric: 27.8374 - val_loss: 28.1152 - val_MinusLogProbMetric: 28.1152 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 435/1000
2023-10-25 06:21:34.325 
Epoch 435/1000 
	 loss: 27.8697, MinusLogProbMetric: 27.8697, val_loss: 28.0952, val_MinusLogProbMetric: 28.0952

Epoch 435: val_loss did not improve from 28.06052
196/196 - 42s - loss: 27.8697 - MinusLogProbMetric: 27.8697 - val_loss: 28.0952 - val_MinusLogProbMetric: 28.0952 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 436/1000
2023-10-25 06:22:16.583 
Epoch 436/1000 
	 loss: 27.9198, MinusLogProbMetric: 27.9198, val_loss: 28.2660, val_MinusLogProbMetric: 28.2660

Epoch 436: val_loss did not improve from 28.06052
196/196 - 42s - loss: 27.9198 - MinusLogProbMetric: 27.9198 - val_loss: 28.2660 - val_MinusLogProbMetric: 28.2660 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 437/1000
2023-10-25 06:22:59.254 
Epoch 437/1000 
	 loss: 27.7742, MinusLogProbMetric: 27.7742, val_loss: 28.1097, val_MinusLogProbMetric: 28.1097

Epoch 437: val_loss did not improve from 28.06052
196/196 - 43s - loss: 27.7742 - MinusLogProbMetric: 27.7742 - val_loss: 28.1097 - val_MinusLogProbMetric: 28.1097 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 438/1000
2023-10-25 06:23:41.563 
Epoch 438/1000 
	 loss: 27.8003, MinusLogProbMetric: 27.8003, val_loss: 28.2719, val_MinusLogProbMetric: 28.2719

Epoch 438: val_loss did not improve from 28.06052
196/196 - 42s - loss: 27.8003 - MinusLogProbMetric: 27.8003 - val_loss: 28.2719 - val_MinusLogProbMetric: 28.2719 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 439/1000
2023-10-25 06:24:23.169 
Epoch 439/1000 
	 loss: 27.8956, MinusLogProbMetric: 27.8956, val_loss: 28.8273, val_MinusLogProbMetric: 28.8273

Epoch 439: val_loss did not improve from 28.06052
196/196 - 42s - loss: 27.8956 - MinusLogProbMetric: 27.8956 - val_loss: 28.8273 - val_MinusLogProbMetric: 28.8273 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 440/1000
2023-10-25 06:25:03.810 
Epoch 440/1000 
	 loss: 27.8206, MinusLogProbMetric: 27.8206, val_loss: 28.1402, val_MinusLogProbMetric: 28.1402

Epoch 440: val_loss did not improve from 28.06052
196/196 - 41s - loss: 27.8206 - MinusLogProbMetric: 27.8206 - val_loss: 28.1402 - val_MinusLogProbMetric: 28.1402 - lr: 1.6667e-04 - 41s/epoch - 207ms/step
Epoch 441/1000
2023-10-25 06:25:46.131 
Epoch 441/1000 
	 loss: 27.8448, MinusLogProbMetric: 27.8448, val_loss: 28.0706, val_MinusLogProbMetric: 28.0706

Epoch 441: val_loss did not improve from 28.06052
196/196 - 42s - loss: 27.8448 - MinusLogProbMetric: 27.8448 - val_loss: 28.0706 - val_MinusLogProbMetric: 28.0706 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 442/1000
2023-10-25 06:26:28.569 
Epoch 442/1000 
	 loss: 27.8252, MinusLogProbMetric: 27.8252, val_loss: 28.0547, val_MinusLogProbMetric: 28.0547

Epoch 442: val_loss improved from 28.06052 to 28.05471, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 27.8252 - MinusLogProbMetric: 27.8252 - val_loss: 28.0547 - val_MinusLogProbMetric: 28.0547 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 443/1000
2023-10-25 06:27:10.182 
Epoch 443/1000 
	 loss: 27.8327, MinusLogProbMetric: 27.8327, val_loss: 28.2230, val_MinusLogProbMetric: 28.2230

Epoch 443: val_loss did not improve from 28.05471
196/196 - 41s - loss: 27.8327 - MinusLogProbMetric: 27.8327 - val_loss: 28.2230 - val_MinusLogProbMetric: 28.2230 - lr: 1.6667e-04 - 41s/epoch - 209ms/step
Epoch 444/1000
2023-10-25 06:27:48.637 
Epoch 444/1000 
	 loss: 27.8059, MinusLogProbMetric: 27.8059, val_loss: 28.1105, val_MinusLogProbMetric: 28.1105

Epoch 444: val_loss did not improve from 28.05471
196/196 - 38s - loss: 27.8059 - MinusLogProbMetric: 27.8059 - val_loss: 28.1105 - val_MinusLogProbMetric: 28.1105 - lr: 1.6667e-04 - 38s/epoch - 196ms/step
Epoch 445/1000
2023-10-25 06:28:30.167 
Epoch 445/1000 
	 loss: 27.8443, MinusLogProbMetric: 27.8443, val_loss: 28.4352, val_MinusLogProbMetric: 28.4352

Epoch 445: val_loss did not improve from 28.05471
196/196 - 42s - loss: 27.8443 - MinusLogProbMetric: 27.8443 - val_loss: 28.4352 - val_MinusLogProbMetric: 28.4352 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 446/1000
2023-10-25 06:29:13.387 
Epoch 446/1000 
	 loss: 27.8392, MinusLogProbMetric: 27.8392, val_loss: 28.1539, val_MinusLogProbMetric: 28.1539

Epoch 446: val_loss did not improve from 28.05471
196/196 - 43s - loss: 27.8392 - MinusLogProbMetric: 27.8392 - val_loss: 28.1539 - val_MinusLogProbMetric: 28.1539 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 447/1000
2023-10-25 06:29:55.864 
Epoch 447/1000 
	 loss: 27.8598, MinusLogProbMetric: 27.8598, val_loss: 28.1066, val_MinusLogProbMetric: 28.1066

Epoch 447: val_loss did not improve from 28.05471
196/196 - 42s - loss: 27.8598 - MinusLogProbMetric: 27.8598 - val_loss: 28.1066 - val_MinusLogProbMetric: 28.1066 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 448/1000
2023-10-25 06:30:38.236 
Epoch 448/1000 
	 loss: 27.8635, MinusLogProbMetric: 27.8635, val_loss: 28.1961, val_MinusLogProbMetric: 28.1961

Epoch 448: val_loss did not improve from 28.05471
196/196 - 42s - loss: 27.8635 - MinusLogProbMetric: 27.8635 - val_loss: 28.1961 - val_MinusLogProbMetric: 28.1961 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 449/1000
2023-10-25 06:31:21.300 
Epoch 449/1000 
	 loss: 27.8226, MinusLogProbMetric: 27.8226, val_loss: 28.1819, val_MinusLogProbMetric: 28.1819

Epoch 449: val_loss did not improve from 28.05471
196/196 - 43s - loss: 27.8226 - MinusLogProbMetric: 27.8226 - val_loss: 28.1819 - val_MinusLogProbMetric: 28.1819 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 450/1000
2023-10-25 06:32:04.207 
Epoch 450/1000 
	 loss: 27.7791, MinusLogProbMetric: 27.7791, val_loss: 28.1308, val_MinusLogProbMetric: 28.1308

Epoch 450: val_loss did not improve from 28.05471
196/196 - 43s - loss: 27.7791 - MinusLogProbMetric: 27.7791 - val_loss: 28.1308 - val_MinusLogProbMetric: 28.1308 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 451/1000
2023-10-25 06:32:46.628 
Epoch 451/1000 
	 loss: 27.8287, MinusLogProbMetric: 27.8287, val_loss: 28.5184, val_MinusLogProbMetric: 28.5184

Epoch 451: val_loss did not improve from 28.05471
196/196 - 42s - loss: 27.8287 - MinusLogProbMetric: 27.8287 - val_loss: 28.5184 - val_MinusLogProbMetric: 28.5184 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 452/1000
2023-10-25 06:33:29.048 
Epoch 452/1000 
	 loss: 27.8734, MinusLogProbMetric: 27.8734, val_loss: 28.4074, val_MinusLogProbMetric: 28.4074

Epoch 452: val_loss did not improve from 28.05471
196/196 - 42s - loss: 27.8734 - MinusLogProbMetric: 27.8734 - val_loss: 28.4074 - val_MinusLogProbMetric: 28.4074 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 453/1000
2023-10-25 06:34:11.038 
Epoch 453/1000 
	 loss: 27.8638, MinusLogProbMetric: 27.8638, val_loss: 28.2333, val_MinusLogProbMetric: 28.2333

Epoch 453: val_loss did not improve from 28.05471
196/196 - 42s - loss: 27.8638 - MinusLogProbMetric: 27.8638 - val_loss: 28.2333 - val_MinusLogProbMetric: 28.2333 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 454/1000
2023-10-25 06:34:53.486 
Epoch 454/1000 
	 loss: 27.8699, MinusLogProbMetric: 27.8699, val_loss: 28.3240, val_MinusLogProbMetric: 28.3240

Epoch 454: val_loss did not improve from 28.05471
196/196 - 42s - loss: 27.8699 - MinusLogProbMetric: 27.8699 - val_loss: 28.3240 - val_MinusLogProbMetric: 28.3240 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 455/1000
2023-10-25 06:35:36.022 
Epoch 455/1000 
	 loss: 27.8726, MinusLogProbMetric: 27.8726, val_loss: 28.6542, val_MinusLogProbMetric: 28.6542

Epoch 455: val_loss did not improve from 28.05471
196/196 - 43s - loss: 27.8726 - MinusLogProbMetric: 27.8726 - val_loss: 28.6542 - val_MinusLogProbMetric: 28.6542 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 456/1000
2023-10-25 06:36:18.678 
Epoch 456/1000 
	 loss: 27.8154, MinusLogProbMetric: 27.8154, val_loss: 28.0642, val_MinusLogProbMetric: 28.0642

Epoch 456: val_loss did not improve from 28.05471
196/196 - 43s - loss: 27.8154 - MinusLogProbMetric: 27.8154 - val_loss: 28.0642 - val_MinusLogProbMetric: 28.0642 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 457/1000
2023-10-25 06:37:00.791 
Epoch 457/1000 
	 loss: 27.8274, MinusLogProbMetric: 27.8274, val_loss: 28.2161, val_MinusLogProbMetric: 28.2161

Epoch 457: val_loss did not improve from 28.05471
196/196 - 42s - loss: 27.8274 - MinusLogProbMetric: 27.8274 - val_loss: 28.2161 - val_MinusLogProbMetric: 28.2161 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 458/1000
2023-10-25 06:37:43.222 
Epoch 458/1000 
	 loss: 27.8710, MinusLogProbMetric: 27.8710, val_loss: 28.0209, val_MinusLogProbMetric: 28.0209

Epoch 458: val_loss improved from 28.05471 to 28.02087, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 27.8710 - MinusLogProbMetric: 27.8710 - val_loss: 28.0209 - val_MinusLogProbMetric: 28.0209 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 459/1000
2023-10-25 06:38:25.254 
Epoch 459/1000 
	 loss: 27.8432, MinusLogProbMetric: 27.8432, val_loss: 28.2060, val_MinusLogProbMetric: 28.2060

Epoch 459: val_loss did not improve from 28.02087
196/196 - 41s - loss: 27.8432 - MinusLogProbMetric: 27.8432 - val_loss: 28.2060 - val_MinusLogProbMetric: 28.2060 - lr: 1.6667e-04 - 41s/epoch - 211ms/step
Epoch 460/1000
2023-10-25 06:39:06.902 
Epoch 460/1000 
	 loss: 27.8148, MinusLogProbMetric: 27.8148, val_loss: 28.9211, val_MinusLogProbMetric: 28.9211

Epoch 460: val_loss did not improve from 28.02087
196/196 - 42s - loss: 27.8148 - MinusLogProbMetric: 27.8148 - val_loss: 28.9211 - val_MinusLogProbMetric: 28.9211 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 461/1000
2023-10-25 06:39:49.568 
Epoch 461/1000 
	 loss: 27.8903, MinusLogProbMetric: 27.8903, val_loss: 28.1643, val_MinusLogProbMetric: 28.1643

Epoch 461: val_loss did not improve from 28.02087
196/196 - 43s - loss: 27.8903 - MinusLogProbMetric: 27.8903 - val_loss: 28.1643 - val_MinusLogProbMetric: 28.1643 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 462/1000
2023-10-25 06:40:30.591 
Epoch 462/1000 
	 loss: 27.7949, MinusLogProbMetric: 27.7949, val_loss: 28.7748, val_MinusLogProbMetric: 28.7748

Epoch 462: val_loss did not improve from 28.02087
196/196 - 41s - loss: 27.7949 - MinusLogProbMetric: 27.7949 - val_loss: 28.7748 - val_MinusLogProbMetric: 28.7748 - lr: 1.6667e-04 - 41s/epoch - 209ms/step
Epoch 463/1000
2023-10-25 06:41:12.621 
Epoch 463/1000 
	 loss: 28.3051, MinusLogProbMetric: 28.3051, val_loss: 28.3483, val_MinusLogProbMetric: 28.3483

Epoch 463: val_loss did not improve from 28.02087
196/196 - 42s - loss: 28.3051 - MinusLogProbMetric: 28.3051 - val_loss: 28.3483 - val_MinusLogProbMetric: 28.3483 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 464/1000
2023-10-25 06:41:54.436 
Epoch 464/1000 
	 loss: 27.7930, MinusLogProbMetric: 27.7930, val_loss: 28.1925, val_MinusLogProbMetric: 28.1925

Epoch 464: val_loss did not improve from 28.02087
196/196 - 42s - loss: 27.7930 - MinusLogProbMetric: 27.7930 - val_loss: 28.1925 - val_MinusLogProbMetric: 28.1925 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 465/1000
2023-10-25 06:42:37.251 
Epoch 465/1000 
	 loss: 27.7807, MinusLogProbMetric: 27.7807, val_loss: 28.3617, val_MinusLogProbMetric: 28.3617

Epoch 465: val_loss did not improve from 28.02087
196/196 - 43s - loss: 27.7807 - MinusLogProbMetric: 27.7807 - val_loss: 28.3617 - val_MinusLogProbMetric: 28.3617 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 466/1000
2023-10-25 06:43:19.573 
Epoch 466/1000 
	 loss: 27.8046, MinusLogProbMetric: 27.8046, val_loss: 28.2431, val_MinusLogProbMetric: 28.2431

Epoch 466: val_loss did not improve from 28.02087
196/196 - 42s - loss: 27.8046 - MinusLogProbMetric: 27.8046 - val_loss: 28.2431 - val_MinusLogProbMetric: 28.2431 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 467/1000
2023-10-25 06:44:02.357 
Epoch 467/1000 
	 loss: 27.8770, MinusLogProbMetric: 27.8770, val_loss: 28.1259, val_MinusLogProbMetric: 28.1259

Epoch 467: val_loss did not improve from 28.02087
196/196 - 43s - loss: 27.8770 - MinusLogProbMetric: 27.8770 - val_loss: 28.1259 - val_MinusLogProbMetric: 28.1259 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 468/1000
2023-10-25 06:44:44.269 
Epoch 468/1000 
	 loss: 27.7819, MinusLogProbMetric: 27.7819, val_loss: 28.1836, val_MinusLogProbMetric: 28.1836

Epoch 468: val_loss did not improve from 28.02087
196/196 - 42s - loss: 27.7819 - MinusLogProbMetric: 27.7819 - val_loss: 28.1836 - val_MinusLogProbMetric: 28.1836 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 469/1000
2023-10-25 06:45:26.960 
Epoch 469/1000 
	 loss: 27.8325, MinusLogProbMetric: 27.8325, val_loss: 28.1738, val_MinusLogProbMetric: 28.1738

Epoch 469: val_loss did not improve from 28.02087
196/196 - 43s - loss: 27.8325 - MinusLogProbMetric: 27.8325 - val_loss: 28.1738 - val_MinusLogProbMetric: 28.1738 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 470/1000
2023-10-25 06:46:09.085 
Epoch 470/1000 
	 loss: 27.8513, MinusLogProbMetric: 27.8513, val_loss: 28.3323, val_MinusLogProbMetric: 28.3323

Epoch 470: val_loss did not improve from 28.02087
196/196 - 42s - loss: 27.8513 - MinusLogProbMetric: 27.8513 - val_loss: 28.3323 - val_MinusLogProbMetric: 28.3323 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 471/1000
2023-10-25 06:46:49.742 
Epoch 471/1000 
	 loss: 27.8080, MinusLogProbMetric: 27.8080, val_loss: 28.1622, val_MinusLogProbMetric: 28.1622

Epoch 471: val_loss did not improve from 28.02087
196/196 - 41s - loss: 27.8080 - MinusLogProbMetric: 27.8080 - val_loss: 28.1622 - val_MinusLogProbMetric: 28.1622 - lr: 1.6667e-04 - 41s/epoch - 207ms/step
Epoch 472/1000
2023-10-25 06:47:31.856 
Epoch 472/1000 
	 loss: 27.7859, MinusLogProbMetric: 27.7859, val_loss: 28.2455, val_MinusLogProbMetric: 28.2455

Epoch 472: val_loss did not improve from 28.02087
196/196 - 42s - loss: 27.7859 - MinusLogProbMetric: 27.7859 - val_loss: 28.2455 - val_MinusLogProbMetric: 28.2455 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 473/1000
2023-10-25 06:48:14.174 
Epoch 473/1000 
	 loss: 27.8550, MinusLogProbMetric: 27.8550, val_loss: 28.0897, val_MinusLogProbMetric: 28.0897

Epoch 473: val_loss did not improve from 28.02087
196/196 - 42s - loss: 27.8550 - MinusLogProbMetric: 27.8550 - val_loss: 28.0897 - val_MinusLogProbMetric: 28.0897 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 474/1000
2023-10-25 06:48:55.354 
Epoch 474/1000 
	 loss: 27.7991, MinusLogProbMetric: 27.7991, val_loss: 28.1137, val_MinusLogProbMetric: 28.1137

Epoch 474: val_loss did not improve from 28.02087
196/196 - 41s - loss: 27.7991 - MinusLogProbMetric: 27.7991 - val_loss: 28.1137 - val_MinusLogProbMetric: 28.1137 - lr: 1.6667e-04 - 41s/epoch - 210ms/step
Epoch 475/1000
2023-10-25 06:49:37.801 
Epoch 475/1000 
	 loss: 27.8711, MinusLogProbMetric: 27.8711, val_loss: 29.0492, val_MinusLogProbMetric: 29.0492

Epoch 475: val_loss did not improve from 28.02087
196/196 - 42s - loss: 27.8711 - MinusLogProbMetric: 27.8711 - val_loss: 29.0492 - val_MinusLogProbMetric: 29.0492 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 476/1000
2023-10-25 06:50:19.797 
Epoch 476/1000 
	 loss: 27.8246, MinusLogProbMetric: 27.8246, val_loss: 28.9833, val_MinusLogProbMetric: 28.9833

Epoch 476: val_loss did not improve from 28.02087
196/196 - 42s - loss: 27.8246 - MinusLogProbMetric: 27.8246 - val_loss: 28.9833 - val_MinusLogProbMetric: 28.9833 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 477/1000
2023-10-25 06:51:02.140 
Epoch 477/1000 
	 loss: 27.8346, MinusLogProbMetric: 27.8346, val_loss: 28.6280, val_MinusLogProbMetric: 28.6280

Epoch 477: val_loss did not improve from 28.02087
196/196 - 42s - loss: 27.8346 - MinusLogProbMetric: 27.8346 - val_loss: 28.6280 - val_MinusLogProbMetric: 28.6280 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 478/1000
2023-10-25 06:51:44.144 
Epoch 478/1000 
	 loss: 27.8163, MinusLogProbMetric: 27.8163, val_loss: 28.2487, val_MinusLogProbMetric: 28.2487

Epoch 478: val_loss did not improve from 28.02087
196/196 - 42s - loss: 27.8163 - MinusLogProbMetric: 27.8163 - val_loss: 28.2487 - val_MinusLogProbMetric: 28.2487 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 479/1000
2023-10-25 06:52:27.142 
Epoch 479/1000 
	 loss: 27.8073, MinusLogProbMetric: 27.8073, val_loss: 28.4752, val_MinusLogProbMetric: 28.4752

Epoch 479: val_loss did not improve from 28.02087
196/196 - 43s - loss: 27.8073 - MinusLogProbMetric: 27.8073 - val_loss: 28.4752 - val_MinusLogProbMetric: 28.4752 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 480/1000
2023-10-25 06:53:09.679 
Epoch 480/1000 
	 loss: 27.7705, MinusLogProbMetric: 27.7705, val_loss: 28.0169, val_MinusLogProbMetric: 28.0169

Epoch 480: val_loss improved from 28.02087 to 28.01686, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 27.7705 - MinusLogProbMetric: 27.7705 - val_loss: 28.0169 - val_MinusLogProbMetric: 28.0169 - lr: 1.6667e-04 - 43s/epoch - 221ms/step
Epoch 481/1000
2023-10-25 06:53:52.945 
Epoch 481/1000 
	 loss: 27.9196, MinusLogProbMetric: 27.9196, val_loss: 28.3108, val_MinusLogProbMetric: 28.3108

Epoch 481: val_loss did not improve from 28.01686
196/196 - 43s - loss: 27.9196 - MinusLogProbMetric: 27.9196 - val_loss: 28.3108 - val_MinusLogProbMetric: 28.3108 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 482/1000
2023-10-25 06:54:34.496 
Epoch 482/1000 
	 loss: 27.7698, MinusLogProbMetric: 27.7698, val_loss: 28.0465, val_MinusLogProbMetric: 28.0465

Epoch 482: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.7698 - MinusLogProbMetric: 27.7698 - val_loss: 28.0465 - val_MinusLogProbMetric: 28.0465 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 483/1000
2023-10-25 06:55:17.215 
Epoch 483/1000 
	 loss: 27.8003, MinusLogProbMetric: 27.8003, val_loss: 28.1325, val_MinusLogProbMetric: 28.1325

Epoch 483: val_loss did not improve from 28.01686
196/196 - 43s - loss: 27.8003 - MinusLogProbMetric: 27.8003 - val_loss: 28.1325 - val_MinusLogProbMetric: 28.1325 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 484/1000
2023-10-25 06:55:59.463 
Epoch 484/1000 
	 loss: 27.8723, MinusLogProbMetric: 27.8723, val_loss: 28.2822, val_MinusLogProbMetric: 28.2822

Epoch 484: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.8723 - MinusLogProbMetric: 27.8723 - val_loss: 28.2822 - val_MinusLogProbMetric: 28.2822 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 485/1000
2023-10-25 06:56:39.493 
Epoch 485/1000 
	 loss: 27.8053, MinusLogProbMetric: 27.8053, val_loss: 28.1926, val_MinusLogProbMetric: 28.1926

Epoch 485: val_loss did not improve from 28.01686
196/196 - 40s - loss: 27.8053 - MinusLogProbMetric: 27.8053 - val_loss: 28.1926 - val_MinusLogProbMetric: 28.1926 - lr: 1.6667e-04 - 40s/epoch - 204ms/step
Epoch 486/1000
2023-10-25 06:57:19.806 
Epoch 486/1000 
	 loss: 27.7872, MinusLogProbMetric: 27.7872, val_loss: 28.2091, val_MinusLogProbMetric: 28.2091

Epoch 486: val_loss did not improve from 28.01686
196/196 - 40s - loss: 27.7872 - MinusLogProbMetric: 27.7872 - val_loss: 28.2091 - val_MinusLogProbMetric: 28.2091 - lr: 1.6667e-04 - 40s/epoch - 206ms/step
Epoch 487/1000
2023-10-25 06:58:01.705 
Epoch 487/1000 
	 loss: 27.8262, MinusLogProbMetric: 27.8262, val_loss: 28.1729, val_MinusLogProbMetric: 28.1729

Epoch 487: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.8262 - MinusLogProbMetric: 27.8262 - val_loss: 28.1729 - val_MinusLogProbMetric: 28.1729 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 488/1000
2023-10-25 06:58:42.686 
Epoch 488/1000 
	 loss: 27.8250, MinusLogProbMetric: 27.8250, val_loss: 28.4326, val_MinusLogProbMetric: 28.4326

Epoch 488: val_loss did not improve from 28.01686
196/196 - 41s - loss: 27.8250 - MinusLogProbMetric: 27.8250 - val_loss: 28.4326 - val_MinusLogProbMetric: 28.4326 - lr: 1.6667e-04 - 41s/epoch - 209ms/step
Epoch 489/1000
2023-10-25 06:59:24.617 
Epoch 489/1000 
	 loss: 27.8087, MinusLogProbMetric: 27.8087, val_loss: 28.3457, val_MinusLogProbMetric: 28.3457

Epoch 489: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.8087 - MinusLogProbMetric: 27.8087 - val_loss: 28.3457 - val_MinusLogProbMetric: 28.3457 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 490/1000
2023-10-25 07:00:06.960 
Epoch 490/1000 
	 loss: 27.8602, MinusLogProbMetric: 27.8602, val_loss: 28.4487, val_MinusLogProbMetric: 28.4487

Epoch 490: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.8602 - MinusLogProbMetric: 27.8602 - val_loss: 28.4487 - val_MinusLogProbMetric: 28.4487 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 491/1000
2023-10-25 07:00:47.321 
Epoch 491/1000 
	 loss: 27.8393, MinusLogProbMetric: 27.8393, val_loss: 28.1172, val_MinusLogProbMetric: 28.1172

Epoch 491: val_loss did not improve from 28.01686
196/196 - 40s - loss: 27.8393 - MinusLogProbMetric: 27.8393 - val_loss: 28.1172 - val_MinusLogProbMetric: 28.1172 - lr: 1.6667e-04 - 40s/epoch - 206ms/step
Epoch 492/1000
2023-10-25 07:01:26.270 
Epoch 492/1000 
	 loss: 27.7768, MinusLogProbMetric: 27.7768, val_loss: 28.0726, val_MinusLogProbMetric: 28.0726

Epoch 492: val_loss did not improve from 28.01686
196/196 - 39s - loss: 27.7768 - MinusLogProbMetric: 27.7768 - val_loss: 28.0726 - val_MinusLogProbMetric: 28.0726 - lr: 1.6667e-04 - 39s/epoch - 199ms/step
Epoch 493/1000
2023-10-25 07:02:08.269 
Epoch 493/1000 
	 loss: 27.8088, MinusLogProbMetric: 27.8088, val_loss: 28.1593, val_MinusLogProbMetric: 28.1593

Epoch 493: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.8088 - MinusLogProbMetric: 27.8088 - val_loss: 28.1593 - val_MinusLogProbMetric: 28.1593 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 494/1000
2023-10-25 07:02:49.947 
Epoch 494/1000 
	 loss: 27.8235, MinusLogProbMetric: 27.8235, val_loss: 28.2092, val_MinusLogProbMetric: 28.2092

Epoch 494: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.8235 - MinusLogProbMetric: 27.8235 - val_loss: 28.2092 - val_MinusLogProbMetric: 28.2092 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 495/1000
2023-10-25 07:03:31.594 
Epoch 495/1000 
	 loss: 27.7935, MinusLogProbMetric: 27.7935, val_loss: 28.1668, val_MinusLogProbMetric: 28.1668

Epoch 495: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.7935 - MinusLogProbMetric: 27.7935 - val_loss: 28.1668 - val_MinusLogProbMetric: 28.1668 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 496/1000
2023-10-25 07:04:13.730 
Epoch 496/1000 
	 loss: 27.8112, MinusLogProbMetric: 27.8112, val_loss: 28.1404, val_MinusLogProbMetric: 28.1404

Epoch 496: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.8112 - MinusLogProbMetric: 27.8112 - val_loss: 28.1404 - val_MinusLogProbMetric: 28.1404 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 497/1000
2023-10-25 07:04:55.306 
Epoch 497/1000 
	 loss: 27.7861, MinusLogProbMetric: 27.7861, val_loss: 28.2859, val_MinusLogProbMetric: 28.2859

Epoch 497: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.7861 - MinusLogProbMetric: 27.7861 - val_loss: 28.2859 - val_MinusLogProbMetric: 28.2859 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 498/1000
2023-10-25 07:05:33.668 
Epoch 498/1000 
	 loss: 27.8208, MinusLogProbMetric: 27.8208, val_loss: 28.0421, val_MinusLogProbMetric: 28.0421

Epoch 498: val_loss did not improve from 28.01686
196/196 - 38s - loss: 27.8208 - MinusLogProbMetric: 27.8208 - val_loss: 28.0421 - val_MinusLogProbMetric: 28.0421 - lr: 1.6667e-04 - 38s/epoch - 196ms/step
Epoch 499/1000
2023-10-25 07:06:16.321 
Epoch 499/1000 
	 loss: 27.8109, MinusLogProbMetric: 27.8109, val_loss: 28.0840, val_MinusLogProbMetric: 28.0840

Epoch 499: val_loss did not improve from 28.01686
196/196 - 43s - loss: 27.8109 - MinusLogProbMetric: 27.8109 - val_loss: 28.0840 - val_MinusLogProbMetric: 28.0840 - lr: 1.6667e-04 - 43s/epoch - 218ms/step
Epoch 500/1000
2023-10-25 07:06:58.715 
Epoch 500/1000 
	 loss: 27.8132, MinusLogProbMetric: 27.8132, val_loss: 28.1283, val_MinusLogProbMetric: 28.1283

Epoch 500: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.8132 - MinusLogProbMetric: 27.8132 - val_loss: 28.1283 - val_MinusLogProbMetric: 28.1283 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 501/1000
2023-10-25 07:07:41.150 
Epoch 501/1000 
	 loss: 27.7994, MinusLogProbMetric: 27.7994, val_loss: 28.4207, val_MinusLogProbMetric: 28.4207

Epoch 501: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.7994 - MinusLogProbMetric: 27.7994 - val_loss: 28.4207 - val_MinusLogProbMetric: 28.4207 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 502/1000
2023-10-25 07:08:23.149 
Epoch 502/1000 
	 loss: 27.8170, MinusLogProbMetric: 27.8170, val_loss: 28.1834, val_MinusLogProbMetric: 28.1834

Epoch 502: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.8170 - MinusLogProbMetric: 27.8170 - val_loss: 28.1834 - val_MinusLogProbMetric: 28.1834 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 503/1000
2023-10-25 07:09:04.572 
Epoch 503/1000 
	 loss: 27.8424, MinusLogProbMetric: 27.8424, val_loss: 28.1506, val_MinusLogProbMetric: 28.1506

Epoch 503: val_loss did not improve from 28.01686
196/196 - 41s - loss: 27.8424 - MinusLogProbMetric: 27.8424 - val_loss: 28.1506 - val_MinusLogProbMetric: 28.1506 - lr: 1.6667e-04 - 41s/epoch - 211ms/step
Epoch 504/1000
2023-10-25 07:09:46.076 
Epoch 504/1000 
	 loss: 27.7908, MinusLogProbMetric: 27.7908, val_loss: 28.0405, val_MinusLogProbMetric: 28.0405

Epoch 504: val_loss did not improve from 28.01686
196/196 - 41s - loss: 27.7908 - MinusLogProbMetric: 27.7908 - val_loss: 28.0405 - val_MinusLogProbMetric: 28.0405 - lr: 1.6667e-04 - 41s/epoch - 212ms/step
Epoch 505/1000
2023-10-25 07:10:28.534 
Epoch 505/1000 
	 loss: 27.7831, MinusLogProbMetric: 27.7831, val_loss: 28.0242, val_MinusLogProbMetric: 28.0242

Epoch 505: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.7831 - MinusLogProbMetric: 27.7831 - val_loss: 28.0242 - val_MinusLogProbMetric: 28.0242 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 506/1000
2023-10-25 07:11:11.129 
Epoch 506/1000 
	 loss: 27.7914, MinusLogProbMetric: 27.7914, val_loss: 28.1429, val_MinusLogProbMetric: 28.1429

Epoch 506: val_loss did not improve from 28.01686
196/196 - 43s - loss: 27.7914 - MinusLogProbMetric: 27.7914 - val_loss: 28.1429 - val_MinusLogProbMetric: 28.1429 - lr: 1.6667e-04 - 43s/epoch - 217ms/step
Epoch 507/1000
2023-10-25 07:11:53.319 
Epoch 507/1000 
	 loss: 27.8078, MinusLogProbMetric: 27.8078, val_loss: 28.1054, val_MinusLogProbMetric: 28.1054

Epoch 507: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.8078 - MinusLogProbMetric: 27.8078 - val_loss: 28.1054 - val_MinusLogProbMetric: 28.1054 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 508/1000
2023-10-25 07:12:33.090 
Epoch 508/1000 
	 loss: 27.7459, MinusLogProbMetric: 27.7459, val_loss: 28.0408, val_MinusLogProbMetric: 28.0408

Epoch 508: val_loss did not improve from 28.01686
196/196 - 40s - loss: 27.7459 - MinusLogProbMetric: 27.7459 - val_loss: 28.0408 - val_MinusLogProbMetric: 28.0408 - lr: 1.6667e-04 - 40s/epoch - 203ms/step
Epoch 509/1000
2023-10-25 07:13:14.250 
Epoch 509/1000 
	 loss: 27.8586, MinusLogProbMetric: 27.8586, val_loss: 28.2014, val_MinusLogProbMetric: 28.2014

Epoch 509: val_loss did not improve from 28.01686
196/196 - 41s - loss: 27.8586 - MinusLogProbMetric: 27.8586 - val_loss: 28.2014 - val_MinusLogProbMetric: 28.2014 - lr: 1.6667e-04 - 41s/epoch - 210ms/step
Epoch 510/1000
2023-10-25 07:13:55.606 
Epoch 510/1000 
	 loss: 27.7942, MinusLogProbMetric: 27.7942, val_loss: 28.1071, val_MinusLogProbMetric: 28.1071

Epoch 510: val_loss did not improve from 28.01686
196/196 - 41s - loss: 27.7942 - MinusLogProbMetric: 27.7942 - val_loss: 28.1071 - val_MinusLogProbMetric: 28.1071 - lr: 1.6667e-04 - 41s/epoch - 211ms/step
Epoch 511/1000
2023-10-25 07:14:37.308 
Epoch 511/1000 
	 loss: 27.8216, MinusLogProbMetric: 27.8216, val_loss: 28.0886, val_MinusLogProbMetric: 28.0886

Epoch 511: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.8216 - MinusLogProbMetric: 27.8216 - val_loss: 28.0886 - val_MinusLogProbMetric: 28.0886 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 512/1000
2023-10-25 07:15:19.316 
Epoch 512/1000 
	 loss: 27.7624, MinusLogProbMetric: 27.7624, val_loss: 28.1083, val_MinusLogProbMetric: 28.1083

Epoch 512: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.7624 - MinusLogProbMetric: 27.7624 - val_loss: 28.1083 - val_MinusLogProbMetric: 28.1083 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 513/1000
2023-10-25 07:16:01.559 
Epoch 513/1000 
	 loss: 27.8396, MinusLogProbMetric: 27.8396, val_loss: 28.0845, val_MinusLogProbMetric: 28.0845

Epoch 513: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.8396 - MinusLogProbMetric: 27.8396 - val_loss: 28.0845 - val_MinusLogProbMetric: 28.0845 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 514/1000
2023-10-25 07:16:41.640 
Epoch 514/1000 
	 loss: 27.8140, MinusLogProbMetric: 27.8140, val_loss: 28.1649, val_MinusLogProbMetric: 28.1649

Epoch 514: val_loss did not improve from 28.01686
196/196 - 40s - loss: 27.8140 - MinusLogProbMetric: 27.8140 - val_loss: 28.1649 - val_MinusLogProbMetric: 28.1649 - lr: 1.6667e-04 - 40s/epoch - 204ms/step
Epoch 515/1000
2023-10-25 07:17:23.211 
Epoch 515/1000 
	 loss: 27.9097, MinusLogProbMetric: 27.9097, val_loss: 28.1839, val_MinusLogProbMetric: 28.1839

Epoch 515: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.9097 - MinusLogProbMetric: 27.9097 - val_loss: 28.1839 - val_MinusLogProbMetric: 28.1839 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 516/1000
2023-10-25 07:18:05.604 
Epoch 516/1000 
	 loss: 27.7700, MinusLogProbMetric: 27.7700, val_loss: 28.1658, val_MinusLogProbMetric: 28.1658

Epoch 516: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.7700 - MinusLogProbMetric: 27.7700 - val_loss: 28.1658 - val_MinusLogProbMetric: 28.1658 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 517/1000
2023-10-25 07:18:47.879 
Epoch 517/1000 
	 loss: 27.7399, MinusLogProbMetric: 27.7399, val_loss: 28.0485, val_MinusLogProbMetric: 28.0485

Epoch 517: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.7399 - MinusLogProbMetric: 27.7399 - val_loss: 28.0485 - val_MinusLogProbMetric: 28.0485 - lr: 1.6667e-04 - 42s/epoch - 216ms/step
Epoch 518/1000
2023-10-25 07:19:29.941 
Epoch 518/1000 
	 loss: 28.3444, MinusLogProbMetric: 28.3444, val_loss: 28.2663, val_MinusLogProbMetric: 28.2663

Epoch 518: val_loss did not improve from 28.01686
196/196 - 42s - loss: 28.3444 - MinusLogProbMetric: 28.3444 - val_loss: 28.2663 - val_MinusLogProbMetric: 28.2663 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 519/1000
2023-10-25 07:20:11.989 
Epoch 519/1000 
	 loss: 27.7888, MinusLogProbMetric: 27.7888, val_loss: 28.1824, val_MinusLogProbMetric: 28.1824

Epoch 519: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.7888 - MinusLogProbMetric: 27.7888 - val_loss: 28.1824 - val_MinusLogProbMetric: 28.1824 - lr: 1.6667e-04 - 42s/epoch - 215ms/step
Epoch 520/1000
2023-10-25 07:20:53.198 
Epoch 520/1000 
	 loss: 27.8099, MinusLogProbMetric: 27.8099, val_loss: 28.1626, val_MinusLogProbMetric: 28.1626

Epoch 520: val_loss did not improve from 28.01686
196/196 - 41s - loss: 27.8099 - MinusLogProbMetric: 27.8099 - val_loss: 28.1626 - val_MinusLogProbMetric: 28.1626 - lr: 1.6667e-04 - 41s/epoch - 210ms/step
Epoch 521/1000
2023-10-25 07:21:36.268 
Epoch 521/1000 
	 loss: 27.7595, MinusLogProbMetric: 27.7595, val_loss: 28.0584, val_MinusLogProbMetric: 28.0584

Epoch 521: val_loss did not improve from 28.01686
196/196 - 43s - loss: 27.7595 - MinusLogProbMetric: 27.7595 - val_loss: 28.0584 - val_MinusLogProbMetric: 28.0584 - lr: 1.6667e-04 - 43s/epoch - 220ms/step
Epoch 522/1000
2023-10-25 07:22:18.262 
Epoch 522/1000 
	 loss: 27.8149, MinusLogProbMetric: 27.8149, val_loss: 28.0804, val_MinusLogProbMetric: 28.0804

Epoch 522: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.8149 - MinusLogProbMetric: 27.8149 - val_loss: 28.0804 - val_MinusLogProbMetric: 28.0804 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 523/1000
2023-10-25 07:22:59.306 
Epoch 523/1000 
	 loss: 27.8151, MinusLogProbMetric: 27.8151, val_loss: 28.3086, val_MinusLogProbMetric: 28.3086

Epoch 523: val_loss did not improve from 28.01686
196/196 - 41s - loss: 27.8151 - MinusLogProbMetric: 27.8151 - val_loss: 28.3086 - val_MinusLogProbMetric: 28.3086 - lr: 1.6667e-04 - 41s/epoch - 209ms/step
Epoch 524/1000
2023-10-25 07:23:41.217 
Epoch 524/1000 
	 loss: 27.8006, MinusLogProbMetric: 27.8006, val_loss: 28.1836, val_MinusLogProbMetric: 28.1836

Epoch 524: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.8006 - MinusLogProbMetric: 27.8006 - val_loss: 28.1836 - val_MinusLogProbMetric: 28.1836 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 525/1000
2023-10-25 07:24:23.232 
Epoch 525/1000 
	 loss: 27.8040, MinusLogProbMetric: 27.8040, val_loss: 28.1222, val_MinusLogProbMetric: 28.1222

Epoch 525: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.8040 - MinusLogProbMetric: 27.8040 - val_loss: 28.1222 - val_MinusLogProbMetric: 28.1222 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 526/1000
2023-10-25 07:25:04.245 
Epoch 526/1000 
	 loss: 27.7992, MinusLogProbMetric: 27.7992, val_loss: 28.2444, val_MinusLogProbMetric: 28.2444

Epoch 526: val_loss did not improve from 28.01686
196/196 - 41s - loss: 27.7992 - MinusLogProbMetric: 27.7992 - val_loss: 28.2444 - val_MinusLogProbMetric: 28.2444 - lr: 1.6667e-04 - 41s/epoch - 209ms/step
Epoch 527/1000
2023-10-25 07:25:44.599 
Epoch 527/1000 
	 loss: 27.8510, MinusLogProbMetric: 27.8510, val_loss: 28.1251, val_MinusLogProbMetric: 28.1251

Epoch 527: val_loss did not improve from 28.01686
196/196 - 40s - loss: 27.8510 - MinusLogProbMetric: 27.8510 - val_loss: 28.1251 - val_MinusLogProbMetric: 28.1251 - lr: 1.6667e-04 - 40s/epoch - 206ms/step
Epoch 528/1000
2023-10-25 07:26:27.088 
Epoch 528/1000 
	 loss: 27.7459, MinusLogProbMetric: 27.7459, val_loss: 28.1188, val_MinusLogProbMetric: 28.1188

Epoch 528: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.7459 - MinusLogProbMetric: 27.7459 - val_loss: 28.1188 - val_MinusLogProbMetric: 28.1188 - lr: 1.6667e-04 - 42s/epoch - 217ms/step
Epoch 529/1000
2023-10-25 07:27:09.985 
Epoch 529/1000 
	 loss: 27.8456, MinusLogProbMetric: 27.8456, val_loss: 28.2384, val_MinusLogProbMetric: 28.2384

Epoch 529: val_loss did not improve from 28.01686
196/196 - 43s - loss: 27.8456 - MinusLogProbMetric: 27.8456 - val_loss: 28.2384 - val_MinusLogProbMetric: 28.2384 - lr: 1.6667e-04 - 43s/epoch - 219ms/step
Epoch 530/1000
2023-10-25 07:27:51.738 
Epoch 530/1000 
	 loss: 27.7517, MinusLogProbMetric: 27.7517, val_loss: 28.0748, val_MinusLogProbMetric: 28.0748

Epoch 530: val_loss did not improve from 28.01686
196/196 - 42s - loss: 27.7517 - MinusLogProbMetric: 27.7517 - val_loss: 28.0748 - val_MinusLogProbMetric: 28.0748 - lr: 1.6667e-04 - 42s/epoch - 213ms/step
Epoch 531/1000
2023-10-25 07:28:33.061 
Epoch 531/1000 
	 loss: 27.5432, MinusLogProbMetric: 27.5432, val_loss: 27.9475, val_MinusLogProbMetric: 27.9475

Epoch 531: val_loss improved from 28.01686 to 27.94749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 27.5432 - MinusLogProbMetric: 27.5432 - val_loss: 27.9475 - val_MinusLogProbMetric: 27.9475 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 532/1000
2023-10-25 07:29:15.749 
Epoch 532/1000 
	 loss: 27.5418, MinusLogProbMetric: 27.5418, val_loss: 28.0330, val_MinusLogProbMetric: 28.0330

Epoch 532: val_loss did not improve from 27.94749
196/196 - 42s - loss: 27.5418 - MinusLogProbMetric: 27.5418 - val_loss: 28.0330 - val_MinusLogProbMetric: 28.0330 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 533/1000
2023-10-25 07:29:55.818 
Epoch 533/1000 
	 loss: 27.5473, MinusLogProbMetric: 27.5473, val_loss: 27.9922, val_MinusLogProbMetric: 27.9922

Epoch 533: val_loss did not improve from 27.94749
196/196 - 40s - loss: 27.5473 - MinusLogProbMetric: 27.5473 - val_loss: 27.9922 - val_MinusLogProbMetric: 27.9922 - lr: 8.3333e-05 - 40s/epoch - 204ms/step
Epoch 534/1000
2023-10-25 07:30:35.238 
Epoch 534/1000 
	 loss: 27.5682, MinusLogProbMetric: 27.5682, val_loss: 27.9236, val_MinusLogProbMetric: 27.9236

Epoch 534: val_loss improved from 27.94749 to 27.92361, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 40s - loss: 27.5682 - MinusLogProbMetric: 27.5682 - val_loss: 27.9236 - val_MinusLogProbMetric: 27.9236 - lr: 8.3333e-05 - 40s/epoch - 204ms/step
Epoch 535/1000
2023-10-25 07:31:17.997 
Epoch 535/1000 
	 loss: 27.5287, MinusLogProbMetric: 27.5287, val_loss: 27.9154, val_MinusLogProbMetric: 27.9154

Epoch 535: val_loss improved from 27.92361 to 27.91545, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 27.5287 - MinusLogProbMetric: 27.5287 - val_loss: 27.9154 - val_MinusLogProbMetric: 27.9154 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 536/1000
2023-10-25 07:32:00.838 
Epoch 536/1000 
	 loss: 27.5432, MinusLogProbMetric: 27.5432, val_loss: 28.0063, val_MinusLogProbMetric: 28.0063

Epoch 536: val_loss did not improve from 27.91545
196/196 - 42s - loss: 27.5432 - MinusLogProbMetric: 27.5432 - val_loss: 28.0063 - val_MinusLogProbMetric: 28.0063 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 537/1000
2023-10-25 07:32:43.560 
Epoch 537/1000 
	 loss: 27.5566, MinusLogProbMetric: 27.5566, val_loss: 28.0316, val_MinusLogProbMetric: 28.0316

Epoch 537: val_loss did not improve from 27.91545
196/196 - 43s - loss: 27.5566 - MinusLogProbMetric: 27.5566 - val_loss: 28.0316 - val_MinusLogProbMetric: 28.0316 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 538/1000
2023-10-25 07:33:26.108 
Epoch 538/1000 
	 loss: 27.5572, MinusLogProbMetric: 27.5572, val_loss: 27.9766, val_MinusLogProbMetric: 27.9766

Epoch 538: val_loss did not improve from 27.91545
196/196 - 43s - loss: 27.5572 - MinusLogProbMetric: 27.5572 - val_loss: 27.9766 - val_MinusLogProbMetric: 27.9766 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 539/1000
2023-10-25 07:34:08.972 
Epoch 539/1000 
	 loss: 27.5461, MinusLogProbMetric: 27.5461, val_loss: 27.9576, val_MinusLogProbMetric: 27.9576

Epoch 539: val_loss did not improve from 27.91545
196/196 - 43s - loss: 27.5461 - MinusLogProbMetric: 27.5461 - val_loss: 27.9576 - val_MinusLogProbMetric: 27.9576 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 540/1000
2023-10-25 07:34:51.500 
Epoch 540/1000 
	 loss: 27.5525, MinusLogProbMetric: 27.5525, val_loss: 28.0220, val_MinusLogProbMetric: 28.0220

Epoch 540: val_loss did not improve from 27.91545
196/196 - 43s - loss: 27.5525 - MinusLogProbMetric: 27.5525 - val_loss: 28.0220 - val_MinusLogProbMetric: 28.0220 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 541/1000
2023-10-25 07:35:33.526 
Epoch 541/1000 
	 loss: 27.5714, MinusLogProbMetric: 27.5714, val_loss: 27.9400, val_MinusLogProbMetric: 27.9400

Epoch 541: val_loss did not improve from 27.91545
196/196 - 42s - loss: 27.5714 - MinusLogProbMetric: 27.5714 - val_loss: 27.9400 - val_MinusLogProbMetric: 27.9400 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 542/1000
2023-10-25 07:36:15.572 
Epoch 542/1000 
	 loss: 27.5564, MinusLogProbMetric: 27.5564, val_loss: 28.0391, val_MinusLogProbMetric: 28.0391

Epoch 542: val_loss did not improve from 27.91545
196/196 - 42s - loss: 27.5564 - MinusLogProbMetric: 27.5564 - val_loss: 28.0391 - val_MinusLogProbMetric: 28.0391 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 543/1000
2023-10-25 07:36:57.586 
Epoch 543/1000 
	 loss: 27.5450, MinusLogProbMetric: 27.5450, val_loss: 27.9892, val_MinusLogProbMetric: 27.9892

Epoch 543: val_loss did not improve from 27.91545
196/196 - 42s - loss: 27.5450 - MinusLogProbMetric: 27.5450 - val_loss: 27.9892 - val_MinusLogProbMetric: 27.9892 - lr: 8.3333e-05 - 42s/epoch - 214ms/step
Epoch 544/1000
2023-10-25 07:37:40.112 
Epoch 544/1000 
	 loss: 27.5302, MinusLogProbMetric: 27.5302, val_loss: 27.9799, val_MinusLogProbMetric: 27.9799

Epoch 544: val_loss did not improve from 27.91545
196/196 - 43s - loss: 27.5302 - MinusLogProbMetric: 27.5302 - val_loss: 27.9799 - val_MinusLogProbMetric: 27.9799 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 545/1000
2023-10-25 07:38:21.898 
Epoch 545/1000 
	 loss: 27.5585, MinusLogProbMetric: 27.5585, val_loss: 27.9422, val_MinusLogProbMetric: 27.9422

Epoch 545: val_loss did not improve from 27.91545
196/196 - 42s - loss: 27.5585 - MinusLogProbMetric: 27.5585 - val_loss: 27.9422 - val_MinusLogProbMetric: 27.9422 - lr: 8.3333e-05 - 42s/epoch - 213ms/step
Epoch 546/1000
2023-10-25 07:39:05.019 
Epoch 546/1000 
	 loss: 27.5442, MinusLogProbMetric: 27.5442, val_loss: 27.9937, val_MinusLogProbMetric: 27.9937

Epoch 546: val_loss did not improve from 27.91545
196/196 - 43s - loss: 27.5442 - MinusLogProbMetric: 27.5442 - val_loss: 27.9937 - val_MinusLogProbMetric: 27.9937 - lr: 8.3333e-05 - 43s/epoch - 220ms/step
Epoch 547/1000
2023-10-25 07:39:46.546 
Epoch 547/1000 
	 loss: 27.5319, MinusLogProbMetric: 27.5319, val_loss: 27.9408, val_MinusLogProbMetric: 27.9408

Epoch 547: val_loss did not improve from 27.91545
196/196 - 42s - loss: 27.5319 - MinusLogProbMetric: 27.5319 - val_loss: 27.9408 - val_MinusLogProbMetric: 27.9408 - lr: 8.3333e-05 - 42s/epoch - 212ms/step
Epoch 548/1000
2023-10-25 07:40:29.013 
Epoch 548/1000 
	 loss: 27.5460, MinusLogProbMetric: 27.5460, val_loss: 27.9174, val_MinusLogProbMetric: 27.9174

Epoch 548: val_loss did not improve from 27.91545
196/196 - 42s - loss: 27.5460 - MinusLogProbMetric: 27.5460 - val_loss: 27.9174 - val_MinusLogProbMetric: 27.9174 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 549/1000
2023-10-25 07:41:11.437 
Epoch 549/1000 
	 loss: 27.5508, MinusLogProbMetric: 27.5508, val_loss: 27.9434, val_MinusLogProbMetric: 27.9434

Epoch 549: val_loss did not improve from 27.91545
196/196 - 42s - loss: 27.5508 - MinusLogProbMetric: 27.5508 - val_loss: 27.9434 - val_MinusLogProbMetric: 27.9434 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 550/1000
2023-10-25 07:41:54.369 
Epoch 550/1000 
	 loss: 27.5308, MinusLogProbMetric: 27.5308, val_loss: 27.9360, val_MinusLogProbMetric: 27.9360

Epoch 550: val_loss did not improve from 27.91545
196/196 - 43s - loss: 27.5308 - MinusLogProbMetric: 27.5308 - val_loss: 27.9360 - val_MinusLogProbMetric: 27.9360 - lr: 8.3333e-05 - 43s/epoch - 219ms/step
Epoch 551/1000
2023-10-25 07:42:37.009 
Epoch 551/1000 
	 loss: 27.5516, MinusLogProbMetric: 27.5516, val_loss: 28.1681, val_MinusLogProbMetric: 28.1681

Epoch 551: val_loss did not improve from 27.91545
196/196 - 43s - loss: 27.5516 - MinusLogProbMetric: 27.5516 - val_loss: 28.1681 - val_MinusLogProbMetric: 28.1681 - lr: 8.3333e-05 - 43s/epoch - 218ms/step
Epoch 552/1000
2023-10-25 07:43:18.567 
Epoch 552/1000 
	 loss: 27.5878, MinusLogProbMetric: 27.5878, val_loss: 28.0368, val_MinusLogProbMetric: 28.0368

Epoch 552: val_loss did not improve from 27.91545
196/196 - 42s - loss: 27.5878 - MinusLogProbMetric: 27.5878 - val_loss: 28.0368 - val_MinusLogProbMetric: 28.0368 - lr: 8.3333e-05 - 42s/epoch - 212ms/step
Epoch 553/1000
2023-10-25 07:44:00.970 
Epoch 553/1000 
	 loss: 27.5647, MinusLogProbMetric: 27.5647, val_loss: 28.2165, val_MinusLogProbMetric: 28.2165

Epoch 553: val_loss did not improve from 27.91545
196/196 - 42s - loss: 27.5647 - MinusLogProbMetric: 27.5647 - val_loss: 28.2165 - val_MinusLogProbMetric: 28.2165 - lr: 8.3333e-05 - 42s/epoch - 216ms/step
Epoch 554/1000
2023-10-25 07:44:40.895 
Epoch 554/1000 
	 loss: 27.5663, MinusLogProbMetric: 27.5663, val_loss: 27.9191, val_MinusLogProbMetric: 27.9191

Epoch 554: val_loss did not improve from 27.91545
196/196 - 40s - loss: 27.5663 - MinusLogProbMetric: 27.5663 - val_loss: 27.9191 - val_MinusLogProbMetric: 27.9191 - lr: 8.3333e-05 - 40s/epoch - 204ms/step
Epoch 555/1000
2023-10-25 07:45:21.527 
Epoch 555/1000 
	 loss: 27.5410, MinusLogProbMetric: 27.5410, val_loss: 27.8738, val_MinusLogProbMetric: 27.8738

Epoch 555: val_loss improved from 27.91545 to 27.87383, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 41s - loss: 27.5410 - MinusLogProbMetric: 27.5410 - val_loss: 27.8738 - val_MinusLogProbMetric: 27.8738 - lr: 8.3333e-05 - 41s/epoch - 211ms/step
Epoch 556/1000
2023-10-25 07:46:03.859 
Epoch 556/1000 
	 loss: 27.5372, MinusLogProbMetric: 27.5372, val_loss: 27.9279, val_MinusLogProbMetric: 27.9279

Epoch 556: val_loss did not improve from 27.87383
196/196 - 42s - loss: 27.5372 - MinusLogProbMetric: 27.5372 - val_loss: 27.9279 - val_MinusLogProbMetric: 27.9279 - lr: 8.3333e-05 - 42s/epoch - 213ms/step
Epoch 557/1000
2023-10-25 07:46:46.314 
Epoch 557/1000 
	 loss: 27.5250, MinusLogProbMetric: 27.5250, val_loss: 27.9598, val_MinusLogProbMetric: 27.9598

Epoch 557: val_loss did not improve from 27.87383
196/196 - 42s - loss: 27.5250 - MinusLogProbMetric: 27.5250 - val_loss: 27.9598 - val_MinusLogProbMetric: 27.9598 - lr: 8.3333e-05 - 42s/epoch - 217ms/step
Epoch 558/1000
2023-10-25 07:47:28.429 
Epoch 558/1000 
	 loss: 27.5509, MinusLogProbMetric: 27.5509, val_loss: 28.0690, val_MinusLogProbMetric: 28.0690

Epoch 558: val_loss did not improve from 27.87383
196/196 - 42s - loss: 27.5509 - MinusLogProbMetric: 27.5509 - val_loss: 28.0690 - val_MinusLogProbMetric: 28.0690 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 559/1000
2023-10-25 07:48:10.565 
Epoch 559/1000 
	 loss: 27.5249, MinusLogProbMetric: 27.5249, val_loss: 27.9255, val_MinusLogProbMetric: 27.9255

Epoch 559: val_loss did not improve from 27.87383
196/196 - 42s - loss: 27.5249 - MinusLogProbMetric: 27.5249 - val_loss: 27.9255 - val_MinusLogProbMetric: 27.9255 - lr: 8.3333e-05 - 42s/epoch - 215ms/step
Epoch 560/1000
2023-10-25 07:48:52.344 
Epoch 560/1000 
	 loss: 27.5326, MinusLogProbMetric: 27.5326, val_loss: 27.9353, val_MinusLogProbMetric: 27.9353

Epoch 560: val_loss did not improve from 27.87383
196/196 - 42s - loss: 27.5326 - MinusLogProbMetric: 27.5326 - val_loss: 27.9353 - val_MinusLogProbMetric: 27.9353 - lr: 8.3333e-05 - 42s/epoch - 213ms/step
Epoch 561/1000
2023-10-25 07:49:34.891 
Epoch 561/1000 
	 loss: 27.5394, MinusLogProbMetric: 27.5394, val_loss: 28.0465, val_MinusLogProbMetric: 28.0465

Epoch 561: val_loss did not improve from 27.87383
196/196 - 43s - loss: 27.5394 - MinusLogProbMetric: 27.5394 - val_loss: 28.0465 - val_MinusLogProbMetric: 28.0465 - lr: 8.3333e-05 - 43s/epoch - 217ms/step
Epoch 562/1000
2023-10-25 07:50:12.035 
Epoch 562/1000 
	 loss: 27.5744, MinusLogProbMetric: 27.5744, val_loss: 27.9566, val_MinusLogProbMetric: 27.9566

Epoch 562: val_loss did not improve from 27.87383
196/196 - 37s - loss: 27.5744 - MinusLogProbMetric: 27.5744 - val_loss: 27.9566 - val_MinusLogProbMetric: 27.9566 - lr: 8.3333e-05 - 37s/epoch - 189ms/step
Epoch 563/1000
2023-10-25 07:50:43.262 
Epoch 563/1000 
	 loss: 27.5265, MinusLogProbMetric: 27.5265, val_loss: 27.9777, val_MinusLogProbMetric: 27.9777

Epoch 563: val_loss did not improve from 27.87383
196/196 - 31s - loss: 27.5265 - MinusLogProbMetric: 27.5265 - val_loss: 27.9777 - val_MinusLogProbMetric: 27.9777 - lr: 8.3333e-05 - 31s/epoch - 159ms/step
Epoch 564/1000
2023-10-25 07:51:14.162 
Epoch 564/1000 
	 loss: 27.5379, MinusLogProbMetric: 27.5379, val_loss: 28.1256, val_MinusLogProbMetric: 28.1256

Epoch 564: val_loss did not improve from 27.87383
196/196 - 31s - loss: 27.5379 - MinusLogProbMetric: 27.5379 - val_loss: 28.1256 - val_MinusLogProbMetric: 28.1256 - lr: 8.3333e-05 - 31s/epoch - 158ms/step
Epoch 565/1000
2023-10-25 07:51:51.554 
Epoch 565/1000 
	 loss: 27.5617, MinusLogProbMetric: 27.5617, val_loss: 28.2019, val_MinusLogProbMetric: 28.2019

Epoch 565: val_loss did not improve from 27.87383
196/196 - 37s - loss: 27.5617 - MinusLogProbMetric: 27.5617 - val_loss: 28.2019 - val_MinusLogProbMetric: 28.2019 - lr: 8.3333e-05 - 37s/epoch - 191ms/step
Epoch 566/1000
2023-10-25 07:52:26.206 
Epoch 566/1000 
	 loss: 27.5481, MinusLogProbMetric: 27.5481, val_loss: 27.9533, val_MinusLogProbMetric: 27.9533

Epoch 566: val_loss did not improve from 27.87383
196/196 - 35s - loss: 27.5481 - MinusLogProbMetric: 27.5481 - val_loss: 27.9533 - val_MinusLogProbMetric: 27.9533 - lr: 8.3333e-05 - 35s/epoch - 177ms/step
Epoch 567/1000
2023-10-25 07:52:56.881 
Epoch 567/1000 
	 loss: 27.5465, MinusLogProbMetric: 27.5465, val_loss: 27.9827, val_MinusLogProbMetric: 27.9827

Epoch 567: val_loss did not improve from 27.87383
196/196 - 31s - loss: 27.5465 - MinusLogProbMetric: 27.5465 - val_loss: 27.9827 - val_MinusLogProbMetric: 27.9827 - lr: 8.3333e-05 - 31s/epoch - 156ms/step
Epoch 568/1000
2023-10-25 07:53:28.871 
Epoch 568/1000 
	 loss: 27.5280, MinusLogProbMetric: 27.5280, val_loss: 27.9300, val_MinusLogProbMetric: 27.9300

Epoch 568: val_loss did not improve from 27.87383
196/196 - 32s - loss: 27.5280 - MinusLogProbMetric: 27.5280 - val_loss: 27.9300 - val_MinusLogProbMetric: 27.9300 - lr: 8.3333e-05 - 32s/epoch - 163ms/step
Epoch 569/1000
2023-10-25 07:54:05.582 
Epoch 569/1000 
	 loss: 27.5385, MinusLogProbMetric: 27.5385, val_loss: 27.9741, val_MinusLogProbMetric: 27.9741

Epoch 569: val_loss did not improve from 27.87383
196/196 - 37s - loss: 27.5385 - MinusLogProbMetric: 27.5385 - val_loss: 27.9741 - val_MinusLogProbMetric: 27.9741 - lr: 8.3333e-05 - 37s/epoch - 187ms/step
Epoch 570/1000
2023-10-25 07:54:38.567 
Epoch 570/1000 
	 loss: 27.5603, MinusLogProbMetric: 27.5603, val_loss: 28.5039, val_MinusLogProbMetric: 28.5039

Epoch 570: val_loss did not improve from 27.87383
196/196 - 33s - loss: 27.5603 - MinusLogProbMetric: 27.5603 - val_loss: 28.5039 - val_MinusLogProbMetric: 28.5039 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 571/1000
2023-10-25 07:55:11.487 
Epoch 571/1000 
	 loss: 27.5509, MinusLogProbMetric: 27.5509, val_loss: 27.9026, val_MinusLogProbMetric: 27.9026

Epoch 571: val_loss did not improve from 27.87383
196/196 - 33s - loss: 27.5509 - MinusLogProbMetric: 27.5509 - val_loss: 27.9026 - val_MinusLogProbMetric: 27.9026 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 572/1000
2023-10-25 07:55:43.022 
Epoch 572/1000 
	 loss: 27.5266, MinusLogProbMetric: 27.5266, val_loss: 27.9063, val_MinusLogProbMetric: 27.9063

Epoch 572: val_loss did not improve from 27.87383
196/196 - 32s - loss: 27.5266 - MinusLogProbMetric: 27.5266 - val_loss: 27.9063 - val_MinusLogProbMetric: 27.9063 - lr: 8.3333e-05 - 32s/epoch - 161ms/step
Epoch 573/1000
2023-10-25 07:56:16.659 
Epoch 573/1000 
	 loss: 27.5137, MinusLogProbMetric: 27.5137, val_loss: 27.9246, val_MinusLogProbMetric: 27.9246

Epoch 573: val_loss did not improve from 27.87383
196/196 - 34s - loss: 27.5137 - MinusLogProbMetric: 27.5137 - val_loss: 27.9246 - val_MinusLogProbMetric: 27.9246 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 574/1000
2023-10-25 07:56:55.236 
Epoch 574/1000 
	 loss: 27.5468, MinusLogProbMetric: 27.5468, val_loss: 27.9071, val_MinusLogProbMetric: 27.9071

Epoch 574: val_loss did not improve from 27.87383
196/196 - 39s - loss: 27.5468 - MinusLogProbMetric: 27.5468 - val_loss: 27.9071 - val_MinusLogProbMetric: 27.9071 - lr: 8.3333e-05 - 39s/epoch - 197ms/step
Epoch 575/1000
2023-10-25 07:57:30.091 
Epoch 575/1000 
	 loss: 27.5581, MinusLogProbMetric: 27.5581, val_loss: 28.0240, val_MinusLogProbMetric: 28.0240

Epoch 575: val_loss did not improve from 27.87383
196/196 - 35s - loss: 27.5581 - MinusLogProbMetric: 27.5581 - val_loss: 28.0240 - val_MinusLogProbMetric: 28.0240 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 576/1000
2023-10-25 07:58:03.114 
Epoch 576/1000 
	 loss: 27.5510, MinusLogProbMetric: 27.5510, val_loss: 28.0539, val_MinusLogProbMetric: 28.0539

Epoch 576: val_loss did not improve from 27.87383
196/196 - 33s - loss: 27.5510 - MinusLogProbMetric: 27.5510 - val_loss: 28.0539 - val_MinusLogProbMetric: 28.0539 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 577/1000
2023-10-25 07:58:36.429 
Epoch 577/1000 
	 loss: 27.5353, MinusLogProbMetric: 27.5353, val_loss: 27.9257, val_MinusLogProbMetric: 27.9257

Epoch 577: val_loss did not improve from 27.87383
196/196 - 33s - loss: 27.5353 - MinusLogProbMetric: 27.5353 - val_loss: 27.9257 - val_MinusLogProbMetric: 27.9257 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 578/1000
2023-10-25 07:59:16.959 
Epoch 578/1000 
	 loss: 27.5523, MinusLogProbMetric: 27.5523, val_loss: 27.9638, val_MinusLogProbMetric: 27.9638

Epoch 578: val_loss did not improve from 27.87383
196/196 - 41s - loss: 27.5523 - MinusLogProbMetric: 27.5523 - val_loss: 27.9638 - val_MinusLogProbMetric: 27.9638 - lr: 8.3333e-05 - 41s/epoch - 207ms/step
Epoch 579/1000
2023-10-25 07:59:49.668 
Epoch 579/1000 
	 loss: 27.5595, MinusLogProbMetric: 27.5595, val_loss: 27.9883, val_MinusLogProbMetric: 27.9883

Epoch 579: val_loss did not improve from 27.87383
196/196 - 33s - loss: 27.5595 - MinusLogProbMetric: 27.5595 - val_loss: 27.9883 - val_MinusLogProbMetric: 27.9883 - lr: 8.3333e-05 - 33s/epoch - 167ms/step
Epoch 580/1000
2023-10-25 08:00:20.561 
Epoch 580/1000 
	 loss: 27.5213, MinusLogProbMetric: 27.5213, val_loss: 28.0978, val_MinusLogProbMetric: 28.0978

Epoch 580: val_loss did not improve from 27.87383
196/196 - 31s - loss: 27.5213 - MinusLogProbMetric: 27.5213 - val_loss: 28.0978 - val_MinusLogProbMetric: 28.0978 - lr: 8.3333e-05 - 31s/epoch - 158ms/step
Epoch 581/1000
2023-10-25 08:00:53.269 
Epoch 581/1000 
	 loss: 27.5508, MinusLogProbMetric: 27.5508, val_loss: 27.9857, val_MinusLogProbMetric: 27.9857

Epoch 581: val_loss did not improve from 27.87383
196/196 - 33s - loss: 27.5508 - MinusLogProbMetric: 27.5508 - val_loss: 27.9857 - val_MinusLogProbMetric: 27.9857 - lr: 8.3333e-05 - 33s/epoch - 167ms/step
Epoch 582/1000
2023-10-25 08:01:27.385 
Epoch 582/1000 
	 loss: 27.5363, MinusLogProbMetric: 27.5363, val_loss: 27.9024, val_MinusLogProbMetric: 27.9024

Epoch 582: val_loss did not improve from 27.87383
196/196 - 34s - loss: 27.5363 - MinusLogProbMetric: 27.5363 - val_loss: 27.9024 - val_MinusLogProbMetric: 27.9024 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 583/1000
2023-10-25 08:02:00.355 
Epoch 583/1000 
	 loss: 27.5322, MinusLogProbMetric: 27.5322, val_loss: 27.9737, val_MinusLogProbMetric: 27.9737

Epoch 583: val_loss did not improve from 27.87383
196/196 - 33s - loss: 27.5322 - MinusLogProbMetric: 27.5322 - val_loss: 27.9737 - val_MinusLogProbMetric: 27.9737 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 584/1000
2023-10-25 08:02:31.570 
Epoch 584/1000 
	 loss: 27.5349, MinusLogProbMetric: 27.5349, val_loss: 28.0954, val_MinusLogProbMetric: 28.0954

Epoch 584: val_loss did not improve from 27.87383
196/196 - 31s - loss: 27.5349 - MinusLogProbMetric: 27.5349 - val_loss: 28.0954 - val_MinusLogProbMetric: 28.0954 - lr: 8.3333e-05 - 31s/epoch - 159ms/step
Epoch 585/1000
2023-10-25 08:03:03.205 
Epoch 585/1000 
	 loss: 27.5400, MinusLogProbMetric: 27.5400, val_loss: 27.9564, val_MinusLogProbMetric: 27.9564

Epoch 585: val_loss did not improve from 27.87383
196/196 - 32s - loss: 27.5400 - MinusLogProbMetric: 27.5400 - val_loss: 27.9564 - val_MinusLogProbMetric: 27.9564 - lr: 8.3333e-05 - 32s/epoch - 161ms/step
Epoch 586/1000
2023-10-25 08:03:40.551 
Epoch 586/1000 
	 loss: 27.5516, MinusLogProbMetric: 27.5516, val_loss: 27.9349, val_MinusLogProbMetric: 27.9349

Epoch 586: val_loss did not improve from 27.87383
196/196 - 37s - loss: 27.5516 - MinusLogProbMetric: 27.5516 - val_loss: 27.9349 - val_MinusLogProbMetric: 27.9349 - lr: 8.3333e-05 - 37s/epoch - 191ms/step
Epoch 587/1000
2023-10-25 08:04:16.122 
Epoch 587/1000 
	 loss: 27.5155, MinusLogProbMetric: 27.5155, val_loss: 27.9830, val_MinusLogProbMetric: 27.9830

Epoch 587: val_loss did not improve from 27.87383
196/196 - 36s - loss: 27.5155 - MinusLogProbMetric: 27.5155 - val_loss: 27.9830 - val_MinusLogProbMetric: 27.9830 - lr: 8.3333e-05 - 36s/epoch - 181ms/step
Epoch 588/1000
2023-10-25 08:04:47.629 
Epoch 588/1000 
	 loss: 27.5338, MinusLogProbMetric: 27.5338, val_loss: 27.9404, val_MinusLogProbMetric: 27.9404

Epoch 588: val_loss did not improve from 27.87383
196/196 - 32s - loss: 27.5338 - MinusLogProbMetric: 27.5338 - val_loss: 27.9404 - val_MinusLogProbMetric: 27.9404 - lr: 8.3333e-05 - 32s/epoch - 161ms/step
Epoch 589/1000
2023-10-25 08:05:19.529 
Epoch 589/1000 
	 loss: 27.5241, MinusLogProbMetric: 27.5241, val_loss: 27.9234, val_MinusLogProbMetric: 27.9234

Epoch 589: val_loss did not improve from 27.87383
196/196 - 32s - loss: 27.5241 - MinusLogProbMetric: 27.5241 - val_loss: 27.9234 - val_MinusLogProbMetric: 27.9234 - lr: 8.3333e-05 - 32s/epoch - 163ms/step
Epoch 590/1000
2023-10-25 08:05:51.635 
Epoch 590/1000 
	 loss: 27.5316, MinusLogProbMetric: 27.5316, val_loss: 27.9749, val_MinusLogProbMetric: 27.9749

Epoch 590: val_loss did not improve from 27.87383
196/196 - 32s - loss: 27.5316 - MinusLogProbMetric: 27.5316 - val_loss: 27.9749 - val_MinusLogProbMetric: 27.9749 - lr: 8.3333e-05 - 32s/epoch - 164ms/step
Epoch 591/1000
2023-10-25 08:06:24.994 
Epoch 591/1000 
	 loss: 27.5506, MinusLogProbMetric: 27.5506, val_loss: 28.0019, val_MinusLogProbMetric: 28.0019

Epoch 591: val_loss did not improve from 27.87383
196/196 - 33s - loss: 27.5506 - MinusLogProbMetric: 27.5506 - val_loss: 28.0019 - val_MinusLogProbMetric: 28.0019 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 592/1000
2023-10-25 08:07:00.010 
Epoch 592/1000 
	 loss: 27.5526, MinusLogProbMetric: 27.5526, val_loss: 28.0683, val_MinusLogProbMetric: 28.0683

Epoch 592: val_loss did not improve from 27.87383
196/196 - 35s - loss: 27.5526 - MinusLogProbMetric: 27.5526 - val_loss: 28.0683 - val_MinusLogProbMetric: 28.0683 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 593/1000
2023-10-25 08:07:35.704 
Epoch 593/1000 
	 loss: 27.5340, MinusLogProbMetric: 27.5340, val_loss: 27.9161, val_MinusLogProbMetric: 27.9161

Epoch 593: val_loss did not improve from 27.87383
196/196 - 36s - loss: 27.5340 - MinusLogProbMetric: 27.5340 - val_loss: 27.9161 - val_MinusLogProbMetric: 27.9161 - lr: 8.3333e-05 - 36s/epoch - 182ms/step
Epoch 594/1000
2023-10-25 08:08:16.302 
Epoch 594/1000 
	 loss: 27.5295, MinusLogProbMetric: 27.5295, val_loss: 27.9415, val_MinusLogProbMetric: 27.9415

Epoch 594: val_loss did not improve from 27.87383
196/196 - 41s - loss: 27.5295 - MinusLogProbMetric: 27.5295 - val_loss: 27.9415 - val_MinusLogProbMetric: 27.9415 - lr: 8.3333e-05 - 41s/epoch - 207ms/step
Epoch 595/1000
2023-10-25 08:08:56.609 
Epoch 595/1000 
	 loss: 27.5569, MinusLogProbMetric: 27.5569, val_loss: 27.9487, val_MinusLogProbMetric: 27.9487

Epoch 595: val_loss did not improve from 27.87383
196/196 - 40s - loss: 27.5569 - MinusLogProbMetric: 27.5569 - val_loss: 27.9487 - val_MinusLogProbMetric: 27.9487 - lr: 8.3333e-05 - 40s/epoch - 206ms/step
Epoch 596/1000
2023-10-25 08:09:31.744 
Epoch 596/1000 
	 loss: 27.5342, MinusLogProbMetric: 27.5342, val_loss: 27.9576, val_MinusLogProbMetric: 27.9576

Epoch 596: val_loss did not improve from 27.87383
196/196 - 35s - loss: 27.5342 - MinusLogProbMetric: 27.5342 - val_loss: 27.9576 - val_MinusLogProbMetric: 27.9576 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 597/1000
2023-10-25 08:10:06.551 
Epoch 597/1000 
	 loss: 27.5255, MinusLogProbMetric: 27.5255, val_loss: 27.9103, val_MinusLogProbMetric: 27.9103

Epoch 597: val_loss did not improve from 27.87383
196/196 - 35s - loss: 27.5255 - MinusLogProbMetric: 27.5255 - val_loss: 27.9103 - val_MinusLogProbMetric: 27.9103 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 598/1000
2023-10-25 08:10:41.854 
Epoch 598/1000 
	 loss: 27.5388, MinusLogProbMetric: 27.5388, val_loss: 27.9028, val_MinusLogProbMetric: 27.9028

Epoch 598: val_loss did not improve from 27.87383
196/196 - 35s - loss: 27.5388 - MinusLogProbMetric: 27.5388 - val_loss: 27.9028 - val_MinusLogProbMetric: 27.9028 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 599/1000
2023-10-25 08:11:18.485 
Epoch 599/1000 
	 loss: 27.5558, MinusLogProbMetric: 27.5558, val_loss: 27.9462, val_MinusLogProbMetric: 27.9462

Epoch 599: val_loss did not improve from 27.87383
196/196 - 37s - loss: 27.5558 - MinusLogProbMetric: 27.5558 - val_loss: 27.9462 - val_MinusLogProbMetric: 27.9462 - lr: 8.3333e-05 - 37s/epoch - 187ms/step
Epoch 600/1000
2023-10-25 08:11:53.736 
Epoch 600/1000 
	 loss: 27.5214, MinusLogProbMetric: 27.5214, val_loss: 27.9657, val_MinusLogProbMetric: 27.9657

Epoch 600: val_loss did not improve from 27.87383
196/196 - 35s - loss: 27.5214 - MinusLogProbMetric: 27.5214 - val_loss: 27.9657 - val_MinusLogProbMetric: 27.9657 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 601/1000
2023-10-25 08:12:28.564 
Epoch 601/1000 
	 loss: 27.5401, MinusLogProbMetric: 27.5401, val_loss: 27.9498, val_MinusLogProbMetric: 27.9498

Epoch 601: val_loss did not improve from 27.87383
196/196 - 35s - loss: 27.5401 - MinusLogProbMetric: 27.5401 - val_loss: 27.9498 - val_MinusLogProbMetric: 27.9498 - lr: 8.3333e-05 - 35s/epoch - 178ms/step
Epoch 602/1000
2023-10-25 08:13:08.021 
Epoch 602/1000 
	 loss: 27.5059, MinusLogProbMetric: 27.5059, val_loss: 27.9610, val_MinusLogProbMetric: 27.9610

Epoch 602: val_loss did not improve from 27.87383
196/196 - 39s - loss: 27.5059 - MinusLogProbMetric: 27.5059 - val_loss: 27.9610 - val_MinusLogProbMetric: 27.9610 - lr: 8.3333e-05 - 39s/epoch - 201ms/step
Epoch 603/1000
2023-10-25 08:13:43.679 
Epoch 603/1000 
	 loss: 27.5443, MinusLogProbMetric: 27.5443, val_loss: 27.9338, val_MinusLogProbMetric: 27.9338

Epoch 603: val_loss did not improve from 27.87383
196/196 - 36s - loss: 27.5443 - MinusLogProbMetric: 27.5443 - val_loss: 27.9338 - val_MinusLogProbMetric: 27.9338 - lr: 8.3333e-05 - 36s/epoch - 182ms/step
Epoch 604/1000
2023-10-25 08:14:18.996 
Epoch 604/1000 
	 loss: 27.5215, MinusLogProbMetric: 27.5215, val_loss: 27.9075, val_MinusLogProbMetric: 27.9075

Epoch 604: val_loss did not improve from 27.87383
196/196 - 35s - loss: 27.5215 - MinusLogProbMetric: 27.5215 - val_loss: 27.9075 - val_MinusLogProbMetric: 27.9075 - lr: 8.3333e-05 - 35s/epoch - 180ms/step
Epoch 605/1000
2023-10-25 08:14:54.152 
Epoch 605/1000 
	 loss: 27.5297, MinusLogProbMetric: 27.5297, val_loss: 27.9484, val_MinusLogProbMetric: 27.9484

Epoch 605: val_loss did not improve from 27.87383
196/196 - 35s - loss: 27.5297 - MinusLogProbMetric: 27.5297 - val_loss: 27.9484 - val_MinusLogProbMetric: 27.9484 - lr: 8.3333e-05 - 35s/epoch - 179ms/step
Epoch 606/1000
2023-10-25 08:15:31.373 
Epoch 606/1000 
	 loss: 27.4494, MinusLogProbMetric: 27.4494, val_loss: 27.8639, val_MinusLogProbMetric: 27.8639

Epoch 606: val_loss improved from 27.87383 to 27.86391, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 38s - loss: 27.4494 - MinusLogProbMetric: 27.4494 - val_loss: 27.8639 - val_MinusLogProbMetric: 27.8639 - lr: 4.1667e-05 - 38s/epoch - 193ms/step
Epoch 607/1000
2023-10-25 08:16:08.188 
Epoch 607/1000 
	 loss: 27.4391, MinusLogProbMetric: 27.4391, val_loss: 27.8536, val_MinusLogProbMetric: 27.8536

Epoch 607: val_loss improved from 27.86391 to 27.85357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 37s - loss: 27.4391 - MinusLogProbMetric: 27.4391 - val_loss: 27.8536 - val_MinusLogProbMetric: 27.8536 - lr: 4.1667e-05 - 37s/epoch - 188ms/step
Epoch 608/1000
2023-10-25 08:16:43.498 
Epoch 608/1000 
	 loss: 27.4340, MinusLogProbMetric: 27.4340, val_loss: 27.9480, val_MinusLogProbMetric: 27.9480

Epoch 608: val_loss did not improve from 27.85357
196/196 - 35s - loss: 27.4340 - MinusLogProbMetric: 27.4340 - val_loss: 27.9480 - val_MinusLogProbMetric: 27.9480 - lr: 4.1667e-05 - 35s/epoch - 177ms/step
Epoch 609/1000
2023-10-25 08:17:19.658 
Epoch 609/1000 
	 loss: 27.4483, MinusLogProbMetric: 27.4483, val_loss: 27.9082, val_MinusLogProbMetric: 27.9082

Epoch 609: val_loss did not improve from 27.85357
196/196 - 36s - loss: 27.4483 - MinusLogProbMetric: 27.4483 - val_loss: 27.9082 - val_MinusLogProbMetric: 27.9082 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 610/1000
2023-10-25 08:17:58.764 
Epoch 610/1000 
	 loss: 27.4416, MinusLogProbMetric: 27.4416, val_loss: 27.9748, val_MinusLogProbMetric: 27.9748

Epoch 610: val_loss did not improve from 27.85357
196/196 - 39s - loss: 27.4416 - MinusLogProbMetric: 27.4416 - val_loss: 27.9748 - val_MinusLogProbMetric: 27.9748 - lr: 4.1667e-05 - 39s/epoch - 200ms/step
Epoch 611/1000
2023-10-25 08:18:36.341 
Epoch 611/1000 
	 loss: 27.4383, MinusLogProbMetric: 27.4383, val_loss: 27.8758, val_MinusLogProbMetric: 27.8758

Epoch 611: val_loss did not improve from 27.85357
196/196 - 38s - loss: 27.4383 - MinusLogProbMetric: 27.4383 - val_loss: 27.8758 - val_MinusLogProbMetric: 27.8758 - lr: 4.1667e-05 - 38s/epoch - 192ms/step
Epoch 612/1000
2023-10-25 08:19:11.535 
Epoch 612/1000 
	 loss: 27.4355, MinusLogProbMetric: 27.4355, val_loss: 27.9202, val_MinusLogProbMetric: 27.9202

Epoch 612: val_loss did not improve from 27.85357
196/196 - 35s - loss: 27.4355 - MinusLogProbMetric: 27.4355 - val_loss: 27.9202 - val_MinusLogProbMetric: 27.9202 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 613/1000
2023-10-25 08:19:47.873 
Epoch 613/1000 
	 loss: 27.4334, MinusLogProbMetric: 27.4334, val_loss: 27.8529, val_MinusLogProbMetric: 27.8529

Epoch 613: val_loss improved from 27.85357 to 27.85289, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 37s - loss: 27.4334 - MinusLogProbMetric: 27.4334 - val_loss: 27.8529 - val_MinusLogProbMetric: 27.8529 - lr: 4.1667e-05 - 37s/epoch - 189ms/step
Epoch 614/1000
2023-10-25 08:20:23.761 
Epoch 614/1000 
	 loss: 27.4347, MinusLogProbMetric: 27.4347, val_loss: 27.8720, val_MinusLogProbMetric: 27.8720

Epoch 614: val_loss did not improve from 27.85289
196/196 - 35s - loss: 27.4347 - MinusLogProbMetric: 27.4347 - val_loss: 27.8720 - val_MinusLogProbMetric: 27.8720 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 615/1000
2023-10-25 08:20:59.598 
Epoch 615/1000 
	 loss: 27.4377, MinusLogProbMetric: 27.4377, val_loss: 27.8861, val_MinusLogProbMetric: 27.8861

Epoch 615: val_loss did not improve from 27.85289
196/196 - 36s - loss: 27.4377 - MinusLogProbMetric: 27.4377 - val_loss: 27.8861 - val_MinusLogProbMetric: 27.8861 - lr: 4.1667e-05 - 36s/epoch - 183ms/step
Epoch 616/1000
2023-10-25 08:21:34.305 
Epoch 616/1000 
	 loss: 27.4365, MinusLogProbMetric: 27.4365, val_loss: 27.8678, val_MinusLogProbMetric: 27.8678

Epoch 616: val_loss did not improve from 27.85289
196/196 - 35s - loss: 27.4365 - MinusLogProbMetric: 27.4365 - val_loss: 27.8678 - val_MinusLogProbMetric: 27.8678 - lr: 4.1667e-05 - 35s/epoch - 177ms/step
Epoch 617/1000
2023-10-25 08:22:10.265 
Epoch 617/1000 
	 loss: 27.4314, MinusLogProbMetric: 27.4314, val_loss: 27.8476, val_MinusLogProbMetric: 27.8476

Epoch 617: val_loss improved from 27.85289 to 27.84758, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 37s - loss: 27.4314 - MinusLogProbMetric: 27.4314 - val_loss: 27.8476 - val_MinusLogProbMetric: 27.8476 - lr: 4.1667e-05 - 37s/epoch - 187ms/step
Epoch 618/1000
2023-10-25 08:22:52.072 
Epoch 618/1000 
	 loss: 27.4348, MinusLogProbMetric: 27.4348, val_loss: 27.8640, val_MinusLogProbMetric: 27.8640

Epoch 618: val_loss did not improve from 27.84758
196/196 - 41s - loss: 27.4348 - MinusLogProbMetric: 27.4348 - val_loss: 27.8640 - val_MinusLogProbMetric: 27.8640 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 619/1000
2023-10-25 08:23:33.727 
Epoch 619/1000 
	 loss: 27.4351, MinusLogProbMetric: 27.4351, val_loss: 27.8730, val_MinusLogProbMetric: 27.8730

Epoch 619: val_loss did not improve from 27.84758
196/196 - 42s - loss: 27.4351 - MinusLogProbMetric: 27.4351 - val_loss: 27.8730 - val_MinusLogProbMetric: 27.8730 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 620/1000
2023-10-25 08:24:13.799 
Epoch 620/1000 
	 loss: 27.4429, MinusLogProbMetric: 27.4429, val_loss: 27.9024, val_MinusLogProbMetric: 27.9024

Epoch 620: val_loss did not improve from 27.84758
196/196 - 40s - loss: 27.4429 - MinusLogProbMetric: 27.4429 - val_loss: 27.9024 - val_MinusLogProbMetric: 27.9024 - lr: 4.1667e-05 - 40s/epoch - 204ms/step
Epoch 621/1000
2023-10-25 08:24:55.344 
Epoch 621/1000 
	 loss: 27.4352, MinusLogProbMetric: 27.4352, val_loss: 27.8793, val_MinusLogProbMetric: 27.8793

Epoch 621: val_loss did not improve from 27.84758
196/196 - 42s - loss: 27.4352 - MinusLogProbMetric: 27.4352 - val_loss: 27.8793 - val_MinusLogProbMetric: 27.8793 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 622/1000
2023-10-25 08:25:37.656 
Epoch 622/1000 
	 loss: 27.4440, MinusLogProbMetric: 27.4440, val_loss: 27.8615, val_MinusLogProbMetric: 27.8615

Epoch 622: val_loss did not improve from 27.84758
196/196 - 42s - loss: 27.4440 - MinusLogProbMetric: 27.4440 - val_loss: 27.8615 - val_MinusLogProbMetric: 27.8615 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 623/1000
2023-10-25 08:26:19.918 
Epoch 623/1000 
	 loss: 27.4341, MinusLogProbMetric: 27.4341, val_loss: 27.8767, val_MinusLogProbMetric: 27.8767

Epoch 623: val_loss did not improve from 27.84758
196/196 - 42s - loss: 27.4341 - MinusLogProbMetric: 27.4341 - val_loss: 27.8767 - val_MinusLogProbMetric: 27.8767 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 624/1000
2023-10-25 08:27:02.009 
Epoch 624/1000 
	 loss: 27.4480, MinusLogProbMetric: 27.4480, val_loss: 27.9254, val_MinusLogProbMetric: 27.9254

Epoch 624: val_loss did not improve from 27.84758
196/196 - 42s - loss: 27.4480 - MinusLogProbMetric: 27.4480 - val_loss: 27.9254 - val_MinusLogProbMetric: 27.9254 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 625/1000
2023-10-25 08:27:44.261 
Epoch 625/1000 
	 loss: 27.4414, MinusLogProbMetric: 27.4414, val_loss: 27.8689, val_MinusLogProbMetric: 27.8689

Epoch 625: val_loss did not improve from 27.84758
196/196 - 42s - loss: 27.4414 - MinusLogProbMetric: 27.4414 - val_loss: 27.8689 - val_MinusLogProbMetric: 27.8689 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 626/1000
2023-10-25 08:28:26.321 
Epoch 626/1000 
	 loss: 27.4295, MinusLogProbMetric: 27.4295, val_loss: 27.8737, val_MinusLogProbMetric: 27.8737

Epoch 626: val_loss did not improve from 27.84758
196/196 - 42s - loss: 27.4295 - MinusLogProbMetric: 27.4295 - val_loss: 27.8737 - val_MinusLogProbMetric: 27.8737 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 627/1000
2023-10-25 08:29:08.250 
Epoch 627/1000 
	 loss: 27.4550, MinusLogProbMetric: 27.4550, val_loss: 27.8507, val_MinusLogProbMetric: 27.8507

Epoch 627: val_loss did not improve from 27.84758
196/196 - 42s - loss: 27.4550 - MinusLogProbMetric: 27.4550 - val_loss: 27.8507 - val_MinusLogProbMetric: 27.8507 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 628/1000
2023-10-25 08:29:50.951 
Epoch 628/1000 
	 loss: 27.4355, MinusLogProbMetric: 27.4355, val_loss: 27.9061, val_MinusLogProbMetric: 27.9061

Epoch 628: val_loss did not improve from 27.84758
196/196 - 43s - loss: 27.4355 - MinusLogProbMetric: 27.4355 - val_loss: 27.9061 - val_MinusLogProbMetric: 27.9061 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 629/1000
2023-10-25 08:30:32.646 
Epoch 629/1000 
	 loss: 27.4455, MinusLogProbMetric: 27.4455, val_loss: 27.8880, val_MinusLogProbMetric: 27.8880

Epoch 629: val_loss did not improve from 27.84758
196/196 - 42s - loss: 27.4455 - MinusLogProbMetric: 27.4455 - val_loss: 27.8880 - val_MinusLogProbMetric: 27.8880 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 630/1000
2023-10-25 08:31:14.258 
Epoch 630/1000 
	 loss: 27.4365, MinusLogProbMetric: 27.4365, val_loss: 27.9011, val_MinusLogProbMetric: 27.9011

Epoch 630: val_loss did not improve from 27.84758
196/196 - 42s - loss: 27.4365 - MinusLogProbMetric: 27.4365 - val_loss: 27.9011 - val_MinusLogProbMetric: 27.9011 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 631/1000
2023-10-25 08:31:56.522 
Epoch 631/1000 
	 loss: 27.4499, MinusLogProbMetric: 27.4499, val_loss: 27.9002, val_MinusLogProbMetric: 27.9002

Epoch 631: val_loss did not improve from 27.84758
196/196 - 42s - loss: 27.4499 - MinusLogProbMetric: 27.4499 - val_loss: 27.9002 - val_MinusLogProbMetric: 27.9002 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 632/1000
2023-10-25 08:32:38.464 
Epoch 632/1000 
	 loss: 27.4391, MinusLogProbMetric: 27.4391, val_loss: 27.8661, val_MinusLogProbMetric: 27.8661

Epoch 632: val_loss did not improve from 27.84758
196/196 - 42s - loss: 27.4391 - MinusLogProbMetric: 27.4391 - val_loss: 27.8661 - val_MinusLogProbMetric: 27.8661 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 633/1000
2023-10-25 08:33:21.241 
Epoch 633/1000 
	 loss: 27.4363, MinusLogProbMetric: 27.4363, val_loss: 27.9195, val_MinusLogProbMetric: 27.9195

Epoch 633: val_loss did not improve from 27.84758
196/196 - 43s - loss: 27.4363 - MinusLogProbMetric: 27.4363 - val_loss: 27.9195 - val_MinusLogProbMetric: 27.9195 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 634/1000
2023-10-25 08:34:03.554 
Epoch 634/1000 
	 loss: 27.4390, MinusLogProbMetric: 27.4390, val_loss: 27.8933, val_MinusLogProbMetric: 27.8933

Epoch 634: val_loss did not improve from 27.84758
196/196 - 42s - loss: 27.4390 - MinusLogProbMetric: 27.4390 - val_loss: 27.8933 - val_MinusLogProbMetric: 27.8933 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 635/1000
2023-10-25 08:34:45.608 
Epoch 635/1000 
	 loss: 27.4407, MinusLogProbMetric: 27.4407, val_loss: 27.9285, val_MinusLogProbMetric: 27.9285

Epoch 635: val_loss did not improve from 27.84758
196/196 - 42s - loss: 27.4407 - MinusLogProbMetric: 27.4407 - val_loss: 27.9285 - val_MinusLogProbMetric: 27.9285 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 636/1000
2023-10-25 08:35:27.379 
Epoch 636/1000 
	 loss: 27.4317, MinusLogProbMetric: 27.4317, val_loss: 27.8789, val_MinusLogProbMetric: 27.8789

Epoch 636: val_loss did not improve from 27.84758
196/196 - 42s - loss: 27.4317 - MinusLogProbMetric: 27.4317 - val_loss: 27.8789 - val_MinusLogProbMetric: 27.8789 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 637/1000
2023-10-25 08:36:09.215 
Epoch 637/1000 
	 loss: 27.4266, MinusLogProbMetric: 27.4266, val_loss: 27.8836, val_MinusLogProbMetric: 27.8836

Epoch 637: val_loss did not improve from 27.84758
196/196 - 42s - loss: 27.4266 - MinusLogProbMetric: 27.4266 - val_loss: 27.8836 - val_MinusLogProbMetric: 27.8836 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 638/1000
2023-10-25 08:36:51.423 
Epoch 638/1000 
	 loss: 27.4283, MinusLogProbMetric: 27.4283, val_loss: 27.9044, val_MinusLogProbMetric: 27.9044

Epoch 638: val_loss did not improve from 27.84758
196/196 - 42s - loss: 27.4283 - MinusLogProbMetric: 27.4283 - val_loss: 27.9044 - val_MinusLogProbMetric: 27.9044 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 639/1000
2023-10-25 08:37:29.774 
Epoch 639/1000 
	 loss: 27.4341, MinusLogProbMetric: 27.4341, val_loss: 27.8759, val_MinusLogProbMetric: 27.8759

Epoch 639: val_loss did not improve from 27.84758
196/196 - 38s - loss: 27.4341 - MinusLogProbMetric: 27.4341 - val_loss: 27.8759 - val_MinusLogProbMetric: 27.8759 - lr: 4.1667e-05 - 38s/epoch - 196ms/step
Epoch 640/1000
2023-10-25 08:38:06.186 
Epoch 640/1000 
	 loss: 27.4395, MinusLogProbMetric: 27.4395, val_loss: 27.8918, val_MinusLogProbMetric: 27.8918

Epoch 640: val_loss did not improve from 27.84758
196/196 - 36s - loss: 27.4395 - MinusLogProbMetric: 27.4395 - val_loss: 27.8918 - val_MinusLogProbMetric: 27.8918 - lr: 4.1667e-05 - 36s/epoch - 186ms/step
Epoch 641/1000
2023-10-25 08:38:42.193 
Epoch 641/1000 
	 loss: 27.4289, MinusLogProbMetric: 27.4289, val_loss: 27.8781, val_MinusLogProbMetric: 27.8781

Epoch 641: val_loss did not improve from 27.84758
196/196 - 36s - loss: 27.4289 - MinusLogProbMetric: 27.4289 - val_loss: 27.8781 - val_MinusLogProbMetric: 27.8781 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 642/1000
2023-10-25 08:39:20.688 
Epoch 642/1000 
	 loss: 27.4354, MinusLogProbMetric: 27.4354, val_loss: 28.0952, val_MinusLogProbMetric: 28.0952

Epoch 642: val_loss did not improve from 27.84758
196/196 - 38s - loss: 27.4354 - MinusLogProbMetric: 27.4354 - val_loss: 28.0952 - val_MinusLogProbMetric: 28.0952 - lr: 4.1667e-05 - 38s/epoch - 196ms/step
Epoch 643/1000
2023-10-25 08:40:00.989 
Epoch 643/1000 
	 loss: 27.4285, MinusLogProbMetric: 27.4285, val_loss: 27.8791, val_MinusLogProbMetric: 27.8791

Epoch 643: val_loss did not improve from 27.84758
196/196 - 40s - loss: 27.4285 - MinusLogProbMetric: 27.4285 - val_loss: 27.8791 - val_MinusLogProbMetric: 27.8791 - lr: 4.1667e-05 - 40s/epoch - 206ms/step
Epoch 644/1000
2023-10-25 08:40:37.348 
Epoch 644/1000 
	 loss: 27.4234, MinusLogProbMetric: 27.4234, val_loss: 27.8574, val_MinusLogProbMetric: 27.8574

Epoch 644: val_loss did not improve from 27.84758
196/196 - 36s - loss: 27.4234 - MinusLogProbMetric: 27.4234 - val_loss: 27.8574 - val_MinusLogProbMetric: 27.8574 - lr: 4.1667e-05 - 36s/epoch - 185ms/step
Epoch 645/1000
2023-10-25 08:41:13.632 
Epoch 645/1000 
	 loss: 27.4254, MinusLogProbMetric: 27.4254, val_loss: 27.9041, val_MinusLogProbMetric: 27.9041

Epoch 645: val_loss did not improve from 27.84758
196/196 - 36s - loss: 27.4254 - MinusLogProbMetric: 27.4254 - val_loss: 27.9041 - val_MinusLogProbMetric: 27.9041 - lr: 4.1667e-05 - 36s/epoch - 185ms/step
Epoch 646/1000
2023-10-25 08:41:50.705 
Epoch 646/1000 
	 loss: 27.4319, MinusLogProbMetric: 27.4319, val_loss: 27.8741, val_MinusLogProbMetric: 27.8741

Epoch 646: val_loss did not improve from 27.84758
196/196 - 37s - loss: 27.4319 - MinusLogProbMetric: 27.4319 - val_loss: 27.8741 - val_MinusLogProbMetric: 27.8741 - lr: 4.1667e-05 - 37s/epoch - 189ms/step
Epoch 647/1000
2023-10-25 08:42:30.370 
Epoch 647/1000 
	 loss: 27.4236, MinusLogProbMetric: 27.4236, val_loss: 27.8783, val_MinusLogProbMetric: 27.8783

Epoch 647: val_loss did not improve from 27.84758
196/196 - 40s - loss: 27.4236 - MinusLogProbMetric: 27.4236 - val_loss: 27.8783 - val_MinusLogProbMetric: 27.8783 - lr: 4.1667e-05 - 40s/epoch - 202ms/step
Epoch 648/1000
2023-10-25 08:43:06.353 
Epoch 648/1000 
	 loss: 27.4268, MinusLogProbMetric: 27.4268, val_loss: 27.8709, val_MinusLogProbMetric: 27.8709

Epoch 648: val_loss did not improve from 27.84758
196/196 - 36s - loss: 27.4268 - MinusLogProbMetric: 27.4268 - val_loss: 27.8709 - val_MinusLogProbMetric: 27.8709 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 649/1000
2023-10-25 08:43:42.530 
Epoch 649/1000 
	 loss: 27.4244, MinusLogProbMetric: 27.4244, val_loss: 27.8604, val_MinusLogProbMetric: 27.8604

Epoch 649: val_loss did not improve from 27.84758
196/196 - 36s - loss: 27.4244 - MinusLogProbMetric: 27.4244 - val_loss: 27.8604 - val_MinusLogProbMetric: 27.8604 - lr: 4.1667e-05 - 36s/epoch - 185ms/step
Epoch 650/1000
2023-10-25 08:44:18.545 
Epoch 650/1000 
	 loss: 27.4289, MinusLogProbMetric: 27.4289, val_loss: 27.8805, val_MinusLogProbMetric: 27.8805

Epoch 650: val_loss did not improve from 27.84758
196/196 - 36s - loss: 27.4289 - MinusLogProbMetric: 27.4289 - val_loss: 27.8805 - val_MinusLogProbMetric: 27.8805 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 651/1000
2023-10-25 08:44:55.523 
Epoch 651/1000 
	 loss: 27.4242, MinusLogProbMetric: 27.4242, val_loss: 27.8666, val_MinusLogProbMetric: 27.8666

Epoch 651: val_loss did not improve from 27.84758
196/196 - 37s - loss: 27.4242 - MinusLogProbMetric: 27.4242 - val_loss: 27.8666 - val_MinusLogProbMetric: 27.8666 - lr: 4.1667e-05 - 37s/epoch - 189ms/step
Epoch 652/1000
2023-10-25 08:45:34.807 
Epoch 652/1000 
	 loss: 27.4272, MinusLogProbMetric: 27.4272, val_loss: 27.8962, val_MinusLogProbMetric: 27.8962

Epoch 652: val_loss did not improve from 27.84758
196/196 - 39s - loss: 27.4272 - MinusLogProbMetric: 27.4272 - val_loss: 27.8962 - val_MinusLogProbMetric: 27.8962 - lr: 4.1667e-05 - 39s/epoch - 200ms/step
Epoch 653/1000
2023-10-25 08:46:10.674 
Epoch 653/1000 
	 loss: 27.4343, MinusLogProbMetric: 27.4343, val_loss: 27.8631, val_MinusLogProbMetric: 27.8631

Epoch 653: val_loss did not improve from 27.84758
196/196 - 36s - loss: 27.4343 - MinusLogProbMetric: 27.4343 - val_loss: 27.8631 - val_MinusLogProbMetric: 27.8631 - lr: 4.1667e-05 - 36s/epoch - 183ms/step
Epoch 654/1000
2023-10-25 08:46:46.778 
Epoch 654/1000 
	 loss: 27.4277, MinusLogProbMetric: 27.4277, val_loss: 27.8813, val_MinusLogProbMetric: 27.8813

Epoch 654: val_loss did not improve from 27.84758
196/196 - 36s - loss: 27.4277 - MinusLogProbMetric: 27.4277 - val_loss: 27.8813 - val_MinusLogProbMetric: 27.8813 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 655/1000
2023-10-25 08:47:23.876 
Epoch 655/1000 
	 loss: 27.4261, MinusLogProbMetric: 27.4261, val_loss: 27.8564, val_MinusLogProbMetric: 27.8564

Epoch 655: val_loss did not improve from 27.84758
196/196 - 37s - loss: 27.4261 - MinusLogProbMetric: 27.4261 - val_loss: 27.8564 - val_MinusLogProbMetric: 27.8564 - lr: 4.1667e-05 - 37s/epoch - 189ms/step
Epoch 656/1000
2023-10-25 08:48:03.717 
Epoch 656/1000 
	 loss: 27.4235, MinusLogProbMetric: 27.4235, val_loss: 27.8593, val_MinusLogProbMetric: 27.8593

Epoch 656: val_loss did not improve from 27.84758
196/196 - 40s - loss: 27.4235 - MinusLogProbMetric: 27.4235 - val_loss: 27.8593 - val_MinusLogProbMetric: 27.8593 - lr: 4.1667e-05 - 40s/epoch - 203ms/step
Epoch 657/1000
2023-10-25 08:48:40.396 
Epoch 657/1000 
	 loss: 27.4220, MinusLogProbMetric: 27.4220, val_loss: 27.9320, val_MinusLogProbMetric: 27.9320

Epoch 657: val_loss did not improve from 27.84758
196/196 - 37s - loss: 27.4220 - MinusLogProbMetric: 27.4220 - val_loss: 27.9320 - val_MinusLogProbMetric: 27.9320 - lr: 4.1667e-05 - 37s/epoch - 187ms/step
Epoch 658/1000
2023-10-25 08:49:16.881 
Epoch 658/1000 
	 loss: 27.4427, MinusLogProbMetric: 27.4427, val_loss: 27.8547, val_MinusLogProbMetric: 27.8547

Epoch 658: val_loss did not improve from 27.84758
196/196 - 36s - loss: 27.4427 - MinusLogProbMetric: 27.4427 - val_loss: 27.8547 - val_MinusLogProbMetric: 27.8547 - lr: 4.1667e-05 - 36s/epoch - 186ms/step
Epoch 659/1000
2023-10-25 08:49:52.949 
Epoch 659/1000 
	 loss: 27.4274, MinusLogProbMetric: 27.4274, val_loss: 27.8935, val_MinusLogProbMetric: 27.8935

Epoch 659: val_loss did not improve from 27.84758
196/196 - 36s - loss: 27.4274 - MinusLogProbMetric: 27.4274 - val_loss: 27.8935 - val_MinusLogProbMetric: 27.8935 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 660/1000
2023-10-25 08:50:33.243 
Epoch 660/1000 
	 loss: 27.4265, MinusLogProbMetric: 27.4265, val_loss: 27.8755, val_MinusLogProbMetric: 27.8755

Epoch 660: val_loss did not improve from 27.84758
196/196 - 40s - loss: 27.4265 - MinusLogProbMetric: 27.4265 - val_loss: 27.8755 - val_MinusLogProbMetric: 27.8755 - lr: 4.1667e-05 - 40s/epoch - 206ms/step
Epoch 661/1000
2023-10-25 08:51:12.254 
Epoch 661/1000 
	 loss: 27.4306, MinusLogProbMetric: 27.4306, val_loss: 27.8433, val_MinusLogProbMetric: 27.8433

Epoch 661: val_loss improved from 27.84758 to 27.84334, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 40s - loss: 27.4306 - MinusLogProbMetric: 27.4306 - val_loss: 27.8433 - val_MinusLogProbMetric: 27.8433 - lr: 4.1667e-05 - 40s/epoch - 202ms/step
Epoch 662/1000
2023-10-25 08:51:48.593 
Epoch 662/1000 
	 loss: 27.4295, MinusLogProbMetric: 27.4295, val_loss: 27.8954, val_MinusLogProbMetric: 27.8954

Epoch 662: val_loss did not improve from 27.84334
196/196 - 36s - loss: 27.4295 - MinusLogProbMetric: 27.4295 - val_loss: 27.8954 - val_MinusLogProbMetric: 27.8954 - lr: 4.1667e-05 - 36s/epoch - 182ms/step
Epoch 663/1000
2023-10-25 08:52:24.942 
Epoch 663/1000 
	 loss: 27.4222, MinusLogProbMetric: 27.4222, val_loss: 27.8626, val_MinusLogProbMetric: 27.8626

Epoch 663: val_loss did not improve from 27.84334
196/196 - 36s - loss: 27.4222 - MinusLogProbMetric: 27.4222 - val_loss: 27.8626 - val_MinusLogProbMetric: 27.8626 - lr: 4.1667e-05 - 36s/epoch - 185ms/step
Epoch 664/1000
2023-10-25 08:53:01.525 
Epoch 664/1000 
	 loss: 27.4293, MinusLogProbMetric: 27.4293, val_loss: 27.8776, val_MinusLogProbMetric: 27.8776

Epoch 664: val_loss did not improve from 27.84334
196/196 - 37s - loss: 27.4293 - MinusLogProbMetric: 27.4293 - val_loss: 27.8776 - val_MinusLogProbMetric: 27.8776 - lr: 4.1667e-05 - 37s/epoch - 187ms/step
Epoch 665/1000
2023-10-25 08:53:39.473 
Epoch 665/1000 
	 loss: 27.4359, MinusLogProbMetric: 27.4359, val_loss: 27.8544, val_MinusLogProbMetric: 27.8544

Epoch 665: val_loss did not improve from 27.84334
196/196 - 38s - loss: 27.4359 - MinusLogProbMetric: 27.4359 - val_loss: 27.8544 - val_MinusLogProbMetric: 27.8544 - lr: 4.1667e-05 - 38s/epoch - 194ms/step
Epoch 666/1000
2023-10-25 08:54:17.242 
Epoch 666/1000 
	 loss: 27.4324, MinusLogProbMetric: 27.4324, val_loss: 27.8705, val_MinusLogProbMetric: 27.8705

Epoch 666: val_loss did not improve from 27.84334
196/196 - 38s - loss: 27.4324 - MinusLogProbMetric: 27.4324 - val_loss: 27.8705 - val_MinusLogProbMetric: 27.8705 - lr: 4.1667e-05 - 38s/epoch - 193ms/step
Epoch 667/1000
2023-10-25 08:54:53.925 
Epoch 667/1000 
	 loss: 27.4279, MinusLogProbMetric: 27.4279, val_loss: 27.8962, val_MinusLogProbMetric: 27.8962

Epoch 667: val_loss did not improve from 27.84334
196/196 - 37s - loss: 27.4279 - MinusLogProbMetric: 27.4279 - val_loss: 27.8962 - val_MinusLogProbMetric: 27.8962 - lr: 4.1667e-05 - 37s/epoch - 187ms/step
Epoch 668/1000
2023-10-25 08:55:29.551 
Epoch 668/1000 
	 loss: 27.4255, MinusLogProbMetric: 27.4255, val_loss: 27.8784, val_MinusLogProbMetric: 27.8784

Epoch 668: val_loss did not improve from 27.84334
196/196 - 36s - loss: 27.4255 - MinusLogProbMetric: 27.4255 - val_loss: 27.8784 - val_MinusLogProbMetric: 27.8784 - lr: 4.1667e-05 - 36s/epoch - 182ms/step
Epoch 669/1000
2023-10-25 08:56:06.770 
Epoch 669/1000 
	 loss: 27.4397, MinusLogProbMetric: 27.4397, val_loss: 27.8567, val_MinusLogProbMetric: 27.8567

Epoch 669: val_loss did not improve from 27.84334
196/196 - 37s - loss: 27.4397 - MinusLogProbMetric: 27.4397 - val_loss: 27.8567 - val_MinusLogProbMetric: 27.8567 - lr: 4.1667e-05 - 37s/epoch - 190ms/step
Epoch 670/1000
2023-10-25 08:56:44.530 
Epoch 670/1000 
	 loss: 27.4340, MinusLogProbMetric: 27.4340, val_loss: 27.8615, val_MinusLogProbMetric: 27.8615

Epoch 670: val_loss did not improve from 27.84334
196/196 - 38s - loss: 27.4340 - MinusLogProbMetric: 27.4340 - val_loss: 27.8615 - val_MinusLogProbMetric: 27.8615 - lr: 4.1667e-05 - 38s/epoch - 193ms/step
Epoch 671/1000
2023-10-25 08:57:21.012 
Epoch 671/1000 
	 loss: 27.4252, MinusLogProbMetric: 27.4252, val_loss: 27.8895, val_MinusLogProbMetric: 27.8895

Epoch 671: val_loss did not improve from 27.84334
196/196 - 36s - loss: 27.4252 - MinusLogProbMetric: 27.4252 - val_loss: 27.8895 - val_MinusLogProbMetric: 27.8895 - lr: 4.1667e-05 - 36s/epoch - 186ms/step
Epoch 672/1000
2023-10-25 08:57:57.013 
Epoch 672/1000 
	 loss: 27.4271, MinusLogProbMetric: 27.4271, val_loss: 27.8497, val_MinusLogProbMetric: 27.8497

Epoch 672: val_loss did not improve from 27.84334
196/196 - 36s - loss: 27.4271 - MinusLogProbMetric: 27.4271 - val_loss: 27.8497 - val_MinusLogProbMetric: 27.8497 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 673/1000
2023-10-25 08:58:32.424 
Epoch 673/1000 
	 loss: 27.4172, MinusLogProbMetric: 27.4172, val_loss: 27.8626, val_MinusLogProbMetric: 27.8626

Epoch 673: val_loss did not improve from 27.84334
196/196 - 35s - loss: 27.4172 - MinusLogProbMetric: 27.4172 - val_loss: 27.8626 - val_MinusLogProbMetric: 27.8626 - lr: 4.1667e-05 - 35s/epoch - 181ms/step
Epoch 674/1000
2023-10-25 08:59:10.969 
Epoch 674/1000 
	 loss: 27.4268, MinusLogProbMetric: 27.4268, val_loss: 27.8971, val_MinusLogProbMetric: 27.8971

Epoch 674: val_loss did not improve from 27.84334
196/196 - 39s - loss: 27.4268 - MinusLogProbMetric: 27.4268 - val_loss: 27.8971 - val_MinusLogProbMetric: 27.8971 - lr: 4.1667e-05 - 39s/epoch - 197ms/step
Epoch 675/1000
2023-10-25 08:59:49.783 
Epoch 675/1000 
	 loss: 27.4330, MinusLogProbMetric: 27.4330, val_loss: 27.8861, val_MinusLogProbMetric: 27.8861

Epoch 675: val_loss did not improve from 27.84334
196/196 - 39s - loss: 27.4330 - MinusLogProbMetric: 27.4330 - val_loss: 27.8861 - val_MinusLogProbMetric: 27.8861 - lr: 4.1667e-05 - 39s/epoch - 198ms/step
Epoch 676/1000
2023-10-25 09:00:26.494 
Epoch 676/1000 
	 loss: 27.4343, MinusLogProbMetric: 27.4343, val_loss: 27.9556, val_MinusLogProbMetric: 27.9556

Epoch 676: val_loss did not improve from 27.84334
196/196 - 37s - loss: 27.4343 - MinusLogProbMetric: 27.4343 - val_loss: 27.9556 - val_MinusLogProbMetric: 27.9556 - lr: 4.1667e-05 - 37s/epoch - 187ms/step
Epoch 677/1000
2023-10-25 09:01:02.598 
Epoch 677/1000 
	 loss: 27.4342, MinusLogProbMetric: 27.4342, val_loss: 27.8465, val_MinusLogProbMetric: 27.8465

Epoch 677: val_loss did not improve from 27.84334
196/196 - 36s - loss: 27.4342 - MinusLogProbMetric: 27.4342 - val_loss: 27.8465 - val_MinusLogProbMetric: 27.8465 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 678/1000
2023-10-25 09:01:38.564 
Epoch 678/1000 
	 loss: 27.4240, MinusLogProbMetric: 27.4240, val_loss: 27.8701, val_MinusLogProbMetric: 27.8701

Epoch 678: val_loss did not improve from 27.84334
196/196 - 36s - loss: 27.4240 - MinusLogProbMetric: 27.4240 - val_loss: 27.8701 - val_MinusLogProbMetric: 27.8701 - lr: 4.1667e-05 - 36s/epoch - 183ms/step
Epoch 679/1000
2023-10-25 09:02:16.056 
Epoch 679/1000 
	 loss: 27.4384, MinusLogProbMetric: 27.4384, val_loss: 27.8619, val_MinusLogProbMetric: 27.8619

Epoch 679: val_loss did not improve from 27.84334
196/196 - 37s - loss: 27.4384 - MinusLogProbMetric: 27.4384 - val_loss: 27.8619 - val_MinusLogProbMetric: 27.8619 - lr: 4.1667e-05 - 37s/epoch - 191ms/step
Epoch 680/1000
2023-10-25 09:02:55.829 
Epoch 680/1000 
	 loss: 27.4290, MinusLogProbMetric: 27.4290, val_loss: 27.8695, val_MinusLogProbMetric: 27.8695

Epoch 680: val_loss did not improve from 27.84334
196/196 - 40s - loss: 27.4290 - MinusLogProbMetric: 27.4290 - val_loss: 27.8695 - val_MinusLogProbMetric: 27.8695 - lr: 4.1667e-05 - 40s/epoch - 203ms/step
Epoch 681/1000
2023-10-25 09:03:31.830 
Epoch 681/1000 
	 loss: 27.4249, MinusLogProbMetric: 27.4249, val_loss: 27.8648, val_MinusLogProbMetric: 27.8648

Epoch 681: val_loss did not improve from 27.84334
196/196 - 36s - loss: 27.4249 - MinusLogProbMetric: 27.4249 - val_loss: 27.8648 - val_MinusLogProbMetric: 27.8648 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 682/1000
2023-10-25 09:04:07.776 
Epoch 682/1000 
	 loss: 27.4246, MinusLogProbMetric: 27.4246, val_loss: 27.8670, val_MinusLogProbMetric: 27.8670

Epoch 682: val_loss did not improve from 27.84334
196/196 - 36s - loss: 27.4246 - MinusLogProbMetric: 27.4246 - val_loss: 27.8670 - val_MinusLogProbMetric: 27.8670 - lr: 4.1667e-05 - 36s/epoch - 183ms/step
Epoch 683/1000
2023-10-25 09:04:45.746 
Epoch 683/1000 
	 loss: 27.4346, MinusLogProbMetric: 27.4346, val_loss: 27.9384, val_MinusLogProbMetric: 27.9384

Epoch 683: val_loss did not improve from 27.84334
196/196 - 38s - loss: 27.4346 - MinusLogProbMetric: 27.4346 - val_loss: 27.9384 - val_MinusLogProbMetric: 27.9384 - lr: 4.1667e-05 - 38s/epoch - 194ms/step
Epoch 684/1000
2023-10-25 09:05:21.882 
Epoch 684/1000 
	 loss: 27.4235, MinusLogProbMetric: 27.4235, val_loss: 27.8877, val_MinusLogProbMetric: 27.8877

Epoch 684: val_loss did not improve from 27.84334
196/196 - 36s - loss: 27.4235 - MinusLogProbMetric: 27.4235 - val_loss: 27.8877 - val_MinusLogProbMetric: 27.8877 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 685/1000
2023-10-25 09:05:56.783 
Epoch 685/1000 
	 loss: 27.4227, MinusLogProbMetric: 27.4227, val_loss: 27.8916, val_MinusLogProbMetric: 27.8916

Epoch 685: val_loss did not improve from 27.84334
196/196 - 35s - loss: 27.4227 - MinusLogProbMetric: 27.4227 - val_loss: 27.8916 - val_MinusLogProbMetric: 27.8916 - lr: 4.1667e-05 - 35s/epoch - 178ms/step
Epoch 686/1000
2023-10-25 09:06:30.900 
Epoch 686/1000 
	 loss: 27.4312, MinusLogProbMetric: 27.4312, val_loss: 27.8838, val_MinusLogProbMetric: 27.8838

Epoch 686: val_loss did not improve from 27.84334
196/196 - 34s - loss: 27.4312 - MinusLogProbMetric: 27.4312 - val_loss: 27.8838 - val_MinusLogProbMetric: 27.8838 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 687/1000
2023-10-25 09:07:05.467 
Epoch 687/1000 
	 loss: 27.4267, MinusLogProbMetric: 27.4267, val_loss: 27.8924, val_MinusLogProbMetric: 27.8924

Epoch 687: val_loss did not improve from 27.84334
196/196 - 35s - loss: 27.4267 - MinusLogProbMetric: 27.4267 - val_loss: 27.8924 - val_MinusLogProbMetric: 27.8924 - lr: 4.1667e-05 - 35s/epoch - 176ms/step
Epoch 688/1000
2023-10-25 09:07:42.109 
Epoch 688/1000 
	 loss: 27.4290, MinusLogProbMetric: 27.4290, val_loss: 27.8842, val_MinusLogProbMetric: 27.8842

Epoch 688: val_loss did not improve from 27.84334
196/196 - 37s - loss: 27.4290 - MinusLogProbMetric: 27.4290 - val_loss: 27.8842 - val_MinusLogProbMetric: 27.8842 - lr: 4.1667e-05 - 37s/epoch - 187ms/step
Epoch 689/1000
2023-10-25 09:08:21.873 
Epoch 689/1000 
	 loss: 27.4254, MinusLogProbMetric: 27.4254, val_loss: 27.9750, val_MinusLogProbMetric: 27.9750

Epoch 689: val_loss did not improve from 27.84334
196/196 - 40s - loss: 27.4254 - MinusLogProbMetric: 27.4254 - val_loss: 27.9750 - val_MinusLogProbMetric: 27.9750 - lr: 4.1667e-05 - 40s/epoch - 203ms/step
Epoch 690/1000
2023-10-25 09:09:00.402 
Epoch 690/1000 
	 loss: 27.4426, MinusLogProbMetric: 27.4426, val_loss: 27.9413, val_MinusLogProbMetric: 27.9413

Epoch 690: val_loss did not improve from 27.84334
196/196 - 39s - loss: 27.4426 - MinusLogProbMetric: 27.4426 - val_loss: 27.9413 - val_MinusLogProbMetric: 27.9413 - lr: 4.1667e-05 - 39s/epoch - 197ms/step
Epoch 691/1000
2023-10-25 09:09:36.268 
Epoch 691/1000 
	 loss: 27.4243, MinusLogProbMetric: 27.4243, val_loss: 27.8765, val_MinusLogProbMetric: 27.8765

Epoch 691: val_loss did not improve from 27.84334
196/196 - 36s - loss: 27.4243 - MinusLogProbMetric: 27.4243 - val_loss: 27.8765 - val_MinusLogProbMetric: 27.8765 - lr: 4.1667e-05 - 36s/epoch - 183ms/step
Epoch 692/1000
2023-10-25 09:10:11.501 
Epoch 692/1000 
	 loss: 27.4342, MinusLogProbMetric: 27.4342, val_loss: 27.8985, val_MinusLogProbMetric: 27.8985

Epoch 692: val_loss did not improve from 27.84334
196/196 - 35s - loss: 27.4342 - MinusLogProbMetric: 27.4342 - val_loss: 27.8985 - val_MinusLogProbMetric: 27.8985 - lr: 4.1667e-05 - 35s/epoch - 180ms/step
Epoch 693/1000
2023-10-25 09:10:49.217 
Epoch 693/1000 
	 loss: 27.4248, MinusLogProbMetric: 27.4248, val_loss: 27.8804, val_MinusLogProbMetric: 27.8804

Epoch 693: val_loss did not improve from 27.84334
196/196 - 38s - loss: 27.4248 - MinusLogProbMetric: 27.4248 - val_loss: 27.8804 - val_MinusLogProbMetric: 27.8804 - lr: 4.1667e-05 - 38s/epoch - 192ms/step
Epoch 694/1000
2023-10-25 09:11:29.027 
Epoch 694/1000 
	 loss: 27.4199, MinusLogProbMetric: 27.4199, val_loss: 27.8481, val_MinusLogProbMetric: 27.8481

Epoch 694: val_loss did not improve from 27.84334
196/196 - 40s - loss: 27.4199 - MinusLogProbMetric: 27.4199 - val_loss: 27.8481 - val_MinusLogProbMetric: 27.8481 - lr: 4.1667e-05 - 40s/epoch - 203ms/step
Epoch 695/1000
2023-10-25 09:12:06.350 
Epoch 695/1000 
	 loss: 27.4290, MinusLogProbMetric: 27.4290, val_loss: 27.8592, val_MinusLogProbMetric: 27.8592

Epoch 695: val_loss did not improve from 27.84334
196/196 - 37s - loss: 27.4290 - MinusLogProbMetric: 27.4290 - val_loss: 27.8592 - val_MinusLogProbMetric: 27.8592 - lr: 4.1667e-05 - 37s/epoch - 190ms/step
Epoch 696/1000
2023-10-25 09:12:42.575 
Epoch 696/1000 
	 loss: 27.4237, MinusLogProbMetric: 27.4237, val_loss: 27.8595, val_MinusLogProbMetric: 27.8595

Epoch 696: val_loss did not improve from 27.84334
196/196 - 36s - loss: 27.4237 - MinusLogProbMetric: 27.4237 - val_loss: 27.8595 - val_MinusLogProbMetric: 27.8595 - lr: 4.1667e-05 - 36s/epoch - 185ms/step
Epoch 697/1000
2023-10-25 09:13:18.486 
Epoch 697/1000 
	 loss: 27.4234, MinusLogProbMetric: 27.4234, val_loss: 27.8458, val_MinusLogProbMetric: 27.8458

Epoch 697: val_loss did not improve from 27.84334
196/196 - 36s - loss: 27.4234 - MinusLogProbMetric: 27.4234 - val_loss: 27.8458 - val_MinusLogProbMetric: 27.8458 - lr: 4.1667e-05 - 36s/epoch - 183ms/step
Epoch 698/1000
2023-10-25 09:13:55.101 
Epoch 698/1000 
	 loss: 27.4165, MinusLogProbMetric: 27.4165, val_loss: 27.8614, val_MinusLogProbMetric: 27.8614

Epoch 698: val_loss did not improve from 27.84334
196/196 - 37s - loss: 27.4165 - MinusLogProbMetric: 27.4165 - val_loss: 27.8614 - val_MinusLogProbMetric: 27.8614 - lr: 4.1667e-05 - 37s/epoch - 187ms/step
Epoch 699/1000
2023-10-25 09:14:34.948 
Epoch 699/1000 
	 loss: 27.4242, MinusLogProbMetric: 27.4242, val_loss: 27.8866, val_MinusLogProbMetric: 27.8866

Epoch 699: val_loss did not improve from 27.84334
196/196 - 40s - loss: 27.4242 - MinusLogProbMetric: 27.4242 - val_loss: 27.8866 - val_MinusLogProbMetric: 27.8866 - lr: 4.1667e-05 - 40s/epoch - 203ms/step
Epoch 700/1000
2023-10-25 09:15:14.659 
Epoch 700/1000 
	 loss: 27.4311, MinusLogProbMetric: 27.4311, val_loss: 27.9269, val_MinusLogProbMetric: 27.9269

Epoch 700: val_loss did not improve from 27.84334
196/196 - 40s - loss: 27.4311 - MinusLogProbMetric: 27.4311 - val_loss: 27.9269 - val_MinusLogProbMetric: 27.9269 - lr: 4.1667e-05 - 40s/epoch - 203ms/step
Epoch 701/1000
2023-10-25 09:15:51.047 
Epoch 701/1000 
	 loss: 27.4273, MinusLogProbMetric: 27.4273, val_loss: 27.8621, val_MinusLogProbMetric: 27.8621

Epoch 701: val_loss did not improve from 27.84334
196/196 - 36s - loss: 27.4273 - MinusLogProbMetric: 27.4273 - val_loss: 27.8621 - val_MinusLogProbMetric: 27.8621 - lr: 4.1667e-05 - 36s/epoch - 186ms/step
Epoch 702/1000
2023-10-25 09:16:28.489 
Epoch 702/1000 
	 loss: 27.4189, MinusLogProbMetric: 27.4189, val_loss: 27.8629, val_MinusLogProbMetric: 27.8629

Epoch 702: val_loss did not improve from 27.84334
196/196 - 37s - loss: 27.4189 - MinusLogProbMetric: 27.4189 - val_loss: 27.8629 - val_MinusLogProbMetric: 27.8629 - lr: 4.1667e-05 - 37s/epoch - 191ms/step
Epoch 703/1000
2023-10-25 09:17:05.830 
Epoch 703/1000 
	 loss: 27.4172, MinusLogProbMetric: 27.4172, val_loss: 27.8431, val_MinusLogProbMetric: 27.8431

Epoch 703: val_loss improved from 27.84334 to 27.84310, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 38s - loss: 27.4172 - MinusLogProbMetric: 27.4172 - val_loss: 27.8431 - val_MinusLogProbMetric: 27.8431 - lr: 4.1667e-05 - 38s/epoch - 194ms/step
Epoch 704/1000
2023-10-25 09:17:43.835 
Epoch 704/1000 
	 loss: 27.4293, MinusLogProbMetric: 27.4293, val_loss: 27.8696, val_MinusLogProbMetric: 27.8696

Epoch 704: val_loss did not improve from 27.84310
196/196 - 37s - loss: 27.4293 - MinusLogProbMetric: 27.4293 - val_loss: 27.8696 - val_MinusLogProbMetric: 27.8696 - lr: 4.1667e-05 - 37s/epoch - 190ms/step
Epoch 705/1000
2023-10-25 09:18:20.266 
Epoch 705/1000 
	 loss: 27.4269, MinusLogProbMetric: 27.4269, val_loss: 27.8949, val_MinusLogProbMetric: 27.8949

Epoch 705: val_loss did not improve from 27.84310
196/196 - 36s - loss: 27.4269 - MinusLogProbMetric: 27.4269 - val_loss: 27.8949 - val_MinusLogProbMetric: 27.8949 - lr: 4.1667e-05 - 36s/epoch - 186ms/step
Epoch 706/1000
2023-10-25 09:18:58.673 
Epoch 706/1000 
	 loss: 27.4343, MinusLogProbMetric: 27.4343, val_loss: 27.9152, val_MinusLogProbMetric: 27.9152

Epoch 706: val_loss did not improve from 27.84310
196/196 - 38s - loss: 27.4343 - MinusLogProbMetric: 27.4343 - val_loss: 27.9152 - val_MinusLogProbMetric: 27.9152 - lr: 4.1667e-05 - 38s/epoch - 196ms/step
Epoch 707/1000
2023-10-25 09:19:34.642 
Epoch 707/1000 
	 loss: 27.4218, MinusLogProbMetric: 27.4218, val_loss: 27.8594, val_MinusLogProbMetric: 27.8594

Epoch 707: val_loss did not improve from 27.84310
196/196 - 36s - loss: 27.4218 - MinusLogProbMetric: 27.4218 - val_loss: 27.8594 - val_MinusLogProbMetric: 27.8594 - lr: 4.1667e-05 - 36s/epoch - 184ms/step
Epoch 708/1000
2023-10-25 09:20:11.561 
Epoch 708/1000 
	 loss: 27.4252, MinusLogProbMetric: 27.4252, val_loss: 27.8889, val_MinusLogProbMetric: 27.8889

Epoch 708: val_loss did not improve from 27.84310
196/196 - 37s - loss: 27.4252 - MinusLogProbMetric: 27.4252 - val_loss: 27.8889 - val_MinusLogProbMetric: 27.8889 - lr: 4.1667e-05 - 37s/epoch - 188ms/step
Epoch 709/1000
2023-10-25 09:20:48.442 
Epoch 709/1000 
	 loss: 27.4327, MinusLogProbMetric: 27.4327, val_loss: 27.8900, val_MinusLogProbMetric: 27.8900

Epoch 709: val_loss did not improve from 27.84310
196/196 - 37s - loss: 27.4327 - MinusLogProbMetric: 27.4327 - val_loss: 27.8900 - val_MinusLogProbMetric: 27.8900 - lr: 4.1667e-05 - 37s/epoch - 188ms/step
Epoch 710/1000
2023-10-25 09:21:29.773 
Epoch 710/1000 
	 loss: 27.4340, MinusLogProbMetric: 27.4340, val_loss: 27.8980, val_MinusLogProbMetric: 27.8980

Epoch 710: val_loss did not improve from 27.84310
196/196 - 41s - loss: 27.4340 - MinusLogProbMetric: 27.4340 - val_loss: 27.8980 - val_MinusLogProbMetric: 27.8980 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 711/1000
2023-10-25 09:22:10.738 
Epoch 711/1000 
	 loss: 27.4226, MinusLogProbMetric: 27.4226, val_loss: 27.8542, val_MinusLogProbMetric: 27.8542

Epoch 711: val_loss did not improve from 27.84310
196/196 - 41s - loss: 27.4226 - MinusLogProbMetric: 27.4226 - val_loss: 27.8542 - val_MinusLogProbMetric: 27.8542 - lr: 4.1667e-05 - 41s/epoch - 209ms/step
Epoch 712/1000
2023-10-25 09:22:50.840 
Epoch 712/1000 
	 loss: 27.4292, MinusLogProbMetric: 27.4292, val_loss: 27.8549, val_MinusLogProbMetric: 27.8549

Epoch 712: val_loss did not improve from 27.84310
196/196 - 40s - loss: 27.4292 - MinusLogProbMetric: 27.4292 - val_loss: 27.8549 - val_MinusLogProbMetric: 27.8549 - lr: 4.1667e-05 - 40s/epoch - 205ms/step
Epoch 713/1000
2023-10-25 09:23:33.473 
Epoch 713/1000 
	 loss: 27.4302, MinusLogProbMetric: 27.4302, val_loss: 27.9263, val_MinusLogProbMetric: 27.9263

Epoch 713: val_loss did not improve from 27.84310
196/196 - 43s - loss: 27.4302 - MinusLogProbMetric: 27.4302 - val_loss: 27.9263 - val_MinusLogProbMetric: 27.9263 - lr: 4.1667e-05 - 43s/epoch - 218ms/step
Epoch 714/1000
2023-10-25 09:24:14.785 
Epoch 714/1000 
	 loss: 27.4340, MinusLogProbMetric: 27.4340, val_loss: 27.8934, val_MinusLogProbMetric: 27.8934

Epoch 714: val_loss did not improve from 27.84310
196/196 - 41s - loss: 27.4340 - MinusLogProbMetric: 27.4340 - val_loss: 27.8934 - val_MinusLogProbMetric: 27.8934 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 715/1000
2023-10-25 09:24:57.261 
Epoch 715/1000 
	 loss: 27.4342, MinusLogProbMetric: 27.4342, val_loss: 27.8527, val_MinusLogProbMetric: 27.8527

Epoch 715: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4342 - MinusLogProbMetric: 27.4342 - val_loss: 27.8527 - val_MinusLogProbMetric: 27.8527 - lr: 4.1667e-05 - 42s/epoch - 217ms/step
Epoch 716/1000
2023-10-25 09:25:39.422 
Epoch 716/1000 
	 loss: 27.4304, MinusLogProbMetric: 27.4304, val_loss: 27.9006, val_MinusLogProbMetric: 27.9006

Epoch 716: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4304 - MinusLogProbMetric: 27.4304 - val_loss: 27.9006 - val_MinusLogProbMetric: 27.9006 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 717/1000
2023-10-25 09:26:21.545 
Epoch 717/1000 
	 loss: 27.4376, MinusLogProbMetric: 27.4376, val_loss: 27.9038, val_MinusLogProbMetric: 27.9038

Epoch 717: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4376 - MinusLogProbMetric: 27.4376 - val_loss: 27.9038 - val_MinusLogProbMetric: 27.9038 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 718/1000
2023-10-25 09:27:03.256 
Epoch 718/1000 
	 loss: 27.4216, MinusLogProbMetric: 27.4216, val_loss: 27.8582, val_MinusLogProbMetric: 27.8582

Epoch 718: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4216 - MinusLogProbMetric: 27.4216 - val_loss: 27.8582 - val_MinusLogProbMetric: 27.8582 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 719/1000
2023-10-25 09:27:45.428 
Epoch 719/1000 
	 loss: 27.4305, MinusLogProbMetric: 27.4305, val_loss: 27.8837, val_MinusLogProbMetric: 27.8837

Epoch 719: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4305 - MinusLogProbMetric: 27.4305 - val_loss: 27.8837 - val_MinusLogProbMetric: 27.8837 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 720/1000
2023-10-25 09:28:26.892 
Epoch 720/1000 
	 loss: 27.4280, MinusLogProbMetric: 27.4280, val_loss: 27.8865, val_MinusLogProbMetric: 27.8865

Epoch 720: val_loss did not improve from 27.84310
196/196 - 41s - loss: 27.4280 - MinusLogProbMetric: 27.4280 - val_loss: 27.8865 - val_MinusLogProbMetric: 27.8865 - lr: 4.1667e-05 - 41s/epoch - 212ms/step
Epoch 721/1000
2023-10-25 09:29:08.182 
Epoch 721/1000 
	 loss: 27.4216, MinusLogProbMetric: 27.4216, val_loss: 27.9183, val_MinusLogProbMetric: 27.9183

Epoch 721: val_loss did not improve from 27.84310
196/196 - 41s - loss: 27.4216 - MinusLogProbMetric: 27.4216 - val_loss: 27.9183 - val_MinusLogProbMetric: 27.9183 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 722/1000
2023-10-25 09:29:46.251 
Epoch 722/1000 
	 loss: 27.4187, MinusLogProbMetric: 27.4187, val_loss: 27.8675, val_MinusLogProbMetric: 27.8675

Epoch 722: val_loss did not improve from 27.84310
196/196 - 38s - loss: 27.4187 - MinusLogProbMetric: 27.4187 - val_loss: 27.8675 - val_MinusLogProbMetric: 27.8675 - lr: 4.1667e-05 - 38s/epoch - 194ms/step
Epoch 723/1000
2023-10-25 09:30:24.626 
Epoch 723/1000 
	 loss: 27.4208, MinusLogProbMetric: 27.4208, val_loss: 27.8559, val_MinusLogProbMetric: 27.8559

Epoch 723: val_loss did not improve from 27.84310
196/196 - 38s - loss: 27.4208 - MinusLogProbMetric: 27.4208 - val_loss: 27.8559 - val_MinusLogProbMetric: 27.8559 - lr: 4.1667e-05 - 38s/epoch - 196ms/step
Epoch 724/1000
2023-10-25 09:31:06.724 
Epoch 724/1000 
	 loss: 27.4183, MinusLogProbMetric: 27.4183, val_loss: 27.9639, val_MinusLogProbMetric: 27.9639

Epoch 724: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4183 - MinusLogProbMetric: 27.4183 - val_loss: 27.9639 - val_MinusLogProbMetric: 27.9639 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 725/1000
2023-10-25 09:31:48.575 
Epoch 725/1000 
	 loss: 27.4221, MinusLogProbMetric: 27.4221, val_loss: 27.8542, val_MinusLogProbMetric: 27.8542

Epoch 725: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4221 - MinusLogProbMetric: 27.4221 - val_loss: 27.8542 - val_MinusLogProbMetric: 27.8542 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 726/1000
2023-10-25 09:32:30.380 
Epoch 726/1000 
	 loss: 27.4233, MinusLogProbMetric: 27.4233, val_loss: 27.8983, val_MinusLogProbMetric: 27.8983

Epoch 726: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4233 - MinusLogProbMetric: 27.4233 - val_loss: 27.8983 - val_MinusLogProbMetric: 27.8983 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 727/1000
2023-10-25 09:33:12.107 
Epoch 727/1000 
	 loss: 27.4232, MinusLogProbMetric: 27.4232, val_loss: 27.8924, val_MinusLogProbMetric: 27.8924

Epoch 727: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4232 - MinusLogProbMetric: 27.4232 - val_loss: 27.8924 - val_MinusLogProbMetric: 27.8924 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 728/1000
2023-10-25 09:33:54.069 
Epoch 728/1000 
	 loss: 27.4227, MinusLogProbMetric: 27.4227, val_loss: 27.8768, val_MinusLogProbMetric: 27.8768

Epoch 728: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4227 - MinusLogProbMetric: 27.4227 - val_loss: 27.8768 - val_MinusLogProbMetric: 27.8768 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 729/1000
2023-10-25 09:34:35.422 
Epoch 729/1000 
	 loss: 27.4246, MinusLogProbMetric: 27.4246, val_loss: 27.8888, val_MinusLogProbMetric: 27.8888

Epoch 729: val_loss did not improve from 27.84310
196/196 - 41s - loss: 27.4246 - MinusLogProbMetric: 27.4246 - val_loss: 27.8888 - val_MinusLogProbMetric: 27.8888 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 730/1000
2023-10-25 09:35:13.076 
Epoch 730/1000 
	 loss: 27.4235, MinusLogProbMetric: 27.4235, val_loss: 27.8585, val_MinusLogProbMetric: 27.8585

Epoch 730: val_loss did not improve from 27.84310
196/196 - 38s - loss: 27.4235 - MinusLogProbMetric: 27.4235 - val_loss: 27.8585 - val_MinusLogProbMetric: 27.8585 - lr: 4.1667e-05 - 38s/epoch - 192ms/step
Epoch 731/1000
2023-10-25 09:35:53.297 
Epoch 731/1000 
	 loss: 27.4366, MinusLogProbMetric: 27.4366, val_loss: 27.8585, val_MinusLogProbMetric: 27.8585

Epoch 731: val_loss did not improve from 27.84310
196/196 - 40s - loss: 27.4366 - MinusLogProbMetric: 27.4366 - val_loss: 27.8585 - val_MinusLogProbMetric: 27.8585 - lr: 4.1667e-05 - 40s/epoch - 205ms/step
Epoch 732/1000
2023-10-25 09:36:34.911 
Epoch 732/1000 
	 loss: 27.4147, MinusLogProbMetric: 27.4147, val_loss: 27.8538, val_MinusLogProbMetric: 27.8538

Epoch 732: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4147 - MinusLogProbMetric: 27.4147 - val_loss: 27.8538 - val_MinusLogProbMetric: 27.8538 - lr: 4.1667e-05 - 42s/epoch - 212ms/step
Epoch 733/1000
2023-10-25 09:37:15.350 
Epoch 733/1000 
	 loss: 27.4313, MinusLogProbMetric: 27.4313, val_loss: 27.8582, val_MinusLogProbMetric: 27.8582

Epoch 733: val_loss did not improve from 27.84310
196/196 - 40s - loss: 27.4313 - MinusLogProbMetric: 27.4313 - val_loss: 27.8582 - val_MinusLogProbMetric: 27.8582 - lr: 4.1667e-05 - 40s/epoch - 206ms/step
Epoch 734/1000
2023-10-25 09:37:57.703 
Epoch 734/1000 
	 loss: 27.4128, MinusLogProbMetric: 27.4128, val_loss: 27.8481, val_MinusLogProbMetric: 27.8481

Epoch 734: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4128 - MinusLogProbMetric: 27.4128 - val_loss: 27.8481 - val_MinusLogProbMetric: 27.8481 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 735/1000
2023-10-25 09:38:40.080 
Epoch 735/1000 
	 loss: 27.4178, MinusLogProbMetric: 27.4178, val_loss: 27.8934, val_MinusLogProbMetric: 27.8934

Epoch 735: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4178 - MinusLogProbMetric: 27.4178 - val_loss: 27.8934 - val_MinusLogProbMetric: 27.8934 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 736/1000
2023-10-25 09:39:22.057 
Epoch 736/1000 
	 loss: 27.4271, MinusLogProbMetric: 27.4271, val_loss: 27.8478, val_MinusLogProbMetric: 27.8478

Epoch 736: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4271 - MinusLogProbMetric: 27.4271 - val_loss: 27.8478 - val_MinusLogProbMetric: 27.8478 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 737/1000
2023-10-25 09:40:03.797 
Epoch 737/1000 
	 loss: 27.4137, MinusLogProbMetric: 27.4137, val_loss: 27.8809, val_MinusLogProbMetric: 27.8809

Epoch 737: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4137 - MinusLogProbMetric: 27.4137 - val_loss: 27.8809 - val_MinusLogProbMetric: 27.8809 - lr: 4.1667e-05 - 42s/epoch - 213ms/step
Epoch 738/1000
2023-10-25 09:40:46.150 
Epoch 738/1000 
	 loss: 27.4154, MinusLogProbMetric: 27.4154, val_loss: 27.8769, val_MinusLogProbMetric: 27.8769

Epoch 738: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4154 - MinusLogProbMetric: 27.4154 - val_loss: 27.8769 - val_MinusLogProbMetric: 27.8769 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 739/1000
2023-10-25 09:41:28.759 
Epoch 739/1000 
	 loss: 27.4180, MinusLogProbMetric: 27.4180, val_loss: 27.9106, val_MinusLogProbMetric: 27.9106

Epoch 739: val_loss did not improve from 27.84310
196/196 - 43s - loss: 27.4180 - MinusLogProbMetric: 27.4180 - val_loss: 27.9106 - val_MinusLogProbMetric: 27.9106 - lr: 4.1667e-05 - 43s/epoch - 217ms/step
Epoch 740/1000
2023-10-25 09:42:10.740 
Epoch 740/1000 
	 loss: 27.4210, MinusLogProbMetric: 27.4210, val_loss: 27.8599, val_MinusLogProbMetric: 27.8599

Epoch 740: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4210 - MinusLogProbMetric: 27.4210 - val_loss: 27.8599 - val_MinusLogProbMetric: 27.8599 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 741/1000
2023-10-25 09:42:51.366 
Epoch 741/1000 
	 loss: 27.4268, MinusLogProbMetric: 27.4268, val_loss: 28.0075, val_MinusLogProbMetric: 28.0075

Epoch 741: val_loss did not improve from 27.84310
196/196 - 41s - loss: 27.4268 - MinusLogProbMetric: 27.4268 - val_loss: 28.0075 - val_MinusLogProbMetric: 28.0075 - lr: 4.1667e-05 - 41s/epoch - 207ms/step
Epoch 742/1000
2023-10-25 09:43:32.574 
Epoch 742/1000 
	 loss: 27.4372, MinusLogProbMetric: 27.4372, val_loss: 27.9143, val_MinusLogProbMetric: 27.9143

Epoch 742: val_loss did not improve from 27.84310
196/196 - 41s - loss: 27.4372 - MinusLogProbMetric: 27.4372 - val_loss: 27.9143 - val_MinusLogProbMetric: 27.9143 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 743/1000
2023-10-25 09:44:14.942 
Epoch 743/1000 
	 loss: 27.4237, MinusLogProbMetric: 27.4237, val_loss: 27.9140, val_MinusLogProbMetric: 27.9140

Epoch 743: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4237 - MinusLogProbMetric: 27.4237 - val_loss: 27.9140 - val_MinusLogProbMetric: 27.9140 - lr: 4.1667e-05 - 42s/epoch - 216ms/step
Epoch 744/1000
2023-10-25 09:44:57.084 
Epoch 744/1000 
	 loss: 27.4227, MinusLogProbMetric: 27.4227, val_loss: 27.8941, val_MinusLogProbMetric: 27.8941

Epoch 744: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4227 - MinusLogProbMetric: 27.4227 - val_loss: 27.8941 - val_MinusLogProbMetric: 27.8941 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 745/1000
2023-10-25 09:45:39.242 
Epoch 745/1000 
	 loss: 27.4195, MinusLogProbMetric: 27.4195, val_loss: 27.9002, val_MinusLogProbMetric: 27.9002

Epoch 745: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4195 - MinusLogProbMetric: 27.4195 - val_loss: 27.9002 - val_MinusLogProbMetric: 27.9002 - lr: 4.1667e-05 - 42s/epoch - 215ms/step
Epoch 746/1000
2023-10-25 09:46:17.552 
Epoch 746/1000 
	 loss: 27.4185, MinusLogProbMetric: 27.4185, val_loss: 27.8679, val_MinusLogProbMetric: 27.8679

Epoch 746: val_loss did not improve from 27.84310
196/196 - 38s - loss: 27.4185 - MinusLogProbMetric: 27.4185 - val_loss: 27.8679 - val_MinusLogProbMetric: 27.8679 - lr: 4.1667e-05 - 38s/epoch - 195ms/step
Epoch 747/1000
2023-10-25 09:46:58.345 
Epoch 747/1000 
	 loss: 27.4195, MinusLogProbMetric: 27.4195, val_loss: 27.9544, val_MinusLogProbMetric: 27.9544

Epoch 747: val_loss did not improve from 27.84310
196/196 - 41s - loss: 27.4195 - MinusLogProbMetric: 27.4195 - val_loss: 27.9544 - val_MinusLogProbMetric: 27.9544 - lr: 4.1667e-05 - 41s/epoch - 208ms/step
Epoch 748/1000
2023-10-25 09:47:40.257 
Epoch 748/1000 
	 loss: 27.4288, MinusLogProbMetric: 27.4288, val_loss: 27.8611, val_MinusLogProbMetric: 27.8611

Epoch 748: val_loss did not improve from 27.84310
196/196 - 42s - loss: 27.4288 - MinusLogProbMetric: 27.4288 - val_loss: 27.8611 - val_MinusLogProbMetric: 27.8611 - lr: 4.1667e-05 - 42s/epoch - 214ms/step
Epoch 749/1000
2023-10-25 09:48:21.656 
Epoch 749/1000 
	 loss: 27.4282, MinusLogProbMetric: 27.4282, val_loss: 27.8989, val_MinusLogProbMetric: 27.8989

Epoch 749: val_loss did not improve from 27.84310
196/196 - 41s - loss: 27.4282 - MinusLogProbMetric: 27.4282 - val_loss: 27.8989 - val_MinusLogProbMetric: 27.8989 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 750/1000
2023-10-25 09:49:03.050 
Epoch 750/1000 
	 loss: 27.4280, MinusLogProbMetric: 27.4280, val_loss: 27.8618, val_MinusLogProbMetric: 27.8618

Epoch 750: val_loss did not improve from 27.84310
196/196 - 41s - loss: 27.4280 - MinusLogProbMetric: 27.4280 - val_loss: 27.8618 - val_MinusLogProbMetric: 27.8618 - lr: 4.1667e-05 - 41s/epoch - 211ms/step
Epoch 751/1000
2023-10-25 09:49:42.920 
Epoch 751/1000 
	 loss: 27.4258, MinusLogProbMetric: 27.4258, val_loss: 27.8870, val_MinusLogProbMetric: 27.8870

Epoch 751: val_loss did not improve from 27.84310
196/196 - 40s - loss: 27.4258 - MinusLogProbMetric: 27.4258 - val_loss: 27.8870 - val_MinusLogProbMetric: 27.8870 - lr: 4.1667e-05 - 40s/epoch - 203ms/step
Epoch 752/1000
2023-10-25 09:50:24.123 
Epoch 752/1000 
	 loss: 27.4163, MinusLogProbMetric: 27.4163, val_loss: 27.8766, val_MinusLogProbMetric: 27.8766

Epoch 752: val_loss did not improve from 27.84310
196/196 - 41s - loss: 27.4163 - MinusLogProbMetric: 27.4163 - val_loss: 27.8766 - val_MinusLogProbMetric: 27.8766 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 753/1000
2023-10-25 09:51:05.361 
Epoch 753/1000 
	 loss: 27.4262, MinusLogProbMetric: 27.4262, val_loss: 27.8715, val_MinusLogProbMetric: 27.8715

Epoch 753: val_loss did not improve from 27.84310
196/196 - 41s - loss: 27.4262 - MinusLogProbMetric: 27.4262 - val_loss: 27.8715 - val_MinusLogProbMetric: 27.8715 - lr: 4.1667e-05 - 41s/epoch - 210ms/step
Epoch 754/1000
2023-10-25 09:51:44.516 
Epoch 754/1000 
	 loss: 27.3854, MinusLogProbMetric: 27.3854, val_loss: 27.8275, val_MinusLogProbMetric: 27.8275

Epoch 754: val_loss improved from 27.84310 to 27.82750, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 40s - loss: 27.3854 - MinusLogProbMetric: 27.3854 - val_loss: 27.8275 - val_MinusLogProbMetric: 27.8275 - lr: 2.0833e-05 - 40s/epoch - 203ms/step
Epoch 755/1000
2023-10-25 09:52:27.152 
Epoch 755/1000 
	 loss: 27.3810, MinusLogProbMetric: 27.3810, val_loss: 27.8263, val_MinusLogProbMetric: 27.8263

Epoch 755: val_loss improved from 27.82750 to 27.82631, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 27.3810 - MinusLogProbMetric: 27.3810 - val_loss: 27.8263 - val_MinusLogProbMetric: 27.8263 - lr: 2.0833e-05 - 43s/epoch - 218ms/step
Epoch 756/1000
2023-10-25 09:53:10.159 
Epoch 756/1000 
	 loss: 27.3825, MinusLogProbMetric: 27.3825, val_loss: 27.8241, val_MinusLogProbMetric: 27.8241

Epoch 756: val_loss improved from 27.82631 to 27.82413, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 43s - loss: 27.3825 - MinusLogProbMetric: 27.3825 - val_loss: 27.8241 - val_MinusLogProbMetric: 27.8241 - lr: 2.0833e-05 - 43s/epoch - 220ms/step
Epoch 757/1000
2023-10-25 09:53:52.216 
Epoch 757/1000 
	 loss: 27.3764, MinusLogProbMetric: 27.3764, val_loss: 27.8269, val_MinusLogProbMetric: 27.8269

Epoch 757: val_loss did not improve from 27.82413
196/196 - 41s - loss: 27.3764 - MinusLogProbMetric: 27.3764 - val_loss: 27.8269 - val_MinusLogProbMetric: 27.8269 - lr: 2.0833e-05 - 41s/epoch - 211ms/step
Epoch 758/1000
2023-10-25 09:54:34.198 
Epoch 758/1000 
	 loss: 27.3812, MinusLogProbMetric: 27.3812, val_loss: 27.8297, val_MinusLogProbMetric: 27.8297

Epoch 758: val_loss did not improve from 27.82413
196/196 - 42s - loss: 27.3812 - MinusLogProbMetric: 27.3812 - val_loss: 27.8297 - val_MinusLogProbMetric: 27.8297 - lr: 2.0833e-05 - 42s/epoch - 214ms/step
Epoch 759/1000
2023-10-25 09:55:15.557 
Epoch 759/1000 
	 loss: 27.3743, MinusLogProbMetric: 27.3743, val_loss: 27.8583, val_MinusLogProbMetric: 27.8583

Epoch 759: val_loss did not improve from 27.82413
196/196 - 41s - loss: 27.3743 - MinusLogProbMetric: 27.3743 - val_loss: 27.8583 - val_MinusLogProbMetric: 27.8583 - lr: 2.0833e-05 - 41s/epoch - 211ms/step
Epoch 760/1000
2023-10-25 09:55:56.464 
Epoch 760/1000 
	 loss: 27.3781, MinusLogProbMetric: 27.3781, val_loss: 27.8363, val_MinusLogProbMetric: 27.8363

Epoch 760: val_loss did not improve from 27.82413
196/196 - 41s - loss: 27.3781 - MinusLogProbMetric: 27.3781 - val_loss: 27.8363 - val_MinusLogProbMetric: 27.8363 - lr: 2.0833e-05 - 41s/epoch - 209ms/step
Epoch 761/1000
2023-10-25 09:56:39.035 
Epoch 761/1000 
	 loss: 27.3780, MinusLogProbMetric: 27.3780, val_loss: 27.8390, val_MinusLogProbMetric: 27.8390

Epoch 761: val_loss did not improve from 27.82413
196/196 - 43s - loss: 27.3780 - MinusLogProbMetric: 27.3780 - val_loss: 27.8390 - val_MinusLogProbMetric: 27.8390 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 762/1000
2023-10-25 09:57:20.620 
Epoch 762/1000 
	 loss: 27.3785, MinusLogProbMetric: 27.3785, val_loss: 27.8463, val_MinusLogProbMetric: 27.8463

Epoch 762: val_loss did not improve from 27.82413
196/196 - 42s - loss: 27.3785 - MinusLogProbMetric: 27.3785 - val_loss: 27.8463 - val_MinusLogProbMetric: 27.8463 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 763/1000
2023-10-25 09:58:01.494 
Epoch 763/1000 
	 loss: 27.3838, MinusLogProbMetric: 27.3838, val_loss: 27.8367, val_MinusLogProbMetric: 27.8367

Epoch 763: val_loss did not improve from 27.82413
196/196 - 41s - loss: 27.3838 - MinusLogProbMetric: 27.3838 - val_loss: 27.8367 - val_MinusLogProbMetric: 27.8367 - lr: 2.0833e-05 - 41s/epoch - 209ms/step
Epoch 764/1000
2023-10-25 09:58:42.202 
Epoch 764/1000 
	 loss: 27.3787, MinusLogProbMetric: 27.3787, val_loss: 27.8818, val_MinusLogProbMetric: 27.8818

Epoch 764: val_loss did not improve from 27.82413
196/196 - 41s - loss: 27.3787 - MinusLogProbMetric: 27.3787 - val_loss: 27.8818 - val_MinusLogProbMetric: 27.8818 - lr: 2.0833e-05 - 41s/epoch - 208ms/step
Epoch 765/1000
2023-10-25 09:59:21.041 
Epoch 765/1000 
	 loss: 27.3806, MinusLogProbMetric: 27.3806, val_loss: 27.8271, val_MinusLogProbMetric: 27.8271

Epoch 765: val_loss did not improve from 27.82413
196/196 - 39s - loss: 27.3806 - MinusLogProbMetric: 27.3806 - val_loss: 27.8271 - val_MinusLogProbMetric: 27.8271 - lr: 2.0833e-05 - 39s/epoch - 198ms/step
Epoch 766/1000
2023-10-25 10:00:01.529 
Epoch 766/1000 
	 loss: 27.3805, MinusLogProbMetric: 27.3805, val_loss: 27.8358, val_MinusLogProbMetric: 27.8358

Epoch 766: val_loss did not improve from 27.82413
196/196 - 40s - loss: 27.3805 - MinusLogProbMetric: 27.3805 - val_loss: 27.8358 - val_MinusLogProbMetric: 27.8358 - lr: 2.0833e-05 - 40s/epoch - 207ms/step
Epoch 767/1000
2023-10-25 10:00:43.039 
Epoch 767/1000 
	 loss: 27.3833, MinusLogProbMetric: 27.3833, val_loss: 27.8301, val_MinusLogProbMetric: 27.8301

Epoch 767: val_loss did not improve from 27.82413
196/196 - 42s - loss: 27.3833 - MinusLogProbMetric: 27.3833 - val_loss: 27.8301 - val_MinusLogProbMetric: 27.8301 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 768/1000
2023-10-25 10:01:25.202 
Epoch 768/1000 
	 loss: 27.3848, MinusLogProbMetric: 27.3848, val_loss: 27.8458, val_MinusLogProbMetric: 27.8458

Epoch 768: val_loss did not improve from 27.82413
196/196 - 42s - loss: 27.3848 - MinusLogProbMetric: 27.3848 - val_loss: 27.8458 - val_MinusLogProbMetric: 27.8458 - lr: 2.0833e-05 - 42s/epoch - 215ms/step
Epoch 769/1000
2023-10-25 10:02:07.814 
Epoch 769/1000 
	 loss: 27.3743, MinusLogProbMetric: 27.3743, val_loss: 27.8437, val_MinusLogProbMetric: 27.8437

Epoch 769: val_loss did not improve from 27.82413
196/196 - 43s - loss: 27.3743 - MinusLogProbMetric: 27.3743 - val_loss: 27.8437 - val_MinusLogProbMetric: 27.8437 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 770/1000
2023-10-25 10:02:50.089 
Epoch 770/1000 
	 loss: 27.3784, MinusLogProbMetric: 27.3784, val_loss: 27.8416, val_MinusLogProbMetric: 27.8416

Epoch 770: val_loss did not improve from 27.82413
196/196 - 42s - loss: 27.3784 - MinusLogProbMetric: 27.3784 - val_loss: 27.8416 - val_MinusLogProbMetric: 27.8416 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 771/1000
2023-10-25 10:03:31.949 
Epoch 771/1000 
	 loss: 27.3821, MinusLogProbMetric: 27.3821, val_loss: 27.8306, val_MinusLogProbMetric: 27.8306

Epoch 771: val_loss did not improve from 27.82413
196/196 - 42s - loss: 27.3821 - MinusLogProbMetric: 27.3821 - val_loss: 27.8306 - val_MinusLogProbMetric: 27.8306 - lr: 2.0833e-05 - 42s/epoch - 214ms/step
Epoch 772/1000
2023-10-25 10:04:14.053 
Epoch 772/1000 
	 loss: 27.3851, MinusLogProbMetric: 27.3851, val_loss: 27.8422, val_MinusLogProbMetric: 27.8422

Epoch 772: val_loss did not improve from 27.82413
196/196 - 42s - loss: 27.3851 - MinusLogProbMetric: 27.3851 - val_loss: 27.8422 - val_MinusLogProbMetric: 27.8422 - lr: 2.0833e-05 - 42s/epoch - 215ms/step
Epoch 773/1000
2023-10-25 10:04:56.318 
Epoch 773/1000 
	 loss: 27.3826, MinusLogProbMetric: 27.3826, val_loss: 27.8483, val_MinusLogProbMetric: 27.8483

Epoch 773: val_loss did not improve from 27.82413
196/196 - 42s - loss: 27.3826 - MinusLogProbMetric: 27.3826 - val_loss: 27.8483 - val_MinusLogProbMetric: 27.8483 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 774/1000
2023-10-25 10:05:38.167 
Epoch 774/1000 
	 loss: 27.3837, MinusLogProbMetric: 27.3837, val_loss: 27.8414, val_MinusLogProbMetric: 27.8414

Epoch 774: val_loss did not improve from 27.82413
196/196 - 42s - loss: 27.3837 - MinusLogProbMetric: 27.3837 - val_loss: 27.8414 - val_MinusLogProbMetric: 27.8414 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 775/1000
2023-10-25 10:06:20.112 
Epoch 775/1000 
	 loss: 27.3790, MinusLogProbMetric: 27.3790, val_loss: 27.8505, val_MinusLogProbMetric: 27.8505

Epoch 775: val_loss did not improve from 27.82413
196/196 - 42s - loss: 27.3790 - MinusLogProbMetric: 27.3790 - val_loss: 27.8505 - val_MinusLogProbMetric: 27.8505 - lr: 2.0833e-05 - 42s/epoch - 214ms/step
Epoch 776/1000
2023-10-25 10:07:01.765 
Epoch 776/1000 
	 loss: 27.3795, MinusLogProbMetric: 27.3795, val_loss: 27.8394, val_MinusLogProbMetric: 27.8394

Epoch 776: val_loss did not improve from 27.82413
196/196 - 42s - loss: 27.3795 - MinusLogProbMetric: 27.3795 - val_loss: 27.8394 - val_MinusLogProbMetric: 27.8394 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 777/1000
2023-10-25 10:07:43.058 
Epoch 777/1000 
	 loss: 27.3807, MinusLogProbMetric: 27.3807, val_loss: 27.8440, val_MinusLogProbMetric: 27.8440

Epoch 777: val_loss did not improve from 27.82413
196/196 - 41s - loss: 27.3807 - MinusLogProbMetric: 27.3807 - val_loss: 27.8440 - val_MinusLogProbMetric: 27.8440 - lr: 2.0833e-05 - 41s/epoch - 211ms/step
Epoch 778/1000
2023-10-25 10:08:25.241 
Epoch 778/1000 
	 loss: 27.3764, MinusLogProbMetric: 27.3764, val_loss: 27.8361, val_MinusLogProbMetric: 27.8361

Epoch 778: val_loss did not improve from 27.82413
196/196 - 42s - loss: 27.3764 - MinusLogProbMetric: 27.3764 - val_loss: 27.8361 - val_MinusLogProbMetric: 27.8361 - lr: 2.0833e-05 - 42s/epoch - 215ms/step
Epoch 779/1000
2023-10-25 10:09:05.201 
Epoch 779/1000 
	 loss: 27.3798, MinusLogProbMetric: 27.3798, val_loss: 27.8569, val_MinusLogProbMetric: 27.8569

Epoch 779: val_loss did not improve from 27.82413
196/196 - 40s - loss: 27.3798 - MinusLogProbMetric: 27.3798 - val_loss: 27.8569 - val_MinusLogProbMetric: 27.8569 - lr: 2.0833e-05 - 40s/epoch - 204ms/step
Epoch 780/1000
2023-10-25 10:09:41.347 
Epoch 780/1000 
	 loss: 27.3842, MinusLogProbMetric: 27.3842, val_loss: 27.8229, val_MinusLogProbMetric: 27.8229

Epoch 780: val_loss improved from 27.82413 to 27.82291, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 37s - loss: 27.3842 - MinusLogProbMetric: 27.3842 - val_loss: 27.8229 - val_MinusLogProbMetric: 27.8229 - lr: 2.0833e-05 - 37s/epoch - 187ms/step
Epoch 781/1000
2023-10-25 10:10:18.206 
Epoch 781/1000 
	 loss: 27.3780, MinusLogProbMetric: 27.3780, val_loss: 27.8554, val_MinusLogProbMetric: 27.8554

Epoch 781: val_loss did not improve from 27.82291
196/196 - 36s - loss: 27.3780 - MinusLogProbMetric: 27.3780 - val_loss: 27.8554 - val_MinusLogProbMetric: 27.8554 - lr: 2.0833e-05 - 36s/epoch - 185ms/step
Epoch 782/1000
2023-10-25 10:10:54.742 
Epoch 782/1000 
	 loss: 27.3809, MinusLogProbMetric: 27.3809, val_loss: 27.8292, val_MinusLogProbMetric: 27.8292

Epoch 782: val_loss did not improve from 27.82291
196/196 - 37s - loss: 27.3809 - MinusLogProbMetric: 27.3809 - val_loss: 27.8292 - val_MinusLogProbMetric: 27.8292 - lr: 2.0833e-05 - 37s/epoch - 186ms/step
Epoch 783/1000
2023-10-25 10:11:33.201 
Epoch 783/1000 
	 loss: 27.3795, MinusLogProbMetric: 27.3795, val_loss: 27.8596, val_MinusLogProbMetric: 27.8596

Epoch 783: val_loss did not improve from 27.82291
196/196 - 38s - loss: 27.3795 - MinusLogProbMetric: 27.3795 - val_loss: 27.8596 - val_MinusLogProbMetric: 27.8596 - lr: 2.0833e-05 - 38s/epoch - 196ms/step
Epoch 784/1000
2023-10-25 10:12:15.781 
Epoch 784/1000 
	 loss: 27.3829, MinusLogProbMetric: 27.3829, val_loss: 27.8837, val_MinusLogProbMetric: 27.8837

Epoch 784: val_loss did not improve from 27.82291
196/196 - 43s - loss: 27.3829 - MinusLogProbMetric: 27.3829 - val_loss: 27.8837 - val_MinusLogProbMetric: 27.8837 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 785/1000
2023-10-25 10:12:56.340 
Epoch 785/1000 
	 loss: 27.3841, MinusLogProbMetric: 27.3841, val_loss: 27.8352, val_MinusLogProbMetric: 27.8352

Epoch 785: val_loss did not improve from 27.82291
196/196 - 41s - loss: 27.3841 - MinusLogProbMetric: 27.3841 - val_loss: 27.8352 - val_MinusLogProbMetric: 27.8352 - lr: 2.0833e-05 - 41s/epoch - 207ms/step
Epoch 786/1000
2023-10-25 10:13:38.142 
Epoch 786/1000 
	 loss: 27.3776, MinusLogProbMetric: 27.3776, val_loss: 27.8728, val_MinusLogProbMetric: 27.8728

Epoch 786: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3776 - MinusLogProbMetric: 27.3776 - val_loss: 27.8728 - val_MinusLogProbMetric: 27.8728 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 787/1000
2023-10-25 10:14:19.772 
Epoch 787/1000 
	 loss: 27.3783, MinusLogProbMetric: 27.3783, val_loss: 27.8274, val_MinusLogProbMetric: 27.8274

Epoch 787: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3783 - MinusLogProbMetric: 27.3783 - val_loss: 27.8274 - val_MinusLogProbMetric: 27.8274 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 788/1000
2023-10-25 10:15:01.326 
Epoch 788/1000 
	 loss: 27.3792, MinusLogProbMetric: 27.3792, val_loss: 27.8305, val_MinusLogProbMetric: 27.8305

Epoch 788: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3792 - MinusLogProbMetric: 27.3792 - val_loss: 27.8305 - val_MinusLogProbMetric: 27.8305 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 789/1000
2023-10-25 10:15:42.341 
Epoch 789/1000 
	 loss: 27.3817, MinusLogProbMetric: 27.3817, val_loss: 27.8571, val_MinusLogProbMetric: 27.8571

Epoch 789: val_loss did not improve from 27.82291
196/196 - 41s - loss: 27.3817 - MinusLogProbMetric: 27.3817 - val_loss: 27.8571 - val_MinusLogProbMetric: 27.8571 - lr: 2.0833e-05 - 41s/epoch - 209ms/step
Epoch 790/1000
2023-10-25 10:16:24.477 
Epoch 790/1000 
	 loss: 27.3783, MinusLogProbMetric: 27.3783, val_loss: 27.8385, val_MinusLogProbMetric: 27.8385

Epoch 790: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3783 - MinusLogProbMetric: 27.3783 - val_loss: 27.8385 - val_MinusLogProbMetric: 27.8385 - lr: 2.0833e-05 - 42s/epoch - 215ms/step
Epoch 791/1000
2023-10-25 10:17:06.811 
Epoch 791/1000 
	 loss: 27.3784, MinusLogProbMetric: 27.3784, val_loss: 27.8282, val_MinusLogProbMetric: 27.8282

Epoch 791: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3784 - MinusLogProbMetric: 27.3784 - val_loss: 27.8282 - val_MinusLogProbMetric: 27.8282 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 792/1000
2023-10-25 10:17:49.020 
Epoch 792/1000 
	 loss: 27.3754, MinusLogProbMetric: 27.3754, val_loss: 27.8493, val_MinusLogProbMetric: 27.8493

Epoch 792: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3754 - MinusLogProbMetric: 27.3754 - val_loss: 27.8493 - val_MinusLogProbMetric: 27.8493 - lr: 2.0833e-05 - 42s/epoch - 215ms/step
Epoch 793/1000
2023-10-25 10:18:30.695 
Epoch 793/1000 
	 loss: 27.3804, MinusLogProbMetric: 27.3804, val_loss: 27.8307, val_MinusLogProbMetric: 27.8307

Epoch 793: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3804 - MinusLogProbMetric: 27.3804 - val_loss: 27.8307 - val_MinusLogProbMetric: 27.8307 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 794/1000
2023-10-25 10:19:12.799 
Epoch 794/1000 
	 loss: 27.3774, MinusLogProbMetric: 27.3774, val_loss: 27.8254, val_MinusLogProbMetric: 27.8254

Epoch 794: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3774 - MinusLogProbMetric: 27.3774 - val_loss: 27.8254 - val_MinusLogProbMetric: 27.8254 - lr: 2.0833e-05 - 42s/epoch - 215ms/step
Epoch 795/1000
2023-10-25 10:19:55.207 
Epoch 795/1000 
	 loss: 27.3733, MinusLogProbMetric: 27.3733, val_loss: 27.8310, val_MinusLogProbMetric: 27.8310

Epoch 795: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3733 - MinusLogProbMetric: 27.3733 - val_loss: 27.8310 - val_MinusLogProbMetric: 27.8310 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 796/1000
2023-10-25 10:20:37.584 
Epoch 796/1000 
	 loss: 27.3787, MinusLogProbMetric: 27.3787, val_loss: 27.8428, val_MinusLogProbMetric: 27.8428

Epoch 796: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3787 - MinusLogProbMetric: 27.3787 - val_loss: 27.8428 - val_MinusLogProbMetric: 27.8428 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 797/1000
2023-10-25 10:21:20.124 
Epoch 797/1000 
	 loss: 27.3783, MinusLogProbMetric: 27.3783, val_loss: 27.8346, val_MinusLogProbMetric: 27.8346

Epoch 797: val_loss did not improve from 27.82291
196/196 - 43s - loss: 27.3783 - MinusLogProbMetric: 27.3783 - val_loss: 27.8346 - val_MinusLogProbMetric: 27.8346 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 798/1000
2023-10-25 10:22:01.825 
Epoch 798/1000 
	 loss: 27.3835, MinusLogProbMetric: 27.3835, val_loss: 27.8557, val_MinusLogProbMetric: 27.8557

Epoch 798: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3835 - MinusLogProbMetric: 27.3835 - val_loss: 27.8557 - val_MinusLogProbMetric: 27.8557 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 799/1000
2023-10-25 10:22:43.488 
Epoch 799/1000 
	 loss: 27.3803, MinusLogProbMetric: 27.3803, val_loss: 27.8432, val_MinusLogProbMetric: 27.8432

Epoch 799: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3803 - MinusLogProbMetric: 27.3803 - val_loss: 27.8432 - val_MinusLogProbMetric: 27.8432 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 800/1000
2023-10-25 10:23:25.061 
Epoch 800/1000 
	 loss: 27.3733, MinusLogProbMetric: 27.3733, val_loss: 27.8467, val_MinusLogProbMetric: 27.8467

Epoch 800: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3733 - MinusLogProbMetric: 27.3733 - val_loss: 27.8467 - val_MinusLogProbMetric: 27.8467 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 801/1000
2023-10-25 10:24:06.770 
Epoch 801/1000 
	 loss: 27.3787, MinusLogProbMetric: 27.3787, val_loss: 27.8317, val_MinusLogProbMetric: 27.8317

Epoch 801: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3787 - MinusLogProbMetric: 27.3787 - val_loss: 27.8317 - val_MinusLogProbMetric: 27.8317 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 802/1000
2023-10-25 10:24:48.295 
Epoch 802/1000 
	 loss: 27.3778, MinusLogProbMetric: 27.3778, val_loss: 27.8363, val_MinusLogProbMetric: 27.8363

Epoch 802: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3778 - MinusLogProbMetric: 27.3778 - val_loss: 27.8363 - val_MinusLogProbMetric: 27.8363 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 803/1000
2023-10-25 10:25:27.871 
Epoch 803/1000 
	 loss: 27.3786, MinusLogProbMetric: 27.3786, val_loss: 27.8468, val_MinusLogProbMetric: 27.8468

Epoch 803: val_loss did not improve from 27.82291
196/196 - 40s - loss: 27.3786 - MinusLogProbMetric: 27.3786 - val_loss: 27.8468 - val_MinusLogProbMetric: 27.8468 - lr: 2.0833e-05 - 40s/epoch - 202ms/step
Epoch 804/1000
2023-10-25 10:26:05.584 
Epoch 804/1000 
	 loss: 27.3824, MinusLogProbMetric: 27.3824, val_loss: 27.8597, val_MinusLogProbMetric: 27.8597

Epoch 804: val_loss did not improve from 27.82291
196/196 - 38s - loss: 27.3824 - MinusLogProbMetric: 27.3824 - val_loss: 27.8597 - val_MinusLogProbMetric: 27.8597 - lr: 2.0833e-05 - 38s/epoch - 192ms/step
Epoch 805/1000
2023-10-25 10:26:45.651 
Epoch 805/1000 
	 loss: 27.3802, MinusLogProbMetric: 27.3802, val_loss: 27.8462, val_MinusLogProbMetric: 27.8462

Epoch 805: val_loss did not improve from 27.82291
196/196 - 40s - loss: 27.3802 - MinusLogProbMetric: 27.3802 - val_loss: 27.8462 - val_MinusLogProbMetric: 27.8462 - lr: 2.0833e-05 - 40s/epoch - 204ms/step
Epoch 806/1000
2023-10-25 10:27:24.355 
Epoch 806/1000 
	 loss: 27.3787, MinusLogProbMetric: 27.3787, val_loss: 27.8353, val_MinusLogProbMetric: 27.8353

Epoch 806: val_loss did not improve from 27.82291
196/196 - 39s - loss: 27.3787 - MinusLogProbMetric: 27.3787 - val_loss: 27.8353 - val_MinusLogProbMetric: 27.8353 - lr: 2.0833e-05 - 39s/epoch - 197ms/step
Epoch 807/1000
2023-10-25 10:28:06.351 
Epoch 807/1000 
	 loss: 27.3738, MinusLogProbMetric: 27.3738, val_loss: 27.8410, val_MinusLogProbMetric: 27.8410

Epoch 807: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3738 - MinusLogProbMetric: 27.3738 - val_loss: 27.8410 - val_MinusLogProbMetric: 27.8410 - lr: 2.0833e-05 - 42s/epoch - 214ms/step
Epoch 808/1000
2023-10-25 10:28:48.521 
Epoch 808/1000 
	 loss: 27.3822, MinusLogProbMetric: 27.3822, val_loss: 27.8314, val_MinusLogProbMetric: 27.8314

Epoch 808: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3822 - MinusLogProbMetric: 27.3822 - val_loss: 27.8314 - val_MinusLogProbMetric: 27.8314 - lr: 2.0833e-05 - 42s/epoch - 215ms/step
Epoch 809/1000
2023-10-25 10:29:31.030 
Epoch 809/1000 
	 loss: 27.3817, MinusLogProbMetric: 27.3817, val_loss: 27.8253, val_MinusLogProbMetric: 27.8253

Epoch 809: val_loss did not improve from 27.82291
196/196 - 43s - loss: 27.3817 - MinusLogProbMetric: 27.3817 - val_loss: 27.8253 - val_MinusLogProbMetric: 27.8253 - lr: 2.0833e-05 - 43s/epoch - 217ms/step
Epoch 810/1000
2023-10-25 10:30:12.595 
Epoch 810/1000 
	 loss: 27.3879, MinusLogProbMetric: 27.3879, val_loss: 27.8309, val_MinusLogProbMetric: 27.8309

Epoch 810: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3879 - MinusLogProbMetric: 27.3879 - val_loss: 27.8309 - val_MinusLogProbMetric: 27.8309 - lr: 2.0833e-05 - 42s/epoch - 212ms/step
Epoch 811/1000
2023-10-25 10:30:52.966 
Epoch 811/1000 
	 loss: 27.3833, MinusLogProbMetric: 27.3833, val_loss: 27.8491, val_MinusLogProbMetric: 27.8491

Epoch 811: val_loss did not improve from 27.82291
196/196 - 40s - loss: 27.3833 - MinusLogProbMetric: 27.3833 - val_loss: 27.8491 - val_MinusLogProbMetric: 27.8491 - lr: 2.0833e-05 - 40s/epoch - 206ms/step
Epoch 812/1000
2023-10-25 10:31:33.971 
Epoch 812/1000 
	 loss: 27.3771, MinusLogProbMetric: 27.3771, val_loss: 27.8314, val_MinusLogProbMetric: 27.8314

Epoch 812: val_loss did not improve from 27.82291
196/196 - 41s - loss: 27.3771 - MinusLogProbMetric: 27.3771 - val_loss: 27.8314 - val_MinusLogProbMetric: 27.8314 - lr: 2.0833e-05 - 41s/epoch - 209ms/step
Epoch 813/1000
2023-10-25 10:32:16.334 
Epoch 813/1000 
	 loss: 27.3750, MinusLogProbMetric: 27.3750, val_loss: 27.8358, val_MinusLogProbMetric: 27.8358

Epoch 813: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3750 - MinusLogProbMetric: 27.3750 - val_loss: 27.8358 - val_MinusLogProbMetric: 27.8358 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 814/1000
2023-10-25 10:32:56.693 
Epoch 814/1000 
	 loss: 27.3720, MinusLogProbMetric: 27.3720, val_loss: 27.8481, val_MinusLogProbMetric: 27.8481

Epoch 814: val_loss did not improve from 27.82291
196/196 - 40s - loss: 27.3720 - MinusLogProbMetric: 27.3720 - val_loss: 27.8481 - val_MinusLogProbMetric: 27.8481 - lr: 2.0833e-05 - 40s/epoch - 206ms/step
Epoch 815/1000
2023-10-25 10:33:37.419 
Epoch 815/1000 
	 loss: 27.3751, MinusLogProbMetric: 27.3751, val_loss: 27.8516, val_MinusLogProbMetric: 27.8516

Epoch 815: val_loss did not improve from 27.82291
196/196 - 41s - loss: 27.3751 - MinusLogProbMetric: 27.3751 - val_loss: 27.8516 - val_MinusLogProbMetric: 27.8516 - lr: 2.0833e-05 - 41s/epoch - 208ms/step
Epoch 816/1000
2023-10-25 10:34:15.000 
Epoch 816/1000 
	 loss: 27.3758, MinusLogProbMetric: 27.3758, val_loss: 27.8335, val_MinusLogProbMetric: 27.8335

Epoch 816: val_loss did not improve from 27.82291
196/196 - 38s - loss: 27.3758 - MinusLogProbMetric: 27.3758 - val_loss: 27.8335 - val_MinusLogProbMetric: 27.8335 - lr: 2.0833e-05 - 38s/epoch - 192ms/step
Epoch 817/1000
2023-10-25 10:34:55.066 
Epoch 817/1000 
	 loss: 27.3820, MinusLogProbMetric: 27.3820, val_loss: 27.8395, val_MinusLogProbMetric: 27.8395

Epoch 817: val_loss did not improve from 27.82291
196/196 - 40s - loss: 27.3820 - MinusLogProbMetric: 27.3820 - val_loss: 27.8395 - val_MinusLogProbMetric: 27.8395 - lr: 2.0833e-05 - 40s/epoch - 204ms/step
Epoch 818/1000
2023-10-25 10:35:36.827 
Epoch 818/1000 
	 loss: 27.3773, MinusLogProbMetric: 27.3773, val_loss: 27.8639, val_MinusLogProbMetric: 27.8639

Epoch 818: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3773 - MinusLogProbMetric: 27.3773 - val_loss: 27.8639 - val_MinusLogProbMetric: 27.8639 - lr: 2.0833e-05 - 42s/epoch - 213ms/step
Epoch 819/1000
2023-10-25 10:36:15.473 
Epoch 819/1000 
	 loss: 27.3829, MinusLogProbMetric: 27.3829, val_loss: 27.8336, val_MinusLogProbMetric: 27.8336

Epoch 819: val_loss did not improve from 27.82291
196/196 - 39s - loss: 27.3829 - MinusLogProbMetric: 27.3829 - val_loss: 27.8336 - val_MinusLogProbMetric: 27.8336 - lr: 2.0833e-05 - 39s/epoch - 197ms/step
Epoch 820/1000
2023-10-25 10:36:57.677 
Epoch 820/1000 
	 loss: 27.3791, MinusLogProbMetric: 27.3791, val_loss: 27.8346, val_MinusLogProbMetric: 27.8346

Epoch 820: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3791 - MinusLogProbMetric: 27.3791 - val_loss: 27.8346 - val_MinusLogProbMetric: 27.8346 - lr: 2.0833e-05 - 42s/epoch - 215ms/step
Epoch 821/1000
2023-10-25 10:37:37.243 
Epoch 821/1000 
	 loss: 27.3781, MinusLogProbMetric: 27.3781, val_loss: 27.8537, val_MinusLogProbMetric: 27.8537

Epoch 821: val_loss did not improve from 27.82291
196/196 - 40s - loss: 27.3781 - MinusLogProbMetric: 27.3781 - val_loss: 27.8537 - val_MinusLogProbMetric: 27.8537 - lr: 2.0833e-05 - 40s/epoch - 202ms/step
Epoch 822/1000
2023-10-25 10:38:16.226 
Epoch 822/1000 
	 loss: 27.3828, MinusLogProbMetric: 27.3828, val_loss: 27.8702, val_MinusLogProbMetric: 27.8702

Epoch 822: val_loss did not improve from 27.82291
196/196 - 39s - loss: 27.3828 - MinusLogProbMetric: 27.3828 - val_loss: 27.8702 - val_MinusLogProbMetric: 27.8702 - lr: 2.0833e-05 - 39s/epoch - 199ms/step
Epoch 823/1000
2023-10-25 10:38:56.389 
Epoch 823/1000 
	 loss: 27.3829, MinusLogProbMetric: 27.3829, val_loss: 27.8386, val_MinusLogProbMetric: 27.8386

Epoch 823: val_loss did not improve from 27.82291
196/196 - 40s - loss: 27.3829 - MinusLogProbMetric: 27.3829 - val_loss: 27.8386 - val_MinusLogProbMetric: 27.8386 - lr: 2.0833e-05 - 40s/epoch - 205ms/step
Epoch 824/1000
2023-10-25 10:39:36.608 
Epoch 824/1000 
	 loss: 27.3781, MinusLogProbMetric: 27.3781, val_loss: 27.8637, val_MinusLogProbMetric: 27.8637

Epoch 824: val_loss did not improve from 27.82291
196/196 - 40s - loss: 27.3781 - MinusLogProbMetric: 27.3781 - val_loss: 27.8637 - val_MinusLogProbMetric: 27.8637 - lr: 2.0833e-05 - 40s/epoch - 205ms/step
Epoch 825/1000
2023-10-25 10:40:16.724 
Epoch 825/1000 
	 loss: 27.3734, MinusLogProbMetric: 27.3734, val_loss: 27.8322, val_MinusLogProbMetric: 27.8322

Epoch 825: val_loss did not improve from 27.82291
196/196 - 40s - loss: 27.3734 - MinusLogProbMetric: 27.3734 - val_loss: 27.8322 - val_MinusLogProbMetric: 27.8322 - lr: 2.0833e-05 - 40s/epoch - 205ms/step
Epoch 826/1000
2023-10-25 10:40:56.791 
Epoch 826/1000 
	 loss: 27.3775, MinusLogProbMetric: 27.3775, val_loss: 27.8889, val_MinusLogProbMetric: 27.8889

Epoch 826: val_loss did not improve from 27.82291
196/196 - 40s - loss: 27.3775 - MinusLogProbMetric: 27.3775 - val_loss: 27.8889 - val_MinusLogProbMetric: 27.8889 - lr: 2.0833e-05 - 40s/epoch - 204ms/step
Epoch 827/1000
2023-10-25 10:41:39.190 
Epoch 827/1000 
	 loss: 27.3885, MinusLogProbMetric: 27.3885, val_loss: 27.8577, val_MinusLogProbMetric: 27.8577

Epoch 827: val_loss did not improve from 27.82291
196/196 - 42s - loss: 27.3885 - MinusLogProbMetric: 27.3885 - val_loss: 27.8577 - val_MinusLogProbMetric: 27.8577 - lr: 2.0833e-05 - 42s/epoch - 216ms/step
Epoch 828/1000
2023-10-25 10:42:20.522 
Epoch 828/1000 
	 loss: 27.3818, MinusLogProbMetric: 27.3818, val_loss: 27.8298, val_MinusLogProbMetric: 27.8298

Epoch 828: val_loss did not improve from 27.82291
196/196 - 41s - loss: 27.3818 - MinusLogProbMetric: 27.3818 - val_loss: 27.8298 - val_MinusLogProbMetric: 27.8298 - lr: 2.0833e-05 - 41s/epoch - 211ms/step
Epoch 829/1000
2023-10-25 10:42:57.920 
Epoch 829/1000 
	 loss: 27.3794, MinusLogProbMetric: 27.3794, val_loss: 27.8377, val_MinusLogProbMetric: 27.8377

Epoch 829: val_loss did not improve from 27.82291
196/196 - 37s - loss: 27.3794 - MinusLogProbMetric: 27.3794 - val_loss: 27.8377 - val_MinusLogProbMetric: 27.8377 - lr: 2.0833e-05 - 37s/epoch - 191ms/step
Epoch 830/1000
2023-10-25 10:43:35.461 
Epoch 830/1000 
	 loss: 27.3837, MinusLogProbMetric: 27.3837, val_loss: 27.8558, val_MinusLogProbMetric: 27.8558

Epoch 830: val_loss did not improve from 27.82291
196/196 - 38s - loss: 27.3837 - MinusLogProbMetric: 27.3837 - val_loss: 27.8558 - val_MinusLogProbMetric: 27.8558 - lr: 2.0833e-05 - 38s/epoch - 192ms/step
Epoch 831/1000
2023-10-25 10:44:16.789 
Epoch 831/1000 
	 loss: 27.3621, MinusLogProbMetric: 27.3621, val_loss: 27.8214, val_MinusLogProbMetric: 27.8214

Epoch 831: val_loss improved from 27.82291 to 27.82137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 27.3621 - MinusLogProbMetric: 27.3621 - val_loss: 27.8214 - val_MinusLogProbMetric: 27.8214 - lr: 1.0417e-05 - 42s/epoch - 214ms/step
Epoch 832/1000
2023-10-25 10:44:56.769 
Epoch 832/1000 
	 loss: 27.3586, MinusLogProbMetric: 27.3586, val_loss: 27.8274, val_MinusLogProbMetric: 27.8274

Epoch 832: val_loss did not improve from 27.82137
196/196 - 39s - loss: 27.3586 - MinusLogProbMetric: 27.3586 - val_loss: 27.8274 - val_MinusLogProbMetric: 27.8274 - lr: 1.0417e-05 - 39s/epoch - 200ms/step
Epoch 833/1000
2023-10-25 10:45:35.592 
Epoch 833/1000 
	 loss: 27.3578, MinusLogProbMetric: 27.3578, val_loss: 27.8195, val_MinusLogProbMetric: 27.8195

Epoch 833: val_loss improved from 27.82137 to 27.81954, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 39s - loss: 27.3578 - MinusLogProbMetric: 27.3578 - val_loss: 27.8195 - val_MinusLogProbMetric: 27.8195 - lr: 1.0417e-05 - 39s/epoch - 202ms/step
Epoch 834/1000
2023-10-25 10:46:14.792 
Epoch 834/1000 
	 loss: 27.3556, MinusLogProbMetric: 27.3556, val_loss: 27.8403, val_MinusLogProbMetric: 27.8403

Epoch 834: val_loss did not improve from 27.81954
196/196 - 39s - loss: 27.3556 - MinusLogProbMetric: 27.3556 - val_loss: 27.8403 - val_MinusLogProbMetric: 27.8403 - lr: 1.0417e-05 - 39s/epoch - 197ms/step
Epoch 835/1000
2023-10-25 10:46:51.971 
Epoch 835/1000 
	 loss: 27.3581, MinusLogProbMetric: 27.3581, val_loss: 27.8164, val_MinusLogProbMetric: 27.8164

Epoch 835: val_loss improved from 27.81954 to 27.81641, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 38s - loss: 27.3581 - MinusLogProbMetric: 27.3581 - val_loss: 27.8164 - val_MinusLogProbMetric: 27.8164 - lr: 1.0417e-05 - 38s/epoch - 193ms/step
Epoch 836/1000
2023-10-25 10:47:31.163 
Epoch 836/1000 
	 loss: 27.3567, MinusLogProbMetric: 27.3567, val_loss: 27.8196, val_MinusLogProbMetric: 27.8196

Epoch 836: val_loss did not improve from 27.81641
196/196 - 39s - loss: 27.3567 - MinusLogProbMetric: 27.3567 - val_loss: 27.8196 - val_MinusLogProbMetric: 27.8196 - lr: 1.0417e-05 - 39s/epoch - 197ms/step
Epoch 837/1000
2023-10-25 10:48:10.128 
Epoch 837/1000 
	 loss: 27.3547, MinusLogProbMetric: 27.3547, val_loss: 27.8194, val_MinusLogProbMetric: 27.8194

Epoch 837: val_loss did not improve from 27.81641
196/196 - 39s - loss: 27.3547 - MinusLogProbMetric: 27.3547 - val_loss: 27.8194 - val_MinusLogProbMetric: 27.8194 - lr: 1.0417e-05 - 39s/epoch - 199ms/step
Epoch 838/1000
2023-10-25 10:48:50.356 
Epoch 838/1000 
	 loss: 27.3581, MinusLogProbMetric: 27.3581, val_loss: 27.8204, val_MinusLogProbMetric: 27.8204

Epoch 838: val_loss did not improve from 27.81641
196/196 - 40s - loss: 27.3581 - MinusLogProbMetric: 27.3581 - val_loss: 27.8204 - val_MinusLogProbMetric: 27.8204 - lr: 1.0417e-05 - 40s/epoch - 205ms/step
Epoch 839/1000
2023-10-25 10:49:30.551 
Epoch 839/1000 
	 loss: 27.3625, MinusLogProbMetric: 27.3625, val_loss: 27.8325, val_MinusLogProbMetric: 27.8325

Epoch 839: val_loss did not improve from 27.81641
196/196 - 40s - loss: 27.3625 - MinusLogProbMetric: 27.3625 - val_loss: 27.8325 - val_MinusLogProbMetric: 27.8325 - lr: 1.0417e-05 - 40s/epoch - 205ms/step
Epoch 840/1000
2023-10-25 10:50:09.495 
Epoch 840/1000 
	 loss: 27.3561, MinusLogProbMetric: 27.3561, val_loss: 27.8218, val_MinusLogProbMetric: 27.8218

Epoch 840: val_loss did not improve from 27.81641
196/196 - 39s - loss: 27.3561 - MinusLogProbMetric: 27.3561 - val_loss: 27.8218 - val_MinusLogProbMetric: 27.8218 - lr: 1.0417e-05 - 39s/epoch - 199ms/step
Epoch 841/1000
2023-10-25 10:50:49.812 
Epoch 841/1000 
	 loss: 27.3576, MinusLogProbMetric: 27.3576, val_loss: 27.8333, val_MinusLogProbMetric: 27.8333

Epoch 841: val_loss did not improve from 27.81641
196/196 - 40s - loss: 27.3576 - MinusLogProbMetric: 27.3576 - val_loss: 27.8333 - val_MinusLogProbMetric: 27.8333 - lr: 1.0417e-05 - 40s/epoch - 206ms/step
Epoch 842/1000
2023-10-25 10:51:30.392 
Epoch 842/1000 
	 loss: 27.3592, MinusLogProbMetric: 27.3592, val_loss: 27.8597, val_MinusLogProbMetric: 27.8597

Epoch 842: val_loss did not improve from 27.81641
196/196 - 41s - loss: 27.3592 - MinusLogProbMetric: 27.3592 - val_loss: 27.8597 - val_MinusLogProbMetric: 27.8597 - lr: 1.0417e-05 - 41s/epoch - 207ms/step
Epoch 843/1000
2023-10-25 10:52:11.586 
Epoch 843/1000 
	 loss: 27.3576, MinusLogProbMetric: 27.3576, val_loss: 27.8200, val_MinusLogProbMetric: 27.8200

Epoch 843: val_loss did not improve from 27.81641
196/196 - 41s - loss: 27.3576 - MinusLogProbMetric: 27.3576 - val_loss: 27.8200 - val_MinusLogProbMetric: 27.8200 - lr: 1.0417e-05 - 41s/epoch - 210ms/step
Epoch 844/1000
2023-10-25 10:52:51.212 
Epoch 844/1000 
	 loss: 27.3573, MinusLogProbMetric: 27.3573, val_loss: 27.8231, val_MinusLogProbMetric: 27.8231

Epoch 844: val_loss did not improve from 27.81641
196/196 - 40s - loss: 27.3573 - MinusLogProbMetric: 27.3573 - val_loss: 27.8231 - val_MinusLogProbMetric: 27.8231 - lr: 1.0417e-05 - 40s/epoch - 202ms/step
Epoch 845/1000
2023-10-25 10:53:28.291 
Epoch 845/1000 
	 loss: 27.3575, MinusLogProbMetric: 27.3575, val_loss: 27.8208, val_MinusLogProbMetric: 27.8208

Epoch 845: val_loss did not improve from 27.81641
196/196 - 37s - loss: 27.3575 - MinusLogProbMetric: 27.3575 - val_loss: 27.8208 - val_MinusLogProbMetric: 27.8208 - lr: 1.0417e-05 - 37s/epoch - 189ms/step
Epoch 846/1000
2023-10-25 10:54:07.279 
Epoch 846/1000 
	 loss: 27.3577, MinusLogProbMetric: 27.3577, val_loss: 27.8155, val_MinusLogProbMetric: 27.8155

Epoch 846: val_loss improved from 27.81641 to 27.81546, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 40s - loss: 27.3577 - MinusLogProbMetric: 27.3577 - val_loss: 27.8155 - val_MinusLogProbMetric: 27.8155 - lr: 1.0417e-05 - 40s/epoch - 202ms/step
Epoch 847/1000
2023-10-25 10:54:46.163 
Epoch 847/1000 
	 loss: 27.3572, MinusLogProbMetric: 27.3572, val_loss: 27.8208, val_MinusLogProbMetric: 27.8208

Epoch 847: val_loss did not improve from 27.81546
196/196 - 38s - loss: 27.3572 - MinusLogProbMetric: 27.3572 - val_loss: 27.8208 - val_MinusLogProbMetric: 27.8208 - lr: 1.0417e-05 - 38s/epoch - 195ms/step
Epoch 848/1000
2023-10-25 10:55:28.140 
Epoch 848/1000 
	 loss: 27.3568, MinusLogProbMetric: 27.3568, val_loss: 27.8275, val_MinusLogProbMetric: 27.8275

Epoch 848: val_loss did not improve from 27.81546
196/196 - 42s - loss: 27.3568 - MinusLogProbMetric: 27.3568 - val_loss: 27.8275 - val_MinusLogProbMetric: 27.8275 - lr: 1.0417e-05 - 42s/epoch - 214ms/step
Epoch 849/1000
2023-10-25 10:56:08.998 
Epoch 849/1000 
	 loss: 27.3608, MinusLogProbMetric: 27.3608, val_loss: 27.8186, val_MinusLogProbMetric: 27.8186

Epoch 849: val_loss did not improve from 27.81546
196/196 - 41s - loss: 27.3608 - MinusLogProbMetric: 27.3608 - val_loss: 27.8186 - val_MinusLogProbMetric: 27.8186 - lr: 1.0417e-05 - 41s/epoch - 208ms/step
Epoch 850/1000
2023-10-25 10:56:47.769 
Epoch 850/1000 
	 loss: 27.3557, MinusLogProbMetric: 27.3557, val_loss: 27.8139, val_MinusLogProbMetric: 27.8139

Epoch 850: val_loss improved from 27.81546 to 27.81392, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 39s - loss: 27.3557 - MinusLogProbMetric: 27.3557 - val_loss: 27.8139 - val_MinusLogProbMetric: 27.8139 - lr: 1.0417e-05 - 39s/epoch - 201ms/step
Epoch 851/1000
2023-10-25 10:57:29.332 
Epoch 851/1000 
	 loss: 27.3555, MinusLogProbMetric: 27.3555, val_loss: 27.8198, val_MinusLogProbMetric: 27.8198

Epoch 851: val_loss did not improve from 27.81392
196/196 - 41s - loss: 27.3555 - MinusLogProbMetric: 27.3555 - val_loss: 27.8198 - val_MinusLogProbMetric: 27.8198 - lr: 1.0417e-05 - 41s/epoch - 209ms/step
Epoch 852/1000
2023-10-25 10:58:10.874 
Epoch 852/1000 
	 loss: 27.3554, MinusLogProbMetric: 27.3554, val_loss: 27.8227, val_MinusLogProbMetric: 27.8227

Epoch 852: val_loss did not improve from 27.81392
196/196 - 42s - loss: 27.3554 - MinusLogProbMetric: 27.3554 - val_loss: 27.8227 - val_MinusLogProbMetric: 27.8227 - lr: 1.0417e-05 - 42s/epoch - 212ms/step
Epoch 853/1000
2023-10-25 10:58:49.518 
Epoch 853/1000 
	 loss: 27.3579, MinusLogProbMetric: 27.3579, val_loss: 27.8151, val_MinusLogProbMetric: 27.8151

Epoch 853: val_loss did not improve from 27.81392
196/196 - 39s - loss: 27.3579 - MinusLogProbMetric: 27.3579 - val_loss: 27.8151 - val_MinusLogProbMetric: 27.8151 - lr: 1.0417e-05 - 39s/epoch - 197ms/step
Epoch 854/1000
2023-10-25 10:59:29.178 
Epoch 854/1000 
	 loss: 27.3598, MinusLogProbMetric: 27.3598, val_loss: 27.8174, val_MinusLogProbMetric: 27.8174

Epoch 854: val_loss did not improve from 27.81392
196/196 - 40s - loss: 27.3598 - MinusLogProbMetric: 27.3598 - val_loss: 27.8174 - val_MinusLogProbMetric: 27.8174 - lr: 1.0417e-05 - 40s/epoch - 202ms/step
Epoch 855/1000
2023-10-25 11:00:08.282 
Epoch 855/1000 
	 loss: 27.3593, MinusLogProbMetric: 27.3593, val_loss: 27.8237, val_MinusLogProbMetric: 27.8237

Epoch 855: val_loss did not improve from 27.81392
196/196 - 39s - loss: 27.3593 - MinusLogProbMetric: 27.3593 - val_loss: 27.8237 - val_MinusLogProbMetric: 27.8237 - lr: 1.0417e-05 - 39s/epoch - 199ms/step
Epoch 856/1000
2023-10-25 11:00:47.294 
Epoch 856/1000 
	 loss: 27.3591, MinusLogProbMetric: 27.3591, val_loss: 27.8192, val_MinusLogProbMetric: 27.8192

Epoch 856: val_loss did not improve from 27.81392
196/196 - 39s - loss: 27.3591 - MinusLogProbMetric: 27.3591 - val_loss: 27.8192 - val_MinusLogProbMetric: 27.8192 - lr: 1.0417e-05 - 39s/epoch - 199ms/step
Epoch 857/1000
2023-10-25 11:01:27.763 
Epoch 857/1000 
	 loss: 27.3576, MinusLogProbMetric: 27.3576, val_loss: 27.8184, val_MinusLogProbMetric: 27.8184

Epoch 857: val_loss did not improve from 27.81392
196/196 - 40s - loss: 27.3576 - MinusLogProbMetric: 27.3576 - val_loss: 27.8184 - val_MinusLogProbMetric: 27.8184 - lr: 1.0417e-05 - 40s/epoch - 206ms/step
Epoch 858/1000
2023-10-25 11:02:07.121 
Epoch 858/1000 
	 loss: 27.3562, MinusLogProbMetric: 27.3562, val_loss: 27.8176, val_MinusLogProbMetric: 27.8176

Epoch 858: val_loss did not improve from 27.81392
196/196 - 39s - loss: 27.3562 - MinusLogProbMetric: 27.3562 - val_loss: 27.8176 - val_MinusLogProbMetric: 27.8176 - lr: 1.0417e-05 - 39s/epoch - 201ms/step
Epoch 859/1000
2023-10-25 11:02:44.859 
Epoch 859/1000 
	 loss: 27.3569, MinusLogProbMetric: 27.3569, val_loss: 27.8187, val_MinusLogProbMetric: 27.8187

Epoch 859: val_loss did not improve from 27.81392
196/196 - 38s - loss: 27.3569 - MinusLogProbMetric: 27.3569 - val_loss: 27.8187 - val_MinusLogProbMetric: 27.8187 - lr: 1.0417e-05 - 38s/epoch - 193ms/step
Epoch 860/1000
2023-10-25 11:03:24.058 
Epoch 860/1000 
	 loss: 27.3577, MinusLogProbMetric: 27.3577, val_loss: 27.8217, val_MinusLogProbMetric: 27.8217

Epoch 860: val_loss did not improve from 27.81392
196/196 - 39s - loss: 27.3577 - MinusLogProbMetric: 27.3577 - val_loss: 27.8217 - val_MinusLogProbMetric: 27.8217 - lr: 1.0417e-05 - 39s/epoch - 200ms/step
Epoch 861/1000
2023-10-25 11:04:02.223 
Epoch 861/1000 
	 loss: 27.3568, MinusLogProbMetric: 27.3568, val_loss: 27.8152, val_MinusLogProbMetric: 27.8152

Epoch 861: val_loss did not improve from 27.81392
196/196 - 38s - loss: 27.3568 - MinusLogProbMetric: 27.3568 - val_loss: 27.8152 - val_MinusLogProbMetric: 27.8152 - lr: 1.0417e-05 - 38s/epoch - 195ms/step
Epoch 862/1000
2023-10-25 11:04:43.377 
Epoch 862/1000 
	 loss: 27.3626, MinusLogProbMetric: 27.3626, val_loss: 27.8362, val_MinusLogProbMetric: 27.8362

Epoch 862: val_loss did not improve from 27.81392
196/196 - 41s - loss: 27.3626 - MinusLogProbMetric: 27.3626 - val_loss: 27.8362 - val_MinusLogProbMetric: 27.8362 - lr: 1.0417e-05 - 41s/epoch - 210ms/step
Epoch 863/1000
2023-10-25 11:05:25.856 
Epoch 863/1000 
	 loss: 27.3550, MinusLogProbMetric: 27.3550, val_loss: 27.8164, val_MinusLogProbMetric: 27.8164

Epoch 863: val_loss did not improve from 27.81392
196/196 - 42s - loss: 27.3550 - MinusLogProbMetric: 27.3550 - val_loss: 27.8164 - val_MinusLogProbMetric: 27.8164 - lr: 1.0417e-05 - 42s/epoch - 217ms/step
Epoch 864/1000
2023-10-25 11:06:07.520 
Epoch 864/1000 
	 loss: 27.3564, MinusLogProbMetric: 27.3564, val_loss: 27.8195, val_MinusLogProbMetric: 27.8195

Epoch 864: val_loss did not improve from 27.81392
196/196 - 42s - loss: 27.3564 - MinusLogProbMetric: 27.3564 - val_loss: 27.8195 - val_MinusLogProbMetric: 27.8195 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 865/1000
2023-10-25 11:06:46.012 
Epoch 865/1000 
	 loss: 27.3544, MinusLogProbMetric: 27.3544, val_loss: 27.8211, val_MinusLogProbMetric: 27.8211

Epoch 865: val_loss did not improve from 27.81392
196/196 - 38s - loss: 27.3544 - MinusLogProbMetric: 27.3544 - val_loss: 27.8211 - val_MinusLogProbMetric: 27.8211 - lr: 1.0417e-05 - 38s/epoch - 196ms/step
Epoch 866/1000
2023-10-25 11:07:26.392 
Epoch 866/1000 
	 loss: 27.3555, MinusLogProbMetric: 27.3555, val_loss: 27.8203, val_MinusLogProbMetric: 27.8203

Epoch 866: val_loss did not improve from 27.81392
196/196 - 40s - loss: 27.3555 - MinusLogProbMetric: 27.3555 - val_loss: 27.8203 - val_MinusLogProbMetric: 27.8203 - lr: 1.0417e-05 - 40s/epoch - 206ms/step
Epoch 867/1000
2023-10-25 11:08:08.744 
Epoch 867/1000 
	 loss: 27.3573, MinusLogProbMetric: 27.3573, val_loss: 27.8150, val_MinusLogProbMetric: 27.8150

Epoch 867: val_loss did not improve from 27.81392
196/196 - 42s - loss: 27.3573 - MinusLogProbMetric: 27.3573 - val_loss: 27.8150 - val_MinusLogProbMetric: 27.8150 - lr: 1.0417e-05 - 42s/epoch - 216ms/step
Epoch 868/1000
2023-10-25 11:08:51.147 
Epoch 868/1000 
	 loss: 27.3575, MinusLogProbMetric: 27.3575, val_loss: 27.8226, val_MinusLogProbMetric: 27.8226

Epoch 868: val_loss did not improve from 27.81392
196/196 - 42s - loss: 27.3575 - MinusLogProbMetric: 27.3575 - val_loss: 27.8226 - val_MinusLogProbMetric: 27.8226 - lr: 1.0417e-05 - 42s/epoch - 216ms/step
Epoch 869/1000
2023-10-25 11:09:30.169 
Epoch 869/1000 
	 loss: 27.3557, MinusLogProbMetric: 27.3557, val_loss: 27.8139, val_MinusLogProbMetric: 27.8139

Epoch 869: val_loss improved from 27.81392 to 27.81391, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 39s - loss: 27.3557 - MinusLogProbMetric: 27.3557 - val_loss: 27.8139 - val_MinusLogProbMetric: 27.8139 - lr: 1.0417e-05 - 39s/epoch - 201ms/step
Epoch 870/1000
2023-10-25 11:10:10.382 
Epoch 870/1000 
	 loss: 27.3593, MinusLogProbMetric: 27.3593, val_loss: 27.8156, val_MinusLogProbMetric: 27.8156

Epoch 870: val_loss did not improve from 27.81391
196/196 - 40s - loss: 27.3593 - MinusLogProbMetric: 27.3593 - val_loss: 27.8156 - val_MinusLogProbMetric: 27.8156 - lr: 1.0417e-05 - 40s/epoch - 203ms/step
Epoch 871/1000
2023-10-25 11:10:51.279 
Epoch 871/1000 
	 loss: 27.3528, MinusLogProbMetric: 27.3528, val_loss: 27.8262, val_MinusLogProbMetric: 27.8262

Epoch 871: val_loss did not improve from 27.81391
196/196 - 41s - loss: 27.3528 - MinusLogProbMetric: 27.3528 - val_loss: 27.8262 - val_MinusLogProbMetric: 27.8262 - lr: 1.0417e-05 - 41s/epoch - 209ms/step
Epoch 872/1000
2023-10-25 11:11:32.800 
Epoch 872/1000 
	 loss: 27.3577, MinusLogProbMetric: 27.3577, val_loss: 27.8158, val_MinusLogProbMetric: 27.8158

Epoch 872: val_loss did not improve from 27.81391
196/196 - 42s - loss: 27.3577 - MinusLogProbMetric: 27.3577 - val_loss: 27.8158 - val_MinusLogProbMetric: 27.8158 - lr: 1.0417e-05 - 42s/epoch - 212ms/step
Epoch 873/1000
2023-10-25 11:12:13.330 
Epoch 873/1000 
	 loss: 27.3562, MinusLogProbMetric: 27.3562, val_loss: 27.8187, val_MinusLogProbMetric: 27.8187

Epoch 873: val_loss did not improve from 27.81391
196/196 - 41s - loss: 27.3562 - MinusLogProbMetric: 27.3562 - val_loss: 27.8187 - val_MinusLogProbMetric: 27.8187 - lr: 1.0417e-05 - 41s/epoch - 207ms/step
Epoch 874/1000
2023-10-25 11:12:54.683 
Epoch 874/1000 
	 loss: 27.3614, MinusLogProbMetric: 27.3614, val_loss: 27.8250, val_MinusLogProbMetric: 27.8250

Epoch 874: val_loss did not improve from 27.81391
196/196 - 41s - loss: 27.3614 - MinusLogProbMetric: 27.3614 - val_loss: 27.8250 - val_MinusLogProbMetric: 27.8250 - lr: 1.0417e-05 - 41s/epoch - 211ms/step
Epoch 875/1000
2023-10-25 11:13:35.426 
Epoch 875/1000 
	 loss: 27.3550, MinusLogProbMetric: 27.3550, val_loss: 27.8154, val_MinusLogProbMetric: 27.8154

Epoch 875: val_loss did not improve from 27.81391
196/196 - 41s - loss: 27.3550 - MinusLogProbMetric: 27.3550 - val_loss: 27.8154 - val_MinusLogProbMetric: 27.8154 - lr: 1.0417e-05 - 41s/epoch - 208ms/step
Epoch 876/1000
2023-10-25 11:14:17.288 
Epoch 876/1000 
	 loss: 27.3562, MinusLogProbMetric: 27.3562, val_loss: 27.8215, val_MinusLogProbMetric: 27.8215

Epoch 876: val_loss did not improve from 27.81391
196/196 - 42s - loss: 27.3562 - MinusLogProbMetric: 27.3562 - val_loss: 27.8215 - val_MinusLogProbMetric: 27.8215 - lr: 1.0417e-05 - 42s/epoch - 214ms/step
Epoch 877/1000
2023-10-25 11:14:59.213 
Epoch 877/1000 
	 loss: 27.3548, MinusLogProbMetric: 27.3548, val_loss: 27.8172, val_MinusLogProbMetric: 27.8172

Epoch 877: val_loss did not improve from 27.81391
196/196 - 42s - loss: 27.3548 - MinusLogProbMetric: 27.3548 - val_loss: 27.8172 - val_MinusLogProbMetric: 27.8172 - lr: 1.0417e-05 - 42s/epoch - 214ms/step
Epoch 878/1000
2023-10-25 11:15:38.934 
Epoch 878/1000 
	 loss: 27.3553, MinusLogProbMetric: 27.3553, val_loss: 27.8199, val_MinusLogProbMetric: 27.8199

Epoch 878: val_loss did not improve from 27.81391
196/196 - 40s - loss: 27.3553 - MinusLogProbMetric: 27.3553 - val_loss: 27.8199 - val_MinusLogProbMetric: 27.8199 - lr: 1.0417e-05 - 40s/epoch - 203ms/step
Epoch 879/1000
2023-10-25 11:16:15.520 
Epoch 879/1000 
	 loss: 27.3557, MinusLogProbMetric: 27.3557, val_loss: 27.8210, val_MinusLogProbMetric: 27.8210

Epoch 879: val_loss did not improve from 27.81391
196/196 - 37s - loss: 27.3557 - MinusLogProbMetric: 27.3557 - val_loss: 27.8210 - val_MinusLogProbMetric: 27.8210 - lr: 1.0417e-05 - 37s/epoch - 187ms/step
Epoch 880/1000
2023-10-25 11:16:57.128 
Epoch 880/1000 
	 loss: 27.3516, MinusLogProbMetric: 27.3516, val_loss: 27.8175, val_MinusLogProbMetric: 27.8175

Epoch 880: val_loss did not improve from 27.81391
196/196 - 42s - loss: 27.3516 - MinusLogProbMetric: 27.3516 - val_loss: 27.8175 - val_MinusLogProbMetric: 27.8175 - lr: 1.0417e-05 - 42s/epoch - 212ms/step
Epoch 881/1000
2023-10-25 11:17:38.734 
Epoch 881/1000 
	 loss: 27.3576, MinusLogProbMetric: 27.3576, val_loss: 27.8179, val_MinusLogProbMetric: 27.8179

Epoch 881: val_loss did not improve from 27.81391
196/196 - 42s - loss: 27.3576 - MinusLogProbMetric: 27.3576 - val_loss: 27.8179 - val_MinusLogProbMetric: 27.8179 - lr: 1.0417e-05 - 42s/epoch - 212ms/step
Epoch 882/1000
2023-10-25 11:18:20.242 
Epoch 882/1000 
	 loss: 27.3541, MinusLogProbMetric: 27.3541, val_loss: 27.8278, val_MinusLogProbMetric: 27.8278

Epoch 882: val_loss did not improve from 27.81391
196/196 - 42s - loss: 27.3541 - MinusLogProbMetric: 27.3541 - val_loss: 27.8278 - val_MinusLogProbMetric: 27.8278 - lr: 1.0417e-05 - 42s/epoch - 212ms/step
Epoch 883/1000
2023-10-25 11:19:00.710 
Epoch 883/1000 
	 loss: 27.3555, MinusLogProbMetric: 27.3555, val_loss: 27.8204, val_MinusLogProbMetric: 27.8204

Epoch 883: val_loss did not improve from 27.81391
196/196 - 40s - loss: 27.3555 - MinusLogProbMetric: 27.3555 - val_loss: 27.8204 - val_MinusLogProbMetric: 27.8204 - lr: 1.0417e-05 - 40s/epoch - 206ms/step
Epoch 884/1000
2023-10-25 11:19:41.066 
Epoch 884/1000 
	 loss: 27.3555, MinusLogProbMetric: 27.3555, val_loss: 27.8234, val_MinusLogProbMetric: 27.8234

Epoch 884: val_loss did not improve from 27.81391
196/196 - 40s - loss: 27.3555 - MinusLogProbMetric: 27.3555 - val_loss: 27.8234 - val_MinusLogProbMetric: 27.8234 - lr: 1.0417e-05 - 40s/epoch - 206ms/step
Epoch 885/1000
2023-10-25 11:20:20.292 
Epoch 885/1000 
	 loss: 27.3583, MinusLogProbMetric: 27.3583, val_loss: 27.8225, val_MinusLogProbMetric: 27.8225

Epoch 885: val_loss did not improve from 27.81391
196/196 - 39s - loss: 27.3583 - MinusLogProbMetric: 27.3583 - val_loss: 27.8225 - val_MinusLogProbMetric: 27.8225 - lr: 1.0417e-05 - 39s/epoch - 200ms/step
Epoch 886/1000
2023-10-25 11:20:59.776 
Epoch 886/1000 
	 loss: 27.3552, MinusLogProbMetric: 27.3552, val_loss: 27.8309, val_MinusLogProbMetric: 27.8309

Epoch 886: val_loss did not improve from 27.81391
196/196 - 39s - loss: 27.3552 - MinusLogProbMetric: 27.3552 - val_loss: 27.8309 - val_MinusLogProbMetric: 27.8309 - lr: 1.0417e-05 - 39s/epoch - 201ms/step
Epoch 887/1000
2023-10-25 11:21:41.988 
Epoch 887/1000 
	 loss: 27.3584, MinusLogProbMetric: 27.3584, val_loss: 27.8275, val_MinusLogProbMetric: 27.8275

Epoch 887: val_loss did not improve from 27.81391
196/196 - 42s - loss: 27.3584 - MinusLogProbMetric: 27.3584 - val_loss: 27.8275 - val_MinusLogProbMetric: 27.8275 - lr: 1.0417e-05 - 42s/epoch - 215ms/step
Epoch 888/1000
2023-10-25 11:22:20.842 
Epoch 888/1000 
	 loss: 27.3537, MinusLogProbMetric: 27.3537, val_loss: 27.8195, val_MinusLogProbMetric: 27.8195

Epoch 888: val_loss did not improve from 27.81391
196/196 - 39s - loss: 27.3537 - MinusLogProbMetric: 27.3537 - val_loss: 27.8195 - val_MinusLogProbMetric: 27.8195 - lr: 1.0417e-05 - 39s/epoch - 198ms/step
Epoch 889/1000
2023-10-25 11:23:03.314 
Epoch 889/1000 
	 loss: 27.3578, MinusLogProbMetric: 27.3578, val_loss: 27.8302, val_MinusLogProbMetric: 27.8302

Epoch 889: val_loss did not improve from 27.81391
196/196 - 42s - loss: 27.3578 - MinusLogProbMetric: 27.3578 - val_loss: 27.8302 - val_MinusLogProbMetric: 27.8302 - lr: 1.0417e-05 - 42s/epoch - 217ms/step
Epoch 890/1000
2023-10-25 11:23:43.672 
Epoch 890/1000 
	 loss: 27.3566, MinusLogProbMetric: 27.3566, val_loss: 27.8237, val_MinusLogProbMetric: 27.8237

Epoch 890: val_loss did not improve from 27.81391
196/196 - 40s - loss: 27.3566 - MinusLogProbMetric: 27.3566 - val_loss: 27.8237 - val_MinusLogProbMetric: 27.8237 - lr: 1.0417e-05 - 40s/epoch - 206ms/step
Epoch 891/1000
2023-10-25 11:24:22.796 
Epoch 891/1000 
	 loss: 27.3567, MinusLogProbMetric: 27.3567, val_loss: 27.8162, val_MinusLogProbMetric: 27.8162

Epoch 891: val_loss did not improve from 27.81391
196/196 - 39s - loss: 27.3567 - MinusLogProbMetric: 27.3567 - val_loss: 27.8162 - val_MinusLogProbMetric: 27.8162 - lr: 1.0417e-05 - 39s/epoch - 200ms/step
Epoch 892/1000
2023-10-25 11:25:02.340 
Epoch 892/1000 
	 loss: 27.3533, MinusLogProbMetric: 27.3533, val_loss: 27.8149, val_MinusLogProbMetric: 27.8149

Epoch 892: val_loss did not improve from 27.81391
196/196 - 40s - loss: 27.3533 - MinusLogProbMetric: 27.3533 - val_loss: 27.8149 - val_MinusLogProbMetric: 27.8149 - lr: 1.0417e-05 - 40s/epoch - 202ms/step
Epoch 893/1000
2023-10-25 11:25:42.525 
Epoch 893/1000 
	 loss: 27.3522, MinusLogProbMetric: 27.3522, val_loss: 27.8256, val_MinusLogProbMetric: 27.8256

Epoch 893: val_loss did not improve from 27.81391
196/196 - 40s - loss: 27.3522 - MinusLogProbMetric: 27.3522 - val_loss: 27.8256 - val_MinusLogProbMetric: 27.8256 - lr: 1.0417e-05 - 40s/epoch - 205ms/step
Epoch 894/1000
2023-10-25 11:26:22.060 
Epoch 894/1000 
	 loss: 27.3556, MinusLogProbMetric: 27.3556, val_loss: 27.8227, val_MinusLogProbMetric: 27.8227

Epoch 894: val_loss did not improve from 27.81391
196/196 - 40s - loss: 27.3556 - MinusLogProbMetric: 27.3556 - val_loss: 27.8227 - val_MinusLogProbMetric: 27.8227 - lr: 1.0417e-05 - 40s/epoch - 202ms/step
Epoch 895/1000
2023-10-25 11:27:02.086 
Epoch 895/1000 
	 loss: 27.3550, MinusLogProbMetric: 27.3550, val_loss: 27.8247, val_MinusLogProbMetric: 27.8247

Epoch 895: val_loss did not improve from 27.81391
196/196 - 40s - loss: 27.3550 - MinusLogProbMetric: 27.3550 - val_loss: 27.8247 - val_MinusLogProbMetric: 27.8247 - lr: 1.0417e-05 - 40s/epoch - 204ms/step
Epoch 896/1000
2023-10-25 11:27:40.712 
Epoch 896/1000 
	 loss: 27.3565, MinusLogProbMetric: 27.3565, val_loss: 27.8225, val_MinusLogProbMetric: 27.8225

Epoch 896: val_loss did not improve from 27.81391
196/196 - 39s - loss: 27.3565 - MinusLogProbMetric: 27.3565 - val_loss: 27.8225 - val_MinusLogProbMetric: 27.8225 - lr: 1.0417e-05 - 39s/epoch - 197ms/step
Epoch 897/1000
2023-10-25 11:28:22.651 
Epoch 897/1000 
	 loss: 27.3550, MinusLogProbMetric: 27.3550, val_loss: 27.8207, val_MinusLogProbMetric: 27.8207

Epoch 897: val_loss did not improve from 27.81391
196/196 - 42s - loss: 27.3550 - MinusLogProbMetric: 27.3550 - val_loss: 27.8207 - val_MinusLogProbMetric: 27.8207 - lr: 1.0417e-05 - 42s/epoch - 214ms/step
Epoch 898/1000
2023-10-25 11:29:04.422 
Epoch 898/1000 
	 loss: 27.3553, MinusLogProbMetric: 27.3553, val_loss: 27.8194, val_MinusLogProbMetric: 27.8194

Epoch 898: val_loss did not improve from 27.81391
196/196 - 42s - loss: 27.3553 - MinusLogProbMetric: 27.3553 - val_loss: 27.8194 - val_MinusLogProbMetric: 27.8194 - lr: 1.0417e-05 - 42s/epoch - 213ms/step
Epoch 899/1000
2023-10-25 11:29:45.013 
Epoch 899/1000 
	 loss: 27.3557, MinusLogProbMetric: 27.3557, val_loss: 27.8162, val_MinusLogProbMetric: 27.8162

Epoch 899: val_loss did not improve from 27.81391
196/196 - 41s - loss: 27.3557 - MinusLogProbMetric: 27.3557 - val_loss: 27.8162 - val_MinusLogProbMetric: 27.8162 - lr: 1.0417e-05 - 41s/epoch - 207ms/step
Epoch 900/1000
2023-10-25 11:30:23.266 
Epoch 900/1000 
	 loss: 27.3527, MinusLogProbMetric: 27.3527, val_loss: 27.8326, val_MinusLogProbMetric: 27.8326

Epoch 900: val_loss did not improve from 27.81391
196/196 - 38s - loss: 27.3527 - MinusLogProbMetric: 27.3527 - val_loss: 27.8326 - val_MinusLogProbMetric: 27.8326 - lr: 1.0417e-05 - 38s/epoch - 195ms/step
Epoch 901/1000
2023-10-25 11:31:04.001 
Epoch 901/1000 
	 loss: 27.3443, MinusLogProbMetric: 27.3443, val_loss: 27.8086, val_MinusLogProbMetric: 27.8086

Epoch 901: val_loss improved from 27.81391 to 27.80855, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 41s - loss: 27.3443 - MinusLogProbMetric: 27.3443 - val_loss: 27.8086 - val_MinusLogProbMetric: 27.8086 - lr: 5.2083e-06 - 41s/epoch - 211ms/step
Epoch 902/1000
2023-10-25 11:31:45.692 
Epoch 902/1000 
	 loss: 27.3455, MinusLogProbMetric: 27.3455, val_loss: 27.8134, val_MinusLogProbMetric: 27.8134

Epoch 902: val_loss did not improve from 27.80855
196/196 - 41s - loss: 27.3455 - MinusLogProbMetric: 27.3455 - val_loss: 27.8134 - val_MinusLogProbMetric: 27.8134 - lr: 5.2083e-06 - 41s/epoch - 209ms/step
Epoch 903/1000
2023-10-25 11:32:25.581 
Epoch 903/1000 
	 loss: 27.3439, MinusLogProbMetric: 27.3439, val_loss: 27.8148, val_MinusLogProbMetric: 27.8148

Epoch 903: val_loss did not improve from 27.80855
196/196 - 40s - loss: 27.3439 - MinusLogProbMetric: 27.3439 - val_loss: 27.8148 - val_MinusLogProbMetric: 27.8148 - lr: 5.2083e-06 - 40s/epoch - 203ms/step
Epoch 904/1000
2023-10-25 11:33:01.866 
Epoch 904/1000 
	 loss: 27.3449, MinusLogProbMetric: 27.3449, val_loss: 27.8122, val_MinusLogProbMetric: 27.8122

Epoch 904: val_loss did not improve from 27.80855
196/196 - 36s - loss: 27.3449 - MinusLogProbMetric: 27.3449 - val_loss: 27.8122 - val_MinusLogProbMetric: 27.8122 - lr: 5.2083e-06 - 36s/epoch - 185ms/step
Epoch 905/1000
2023-10-25 11:33:42.659 
Epoch 905/1000 
	 loss: 27.3459, MinusLogProbMetric: 27.3459, val_loss: 27.8121, val_MinusLogProbMetric: 27.8121

Epoch 905: val_loss did not improve from 27.80855
196/196 - 41s - loss: 27.3459 - MinusLogProbMetric: 27.3459 - val_loss: 27.8121 - val_MinusLogProbMetric: 27.8121 - lr: 5.2083e-06 - 41s/epoch - 208ms/step
Epoch 906/1000
2023-10-25 11:34:20.386 
Epoch 906/1000 
	 loss: 27.3460, MinusLogProbMetric: 27.3460, val_loss: 27.8157, val_MinusLogProbMetric: 27.8157

Epoch 906: val_loss did not improve from 27.80855
196/196 - 38s - loss: 27.3460 - MinusLogProbMetric: 27.3460 - val_loss: 27.8157 - val_MinusLogProbMetric: 27.8157 - lr: 5.2083e-06 - 38s/epoch - 192ms/step
Epoch 907/1000
2023-10-25 11:35:01.772 
Epoch 907/1000 
	 loss: 27.3468, MinusLogProbMetric: 27.3468, val_loss: 27.8125, val_MinusLogProbMetric: 27.8125

Epoch 907: val_loss did not improve from 27.80855
196/196 - 41s - loss: 27.3468 - MinusLogProbMetric: 27.3468 - val_loss: 27.8125 - val_MinusLogProbMetric: 27.8125 - lr: 5.2083e-06 - 41s/epoch - 211ms/step
Epoch 908/1000
2023-10-25 11:35:40.716 
Epoch 908/1000 
	 loss: 27.3442, MinusLogProbMetric: 27.3442, val_loss: 27.8146, val_MinusLogProbMetric: 27.8146

Epoch 908: val_loss did not improve from 27.80855
196/196 - 39s - loss: 27.3442 - MinusLogProbMetric: 27.3442 - val_loss: 27.8146 - val_MinusLogProbMetric: 27.8146 - lr: 5.2083e-06 - 39s/epoch - 199ms/step
Epoch 909/1000
2023-10-25 11:36:23.203 
Epoch 909/1000 
	 loss: 27.3460, MinusLogProbMetric: 27.3460, val_loss: 27.8145, val_MinusLogProbMetric: 27.8145

Epoch 909: val_loss did not improve from 27.80855
196/196 - 42s - loss: 27.3460 - MinusLogProbMetric: 27.3460 - val_loss: 27.8145 - val_MinusLogProbMetric: 27.8145 - lr: 5.2083e-06 - 42s/epoch - 217ms/step
Epoch 910/1000
2023-10-25 11:37:04.924 
Epoch 910/1000 
	 loss: 27.3449, MinusLogProbMetric: 27.3449, val_loss: 27.8150, val_MinusLogProbMetric: 27.8150

Epoch 910: val_loss did not improve from 27.80855
196/196 - 42s - loss: 27.3449 - MinusLogProbMetric: 27.3449 - val_loss: 27.8150 - val_MinusLogProbMetric: 27.8150 - lr: 5.2083e-06 - 42s/epoch - 213ms/step
Epoch 911/1000
2023-10-25 11:37:44.183 
Epoch 911/1000 
	 loss: 27.3444, MinusLogProbMetric: 27.3444, val_loss: 27.8123, val_MinusLogProbMetric: 27.8123

Epoch 911: val_loss did not improve from 27.80855
196/196 - 39s - loss: 27.3444 - MinusLogProbMetric: 27.3444 - val_loss: 27.8123 - val_MinusLogProbMetric: 27.8123 - lr: 5.2083e-06 - 39s/epoch - 200ms/step
Epoch 912/1000
2023-10-25 11:38:23.687 
Epoch 912/1000 
	 loss: 27.3461, MinusLogProbMetric: 27.3461, val_loss: 27.8113, val_MinusLogProbMetric: 27.8113

Epoch 912: val_loss did not improve from 27.80855
196/196 - 39s - loss: 27.3461 - MinusLogProbMetric: 27.3461 - val_loss: 27.8113 - val_MinusLogProbMetric: 27.8113 - lr: 5.2083e-06 - 39s/epoch - 202ms/step
Epoch 913/1000
2023-10-25 11:39:05.851 
Epoch 913/1000 
	 loss: 27.3447, MinusLogProbMetric: 27.3447, val_loss: 27.8125, val_MinusLogProbMetric: 27.8125

Epoch 913: val_loss did not improve from 27.80855
196/196 - 42s - loss: 27.3447 - MinusLogProbMetric: 27.3447 - val_loss: 27.8125 - val_MinusLogProbMetric: 27.8125 - lr: 5.2083e-06 - 42s/epoch - 215ms/step
Epoch 914/1000
2023-10-25 11:39:43.982 
Epoch 914/1000 
	 loss: 27.3443, MinusLogProbMetric: 27.3443, val_loss: 27.8209, val_MinusLogProbMetric: 27.8209

Epoch 914: val_loss did not improve from 27.80855
196/196 - 38s - loss: 27.3443 - MinusLogProbMetric: 27.3443 - val_loss: 27.8209 - val_MinusLogProbMetric: 27.8209 - lr: 5.2083e-06 - 38s/epoch - 195ms/step
Epoch 915/1000
2023-10-25 11:40:23.792 
Epoch 915/1000 
	 loss: 27.3450, MinusLogProbMetric: 27.3450, val_loss: 27.8093, val_MinusLogProbMetric: 27.8093

Epoch 915: val_loss did not improve from 27.80855
196/196 - 40s - loss: 27.3450 - MinusLogProbMetric: 27.3450 - val_loss: 27.8093 - val_MinusLogProbMetric: 27.8093 - lr: 5.2083e-06 - 40s/epoch - 203ms/step
Epoch 916/1000
2023-10-25 11:41:02.771 
Epoch 916/1000 
	 loss: 27.3448, MinusLogProbMetric: 27.3448, val_loss: 27.8307, val_MinusLogProbMetric: 27.8307

Epoch 916: val_loss did not improve from 27.80855
196/196 - 39s - loss: 27.3448 - MinusLogProbMetric: 27.3448 - val_loss: 27.8307 - val_MinusLogProbMetric: 27.8307 - lr: 5.2083e-06 - 39s/epoch - 199ms/step
Epoch 917/1000
2023-10-25 11:41:42.053 
Epoch 917/1000 
	 loss: 27.3455, MinusLogProbMetric: 27.3455, val_loss: 27.8156, val_MinusLogProbMetric: 27.8156

Epoch 917: val_loss did not improve from 27.80855
196/196 - 39s - loss: 27.3455 - MinusLogProbMetric: 27.3455 - val_loss: 27.8156 - val_MinusLogProbMetric: 27.8156 - lr: 5.2083e-06 - 39s/epoch - 200ms/step
Epoch 918/1000
2023-10-25 11:42:20.974 
Epoch 918/1000 
	 loss: 27.3427, MinusLogProbMetric: 27.3427, val_loss: 27.8128, val_MinusLogProbMetric: 27.8128

Epoch 918: val_loss did not improve from 27.80855
196/196 - 39s - loss: 27.3427 - MinusLogProbMetric: 27.3427 - val_loss: 27.8128 - val_MinusLogProbMetric: 27.8128 - lr: 5.2083e-06 - 39s/epoch - 199ms/step
Epoch 919/1000
2023-10-25 11:43:00.350 
Epoch 919/1000 
	 loss: 27.3440, MinusLogProbMetric: 27.3440, val_loss: 27.8091, val_MinusLogProbMetric: 27.8091

Epoch 919: val_loss did not improve from 27.80855
196/196 - 39s - loss: 27.3440 - MinusLogProbMetric: 27.3440 - val_loss: 27.8091 - val_MinusLogProbMetric: 27.8091 - lr: 5.2083e-06 - 39s/epoch - 201ms/step
Epoch 920/1000
2023-10-25 11:43:38.871 
Epoch 920/1000 
	 loss: 27.3451, MinusLogProbMetric: 27.3451, val_loss: 27.8166, val_MinusLogProbMetric: 27.8166

Epoch 920: val_loss did not improve from 27.80855
196/196 - 39s - loss: 27.3451 - MinusLogProbMetric: 27.3451 - val_loss: 27.8166 - val_MinusLogProbMetric: 27.8166 - lr: 5.2083e-06 - 39s/epoch - 197ms/step
Epoch 921/1000
2023-10-25 11:44:18.740 
Epoch 921/1000 
	 loss: 27.3443, MinusLogProbMetric: 27.3443, val_loss: 27.8112, val_MinusLogProbMetric: 27.8112

Epoch 921: val_loss did not improve from 27.80855
196/196 - 40s - loss: 27.3443 - MinusLogProbMetric: 27.3443 - val_loss: 27.8112 - val_MinusLogProbMetric: 27.8112 - lr: 5.2083e-06 - 40s/epoch - 203ms/step
Epoch 922/1000
2023-10-25 11:44:59.898 
Epoch 922/1000 
	 loss: 27.3428, MinusLogProbMetric: 27.3428, val_loss: 27.8121, val_MinusLogProbMetric: 27.8121

Epoch 922: val_loss did not improve from 27.80855
196/196 - 41s - loss: 27.3428 - MinusLogProbMetric: 27.3428 - val_loss: 27.8121 - val_MinusLogProbMetric: 27.8121 - lr: 5.2083e-06 - 41s/epoch - 210ms/step
Epoch 923/1000
2023-10-25 11:45:41.365 
Epoch 923/1000 
	 loss: 27.3437, MinusLogProbMetric: 27.3437, val_loss: 27.8098, val_MinusLogProbMetric: 27.8098

Epoch 923: val_loss did not improve from 27.80855
196/196 - 41s - loss: 27.3437 - MinusLogProbMetric: 27.3437 - val_loss: 27.8098 - val_MinusLogProbMetric: 27.8098 - lr: 5.2083e-06 - 41s/epoch - 212ms/step
Epoch 924/1000
2023-10-25 11:46:22.218 
Epoch 924/1000 
	 loss: 27.3452, MinusLogProbMetric: 27.3452, val_loss: 27.8120, val_MinusLogProbMetric: 27.8120

Epoch 924: val_loss did not improve from 27.80855
196/196 - 41s - loss: 27.3452 - MinusLogProbMetric: 27.3452 - val_loss: 27.8120 - val_MinusLogProbMetric: 27.8120 - lr: 5.2083e-06 - 41s/epoch - 208ms/step
Epoch 925/1000
2023-10-25 11:47:03.523 
Epoch 925/1000 
	 loss: 27.3442, MinusLogProbMetric: 27.3442, val_loss: 27.8111, val_MinusLogProbMetric: 27.8111

Epoch 925: val_loss did not improve from 27.80855
196/196 - 41s - loss: 27.3442 - MinusLogProbMetric: 27.3442 - val_loss: 27.8111 - val_MinusLogProbMetric: 27.8111 - lr: 5.2083e-06 - 41s/epoch - 211ms/step
Epoch 926/1000
2023-10-25 11:47:44.973 
Epoch 926/1000 
	 loss: 27.3425, MinusLogProbMetric: 27.3425, val_loss: 27.8123, val_MinusLogProbMetric: 27.8123

Epoch 926: val_loss did not improve from 27.80855
196/196 - 41s - loss: 27.3425 - MinusLogProbMetric: 27.3425 - val_loss: 27.8123 - val_MinusLogProbMetric: 27.8123 - lr: 5.2083e-06 - 41s/epoch - 211ms/step
Epoch 927/1000
2023-10-25 11:48:26.677 
Epoch 927/1000 
	 loss: 27.3449, MinusLogProbMetric: 27.3449, val_loss: 27.8105, val_MinusLogProbMetric: 27.8105

Epoch 927: val_loss did not improve from 27.80855
196/196 - 42s - loss: 27.3449 - MinusLogProbMetric: 27.3449 - val_loss: 27.8105 - val_MinusLogProbMetric: 27.8105 - lr: 5.2083e-06 - 42s/epoch - 213ms/step
Epoch 928/1000
2023-10-25 11:49:06.067 
Epoch 928/1000 
	 loss: 27.3449, MinusLogProbMetric: 27.3449, val_loss: 27.8106, val_MinusLogProbMetric: 27.8106

Epoch 928: val_loss did not improve from 27.80855
196/196 - 39s - loss: 27.3449 - MinusLogProbMetric: 27.3449 - val_loss: 27.8106 - val_MinusLogProbMetric: 27.8106 - lr: 5.2083e-06 - 39s/epoch - 201ms/step
Epoch 929/1000
2023-10-25 11:49:44.563 
Epoch 929/1000 
	 loss: 27.3429, MinusLogProbMetric: 27.3429, val_loss: 27.8117, val_MinusLogProbMetric: 27.8117

Epoch 929: val_loss did not improve from 27.80855
196/196 - 38s - loss: 27.3429 - MinusLogProbMetric: 27.3429 - val_loss: 27.8117 - val_MinusLogProbMetric: 27.8117 - lr: 5.2083e-06 - 38s/epoch - 196ms/step
Epoch 930/1000
2023-10-25 11:50:25.894 
Epoch 930/1000 
	 loss: 27.3443, MinusLogProbMetric: 27.3443, val_loss: 27.8135, val_MinusLogProbMetric: 27.8135

Epoch 930: val_loss did not improve from 27.80855
196/196 - 41s - loss: 27.3443 - MinusLogProbMetric: 27.3443 - val_loss: 27.8135 - val_MinusLogProbMetric: 27.8135 - lr: 5.2083e-06 - 41s/epoch - 211ms/step
Epoch 931/1000
2023-10-25 11:51:07.431 
Epoch 931/1000 
	 loss: 27.3431, MinusLogProbMetric: 27.3431, val_loss: 27.8124, val_MinusLogProbMetric: 27.8124

Epoch 931: val_loss did not improve from 27.80855
196/196 - 42s - loss: 27.3431 - MinusLogProbMetric: 27.3431 - val_loss: 27.8124 - val_MinusLogProbMetric: 27.8124 - lr: 5.2083e-06 - 42s/epoch - 212ms/step
Epoch 932/1000
2023-10-25 11:51:46.869 
Epoch 932/1000 
	 loss: 27.3435, MinusLogProbMetric: 27.3435, val_loss: 27.8141, val_MinusLogProbMetric: 27.8141

Epoch 932: val_loss did not improve from 27.80855
196/196 - 39s - loss: 27.3435 - MinusLogProbMetric: 27.3435 - val_loss: 27.8141 - val_MinusLogProbMetric: 27.8141 - lr: 5.2083e-06 - 39s/epoch - 201ms/step
Epoch 933/1000
2023-10-25 11:52:27.604 
Epoch 933/1000 
	 loss: 27.3442, MinusLogProbMetric: 27.3442, val_loss: 27.8150, val_MinusLogProbMetric: 27.8150

Epoch 933: val_loss did not improve from 27.80855
196/196 - 41s - loss: 27.3442 - MinusLogProbMetric: 27.3442 - val_loss: 27.8150 - val_MinusLogProbMetric: 27.8150 - lr: 5.2083e-06 - 41s/epoch - 208ms/step
Epoch 934/1000
2023-10-25 11:53:05.095 
Epoch 934/1000 
	 loss: 27.3444, MinusLogProbMetric: 27.3444, val_loss: 27.8137, val_MinusLogProbMetric: 27.8137

Epoch 934: val_loss did not improve from 27.80855
196/196 - 37s - loss: 27.3444 - MinusLogProbMetric: 27.3444 - val_loss: 27.8137 - val_MinusLogProbMetric: 27.8137 - lr: 5.2083e-06 - 37s/epoch - 191ms/step
Epoch 935/1000
2023-10-25 11:53:42.636 
Epoch 935/1000 
	 loss: 27.3439, MinusLogProbMetric: 27.3439, val_loss: 27.8110, val_MinusLogProbMetric: 27.8110

Epoch 935: val_loss did not improve from 27.80855
196/196 - 38s - loss: 27.3439 - MinusLogProbMetric: 27.3439 - val_loss: 27.8110 - val_MinusLogProbMetric: 27.8110 - lr: 5.2083e-06 - 38s/epoch - 192ms/step
Epoch 936/1000
2023-10-25 11:54:24.386 
Epoch 936/1000 
	 loss: 27.3473, MinusLogProbMetric: 27.3473, val_loss: 27.8220, val_MinusLogProbMetric: 27.8220

Epoch 936: val_loss did not improve from 27.80855
196/196 - 42s - loss: 27.3473 - MinusLogProbMetric: 27.3473 - val_loss: 27.8220 - val_MinusLogProbMetric: 27.8220 - lr: 5.2083e-06 - 42s/epoch - 213ms/step
Epoch 937/1000
2023-10-25 11:55:06.287 
Epoch 937/1000 
	 loss: 27.3448, MinusLogProbMetric: 27.3448, val_loss: 27.8152, val_MinusLogProbMetric: 27.8152

Epoch 937: val_loss did not improve from 27.80855
196/196 - 42s - loss: 27.3448 - MinusLogProbMetric: 27.3448 - val_loss: 27.8152 - val_MinusLogProbMetric: 27.8152 - lr: 5.2083e-06 - 42s/epoch - 214ms/step
Epoch 938/1000
2023-10-25 11:55:48.071 
Epoch 938/1000 
	 loss: 27.3448, MinusLogProbMetric: 27.3448, val_loss: 27.8121, val_MinusLogProbMetric: 27.8121

Epoch 938: val_loss did not improve from 27.80855
196/196 - 42s - loss: 27.3448 - MinusLogProbMetric: 27.3448 - val_loss: 27.8121 - val_MinusLogProbMetric: 27.8121 - lr: 5.2083e-06 - 42s/epoch - 213ms/step
Epoch 939/1000
2023-10-25 11:56:27.361 
Epoch 939/1000 
	 loss: 27.3450, MinusLogProbMetric: 27.3450, val_loss: 27.8199, val_MinusLogProbMetric: 27.8199

Epoch 939: val_loss did not improve from 27.80855
196/196 - 39s - loss: 27.3450 - MinusLogProbMetric: 27.3450 - val_loss: 27.8199 - val_MinusLogProbMetric: 27.8199 - lr: 5.2083e-06 - 39s/epoch - 200ms/step
Epoch 940/1000
2023-10-25 11:57:08.214 
Epoch 940/1000 
	 loss: 27.3456, MinusLogProbMetric: 27.3456, val_loss: 27.8128, val_MinusLogProbMetric: 27.8128

Epoch 940: val_loss did not improve from 27.80855
196/196 - 41s - loss: 27.3456 - MinusLogProbMetric: 27.3456 - val_loss: 27.8128 - val_MinusLogProbMetric: 27.8128 - lr: 5.2083e-06 - 41s/epoch - 208ms/step
Epoch 941/1000
2023-10-25 11:57:46.646 
Epoch 941/1000 
	 loss: 27.3454, MinusLogProbMetric: 27.3454, val_loss: 27.8143, val_MinusLogProbMetric: 27.8143

Epoch 941: val_loss did not improve from 27.80855
196/196 - 38s - loss: 27.3454 - MinusLogProbMetric: 27.3454 - val_loss: 27.8143 - val_MinusLogProbMetric: 27.8143 - lr: 5.2083e-06 - 38s/epoch - 196ms/step
Epoch 942/1000
2023-10-25 11:58:26.520 
Epoch 942/1000 
	 loss: 27.3445, MinusLogProbMetric: 27.3445, val_loss: 27.8148, val_MinusLogProbMetric: 27.8148

Epoch 942: val_loss did not improve from 27.80855
196/196 - 40s - loss: 27.3445 - MinusLogProbMetric: 27.3445 - val_loss: 27.8148 - val_MinusLogProbMetric: 27.8148 - lr: 5.2083e-06 - 40s/epoch - 203ms/step
Epoch 943/1000
2023-10-25 11:59:05.760 
Epoch 943/1000 
	 loss: 27.3442, MinusLogProbMetric: 27.3442, val_loss: 27.8136, val_MinusLogProbMetric: 27.8136

Epoch 943: val_loss did not improve from 27.80855
196/196 - 39s - loss: 27.3442 - MinusLogProbMetric: 27.3442 - val_loss: 27.8136 - val_MinusLogProbMetric: 27.8136 - lr: 5.2083e-06 - 39s/epoch - 200ms/step
Epoch 944/1000
2023-10-25 11:59:45.678 
Epoch 944/1000 
	 loss: 27.3432, MinusLogProbMetric: 27.3432, val_loss: 27.8095, val_MinusLogProbMetric: 27.8095

Epoch 944: val_loss did not improve from 27.80855
196/196 - 40s - loss: 27.3432 - MinusLogProbMetric: 27.3432 - val_loss: 27.8095 - val_MinusLogProbMetric: 27.8095 - lr: 5.2083e-06 - 40s/epoch - 204ms/step
Epoch 945/1000
2023-10-25 12:00:24.518 
Epoch 945/1000 
	 loss: 27.3432, MinusLogProbMetric: 27.3432, val_loss: 27.8149, val_MinusLogProbMetric: 27.8149

Epoch 945: val_loss did not improve from 27.80855
196/196 - 39s - loss: 27.3432 - MinusLogProbMetric: 27.3432 - val_loss: 27.8149 - val_MinusLogProbMetric: 27.8149 - lr: 5.2083e-06 - 39s/epoch - 198ms/step
Epoch 946/1000
2023-10-25 12:01:03.300 
Epoch 946/1000 
	 loss: 27.3439, MinusLogProbMetric: 27.3439, val_loss: 27.8151, val_MinusLogProbMetric: 27.8151

Epoch 946: val_loss did not improve from 27.80855
196/196 - 39s - loss: 27.3439 - MinusLogProbMetric: 27.3439 - val_loss: 27.8151 - val_MinusLogProbMetric: 27.8151 - lr: 5.2083e-06 - 39s/epoch - 198ms/step
Epoch 947/1000
2023-10-25 12:01:45.435 
Epoch 947/1000 
	 loss: 27.3429, MinusLogProbMetric: 27.3429, val_loss: 27.8130, val_MinusLogProbMetric: 27.8130

Epoch 947: val_loss did not improve from 27.80855
196/196 - 42s - loss: 27.3429 - MinusLogProbMetric: 27.3429 - val_loss: 27.8130 - val_MinusLogProbMetric: 27.8130 - lr: 5.2083e-06 - 42s/epoch - 215ms/step
Epoch 948/1000
2023-10-25 12:02:25.547 
Epoch 948/1000 
	 loss: 27.3426, MinusLogProbMetric: 27.3426, val_loss: 27.8086, val_MinusLogProbMetric: 27.8086

Epoch 948: val_loss did not improve from 27.80855
196/196 - 40s - loss: 27.3426 - MinusLogProbMetric: 27.3426 - val_loss: 27.8086 - val_MinusLogProbMetric: 27.8086 - lr: 5.2083e-06 - 40s/epoch - 205ms/step
Epoch 949/1000
2023-10-25 12:03:05.105 
Epoch 949/1000 
	 loss: 27.3427, MinusLogProbMetric: 27.3427, val_loss: 27.8130, val_MinusLogProbMetric: 27.8130

Epoch 949: val_loss did not improve from 27.80855
196/196 - 40s - loss: 27.3427 - MinusLogProbMetric: 27.3427 - val_loss: 27.8130 - val_MinusLogProbMetric: 27.8130 - lr: 5.2083e-06 - 40s/epoch - 202ms/step
Epoch 950/1000
2023-10-25 12:03:45.675 
Epoch 950/1000 
	 loss: 27.3435, MinusLogProbMetric: 27.3435, val_loss: 27.8132, val_MinusLogProbMetric: 27.8132

Epoch 950: val_loss did not improve from 27.80855
196/196 - 41s - loss: 27.3435 - MinusLogProbMetric: 27.3435 - val_loss: 27.8132 - val_MinusLogProbMetric: 27.8132 - lr: 5.2083e-06 - 41s/epoch - 207ms/step
Epoch 951/1000
2023-10-25 12:04:24.920 
Epoch 951/1000 
	 loss: 27.3426, MinusLogProbMetric: 27.3426, val_loss: 27.8130, val_MinusLogProbMetric: 27.8130

Epoch 951: val_loss did not improve from 27.80855
196/196 - 39s - loss: 27.3426 - MinusLogProbMetric: 27.3426 - val_loss: 27.8130 - val_MinusLogProbMetric: 27.8130 - lr: 5.2083e-06 - 39s/epoch - 200ms/step
Epoch 952/1000
2023-10-25 12:05:05.308 
Epoch 952/1000 
	 loss: 27.3387, MinusLogProbMetric: 27.3387, val_loss: 27.8078, val_MinusLogProbMetric: 27.8078

Epoch 952: val_loss improved from 27.80855 to 27.80779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 41s - loss: 27.3387 - MinusLogProbMetric: 27.3387 - val_loss: 27.8078 - val_MinusLogProbMetric: 27.8078 - lr: 2.6042e-06 - 41s/epoch - 209ms/step
Epoch 953/1000
2023-10-25 12:05:43.805 
Epoch 953/1000 
	 loss: 27.3381, MinusLogProbMetric: 27.3381, val_loss: 27.8107, val_MinusLogProbMetric: 27.8107

Epoch 953: val_loss did not improve from 27.80779
196/196 - 38s - loss: 27.3381 - MinusLogProbMetric: 27.3381 - val_loss: 27.8107 - val_MinusLogProbMetric: 27.8107 - lr: 2.6042e-06 - 38s/epoch - 193ms/step
Epoch 954/1000
2023-10-25 12:06:22.890 
Epoch 954/1000 
	 loss: 27.3395, MinusLogProbMetric: 27.3395, val_loss: 27.8093, val_MinusLogProbMetric: 27.8093

Epoch 954: val_loss did not improve from 27.80779
196/196 - 39s - loss: 27.3395 - MinusLogProbMetric: 27.3395 - val_loss: 27.8093 - val_MinusLogProbMetric: 27.8093 - lr: 2.6042e-06 - 39s/epoch - 199ms/step
Epoch 955/1000
2023-10-25 12:07:03.085 
Epoch 955/1000 
	 loss: 27.3377, MinusLogProbMetric: 27.3377, val_loss: 27.8114, val_MinusLogProbMetric: 27.8114

Epoch 955: val_loss did not improve from 27.80779
196/196 - 40s - loss: 27.3377 - MinusLogProbMetric: 27.3377 - val_loss: 27.8114 - val_MinusLogProbMetric: 27.8114 - lr: 2.6042e-06 - 40s/epoch - 205ms/step
Epoch 956/1000
2023-10-25 12:07:40.401 
Epoch 956/1000 
	 loss: 27.3389, MinusLogProbMetric: 27.3389, val_loss: 27.8105, val_MinusLogProbMetric: 27.8105

Epoch 956: val_loss did not improve from 27.80779
196/196 - 37s - loss: 27.3389 - MinusLogProbMetric: 27.3389 - val_loss: 27.8105 - val_MinusLogProbMetric: 27.8105 - lr: 2.6042e-06 - 37s/epoch - 190ms/step
Epoch 957/1000
2023-10-25 12:08:21.453 
Epoch 957/1000 
	 loss: 27.3388, MinusLogProbMetric: 27.3388, val_loss: 27.8078, val_MinusLogProbMetric: 27.8078

Epoch 957: val_loss did not improve from 27.80779
196/196 - 41s - loss: 27.3388 - MinusLogProbMetric: 27.3388 - val_loss: 27.8078 - val_MinusLogProbMetric: 27.8078 - lr: 2.6042e-06 - 41s/epoch - 209ms/step
Epoch 958/1000
2023-10-25 12:09:02.622 
Epoch 958/1000 
	 loss: 27.3387, MinusLogProbMetric: 27.3387, val_loss: 27.8086, val_MinusLogProbMetric: 27.8086

Epoch 958: val_loss did not improve from 27.80779
196/196 - 41s - loss: 27.3387 - MinusLogProbMetric: 27.3387 - val_loss: 27.8086 - val_MinusLogProbMetric: 27.8086 - lr: 2.6042e-06 - 41s/epoch - 210ms/step
Epoch 959/1000
2023-10-25 12:09:42.016 
Epoch 959/1000 
	 loss: 27.3395, MinusLogProbMetric: 27.3395, val_loss: 27.8080, val_MinusLogProbMetric: 27.8080

Epoch 959: val_loss did not improve from 27.80779
196/196 - 39s - loss: 27.3395 - MinusLogProbMetric: 27.3395 - val_loss: 27.8080 - val_MinusLogProbMetric: 27.8080 - lr: 2.6042e-06 - 39s/epoch - 201ms/step
Epoch 960/1000
2023-10-25 12:10:19.540 
Epoch 960/1000 
	 loss: 27.3388, MinusLogProbMetric: 27.3388, val_loss: 27.8096, val_MinusLogProbMetric: 27.8096

Epoch 960: val_loss did not improve from 27.80779
196/196 - 38s - loss: 27.3388 - MinusLogProbMetric: 27.3388 - val_loss: 27.8096 - val_MinusLogProbMetric: 27.8096 - lr: 2.6042e-06 - 38s/epoch - 191ms/step
Epoch 961/1000
2023-10-25 12:10:57.577 
Epoch 961/1000 
	 loss: 27.3386, MinusLogProbMetric: 27.3386, val_loss: 27.8095, val_MinusLogProbMetric: 27.8095

Epoch 961: val_loss did not improve from 27.80779
196/196 - 38s - loss: 27.3386 - MinusLogProbMetric: 27.3386 - val_loss: 27.8095 - val_MinusLogProbMetric: 27.8095 - lr: 2.6042e-06 - 38s/epoch - 194ms/step
Epoch 962/1000
2023-10-25 12:11:39.189 
Epoch 962/1000 
	 loss: 27.3380, MinusLogProbMetric: 27.3380, val_loss: 27.8100, val_MinusLogProbMetric: 27.8100

Epoch 962: val_loss did not improve from 27.80779
196/196 - 42s - loss: 27.3380 - MinusLogProbMetric: 27.3380 - val_loss: 27.8100 - val_MinusLogProbMetric: 27.8100 - lr: 2.6042e-06 - 42s/epoch - 212ms/step
Epoch 963/1000
2023-10-25 12:12:18.233 
Epoch 963/1000 
	 loss: 27.3378, MinusLogProbMetric: 27.3378, val_loss: 27.8080, val_MinusLogProbMetric: 27.8080

Epoch 963: val_loss did not improve from 27.80779
196/196 - 39s - loss: 27.3378 - MinusLogProbMetric: 27.3378 - val_loss: 27.8080 - val_MinusLogProbMetric: 27.8080 - lr: 2.6042e-06 - 39s/epoch - 199ms/step
Epoch 964/1000
2023-10-25 12:12:56.047 
Epoch 964/1000 
	 loss: 27.3387, MinusLogProbMetric: 27.3387, val_loss: 27.8073, val_MinusLogProbMetric: 27.8073

Epoch 964: val_loss improved from 27.80779 to 27.80733, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 38s - loss: 27.3387 - MinusLogProbMetric: 27.3387 - val_loss: 27.8073 - val_MinusLogProbMetric: 27.8073 - lr: 2.6042e-06 - 38s/epoch - 196ms/step
Epoch 965/1000
2023-10-25 12:13:33.860 
Epoch 965/1000 
	 loss: 27.3388, MinusLogProbMetric: 27.3388, val_loss: 27.8080, val_MinusLogProbMetric: 27.8080

Epoch 965: val_loss did not improve from 27.80733
196/196 - 37s - loss: 27.3388 - MinusLogProbMetric: 27.3388 - val_loss: 27.8080 - val_MinusLogProbMetric: 27.8080 - lr: 2.6042e-06 - 37s/epoch - 190ms/step
Epoch 966/1000
2023-10-25 12:14:13.216 
Epoch 966/1000 
	 loss: 27.3382, MinusLogProbMetric: 27.3382, val_loss: 27.8078, val_MinusLogProbMetric: 27.8078

Epoch 966: val_loss did not improve from 27.80733
196/196 - 39s - loss: 27.3382 - MinusLogProbMetric: 27.3382 - val_loss: 27.8078 - val_MinusLogProbMetric: 27.8078 - lr: 2.6042e-06 - 39s/epoch - 201ms/step
Epoch 967/1000
2023-10-25 12:14:50.659 
Epoch 967/1000 
	 loss: 27.3381, MinusLogProbMetric: 27.3381, val_loss: 27.8081, val_MinusLogProbMetric: 27.8081

Epoch 967: val_loss did not improve from 27.80733
196/196 - 37s - loss: 27.3381 - MinusLogProbMetric: 27.3381 - val_loss: 27.8081 - val_MinusLogProbMetric: 27.8081 - lr: 2.6042e-06 - 37s/epoch - 191ms/step
Epoch 968/1000
2023-10-25 12:15:30.650 
Epoch 968/1000 
	 loss: 27.3381, MinusLogProbMetric: 27.3381, val_loss: 27.8088, val_MinusLogProbMetric: 27.8088

Epoch 968: val_loss did not improve from 27.80733
196/196 - 40s - loss: 27.3381 - MinusLogProbMetric: 27.3381 - val_loss: 27.8088 - val_MinusLogProbMetric: 27.8088 - lr: 2.6042e-06 - 40s/epoch - 204ms/step
Epoch 969/1000
2023-10-25 12:16:09.112 
Epoch 969/1000 
	 loss: 27.3384, MinusLogProbMetric: 27.3384, val_loss: 27.8081, val_MinusLogProbMetric: 27.8081

Epoch 969: val_loss did not improve from 27.80733
196/196 - 38s - loss: 27.3384 - MinusLogProbMetric: 27.3384 - val_loss: 27.8081 - val_MinusLogProbMetric: 27.8081 - lr: 2.6042e-06 - 38s/epoch - 196ms/step
Epoch 970/1000
2023-10-25 12:16:48.194 
Epoch 970/1000 
	 loss: 27.3381, MinusLogProbMetric: 27.3381, val_loss: 27.8106, val_MinusLogProbMetric: 27.8106

Epoch 970: val_loss did not improve from 27.80733
196/196 - 39s - loss: 27.3381 - MinusLogProbMetric: 27.3381 - val_loss: 27.8106 - val_MinusLogProbMetric: 27.8106 - lr: 2.6042e-06 - 39s/epoch - 199ms/step
Epoch 971/1000
2023-10-25 12:17:25.278 
Epoch 971/1000 
	 loss: 27.3381, MinusLogProbMetric: 27.3381, val_loss: 27.8078, val_MinusLogProbMetric: 27.8078

Epoch 971: val_loss did not improve from 27.80733
196/196 - 37s - loss: 27.3381 - MinusLogProbMetric: 27.3381 - val_loss: 27.8078 - val_MinusLogProbMetric: 27.8078 - lr: 2.6042e-06 - 37s/epoch - 189ms/step
Epoch 972/1000
2023-10-25 12:18:04.777 
Epoch 972/1000 
	 loss: 27.3380, MinusLogProbMetric: 27.3380, val_loss: 27.8115, val_MinusLogProbMetric: 27.8115

Epoch 972: val_loss did not improve from 27.80733
196/196 - 39s - loss: 27.3380 - MinusLogProbMetric: 27.3380 - val_loss: 27.8115 - val_MinusLogProbMetric: 27.8115 - lr: 2.6042e-06 - 39s/epoch - 202ms/step
Epoch 973/1000
2023-10-25 12:18:43.954 
Epoch 973/1000 
	 loss: 27.3388, MinusLogProbMetric: 27.3388, val_loss: 27.8085, val_MinusLogProbMetric: 27.8085

Epoch 973: val_loss did not improve from 27.80733
196/196 - 39s - loss: 27.3388 - MinusLogProbMetric: 27.3388 - val_loss: 27.8085 - val_MinusLogProbMetric: 27.8085 - lr: 2.6042e-06 - 39s/epoch - 200ms/step
Epoch 974/1000
2023-10-25 12:19:24.051 
Epoch 974/1000 
	 loss: 27.3383, MinusLogProbMetric: 27.3383, val_loss: 27.8120, val_MinusLogProbMetric: 27.8120

Epoch 974: val_loss did not improve from 27.80733
196/196 - 40s - loss: 27.3383 - MinusLogProbMetric: 27.3383 - val_loss: 27.8120 - val_MinusLogProbMetric: 27.8120 - lr: 2.6042e-06 - 40s/epoch - 205ms/step
Epoch 975/1000
2023-10-25 12:20:03.698 
Epoch 975/1000 
	 loss: 27.3389, MinusLogProbMetric: 27.3389, val_loss: 27.8081, val_MinusLogProbMetric: 27.8081

Epoch 975: val_loss did not improve from 27.80733
196/196 - 40s - loss: 27.3389 - MinusLogProbMetric: 27.3389 - val_loss: 27.8081 - val_MinusLogProbMetric: 27.8081 - lr: 2.6042e-06 - 40s/epoch - 202ms/step
Epoch 976/1000
2023-10-25 12:20:45.515 
Epoch 976/1000 
	 loss: 27.3387, MinusLogProbMetric: 27.3387, val_loss: 27.8073, val_MinusLogProbMetric: 27.8073

Epoch 976: val_loss improved from 27.80733 to 27.80732, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_347/weights/best_weights.h5
196/196 - 42s - loss: 27.3387 - MinusLogProbMetric: 27.3387 - val_loss: 27.8073 - val_MinusLogProbMetric: 27.8073 - lr: 2.6042e-06 - 42s/epoch - 216ms/step
Epoch 977/1000
2023-10-25 12:21:26.110 
Epoch 977/1000 
	 loss: 27.3385, MinusLogProbMetric: 27.3385, val_loss: 27.8118, val_MinusLogProbMetric: 27.8118

Epoch 977: val_loss did not improve from 27.80732
196/196 - 40s - loss: 27.3385 - MinusLogProbMetric: 27.3385 - val_loss: 27.8118 - val_MinusLogProbMetric: 27.8118 - lr: 2.6042e-06 - 40s/epoch - 205ms/step
Epoch 978/1000
2023-10-25 12:22:08.430 
Epoch 978/1000 
	 loss: 27.3392, MinusLogProbMetric: 27.3392, val_loss: 27.8095, val_MinusLogProbMetric: 27.8095

Epoch 978: val_loss did not improve from 27.80732
196/196 - 42s - loss: 27.3392 - MinusLogProbMetric: 27.3392 - val_loss: 27.8095 - val_MinusLogProbMetric: 27.8095 - lr: 2.6042e-06 - 42s/epoch - 216ms/step
Epoch 979/1000
2023-10-25 12:22:49.477 
Epoch 979/1000 
	 loss: 27.3389, MinusLogProbMetric: 27.3389, val_loss: 27.8083, val_MinusLogProbMetric: 27.8083

Epoch 979: val_loss did not improve from 27.80732
196/196 - 41s - loss: 27.3389 - MinusLogProbMetric: 27.3389 - val_loss: 27.8083 - val_MinusLogProbMetric: 27.8083 - lr: 2.6042e-06 - 41s/epoch - 209ms/step
Epoch 980/1000
2023-10-25 12:23:30.132 
Epoch 980/1000 
	 loss: 27.3390, MinusLogProbMetric: 27.3390, val_loss: 27.8085, val_MinusLogProbMetric: 27.8085

Epoch 980: val_loss did not improve from 27.80732
196/196 - 41s - loss: 27.3390 - MinusLogProbMetric: 27.3390 - val_loss: 27.8085 - val_MinusLogProbMetric: 27.8085 - lr: 2.6042e-06 - 41s/epoch - 207ms/step
Epoch 981/1000
2023-10-25 12:24:10.139 
Epoch 981/1000 
	 loss: 27.3386, MinusLogProbMetric: 27.3386, val_loss: 27.8098, val_MinusLogProbMetric: 27.8098

Epoch 981: val_loss did not improve from 27.80732
196/196 - 40s - loss: 27.3386 - MinusLogProbMetric: 27.3386 - val_loss: 27.8098 - val_MinusLogProbMetric: 27.8098 - lr: 2.6042e-06 - 40s/epoch - 204ms/step
Epoch 982/1000
2023-10-25 12:24:46.739 
Epoch 982/1000 
	 loss: 27.3382, MinusLogProbMetric: 27.3382, val_loss: 27.8100, val_MinusLogProbMetric: 27.8100

Epoch 982: val_loss did not improve from 27.80732
196/196 - 37s - loss: 27.3382 - MinusLogProbMetric: 27.3382 - val_loss: 27.8100 - val_MinusLogProbMetric: 27.8100 - lr: 2.6042e-06 - 37s/epoch - 187ms/step
Epoch 983/1000
2023-10-25 12:25:25.251 
Epoch 983/1000 
	 loss: 27.3391, MinusLogProbMetric: 27.3391, val_loss: 27.8107, val_MinusLogProbMetric: 27.8107

Epoch 983: val_loss did not improve from 27.80732
196/196 - 39s - loss: 27.3391 - MinusLogProbMetric: 27.3391 - val_loss: 27.8107 - val_MinusLogProbMetric: 27.8107 - lr: 2.6042e-06 - 39s/epoch - 196ms/step
Epoch 984/1000
2023-10-25 12:26:05.535 
Epoch 984/1000 
	 loss: 27.3393, MinusLogProbMetric: 27.3393, val_loss: 27.8113, val_MinusLogProbMetric: 27.8113

Epoch 984: val_loss did not improve from 27.80732
196/196 - 40s - loss: 27.3393 - MinusLogProbMetric: 27.3393 - val_loss: 27.8113 - val_MinusLogProbMetric: 27.8113 - lr: 2.6042e-06 - 40s/epoch - 206ms/step
Epoch 985/1000
2023-10-25 12:26:47.477 
Epoch 985/1000 
	 loss: 27.3390, MinusLogProbMetric: 27.3390, val_loss: 27.8125, val_MinusLogProbMetric: 27.8125

Epoch 985: val_loss did not improve from 27.80732
196/196 - 42s - loss: 27.3390 - MinusLogProbMetric: 27.3390 - val_loss: 27.8125 - val_MinusLogProbMetric: 27.8125 - lr: 2.6042e-06 - 42s/epoch - 214ms/step
Epoch 986/1000
2023-10-25 12:27:26.775 
Epoch 986/1000 
	 loss: 27.3395, MinusLogProbMetric: 27.3395, val_loss: 27.8079, val_MinusLogProbMetric: 27.8079

Epoch 986: val_loss did not improve from 27.80732
196/196 - 39s - loss: 27.3395 - MinusLogProbMetric: 27.3395 - val_loss: 27.8079 - val_MinusLogProbMetric: 27.8079 - lr: 2.6042e-06 - 39s/epoch - 200ms/step
Epoch 987/1000
2023-10-25 12:28:05.544 
Epoch 987/1000 
	 loss: 27.3381, MinusLogProbMetric: 27.3381, val_loss: 27.8081, val_MinusLogProbMetric: 27.8081

Epoch 987: val_loss did not improve from 27.80732
196/196 - 39s - loss: 27.3381 - MinusLogProbMetric: 27.3381 - val_loss: 27.8081 - val_MinusLogProbMetric: 27.8081 - lr: 2.6042e-06 - 39s/epoch - 198ms/step
Epoch 988/1000
2023-10-25 12:28:44.086 
Epoch 988/1000 
	 loss: 27.3384, MinusLogProbMetric: 27.3384, val_loss: 27.8092, val_MinusLogProbMetric: 27.8092

Epoch 988: val_loss did not improve from 27.80732
196/196 - 39s - loss: 27.3384 - MinusLogProbMetric: 27.3384 - val_loss: 27.8092 - val_MinusLogProbMetric: 27.8092 - lr: 2.6042e-06 - 39s/epoch - 197ms/step
Epoch 989/1000
2023-10-25 12:29:21.835 
Epoch 989/1000 
	 loss: 27.3377, MinusLogProbMetric: 27.3377, val_loss: 27.8122, val_MinusLogProbMetric: 27.8122

Epoch 989: val_loss did not improve from 27.80732
196/196 - 38s - loss: 27.3377 - MinusLogProbMetric: 27.3377 - val_loss: 27.8122 - val_MinusLogProbMetric: 27.8122 - lr: 2.6042e-06 - 38s/epoch - 193ms/step
Epoch 990/1000
2023-10-25 12:30:02.185 
Epoch 990/1000 
	 loss: 27.3391, MinusLogProbMetric: 27.3391, val_loss: 27.8147, val_MinusLogProbMetric: 27.8147

Epoch 990: val_loss did not improve from 27.80732
196/196 - 40s - loss: 27.3391 - MinusLogProbMetric: 27.3391 - val_loss: 27.8147 - val_MinusLogProbMetric: 27.8147 - lr: 2.6042e-06 - 40s/epoch - 206ms/step
Epoch 991/1000
2023-10-25 12:30:41.609 
Epoch 991/1000 
	 loss: 27.3392, MinusLogProbMetric: 27.3392, val_loss: 27.8090, val_MinusLogProbMetric: 27.8090

Epoch 991: val_loss did not improve from 27.80732
196/196 - 39s - loss: 27.3392 - MinusLogProbMetric: 27.3392 - val_loss: 27.8090 - val_MinusLogProbMetric: 27.8090 - lr: 2.6042e-06 - 39s/epoch - 201ms/step
Epoch 992/1000
2023-10-25 12:31:20.615 
Epoch 992/1000 
	 loss: 27.3378, MinusLogProbMetric: 27.3378, val_loss: 27.8089, val_MinusLogProbMetric: 27.8089

Epoch 992: val_loss did not improve from 27.80732
196/196 - 39s - loss: 27.3378 - MinusLogProbMetric: 27.3378 - val_loss: 27.8089 - val_MinusLogProbMetric: 27.8089 - lr: 2.6042e-06 - 39s/epoch - 199ms/step
Epoch 993/1000
2023-10-25 12:31:59.685 
Epoch 993/1000 
	 loss: 27.3385, MinusLogProbMetric: 27.3385, val_loss: 27.8080, val_MinusLogProbMetric: 27.8080

Epoch 993: val_loss did not improve from 27.80732
196/196 - 39s - loss: 27.3385 - MinusLogProbMetric: 27.3385 - val_loss: 27.8080 - val_MinusLogProbMetric: 27.8080 - lr: 2.6042e-06 - 39s/epoch - 199ms/step
Epoch 994/1000
2023-10-25 12:32:40.089 
Epoch 994/1000 
	 loss: 27.3383, MinusLogProbMetric: 27.3383, val_loss: 27.8120, val_MinusLogProbMetric: 27.8120

Epoch 994: val_loss did not improve from 27.80732
196/196 - 40s - loss: 27.3383 - MinusLogProbMetric: 27.3383 - val_loss: 27.8120 - val_MinusLogProbMetric: 27.8120 - lr: 2.6042e-06 - 40s/epoch - 206ms/step
Epoch 995/1000
2023-10-25 12:33:18.524 
Epoch 995/1000 
	 loss: 27.3399, MinusLogProbMetric: 27.3399, val_loss: 27.8077, val_MinusLogProbMetric: 27.8077

Epoch 995: val_loss did not improve from 27.80732
196/196 - 38s - loss: 27.3399 - MinusLogProbMetric: 27.3399 - val_loss: 27.8077 - val_MinusLogProbMetric: 27.8077 - lr: 2.6042e-06 - 38s/epoch - 196ms/step
Epoch 996/1000
2023-10-25 12:33:57.634 
Epoch 996/1000 
	 loss: 27.3395, MinusLogProbMetric: 27.3395, val_loss: 27.8096, val_MinusLogProbMetric: 27.8096

Epoch 996: val_loss did not improve from 27.80732
196/196 - 39s - loss: 27.3395 - MinusLogProbMetric: 27.3395 - val_loss: 27.8096 - val_MinusLogProbMetric: 27.8096 - lr: 2.6042e-06 - 39s/epoch - 200ms/step
Epoch 997/1000
2023-10-25 12:34:38.396 
Epoch 997/1000 
	 loss: 27.3384, MinusLogProbMetric: 27.3384, val_loss: 27.8086, val_MinusLogProbMetric: 27.8086

Epoch 997: val_loss did not improve from 27.80732
196/196 - 41s - loss: 27.3384 - MinusLogProbMetric: 27.3384 - val_loss: 27.8086 - val_MinusLogProbMetric: 27.8086 - lr: 2.6042e-06 - 41s/epoch - 208ms/step
Epoch 998/1000
2023-10-25 12:35:17.455 
Epoch 998/1000 
	 loss: 27.3384, MinusLogProbMetric: 27.3384, val_loss: 27.8098, val_MinusLogProbMetric: 27.8098

Epoch 998: val_loss did not improve from 27.80732
196/196 - 39s - loss: 27.3384 - MinusLogProbMetric: 27.3384 - val_loss: 27.8098 - val_MinusLogProbMetric: 27.8098 - lr: 2.6042e-06 - 39s/epoch - 199ms/step
Epoch 999/1000
2023-10-25 12:35:57.733 
Epoch 999/1000 
	 loss: 27.3390, MinusLogProbMetric: 27.3390, val_loss: 27.8100, val_MinusLogProbMetric: 27.8100

Epoch 999: val_loss did not improve from 27.80732
196/196 - 40s - loss: 27.3390 - MinusLogProbMetric: 27.3390 - val_loss: 27.8100 - val_MinusLogProbMetric: 27.8100 - lr: 2.6042e-06 - 40s/epoch - 205ms/step
Epoch 1000/1000
2023-10-25 12:36:37.147 
Epoch 1000/1000 
	 loss: 27.3383, MinusLogProbMetric: 27.3383, val_loss: 27.8088, val_MinusLogProbMetric: 27.8088

Epoch 1000: val_loss did not improve from 27.80732
196/196 - 39s - loss: 27.3383 - MinusLogProbMetric: 27.3383 - val_loss: 27.8088 - val_MinusLogProbMetric: 27.8088 - lr: 2.6042e-06 - 39s/epoch - 201ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 440.
Model trained in 40808.26 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.80 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.05 s.
===========
Run 347/720 done in 40967.97 s.
===========

Directory ../../results/CsplineN_new/run_348/ already exists.
Skipping it.
===========
Run 348/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_349/ already exists.
Skipping it.
===========
Run 349/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_350/ already exists.
Skipping it.
===========
Run 350/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_351/ already exists.
Skipping it.
===========
Run 351/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_352/ already exists.
Skipping it.
===========
Run 352/720 already exists. Skipping it.
===========

===========
Generating train data for run 353.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_68"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_69 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  660960    
 yer)                                                            
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7f17c5e65ab0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1a2840a440>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1a2840a440>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f18107de7d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1888373580>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1888370760>, <keras.callbacks.ModelCheckpoint object at 0x7f1888372170>, <keras.callbacks.EarlyStopping object at 0x7f1888371870>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1888370ac0>, <keras.callbacks.TerminateOnNaN object at 0x7f1888373f40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-25 12:36:42.024752
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 12:37:33.884 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 52s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 52s/epoch - 264ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0003333333333333333.
===========
Generating train data for run 353.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_74"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_75 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  660960    
 yer)                                                            
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7f164c4cd3f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f164c45feb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f164c45feb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f166c2a12d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f178c228850>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f178c228dc0>, <keras.callbacks.ModelCheckpoint object at 0x7f178c228e80>, <keras.callbacks.EarlyStopping object at 0x7f178c2290f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f178c229120>, <keras.callbacks.TerminateOnNaN object at 0x7f178c228d60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-25 12:37:37.857150
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 12:38:30.393 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 52s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 52s/epoch - 267ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0001111111111111111.
===========
Generating train data for run 353.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_80"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_81 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7f164c4b87c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f18010a1c60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f18010a1c60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1aa05b2e60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1aa05b2830>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f18f828ac80>, <keras.callbacks.ModelCheckpoint object at 0x7f18f8289900>, <keras.callbacks.EarlyStopping object at 0x7f18f828b5b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f18f828bb20>, <keras.callbacks.TerminateOnNaN object at 0x7f18f8289ed0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-25 12:38:37.875525
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 12:39:33.869 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 56s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 56s/epoch - 285ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 3.703703703703703e-05.
===========
Generating train data for run 353.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_86"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_87 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7f1797367a90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1797437a60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1797437a60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f18e0778e80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f17973beaa0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f17973bf010>, <keras.callbacks.ModelCheckpoint object at 0x7f17973bf0d0>, <keras.callbacks.EarlyStopping object at 0x7f17973bf340>, <keras.callbacks.ReduceLROnPlateau object at 0x7f17973bf370>, <keras.callbacks.TerminateOnNaN object at 0x7f17973befb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-25 12:39:38.191530
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
WARNING:tensorflow:5 out of the last 196007 calls to <function Model.make_train_function.<locals>.train_function at 0x7f18017ea170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 12:40:32.749 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 54s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 54s/epoch - 278ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.2345679012345677e-05.
===========
Generating train data for run 353.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_92"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_93 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7f1720552380>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f19c8242b00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f19c8242b00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f19301b3b20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f166cd7f160>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f166cd7f6d0>, <keras.callbacks.ModelCheckpoint object at 0x7f166cd7f790>, <keras.callbacks.EarlyStopping object at 0x7f166cd7fa00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f166cd7fa30>, <keras.callbacks.TerminateOnNaN object at 0x7f166cd7f670>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-25 12:40:37.089755
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
WARNING:tensorflow:6 out of the last 196009 calls to <function Model.make_train_function.<locals>.train_function at 0x7f17c620f1c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 12:41:33.162 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 56s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 56s/epoch - 285ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.115226337448558e-06.
===========
Generating train data for run 353.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_98"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_99 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7f197ef0fb80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f178c2c3b20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f178c2c3b20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f17c4141f00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f18e07f4700>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f18e07f4c70>, <keras.callbacks.ModelCheckpoint object at 0x7f18e07f4d30>, <keras.callbacks.EarlyStopping object at 0x7f18e07f4fa0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f18e07f4fd0>, <keras.callbacks.TerminateOnNaN object at 0x7f18e07f4c10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-25 12:41:37.110592
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 12:42:31.827 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 55s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 55s/epoch - 279ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.3717421124828526e-06.
===========
Generating train data for run 353.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_104"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_105 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7f1830714a00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f183070d270>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f183070d270>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f166d0426e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f17d338ab60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f17d338b0d0>, <keras.callbacks.ModelCheckpoint object at 0x7f17d338b190>, <keras.callbacks.EarlyStopping object at 0x7f17d338b400>, <keras.callbacks.ReduceLROnPlateau object at 0x7f17d338b430>, <keras.callbacks.TerminateOnNaN object at 0x7f17d338b070>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-25 12:42:35.830820
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 12:43:31.787 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 56s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 56s/epoch - 285ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.572473708276175e-07.
===========
Generating train data for run 353.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_110"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_111 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7f19c86acf70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f17e03bc910>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f17e03bc910>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f18f84b1000>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f17d0d40f40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f17d0d41660>, <keras.callbacks.ModelCheckpoint object at 0x7f178c7a3100>, <keras.callbacks.EarlyStopping object at 0x7f178c7a2200>, <keras.callbacks.ReduceLROnPlateau object at 0x7f178c7a2590>, <keras.callbacks.TerminateOnNaN object at 0x7f178c7a1360>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-25 12:43:36.386948
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 12:44:32.113 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 56s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 56s/epoch - 284ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.524157902758725e-07.
===========
Generating train data for run 353.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_116"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_117 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7f17d2b035e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f18e077a380>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f18e077a380>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f178c2c14e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1e999bf4f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1e999be350>, <keras.callbacks.ModelCheckpoint object at 0x7f1e999be440>, <keras.callbacks.EarlyStopping object at 0x7f1e999bde70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1e999bf280>, <keras.callbacks.TerminateOnNaN object at 0x7f1e999bdd50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-25 12:44:36.512223
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 12:45:28.955 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 52s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 52s/epoch - 267ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.0805263425290834e-08.
===========
Generating train data for run 353.
===========
Train data generated in 0.12 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_122"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_123 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7f1795e27e50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1796333190>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1796333190>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1795f8fa90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1795e8b640>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1795e8bbb0>, <keras.callbacks.ModelCheckpoint object at 0x7f1795e8bc70>, <keras.callbacks.EarlyStopping object at 0x7f1795e8bee0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1795e8bf10>, <keras.callbacks.TerminateOnNaN object at 0x7f1795e8bb50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-25 12:45:31.733542
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 12:46:24.492 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 53s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 53s/epoch - 269ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.6935087808430278e-08.
===========
Generating train data for run 353.
===========
Train data generated in 0.13 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_353/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_353
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_128"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_129 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7f166c0eae00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1720497760>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1720497760>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f17d0bc6d70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f16c80fed40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_353/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f16c80ff2b0>, <keras.callbacks.ModelCheckpoint object at 0x7f16c80ff370>, <keras.callbacks.EarlyStopping object at 0x7f16c80ff5e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f16c80ff610>, <keras.callbacks.TerminateOnNaN object at 0x7f16c80ff250>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_353/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 353/720 with hyperparameters:
timestamp = 2023-10-25 12:46:27.185158
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 12:47:21.000 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6323.5312, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 54s - loss: nan - MinusLogProbMetric: 6323.5312 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 54s/epoch - 274ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.645029269476759e-09.
===========
Run 353/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 354.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_354/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_354/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_354/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_354
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_134"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_135 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  1645920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,645,920
Trainable params: 1,645,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7f18cc53f820>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f17c7514580>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f17c7514580>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1ac0266560>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1aa07493f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1aa0749960>, <keras.callbacks.ModelCheckpoint object at 0x7f1aa0749a20>, <keras.callbacks.EarlyStopping object at 0x7f1aa0749c90>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1aa0749cc0>, <keras.callbacks.TerminateOnNaN object at 0x7f1aa0749900>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_354/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 354/720 with hyperparameters:
timestamp = 2023-10-25 12:47:25.539561
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1645920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
2023-10-25 12:48:50.444 
Epoch 1/1000 
	 loss: 401.6162, MinusLogProbMetric: 401.6162, val_loss: 126.5814, val_MinusLogProbMetric: 126.5814

Epoch 1: val_loss improved from inf to 126.58139, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 85s - loss: 401.6162 - MinusLogProbMetric: 401.6162 - val_loss: 126.5814 - val_MinusLogProbMetric: 126.5814 - lr: 0.0010 - 85s/epoch - 435ms/step
Epoch 2/1000
2023-10-25 12:49:22.222 
Epoch 2/1000 
	 loss: 83.1945, MinusLogProbMetric: 83.1945, val_loss: 65.0480, val_MinusLogProbMetric: 65.0480

Epoch 2: val_loss improved from 126.58139 to 65.04803, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 32s - loss: 83.1945 - MinusLogProbMetric: 83.1945 - val_loss: 65.0480 - val_MinusLogProbMetric: 65.0480 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 3/1000
2023-10-25 12:49:54.145 
Epoch 3/1000 
	 loss: 60.2836, MinusLogProbMetric: 60.2836, val_loss: 54.8222, val_MinusLogProbMetric: 54.8222

Epoch 3: val_loss improved from 65.04803 to 54.82215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 32s - loss: 60.2836 - MinusLogProbMetric: 60.2836 - val_loss: 54.8222 - val_MinusLogProbMetric: 54.8222 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 4/1000
2023-10-25 12:50:26.319 
Epoch 4/1000 
	 loss: 51.7202, MinusLogProbMetric: 51.7202, val_loss: 49.7148, val_MinusLogProbMetric: 49.7148

Epoch 4: val_loss improved from 54.82215 to 49.71482, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 32s - loss: 51.7202 - MinusLogProbMetric: 51.7202 - val_loss: 49.7148 - val_MinusLogProbMetric: 49.7148 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 5/1000
2023-10-25 12:51:00.179 
Epoch 5/1000 
	 loss: 46.8812, MinusLogProbMetric: 46.8812, val_loss: 45.2461, val_MinusLogProbMetric: 45.2461

Epoch 5: val_loss improved from 49.71482 to 45.24612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 34s - loss: 46.8812 - MinusLogProbMetric: 46.8812 - val_loss: 45.2461 - val_MinusLogProbMetric: 45.2461 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 6/1000
2023-10-25 12:51:31.599 
Epoch 6/1000 
	 loss: 44.3406, MinusLogProbMetric: 44.3406, val_loss: 44.1907, val_MinusLogProbMetric: 44.1907

Epoch 6: val_loss improved from 45.24612 to 44.19072, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 32s - loss: 44.3406 - MinusLogProbMetric: 44.3406 - val_loss: 44.1907 - val_MinusLogProbMetric: 44.1907 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 7/1000
2023-10-25 12:52:04.963 
Epoch 7/1000 
	 loss: 42.4860, MinusLogProbMetric: 42.4860, val_loss: 41.8635, val_MinusLogProbMetric: 41.8635

Epoch 7: val_loss improved from 44.19072 to 41.86347, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 33s - loss: 42.4860 - MinusLogProbMetric: 42.4860 - val_loss: 41.8635 - val_MinusLogProbMetric: 41.8635 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 8/1000
2023-10-25 12:52:39.991 
Epoch 8/1000 
	 loss: 40.9297, MinusLogProbMetric: 40.9297, val_loss: 42.8106, val_MinusLogProbMetric: 42.8106

Epoch 8: val_loss did not improve from 41.86347
196/196 - 35s - loss: 40.9297 - MinusLogProbMetric: 40.9297 - val_loss: 42.8106 - val_MinusLogProbMetric: 42.8106 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 9/1000
2023-10-25 12:53:11.523 
Epoch 9/1000 
	 loss: 40.1522, MinusLogProbMetric: 40.1522, val_loss: 40.1596, val_MinusLogProbMetric: 40.1596

Epoch 9: val_loss improved from 41.86347 to 40.15963, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 32s - loss: 40.1522 - MinusLogProbMetric: 40.1522 - val_loss: 40.1596 - val_MinusLogProbMetric: 40.1596 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 10/1000
2023-10-25 12:53:45.729 
Epoch 10/1000 
	 loss: 39.2044, MinusLogProbMetric: 39.2044, val_loss: 38.3321, val_MinusLogProbMetric: 38.3321

Epoch 10: val_loss improved from 40.15963 to 38.33213, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 34s - loss: 39.2044 - MinusLogProbMetric: 39.2044 - val_loss: 38.3321 - val_MinusLogProbMetric: 38.3321 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 11/1000
2023-10-25 12:54:20.290 
Epoch 11/1000 
	 loss: 38.2406, MinusLogProbMetric: 38.2406, val_loss: 43.3277, val_MinusLogProbMetric: 43.3277

Epoch 11: val_loss did not improve from 38.33213
196/196 - 34s - loss: 38.2406 - MinusLogProbMetric: 38.2406 - val_loss: 43.3277 - val_MinusLogProbMetric: 43.3277 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 12/1000
2023-10-25 12:54:54.259 
Epoch 12/1000 
	 loss: 37.9109, MinusLogProbMetric: 37.9109, val_loss: 37.7202, val_MinusLogProbMetric: 37.7202

Epoch 12: val_loss improved from 38.33213 to 37.72025, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 37.9109 - MinusLogProbMetric: 37.9109 - val_loss: 37.7202 - val_MinusLogProbMetric: 37.7202 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 13/1000
2023-10-25 12:55:29.078 
Epoch 13/1000 
	 loss: 37.3512, MinusLogProbMetric: 37.3512, val_loss: 36.8154, val_MinusLogProbMetric: 36.8154

Epoch 13: val_loss improved from 37.72025 to 36.81540, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 37.3512 - MinusLogProbMetric: 37.3512 - val_loss: 36.8154 - val_MinusLogProbMetric: 36.8154 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 14/1000
2023-10-25 12:56:04.356 
Epoch 14/1000 
	 loss: 36.6177, MinusLogProbMetric: 36.6177, val_loss: 37.0299, val_MinusLogProbMetric: 37.0299

Epoch 14: val_loss did not improve from 36.81540
196/196 - 35s - loss: 36.6177 - MinusLogProbMetric: 36.6177 - val_loss: 37.0299 - val_MinusLogProbMetric: 37.0299 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 15/1000
2023-10-25 12:56:38.515 
Epoch 15/1000 
	 loss: 36.2080, MinusLogProbMetric: 36.2080, val_loss: 34.7491, val_MinusLogProbMetric: 34.7491

Epoch 15: val_loss improved from 36.81540 to 34.74905, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 36.2080 - MinusLogProbMetric: 36.2080 - val_loss: 34.7491 - val_MinusLogProbMetric: 34.7491 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 16/1000
2023-10-25 12:57:12.795 
Epoch 16/1000 
	 loss: 36.0071, MinusLogProbMetric: 36.0071, val_loss: 35.2863, val_MinusLogProbMetric: 35.2863

Epoch 16: val_loss did not improve from 34.74905
196/196 - 34s - loss: 36.0071 - MinusLogProbMetric: 36.0071 - val_loss: 35.2863 - val_MinusLogProbMetric: 35.2863 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 17/1000
2023-10-25 12:57:47.046 
Epoch 17/1000 
	 loss: 35.8799, MinusLogProbMetric: 35.8799, val_loss: 37.6101, val_MinusLogProbMetric: 37.6101

Epoch 17: val_loss did not improve from 34.74905
196/196 - 34s - loss: 35.8799 - MinusLogProbMetric: 35.8799 - val_loss: 37.6101 - val_MinusLogProbMetric: 37.6101 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 18/1000
2023-10-25 12:58:19.543 
Epoch 18/1000 
	 loss: 35.6921, MinusLogProbMetric: 35.6921, val_loss: 35.2068, val_MinusLogProbMetric: 35.2068

Epoch 18: val_loss did not improve from 34.74905
196/196 - 32s - loss: 35.6921 - MinusLogProbMetric: 35.6921 - val_loss: 35.2068 - val_MinusLogProbMetric: 35.2068 - lr: 0.0010 - 32s/epoch - 166ms/step
Epoch 19/1000
2023-10-25 12:58:53.870 
Epoch 19/1000 
	 loss: 34.7587, MinusLogProbMetric: 34.7587, val_loss: 35.5454, val_MinusLogProbMetric: 35.5454

Epoch 19: val_loss did not improve from 34.74905
196/196 - 34s - loss: 34.7587 - MinusLogProbMetric: 34.7587 - val_loss: 35.5454 - val_MinusLogProbMetric: 35.5454 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 20/1000
2023-10-25 12:59:28.324 
Epoch 20/1000 
	 loss: 34.9130, MinusLogProbMetric: 34.9130, val_loss: 33.7187, val_MinusLogProbMetric: 33.7187

Epoch 20: val_loss improved from 34.74905 to 33.71868, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 34.9130 - MinusLogProbMetric: 34.9130 - val_loss: 33.7187 - val_MinusLogProbMetric: 33.7187 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 21/1000
2023-10-25 13:00:03.342 
Epoch 21/1000 
	 loss: 34.7038, MinusLogProbMetric: 34.7038, val_loss: 36.7892, val_MinusLogProbMetric: 36.7892

Epoch 21: val_loss did not improve from 33.71868
196/196 - 35s - loss: 34.7038 - MinusLogProbMetric: 34.7038 - val_loss: 36.7892 - val_MinusLogProbMetric: 36.7892 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 22/1000
2023-10-25 13:00:38.089 
Epoch 22/1000 
	 loss: 34.7612, MinusLogProbMetric: 34.7612, val_loss: 34.1816, val_MinusLogProbMetric: 34.1816

Epoch 22: val_loss did not improve from 33.71868
196/196 - 35s - loss: 34.7612 - MinusLogProbMetric: 34.7612 - val_loss: 34.1816 - val_MinusLogProbMetric: 34.1816 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 23/1000
2023-10-25 13:01:12.504 
Epoch 23/1000 
	 loss: 34.3832, MinusLogProbMetric: 34.3832, val_loss: 34.5491, val_MinusLogProbMetric: 34.5491

Epoch 23: val_loss did not improve from 33.71868
196/196 - 34s - loss: 34.3832 - MinusLogProbMetric: 34.3832 - val_loss: 34.5491 - val_MinusLogProbMetric: 34.5491 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 24/1000
2023-10-25 13:01:47.141 
Epoch 24/1000 
	 loss: 33.8014, MinusLogProbMetric: 33.8014, val_loss: 33.7407, val_MinusLogProbMetric: 33.7407

Epoch 24: val_loss did not improve from 33.71868
196/196 - 35s - loss: 33.8014 - MinusLogProbMetric: 33.8014 - val_loss: 33.7407 - val_MinusLogProbMetric: 33.7407 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 25/1000
2023-10-25 13:02:21.464 
Epoch 25/1000 
	 loss: 34.1739, MinusLogProbMetric: 34.1739, val_loss: 33.5013, val_MinusLogProbMetric: 33.5013

Epoch 25: val_loss improved from 33.71868 to 33.50135, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 34.1739 - MinusLogProbMetric: 34.1739 - val_loss: 33.5013 - val_MinusLogProbMetric: 33.5013 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 26/1000
2023-10-25 13:02:56.446 
Epoch 26/1000 
	 loss: 33.7361, MinusLogProbMetric: 33.7361, val_loss: 33.0591, val_MinusLogProbMetric: 33.0591

Epoch 26: val_loss improved from 33.50135 to 33.05913, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 33.7361 - MinusLogProbMetric: 33.7361 - val_loss: 33.0591 - val_MinusLogProbMetric: 33.0591 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 27/1000
2023-10-25 13:03:30.665 
Epoch 27/1000 
	 loss: 34.0596, MinusLogProbMetric: 34.0596, val_loss: 34.5870, val_MinusLogProbMetric: 34.5870

Epoch 27: val_loss did not improve from 33.05913
196/196 - 34s - loss: 34.0596 - MinusLogProbMetric: 34.0596 - val_loss: 34.5870 - val_MinusLogProbMetric: 34.5870 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 28/1000
2023-10-25 13:04:04.059 
Epoch 28/1000 
	 loss: 33.9710, MinusLogProbMetric: 33.9710, val_loss: 33.4079, val_MinusLogProbMetric: 33.4079

Epoch 28: val_loss did not improve from 33.05913
196/196 - 33s - loss: 33.9710 - MinusLogProbMetric: 33.9710 - val_loss: 33.4079 - val_MinusLogProbMetric: 33.4079 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 29/1000
2023-10-25 13:04:38.679 
Epoch 29/1000 
	 loss: 33.4097, MinusLogProbMetric: 33.4097, val_loss: 33.2177, val_MinusLogProbMetric: 33.2177

Epoch 29: val_loss did not improve from 33.05913
196/196 - 35s - loss: 33.4097 - MinusLogProbMetric: 33.4097 - val_loss: 33.2177 - val_MinusLogProbMetric: 33.2177 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 30/1000
2023-10-25 13:05:13.174 
Epoch 30/1000 
	 loss: 33.2883, MinusLogProbMetric: 33.2883, val_loss: 32.9796, val_MinusLogProbMetric: 32.9796

Epoch 30: val_loss improved from 33.05913 to 32.97962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 33.2883 - MinusLogProbMetric: 33.2883 - val_loss: 32.9796 - val_MinusLogProbMetric: 32.9796 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 31/1000
2023-10-25 13:05:45.726 
Epoch 31/1000 
	 loss: 33.2119, MinusLogProbMetric: 33.2119, val_loss: 34.3975, val_MinusLogProbMetric: 34.3975

Epoch 31: val_loss did not improve from 32.97962
196/196 - 32s - loss: 33.2119 - MinusLogProbMetric: 33.2119 - val_loss: 34.3975 - val_MinusLogProbMetric: 34.3975 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 32/1000
2023-10-25 13:06:20.078 
Epoch 32/1000 
	 loss: 33.1815, MinusLogProbMetric: 33.1815, val_loss: 32.6394, val_MinusLogProbMetric: 32.6394

Epoch 32: val_loss improved from 32.97962 to 32.63944, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 33.1815 - MinusLogProbMetric: 33.1815 - val_loss: 32.6394 - val_MinusLogProbMetric: 32.6394 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 33/1000
2023-10-25 13:06:53.504 
Epoch 33/1000 
	 loss: 32.9964, MinusLogProbMetric: 32.9964, val_loss: 32.8021, val_MinusLogProbMetric: 32.8021

Epoch 33: val_loss did not improve from 32.63944
196/196 - 33s - loss: 32.9964 - MinusLogProbMetric: 32.9964 - val_loss: 32.8021 - val_MinusLogProbMetric: 32.8021 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 34/1000
2023-10-25 13:07:28.189 
Epoch 34/1000 
	 loss: 32.7127, MinusLogProbMetric: 32.7127, val_loss: 33.5226, val_MinusLogProbMetric: 33.5226

Epoch 34: val_loss did not improve from 32.63944
196/196 - 35s - loss: 32.7127 - MinusLogProbMetric: 32.7127 - val_loss: 33.5226 - val_MinusLogProbMetric: 33.5226 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 35/1000
2023-10-25 13:08:02.478 
Epoch 35/1000 
	 loss: 32.7307, MinusLogProbMetric: 32.7307, val_loss: 33.4588, val_MinusLogProbMetric: 33.4588

Epoch 35: val_loss did not improve from 32.63944
196/196 - 34s - loss: 32.7307 - MinusLogProbMetric: 32.7307 - val_loss: 33.4588 - val_MinusLogProbMetric: 33.4588 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 36/1000
2023-10-25 13:08:36.697 
Epoch 36/1000 
	 loss: 32.5200, MinusLogProbMetric: 32.5200, val_loss: 32.9277, val_MinusLogProbMetric: 32.9277

Epoch 36: val_loss did not improve from 32.63944
196/196 - 34s - loss: 32.5200 - MinusLogProbMetric: 32.5200 - val_loss: 32.9277 - val_MinusLogProbMetric: 32.9277 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 37/1000
2023-10-25 13:09:11.117 
Epoch 37/1000 
	 loss: 32.5287, MinusLogProbMetric: 32.5287, val_loss: 32.4119, val_MinusLogProbMetric: 32.4119

Epoch 37: val_loss improved from 32.63944 to 32.41189, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 32.5287 - MinusLogProbMetric: 32.5287 - val_loss: 32.4119 - val_MinusLogProbMetric: 32.4119 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 38/1000
2023-10-25 13:09:46.401 
Epoch 38/1000 
	 loss: 32.3379, MinusLogProbMetric: 32.3379, val_loss: 32.6070, val_MinusLogProbMetric: 32.6070

Epoch 38: val_loss did not improve from 32.41189
196/196 - 35s - loss: 32.3379 - MinusLogProbMetric: 32.3379 - val_loss: 32.6070 - val_MinusLogProbMetric: 32.6070 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 39/1000
2023-10-25 13:10:20.134 
Epoch 39/1000 
	 loss: 32.4098, MinusLogProbMetric: 32.4098, val_loss: 32.2070, val_MinusLogProbMetric: 32.2070

Epoch 39: val_loss improved from 32.41189 to 32.20705, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 34s - loss: 32.4098 - MinusLogProbMetric: 32.4098 - val_loss: 32.2070 - val_MinusLogProbMetric: 32.2070 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 40/1000
2023-10-25 13:10:55.309 
Epoch 40/1000 
	 loss: 32.4420, MinusLogProbMetric: 32.4420, val_loss: 31.8374, val_MinusLogProbMetric: 31.8374

Epoch 40: val_loss improved from 32.20705 to 31.83743, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 32.4420 - MinusLogProbMetric: 32.4420 - val_loss: 31.8374 - val_MinusLogProbMetric: 31.8374 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 41/1000
2023-10-25 13:11:30.576 
Epoch 41/1000 
	 loss: 32.2560, MinusLogProbMetric: 32.2560, val_loss: 35.5135, val_MinusLogProbMetric: 35.5135

Epoch 41: val_loss did not improve from 31.83743
196/196 - 35s - loss: 32.2560 - MinusLogProbMetric: 32.2560 - val_loss: 35.5135 - val_MinusLogProbMetric: 35.5135 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 42/1000
2023-10-25 13:12:05.333 
Epoch 42/1000 
	 loss: 32.1867, MinusLogProbMetric: 32.1867, val_loss: 34.4254, val_MinusLogProbMetric: 34.4254

Epoch 42: val_loss did not improve from 31.83743
196/196 - 35s - loss: 32.1867 - MinusLogProbMetric: 32.1867 - val_loss: 34.4254 - val_MinusLogProbMetric: 34.4254 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 43/1000
2023-10-25 13:12:39.914 
Epoch 43/1000 
	 loss: 32.2167, MinusLogProbMetric: 32.2167, val_loss: 32.0256, val_MinusLogProbMetric: 32.0256

Epoch 43: val_loss did not improve from 31.83743
196/196 - 35s - loss: 32.2167 - MinusLogProbMetric: 32.2167 - val_loss: 32.0256 - val_MinusLogProbMetric: 32.0256 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 44/1000
2023-10-25 13:13:14.371 
Epoch 44/1000 
	 loss: 31.8725, MinusLogProbMetric: 31.8725, val_loss: 32.5563, val_MinusLogProbMetric: 32.5563

Epoch 44: val_loss did not improve from 31.83743
196/196 - 34s - loss: 31.8725 - MinusLogProbMetric: 31.8725 - val_loss: 32.5563 - val_MinusLogProbMetric: 32.5563 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 45/1000
2023-10-25 13:13:48.574 
Epoch 45/1000 
	 loss: 32.0237, MinusLogProbMetric: 32.0237, val_loss: 31.8509, val_MinusLogProbMetric: 31.8509

Epoch 45: val_loss did not improve from 31.83743
196/196 - 34s - loss: 32.0237 - MinusLogProbMetric: 32.0237 - val_loss: 31.8509 - val_MinusLogProbMetric: 31.8509 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 46/1000
2023-10-25 13:14:23.191 
Epoch 46/1000 
	 loss: 31.9302, MinusLogProbMetric: 31.9302, val_loss: 31.9368, val_MinusLogProbMetric: 31.9368

Epoch 46: val_loss did not improve from 31.83743
196/196 - 35s - loss: 31.9302 - MinusLogProbMetric: 31.9302 - val_loss: 31.9368 - val_MinusLogProbMetric: 31.9368 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 47/1000
2023-10-25 13:14:57.349 
Epoch 47/1000 
	 loss: 31.6823, MinusLogProbMetric: 31.6823, val_loss: 31.4593, val_MinusLogProbMetric: 31.4593

Epoch 47: val_loss improved from 31.83743 to 31.45926, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 31.6823 - MinusLogProbMetric: 31.6823 - val_loss: 31.4593 - val_MinusLogProbMetric: 31.4593 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 48/1000
2023-10-25 13:15:32.705 
Epoch 48/1000 
	 loss: 31.7176, MinusLogProbMetric: 31.7176, val_loss: 31.5356, val_MinusLogProbMetric: 31.5356

Epoch 48: val_loss did not improve from 31.45926
196/196 - 35s - loss: 31.7176 - MinusLogProbMetric: 31.7176 - val_loss: 31.5356 - val_MinusLogProbMetric: 31.5356 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 49/1000
2023-10-25 13:16:06.725 
Epoch 49/1000 
	 loss: 31.7992, MinusLogProbMetric: 31.7992, val_loss: 32.4583, val_MinusLogProbMetric: 32.4583

Epoch 49: val_loss did not improve from 31.45926
196/196 - 34s - loss: 31.7992 - MinusLogProbMetric: 31.7992 - val_loss: 32.4583 - val_MinusLogProbMetric: 32.4583 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 50/1000
2023-10-25 13:16:40.969 
Epoch 50/1000 
	 loss: 31.7233, MinusLogProbMetric: 31.7233, val_loss: 31.3025, val_MinusLogProbMetric: 31.3025

Epoch 50: val_loss improved from 31.45926 to 31.30249, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 31.7233 - MinusLogProbMetric: 31.7233 - val_loss: 31.3025 - val_MinusLogProbMetric: 31.3025 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 51/1000
2023-10-25 13:17:15.964 
Epoch 51/1000 
	 loss: 31.4701, MinusLogProbMetric: 31.4701, val_loss: 32.3253, val_MinusLogProbMetric: 32.3253

Epoch 51: val_loss did not improve from 31.30249
196/196 - 34s - loss: 31.4701 - MinusLogProbMetric: 31.4701 - val_loss: 32.3253 - val_MinusLogProbMetric: 32.3253 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 52/1000
2023-10-25 13:17:50.290 
Epoch 52/1000 
	 loss: 31.7102, MinusLogProbMetric: 31.7102, val_loss: 31.6681, val_MinusLogProbMetric: 31.6681

Epoch 52: val_loss did not improve from 31.30249
196/196 - 34s - loss: 31.7102 - MinusLogProbMetric: 31.7102 - val_loss: 31.6681 - val_MinusLogProbMetric: 31.6681 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 53/1000
2023-10-25 13:18:25.019 
Epoch 53/1000 
	 loss: 31.3765, MinusLogProbMetric: 31.3765, val_loss: 31.9303, val_MinusLogProbMetric: 31.9303

Epoch 53: val_loss did not improve from 31.30249
196/196 - 35s - loss: 31.3765 - MinusLogProbMetric: 31.3765 - val_loss: 31.9303 - val_MinusLogProbMetric: 31.9303 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 54/1000
2023-10-25 13:18:59.308 
Epoch 54/1000 
	 loss: 31.2116, MinusLogProbMetric: 31.2116, val_loss: 35.5059, val_MinusLogProbMetric: 35.5059

Epoch 54: val_loss did not improve from 31.30249
196/196 - 34s - loss: 31.2116 - MinusLogProbMetric: 31.2116 - val_loss: 35.5059 - val_MinusLogProbMetric: 35.5059 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 55/1000
2023-10-25 13:19:33.967 
Epoch 55/1000 
	 loss: 31.6621, MinusLogProbMetric: 31.6621, val_loss: 31.9469, val_MinusLogProbMetric: 31.9469

Epoch 55: val_loss did not improve from 31.30249
196/196 - 35s - loss: 31.6621 - MinusLogProbMetric: 31.6621 - val_loss: 31.9469 - val_MinusLogProbMetric: 31.9469 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 56/1000
2023-10-25 13:20:08.599 
Epoch 56/1000 
	 loss: 31.3111, MinusLogProbMetric: 31.3111, val_loss: 31.4005, val_MinusLogProbMetric: 31.4005

Epoch 56: val_loss did not improve from 31.30249
196/196 - 35s - loss: 31.3111 - MinusLogProbMetric: 31.3111 - val_loss: 31.4005 - val_MinusLogProbMetric: 31.4005 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 57/1000
2023-10-25 13:20:43.120 
Epoch 57/1000 
	 loss: 31.3291, MinusLogProbMetric: 31.3291, val_loss: 31.1888, val_MinusLogProbMetric: 31.1888

Epoch 57: val_loss improved from 31.30249 to 31.18877, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 31.3291 - MinusLogProbMetric: 31.3291 - val_loss: 31.1888 - val_MinusLogProbMetric: 31.1888 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 58/1000
2023-10-25 13:21:18.005 
Epoch 58/1000 
	 loss: 31.3196, MinusLogProbMetric: 31.3196, val_loss: 30.9270, val_MinusLogProbMetric: 30.9270

Epoch 58: val_loss improved from 31.18877 to 30.92697, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 31.3196 - MinusLogProbMetric: 31.3196 - val_loss: 30.9270 - val_MinusLogProbMetric: 30.9270 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 59/1000
2023-10-25 13:21:53.319 
Epoch 59/1000 
	 loss: 31.2494, MinusLogProbMetric: 31.2494, val_loss: 32.1042, val_MinusLogProbMetric: 32.1042

Epoch 59: val_loss did not improve from 30.92697
196/196 - 35s - loss: 31.2494 - MinusLogProbMetric: 31.2494 - val_loss: 32.1042 - val_MinusLogProbMetric: 32.1042 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 60/1000
2023-10-25 13:22:27.738 
Epoch 60/1000 
	 loss: 31.2001, MinusLogProbMetric: 31.2001, val_loss: 31.3991, val_MinusLogProbMetric: 31.3991

Epoch 60: val_loss did not improve from 30.92697
196/196 - 34s - loss: 31.2001 - MinusLogProbMetric: 31.2001 - val_loss: 31.3991 - val_MinusLogProbMetric: 31.3991 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 61/1000
2023-10-25 13:23:02.447 
Epoch 61/1000 
	 loss: 30.9495, MinusLogProbMetric: 30.9495, val_loss: 32.6187, val_MinusLogProbMetric: 32.6187

Epoch 61: val_loss did not improve from 30.92697
196/196 - 35s - loss: 30.9495 - MinusLogProbMetric: 30.9495 - val_loss: 32.6187 - val_MinusLogProbMetric: 32.6187 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 62/1000
2023-10-25 13:23:37.011 
Epoch 62/1000 
	 loss: 31.4337, MinusLogProbMetric: 31.4337, val_loss: 31.2675, val_MinusLogProbMetric: 31.2675

Epoch 62: val_loss did not improve from 30.92697
196/196 - 35s - loss: 31.4337 - MinusLogProbMetric: 31.4337 - val_loss: 31.2675 - val_MinusLogProbMetric: 31.2675 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 63/1000
2023-10-25 13:24:11.771 
Epoch 63/1000 
	 loss: 31.0659, MinusLogProbMetric: 31.0659, val_loss: 33.4191, val_MinusLogProbMetric: 33.4191

Epoch 63: val_loss did not improve from 30.92697
196/196 - 35s - loss: 31.0659 - MinusLogProbMetric: 31.0659 - val_loss: 33.4191 - val_MinusLogProbMetric: 33.4191 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 64/1000
2023-10-25 13:24:46.663 
Epoch 64/1000 
	 loss: 31.1396, MinusLogProbMetric: 31.1396, val_loss: 30.7364, val_MinusLogProbMetric: 30.7364

Epoch 64: val_loss improved from 30.92697 to 30.73644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 31.1396 - MinusLogProbMetric: 31.1396 - val_loss: 30.7364 - val_MinusLogProbMetric: 30.7364 - lr: 0.0010 - 35s/epoch - 181ms/step
Epoch 65/1000
2023-10-25 13:25:17.850 
Epoch 65/1000 
	 loss: 31.0636, MinusLogProbMetric: 31.0636, val_loss: 31.0273, val_MinusLogProbMetric: 31.0273

Epoch 65: val_loss did not improve from 30.73644
196/196 - 31s - loss: 31.0636 - MinusLogProbMetric: 31.0636 - val_loss: 31.0273 - val_MinusLogProbMetric: 31.0273 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 66/1000
2023-10-25 13:25:52.413 
Epoch 66/1000 
	 loss: 30.8048, MinusLogProbMetric: 30.8048, val_loss: 30.8584, val_MinusLogProbMetric: 30.8584

Epoch 66: val_loss did not improve from 30.73644
196/196 - 35s - loss: 30.8048 - MinusLogProbMetric: 30.8048 - val_loss: 30.8584 - val_MinusLogProbMetric: 30.8584 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 67/1000
2023-10-25 13:26:26.807 
Epoch 67/1000 
	 loss: 31.0821, MinusLogProbMetric: 31.0821, val_loss: 30.8242, val_MinusLogProbMetric: 30.8242

Epoch 67: val_loss did not improve from 30.73644
196/196 - 34s - loss: 31.0821 - MinusLogProbMetric: 31.0821 - val_loss: 30.8242 - val_MinusLogProbMetric: 30.8242 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 68/1000
2023-10-25 13:27:01.372 
Epoch 68/1000 
	 loss: 30.8041, MinusLogProbMetric: 30.8041, val_loss: 31.2355, val_MinusLogProbMetric: 31.2355

Epoch 68: val_loss did not improve from 30.73644
196/196 - 35s - loss: 30.8041 - MinusLogProbMetric: 30.8041 - val_loss: 31.2355 - val_MinusLogProbMetric: 31.2355 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 69/1000
2023-10-25 13:27:34.872 
Epoch 69/1000 
	 loss: 30.8297, MinusLogProbMetric: 30.8297, val_loss: 31.3667, val_MinusLogProbMetric: 31.3667

Epoch 69: val_loss did not improve from 30.73644
196/196 - 33s - loss: 30.8297 - MinusLogProbMetric: 30.8297 - val_loss: 31.3667 - val_MinusLogProbMetric: 31.3667 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 70/1000
2023-10-25 13:28:09.256 
Epoch 70/1000 
	 loss: 30.7924, MinusLogProbMetric: 30.7924, val_loss: 31.2445, val_MinusLogProbMetric: 31.2445

Epoch 70: val_loss did not improve from 30.73644
196/196 - 34s - loss: 30.7924 - MinusLogProbMetric: 30.7924 - val_loss: 31.2445 - val_MinusLogProbMetric: 31.2445 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 71/1000
2023-10-25 13:28:43.492 
Epoch 71/1000 
	 loss: 31.0195, MinusLogProbMetric: 31.0195, val_loss: 30.9534, val_MinusLogProbMetric: 30.9534

Epoch 71: val_loss did not improve from 30.73644
196/196 - 34s - loss: 31.0195 - MinusLogProbMetric: 31.0195 - val_loss: 30.9534 - val_MinusLogProbMetric: 30.9534 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 72/1000
2023-10-25 13:29:17.973 
Epoch 72/1000 
	 loss: 30.7751, MinusLogProbMetric: 30.7751, val_loss: 31.8401, val_MinusLogProbMetric: 31.8401

Epoch 72: val_loss did not improve from 30.73644
196/196 - 34s - loss: 30.7751 - MinusLogProbMetric: 30.7751 - val_loss: 31.8401 - val_MinusLogProbMetric: 31.8401 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 73/1000
2023-10-25 13:29:52.532 
Epoch 73/1000 
	 loss: 30.7114, MinusLogProbMetric: 30.7114, val_loss: 30.2670, val_MinusLogProbMetric: 30.2670

Epoch 73: val_loss improved from 30.73644 to 30.26698, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 30.7114 - MinusLogProbMetric: 30.7114 - val_loss: 30.2670 - val_MinusLogProbMetric: 30.2670 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 74/1000
2023-10-25 13:30:27.695 
Epoch 74/1000 
	 loss: 30.8612, MinusLogProbMetric: 30.8612, val_loss: 31.3187, val_MinusLogProbMetric: 31.3187

Epoch 74: val_loss did not improve from 30.26698
196/196 - 35s - loss: 30.8612 - MinusLogProbMetric: 30.8612 - val_loss: 31.3187 - val_MinusLogProbMetric: 31.3187 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 75/1000
2023-10-25 13:31:02.532 
Epoch 75/1000 
	 loss: 30.5981, MinusLogProbMetric: 30.5981, val_loss: 30.5281, val_MinusLogProbMetric: 30.5281

Epoch 75: val_loss did not improve from 30.26698
196/196 - 35s - loss: 30.5981 - MinusLogProbMetric: 30.5981 - val_loss: 30.5281 - val_MinusLogProbMetric: 30.5281 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 76/1000
2023-10-25 13:31:36.651 
Epoch 76/1000 
	 loss: 30.5365, MinusLogProbMetric: 30.5365, val_loss: 31.5849, val_MinusLogProbMetric: 31.5849

Epoch 76: val_loss did not improve from 30.26698
196/196 - 34s - loss: 30.5365 - MinusLogProbMetric: 30.5365 - val_loss: 31.5849 - val_MinusLogProbMetric: 31.5849 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 77/1000
2023-10-25 13:32:11.290 
Epoch 77/1000 
	 loss: 30.5129, MinusLogProbMetric: 30.5129, val_loss: 31.5971, val_MinusLogProbMetric: 31.5971

Epoch 77: val_loss did not improve from 30.26698
196/196 - 35s - loss: 30.5129 - MinusLogProbMetric: 30.5129 - val_loss: 31.5971 - val_MinusLogProbMetric: 31.5971 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 78/1000
2023-10-25 13:32:45.701 
Epoch 78/1000 
	 loss: 30.5429, MinusLogProbMetric: 30.5429, val_loss: 30.4724, val_MinusLogProbMetric: 30.4724

Epoch 78: val_loss did not improve from 30.26698
196/196 - 34s - loss: 30.5429 - MinusLogProbMetric: 30.5429 - val_loss: 30.4724 - val_MinusLogProbMetric: 30.4724 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 79/1000
2023-10-25 13:33:20.447 
Epoch 79/1000 
	 loss: 30.4099, MinusLogProbMetric: 30.4099, val_loss: 31.9269, val_MinusLogProbMetric: 31.9269

Epoch 79: val_loss did not improve from 30.26698
196/196 - 35s - loss: 30.4099 - MinusLogProbMetric: 30.4099 - val_loss: 31.9269 - val_MinusLogProbMetric: 31.9269 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 80/1000
2023-10-25 13:33:54.907 
Epoch 80/1000 
	 loss: 30.3666, MinusLogProbMetric: 30.3666, val_loss: 30.4168, val_MinusLogProbMetric: 30.4168

Epoch 80: val_loss did not improve from 30.26698
196/196 - 34s - loss: 30.3666 - MinusLogProbMetric: 30.3666 - val_loss: 30.4168 - val_MinusLogProbMetric: 30.4168 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 81/1000
2023-10-25 13:34:29.330 
Epoch 81/1000 
	 loss: 30.3907, MinusLogProbMetric: 30.3907, val_loss: 31.3974, val_MinusLogProbMetric: 31.3974

Epoch 81: val_loss did not improve from 30.26698
196/196 - 34s - loss: 30.3907 - MinusLogProbMetric: 30.3907 - val_loss: 31.3974 - val_MinusLogProbMetric: 31.3974 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 82/1000
2023-10-25 13:35:02.768 
Epoch 82/1000 
	 loss: 30.5334, MinusLogProbMetric: 30.5334, val_loss: 31.1411, val_MinusLogProbMetric: 31.1411

Epoch 82: val_loss did not improve from 30.26698
196/196 - 33s - loss: 30.5334 - MinusLogProbMetric: 30.5334 - val_loss: 31.1411 - val_MinusLogProbMetric: 31.1411 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 83/1000
2023-10-25 13:35:37.146 
Epoch 83/1000 
	 loss: 30.4672, MinusLogProbMetric: 30.4672, val_loss: 30.9248, val_MinusLogProbMetric: 30.9248

Epoch 83: val_loss did not improve from 30.26698
196/196 - 34s - loss: 30.4672 - MinusLogProbMetric: 30.4672 - val_loss: 30.9248 - val_MinusLogProbMetric: 30.9248 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 84/1000
2023-10-25 13:36:12.043 
Epoch 84/1000 
	 loss: 30.4269, MinusLogProbMetric: 30.4269, val_loss: 30.5026, val_MinusLogProbMetric: 30.5026

Epoch 84: val_loss did not improve from 30.26698
196/196 - 35s - loss: 30.4269 - MinusLogProbMetric: 30.4269 - val_loss: 30.5026 - val_MinusLogProbMetric: 30.5026 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 85/1000
2023-10-25 13:36:45.773 
Epoch 85/1000 
	 loss: 30.4481, MinusLogProbMetric: 30.4481, val_loss: 32.1142, val_MinusLogProbMetric: 32.1142

Epoch 85: val_loss did not improve from 30.26698
196/196 - 34s - loss: 30.4481 - MinusLogProbMetric: 30.4481 - val_loss: 32.1142 - val_MinusLogProbMetric: 32.1142 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 86/1000
2023-10-25 13:37:20.514 
Epoch 86/1000 
	 loss: 30.3137, MinusLogProbMetric: 30.3137, val_loss: 31.2098, val_MinusLogProbMetric: 31.2098

Epoch 86: val_loss did not improve from 30.26698
196/196 - 35s - loss: 30.3137 - MinusLogProbMetric: 30.3137 - val_loss: 31.2098 - val_MinusLogProbMetric: 31.2098 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 87/1000
2023-10-25 13:37:55.237 
Epoch 87/1000 
	 loss: 30.2152, MinusLogProbMetric: 30.2152, val_loss: 30.9989, val_MinusLogProbMetric: 30.9989

Epoch 87: val_loss did not improve from 30.26698
196/196 - 35s - loss: 30.2152 - MinusLogProbMetric: 30.2152 - val_loss: 30.9989 - val_MinusLogProbMetric: 30.9989 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 88/1000
2023-10-25 13:38:29.903 
Epoch 88/1000 
	 loss: 30.1944, MinusLogProbMetric: 30.1944, val_loss: 29.9853, val_MinusLogProbMetric: 29.9853

Epoch 88: val_loss improved from 30.26698 to 29.98526, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 30.1944 - MinusLogProbMetric: 30.1944 - val_loss: 29.9853 - val_MinusLogProbMetric: 29.9853 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 89/1000
2023-10-25 13:39:04.053 
Epoch 89/1000 
	 loss: 30.4080, MinusLogProbMetric: 30.4080, val_loss: 30.5932, val_MinusLogProbMetric: 30.5932

Epoch 89: val_loss did not improve from 29.98526
196/196 - 34s - loss: 30.4080 - MinusLogProbMetric: 30.4080 - val_loss: 30.5932 - val_MinusLogProbMetric: 30.5932 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 90/1000
2023-10-25 13:39:38.591 
Epoch 90/1000 
	 loss: 30.2800, MinusLogProbMetric: 30.2800, val_loss: 30.6573, val_MinusLogProbMetric: 30.6573

Epoch 90: val_loss did not improve from 29.98526
196/196 - 35s - loss: 30.2800 - MinusLogProbMetric: 30.2800 - val_loss: 30.6573 - val_MinusLogProbMetric: 30.6573 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 91/1000
2023-10-25 13:40:13.172 
Epoch 91/1000 
	 loss: 30.4091, MinusLogProbMetric: 30.4091, val_loss: 30.6113, val_MinusLogProbMetric: 30.6113

Epoch 91: val_loss did not improve from 29.98526
196/196 - 35s - loss: 30.4091 - MinusLogProbMetric: 30.4091 - val_loss: 30.6113 - val_MinusLogProbMetric: 30.6113 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 92/1000
2023-10-25 13:40:47.913 
Epoch 92/1000 
	 loss: 30.1282, MinusLogProbMetric: 30.1282, val_loss: 31.2940, val_MinusLogProbMetric: 31.2940

Epoch 92: val_loss did not improve from 29.98526
196/196 - 35s - loss: 30.1282 - MinusLogProbMetric: 30.1282 - val_loss: 31.2940 - val_MinusLogProbMetric: 31.2940 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 93/1000
2023-10-25 13:41:22.374 
Epoch 93/1000 
	 loss: 30.1659, MinusLogProbMetric: 30.1659, val_loss: 34.3833, val_MinusLogProbMetric: 34.3833

Epoch 93: val_loss did not improve from 29.98526
196/196 - 34s - loss: 30.1659 - MinusLogProbMetric: 30.1659 - val_loss: 34.3833 - val_MinusLogProbMetric: 34.3833 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 94/1000
2023-10-25 13:41:55.977 
Epoch 94/1000 
	 loss: 30.2471, MinusLogProbMetric: 30.2471, val_loss: 30.2908, val_MinusLogProbMetric: 30.2908

Epoch 94: val_loss did not improve from 29.98526
196/196 - 34s - loss: 30.2471 - MinusLogProbMetric: 30.2471 - val_loss: 30.2908 - val_MinusLogProbMetric: 30.2908 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 95/1000
2023-10-25 13:42:30.497 
Epoch 95/1000 
	 loss: 30.2368, MinusLogProbMetric: 30.2368, val_loss: 31.2584, val_MinusLogProbMetric: 31.2584

Epoch 95: val_loss did not improve from 29.98526
196/196 - 35s - loss: 30.2368 - MinusLogProbMetric: 30.2368 - val_loss: 31.2584 - val_MinusLogProbMetric: 31.2584 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 96/1000
2023-10-25 13:43:05.060 
Epoch 96/1000 
	 loss: 30.1801, MinusLogProbMetric: 30.1801, val_loss: 31.2001, val_MinusLogProbMetric: 31.2001

Epoch 96: val_loss did not improve from 29.98526
196/196 - 35s - loss: 30.1801 - MinusLogProbMetric: 30.1801 - val_loss: 31.2001 - val_MinusLogProbMetric: 31.2001 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 97/1000
2023-10-25 13:43:39.668 
Epoch 97/1000 
	 loss: 30.3245, MinusLogProbMetric: 30.3245, val_loss: 31.2148, val_MinusLogProbMetric: 31.2148

Epoch 97: val_loss did not improve from 29.98526
196/196 - 35s - loss: 30.3245 - MinusLogProbMetric: 30.3245 - val_loss: 31.2148 - val_MinusLogProbMetric: 31.2148 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 98/1000
2023-10-25 13:44:14.300 
Epoch 98/1000 
	 loss: 29.9743, MinusLogProbMetric: 29.9743, val_loss: 30.2415, val_MinusLogProbMetric: 30.2415

Epoch 98: val_loss did not improve from 29.98526
196/196 - 35s - loss: 29.9743 - MinusLogProbMetric: 29.9743 - val_loss: 30.2415 - val_MinusLogProbMetric: 30.2415 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 99/1000
2023-10-25 13:44:49.603 
Epoch 99/1000 
	 loss: 30.1120, MinusLogProbMetric: 30.1120, val_loss: 30.1397, val_MinusLogProbMetric: 30.1397

Epoch 99: val_loss did not improve from 29.98526
196/196 - 35s - loss: 30.1120 - MinusLogProbMetric: 30.1120 - val_loss: 30.1397 - val_MinusLogProbMetric: 30.1397 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 100/1000
2023-10-25 13:45:24.664 
Epoch 100/1000 
	 loss: 29.9813, MinusLogProbMetric: 29.9813, val_loss: 31.5872, val_MinusLogProbMetric: 31.5872

Epoch 100: val_loss did not improve from 29.98526
196/196 - 35s - loss: 29.9813 - MinusLogProbMetric: 29.9813 - val_loss: 31.5872 - val_MinusLogProbMetric: 31.5872 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 101/1000
2023-10-25 13:45:59.076 
Epoch 101/1000 
	 loss: 30.0376, MinusLogProbMetric: 30.0376, val_loss: 30.8126, val_MinusLogProbMetric: 30.8126

Epoch 101: val_loss did not improve from 29.98526
196/196 - 34s - loss: 30.0376 - MinusLogProbMetric: 30.0376 - val_loss: 30.8126 - val_MinusLogProbMetric: 30.8126 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 102/1000
2023-10-25 13:46:33.393 
Epoch 102/1000 
	 loss: 29.8465, MinusLogProbMetric: 29.8465, val_loss: 31.3871, val_MinusLogProbMetric: 31.3871

Epoch 102: val_loss did not improve from 29.98526
196/196 - 34s - loss: 29.8465 - MinusLogProbMetric: 29.8465 - val_loss: 31.3871 - val_MinusLogProbMetric: 31.3871 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 103/1000
2023-10-25 13:47:07.687 
Epoch 103/1000 
	 loss: 30.0331, MinusLogProbMetric: 30.0331, val_loss: 31.1442, val_MinusLogProbMetric: 31.1442

Epoch 103: val_loss did not improve from 29.98526
196/196 - 34s - loss: 30.0331 - MinusLogProbMetric: 30.0331 - val_loss: 31.1442 - val_MinusLogProbMetric: 31.1442 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 104/1000
2023-10-25 13:47:42.020 
Epoch 104/1000 
	 loss: 30.0455, MinusLogProbMetric: 30.0455, val_loss: 30.6517, val_MinusLogProbMetric: 30.6517

Epoch 104: val_loss did not improve from 29.98526
196/196 - 34s - loss: 30.0455 - MinusLogProbMetric: 30.0455 - val_loss: 30.6517 - val_MinusLogProbMetric: 30.6517 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 105/1000
2023-10-25 13:48:16.793 
Epoch 105/1000 
	 loss: 29.9820, MinusLogProbMetric: 29.9820, val_loss: 30.0708, val_MinusLogProbMetric: 30.0708

Epoch 105: val_loss did not improve from 29.98526
196/196 - 35s - loss: 29.9820 - MinusLogProbMetric: 29.9820 - val_loss: 30.0708 - val_MinusLogProbMetric: 30.0708 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 106/1000
2023-10-25 13:48:51.500 
Epoch 106/1000 
	 loss: 29.9176, MinusLogProbMetric: 29.9176, val_loss: 30.2857, val_MinusLogProbMetric: 30.2857

Epoch 106: val_loss did not improve from 29.98526
196/196 - 35s - loss: 29.9176 - MinusLogProbMetric: 29.9176 - val_loss: 30.2857 - val_MinusLogProbMetric: 30.2857 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 107/1000
2023-10-25 13:49:25.906 
Epoch 107/1000 
	 loss: 29.9104, MinusLogProbMetric: 29.9104, val_loss: 30.1762, val_MinusLogProbMetric: 30.1762

Epoch 107: val_loss did not improve from 29.98526
196/196 - 34s - loss: 29.9104 - MinusLogProbMetric: 29.9104 - val_loss: 30.1762 - val_MinusLogProbMetric: 30.1762 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 108/1000
2023-10-25 13:50:00.653 
Epoch 108/1000 
	 loss: 29.9977, MinusLogProbMetric: 29.9977, val_loss: 30.8322, val_MinusLogProbMetric: 30.8322

Epoch 108: val_loss did not improve from 29.98526
196/196 - 35s - loss: 29.9977 - MinusLogProbMetric: 29.9977 - val_loss: 30.8322 - val_MinusLogProbMetric: 30.8322 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 109/1000
2023-10-25 13:50:35.411 
Epoch 109/1000 
	 loss: 29.8556, MinusLogProbMetric: 29.8556, val_loss: 30.8802, val_MinusLogProbMetric: 30.8802

Epoch 109: val_loss did not improve from 29.98526
196/196 - 35s - loss: 29.8556 - MinusLogProbMetric: 29.8556 - val_loss: 30.8802 - val_MinusLogProbMetric: 30.8802 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 110/1000
2023-10-25 13:51:09.616 
Epoch 110/1000 
	 loss: 29.7021, MinusLogProbMetric: 29.7021, val_loss: 30.0943, val_MinusLogProbMetric: 30.0943

Epoch 110: val_loss did not improve from 29.98526
196/196 - 34s - loss: 29.7021 - MinusLogProbMetric: 29.7021 - val_loss: 30.0943 - val_MinusLogProbMetric: 30.0943 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 111/1000
2023-10-25 13:51:44.022 
Epoch 111/1000 
	 loss: 29.7339, MinusLogProbMetric: 29.7339, val_loss: 30.1294, val_MinusLogProbMetric: 30.1294

Epoch 111: val_loss did not improve from 29.98526
196/196 - 34s - loss: 29.7339 - MinusLogProbMetric: 29.7339 - val_loss: 30.1294 - val_MinusLogProbMetric: 30.1294 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 112/1000
2023-10-25 13:52:17.116 
Epoch 112/1000 
	 loss: 29.9557, MinusLogProbMetric: 29.9557, val_loss: 30.5295, val_MinusLogProbMetric: 30.5295

Epoch 112: val_loss did not improve from 29.98526
196/196 - 33s - loss: 29.9557 - MinusLogProbMetric: 29.9557 - val_loss: 30.5295 - val_MinusLogProbMetric: 30.5295 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 113/1000
2023-10-25 13:52:51.216 
Epoch 113/1000 
	 loss: 29.7434, MinusLogProbMetric: 29.7434, val_loss: 29.7205, val_MinusLogProbMetric: 29.7205

Epoch 113: val_loss improved from 29.98526 to 29.72046, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 29.7434 - MinusLogProbMetric: 29.7434 - val_loss: 29.7205 - val_MinusLogProbMetric: 29.7205 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 114/1000
2023-10-25 13:53:25.866 
Epoch 114/1000 
	 loss: 29.9093, MinusLogProbMetric: 29.9093, val_loss: 30.3808, val_MinusLogProbMetric: 30.3808

Epoch 114: val_loss did not improve from 29.72046
196/196 - 34s - loss: 29.9093 - MinusLogProbMetric: 29.9093 - val_loss: 30.3808 - val_MinusLogProbMetric: 30.3808 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 115/1000
2023-10-25 13:54:00.514 
Epoch 115/1000 
	 loss: 29.9392, MinusLogProbMetric: 29.9392, val_loss: 31.3339, val_MinusLogProbMetric: 31.3339

Epoch 115: val_loss did not improve from 29.72046
196/196 - 35s - loss: 29.9392 - MinusLogProbMetric: 29.9392 - val_loss: 31.3339 - val_MinusLogProbMetric: 31.3339 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 116/1000
2023-10-25 13:54:34.834 
Epoch 116/1000 
	 loss: 30.0310, MinusLogProbMetric: 30.0310, val_loss: 30.3500, val_MinusLogProbMetric: 30.3500

Epoch 116: val_loss did not improve from 29.72046
196/196 - 34s - loss: 30.0310 - MinusLogProbMetric: 30.0310 - val_loss: 30.3500 - val_MinusLogProbMetric: 30.3500 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 117/1000
2023-10-25 13:55:06.454 
Epoch 117/1000 
	 loss: 29.8067, MinusLogProbMetric: 29.8067, val_loss: 29.9485, val_MinusLogProbMetric: 29.9485

Epoch 117: val_loss did not improve from 29.72046
196/196 - 32s - loss: 29.8067 - MinusLogProbMetric: 29.8067 - val_loss: 29.9485 - val_MinusLogProbMetric: 29.9485 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 118/1000
2023-10-25 13:55:38.781 
Epoch 118/1000 
	 loss: 29.6504, MinusLogProbMetric: 29.6504, val_loss: 30.6702, val_MinusLogProbMetric: 30.6702

Epoch 118: val_loss did not improve from 29.72046
196/196 - 32s - loss: 29.6504 - MinusLogProbMetric: 29.6504 - val_loss: 30.6702 - val_MinusLogProbMetric: 30.6702 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 119/1000
2023-10-25 13:56:13.201 
Epoch 119/1000 
	 loss: 29.8500, MinusLogProbMetric: 29.8500, val_loss: 31.1000, val_MinusLogProbMetric: 31.1000

Epoch 119: val_loss did not improve from 29.72046
196/196 - 34s - loss: 29.8500 - MinusLogProbMetric: 29.8500 - val_loss: 31.1000 - val_MinusLogProbMetric: 31.1000 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 120/1000
2023-10-25 13:56:47.310 
Epoch 120/1000 
	 loss: 29.6432, MinusLogProbMetric: 29.6432, val_loss: 29.6377, val_MinusLogProbMetric: 29.6377

Epoch 120: val_loss improved from 29.72046 to 29.63772, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 29.6432 - MinusLogProbMetric: 29.6432 - val_loss: 29.6377 - val_MinusLogProbMetric: 29.6377 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 121/1000
2023-10-25 13:57:21.689 
Epoch 121/1000 
	 loss: 29.5729, MinusLogProbMetric: 29.5729, val_loss: 30.4263, val_MinusLogProbMetric: 30.4263

Epoch 121: val_loss did not improve from 29.63772
196/196 - 34s - loss: 29.5729 - MinusLogProbMetric: 29.5729 - val_loss: 30.4263 - val_MinusLogProbMetric: 30.4263 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 122/1000
2023-10-25 13:57:55.823 
Epoch 122/1000 
	 loss: 29.7205, MinusLogProbMetric: 29.7205, val_loss: 29.5766, val_MinusLogProbMetric: 29.5766

Epoch 122: val_loss improved from 29.63772 to 29.57664, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 29.7205 - MinusLogProbMetric: 29.7205 - val_loss: 29.5766 - val_MinusLogProbMetric: 29.5766 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 123/1000
2023-10-25 13:58:30.020 
Epoch 123/1000 
	 loss: 29.6255, MinusLogProbMetric: 29.6255, val_loss: 29.9101, val_MinusLogProbMetric: 29.9101

Epoch 123: val_loss did not improve from 29.57664
196/196 - 34s - loss: 29.6255 - MinusLogProbMetric: 29.6255 - val_loss: 29.9101 - val_MinusLogProbMetric: 29.9101 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 124/1000
2023-10-25 13:59:04.637 
Epoch 124/1000 
	 loss: 29.6544, MinusLogProbMetric: 29.6544, val_loss: 30.1118, val_MinusLogProbMetric: 30.1118

Epoch 124: val_loss did not improve from 29.57664
196/196 - 35s - loss: 29.6544 - MinusLogProbMetric: 29.6544 - val_loss: 30.1118 - val_MinusLogProbMetric: 30.1118 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 125/1000
2023-10-25 13:59:37.685 
Epoch 125/1000 
	 loss: 29.7165, MinusLogProbMetric: 29.7165, val_loss: 30.0834, val_MinusLogProbMetric: 30.0834

Epoch 125: val_loss did not improve from 29.57664
196/196 - 33s - loss: 29.7165 - MinusLogProbMetric: 29.7165 - val_loss: 30.0834 - val_MinusLogProbMetric: 30.0834 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 126/1000
2023-10-25 14:00:11.937 
Epoch 126/1000 
	 loss: 29.6637, MinusLogProbMetric: 29.6637, val_loss: 29.5831, val_MinusLogProbMetric: 29.5831

Epoch 126: val_loss did not improve from 29.57664
196/196 - 34s - loss: 29.6637 - MinusLogProbMetric: 29.6637 - val_loss: 29.5831 - val_MinusLogProbMetric: 29.5831 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 127/1000
2023-10-25 14:00:46.568 
Epoch 127/1000 
	 loss: 29.5576, MinusLogProbMetric: 29.5576, val_loss: 29.9846, val_MinusLogProbMetric: 29.9846

Epoch 127: val_loss did not improve from 29.57664
196/196 - 35s - loss: 29.5576 - MinusLogProbMetric: 29.5576 - val_loss: 29.9846 - val_MinusLogProbMetric: 29.9846 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 128/1000
2023-10-25 14:01:21.074 
Epoch 128/1000 
	 loss: 29.3883, MinusLogProbMetric: 29.3883, val_loss: 29.4400, val_MinusLogProbMetric: 29.4400

Epoch 128: val_loss improved from 29.57664 to 29.43999, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 29.3883 - MinusLogProbMetric: 29.3883 - val_loss: 29.4400 - val_MinusLogProbMetric: 29.4400 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 129/1000
2023-10-25 14:01:56.075 
Epoch 129/1000 
	 loss: 29.5702, MinusLogProbMetric: 29.5702, val_loss: 30.0917, val_MinusLogProbMetric: 30.0917

Epoch 129: val_loss did not improve from 29.43999
196/196 - 34s - loss: 29.5702 - MinusLogProbMetric: 29.5702 - val_loss: 30.0917 - val_MinusLogProbMetric: 30.0917 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 130/1000
2023-10-25 14:02:30.404 
Epoch 130/1000 
	 loss: 29.8717, MinusLogProbMetric: 29.8717, val_loss: 29.9536, val_MinusLogProbMetric: 29.9536

Epoch 130: val_loss did not improve from 29.43999
196/196 - 34s - loss: 29.8717 - MinusLogProbMetric: 29.8717 - val_loss: 29.9536 - val_MinusLogProbMetric: 29.9536 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 131/1000
2023-10-25 14:03:04.914 
Epoch 131/1000 
	 loss: 29.5506, MinusLogProbMetric: 29.5506, val_loss: 29.9087, val_MinusLogProbMetric: 29.9087

Epoch 131: val_loss did not improve from 29.43999
196/196 - 35s - loss: 29.5506 - MinusLogProbMetric: 29.5506 - val_loss: 29.9087 - val_MinusLogProbMetric: 29.9087 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 132/1000
2023-10-25 14:03:39.415 
Epoch 132/1000 
	 loss: 29.3843, MinusLogProbMetric: 29.3843, val_loss: 29.8723, val_MinusLogProbMetric: 29.8723

Epoch 132: val_loss did not improve from 29.43999
196/196 - 34s - loss: 29.3843 - MinusLogProbMetric: 29.3843 - val_loss: 29.8723 - val_MinusLogProbMetric: 29.8723 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 133/1000
2023-10-25 14:04:13.598 
Epoch 133/1000 
	 loss: 29.4507, MinusLogProbMetric: 29.4507, val_loss: 31.8377, val_MinusLogProbMetric: 31.8377

Epoch 133: val_loss did not improve from 29.43999
196/196 - 34s - loss: 29.4507 - MinusLogProbMetric: 29.4507 - val_loss: 31.8377 - val_MinusLogProbMetric: 31.8377 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 134/1000
2023-10-25 14:04:47.562 
Epoch 134/1000 
	 loss: 29.6268, MinusLogProbMetric: 29.6268, val_loss: 30.1162, val_MinusLogProbMetric: 30.1162

Epoch 134: val_loss did not improve from 29.43999
196/196 - 34s - loss: 29.6268 - MinusLogProbMetric: 29.6268 - val_loss: 30.1162 - val_MinusLogProbMetric: 30.1162 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 135/1000
2023-10-25 14:05:21.094 
Epoch 135/1000 
	 loss: 29.4129, MinusLogProbMetric: 29.4129, val_loss: 29.4741, val_MinusLogProbMetric: 29.4741

Epoch 135: val_loss did not improve from 29.43999
196/196 - 34s - loss: 29.4129 - MinusLogProbMetric: 29.4129 - val_loss: 29.4741 - val_MinusLogProbMetric: 29.4741 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 136/1000
2023-10-25 14:05:55.247 
Epoch 136/1000 
	 loss: 29.5623, MinusLogProbMetric: 29.5623, val_loss: 30.1713, val_MinusLogProbMetric: 30.1713

Epoch 136: val_loss did not improve from 29.43999
196/196 - 34s - loss: 29.5623 - MinusLogProbMetric: 29.5623 - val_loss: 30.1713 - val_MinusLogProbMetric: 30.1713 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 137/1000
2023-10-25 14:06:28.834 
Epoch 137/1000 
	 loss: 29.4534, MinusLogProbMetric: 29.4534, val_loss: 29.8563, val_MinusLogProbMetric: 29.8563

Epoch 137: val_loss did not improve from 29.43999
196/196 - 34s - loss: 29.4534 - MinusLogProbMetric: 29.4534 - val_loss: 29.8563 - val_MinusLogProbMetric: 29.8563 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 138/1000
2023-10-25 14:07:03.416 
Epoch 138/1000 
	 loss: 29.5556, MinusLogProbMetric: 29.5556, val_loss: 29.8424, val_MinusLogProbMetric: 29.8424

Epoch 138: val_loss did not improve from 29.43999
196/196 - 35s - loss: 29.5556 - MinusLogProbMetric: 29.5556 - val_loss: 29.8424 - val_MinusLogProbMetric: 29.8424 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 139/1000
2023-10-25 14:07:38.198 
Epoch 139/1000 
	 loss: 29.5706, MinusLogProbMetric: 29.5706, val_loss: 29.7314, val_MinusLogProbMetric: 29.7314

Epoch 139: val_loss did not improve from 29.43999
196/196 - 35s - loss: 29.5706 - MinusLogProbMetric: 29.5706 - val_loss: 29.7314 - val_MinusLogProbMetric: 29.7314 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 140/1000
2023-10-25 14:08:13.014 
Epoch 140/1000 
	 loss: 29.4229, MinusLogProbMetric: 29.4229, val_loss: 29.9893, val_MinusLogProbMetric: 29.9893

Epoch 140: val_loss did not improve from 29.43999
196/196 - 35s - loss: 29.4229 - MinusLogProbMetric: 29.4229 - val_loss: 29.9893 - val_MinusLogProbMetric: 29.9893 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 141/1000
2023-10-25 14:08:47.560 
Epoch 141/1000 
	 loss: 29.3604, MinusLogProbMetric: 29.3604, val_loss: 29.5966, val_MinusLogProbMetric: 29.5966

Epoch 141: val_loss did not improve from 29.43999
196/196 - 35s - loss: 29.3604 - MinusLogProbMetric: 29.3604 - val_loss: 29.5966 - val_MinusLogProbMetric: 29.5966 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 142/1000
2023-10-25 14:09:21.972 
Epoch 142/1000 
	 loss: 29.3765, MinusLogProbMetric: 29.3765, val_loss: 29.7096, val_MinusLogProbMetric: 29.7096

Epoch 142: val_loss did not improve from 29.43999
196/196 - 34s - loss: 29.3765 - MinusLogProbMetric: 29.3765 - val_loss: 29.7096 - val_MinusLogProbMetric: 29.7096 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 143/1000
2023-10-25 14:09:56.537 
Epoch 143/1000 
	 loss: 29.4237, MinusLogProbMetric: 29.4237, val_loss: 30.1468, val_MinusLogProbMetric: 30.1468

Epoch 143: val_loss did not improve from 29.43999
196/196 - 35s - loss: 29.4237 - MinusLogProbMetric: 29.4237 - val_loss: 30.1468 - val_MinusLogProbMetric: 30.1468 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 144/1000
2023-10-25 14:10:30.740 
Epoch 144/1000 
	 loss: 29.3651, MinusLogProbMetric: 29.3651, val_loss: 29.6558, val_MinusLogProbMetric: 29.6558

Epoch 144: val_loss did not improve from 29.43999
196/196 - 34s - loss: 29.3651 - MinusLogProbMetric: 29.3651 - val_loss: 29.6558 - val_MinusLogProbMetric: 29.6558 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 145/1000
2023-10-25 14:11:04.671 
Epoch 145/1000 
	 loss: 29.4818, MinusLogProbMetric: 29.4818, val_loss: 30.7904, val_MinusLogProbMetric: 30.7904

Epoch 145: val_loss did not improve from 29.43999
196/196 - 34s - loss: 29.4818 - MinusLogProbMetric: 29.4818 - val_loss: 30.7904 - val_MinusLogProbMetric: 30.7904 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 146/1000
2023-10-25 14:11:39.106 
Epoch 146/1000 
	 loss: 29.5121, MinusLogProbMetric: 29.5121, val_loss: 30.2157, val_MinusLogProbMetric: 30.2157

Epoch 146: val_loss did not improve from 29.43999
196/196 - 34s - loss: 29.5121 - MinusLogProbMetric: 29.5121 - val_loss: 30.2157 - val_MinusLogProbMetric: 30.2157 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 147/1000
2023-10-25 14:12:13.625 
Epoch 147/1000 
	 loss: 29.3307, MinusLogProbMetric: 29.3307, val_loss: 29.5166, val_MinusLogProbMetric: 29.5166

Epoch 147: val_loss did not improve from 29.43999
196/196 - 35s - loss: 29.3307 - MinusLogProbMetric: 29.3307 - val_loss: 29.5166 - val_MinusLogProbMetric: 29.5166 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 148/1000
2023-10-25 14:12:48.136 
Epoch 148/1000 
	 loss: 29.3667, MinusLogProbMetric: 29.3667, val_loss: 29.5705, val_MinusLogProbMetric: 29.5705

Epoch 148: val_loss did not improve from 29.43999
196/196 - 35s - loss: 29.3667 - MinusLogProbMetric: 29.3667 - val_loss: 29.5705 - val_MinusLogProbMetric: 29.5705 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 149/1000
2023-10-25 14:13:22.882 
Epoch 149/1000 
	 loss: 29.4242, MinusLogProbMetric: 29.4242, val_loss: 29.7170, val_MinusLogProbMetric: 29.7170

Epoch 149: val_loss did not improve from 29.43999
196/196 - 35s - loss: 29.4242 - MinusLogProbMetric: 29.4242 - val_loss: 29.7170 - val_MinusLogProbMetric: 29.7170 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 150/1000
2023-10-25 14:13:57.633 
Epoch 150/1000 
	 loss: 29.2822, MinusLogProbMetric: 29.2822, val_loss: 32.0879, val_MinusLogProbMetric: 32.0879

Epoch 150: val_loss did not improve from 29.43999
196/196 - 35s - loss: 29.2822 - MinusLogProbMetric: 29.2822 - val_loss: 32.0879 - val_MinusLogProbMetric: 32.0879 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 151/1000
2023-10-25 14:14:32.064 
Epoch 151/1000 
	 loss: 29.3277, MinusLogProbMetric: 29.3277, val_loss: 30.9962, val_MinusLogProbMetric: 30.9962

Epoch 151: val_loss did not improve from 29.43999
196/196 - 34s - loss: 29.3277 - MinusLogProbMetric: 29.3277 - val_loss: 30.9962 - val_MinusLogProbMetric: 30.9962 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 152/1000
2023-10-25 14:15:07.118 
Epoch 152/1000 
	 loss: 29.3936, MinusLogProbMetric: 29.3936, val_loss: 30.4880, val_MinusLogProbMetric: 30.4880

Epoch 152: val_loss did not improve from 29.43999
196/196 - 35s - loss: 29.3936 - MinusLogProbMetric: 29.3936 - val_loss: 30.4880 - val_MinusLogProbMetric: 30.4880 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 153/1000
2023-10-25 14:15:41.934 
Epoch 153/1000 
	 loss: 29.4792, MinusLogProbMetric: 29.4792, val_loss: 29.7474, val_MinusLogProbMetric: 29.7474

Epoch 153: val_loss did not improve from 29.43999
196/196 - 35s - loss: 29.4792 - MinusLogProbMetric: 29.4792 - val_loss: 29.7474 - val_MinusLogProbMetric: 29.7474 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 154/1000
2023-10-25 14:16:14.166 
Epoch 154/1000 
	 loss: 29.1222, MinusLogProbMetric: 29.1222, val_loss: 29.2524, val_MinusLogProbMetric: 29.2524

Epoch 154: val_loss improved from 29.43999 to 29.25239, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 33s - loss: 29.1222 - MinusLogProbMetric: 29.1222 - val_loss: 29.2524 - val_MinusLogProbMetric: 29.2524 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 155/1000
2023-10-25 14:16:48.194 
Epoch 155/1000 
	 loss: 29.3750, MinusLogProbMetric: 29.3750, val_loss: 29.7302, val_MinusLogProbMetric: 29.7302

Epoch 155: val_loss did not improve from 29.25239
196/196 - 33s - loss: 29.3750 - MinusLogProbMetric: 29.3750 - val_loss: 29.7302 - val_MinusLogProbMetric: 29.7302 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 156/1000
2023-10-25 14:17:22.304 
Epoch 156/1000 
	 loss: 29.3267, MinusLogProbMetric: 29.3267, val_loss: 29.5892, val_MinusLogProbMetric: 29.5892

Epoch 156: val_loss did not improve from 29.25239
196/196 - 34s - loss: 29.3267 - MinusLogProbMetric: 29.3267 - val_loss: 29.5892 - val_MinusLogProbMetric: 29.5892 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 157/1000
2023-10-25 14:17:56.733 
Epoch 157/1000 
	 loss: 29.1676, MinusLogProbMetric: 29.1676, val_loss: 29.4829, val_MinusLogProbMetric: 29.4829

Epoch 157: val_loss did not improve from 29.25239
196/196 - 34s - loss: 29.1676 - MinusLogProbMetric: 29.1676 - val_loss: 29.4829 - val_MinusLogProbMetric: 29.4829 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 158/1000
2023-10-25 14:18:31.372 
Epoch 158/1000 
	 loss: 29.3711, MinusLogProbMetric: 29.3711, val_loss: 29.7008, val_MinusLogProbMetric: 29.7008

Epoch 158: val_loss did not improve from 29.25239
196/196 - 35s - loss: 29.3711 - MinusLogProbMetric: 29.3711 - val_loss: 29.7008 - val_MinusLogProbMetric: 29.7008 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 159/1000
2023-10-25 14:19:05.808 
Epoch 159/1000 
	 loss: 29.1794, MinusLogProbMetric: 29.1794, val_loss: 30.3126, val_MinusLogProbMetric: 30.3126

Epoch 159: val_loss did not improve from 29.25239
196/196 - 34s - loss: 29.1794 - MinusLogProbMetric: 29.1794 - val_loss: 30.3126 - val_MinusLogProbMetric: 30.3126 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 160/1000
2023-10-25 14:19:40.667 
Epoch 160/1000 
	 loss: 29.2114, MinusLogProbMetric: 29.2114, val_loss: 29.6282, val_MinusLogProbMetric: 29.6282

Epoch 160: val_loss did not improve from 29.25239
196/196 - 35s - loss: 29.2114 - MinusLogProbMetric: 29.2114 - val_loss: 29.6282 - val_MinusLogProbMetric: 29.6282 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 161/1000
2023-10-25 14:20:15.297 
Epoch 161/1000 
	 loss: 29.3749, MinusLogProbMetric: 29.3749, val_loss: 29.6297, val_MinusLogProbMetric: 29.6297

Epoch 161: val_loss did not improve from 29.25239
196/196 - 35s - loss: 29.3749 - MinusLogProbMetric: 29.3749 - val_loss: 29.6297 - val_MinusLogProbMetric: 29.6297 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 162/1000
2023-10-25 14:20:49.437 
Epoch 162/1000 
	 loss: 29.0512, MinusLogProbMetric: 29.0512, val_loss: 30.1771, val_MinusLogProbMetric: 30.1771

Epoch 162: val_loss did not improve from 29.25239
196/196 - 34s - loss: 29.0512 - MinusLogProbMetric: 29.0512 - val_loss: 30.1771 - val_MinusLogProbMetric: 30.1771 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 163/1000
2023-10-25 14:21:24.331 
Epoch 163/1000 
	 loss: 29.3216, MinusLogProbMetric: 29.3216, val_loss: 29.7898, val_MinusLogProbMetric: 29.7898

Epoch 163: val_loss did not improve from 29.25239
196/196 - 35s - loss: 29.3216 - MinusLogProbMetric: 29.3216 - val_loss: 29.7898 - val_MinusLogProbMetric: 29.7898 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 164/1000
2023-10-25 14:21:58.272 
Epoch 164/1000 
	 loss: 29.1467, MinusLogProbMetric: 29.1467, val_loss: 30.4961, val_MinusLogProbMetric: 30.4961

Epoch 164: val_loss did not improve from 29.25239
196/196 - 34s - loss: 29.1467 - MinusLogProbMetric: 29.1467 - val_loss: 30.4961 - val_MinusLogProbMetric: 30.4961 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 165/1000
2023-10-25 14:22:32.510 
Epoch 165/1000 
	 loss: 29.1198, MinusLogProbMetric: 29.1198, val_loss: 30.2709, val_MinusLogProbMetric: 30.2709

Epoch 165: val_loss did not improve from 29.25239
196/196 - 34s - loss: 29.1198 - MinusLogProbMetric: 29.1198 - val_loss: 30.2709 - val_MinusLogProbMetric: 30.2709 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 166/1000
2023-10-25 14:23:06.998 
Epoch 166/1000 
	 loss: 29.1394, MinusLogProbMetric: 29.1394, val_loss: 29.7031, val_MinusLogProbMetric: 29.7031

Epoch 166: val_loss did not improve from 29.25239
196/196 - 34s - loss: 29.1394 - MinusLogProbMetric: 29.1394 - val_loss: 29.7031 - val_MinusLogProbMetric: 29.7031 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 167/1000
2023-10-25 14:23:40.870 
Epoch 167/1000 
	 loss: 29.2837, MinusLogProbMetric: 29.2837, val_loss: 29.6741, val_MinusLogProbMetric: 29.6741

Epoch 167: val_loss did not improve from 29.25239
196/196 - 34s - loss: 29.2837 - MinusLogProbMetric: 29.2837 - val_loss: 29.6741 - val_MinusLogProbMetric: 29.6741 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 168/1000
2023-10-25 14:24:16.025 
Epoch 168/1000 
	 loss: 29.2336, MinusLogProbMetric: 29.2336, val_loss: 29.5984, val_MinusLogProbMetric: 29.5984

Epoch 168: val_loss did not improve from 29.25239
196/196 - 35s - loss: 29.2336 - MinusLogProbMetric: 29.2336 - val_loss: 29.5984 - val_MinusLogProbMetric: 29.5984 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 169/1000
2023-10-25 14:24:50.375 
Epoch 169/1000 
	 loss: 29.1560, MinusLogProbMetric: 29.1560, val_loss: 29.7818, val_MinusLogProbMetric: 29.7818

Epoch 169: val_loss did not improve from 29.25239
196/196 - 34s - loss: 29.1560 - MinusLogProbMetric: 29.1560 - val_loss: 29.7818 - val_MinusLogProbMetric: 29.7818 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 170/1000
2023-10-25 14:25:25.315 
Epoch 170/1000 
	 loss: 29.0117, MinusLogProbMetric: 29.0117, val_loss: 29.5885, val_MinusLogProbMetric: 29.5885

Epoch 170: val_loss did not improve from 29.25239
196/196 - 35s - loss: 29.0117 - MinusLogProbMetric: 29.0117 - val_loss: 29.5885 - val_MinusLogProbMetric: 29.5885 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 171/1000
2023-10-25 14:25:59.741 
Epoch 171/1000 
	 loss: 29.1118, MinusLogProbMetric: 29.1118, val_loss: 29.4752, val_MinusLogProbMetric: 29.4752

Epoch 171: val_loss did not improve from 29.25239
196/196 - 34s - loss: 29.1118 - MinusLogProbMetric: 29.1118 - val_loss: 29.4752 - val_MinusLogProbMetric: 29.4752 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 172/1000
2023-10-25 14:26:34.152 
Epoch 172/1000 
	 loss: 29.0626, MinusLogProbMetric: 29.0626, val_loss: 29.5525, val_MinusLogProbMetric: 29.5525

Epoch 172: val_loss did not improve from 29.25239
196/196 - 34s - loss: 29.0626 - MinusLogProbMetric: 29.0626 - val_loss: 29.5525 - val_MinusLogProbMetric: 29.5525 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 173/1000
2023-10-25 14:27:08.765 
Epoch 173/1000 
	 loss: 29.1270, MinusLogProbMetric: 29.1270, val_loss: 30.0577, val_MinusLogProbMetric: 30.0577

Epoch 173: val_loss did not improve from 29.25239
196/196 - 35s - loss: 29.1270 - MinusLogProbMetric: 29.1270 - val_loss: 30.0577 - val_MinusLogProbMetric: 30.0577 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 174/1000
2023-10-25 14:27:40.249 
Epoch 174/1000 
	 loss: 29.0910, MinusLogProbMetric: 29.0910, val_loss: 29.8518, val_MinusLogProbMetric: 29.8518

Epoch 174: val_loss did not improve from 29.25239
196/196 - 31s - loss: 29.0910 - MinusLogProbMetric: 29.0910 - val_loss: 29.8518 - val_MinusLogProbMetric: 29.8518 - lr: 0.0010 - 31s/epoch - 161ms/step
Epoch 175/1000
2023-10-25 14:28:09.085 
Epoch 175/1000 
	 loss: 29.0220, MinusLogProbMetric: 29.0220, val_loss: 29.9677, val_MinusLogProbMetric: 29.9677

Epoch 175: val_loss did not improve from 29.25239
196/196 - 29s - loss: 29.0220 - MinusLogProbMetric: 29.0220 - val_loss: 29.9677 - val_MinusLogProbMetric: 29.9677 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 176/1000
2023-10-25 14:28:39.038 
Epoch 176/1000 
	 loss: 29.0219, MinusLogProbMetric: 29.0219, val_loss: 29.3526, val_MinusLogProbMetric: 29.3526

Epoch 176: val_loss did not improve from 29.25239
196/196 - 30s - loss: 29.0219 - MinusLogProbMetric: 29.0219 - val_loss: 29.3526 - val_MinusLogProbMetric: 29.3526 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 177/1000
2023-10-25 14:29:07.651 
Epoch 177/1000 
	 loss: 29.0267, MinusLogProbMetric: 29.0267, val_loss: 29.4086, val_MinusLogProbMetric: 29.4086

Epoch 177: val_loss did not improve from 29.25239
196/196 - 29s - loss: 29.0267 - MinusLogProbMetric: 29.0267 - val_loss: 29.4086 - val_MinusLogProbMetric: 29.4086 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 178/1000
2023-10-25 14:29:37.673 
Epoch 178/1000 
	 loss: 29.0643, MinusLogProbMetric: 29.0643, val_loss: 29.7078, val_MinusLogProbMetric: 29.7078

Epoch 178: val_loss did not improve from 29.25239
196/196 - 30s - loss: 29.0643 - MinusLogProbMetric: 29.0643 - val_loss: 29.7078 - val_MinusLogProbMetric: 29.7078 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 179/1000
2023-10-25 14:30:09.802 
Epoch 179/1000 
	 loss: 29.0868, MinusLogProbMetric: 29.0868, val_loss: 29.6908, val_MinusLogProbMetric: 29.6908

Epoch 179: val_loss did not improve from 29.25239
196/196 - 32s - loss: 29.0868 - MinusLogProbMetric: 29.0868 - val_loss: 29.6908 - val_MinusLogProbMetric: 29.6908 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 180/1000
2023-10-25 14:30:38.885 
Epoch 180/1000 
	 loss: 29.0945, MinusLogProbMetric: 29.0945, val_loss: 29.7817, val_MinusLogProbMetric: 29.7817

Epoch 180: val_loss did not improve from 29.25239
196/196 - 29s - loss: 29.0945 - MinusLogProbMetric: 29.0945 - val_loss: 29.7817 - val_MinusLogProbMetric: 29.7817 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 181/1000
2023-10-25 14:31:08.388 
Epoch 181/1000 
	 loss: 29.0359, MinusLogProbMetric: 29.0359, val_loss: 29.6724, val_MinusLogProbMetric: 29.6724

Epoch 181: val_loss did not improve from 29.25239
196/196 - 30s - loss: 29.0359 - MinusLogProbMetric: 29.0359 - val_loss: 29.6724 - val_MinusLogProbMetric: 29.6724 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 182/1000
2023-10-25 14:31:38.331 
Epoch 182/1000 
	 loss: 28.9519, MinusLogProbMetric: 28.9519, val_loss: 29.5010, val_MinusLogProbMetric: 29.5010

Epoch 182: val_loss did not improve from 29.25239
196/196 - 30s - loss: 28.9519 - MinusLogProbMetric: 28.9519 - val_loss: 29.5010 - val_MinusLogProbMetric: 29.5010 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 183/1000
2023-10-25 14:32:07.303 
Epoch 183/1000 
	 loss: 29.0809, MinusLogProbMetric: 29.0809, val_loss: 29.6008, val_MinusLogProbMetric: 29.6008

Epoch 183: val_loss did not improve from 29.25239
196/196 - 29s - loss: 29.0809 - MinusLogProbMetric: 29.0809 - val_loss: 29.6008 - val_MinusLogProbMetric: 29.6008 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 184/1000
2023-10-25 14:32:39.520 
Epoch 184/1000 
	 loss: 29.0515, MinusLogProbMetric: 29.0515, val_loss: 29.7940, val_MinusLogProbMetric: 29.7940

Epoch 184: val_loss did not improve from 29.25239
196/196 - 32s - loss: 29.0515 - MinusLogProbMetric: 29.0515 - val_loss: 29.7940 - val_MinusLogProbMetric: 29.7940 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 185/1000
2023-10-25 14:33:07.873 
Epoch 185/1000 
	 loss: 29.1181, MinusLogProbMetric: 29.1181, val_loss: 29.4935, val_MinusLogProbMetric: 29.4935

Epoch 185: val_loss did not improve from 29.25239
196/196 - 28s - loss: 29.1181 - MinusLogProbMetric: 29.1181 - val_loss: 29.4935 - val_MinusLogProbMetric: 29.4935 - lr: 0.0010 - 28s/epoch - 145ms/step
Epoch 186/1000
2023-10-25 14:33:38.395 
Epoch 186/1000 
	 loss: 29.0136, MinusLogProbMetric: 29.0136, val_loss: 29.6205, val_MinusLogProbMetric: 29.6205

Epoch 186: val_loss did not improve from 29.25239
196/196 - 31s - loss: 29.0136 - MinusLogProbMetric: 29.0136 - val_loss: 29.6205 - val_MinusLogProbMetric: 29.6205 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 187/1000
2023-10-25 14:34:09.214 
Epoch 187/1000 
	 loss: 29.0851, MinusLogProbMetric: 29.0851, val_loss: 29.3962, val_MinusLogProbMetric: 29.3962

Epoch 187: val_loss did not improve from 29.25239
196/196 - 31s - loss: 29.0851 - MinusLogProbMetric: 29.0851 - val_loss: 29.3962 - val_MinusLogProbMetric: 29.3962 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 188/1000
2023-10-25 14:34:37.923 
Epoch 188/1000 
	 loss: 28.9796, MinusLogProbMetric: 28.9796, val_loss: 29.1520, val_MinusLogProbMetric: 29.1520

Epoch 188: val_loss improved from 29.25239 to 29.15202, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 29s - loss: 28.9796 - MinusLogProbMetric: 28.9796 - val_loss: 29.1520 - val_MinusLogProbMetric: 29.1520 - lr: 0.0010 - 29s/epoch - 149ms/step
Epoch 189/1000
2023-10-25 14:35:09.522 
Epoch 189/1000 
	 loss: 28.8423, MinusLogProbMetric: 28.8423, val_loss: 29.3346, val_MinusLogProbMetric: 29.3346

Epoch 189: val_loss did not improve from 29.15202
196/196 - 31s - loss: 28.8423 - MinusLogProbMetric: 28.8423 - val_loss: 29.3346 - val_MinusLogProbMetric: 29.3346 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 190/1000
2023-10-25 14:35:39.352 
Epoch 190/1000 
	 loss: 28.9722, MinusLogProbMetric: 28.9722, val_loss: 29.3749, val_MinusLogProbMetric: 29.3749

Epoch 190: val_loss did not improve from 29.15202
196/196 - 30s - loss: 28.9722 - MinusLogProbMetric: 28.9722 - val_loss: 29.3749 - val_MinusLogProbMetric: 29.3749 - lr: 0.0010 - 30s/epoch - 152ms/step
Epoch 191/1000
2023-10-25 14:36:09.339 
Epoch 191/1000 
	 loss: 28.8802, MinusLogProbMetric: 28.8802, val_loss: 29.9928, val_MinusLogProbMetric: 29.9928

Epoch 191: val_loss did not improve from 29.15202
196/196 - 30s - loss: 28.8802 - MinusLogProbMetric: 28.8802 - val_loss: 29.9928 - val_MinusLogProbMetric: 29.9928 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 192/1000
2023-10-25 14:36:39.747 
Epoch 192/1000 
	 loss: 28.9974, MinusLogProbMetric: 28.9974, val_loss: 29.7814, val_MinusLogProbMetric: 29.7814

Epoch 192: val_loss did not improve from 29.15202
196/196 - 30s - loss: 28.9974 - MinusLogProbMetric: 28.9974 - val_loss: 29.7814 - val_MinusLogProbMetric: 29.7814 - lr: 0.0010 - 30s/epoch - 155ms/step
Epoch 193/1000
2023-10-25 14:37:08.607 
Epoch 193/1000 
	 loss: 28.8660, MinusLogProbMetric: 28.8660, val_loss: 29.2612, val_MinusLogProbMetric: 29.2612

Epoch 193: val_loss did not improve from 29.15202
196/196 - 29s - loss: 28.8660 - MinusLogProbMetric: 28.8660 - val_loss: 29.2612 - val_MinusLogProbMetric: 29.2612 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 194/1000
2023-10-25 14:37:37.566 
Epoch 194/1000 
	 loss: 29.0811, MinusLogProbMetric: 29.0811, val_loss: 29.9321, val_MinusLogProbMetric: 29.9321

Epoch 194: val_loss did not improve from 29.15202
196/196 - 29s - loss: 29.0811 - MinusLogProbMetric: 29.0811 - val_loss: 29.9321 - val_MinusLogProbMetric: 29.9321 - lr: 0.0010 - 29s/epoch - 148ms/step
Epoch 195/1000
2023-10-25 14:38:08.262 
Epoch 195/1000 
	 loss: 28.8541, MinusLogProbMetric: 28.8541, val_loss: 29.0733, val_MinusLogProbMetric: 29.0733

Epoch 195: val_loss improved from 29.15202 to 29.07330, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 31s - loss: 28.8541 - MinusLogProbMetric: 28.8541 - val_loss: 29.0733 - val_MinusLogProbMetric: 29.0733 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 196/1000
2023-10-25 14:38:37.418 
Epoch 196/1000 
	 loss: 28.9751, MinusLogProbMetric: 28.9751, val_loss: 29.2190, val_MinusLogProbMetric: 29.2190

Epoch 196: val_loss did not improve from 29.07330
196/196 - 29s - loss: 28.9751 - MinusLogProbMetric: 28.9751 - val_loss: 29.2190 - val_MinusLogProbMetric: 29.2190 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 197/1000
2023-10-25 14:39:08.369 
Epoch 197/1000 
	 loss: 28.9560, MinusLogProbMetric: 28.9560, val_loss: 29.2062, val_MinusLogProbMetric: 29.2062

Epoch 197: val_loss did not improve from 29.07330
196/196 - 31s - loss: 28.9560 - MinusLogProbMetric: 28.9560 - val_loss: 29.2062 - val_MinusLogProbMetric: 29.2062 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 198/1000
2023-10-25 14:39:37.173 
Epoch 198/1000 
	 loss: 28.9718, MinusLogProbMetric: 28.9718, val_loss: 29.5152, val_MinusLogProbMetric: 29.5152

Epoch 198: val_loss did not improve from 29.07330
196/196 - 29s - loss: 28.9718 - MinusLogProbMetric: 28.9718 - val_loss: 29.5152 - val_MinusLogProbMetric: 29.5152 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 199/1000
2023-10-25 14:40:07.099 
Epoch 199/1000 
	 loss: 29.0276, MinusLogProbMetric: 29.0276, val_loss: 29.5461, val_MinusLogProbMetric: 29.5461

Epoch 199: val_loss did not improve from 29.07330
196/196 - 30s - loss: 29.0276 - MinusLogProbMetric: 29.0276 - val_loss: 29.5461 - val_MinusLogProbMetric: 29.5461 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 200/1000
2023-10-25 14:40:37.776 
Epoch 200/1000 
	 loss: 28.9812, MinusLogProbMetric: 28.9812, val_loss: 29.4859, val_MinusLogProbMetric: 29.4859

Epoch 200: val_loss did not improve from 29.07330
196/196 - 31s - loss: 28.9812 - MinusLogProbMetric: 28.9812 - val_loss: 29.4859 - val_MinusLogProbMetric: 29.4859 - lr: 0.0010 - 31s/epoch - 156ms/step
Epoch 201/1000
2023-10-25 14:41:06.416 
Epoch 201/1000 
	 loss: 28.9084, MinusLogProbMetric: 28.9084, val_loss: 29.7778, val_MinusLogProbMetric: 29.7778

Epoch 201: val_loss did not improve from 29.07330
196/196 - 29s - loss: 28.9084 - MinusLogProbMetric: 28.9084 - val_loss: 29.7778 - val_MinusLogProbMetric: 29.7778 - lr: 0.0010 - 29s/epoch - 146ms/step
Epoch 202/1000
2023-10-25 14:41:37.247 
Epoch 202/1000 
	 loss: 28.8591, MinusLogProbMetric: 28.8591, val_loss: 29.4390, val_MinusLogProbMetric: 29.4390

Epoch 202: val_loss did not improve from 29.07330
196/196 - 31s - loss: 28.8591 - MinusLogProbMetric: 28.8591 - val_loss: 29.4390 - val_MinusLogProbMetric: 29.4390 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 203/1000
2023-10-25 14:42:08.041 
Epoch 203/1000 
	 loss: 28.9467, MinusLogProbMetric: 28.9467, val_loss: 29.5921, val_MinusLogProbMetric: 29.5921

Epoch 203: val_loss did not improve from 29.07330
196/196 - 31s - loss: 28.9467 - MinusLogProbMetric: 28.9467 - val_loss: 29.5921 - val_MinusLogProbMetric: 29.5921 - lr: 0.0010 - 31s/epoch - 157ms/step
Epoch 204/1000
2023-10-25 14:42:36.827 
Epoch 204/1000 
	 loss: 28.8056, MinusLogProbMetric: 28.8056, val_loss: 30.0390, val_MinusLogProbMetric: 30.0390

Epoch 204: val_loss did not improve from 29.07330
196/196 - 29s - loss: 28.8056 - MinusLogProbMetric: 28.8056 - val_loss: 30.0390 - val_MinusLogProbMetric: 30.0390 - lr: 0.0010 - 29s/epoch - 147ms/step
Epoch 205/1000
2023-10-25 14:43:09.525 
Epoch 205/1000 
	 loss: 28.9098, MinusLogProbMetric: 28.9098, val_loss: 29.5298, val_MinusLogProbMetric: 29.5298

Epoch 205: val_loss did not improve from 29.07330
196/196 - 33s - loss: 28.9098 - MinusLogProbMetric: 28.9098 - val_loss: 29.5298 - val_MinusLogProbMetric: 29.5298 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 206/1000
2023-10-25 14:43:43.038 
Epoch 206/1000 
	 loss: 28.7791, MinusLogProbMetric: 28.7791, val_loss: 30.4580, val_MinusLogProbMetric: 30.4580

Epoch 206: val_loss did not improve from 29.07330
196/196 - 34s - loss: 28.7791 - MinusLogProbMetric: 28.7791 - val_loss: 30.4580 - val_MinusLogProbMetric: 30.4580 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 207/1000
2023-10-25 14:44:17.431 
Epoch 207/1000 
	 loss: 28.9660, MinusLogProbMetric: 28.9660, val_loss: 29.7519, val_MinusLogProbMetric: 29.7519

Epoch 207: val_loss did not improve from 29.07330
196/196 - 34s - loss: 28.9660 - MinusLogProbMetric: 28.9660 - val_loss: 29.7519 - val_MinusLogProbMetric: 29.7519 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 208/1000
2023-10-25 14:44:52.083 
Epoch 208/1000 
	 loss: 28.8495, MinusLogProbMetric: 28.8495, val_loss: 29.4802, val_MinusLogProbMetric: 29.4802

Epoch 208: val_loss did not improve from 29.07330
196/196 - 35s - loss: 28.8495 - MinusLogProbMetric: 28.8495 - val_loss: 29.4802 - val_MinusLogProbMetric: 29.4802 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 209/1000
2023-10-25 14:45:26.007 
Epoch 209/1000 
	 loss: 29.0471, MinusLogProbMetric: 29.0471, val_loss: 29.4756, val_MinusLogProbMetric: 29.4756

Epoch 209: val_loss did not improve from 29.07330
196/196 - 34s - loss: 29.0471 - MinusLogProbMetric: 29.0471 - val_loss: 29.4756 - val_MinusLogProbMetric: 29.4756 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 210/1000
2023-10-25 14:46:00.577 
Epoch 210/1000 
	 loss: 28.9070, MinusLogProbMetric: 28.9070, val_loss: 29.1411, val_MinusLogProbMetric: 29.1411

Epoch 210: val_loss did not improve from 29.07330
196/196 - 35s - loss: 28.9070 - MinusLogProbMetric: 28.9070 - val_loss: 29.1411 - val_MinusLogProbMetric: 29.1411 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 211/1000
2023-10-25 14:46:35.073 
Epoch 211/1000 
	 loss: 28.8114, MinusLogProbMetric: 28.8114, val_loss: 29.2624, val_MinusLogProbMetric: 29.2624

Epoch 211: val_loss did not improve from 29.07330
196/196 - 34s - loss: 28.8114 - MinusLogProbMetric: 28.8114 - val_loss: 29.2624 - val_MinusLogProbMetric: 29.2624 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 212/1000
2023-10-25 14:47:09.544 
Epoch 212/1000 
	 loss: 28.6719, MinusLogProbMetric: 28.6719, val_loss: 29.1511, val_MinusLogProbMetric: 29.1511

Epoch 212: val_loss did not improve from 29.07330
196/196 - 34s - loss: 28.6719 - MinusLogProbMetric: 28.6719 - val_loss: 29.1511 - val_MinusLogProbMetric: 29.1511 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 213/1000
2023-10-25 14:47:43.996 
Epoch 213/1000 
	 loss: 28.7609, MinusLogProbMetric: 28.7609, val_loss: 30.3534, val_MinusLogProbMetric: 30.3534

Epoch 213: val_loss did not improve from 29.07330
196/196 - 34s - loss: 28.7609 - MinusLogProbMetric: 28.7609 - val_loss: 30.3534 - val_MinusLogProbMetric: 30.3534 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 214/1000
2023-10-25 14:48:18.684 
Epoch 214/1000 
	 loss: 28.8307, MinusLogProbMetric: 28.8307, val_loss: 30.3766, val_MinusLogProbMetric: 30.3766

Epoch 214: val_loss did not improve from 29.07330
196/196 - 35s - loss: 28.8307 - MinusLogProbMetric: 28.8307 - val_loss: 30.3766 - val_MinusLogProbMetric: 30.3766 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 215/1000
2023-10-25 14:48:53.297 
Epoch 215/1000 
	 loss: 28.8757, MinusLogProbMetric: 28.8757, val_loss: 29.7445, val_MinusLogProbMetric: 29.7445

Epoch 215: val_loss did not improve from 29.07330
196/196 - 35s - loss: 28.8757 - MinusLogProbMetric: 28.8757 - val_loss: 29.7445 - val_MinusLogProbMetric: 29.7445 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 216/1000
2023-10-25 14:49:27.421 
Epoch 216/1000 
	 loss: 28.7493, MinusLogProbMetric: 28.7493, val_loss: 29.4277, val_MinusLogProbMetric: 29.4277

Epoch 216: val_loss did not improve from 29.07330
196/196 - 34s - loss: 28.7493 - MinusLogProbMetric: 28.7493 - val_loss: 29.4277 - val_MinusLogProbMetric: 29.4277 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 217/1000
2023-10-25 14:49:54.040 
Epoch 217/1000 
	 loss: 28.8125, MinusLogProbMetric: 28.8125, val_loss: 29.5921, val_MinusLogProbMetric: 29.5921

Epoch 217: val_loss did not improve from 29.07330
196/196 - 27s - loss: 28.8125 - MinusLogProbMetric: 28.8125 - val_loss: 29.5921 - val_MinusLogProbMetric: 29.5921 - lr: 0.0010 - 27s/epoch - 136ms/step
Epoch 218/1000
2023-10-25 14:50:24.049 
Epoch 218/1000 
	 loss: 28.6483, MinusLogProbMetric: 28.6483, val_loss: 29.1124, val_MinusLogProbMetric: 29.1124

Epoch 218: val_loss did not improve from 29.07330
196/196 - 30s - loss: 28.6483 - MinusLogProbMetric: 28.6483 - val_loss: 29.1124 - val_MinusLogProbMetric: 29.1124 - lr: 0.0010 - 30s/epoch - 153ms/step
Epoch 219/1000
2023-10-25 14:50:53.486 
Epoch 219/1000 
	 loss: 28.7052, MinusLogProbMetric: 28.7052, val_loss: 29.3005, val_MinusLogProbMetric: 29.3005

Epoch 219: val_loss did not improve from 29.07330
196/196 - 29s - loss: 28.7052 - MinusLogProbMetric: 28.7052 - val_loss: 29.3005 - val_MinusLogProbMetric: 29.3005 - lr: 0.0010 - 29s/epoch - 150ms/step
Epoch 220/1000
2023-10-25 14:51:23.162 
Epoch 220/1000 
	 loss: 28.7145, MinusLogProbMetric: 28.7145, val_loss: 29.8564, val_MinusLogProbMetric: 29.8564

Epoch 220: val_loss did not improve from 29.07330
196/196 - 30s - loss: 28.7145 - MinusLogProbMetric: 28.7145 - val_loss: 29.8564 - val_MinusLogProbMetric: 29.8564 - lr: 0.0010 - 30s/epoch - 151ms/step
Epoch 221/1000
2023-10-25 14:51:54.042 
Epoch 221/1000 
	 loss: 28.7063, MinusLogProbMetric: 28.7063, val_loss: 29.2593, val_MinusLogProbMetric: 29.2593

Epoch 221: val_loss did not improve from 29.07330
196/196 - 31s - loss: 28.7063 - MinusLogProbMetric: 28.7063 - val_loss: 29.2593 - val_MinusLogProbMetric: 29.2593 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 222/1000
2023-10-25 14:52:27.189 
Epoch 222/1000 
	 loss: 28.8591, MinusLogProbMetric: 28.8591, val_loss: 29.9362, val_MinusLogProbMetric: 29.9362

Epoch 222: val_loss did not improve from 29.07330
196/196 - 33s - loss: 28.8591 - MinusLogProbMetric: 28.8591 - val_loss: 29.9362 - val_MinusLogProbMetric: 29.9362 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 223/1000
2023-10-25 14:52:58.451 
Epoch 223/1000 
	 loss: 28.7345, MinusLogProbMetric: 28.7345, val_loss: 29.5343, val_MinusLogProbMetric: 29.5343

Epoch 223: val_loss did not improve from 29.07330
196/196 - 31s - loss: 28.7345 - MinusLogProbMetric: 28.7345 - val_loss: 29.5343 - val_MinusLogProbMetric: 29.5343 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 224/1000
2023-10-25 14:53:32.081 
Epoch 224/1000 
	 loss: 28.7145, MinusLogProbMetric: 28.7145, val_loss: 29.1835, val_MinusLogProbMetric: 29.1835

Epoch 224: val_loss did not improve from 29.07330
196/196 - 34s - loss: 28.7145 - MinusLogProbMetric: 28.7145 - val_loss: 29.1835 - val_MinusLogProbMetric: 29.1835 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 225/1000
2023-10-25 14:54:06.773 
Epoch 225/1000 
	 loss: 28.6935, MinusLogProbMetric: 28.6935, val_loss: 29.7801, val_MinusLogProbMetric: 29.7801

Epoch 225: val_loss did not improve from 29.07330
196/196 - 35s - loss: 28.6935 - MinusLogProbMetric: 28.6935 - val_loss: 29.7801 - val_MinusLogProbMetric: 29.7801 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 226/1000
2023-10-25 14:54:41.511 
Epoch 226/1000 
	 loss: 28.6934, MinusLogProbMetric: 28.6934, val_loss: 29.7045, val_MinusLogProbMetric: 29.7045

Epoch 226: val_loss did not improve from 29.07330
196/196 - 35s - loss: 28.6934 - MinusLogProbMetric: 28.6934 - val_loss: 29.7045 - val_MinusLogProbMetric: 29.7045 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 227/1000
2023-10-25 14:55:15.535 
Epoch 227/1000 
	 loss: 28.8583, MinusLogProbMetric: 28.8583, val_loss: 29.5781, val_MinusLogProbMetric: 29.5781

Epoch 227: val_loss did not improve from 29.07330
196/196 - 34s - loss: 28.8583 - MinusLogProbMetric: 28.8583 - val_loss: 29.5781 - val_MinusLogProbMetric: 29.5781 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 228/1000
2023-10-25 14:55:50.165 
Epoch 228/1000 
	 loss: 28.6087, MinusLogProbMetric: 28.6087, val_loss: 28.9067, val_MinusLogProbMetric: 28.9067

Epoch 228: val_loss improved from 29.07330 to 28.90667, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 28.6087 - MinusLogProbMetric: 28.6087 - val_loss: 28.9067 - val_MinusLogProbMetric: 28.9067 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 229/1000
2023-10-25 14:56:24.688 
Epoch 229/1000 
	 loss: 28.8845, MinusLogProbMetric: 28.8845, val_loss: 29.0996, val_MinusLogProbMetric: 29.0996

Epoch 229: val_loss did not improve from 28.90667
196/196 - 34s - loss: 28.8845 - MinusLogProbMetric: 28.8845 - val_loss: 29.0996 - val_MinusLogProbMetric: 29.0996 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 230/1000
2023-10-25 14:56:59.066 
Epoch 230/1000 
	 loss: 28.5927, MinusLogProbMetric: 28.5927, val_loss: 29.3090, val_MinusLogProbMetric: 29.3090

Epoch 230: val_loss did not improve from 28.90667
196/196 - 34s - loss: 28.5927 - MinusLogProbMetric: 28.5927 - val_loss: 29.3090 - val_MinusLogProbMetric: 29.3090 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 231/1000
2023-10-25 14:57:33.639 
Epoch 231/1000 
	 loss: 28.6625, MinusLogProbMetric: 28.6625, val_loss: 30.0859, val_MinusLogProbMetric: 30.0859

Epoch 231: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.6625 - MinusLogProbMetric: 28.6625 - val_loss: 30.0859 - val_MinusLogProbMetric: 30.0859 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 232/1000
2023-10-25 14:58:08.228 
Epoch 232/1000 
	 loss: 28.7261, MinusLogProbMetric: 28.7261, val_loss: 29.1725, val_MinusLogProbMetric: 29.1725

Epoch 232: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.7261 - MinusLogProbMetric: 28.7261 - val_loss: 29.1725 - val_MinusLogProbMetric: 29.1725 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 233/1000
2023-10-25 14:58:42.826 
Epoch 233/1000 
	 loss: 28.6515, MinusLogProbMetric: 28.6515, val_loss: 29.3049, val_MinusLogProbMetric: 29.3049

Epoch 233: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.6515 - MinusLogProbMetric: 28.6515 - val_loss: 29.3049 - val_MinusLogProbMetric: 29.3049 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 234/1000
2023-10-25 14:59:17.129 
Epoch 234/1000 
	 loss: 28.7031, MinusLogProbMetric: 28.7031, val_loss: 29.7210, val_MinusLogProbMetric: 29.7210

Epoch 234: val_loss did not improve from 28.90667
196/196 - 34s - loss: 28.7031 - MinusLogProbMetric: 28.7031 - val_loss: 29.7210 - val_MinusLogProbMetric: 29.7210 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 235/1000
2023-10-25 14:59:51.782 
Epoch 235/1000 
	 loss: 28.6509, MinusLogProbMetric: 28.6509, val_loss: 29.6380, val_MinusLogProbMetric: 29.6380

Epoch 235: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.6509 - MinusLogProbMetric: 28.6509 - val_loss: 29.6380 - val_MinusLogProbMetric: 29.6380 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 236/1000
2023-10-25 15:00:26.316 
Epoch 236/1000 
	 loss: 28.6352, MinusLogProbMetric: 28.6352, val_loss: 29.1686, val_MinusLogProbMetric: 29.1686

Epoch 236: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.6352 - MinusLogProbMetric: 28.6352 - val_loss: 29.1686 - val_MinusLogProbMetric: 29.1686 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 237/1000
2023-10-25 15:01:00.948 
Epoch 237/1000 
	 loss: 28.6087, MinusLogProbMetric: 28.6087, val_loss: 29.4934, val_MinusLogProbMetric: 29.4934

Epoch 237: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.6087 - MinusLogProbMetric: 28.6087 - val_loss: 29.4934 - val_MinusLogProbMetric: 29.4934 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 238/1000
2023-10-25 15:01:35.602 
Epoch 238/1000 
	 loss: 28.6598, MinusLogProbMetric: 28.6598, val_loss: 29.3899, val_MinusLogProbMetric: 29.3899

Epoch 238: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.6598 - MinusLogProbMetric: 28.6598 - val_loss: 29.3899 - val_MinusLogProbMetric: 29.3899 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 239/1000
2023-10-25 15:02:10.328 
Epoch 239/1000 
	 loss: 28.6240, MinusLogProbMetric: 28.6240, val_loss: 28.9799, val_MinusLogProbMetric: 28.9799

Epoch 239: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.6240 - MinusLogProbMetric: 28.6240 - val_loss: 28.9799 - val_MinusLogProbMetric: 28.9799 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 240/1000
2023-10-25 15:02:44.377 
Epoch 240/1000 
	 loss: 28.8053, MinusLogProbMetric: 28.8053, val_loss: 29.1382, val_MinusLogProbMetric: 29.1382

Epoch 240: val_loss did not improve from 28.90667
196/196 - 34s - loss: 28.8053 - MinusLogProbMetric: 28.8053 - val_loss: 29.1382 - val_MinusLogProbMetric: 29.1382 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 241/1000
2023-10-25 15:03:18.411 
Epoch 241/1000 
	 loss: 28.5949, MinusLogProbMetric: 28.5949, val_loss: 29.7915, val_MinusLogProbMetric: 29.7915

Epoch 241: val_loss did not improve from 28.90667
196/196 - 34s - loss: 28.5949 - MinusLogProbMetric: 28.5949 - val_loss: 29.7915 - val_MinusLogProbMetric: 29.7915 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 242/1000
2023-10-25 15:03:52.825 
Epoch 242/1000 
	 loss: 28.6874, MinusLogProbMetric: 28.6874, val_loss: 29.3788, val_MinusLogProbMetric: 29.3788

Epoch 242: val_loss did not improve from 28.90667
196/196 - 34s - loss: 28.6874 - MinusLogProbMetric: 28.6874 - val_loss: 29.3788 - val_MinusLogProbMetric: 29.3788 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 243/1000
2023-10-25 15:04:27.008 
Epoch 243/1000 
	 loss: 28.5500, MinusLogProbMetric: 28.5500, val_loss: 29.1467, val_MinusLogProbMetric: 29.1467

Epoch 243: val_loss did not improve from 28.90667
196/196 - 34s - loss: 28.5500 - MinusLogProbMetric: 28.5500 - val_loss: 29.1467 - val_MinusLogProbMetric: 29.1467 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 244/1000
2023-10-25 15:05:01.586 
Epoch 244/1000 
	 loss: 28.5996, MinusLogProbMetric: 28.5996, val_loss: 29.7019, val_MinusLogProbMetric: 29.7019

Epoch 244: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.5996 - MinusLogProbMetric: 28.5996 - val_loss: 29.7019 - val_MinusLogProbMetric: 29.7019 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 245/1000
2023-10-25 15:05:36.154 
Epoch 245/1000 
	 loss: 28.6297, MinusLogProbMetric: 28.6297, val_loss: 29.6939, val_MinusLogProbMetric: 29.6939

Epoch 245: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.6297 - MinusLogProbMetric: 28.6297 - val_loss: 29.6939 - val_MinusLogProbMetric: 29.6939 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 246/1000
2023-10-25 15:06:10.392 
Epoch 246/1000 
	 loss: 28.5628, MinusLogProbMetric: 28.5628, val_loss: 29.4574, val_MinusLogProbMetric: 29.4574

Epoch 246: val_loss did not improve from 28.90667
196/196 - 34s - loss: 28.5628 - MinusLogProbMetric: 28.5628 - val_loss: 29.4574 - val_MinusLogProbMetric: 29.4574 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 247/1000
2023-10-25 15:06:44.750 
Epoch 247/1000 
	 loss: 28.6986, MinusLogProbMetric: 28.6986, val_loss: 31.2377, val_MinusLogProbMetric: 31.2377

Epoch 247: val_loss did not improve from 28.90667
196/196 - 34s - loss: 28.6986 - MinusLogProbMetric: 28.6986 - val_loss: 31.2377 - val_MinusLogProbMetric: 31.2377 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 248/1000
2023-10-25 15:07:18.997 
Epoch 248/1000 
	 loss: 28.8078, MinusLogProbMetric: 28.8078, val_loss: 29.1974, val_MinusLogProbMetric: 29.1974

Epoch 248: val_loss did not improve from 28.90667
196/196 - 34s - loss: 28.8078 - MinusLogProbMetric: 28.8078 - val_loss: 29.1974 - val_MinusLogProbMetric: 29.1974 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 249/1000
2023-10-25 15:07:53.691 
Epoch 249/1000 
	 loss: 28.5256, MinusLogProbMetric: 28.5256, val_loss: 29.1134, val_MinusLogProbMetric: 29.1134

Epoch 249: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.5256 - MinusLogProbMetric: 28.5256 - val_loss: 29.1134 - val_MinusLogProbMetric: 29.1134 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 250/1000
2023-10-25 15:08:28.738 
Epoch 250/1000 
	 loss: 28.7815, MinusLogProbMetric: 28.7815, val_loss: 29.4706, val_MinusLogProbMetric: 29.4706

Epoch 250: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.7815 - MinusLogProbMetric: 28.7815 - val_loss: 29.4706 - val_MinusLogProbMetric: 29.4706 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 251/1000
2023-10-25 15:09:03.311 
Epoch 251/1000 
	 loss: 28.6094, MinusLogProbMetric: 28.6094, val_loss: 29.2239, val_MinusLogProbMetric: 29.2239

Epoch 251: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.6094 - MinusLogProbMetric: 28.6094 - val_loss: 29.2239 - val_MinusLogProbMetric: 29.2239 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 252/1000
2023-10-25 15:09:38.247 
Epoch 252/1000 
	 loss: 28.5370, MinusLogProbMetric: 28.5370, val_loss: 29.2556, val_MinusLogProbMetric: 29.2556

Epoch 252: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.5370 - MinusLogProbMetric: 28.5370 - val_loss: 29.2556 - val_MinusLogProbMetric: 29.2556 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 253/1000
2023-10-25 15:10:12.935 
Epoch 253/1000 
	 loss: 28.6866, MinusLogProbMetric: 28.6866, val_loss: 28.9247, val_MinusLogProbMetric: 28.9247

Epoch 253: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.6866 - MinusLogProbMetric: 28.6866 - val_loss: 28.9247 - val_MinusLogProbMetric: 28.9247 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 254/1000
2023-10-25 15:10:47.453 
Epoch 254/1000 
	 loss: 28.7165, MinusLogProbMetric: 28.7165, val_loss: 29.5202, val_MinusLogProbMetric: 29.5202

Epoch 254: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.7165 - MinusLogProbMetric: 28.7165 - val_loss: 29.5202 - val_MinusLogProbMetric: 29.5202 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 255/1000
2023-10-25 15:11:22.004 
Epoch 255/1000 
	 loss: 28.4933, MinusLogProbMetric: 28.4933, val_loss: 29.4303, val_MinusLogProbMetric: 29.4303

Epoch 255: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.4933 - MinusLogProbMetric: 28.4933 - val_loss: 29.4303 - val_MinusLogProbMetric: 29.4303 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 256/1000
2023-10-25 15:11:56.240 
Epoch 256/1000 
	 loss: 28.5041, MinusLogProbMetric: 28.5041, val_loss: 29.2372, val_MinusLogProbMetric: 29.2372

Epoch 256: val_loss did not improve from 28.90667
196/196 - 34s - loss: 28.5041 - MinusLogProbMetric: 28.5041 - val_loss: 29.2372 - val_MinusLogProbMetric: 29.2372 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 257/1000
2023-10-25 15:12:31.012 
Epoch 257/1000 
	 loss: 28.6349, MinusLogProbMetric: 28.6349, val_loss: 29.2206, val_MinusLogProbMetric: 29.2206

Epoch 257: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.6349 - MinusLogProbMetric: 28.6349 - val_loss: 29.2206 - val_MinusLogProbMetric: 29.2206 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 258/1000
2023-10-25 15:13:03.819 
Epoch 258/1000 
	 loss: 28.4517, MinusLogProbMetric: 28.4517, val_loss: 29.2530, val_MinusLogProbMetric: 29.2530

Epoch 258: val_loss did not improve from 28.90667
196/196 - 33s - loss: 28.4517 - MinusLogProbMetric: 28.4517 - val_loss: 29.2530 - val_MinusLogProbMetric: 29.2530 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 259/1000
2023-10-25 15:13:35.189 
Epoch 259/1000 
	 loss: 28.6278, MinusLogProbMetric: 28.6278, val_loss: 29.0377, val_MinusLogProbMetric: 29.0377

Epoch 259: val_loss did not improve from 28.90667
196/196 - 31s - loss: 28.6278 - MinusLogProbMetric: 28.6278 - val_loss: 29.0377 - val_MinusLogProbMetric: 29.0377 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 260/1000
2023-10-25 15:14:06.764 
Epoch 260/1000 
	 loss: 28.4426, MinusLogProbMetric: 28.4426, val_loss: 29.1302, val_MinusLogProbMetric: 29.1302

Epoch 260: val_loss did not improve from 28.90667
196/196 - 32s - loss: 28.4426 - MinusLogProbMetric: 28.4426 - val_loss: 29.1302 - val_MinusLogProbMetric: 29.1302 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 261/1000
2023-10-25 15:14:41.339 
Epoch 261/1000 
	 loss: 28.6386, MinusLogProbMetric: 28.6386, val_loss: 29.1032, val_MinusLogProbMetric: 29.1032

Epoch 261: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.6386 - MinusLogProbMetric: 28.6386 - val_loss: 29.1032 - val_MinusLogProbMetric: 29.1032 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 262/1000
2023-10-25 15:15:16.184 
Epoch 262/1000 
	 loss: 28.5549, MinusLogProbMetric: 28.5549, val_loss: 29.6032, val_MinusLogProbMetric: 29.6032

Epoch 262: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.5549 - MinusLogProbMetric: 28.5549 - val_loss: 29.6032 - val_MinusLogProbMetric: 29.6032 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 263/1000
2023-10-25 15:15:50.682 
Epoch 263/1000 
	 loss: 28.6169, MinusLogProbMetric: 28.6169, val_loss: 29.2011, val_MinusLogProbMetric: 29.2011

Epoch 263: val_loss did not improve from 28.90667
196/196 - 34s - loss: 28.6169 - MinusLogProbMetric: 28.6169 - val_loss: 29.2011 - val_MinusLogProbMetric: 29.2011 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 264/1000
2023-10-25 15:16:25.236 
Epoch 264/1000 
	 loss: 28.4416, MinusLogProbMetric: 28.4416, val_loss: 28.9364, val_MinusLogProbMetric: 28.9364

Epoch 264: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.4416 - MinusLogProbMetric: 28.4416 - val_loss: 28.9364 - val_MinusLogProbMetric: 28.9364 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 265/1000
2023-10-25 15:16:59.513 
Epoch 265/1000 
	 loss: 28.6697, MinusLogProbMetric: 28.6697, val_loss: 29.2386, val_MinusLogProbMetric: 29.2386

Epoch 265: val_loss did not improve from 28.90667
196/196 - 34s - loss: 28.6697 - MinusLogProbMetric: 28.6697 - val_loss: 29.2386 - val_MinusLogProbMetric: 29.2386 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 266/1000
2023-10-25 15:17:34.318 
Epoch 266/1000 
	 loss: 28.4878, MinusLogProbMetric: 28.4878, val_loss: 29.1033, val_MinusLogProbMetric: 29.1033

Epoch 266: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.4878 - MinusLogProbMetric: 28.4878 - val_loss: 29.1033 - val_MinusLogProbMetric: 29.1033 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 267/1000
2023-10-25 15:18:08.365 
Epoch 267/1000 
	 loss: 28.5520, MinusLogProbMetric: 28.5520, val_loss: 29.2488, val_MinusLogProbMetric: 29.2488

Epoch 267: val_loss did not improve from 28.90667
196/196 - 34s - loss: 28.5520 - MinusLogProbMetric: 28.5520 - val_loss: 29.2488 - val_MinusLogProbMetric: 29.2488 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 268/1000
2023-10-25 15:18:43.322 
Epoch 268/1000 
	 loss: 28.4686, MinusLogProbMetric: 28.4686, val_loss: 29.2466, val_MinusLogProbMetric: 29.2466

Epoch 268: val_loss did not improve from 28.90667
196/196 - 35s - loss: 28.4686 - MinusLogProbMetric: 28.4686 - val_loss: 29.2466 - val_MinusLogProbMetric: 29.2466 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 269/1000
2023-10-25 15:19:17.859 
Epoch 269/1000 
	 loss: 28.4550, MinusLogProbMetric: 28.4550, val_loss: 28.8759, val_MinusLogProbMetric: 28.8759

Epoch 269: val_loss improved from 28.90667 to 28.87593, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 35s - loss: 28.4550 - MinusLogProbMetric: 28.4550 - val_loss: 28.8759 - val_MinusLogProbMetric: 28.8759 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 270/1000
2023-10-25 15:19:50.302 
Epoch 270/1000 
	 loss: 28.4809, MinusLogProbMetric: 28.4809, val_loss: 29.9176, val_MinusLogProbMetric: 29.9176

Epoch 270: val_loss did not improve from 28.87593
196/196 - 32s - loss: 28.4809 - MinusLogProbMetric: 28.4809 - val_loss: 29.9176 - val_MinusLogProbMetric: 29.9176 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 271/1000
2023-10-25 15:20:24.820 
Epoch 271/1000 
	 loss: 28.5084, MinusLogProbMetric: 28.5084, val_loss: 29.1367, val_MinusLogProbMetric: 29.1367

Epoch 271: val_loss did not improve from 28.87593
196/196 - 35s - loss: 28.5084 - MinusLogProbMetric: 28.5084 - val_loss: 29.1367 - val_MinusLogProbMetric: 29.1367 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 272/1000
2023-10-25 15:20:59.252 
Epoch 272/1000 
	 loss: 28.3892, MinusLogProbMetric: 28.3892, val_loss: 28.9968, val_MinusLogProbMetric: 28.9968

Epoch 272: val_loss did not improve from 28.87593
196/196 - 34s - loss: 28.3892 - MinusLogProbMetric: 28.3892 - val_loss: 28.9968 - val_MinusLogProbMetric: 28.9968 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 273/1000
2023-10-25 15:21:33.103 
Epoch 273/1000 
	 loss: 28.4857, MinusLogProbMetric: 28.4857, val_loss: 29.2868, val_MinusLogProbMetric: 29.2868

Epoch 273: val_loss did not improve from 28.87593
196/196 - 34s - loss: 28.4857 - MinusLogProbMetric: 28.4857 - val_loss: 29.2868 - val_MinusLogProbMetric: 29.2868 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 274/1000
2023-10-25 15:22:07.509 
Epoch 274/1000 
	 loss: 28.4021, MinusLogProbMetric: 28.4021, val_loss: 29.0500, val_MinusLogProbMetric: 29.0500

Epoch 274: val_loss did not improve from 28.87593
196/196 - 34s - loss: 28.4021 - MinusLogProbMetric: 28.4021 - val_loss: 29.0500 - val_MinusLogProbMetric: 29.0500 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 275/1000
2023-10-25 15:22:42.220 
Epoch 275/1000 
	 loss: 28.4665, MinusLogProbMetric: 28.4665, val_loss: 29.8693, val_MinusLogProbMetric: 29.8693

Epoch 275: val_loss did not improve from 28.87593
196/196 - 35s - loss: 28.4665 - MinusLogProbMetric: 28.4665 - val_loss: 29.8693 - val_MinusLogProbMetric: 29.8693 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 276/1000
2023-10-25 15:23:16.802 
Epoch 276/1000 
	 loss: 28.5158, MinusLogProbMetric: 28.5158, val_loss: 29.4764, val_MinusLogProbMetric: 29.4764

Epoch 276: val_loss did not improve from 28.87593
196/196 - 35s - loss: 28.5158 - MinusLogProbMetric: 28.5158 - val_loss: 29.4764 - val_MinusLogProbMetric: 29.4764 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 277/1000
2023-10-25 15:23:51.272 
Epoch 277/1000 
	 loss: 28.3721, MinusLogProbMetric: 28.3721, val_loss: 30.8459, val_MinusLogProbMetric: 30.8459

Epoch 277: val_loss did not improve from 28.87593
196/196 - 34s - loss: 28.3721 - MinusLogProbMetric: 28.3721 - val_loss: 30.8459 - val_MinusLogProbMetric: 30.8459 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 278/1000
2023-10-25 15:24:24.169 
Epoch 278/1000 
	 loss: 28.4751, MinusLogProbMetric: 28.4751, val_loss: 29.1063, val_MinusLogProbMetric: 29.1063

Epoch 278: val_loss did not improve from 28.87593
196/196 - 33s - loss: 28.4751 - MinusLogProbMetric: 28.4751 - val_loss: 29.1063 - val_MinusLogProbMetric: 29.1063 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 279/1000
2023-10-25 15:24:55.053 
Epoch 279/1000 
	 loss: 28.5146, MinusLogProbMetric: 28.5146, val_loss: 29.2206, val_MinusLogProbMetric: 29.2206

Epoch 279: val_loss did not improve from 28.87593
196/196 - 31s - loss: 28.5146 - MinusLogProbMetric: 28.5146 - val_loss: 29.2206 - val_MinusLogProbMetric: 29.2206 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 280/1000
2023-10-25 15:25:29.726 
Epoch 280/1000 
	 loss: 28.4942, MinusLogProbMetric: 28.4942, val_loss: 29.4937, val_MinusLogProbMetric: 29.4937

Epoch 280: val_loss did not improve from 28.87593
196/196 - 35s - loss: 28.4942 - MinusLogProbMetric: 28.4942 - val_loss: 29.4937 - val_MinusLogProbMetric: 29.4937 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 281/1000
2023-10-25 15:26:04.544 
Epoch 281/1000 
	 loss: 28.3188, MinusLogProbMetric: 28.3188, val_loss: 29.1943, val_MinusLogProbMetric: 29.1943

Epoch 281: val_loss did not improve from 28.87593
196/196 - 35s - loss: 28.3188 - MinusLogProbMetric: 28.3188 - val_loss: 29.1943 - val_MinusLogProbMetric: 29.1943 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 282/1000
2023-10-25 15:26:39.139 
Epoch 282/1000 
	 loss: 28.4106, MinusLogProbMetric: 28.4106, val_loss: 29.6588, val_MinusLogProbMetric: 29.6588

Epoch 282: val_loss did not improve from 28.87593
196/196 - 35s - loss: 28.4106 - MinusLogProbMetric: 28.4106 - val_loss: 29.6588 - val_MinusLogProbMetric: 29.6588 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 283/1000
2023-10-25 15:27:13.730 
Epoch 283/1000 
	 loss: 28.4674, MinusLogProbMetric: 28.4674, val_loss: 29.2298, val_MinusLogProbMetric: 29.2298

Epoch 283: val_loss did not improve from 28.87593
196/196 - 35s - loss: 28.4674 - MinusLogProbMetric: 28.4674 - val_loss: 29.2298 - val_MinusLogProbMetric: 29.2298 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 284/1000
2023-10-25 15:27:46.921 
Epoch 284/1000 
	 loss: 28.2987, MinusLogProbMetric: 28.2987, val_loss: 29.3418, val_MinusLogProbMetric: 29.3418

Epoch 284: val_loss did not improve from 28.87593
196/196 - 33s - loss: 28.2987 - MinusLogProbMetric: 28.2987 - val_loss: 29.3418 - val_MinusLogProbMetric: 29.3418 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 285/1000
2023-10-25 15:28:19.742 
Epoch 285/1000 
	 loss: 28.5445, MinusLogProbMetric: 28.5445, val_loss: 29.4290, val_MinusLogProbMetric: 29.4290

Epoch 285: val_loss did not improve from 28.87593
196/196 - 33s - loss: 28.5445 - MinusLogProbMetric: 28.5445 - val_loss: 29.4290 - val_MinusLogProbMetric: 29.4290 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 286/1000
2023-10-25 15:28:54.376 
Epoch 286/1000 
	 loss: 28.2923, MinusLogProbMetric: 28.2923, val_loss: 29.5868, val_MinusLogProbMetric: 29.5868

Epoch 286: val_loss did not improve from 28.87593
196/196 - 35s - loss: 28.2923 - MinusLogProbMetric: 28.2923 - val_loss: 29.5868 - val_MinusLogProbMetric: 29.5868 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 287/1000
2023-10-25 15:29:29.204 
Epoch 287/1000 
	 loss: 28.4225, MinusLogProbMetric: 28.4225, val_loss: 29.0561, val_MinusLogProbMetric: 29.0561

Epoch 287: val_loss did not improve from 28.87593
196/196 - 35s - loss: 28.4225 - MinusLogProbMetric: 28.4225 - val_loss: 29.0561 - val_MinusLogProbMetric: 29.0561 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 288/1000
2023-10-25 15:30:03.795 
Epoch 288/1000 
	 loss: 28.3813, MinusLogProbMetric: 28.3813, val_loss: 29.1511, val_MinusLogProbMetric: 29.1511

Epoch 288: val_loss did not improve from 28.87593
196/196 - 35s - loss: 28.3813 - MinusLogProbMetric: 28.3813 - val_loss: 29.1511 - val_MinusLogProbMetric: 29.1511 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 289/1000
2023-10-25 15:30:37.617 
Epoch 289/1000 
	 loss: 28.4318, MinusLogProbMetric: 28.4318, val_loss: 29.2986, val_MinusLogProbMetric: 29.2986

Epoch 289: val_loss did not improve from 28.87593
196/196 - 34s - loss: 28.4318 - MinusLogProbMetric: 28.4318 - val_loss: 29.2986 - val_MinusLogProbMetric: 29.2986 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 290/1000
2023-10-25 15:31:10.179 
Epoch 290/1000 
	 loss: 28.3336, MinusLogProbMetric: 28.3336, val_loss: 29.1114, val_MinusLogProbMetric: 29.1114

Epoch 290: val_loss did not improve from 28.87593
196/196 - 33s - loss: 28.3336 - MinusLogProbMetric: 28.3336 - val_loss: 29.1114 - val_MinusLogProbMetric: 29.1114 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 291/1000
2023-10-25 15:31:41.083 
Epoch 291/1000 
	 loss: 28.3674, MinusLogProbMetric: 28.3674, val_loss: 29.4274, val_MinusLogProbMetric: 29.4274

Epoch 291: val_loss did not improve from 28.87593
196/196 - 31s - loss: 28.3674 - MinusLogProbMetric: 28.3674 - val_loss: 29.4274 - val_MinusLogProbMetric: 29.4274 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 292/1000
2023-10-25 15:32:15.970 
Epoch 292/1000 
	 loss: 28.3634, MinusLogProbMetric: 28.3634, val_loss: 29.1302, val_MinusLogProbMetric: 29.1302

Epoch 292: val_loss did not improve from 28.87593
196/196 - 35s - loss: 28.3634 - MinusLogProbMetric: 28.3634 - val_loss: 29.1302 - val_MinusLogProbMetric: 29.1302 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 293/1000
2023-10-25 15:32:50.865 
Epoch 293/1000 
	 loss: 28.4371, MinusLogProbMetric: 28.4371, val_loss: 29.1163, val_MinusLogProbMetric: 29.1163

Epoch 293: val_loss did not improve from 28.87593
196/196 - 35s - loss: 28.4371 - MinusLogProbMetric: 28.4371 - val_loss: 29.1163 - val_MinusLogProbMetric: 29.1163 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 294/1000
2023-10-25 15:33:24.160 
Epoch 294/1000 
	 loss: 28.3470, MinusLogProbMetric: 28.3470, val_loss: 28.8832, val_MinusLogProbMetric: 28.8832

Epoch 294: val_loss did not improve from 28.87593
196/196 - 33s - loss: 28.3470 - MinusLogProbMetric: 28.3470 - val_loss: 28.8832 - val_MinusLogProbMetric: 28.8832 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 295/1000
2023-10-25 15:33:55.770 
Epoch 295/1000 
	 loss: 28.3620, MinusLogProbMetric: 28.3620, val_loss: 30.3896, val_MinusLogProbMetric: 30.3896

Epoch 295: val_loss did not improve from 28.87593
196/196 - 32s - loss: 28.3620 - MinusLogProbMetric: 28.3620 - val_loss: 30.3896 - val_MinusLogProbMetric: 30.3896 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 296/1000
2023-10-25 15:34:30.152 
Epoch 296/1000 
	 loss: 28.3544, MinusLogProbMetric: 28.3544, val_loss: 29.3617, val_MinusLogProbMetric: 29.3617

Epoch 296: val_loss did not improve from 28.87593
196/196 - 34s - loss: 28.3544 - MinusLogProbMetric: 28.3544 - val_loss: 29.3617 - val_MinusLogProbMetric: 29.3617 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 297/1000
2023-10-25 15:35:04.973 
Epoch 297/1000 
	 loss: 28.2804, MinusLogProbMetric: 28.2804, val_loss: 29.0968, val_MinusLogProbMetric: 29.0968

Epoch 297: val_loss did not improve from 28.87593
196/196 - 35s - loss: 28.2804 - MinusLogProbMetric: 28.2804 - val_loss: 29.0968 - val_MinusLogProbMetric: 29.0968 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 298/1000
2023-10-25 15:35:38.790 
Epoch 298/1000 
	 loss: 28.4347, MinusLogProbMetric: 28.4347, val_loss: 29.3891, val_MinusLogProbMetric: 29.3891

Epoch 298: val_loss did not improve from 28.87593
196/196 - 34s - loss: 28.4347 - MinusLogProbMetric: 28.4347 - val_loss: 29.3891 - val_MinusLogProbMetric: 29.3891 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 299/1000
2023-10-25 15:36:09.739 
Epoch 299/1000 
	 loss: 28.4084, MinusLogProbMetric: 28.4084, val_loss: 29.1355, val_MinusLogProbMetric: 29.1355

Epoch 299: val_loss did not improve from 28.87593
196/196 - 31s - loss: 28.4084 - MinusLogProbMetric: 28.4084 - val_loss: 29.1355 - val_MinusLogProbMetric: 29.1355 - lr: 0.0010 - 31s/epoch - 158ms/step
Epoch 300/1000
2023-10-25 15:36:41.478 
Epoch 300/1000 
	 loss: 28.3578, MinusLogProbMetric: 28.3578, val_loss: 29.7094, val_MinusLogProbMetric: 29.7094

Epoch 300: val_loss did not improve from 28.87593
196/196 - 32s - loss: 28.3578 - MinusLogProbMetric: 28.3578 - val_loss: 29.7094 - val_MinusLogProbMetric: 29.7094 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 301/1000
2023-10-25 15:37:13.530 
Epoch 301/1000 
	 loss: 28.3379, MinusLogProbMetric: 28.3379, val_loss: 30.3946, val_MinusLogProbMetric: 30.3946

Epoch 301: val_loss did not improve from 28.87593
196/196 - 32s - loss: 28.3379 - MinusLogProbMetric: 28.3379 - val_loss: 30.3946 - val_MinusLogProbMetric: 30.3946 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 302/1000
2023-10-25 15:37:45.719 
Epoch 302/1000 
	 loss: 28.3326, MinusLogProbMetric: 28.3326, val_loss: 29.3486, val_MinusLogProbMetric: 29.3486

Epoch 302: val_loss did not improve from 28.87593
196/196 - 32s - loss: 28.3326 - MinusLogProbMetric: 28.3326 - val_loss: 29.3486 - val_MinusLogProbMetric: 29.3486 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 303/1000
2023-10-25 15:38:18.674 
Epoch 303/1000 
	 loss: 28.2347, MinusLogProbMetric: 28.2347, val_loss: 29.0355, val_MinusLogProbMetric: 29.0355

Epoch 303: val_loss did not improve from 28.87593
196/196 - 33s - loss: 28.2347 - MinusLogProbMetric: 28.2347 - val_loss: 29.0355 - val_MinusLogProbMetric: 29.0355 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 304/1000
2023-10-25 15:38:50.304 
Epoch 304/1000 
	 loss: 28.4309, MinusLogProbMetric: 28.4309, val_loss: 30.0371, val_MinusLogProbMetric: 30.0371

Epoch 304: val_loss did not improve from 28.87593
196/196 - 32s - loss: 28.4309 - MinusLogProbMetric: 28.4309 - val_loss: 30.0371 - val_MinusLogProbMetric: 30.0371 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 305/1000
2023-10-25 15:39:22.625 
Epoch 305/1000 
	 loss: 28.3821, MinusLogProbMetric: 28.3821, val_loss: 29.3275, val_MinusLogProbMetric: 29.3275

Epoch 305: val_loss did not improve from 28.87593
196/196 - 32s - loss: 28.3821 - MinusLogProbMetric: 28.3821 - val_loss: 29.3275 - val_MinusLogProbMetric: 29.3275 - lr: 0.0010 - 32s/epoch - 165ms/step
Epoch 306/1000
2023-10-25 15:39:53.949 
Epoch 306/1000 
	 loss: 28.3415, MinusLogProbMetric: 28.3415, val_loss: 29.1949, val_MinusLogProbMetric: 29.1949

Epoch 306: val_loss did not improve from 28.87593
196/196 - 31s - loss: 28.3415 - MinusLogProbMetric: 28.3415 - val_loss: 29.1949 - val_MinusLogProbMetric: 29.1949 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 307/1000
2023-10-25 15:40:25.254 
Epoch 307/1000 
	 loss: 28.3688, MinusLogProbMetric: 28.3688, val_loss: 28.8875, val_MinusLogProbMetric: 28.8875

Epoch 307: val_loss did not improve from 28.87593
196/196 - 31s - loss: 28.3688 - MinusLogProbMetric: 28.3688 - val_loss: 28.8875 - val_MinusLogProbMetric: 28.8875 - lr: 0.0010 - 31s/epoch - 160ms/step
Epoch 308/1000
2023-10-25 15:40:58.135 
Epoch 308/1000 
	 loss: 28.2312, MinusLogProbMetric: 28.2312, val_loss: 29.3126, val_MinusLogProbMetric: 29.3126

Epoch 308: val_loss did not improve from 28.87593
196/196 - 33s - loss: 28.2312 - MinusLogProbMetric: 28.2312 - val_loss: 29.3126 - val_MinusLogProbMetric: 29.3126 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 309/1000
2023-10-25 15:41:30.063 
Epoch 309/1000 
	 loss: 28.4245, MinusLogProbMetric: 28.4245, val_loss: 28.9404, val_MinusLogProbMetric: 28.9404

Epoch 309: val_loss did not improve from 28.87593
196/196 - 32s - loss: 28.4245 - MinusLogProbMetric: 28.4245 - val_loss: 28.9404 - val_MinusLogProbMetric: 28.9404 - lr: 0.0010 - 32s/epoch - 163ms/step
Epoch 310/1000
2023-10-25 15:42:02.857 
Epoch 310/1000 
	 loss: 28.3906, MinusLogProbMetric: 28.3906, val_loss: 30.2407, val_MinusLogProbMetric: 30.2407

Epoch 310: val_loss did not improve from 28.87593
196/196 - 33s - loss: 28.3906 - MinusLogProbMetric: 28.3906 - val_loss: 30.2407 - val_MinusLogProbMetric: 30.2407 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 311/1000
2023-10-25 15:42:35.681 
Epoch 311/1000 
	 loss: 28.4271, MinusLogProbMetric: 28.4271, val_loss: 29.6474, val_MinusLogProbMetric: 29.6474

Epoch 311: val_loss did not improve from 28.87593
196/196 - 33s - loss: 28.4271 - MinusLogProbMetric: 28.4271 - val_loss: 29.6474 - val_MinusLogProbMetric: 29.6474 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 312/1000
2023-10-25 15:43:09.895 
Epoch 312/1000 
	 loss: 28.2478, MinusLogProbMetric: 28.2478, val_loss: 28.9871, val_MinusLogProbMetric: 28.9871

Epoch 312: val_loss did not improve from 28.87593
196/196 - 34s - loss: 28.2478 - MinusLogProbMetric: 28.2478 - val_loss: 28.9871 - val_MinusLogProbMetric: 28.9871 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 313/1000
2023-10-25 15:43:44.436 
Epoch 313/1000 
	 loss: 28.2266, MinusLogProbMetric: 28.2266, val_loss: 29.0584, val_MinusLogProbMetric: 29.0584

Epoch 313: val_loss did not improve from 28.87593
196/196 - 35s - loss: 28.2266 - MinusLogProbMetric: 28.2266 - val_loss: 29.0584 - val_MinusLogProbMetric: 29.0584 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 314/1000
2023-10-25 15:44:18.272 
Epoch 314/1000 
	 loss: 28.2485, MinusLogProbMetric: 28.2485, val_loss: 29.5372, val_MinusLogProbMetric: 29.5372

Epoch 314: val_loss did not improve from 28.87593
196/196 - 34s - loss: 28.2485 - MinusLogProbMetric: 28.2485 - val_loss: 29.5372 - val_MinusLogProbMetric: 29.5372 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 315/1000
2023-10-25 15:44:50.500 
Epoch 315/1000 
	 loss: 28.2858, MinusLogProbMetric: 28.2858, val_loss: 29.1017, val_MinusLogProbMetric: 29.1017

Epoch 315: val_loss did not improve from 28.87593
196/196 - 32s - loss: 28.2858 - MinusLogProbMetric: 28.2858 - val_loss: 29.1017 - val_MinusLogProbMetric: 29.1017 - lr: 0.0010 - 32s/epoch - 164ms/step
Epoch 316/1000
2023-10-25 15:45:22.123 
Epoch 316/1000 
	 loss: 28.2251, MinusLogProbMetric: 28.2251, val_loss: 29.7717, val_MinusLogProbMetric: 29.7717

Epoch 316: val_loss did not improve from 28.87593
196/196 - 32s - loss: 28.2251 - MinusLogProbMetric: 28.2251 - val_loss: 29.7717 - val_MinusLogProbMetric: 29.7717 - lr: 0.0010 - 32s/epoch - 161ms/step
Epoch 317/1000
2023-10-25 15:45:53.780 
Epoch 317/1000 
	 loss: 28.3474, MinusLogProbMetric: 28.3474, val_loss: 29.0692, val_MinusLogProbMetric: 29.0692

Epoch 317: val_loss did not improve from 28.87593
196/196 - 32s - loss: 28.3474 - MinusLogProbMetric: 28.3474 - val_loss: 29.0692 - val_MinusLogProbMetric: 29.0692 - lr: 0.0010 - 32s/epoch - 162ms/step
Epoch 318/1000
2023-10-25 15:46:26.610 
Epoch 318/1000 
	 loss: 28.2292, MinusLogProbMetric: 28.2292, val_loss: 29.1236, val_MinusLogProbMetric: 29.1236

Epoch 318: val_loss did not improve from 28.87593
196/196 - 33s - loss: 28.2292 - MinusLogProbMetric: 28.2292 - val_loss: 29.1236 - val_MinusLogProbMetric: 29.1236 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 319/1000
2023-10-25 15:46:57.802 
Epoch 319/1000 
	 loss: 28.1799, MinusLogProbMetric: 28.1799, val_loss: 29.2752, val_MinusLogProbMetric: 29.2752

Epoch 319: val_loss did not improve from 28.87593
196/196 - 31s - loss: 28.1799 - MinusLogProbMetric: 28.1799 - val_loss: 29.2752 - val_MinusLogProbMetric: 29.2752 - lr: 0.0010 - 31s/epoch - 159ms/step
Epoch 320/1000
2023-10-25 15:47:31.573 
Epoch 320/1000 
	 loss: 27.6633, MinusLogProbMetric: 27.6633, val_loss: 28.6817, val_MinusLogProbMetric: 28.6817

Epoch 320: val_loss improved from 28.87593 to 28.68174, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 34s - loss: 27.6633 - MinusLogProbMetric: 27.6633 - val_loss: 28.6817 - val_MinusLogProbMetric: 28.6817 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 321/1000
2023-10-25 15:48:04.045 
Epoch 321/1000 
	 loss: 27.6264, MinusLogProbMetric: 27.6264, val_loss: 28.6570, val_MinusLogProbMetric: 28.6570

Epoch 321: val_loss improved from 28.68174 to 28.65699, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 32s - loss: 27.6264 - MinusLogProbMetric: 27.6264 - val_loss: 28.6570 - val_MinusLogProbMetric: 28.6570 - lr: 5.0000e-04 - 32s/epoch - 166ms/step
Epoch 322/1000
2023-10-25 15:48:37.610 
Epoch 322/1000 
	 loss: 27.6409, MinusLogProbMetric: 27.6409, val_loss: 28.6344, val_MinusLogProbMetric: 28.6344

Epoch 322: val_loss improved from 28.65699 to 28.63445, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 34s - loss: 27.6409 - MinusLogProbMetric: 27.6409 - val_loss: 28.6344 - val_MinusLogProbMetric: 28.6344 - lr: 5.0000e-04 - 34s/epoch - 172ms/step
Epoch 323/1000
2023-10-25 15:49:13.060 
Epoch 323/1000 
	 loss: 27.6409, MinusLogProbMetric: 27.6409, val_loss: 28.7263, val_MinusLogProbMetric: 28.7263

Epoch 323: val_loss did not improve from 28.63445
196/196 - 35s - loss: 27.6409 - MinusLogProbMetric: 27.6409 - val_loss: 28.7263 - val_MinusLogProbMetric: 28.7263 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 324/1000
2023-10-25 15:49:44.984 
Epoch 324/1000 
	 loss: 27.6765, MinusLogProbMetric: 27.6765, val_loss: 28.6307, val_MinusLogProbMetric: 28.6307

Epoch 324: val_loss improved from 28.63445 to 28.63073, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 32s - loss: 27.6765 - MinusLogProbMetric: 27.6765 - val_loss: 28.6307 - val_MinusLogProbMetric: 28.6307 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 325/1000
2023-10-25 15:50:19.668 
Epoch 325/1000 
	 loss: 27.6797, MinusLogProbMetric: 27.6797, val_loss: 28.7422, val_MinusLogProbMetric: 28.7422

Epoch 325: val_loss did not improve from 28.63073
196/196 - 34s - loss: 27.6797 - MinusLogProbMetric: 27.6797 - val_loss: 28.7422 - val_MinusLogProbMetric: 28.7422 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 326/1000
2023-10-25 15:50:51.516 
Epoch 326/1000 
	 loss: 27.6223, MinusLogProbMetric: 27.6223, val_loss: 28.6472, val_MinusLogProbMetric: 28.6472

Epoch 326: val_loss did not improve from 28.63073
196/196 - 32s - loss: 27.6223 - MinusLogProbMetric: 27.6223 - val_loss: 28.6472 - val_MinusLogProbMetric: 28.6472 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 327/1000
2023-10-25 15:51:24.775 
Epoch 327/1000 
	 loss: 27.6760, MinusLogProbMetric: 27.6760, val_loss: 28.8400, val_MinusLogProbMetric: 28.8400

Epoch 327: val_loss did not improve from 28.63073
196/196 - 33s - loss: 27.6760 - MinusLogProbMetric: 27.6760 - val_loss: 28.8400 - val_MinusLogProbMetric: 28.8400 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 328/1000
2023-10-25 15:51:55.979 
Epoch 328/1000 
	 loss: 27.6840, MinusLogProbMetric: 27.6840, val_loss: 28.7594, val_MinusLogProbMetric: 28.7594

Epoch 328: val_loss did not improve from 28.63073
196/196 - 31s - loss: 27.6840 - MinusLogProbMetric: 27.6840 - val_loss: 28.7594 - val_MinusLogProbMetric: 28.7594 - lr: 5.0000e-04 - 31s/epoch - 159ms/step
Epoch 329/1000
2023-10-25 15:52:28.218 
Epoch 329/1000 
	 loss: 27.6900, MinusLogProbMetric: 27.6900, val_loss: 28.6462, val_MinusLogProbMetric: 28.6462

Epoch 329: val_loss did not improve from 28.63073
196/196 - 32s - loss: 27.6900 - MinusLogProbMetric: 27.6900 - val_loss: 28.6462 - val_MinusLogProbMetric: 28.6462 - lr: 5.0000e-04 - 32s/epoch - 164ms/step
Epoch 330/1000
2023-10-25 15:53:01.584 
Epoch 330/1000 
	 loss: 27.6437, MinusLogProbMetric: 27.6437, val_loss: 28.6275, val_MinusLogProbMetric: 28.6275

Epoch 330: val_loss improved from 28.63073 to 28.62754, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 34s - loss: 27.6437 - MinusLogProbMetric: 27.6437 - val_loss: 28.6275 - val_MinusLogProbMetric: 28.6275 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 331/1000
2023-10-25 15:53:33.349 
Epoch 331/1000 
	 loss: 27.6018, MinusLogProbMetric: 27.6018, val_loss: 28.9370, val_MinusLogProbMetric: 28.9370

Epoch 331: val_loss did not improve from 28.62754
196/196 - 31s - loss: 27.6018 - MinusLogProbMetric: 27.6018 - val_loss: 28.9370 - val_MinusLogProbMetric: 28.9370 - lr: 5.0000e-04 - 31s/epoch - 160ms/step
Epoch 332/1000
2023-10-25 15:54:06.002 
Epoch 332/1000 
	 loss: 27.6892, MinusLogProbMetric: 27.6892, val_loss: 28.6827, val_MinusLogProbMetric: 28.6827

Epoch 332: val_loss did not improve from 28.62754
196/196 - 33s - loss: 27.6892 - MinusLogProbMetric: 27.6892 - val_loss: 28.6827 - val_MinusLogProbMetric: 28.6827 - lr: 5.0000e-04 - 33s/epoch - 167ms/step
Epoch 333/1000
2023-10-25 15:54:38.272 
Epoch 333/1000 
	 loss: 27.6834, MinusLogProbMetric: 27.6834, val_loss: 28.7186, val_MinusLogProbMetric: 28.7186

Epoch 333: val_loss did not improve from 28.62754
196/196 - 32s - loss: 27.6834 - MinusLogProbMetric: 27.6834 - val_loss: 28.7186 - val_MinusLogProbMetric: 28.7186 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 334/1000
2023-10-25 15:55:11.060 
Epoch 334/1000 
	 loss: 27.6458, MinusLogProbMetric: 27.6458, val_loss: 28.5339, val_MinusLogProbMetric: 28.5339

Epoch 334: val_loss improved from 28.62754 to 28.53391, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 33s - loss: 27.6458 - MinusLogProbMetric: 27.6458 - val_loss: 28.5339 - val_MinusLogProbMetric: 28.5339 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 335/1000
2023-10-25 15:55:43.109 
Epoch 335/1000 
	 loss: 27.6175, MinusLogProbMetric: 27.6175, val_loss: 28.6101, val_MinusLogProbMetric: 28.6101

Epoch 335: val_loss did not improve from 28.53391
196/196 - 32s - loss: 27.6175 - MinusLogProbMetric: 27.6175 - val_loss: 28.6101 - val_MinusLogProbMetric: 28.6101 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 336/1000
2023-10-25 15:56:14.825 
Epoch 336/1000 
	 loss: 27.6672, MinusLogProbMetric: 27.6672, val_loss: 28.7061, val_MinusLogProbMetric: 28.7061

Epoch 336: val_loss did not improve from 28.53391
196/196 - 32s - loss: 27.6672 - MinusLogProbMetric: 27.6672 - val_loss: 28.7061 - val_MinusLogProbMetric: 28.7061 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 337/1000
2023-10-25 15:56:47.322 
Epoch 337/1000 
	 loss: 27.6268, MinusLogProbMetric: 27.6268, val_loss: 29.0140, val_MinusLogProbMetric: 29.0140

Epoch 337: val_loss did not improve from 28.53391
196/196 - 32s - loss: 27.6268 - MinusLogProbMetric: 27.6268 - val_loss: 29.0140 - val_MinusLogProbMetric: 29.0140 - lr: 5.0000e-04 - 32s/epoch - 166ms/step
Epoch 338/1000
2023-10-25 15:57:19.819 
Epoch 338/1000 
	 loss: 27.6338, MinusLogProbMetric: 27.6338, val_loss: 28.9332, val_MinusLogProbMetric: 28.9332

Epoch 338: val_loss did not improve from 28.53391
196/196 - 32s - loss: 27.6338 - MinusLogProbMetric: 27.6338 - val_loss: 28.9332 - val_MinusLogProbMetric: 28.9332 - lr: 5.0000e-04 - 32s/epoch - 166ms/step
Epoch 339/1000
2023-10-25 15:57:50.293 
Epoch 339/1000 
	 loss: 27.6668, MinusLogProbMetric: 27.6668, val_loss: 28.7689, val_MinusLogProbMetric: 28.7689

Epoch 339: val_loss did not improve from 28.53391
196/196 - 30s - loss: 27.6668 - MinusLogProbMetric: 27.6668 - val_loss: 28.7689 - val_MinusLogProbMetric: 28.7689 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 340/1000
2023-10-25 15:58:21.097 
Epoch 340/1000 
	 loss: 27.6436, MinusLogProbMetric: 27.6436, val_loss: 28.7518, val_MinusLogProbMetric: 28.7518

Epoch 340: val_loss did not improve from 28.53391
196/196 - 31s - loss: 27.6436 - MinusLogProbMetric: 27.6436 - val_loss: 28.7518 - val_MinusLogProbMetric: 28.7518 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 341/1000
2023-10-25 15:58:52.842 
Epoch 341/1000 
	 loss: 27.6359, MinusLogProbMetric: 27.6359, val_loss: 28.8233, val_MinusLogProbMetric: 28.8233

Epoch 341: val_loss did not improve from 28.53391
196/196 - 32s - loss: 27.6359 - MinusLogProbMetric: 27.6359 - val_loss: 28.8233 - val_MinusLogProbMetric: 28.8233 - lr: 5.0000e-04 - 32s/epoch - 162ms/step
Epoch 342/1000
2023-10-25 15:59:23.712 
Epoch 342/1000 
	 loss: 27.6801, MinusLogProbMetric: 27.6801, val_loss: 28.8359, val_MinusLogProbMetric: 28.8359

Epoch 342: val_loss did not improve from 28.53391
196/196 - 31s - loss: 27.6801 - MinusLogProbMetric: 27.6801 - val_loss: 28.8359 - val_MinusLogProbMetric: 28.8359 - lr: 5.0000e-04 - 31s/epoch - 157ms/step
Epoch 343/1000
2023-10-25 15:59:54.263 
Epoch 343/1000 
	 loss: 27.6628, MinusLogProbMetric: 27.6628, val_loss: 28.6063, val_MinusLogProbMetric: 28.6063

Epoch 343: val_loss did not improve from 28.53391
196/196 - 31s - loss: 27.6628 - MinusLogProbMetric: 27.6628 - val_loss: 28.6063 - val_MinusLogProbMetric: 28.6063 - lr: 5.0000e-04 - 31s/epoch - 156ms/step
Epoch 344/1000
2023-10-25 16:00:24.718 
Epoch 344/1000 
	 loss: 27.6030, MinusLogProbMetric: 27.6030, val_loss: 28.6388, val_MinusLogProbMetric: 28.6388

Epoch 344: val_loss did not improve from 28.53391
196/196 - 30s - loss: 27.6030 - MinusLogProbMetric: 27.6030 - val_loss: 28.6388 - val_MinusLogProbMetric: 28.6388 - lr: 5.0000e-04 - 30s/epoch - 155ms/step
Epoch 345/1000
2023-10-25 16:00:56.202 
Epoch 345/1000 
	 loss: 27.6524, MinusLogProbMetric: 27.6524, val_loss: 28.6412, val_MinusLogProbMetric: 28.6412

Epoch 345: val_loss did not improve from 28.53391
196/196 - 31s - loss: 27.6524 - MinusLogProbMetric: 27.6524 - val_loss: 28.6412 - val_MinusLogProbMetric: 28.6412 - lr: 5.0000e-04 - 31s/epoch - 161ms/step
Epoch 346/1000
2023-10-25 16:01:27.834 
Epoch 346/1000 
	 loss: 27.6281, MinusLogProbMetric: 27.6281, val_loss: 28.7907, val_MinusLogProbMetric: 28.7907

Epoch 346: val_loss did not improve from 28.53391
196/196 - 32s - loss: 27.6281 - MinusLogProbMetric: 27.6281 - val_loss: 28.7907 - val_MinusLogProbMetric: 28.7907 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 347/1000
2023-10-25 16:02:02.094 
Epoch 347/1000 
	 loss: 27.6127, MinusLogProbMetric: 27.6127, val_loss: 28.6425, val_MinusLogProbMetric: 28.6425

Epoch 347: val_loss did not improve from 28.53391
196/196 - 34s - loss: 27.6127 - MinusLogProbMetric: 27.6127 - val_loss: 28.6425 - val_MinusLogProbMetric: 28.6425 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 348/1000
2023-10-25 16:02:36.087 
Epoch 348/1000 
	 loss: 27.5923, MinusLogProbMetric: 27.5923, val_loss: 28.7062, val_MinusLogProbMetric: 28.7062

Epoch 348: val_loss did not improve from 28.53391
196/196 - 34s - loss: 27.5923 - MinusLogProbMetric: 27.5923 - val_loss: 28.7062 - val_MinusLogProbMetric: 28.7062 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 349/1000
2023-10-25 16:03:10.791 
Epoch 349/1000 
	 loss: 27.6124, MinusLogProbMetric: 27.6124, val_loss: 28.5757, val_MinusLogProbMetric: 28.5757

Epoch 349: val_loss did not improve from 28.53391
196/196 - 35s - loss: 27.6124 - MinusLogProbMetric: 27.6124 - val_loss: 28.5757 - val_MinusLogProbMetric: 28.5757 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 350/1000
2023-10-25 16:03:45.705 
Epoch 350/1000 
	 loss: 27.8048, MinusLogProbMetric: 27.8048, val_loss: 28.6431, val_MinusLogProbMetric: 28.6431

Epoch 350: val_loss did not improve from 28.53391
196/196 - 35s - loss: 27.8048 - MinusLogProbMetric: 27.8048 - val_loss: 28.6431 - val_MinusLogProbMetric: 28.6431 - lr: 5.0000e-04 - 35s/epoch - 178ms/step
Epoch 351/1000
2023-10-25 16:04:22.015 
Epoch 351/1000 
	 loss: 27.6034, MinusLogProbMetric: 27.6034, val_loss: 28.7546, val_MinusLogProbMetric: 28.7546

Epoch 351: val_loss did not improve from 28.53391
196/196 - 36s - loss: 27.6034 - MinusLogProbMetric: 27.6034 - val_loss: 28.7546 - val_MinusLogProbMetric: 28.7546 - lr: 5.0000e-04 - 36s/epoch - 185ms/step
Epoch 352/1000
2023-10-25 16:04:58.129 
Epoch 352/1000 
	 loss: 27.5983, MinusLogProbMetric: 27.5983, val_loss: 28.6104, val_MinusLogProbMetric: 28.6104

Epoch 352: val_loss did not improve from 28.53391
196/196 - 36s - loss: 27.5983 - MinusLogProbMetric: 27.5983 - val_loss: 28.6104 - val_MinusLogProbMetric: 28.6104 - lr: 5.0000e-04 - 36s/epoch - 184ms/step
Epoch 353/1000
2023-10-25 16:05:32.483 
Epoch 353/1000 
	 loss: 27.6171, MinusLogProbMetric: 27.6171, val_loss: 28.7311, val_MinusLogProbMetric: 28.7311

Epoch 353: val_loss did not improve from 28.53391
196/196 - 34s - loss: 27.6171 - MinusLogProbMetric: 27.6171 - val_loss: 28.7311 - val_MinusLogProbMetric: 28.7311 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 354/1000
2023-10-25 16:06:02.395 
Epoch 354/1000 
	 loss: 27.6380, MinusLogProbMetric: 27.6380, val_loss: 28.7807, val_MinusLogProbMetric: 28.7807

Epoch 354: val_loss did not improve from 28.53391
196/196 - 30s - loss: 27.6380 - MinusLogProbMetric: 27.6380 - val_loss: 28.7807 - val_MinusLogProbMetric: 28.7807 - lr: 5.0000e-04 - 30s/epoch - 153ms/step
Epoch 355/1000
2023-10-25 16:06:30.653 
Epoch 355/1000 
	 loss: 27.5643, MinusLogProbMetric: 27.5643, val_loss: 28.5638, val_MinusLogProbMetric: 28.5638

Epoch 355: val_loss did not improve from 28.53391
196/196 - 28s - loss: 27.5643 - MinusLogProbMetric: 27.5643 - val_loss: 28.5638 - val_MinusLogProbMetric: 28.5638 - lr: 5.0000e-04 - 28s/epoch - 144ms/step
Epoch 356/1000
2023-10-25 16:06:57.822 
Epoch 356/1000 
	 loss: 27.6575, MinusLogProbMetric: 27.6575, val_loss: 28.6054, val_MinusLogProbMetric: 28.6054

Epoch 356: val_loss did not improve from 28.53391
196/196 - 27s - loss: 27.6575 - MinusLogProbMetric: 27.6575 - val_loss: 28.6054 - val_MinusLogProbMetric: 28.6054 - lr: 5.0000e-04 - 27s/epoch - 139ms/step
Epoch 357/1000
2023-10-25 16:07:26.019 
Epoch 357/1000 
	 loss: 27.5631, MinusLogProbMetric: 27.5631, val_loss: 28.8764, val_MinusLogProbMetric: 28.8764

Epoch 357: val_loss did not improve from 28.53391
196/196 - 28s - loss: 27.5631 - MinusLogProbMetric: 27.5631 - val_loss: 28.8764 - val_MinusLogProbMetric: 28.8764 - lr: 5.0000e-04 - 28s/epoch - 144ms/step
Epoch 358/1000
2023-10-25 16:07:52.622 
Epoch 358/1000 
	 loss: 27.6357, MinusLogProbMetric: 27.6357, val_loss: 28.5981, val_MinusLogProbMetric: 28.5981

Epoch 358: val_loss did not improve from 28.53391
196/196 - 27s - loss: 27.6357 - MinusLogProbMetric: 27.6357 - val_loss: 28.5981 - val_MinusLogProbMetric: 28.5981 - lr: 5.0000e-04 - 27s/epoch - 136ms/step
Epoch 359/1000
2023-10-25 16:08:18.711 
Epoch 359/1000 
	 loss: 27.5993, MinusLogProbMetric: 27.5993, val_loss: 28.6269, val_MinusLogProbMetric: 28.6269

Epoch 359: val_loss did not improve from 28.53391
196/196 - 26s - loss: 27.5993 - MinusLogProbMetric: 27.5993 - val_loss: 28.6269 - val_MinusLogProbMetric: 28.6269 - lr: 5.0000e-04 - 26s/epoch - 133ms/step
Epoch 360/1000
2023-10-25 16:08:44.977 
Epoch 360/1000 
	 loss: 27.6811, MinusLogProbMetric: 27.6811, val_loss: 28.6320, val_MinusLogProbMetric: 28.6320

Epoch 360: val_loss did not improve from 28.53391
196/196 - 26s - loss: 27.6811 - MinusLogProbMetric: 27.6811 - val_loss: 28.6320 - val_MinusLogProbMetric: 28.6320 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 361/1000
2023-10-25 16:09:10.836 
Epoch 361/1000 
	 loss: 27.6013, MinusLogProbMetric: 27.6013, val_loss: 28.7519, val_MinusLogProbMetric: 28.7519

Epoch 361: val_loss did not improve from 28.53391
196/196 - 26s - loss: 27.6013 - MinusLogProbMetric: 27.6013 - val_loss: 28.7519 - val_MinusLogProbMetric: 28.7519 - lr: 5.0000e-04 - 26s/epoch - 132ms/step
Epoch 362/1000
2023-10-25 16:09:37.427 
Epoch 362/1000 
	 loss: 27.5753, MinusLogProbMetric: 27.5753, val_loss: 29.0615, val_MinusLogProbMetric: 29.0615

Epoch 362: val_loss did not improve from 28.53391
196/196 - 27s - loss: 27.5753 - MinusLogProbMetric: 27.5753 - val_loss: 29.0615 - val_MinusLogProbMetric: 29.0615 - lr: 5.0000e-04 - 27s/epoch - 136ms/step
Epoch 363/1000
2023-10-25 16:10:03.885 
Epoch 363/1000 
	 loss: 27.6437, MinusLogProbMetric: 27.6437, val_loss: 28.8900, val_MinusLogProbMetric: 28.8900

Epoch 363: val_loss did not improve from 28.53391
196/196 - 26s - loss: 27.6437 - MinusLogProbMetric: 27.6437 - val_loss: 28.8900 - val_MinusLogProbMetric: 28.8900 - lr: 5.0000e-04 - 26s/epoch - 135ms/step
Epoch 364/1000
2023-10-25 16:10:30.239 
Epoch 364/1000 
	 loss: 27.5986, MinusLogProbMetric: 27.5986, val_loss: 28.6359, val_MinusLogProbMetric: 28.6359

Epoch 364: val_loss did not improve from 28.53391
196/196 - 26s - loss: 27.5986 - MinusLogProbMetric: 27.5986 - val_loss: 28.6359 - val_MinusLogProbMetric: 28.6359 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 365/1000
2023-10-25 16:10:56.461 
Epoch 365/1000 
	 loss: 27.5687, MinusLogProbMetric: 27.5687, val_loss: 28.7482, val_MinusLogProbMetric: 28.7482

Epoch 365: val_loss did not improve from 28.53391
196/196 - 26s - loss: 27.5687 - MinusLogProbMetric: 27.5687 - val_loss: 28.7482 - val_MinusLogProbMetric: 28.7482 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 366/1000
2023-10-25 16:11:22.678 
Epoch 366/1000 
	 loss: 27.6114, MinusLogProbMetric: 27.6114, val_loss: 28.6306, val_MinusLogProbMetric: 28.6306

Epoch 366: val_loss did not improve from 28.53391
196/196 - 26s - loss: 27.6114 - MinusLogProbMetric: 27.6114 - val_loss: 28.6306 - val_MinusLogProbMetric: 28.6306 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 367/1000
2023-10-25 16:11:49.549 
Epoch 367/1000 
	 loss: 27.6321, MinusLogProbMetric: 27.6321, val_loss: 28.6161, val_MinusLogProbMetric: 28.6161

Epoch 367: val_loss did not improve from 28.53391
196/196 - 27s - loss: 27.6321 - MinusLogProbMetric: 27.6321 - val_loss: 28.6161 - val_MinusLogProbMetric: 28.6161 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 368/1000
2023-10-25 16:12:16.450 
Epoch 368/1000 
	 loss: 27.5821, MinusLogProbMetric: 27.5821, val_loss: 28.6613, val_MinusLogProbMetric: 28.6613

Epoch 368: val_loss did not improve from 28.53391
196/196 - 27s - loss: 27.5821 - MinusLogProbMetric: 27.5821 - val_loss: 28.6613 - val_MinusLogProbMetric: 28.6613 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 369/1000
2023-10-25 16:12:42.640 
Epoch 369/1000 
	 loss: 27.5533, MinusLogProbMetric: 27.5533, val_loss: 28.6885, val_MinusLogProbMetric: 28.6885

Epoch 369: val_loss did not improve from 28.53391
196/196 - 26s - loss: 27.5533 - MinusLogProbMetric: 27.5533 - val_loss: 28.6885 - val_MinusLogProbMetric: 28.6885 - lr: 5.0000e-04 - 26s/epoch - 134ms/step
Epoch 370/1000
2023-10-25 16:13:10.036 
Epoch 370/1000 
	 loss: 27.6410, MinusLogProbMetric: 27.6410, val_loss: 28.6825, val_MinusLogProbMetric: 28.6825

Epoch 370: val_loss did not improve from 28.53391
196/196 - 27s - loss: 27.6410 - MinusLogProbMetric: 27.6410 - val_loss: 28.6825 - val_MinusLogProbMetric: 28.6825 - lr: 5.0000e-04 - 27s/epoch - 140ms/step
Epoch 371/1000
2023-10-25 16:13:37.291 
Epoch 371/1000 
	 loss: 27.5595, MinusLogProbMetric: 27.5595, val_loss: 28.6345, val_MinusLogProbMetric: 28.6345

Epoch 371: val_loss did not improve from 28.53391
196/196 - 27s - loss: 27.5595 - MinusLogProbMetric: 27.5595 - val_loss: 28.6345 - val_MinusLogProbMetric: 28.6345 - lr: 5.0000e-04 - 27s/epoch - 139ms/step
Epoch 372/1000
2023-10-25 16:14:04.438 
Epoch 372/1000 
	 loss: 27.6291, MinusLogProbMetric: 27.6291, val_loss: 29.1564, val_MinusLogProbMetric: 29.1564

Epoch 372: val_loss did not improve from 28.53391
196/196 - 27s - loss: 27.6291 - MinusLogProbMetric: 27.6291 - val_loss: 29.1564 - val_MinusLogProbMetric: 29.1564 - lr: 5.0000e-04 - 27s/epoch - 138ms/step
Epoch 373/1000
2023-10-25 16:14:35.971 
Epoch 373/1000 
	 loss: 27.6148, MinusLogProbMetric: 27.6148, val_loss: 28.6233, val_MinusLogProbMetric: 28.6233

Epoch 373: val_loss did not improve from 28.53391
196/196 - 32s - loss: 27.6148 - MinusLogProbMetric: 27.6148 - val_loss: 28.6233 - val_MinusLogProbMetric: 28.6233 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 374/1000
2023-10-25 16:15:03.497 
Epoch 374/1000 
	 loss: 27.5544, MinusLogProbMetric: 27.5544, val_loss: 28.7471, val_MinusLogProbMetric: 28.7471

Epoch 374: val_loss did not improve from 28.53391
196/196 - 28s - loss: 27.5544 - MinusLogProbMetric: 27.5544 - val_loss: 28.7471 - val_MinusLogProbMetric: 28.7471 - lr: 5.0000e-04 - 28s/epoch - 140ms/step
Epoch 375/1000
2023-10-25 16:15:30.384 
Epoch 375/1000 
	 loss: 27.6292, MinusLogProbMetric: 27.6292, val_loss: 28.7422, val_MinusLogProbMetric: 28.7422

Epoch 375: val_loss did not improve from 28.53391
196/196 - 27s - loss: 27.6292 - MinusLogProbMetric: 27.6292 - val_loss: 28.7422 - val_MinusLogProbMetric: 28.7422 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 376/1000
2023-10-25 16:15:57.335 
Epoch 376/1000 
	 loss: 27.5897, MinusLogProbMetric: 27.5897, val_loss: 28.7122, val_MinusLogProbMetric: 28.7122

Epoch 376: val_loss did not improve from 28.53391
196/196 - 27s - loss: 27.5897 - MinusLogProbMetric: 27.5897 - val_loss: 28.7122 - val_MinusLogProbMetric: 28.7122 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 377/1000
2023-10-25 16:16:24.827 
Epoch 377/1000 
	 loss: 27.6330, MinusLogProbMetric: 27.6330, val_loss: 28.6714, val_MinusLogProbMetric: 28.6714

Epoch 377: val_loss did not improve from 28.53391
196/196 - 27s - loss: 27.6330 - MinusLogProbMetric: 27.6330 - val_loss: 28.6714 - val_MinusLogProbMetric: 28.6714 - lr: 5.0000e-04 - 27s/epoch - 140ms/step
Epoch 378/1000
2023-10-25 16:16:51.637 
Epoch 378/1000 
	 loss: 27.5959, MinusLogProbMetric: 27.5959, val_loss: 28.8506, val_MinusLogProbMetric: 28.8506

Epoch 378: val_loss did not improve from 28.53391
196/196 - 27s - loss: 27.5959 - MinusLogProbMetric: 27.5959 - val_loss: 28.8506 - val_MinusLogProbMetric: 28.8506 - lr: 5.0000e-04 - 27s/epoch - 137ms/step
Epoch 379/1000
2023-10-25 16:17:19.002 
Epoch 379/1000 
	 loss: 27.5924, MinusLogProbMetric: 27.5924, val_loss: 28.7800, val_MinusLogProbMetric: 28.7800

Epoch 379: val_loss did not improve from 28.53391
196/196 - 27s - loss: 27.5924 - MinusLogProbMetric: 27.5924 - val_loss: 28.7800 - val_MinusLogProbMetric: 28.7800 - lr: 5.0000e-04 - 27s/epoch - 140ms/step
Epoch 380/1000
2023-10-25 16:17:47.011 
Epoch 380/1000 
	 loss: 27.6246, MinusLogProbMetric: 27.6246, val_loss: 28.7099, val_MinusLogProbMetric: 28.7099

Epoch 380: val_loss did not improve from 28.53391
196/196 - 28s - loss: 27.6246 - MinusLogProbMetric: 27.6246 - val_loss: 28.7099 - val_MinusLogProbMetric: 28.7099 - lr: 5.0000e-04 - 28s/epoch - 143ms/step
Epoch 381/1000
2023-10-25 16:18:15.540 
Epoch 381/1000 
	 loss: 27.5566, MinusLogProbMetric: 27.5566, val_loss: 28.7521, val_MinusLogProbMetric: 28.7521

Epoch 381: val_loss did not improve from 28.53391
196/196 - 29s - loss: 27.5566 - MinusLogProbMetric: 27.5566 - val_loss: 28.7521 - val_MinusLogProbMetric: 28.7521 - lr: 5.0000e-04 - 29s/epoch - 146ms/step
Epoch 382/1000
2023-10-25 16:18:47.176 
Epoch 382/1000 
	 loss: 27.6079, MinusLogProbMetric: 27.6079, val_loss: 28.6255, val_MinusLogProbMetric: 28.6255

Epoch 382: val_loss did not improve from 28.53391
196/196 - 32s - loss: 27.6079 - MinusLogProbMetric: 27.6079 - val_loss: 28.6255 - val_MinusLogProbMetric: 28.6255 - lr: 5.0000e-04 - 32s/epoch - 161ms/step
Epoch 383/1000
2023-10-25 16:19:15.656 
Epoch 383/1000 
	 loss: 27.6067, MinusLogProbMetric: 27.6067, val_loss: 28.7078, val_MinusLogProbMetric: 28.7078

Epoch 383: val_loss did not improve from 28.53391
196/196 - 28s - loss: 27.6067 - MinusLogProbMetric: 27.6067 - val_loss: 28.7078 - val_MinusLogProbMetric: 28.7078 - lr: 5.0000e-04 - 28s/epoch - 145ms/step
Epoch 384/1000
2023-10-25 16:19:42.676 
Epoch 384/1000 
	 loss: 27.6043, MinusLogProbMetric: 27.6043, val_loss: 29.1920, val_MinusLogProbMetric: 29.1920

Epoch 384: val_loss did not improve from 28.53391
196/196 - 27s - loss: 27.6043 - MinusLogProbMetric: 27.6043 - val_loss: 29.1920 - val_MinusLogProbMetric: 29.1920 - lr: 5.0000e-04 - 27s/epoch - 138ms/step
Epoch 385/1000
2023-10-25 16:20:09.804 
Epoch 385/1000 
	 loss: 27.3608, MinusLogProbMetric: 27.3608, val_loss: 28.5927, val_MinusLogProbMetric: 28.5927

Epoch 385: val_loss did not improve from 28.53391
196/196 - 27s - loss: 27.3608 - MinusLogProbMetric: 27.3608 - val_loss: 28.5927 - val_MinusLogProbMetric: 28.5927 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 386/1000
2023-10-25 16:20:37.286 
Epoch 386/1000 
	 loss: 27.3418, MinusLogProbMetric: 27.3418, val_loss: 28.5466, val_MinusLogProbMetric: 28.5466

Epoch 386: val_loss did not improve from 28.53391
196/196 - 27s - loss: 27.3418 - MinusLogProbMetric: 27.3418 - val_loss: 28.5466 - val_MinusLogProbMetric: 28.5466 - lr: 2.5000e-04 - 27s/epoch - 140ms/step
Epoch 387/1000
2023-10-25 16:21:05.574 
Epoch 387/1000 
	 loss: 27.3417, MinusLogProbMetric: 27.3417, val_loss: 28.5422, val_MinusLogProbMetric: 28.5422

Epoch 387: val_loss did not improve from 28.53391
196/196 - 28s - loss: 27.3417 - MinusLogProbMetric: 27.3417 - val_loss: 28.5422 - val_MinusLogProbMetric: 28.5422 - lr: 2.5000e-04 - 28s/epoch - 144ms/step
Epoch 388/1000
2023-10-25 16:21:37.162 
Epoch 388/1000 
	 loss: 27.3227, MinusLogProbMetric: 27.3227, val_loss: 28.5988, val_MinusLogProbMetric: 28.5988

Epoch 388: val_loss did not improve from 28.53391
196/196 - 32s - loss: 27.3227 - MinusLogProbMetric: 27.3227 - val_loss: 28.5988 - val_MinusLogProbMetric: 28.5988 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 389/1000
2023-10-25 16:22:06.635 
Epoch 389/1000 
	 loss: 27.3661, MinusLogProbMetric: 27.3661, val_loss: 28.5457, val_MinusLogProbMetric: 28.5457

Epoch 389: val_loss did not improve from 28.53391
196/196 - 29s - loss: 27.3661 - MinusLogProbMetric: 27.3661 - val_loss: 28.5457 - val_MinusLogProbMetric: 28.5457 - lr: 2.5000e-04 - 29s/epoch - 150ms/step
Epoch 390/1000
2023-10-25 16:22:36.309 
Epoch 390/1000 
	 loss: 27.3353, MinusLogProbMetric: 27.3353, val_loss: 28.5274, val_MinusLogProbMetric: 28.5274

Epoch 390: val_loss improved from 28.53391 to 28.52736, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 30s - loss: 27.3353 - MinusLogProbMetric: 27.3353 - val_loss: 28.5274 - val_MinusLogProbMetric: 28.5274 - lr: 2.5000e-04 - 30s/epoch - 154ms/step
Epoch 391/1000
2023-10-25 16:23:03.902 
Epoch 391/1000 
	 loss: 27.3241, MinusLogProbMetric: 27.3241, val_loss: 28.5653, val_MinusLogProbMetric: 28.5653

Epoch 391: val_loss did not improve from 28.52736
196/196 - 27s - loss: 27.3241 - MinusLogProbMetric: 27.3241 - val_loss: 28.5653 - val_MinusLogProbMetric: 28.5653 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 392/1000
2023-10-25 16:23:31.488 
Epoch 392/1000 
	 loss: 27.3353, MinusLogProbMetric: 27.3353, val_loss: 28.5884, val_MinusLogProbMetric: 28.5884

Epoch 392: val_loss did not improve from 28.52736
196/196 - 28s - loss: 27.3353 - MinusLogProbMetric: 27.3353 - val_loss: 28.5884 - val_MinusLogProbMetric: 28.5884 - lr: 2.5000e-04 - 28s/epoch - 141ms/step
Epoch 393/1000
2023-10-25 16:23:58.383 
Epoch 393/1000 
	 loss: 27.3158, MinusLogProbMetric: 27.3158, val_loss: 28.5881, val_MinusLogProbMetric: 28.5881

Epoch 393: val_loss did not improve from 28.52736
196/196 - 27s - loss: 27.3158 - MinusLogProbMetric: 27.3158 - val_loss: 28.5881 - val_MinusLogProbMetric: 28.5881 - lr: 2.5000e-04 - 27s/epoch - 137ms/step
Epoch 394/1000
2023-10-25 16:24:25.843 
Epoch 394/1000 
	 loss: 27.3439, MinusLogProbMetric: 27.3439, val_loss: 28.6827, val_MinusLogProbMetric: 28.6827

Epoch 394: val_loss did not improve from 28.52736
196/196 - 27s - loss: 27.3439 - MinusLogProbMetric: 27.3439 - val_loss: 28.6827 - val_MinusLogProbMetric: 28.6827 - lr: 2.5000e-04 - 27s/epoch - 140ms/step
Epoch 395/1000
2023-10-25 16:24:56.272 
Epoch 395/1000 
	 loss: 27.3328, MinusLogProbMetric: 27.3328, val_loss: 28.5420, val_MinusLogProbMetric: 28.5420

Epoch 395: val_loss did not improve from 28.52736
196/196 - 30s - loss: 27.3328 - MinusLogProbMetric: 27.3328 - val_loss: 28.5420 - val_MinusLogProbMetric: 28.5420 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 396/1000
2023-10-25 16:25:29.139 
Epoch 396/1000 
	 loss: 27.3249, MinusLogProbMetric: 27.3249, val_loss: 28.6042, val_MinusLogProbMetric: 28.6042

Epoch 396: val_loss did not improve from 28.52736
196/196 - 33s - loss: 27.3249 - MinusLogProbMetric: 27.3249 - val_loss: 28.6042 - val_MinusLogProbMetric: 28.6042 - lr: 2.5000e-04 - 33s/epoch - 168ms/step
Epoch 397/1000
2023-10-25 16:26:02.557 
Epoch 397/1000 
	 loss: 27.3229, MinusLogProbMetric: 27.3229, val_loss: 28.5140, val_MinusLogProbMetric: 28.5140

Epoch 397: val_loss improved from 28.52736 to 28.51404, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 34s - loss: 27.3229 - MinusLogProbMetric: 27.3229 - val_loss: 28.5140 - val_MinusLogProbMetric: 28.5140 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 398/1000
2023-10-25 16:26:32.595 
Epoch 398/1000 
	 loss: 27.3336, MinusLogProbMetric: 27.3336, val_loss: 28.5803, val_MinusLogProbMetric: 28.5803

Epoch 398: val_loss did not improve from 28.51404
196/196 - 30s - loss: 27.3336 - MinusLogProbMetric: 27.3336 - val_loss: 28.5803 - val_MinusLogProbMetric: 28.5803 - lr: 2.5000e-04 - 30s/epoch - 151ms/step
Epoch 399/1000
2023-10-25 16:27:02.250 
Epoch 399/1000 
	 loss: 27.3289, MinusLogProbMetric: 27.3289, val_loss: 28.5892, val_MinusLogProbMetric: 28.5892

Epoch 399: val_loss did not improve from 28.51404
196/196 - 30s - loss: 27.3289 - MinusLogProbMetric: 27.3289 - val_loss: 28.5892 - val_MinusLogProbMetric: 28.5892 - lr: 2.5000e-04 - 30s/epoch - 151ms/step
Epoch 400/1000
2023-10-25 16:27:32.357 
Epoch 400/1000 
	 loss: 27.3256, MinusLogProbMetric: 27.3256, val_loss: 28.4816, val_MinusLogProbMetric: 28.4816

Epoch 400: val_loss improved from 28.51404 to 28.48164, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 31s - loss: 27.3256 - MinusLogProbMetric: 27.3256 - val_loss: 28.4816 - val_MinusLogProbMetric: 28.4816 - lr: 2.5000e-04 - 31s/epoch - 156ms/step
Epoch 401/1000
2023-10-25 16:28:05.586 
Epoch 401/1000 
	 loss: 27.3137, MinusLogProbMetric: 27.3137, val_loss: 28.5470, val_MinusLogProbMetric: 28.5470

Epoch 401: val_loss did not improve from 28.48164
196/196 - 33s - loss: 27.3137 - MinusLogProbMetric: 27.3137 - val_loss: 28.5470 - val_MinusLogProbMetric: 28.5470 - lr: 2.5000e-04 - 33s/epoch - 167ms/step
Epoch 402/1000
2023-10-25 16:28:36.375 
Epoch 402/1000 
	 loss: 27.3347, MinusLogProbMetric: 27.3347, val_loss: 28.7121, val_MinusLogProbMetric: 28.7121

Epoch 402: val_loss did not improve from 28.48164
196/196 - 31s - loss: 27.3347 - MinusLogProbMetric: 27.3347 - val_loss: 28.7121 - val_MinusLogProbMetric: 28.7121 - lr: 2.5000e-04 - 31s/epoch - 157ms/step
Epoch 403/1000
2023-10-25 16:29:05.836 
Epoch 403/1000 
	 loss: 27.3337, MinusLogProbMetric: 27.3337, val_loss: 28.6353, val_MinusLogProbMetric: 28.6353

Epoch 403: val_loss did not improve from 28.48164
196/196 - 29s - loss: 27.3337 - MinusLogProbMetric: 27.3337 - val_loss: 28.6353 - val_MinusLogProbMetric: 28.6353 - lr: 2.5000e-04 - 29s/epoch - 150ms/step
Epoch 404/1000
2023-10-25 16:29:35.267 
Epoch 404/1000 
	 loss: 27.3124, MinusLogProbMetric: 27.3124, val_loss: 28.5830, val_MinusLogProbMetric: 28.5830

Epoch 404: val_loss did not improve from 28.48164
196/196 - 29s - loss: 27.3124 - MinusLogProbMetric: 27.3124 - val_loss: 28.5830 - val_MinusLogProbMetric: 28.5830 - lr: 2.5000e-04 - 29s/epoch - 150ms/step
Epoch 405/1000
2023-10-25 16:30:05.688 
Epoch 405/1000 
	 loss: 27.3148, MinusLogProbMetric: 27.3148, val_loss: 28.7203, val_MinusLogProbMetric: 28.7203

Epoch 405: val_loss did not improve from 28.48164
196/196 - 30s - loss: 27.3148 - MinusLogProbMetric: 27.3148 - val_loss: 28.7203 - val_MinusLogProbMetric: 28.7203 - lr: 2.5000e-04 - 30s/epoch - 155ms/step
Epoch 406/1000
2023-10-25 16:30:32.786 
Epoch 406/1000 
	 loss: 27.3253, MinusLogProbMetric: 27.3253, val_loss: 28.5286, val_MinusLogProbMetric: 28.5286

Epoch 406: val_loss did not improve from 28.48164
196/196 - 27s - loss: 27.3253 - MinusLogProbMetric: 27.3253 - val_loss: 28.5286 - val_MinusLogProbMetric: 28.5286 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 407/1000
2023-10-25 16:31:00.333 
Epoch 407/1000 
	 loss: 27.3212, MinusLogProbMetric: 27.3212, val_loss: 28.5552, val_MinusLogProbMetric: 28.5552

Epoch 407: val_loss did not improve from 28.48164
196/196 - 28s - loss: 27.3212 - MinusLogProbMetric: 27.3212 - val_loss: 28.5552 - val_MinusLogProbMetric: 28.5552 - lr: 2.5000e-04 - 28s/epoch - 141ms/step
Epoch 408/1000
2023-10-25 16:31:28.075 
Epoch 408/1000 
	 loss: 27.3218, MinusLogProbMetric: 27.3218, val_loss: 28.6529, val_MinusLogProbMetric: 28.6529

Epoch 408: val_loss did not improve from 28.48164
196/196 - 28s - loss: 27.3218 - MinusLogProbMetric: 27.3218 - val_loss: 28.6529 - val_MinusLogProbMetric: 28.6529 - lr: 2.5000e-04 - 28s/epoch - 142ms/step
Epoch 409/1000
2023-10-25 16:32:00.152 
Epoch 409/1000 
	 loss: 27.3062, MinusLogProbMetric: 27.3062, val_loss: 28.5372, val_MinusLogProbMetric: 28.5372

Epoch 409: val_loss did not improve from 28.48164
196/196 - 32s - loss: 27.3062 - MinusLogProbMetric: 27.3062 - val_loss: 28.5372 - val_MinusLogProbMetric: 28.5372 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 410/1000
2023-10-25 16:32:31.244 
Epoch 410/1000 
	 loss: 27.3166, MinusLogProbMetric: 27.3166, val_loss: 28.6595, val_MinusLogProbMetric: 28.6595

Epoch 410: val_loss did not improve from 28.48164
196/196 - 31s - loss: 27.3166 - MinusLogProbMetric: 27.3166 - val_loss: 28.6595 - val_MinusLogProbMetric: 28.6595 - lr: 2.5000e-04 - 31s/epoch - 159ms/step
Epoch 411/1000
2023-10-25 16:33:03.318 
Epoch 411/1000 
	 loss: 27.3201, MinusLogProbMetric: 27.3201, val_loss: 28.5889, val_MinusLogProbMetric: 28.5889

Epoch 411: val_loss did not improve from 28.48164
196/196 - 32s - loss: 27.3201 - MinusLogProbMetric: 27.3201 - val_loss: 28.5889 - val_MinusLogProbMetric: 28.5889 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 412/1000
2023-10-25 16:33:32.413 
Epoch 412/1000 
	 loss: 27.3342, MinusLogProbMetric: 27.3342, val_loss: 28.4980, val_MinusLogProbMetric: 28.4980

Epoch 412: val_loss did not improve from 28.48164
196/196 - 29s - loss: 27.3342 - MinusLogProbMetric: 27.3342 - val_loss: 28.4980 - val_MinusLogProbMetric: 28.4980 - lr: 2.5000e-04 - 29s/epoch - 148ms/step
Epoch 413/1000
2023-10-25 16:34:00.218 
Epoch 413/1000 
	 loss: 27.3147, MinusLogProbMetric: 27.3147, val_loss: 28.5335, val_MinusLogProbMetric: 28.5335

Epoch 413: val_loss did not improve from 28.48164
196/196 - 28s - loss: 27.3147 - MinusLogProbMetric: 27.3147 - val_loss: 28.5335 - val_MinusLogProbMetric: 28.5335 - lr: 2.5000e-04 - 28s/epoch - 142ms/step
Epoch 414/1000
2023-10-25 16:34:27.854 
Epoch 414/1000 
	 loss: 27.3121, MinusLogProbMetric: 27.3121, val_loss: 28.7391, val_MinusLogProbMetric: 28.7391

Epoch 414: val_loss did not improve from 28.48164
196/196 - 28s - loss: 27.3121 - MinusLogProbMetric: 27.3121 - val_loss: 28.7391 - val_MinusLogProbMetric: 28.7391 - lr: 2.5000e-04 - 28s/epoch - 141ms/step
Epoch 415/1000
2023-10-25 16:34:55.337 
Epoch 415/1000 
	 loss: 27.3326, MinusLogProbMetric: 27.3326, val_loss: 28.6224, val_MinusLogProbMetric: 28.6224

Epoch 415: val_loss did not improve from 28.48164
196/196 - 27s - loss: 27.3326 - MinusLogProbMetric: 27.3326 - val_loss: 28.6224 - val_MinusLogProbMetric: 28.6224 - lr: 2.5000e-04 - 27s/epoch - 140ms/step
Epoch 416/1000
2023-10-25 16:35:22.896 
Epoch 416/1000 
	 loss: 27.3214, MinusLogProbMetric: 27.3214, val_loss: 28.9644, val_MinusLogProbMetric: 28.9644

Epoch 416: val_loss did not improve from 28.48164
196/196 - 28s - loss: 27.3214 - MinusLogProbMetric: 27.3214 - val_loss: 28.9644 - val_MinusLogProbMetric: 28.9644 - lr: 2.5000e-04 - 28s/epoch - 141ms/step
Epoch 417/1000
2023-10-25 16:35:49.961 
Epoch 417/1000 
	 loss: 27.3226, MinusLogProbMetric: 27.3226, val_loss: 28.5839, val_MinusLogProbMetric: 28.5839

Epoch 417: val_loss did not improve from 28.48164
196/196 - 27s - loss: 27.3226 - MinusLogProbMetric: 27.3226 - val_loss: 28.5839 - val_MinusLogProbMetric: 28.5839 - lr: 2.5000e-04 - 27s/epoch - 138ms/step
Epoch 418/1000
2023-10-25 16:36:17.681 
Epoch 418/1000 
	 loss: 27.3090, MinusLogProbMetric: 27.3090, val_loss: 28.5623, val_MinusLogProbMetric: 28.5623

Epoch 418: val_loss did not improve from 28.48164
196/196 - 28s - loss: 27.3090 - MinusLogProbMetric: 27.3090 - val_loss: 28.5623 - val_MinusLogProbMetric: 28.5623 - lr: 2.5000e-04 - 28s/epoch - 141ms/step
Epoch 419/1000
2023-10-25 16:36:45.488 
Epoch 419/1000 
	 loss: 27.3081, MinusLogProbMetric: 27.3081, val_loss: 28.6399, val_MinusLogProbMetric: 28.6399

Epoch 419: val_loss did not improve from 28.48164
196/196 - 28s - loss: 27.3081 - MinusLogProbMetric: 27.3081 - val_loss: 28.6399 - val_MinusLogProbMetric: 28.6399 - lr: 2.5000e-04 - 28s/epoch - 142ms/step
Epoch 420/1000
2023-10-25 16:37:14.077 
Epoch 420/1000 
	 loss: 27.3091, MinusLogProbMetric: 27.3091, val_loss: 28.7127, val_MinusLogProbMetric: 28.7127

Epoch 420: val_loss did not improve from 28.48164
196/196 - 29s - loss: 27.3091 - MinusLogProbMetric: 27.3091 - val_loss: 28.7127 - val_MinusLogProbMetric: 28.7127 - lr: 2.5000e-04 - 29s/epoch - 146ms/step
Epoch 421/1000
2023-10-25 16:37:41.904 
Epoch 421/1000 
	 loss: 27.3041, MinusLogProbMetric: 27.3041, val_loss: 28.5411, val_MinusLogProbMetric: 28.5411

Epoch 421: val_loss did not improve from 28.48164
196/196 - 28s - loss: 27.3041 - MinusLogProbMetric: 27.3041 - val_loss: 28.5411 - val_MinusLogProbMetric: 28.5411 - lr: 2.5000e-04 - 28s/epoch - 142ms/step
Epoch 422/1000
2023-10-25 16:38:09.457 
Epoch 422/1000 
	 loss: 27.3192, MinusLogProbMetric: 27.3192, val_loss: 28.5216, val_MinusLogProbMetric: 28.5216

Epoch 422: val_loss did not improve from 28.48164
196/196 - 28s - loss: 27.3192 - MinusLogProbMetric: 27.3192 - val_loss: 28.5216 - val_MinusLogProbMetric: 28.5216 - lr: 2.5000e-04 - 28s/epoch - 141ms/step
Epoch 423/1000
2023-10-25 16:38:36.893 
Epoch 423/1000 
	 loss: 27.2988, MinusLogProbMetric: 27.2988, val_loss: 28.5913, val_MinusLogProbMetric: 28.5913

Epoch 423: val_loss did not improve from 28.48164
196/196 - 27s - loss: 27.2988 - MinusLogProbMetric: 27.2988 - val_loss: 28.5913 - val_MinusLogProbMetric: 28.5913 - lr: 2.5000e-04 - 27s/epoch - 140ms/step
Epoch 424/1000
2023-10-25 16:39:04.501 
Epoch 424/1000 
	 loss: 27.3135, MinusLogProbMetric: 27.3135, val_loss: 28.5359, val_MinusLogProbMetric: 28.5359

Epoch 424: val_loss did not improve from 28.48164
196/196 - 28s - loss: 27.3135 - MinusLogProbMetric: 27.3135 - val_loss: 28.5359 - val_MinusLogProbMetric: 28.5359 - lr: 2.5000e-04 - 28s/epoch - 141ms/step
Epoch 425/1000
2023-10-25 16:39:32.132 
Epoch 425/1000 
	 loss: 27.2992, MinusLogProbMetric: 27.2992, val_loss: 28.5604, val_MinusLogProbMetric: 28.5604

Epoch 425: val_loss did not improve from 28.48164
196/196 - 28s - loss: 27.2992 - MinusLogProbMetric: 27.2992 - val_loss: 28.5604 - val_MinusLogProbMetric: 28.5604 - lr: 2.5000e-04 - 28s/epoch - 141ms/step
Epoch 426/1000
2023-10-25 16:40:00.256 
Epoch 426/1000 
	 loss: 27.2986, MinusLogProbMetric: 27.2986, val_loss: 28.6462, val_MinusLogProbMetric: 28.6462

Epoch 426: val_loss did not improve from 28.48164
196/196 - 28s - loss: 27.2986 - MinusLogProbMetric: 27.2986 - val_loss: 28.6462 - val_MinusLogProbMetric: 28.6462 - lr: 2.5000e-04 - 28s/epoch - 143ms/step
Epoch 427/1000
2023-10-25 16:40:30.889 
Epoch 427/1000 
	 loss: 27.3063, MinusLogProbMetric: 27.3063, val_loss: 28.5937, val_MinusLogProbMetric: 28.5937

Epoch 427: val_loss did not improve from 28.48164
196/196 - 31s - loss: 27.3063 - MinusLogProbMetric: 27.3063 - val_loss: 28.5937 - val_MinusLogProbMetric: 28.5937 - lr: 2.5000e-04 - 31s/epoch - 156ms/step
Epoch 428/1000
2023-10-25 16:41:03.496 
Epoch 428/1000 
	 loss: 27.2981, MinusLogProbMetric: 27.2981, val_loss: 28.5629, val_MinusLogProbMetric: 28.5629

Epoch 428: val_loss did not improve from 28.48164
196/196 - 33s - loss: 27.2981 - MinusLogProbMetric: 27.2981 - val_loss: 28.5629 - val_MinusLogProbMetric: 28.5629 - lr: 2.5000e-04 - 33s/epoch - 166ms/step
Epoch 429/1000
2023-10-25 16:41:33.566 
Epoch 429/1000 
	 loss: 27.3199, MinusLogProbMetric: 27.3199, val_loss: 28.5547, val_MinusLogProbMetric: 28.5547

Epoch 429: val_loss did not improve from 28.48164
196/196 - 30s - loss: 27.3199 - MinusLogProbMetric: 27.3199 - val_loss: 28.5547 - val_MinusLogProbMetric: 28.5547 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 430/1000
2023-10-25 16:42:00.889 
Epoch 430/1000 
	 loss: 27.3010, MinusLogProbMetric: 27.3010, val_loss: 28.5993, val_MinusLogProbMetric: 28.5993

Epoch 430: val_loss did not improve from 28.48164
196/196 - 27s - loss: 27.3010 - MinusLogProbMetric: 27.3010 - val_loss: 28.5993 - val_MinusLogProbMetric: 28.5993 - lr: 2.5000e-04 - 27s/epoch - 139ms/step
Epoch 431/1000
2023-10-25 16:42:28.726 
Epoch 431/1000 
	 loss: 27.3071, MinusLogProbMetric: 27.3071, val_loss: 28.7745, val_MinusLogProbMetric: 28.7745

Epoch 431: val_loss did not improve from 28.48164
196/196 - 28s - loss: 27.3071 - MinusLogProbMetric: 27.3071 - val_loss: 28.7745 - val_MinusLogProbMetric: 28.7745 - lr: 2.5000e-04 - 28s/epoch - 142ms/step
Epoch 432/1000
2023-10-25 16:42:56.537 
Epoch 432/1000 
	 loss: 27.3082, MinusLogProbMetric: 27.3082, val_loss: 28.6799, val_MinusLogProbMetric: 28.6799

Epoch 432: val_loss did not improve from 28.48164
196/196 - 28s - loss: 27.3082 - MinusLogProbMetric: 27.3082 - val_loss: 28.6799 - val_MinusLogProbMetric: 28.6799 - lr: 2.5000e-04 - 28s/epoch - 142ms/step
Epoch 433/1000
2023-10-25 16:43:26.509 
Epoch 433/1000 
	 loss: 27.3050, MinusLogProbMetric: 27.3050, val_loss: 28.5678, val_MinusLogProbMetric: 28.5678

Epoch 433: val_loss did not improve from 28.48164
196/196 - 30s - loss: 27.3050 - MinusLogProbMetric: 27.3050 - val_loss: 28.5678 - val_MinusLogProbMetric: 28.5678 - lr: 2.5000e-04 - 30s/epoch - 153ms/step
Epoch 434/1000
2023-10-25 16:44:00.035 
Epoch 434/1000 
	 loss: 27.2981, MinusLogProbMetric: 27.2981, val_loss: 28.5901, val_MinusLogProbMetric: 28.5901

Epoch 434: val_loss did not improve from 28.48164
196/196 - 34s - loss: 27.2981 - MinusLogProbMetric: 27.2981 - val_loss: 28.5901 - val_MinusLogProbMetric: 28.5901 - lr: 2.5000e-04 - 34s/epoch - 171ms/step
Epoch 435/1000
2023-10-25 16:44:34.807 
Epoch 435/1000 
	 loss: 27.3011, MinusLogProbMetric: 27.3011, val_loss: 28.5775, val_MinusLogProbMetric: 28.5775

Epoch 435: val_loss did not improve from 28.48164
196/196 - 35s - loss: 27.3011 - MinusLogProbMetric: 27.3011 - val_loss: 28.5775 - val_MinusLogProbMetric: 28.5775 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 436/1000
2023-10-25 16:45:09.714 
Epoch 436/1000 
	 loss: 27.2930, MinusLogProbMetric: 27.2930, val_loss: 28.5717, val_MinusLogProbMetric: 28.5717

Epoch 436: val_loss did not improve from 28.48164
196/196 - 35s - loss: 27.2930 - MinusLogProbMetric: 27.2930 - val_loss: 28.5717 - val_MinusLogProbMetric: 28.5717 - lr: 2.5000e-04 - 35s/epoch - 178ms/step
Epoch 437/1000
2023-10-25 16:45:43.025 
Epoch 437/1000 
	 loss: 27.3162, MinusLogProbMetric: 27.3162, val_loss: 28.6461, val_MinusLogProbMetric: 28.6461

Epoch 437: val_loss did not improve from 28.48164
196/196 - 33s - loss: 27.3162 - MinusLogProbMetric: 27.3162 - val_loss: 28.6461 - val_MinusLogProbMetric: 28.6461 - lr: 2.5000e-04 - 33s/epoch - 170ms/step
Epoch 438/1000
2023-10-25 16:46:18.076 
Epoch 438/1000 
	 loss: 27.2843, MinusLogProbMetric: 27.2843, val_loss: 28.5809, val_MinusLogProbMetric: 28.5809

Epoch 438: val_loss did not improve from 28.48164
196/196 - 35s - loss: 27.2843 - MinusLogProbMetric: 27.2843 - val_loss: 28.5809 - val_MinusLogProbMetric: 28.5809 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 439/1000
2023-10-25 16:46:53.317 
Epoch 439/1000 
	 loss: 27.3007, MinusLogProbMetric: 27.3007, val_loss: 28.5681, val_MinusLogProbMetric: 28.5681

Epoch 439: val_loss did not improve from 28.48164
196/196 - 35s - loss: 27.3007 - MinusLogProbMetric: 27.3007 - val_loss: 28.5681 - val_MinusLogProbMetric: 28.5681 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 440/1000
2023-10-25 16:47:28.430 
Epoch 440/1000 
	 loss: 27.3147, MinusLogProbMetric: 27.3147, val_loss: 28.5667, val_MinusLogProbMetric: 28.5667

Epoch 440: val_loss did not improve from 28.48164
196/196 - 35s - loss: 27.3147 - MinusLogProbMetric: 27.3147 - val_loss: 28.5667 - val_MinusLogProbMetric: 28.5667 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 441/1000
2023-10-25 16:48:03.612 
Epoch 441/1000 
	 loss: 27.2785, MinusLogProbMetric: 27.2785, val_loss: 28.6085, val_MinusLogProbMetric: 28.6085

Epoch 441: val_loss did not improve from 28.48164
196/196 - 35s - loss: 27.2785 - MinusLogProbMetric: 27.2785 - val_loss: 28.6085 - val_MinusLogProbMetric: 28.6085 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 442/1000
2023-10-25 16:48:38.940 
Epoch 442/1000 
	 loss: 27.3025, MinusLogProbMetric: 27.3025, val_loss: 28.5610, val_MinusLogProbMetric: 28.5610

Epoch 442: val_loss did not improve from 28.48164
196/196 - 35s - loss: 27.3025 - MinusLogProbMetric: 27.3025 - val_loss: 28.5610 - val_MinusLogProbMetric: 28.5610 - lr: 2.5000e-04 - 35s/epoch - 180ms/step
Epoch 443/1000
2023-10-25 16:49:13.938 
Epoch 443/1000 
	 loss: 27.3013, MinusLogProbMetric: 27.3013, val_loss: 28.6583, val_MinusLogProbMetric: 28.6583

Epoch 443: val_loss did not improve from 28.48164
196/196 - 35s - loss: 27.3013 - MinusLogProbMetric: 27.3013 - val_loss: 28.6583 - val_MinusLogProbMetric: 28.6583 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 444/1000
2023-10-25 16:49:49.028 
Epoch 444/1000 
	 loss: 27.3011, MinusLogProbMetric: 27.3011, val_loss: 28.6326, val_MinusLogProbMetric: 28.6326

Epoch 444: val_loss did not improve from 28.48164
196/196 - 35s - loss: 27.3011 - MinusLogProbMetric: 27.3011 - val_loss: 28.6326 - val_MinusLogProbMetric: 28.6326 - lr: 2.5000e-04 - 35s/epoch - 179ms/step
Epoch 445/1000
2023-10-25 16:50:22.620 
Epoch 445/1000 
	 loss: 27.3033, MinusLogProbMetric: 27.3033, val_loss: 28.7004, val_MinusLogProbMetric: 28.7004

Epoch 445: val_loss did not improve from 28.48164
196/196 - 34s - loss: 27.3033 - MinusLogProbMetric: 27.3033 - val_loss: 28.7004 - val_MinusLogProbMetric: 28.7004 - lr: 2.5000e-04 - 34s/epoch - 171ms/step
Epoch 446/1000
2023-10-25 16:50:54.328 
Epoch 446/1000 
	 loss: 27.3164, MinusLogProbMetric: 27.3164, val_loss: 28.5329, val_MinusLogProbMetric: 28.5329

Epoch 446: val_loss did not improve from 28.48164
196/196 - 32s - loss: 27.3164 - MinusLogProbMetric: 27.3164 - val_loss: 28.5329 - val_MinusLogProbMetric: 28.5329 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 447/1000
2023-10-25 16:51:28.334 
Epoch 447/1000 
	 loss: 27.2875, MinusLogProbMetric: 27.2875, val_loss: 28.5784, val_MinusLogProbMetric: 28.5784

Epoch 447: val_loss did not improve from 28.48164
196/196 - 34s - loss: 27.2875 - MinusLogProbMetric: 27.2875 - val_loss: 28.5784 - val_MinusLogProbMetric: 28.5784 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 448/1000
2023-10-25 16:51:58.022 
Epoch 448/1000 
	 loss: 27.2864, MinusLogProbMetric: 27.2864, val_loss: 28.6893, val_MinusLogProbMetric: 28.6893

Epoch 448: val_loss did not improve from 28.48164
196/196 - 30s - loss: 27.2864 - MinusLogProbMetric: 27.2864 - val_loss: 28.6893 - val_MinusLogProbMetric: 28.6893 - lr: 2.5000e-04 - 30s/epoch - 151ms/step
Epoch 449/1000
2023-10-25 16:52:27.817 
Epoch 449/1000 
	 loss: 27.2771, MinusLogProbMetric: 27.2771, val_loss: 28.5337, val_MinusLogProbMetric: 28.5337

Epoch 449: val_loss did not improve from 28.48164
196/196 - 30s - loss: 27.2771 - MinusLogProbMetric: 27.2771 - val_loss: 28.5337 - val_MinusLogProbMetric: 28.5337 - lr: 2.5000e-04 - 30s/epoch - 152ms/step
Epoch 450/1000
2023-10-25 16:53:01.425 
Epoch 450/1000 
	 loss: 27.3263, MinusLogProbMetric: 27.3263, val_loss: 28.6125, val_MinusLogProbMetric: 28.6125

Epoch 450: val_loss did not improve from 28.48164
196/196 - 34s - loss: 27.3263 - MinusLogProbMetric: 27.3263 - val_loss: 28.6125 - val_MinusLogProbMetric: 28.6125 - lr: 2.5000e-04 - 34s/epoch - 171ms/step
Epoch 451/1000
2023-10-25 16:53:36.373 
Epoch 451/1000 
	 loss: 27.1936, MinusLogProbMetric: 27.1936, val_loss: 28.5046, val_MinusLogProbMetric: 28.5046

Epoch 451: val_loss did not improve from 28.48164
196/196 - 35s - loss: 27.1936 - MinusLogProbMetric: 27.1936 - val_loss: 28.5046 - val_MinusLogProbMetric: 28.5046 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 452/1000
2023-10-25 16:54:11.556 
Epoch 452/1000 
	 loss: 27.1902, MinusLogProbMetric: 27.1902, val_loss: 28.4950, val_MinusLogProbMetric: 28.4950

Epoch 452: val_loss did not improve from 28.48164
196/196 - 35s - loss: 27.1902 - MinusLogProbMetric: 27.1902 - val_loss: 28.4950 - val_MinusLogProbMetric: 28.4950 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 453/1000
2023-10-25 16:54:46.514 
Epoch 453/1000 
	 loss: 27.2019, MinusLogProbMetric: 27.2019, val_loss: 28.5235, val_MinusLogProbMetric: 28.5235

Epoch 453: val_loss did not improve from 28.48164
196/196 - 35s - loss: 27.2019 - MinusLogProbMetric: 27.2019 - val_loss: 28.5235 - val_MinusLogProbMetric: 28.5235 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 454/1000
2023-10-25 16:55:20.867 
Epoch 454/1000 
	 loss: 27.1902, MinusLogProbMetric: 27.1902, val_loss: 28.4916, val_MinusLogProbMetric: 28.4916

Epoch 454: val_loss did not improve from 28.48164
196/196 - 34s - loss: 27.1902 - MinusLogProbMetric: 27.1902 - val_loss: 28.4916 - val_MinusLogProbMetric: 28.4916 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 455/1000
2023-10-25 16:55:54.666 
Epoch 455/1000 
	 loss: 27.1922, MinusLogProbMetric: 27.1922, val_loss: 28.5301, val_MinusLogProbMetric: 28.5301

Epoch 455: val_loss did not improve from 28.48164
196/196 - 34s - loss: 27.1922 - MinusLogProbMetric: 27.1922 - val_loss: 28.5301 - val_MinusLogProbMetric: 28.5301 - lr: 1.2500e-04 - 34s/epoch - 172ms/step
Epoch 456/1000
2023-10-25 16:56:25.263 
Epoch 456/1000 
	 loss: 27.1916, MinusLogProbMetric: 27.1916, val_loss: 28.5856, val_MinusLogProbMetric: 28.5856

Epoch 456: val_loss did not improve from 28.48164
196/196 - 31s - loss: 27.1916 - MinusLogProbMetric: 27.1916 - val_loss: 28.5856 - val_MinusLogProbMetric: 28.5856 - lr: 1.2500e-04 - 31s/epoch - 156ms/step
Epoch 457/1000
2023-10-25 16:56:57.826 
Epoch 457/1000 
	 loss: 27.1992, MinusLogProbMetric: 27.1992, val_loss: 28.4810, val_MinusLogProbMetric: 28.4810

Epoch 457: val_loss improved from 28.48164 to 28.48097, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_354/weights/best_weights.h5
196/196 - 33s - loss: 27.1992 - MinusLogProbMetric: 27.1992 - val_loss: 28.4810 - val_MinusLogProbMetric: 28.4810 - lr: 1.2500e-04 - 33s/epoch - 169ms/step
Epoch 458/1000
2023-10-25 16:57:31.309 
Epoch 458/1000 
	 loss: 27.1815, MinusLogProbMetric: 27.1815, val_loss: 28.5186, val_MinusLogProbMetric: 28.5186

Epoch 458: val_loss did not improve from 28.48097
196/196 - 33s - loss: 27.1815 - MinusLogProbMetric: 27.1815 - val_loss: 28.5186 - val_MinusLogProbMetric: 28.5186 - lr: 1.2500e-04 - 33s/epoch - 168ms/step
Epoch 459/1000
2023-10-25 16:58:03.962 
Epoch 459/1000 
	 loss: 27.1786, MinusLogProbMetric: 27.1786, val_loss: 28.5447, val_MinusLogProbMetric: 28.5447

Epoch 459: val_loss did not improve from 28.48097
196/196 - 33s - loss: 27.1786 - MinusLogProbMetric: 27.1786 - val_loss: 28.5447 - val_MinusLogProbMetric: 28.5447 - lr: 1.2500e-04 - 33s/epoch - 167ms/step
Epoch 460/1000
2023-10-25 16:58:38.527 
Epoch 460/1000 
	 loss: 27.1865, MinusLogProbMetric: 27.1865, val_loss: 28.5023, val_MinusLogProbMetric: 28.5023

Epoch 460: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1865 - MinusLogProbMetric: 27.1865 - val_loss: 28.5023 - val_MinusLogProbMetric: 28.5023 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 461/1000
2023-10-25 16:59:13.737 
Epoch 461/1000 
	 loss: 27.1913, MinusLogProbMetric: 27.1913, val_loss: 28.5602, val_MinusLogProbMetric: 28.5602

Epoch 461: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1913 - MinusLogProbMetric: 27.1913 - val_loss: 28.5602 - val_MinusLogProbMetric: 28.5602 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 462/1000
2023-10-25 16:59:48.746 
Epoch 462/1000 
	 loss: 27.1907, MinusLogProbMetric: 27.1907, val_loss: 28.5269, val_MinusLogProbMetric: 28.5269

Epoch 462: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1907 - MinusLogProbMetric: 27.1907 - val_loss: 28.5269 - val_MinusLogProbMetric: 28.5269 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 463/1000
2023-10-25 17:00:23.820 
Epoch 463/1000 
	 loss: 27.1905, MinusLogProbMetric: 27.1905, val_loss: 28.5255, val_MinusLogProbMetric: 28.5255

Epoch 463: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1905 - MinusLogProbMetric: 27.1905 - val_loss: 28.5255 - val_MinusLogProbMetric: 28.5255 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 464/1000
2023-10-25 17:00:58.806 
Epoch 464/1000 
	 loss: 27.1897, MinusLogProbMetric: 27.1897, val_loss: 28.5761, val_MinusLogProbMetric: 28.5761

Epoch 464: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1897 - MinusLogProbMetric: 27.1897 - val_loss: 28.5761 - val_MinusLogProbMetric: 28.5761 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 465/1000
2023-10-25 17:01:33.644 
Epoch 465/1000 
	 loss: 27.1868, MinusLogProbMetric: 27.1868, val_loss: 28.6062, val_MinusLogProbMetric: 28.6062

Epoch 465: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1868 - MinusLogProbMetric: 27.1868 - val_loss: 28.6062 - val_MinusLogProbMetric: 28.6062 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 466/1000
2023-10-25 17:02:08.453 
Epoch 466/1000 
	 loss: 27.1871, MinusLogProbMetric: 27.1871, val_loss: 28.6285, val_MinusLogProbMetric: 28.6285

Epoch 466: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1871 - MinusLogProbMetric: 27.1871 - val_loss: 28.6285 - val_MinusLogProbMetric: 28.6285 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 467/1000
2023-10-25 17:02:43.030 
Epoch 467/1000 
	 loss: 27.1883, MinusLogProbMetric: 27.1883, val_loss: 28.5172, val_MinusLogProbMetric: 28.5172

Epoch 467: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1883 - MinusLogProbMetric: 27.1883 - val_loss: 28.5172 - val_MinusLogProbMetric: 28.5172 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 468/1000
2023-10-25 17:03:17.911 
Epoch 468/1000 
	 loss: 27.1903, MinusLogProbMetric: 27.1903, val_loss: 28.5008, val_MinusLogProbMetric: 28.5008

Epoch 468: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1903 - MinusLogProbMetric: 27.1903 - val_loss: 28.5008 - val_MinusLogProbMetric: 28.5008 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 469/1000
2023-10-25 17:03:49.967 
Epoch 469/1000 
	 loss: 27.1917, MinusLogProbMetric: 27.1917, val_loss: 28.4814, val_MinusLogProbMetric: 28.4814

Epoch 469: val_loss did not improve from 28.48097
196/196 - 32s - loss: 27.1917 - MinusLogProbMetric: 27.1917 - val_loss: 28.4814 - val_MinusLogProbMetric: 28.4814 - lr: 1.2500e-04 - 32s/epoch - 164ms/step
Epoch 470/1000
2023-10-25 17:04:24.295 
Epoch 470/1000 
	 loss: 27.1794, MinusLogProbMetric: 27.1794, val_loss: 28.5293, val_MinusLogProbMetric: 28.5293

Epoch 470: val_loss did not improve from 28.48097
196/196 - 34s - loss: 27.1794 - MinusLogProbMetric: 27.1794 - val_loss: 28.5293 - val_MinusLogProbMetric: 28.5293 - lr: 1.2500e-04 - 34s/epoch - 175ms/step
Epoch 471/1000
2023-10-25 17:04:56.374 
Epoch 471/1000 
	 loss: 27.1861, MinusLogProbMetric: 27.1861, val_loss: 28.5485, val_MinusLogProbMetric: 28.5485

Epoch 471: val_loss did not improve from 28.48097
196/196 - 32s - loss: 27.1861 - MinusLogProbMetric: 27.1861 - val_loss: 28.5485 - val_MinusLogProbMetric: 28.5485 - lr: 1.2500e-04 - 32s/epoch - 164ms/step
Epoch 472/1000
2023-10-25 17:05:31.008 
Epoch 472/1000 
	 loss: 27.1936, MinusLogProbMetric: 27.1936, val_loss: 28.5354, val_MinusLogProbMetric: 28.5354

Epoch 472: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1936 - MinusLogProbMetric: 27.1936 - val_loss: 28.5354 - val_MinusLogProbMetric: 28.5354 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 473/1000
2023-10-25 17:06:05.687 
Epoch 473/1000 
	 loss: 27.1834, MinusLogProbMetric: 27.1834, val_loss: 28.5049, val_MinusLogProbMetric: 28.5049

Epoch 473: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1834 - MinusLogProbMetric: 27.1834 - val_loss: 28.5049 - val_MinusLogProbMetric: 28.5049 - lr: 1.2500e-04 - 35s/epoch - 177ms/step
Epoch 474/1000
2023-10-25 17:06:40.722 
Epoch 474/1000 
	 loss: 27.1813, MinusLogProbMetric: 27.1813, val_loss: 28.5156, val_MinusLogProbMetric: 28.5156

Epoch 474: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1813 - MinusLogProbMetric: 27.1813 - val_loss: 28.5156 - val_MinusLogProbMetric: 28.5156 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 475/1000
2023-10-25 17:07:15.730 
Epoch 475/1000 
	 loss: 27.1740, MinusLogProbMetric: 27.1740, val_loss: 28.5395, val_MinusLogProbMetric: 28.5395

Epoch 475: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1740 - MinusLogProbMetric: 27.1740 - val_loss: 28.5395 - val_MinusLogProbMetric: 28.5395 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 476/1000
2023-10-25 17:07:50.318 
Epoch 476/1000 
	 loss: 27.1840, MinusLogProbMetric: 27.1840, val_loss: 28.5121, val_MinusLogProbMetric: 28.5121

Epoch 476: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1840 - MinusLogProbMetric: 27.1840 - val_loss: 28.5121 - val_MinusLogProbMetric: 28.5121 - lr: 1.2500e-04 - 35s/epoch - 176ms/step
Epoch 477/1000
2023-10-25 17:08:25.178 
Epoch 477/1000 
	 loss: 27.1811, MinusLogProbMetric: 27.1811, val_loss: 28.5736, val_MinusLogProbMetric: 28.5736

Epoch 477: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1811 - MinusLogProbMetric: 27.1811 - val_loss: 28.5736 - val_MinusLogProbMetric: 28.5736 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 478/1000
2023-10-25 17:08:58.168 
Epoch 478/1000 
	 loss: 27.1951, MinusLogProbMetric: 27.1951, val_loss: 28.5601, val_MinusLogProbMetric: 28.5601

Epoch 478: val_loss did not improve from 28.48097
196/196 - 33s - loss: 27.1951 - MinusLogProbMetric: 27.1951 - val_loss: 28.5601 - val_MinusLogProbMetric: 28.5601 - lr: 1.2500e-04 - 33s/epoch - 168ms/step
Epoch 479/1000
2023-10-25 17:09:31.679 
Epoch 479/1000 
	 loss: 27.1909, MinusLogProbMetric: 27.1909, val_loss: 28.5138, val_MinusLogProbMetric: 28.5138

Epoch 479: val_loss did not improve from 28.48097
196/196 - 34s - loss: 27.1909 - MinusLogProbMetric: 27.1909 - val_loss: 28.5138 - val_MinusLogProbMetric: 28.5138 - lr: 1.2500e-04 - 34s/epoch - 171ms/step
Epoch 480/1000
2023-10-25 17:10:06.780 
Epoch 480/1000 
	 loss: 27.1901, MinusLogProbMetric: 27.1901, val_loss: 28.5944, val_MinusLogProbMetric: 28.5944

Epoch 480: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1901 - MinusLogProbMetric: 27.1901 - val_loss: 28.5944 - val_MinusLogProbMetric: 28.5944 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 481/1000
2023-10-25 17:10:38.125 
Epoch 481/1000 
	 loss: 27.1804, MinusLogProbMetric: 27.1804, val_loss: 28.6095, val_MinusLogProbMetric: 28.6095

Epoch 481: val_loss did not improve from 28.48097
196/196 - 31s - loss: 27.1804 - MinusLogProbMetric: 27.1804 - val_loss: 28.6095 - val_MinusLogProbMetric: 28.6095 - lr: 1.2500e-04 - 31s/epoch - 160ms/step
Epoch 482/1000
2023-10-25 17:11:07.675 
Epoch 482/1000 
	 loss: 27.1870, MinusLogProbMetric: 27.1870, val_loss: 28.5336, val_MinusLogProbMetric: 28.5336

Epoch 482: val_loss did not improve from 28.48097
196/196 - 30s - loss: 27.1870 - MinusLogProbMetric: 27.1870 - val_loss: 28.5336 - val_MinusLogProbMetric: 28.5336 - lr: 1.2500e-04 - 30s/epoch - 151ms/step
Epoch 483/1000
2023-10-25 17:11:42.895 
Epoch 483/1000 
	 loss: 27.1908, MinusLogProbMetric: 27.1908, val_loss: 28.5088, val_MinusLogProbMetric: 28.5088

Epoch 483: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1908 - MinusLogProbMetric: 27.1908 - val_loss: 28.5088 - val_MinusLogProbMetric: 28.5088 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 484/1000
2023-10-25 17:12:17.973 
Epoch 484/1000 
	 loss: 27.1759, MinusLogProbMetric: 27.1759, val_loss: 28.4994, val_MinusLogProbMetric: 28.4994

Epoch 484: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1759 - MinusLogProbMetric: 27.1759 - val_loss: 28.4994 - val_MinusLogProbMetric: 28.4994 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 485/1000
2023-10-25 17:12:53.224 
Epoch 485/1000 
	 loss: 27.1797, MinusLogProbMetric: 27.1797, val_loss: 28.5156, val_MinusLogProbMetric: 28.5156

Epoch 485: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1797 - MinusLogProbMetric: 27.1797 - val_loss: 28.5156 - val_MinusLogProbMetric: 28.5156 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 486/1000
2023-10-25 17:13:28.407 
Epoch 486/1000 
	 loss: 27.1788, MinusLogProbMetric: 27.1788, val_loss: 28.5922, val_MinusLogProbMetric: 28.5922

Epoch 486: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1788 - MinusLogProbMetric: 27.1788 - val_loss: 28.5922 - val_MinusLogProbMetric: 28.5922 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 487/1000
2023-10-25 17:14:03.472 
Epoch 487/1000 
	 loss: 27.1894, MinusLogProbMetric: 27.1894, val_loss: 28.5450, val_MinusLogProbMetric: 28.5450

Epoch 487: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1894 - MinusLogProbMetric: 27.1894 - val_loss: 28.5450 - val_MinusLogProbMetric: 28.5450 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 488/1000
2023-10-25 17:14:38.760 
Epoch 488/1000 
	 loss: 27.1866, MinusLogProbMetric: 27.1866, val_loss: 28.5343, val_MinusLogProbMetric: 28.5343

Epoch 488: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1866 - MinusLogProbMetric: 27.1866 - val_loss: 28.5343 - val_MinusLogProbMetric: 28.5343 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 489/1000
2023-10-25 17:15:14.178 
Epoch 489/1000 
	 loss: 27.1830, MinusLogProbMetric: 27.1830, val_loss: 28.5955, val_MinusLogProbMetric: 28.5955

Epoch 489: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1830 - MinusLogProbMetric: 27.1830 - val_loss: 28.5955 - val_MinusLogProbMetric: 28.5955 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 490/1000
2023-10-25 17:15:49.195 
Epoch 490/1000 
	 loss: 27.1713, MinusLogProbMetric: 27.1713, val_loss: 28.5503, val_MinusLogProbMetric: 28.5503

Epoch 490: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1713 - MinusLogProbMetric: 27.1713 - val_loss: 28.5503 - val_MinusLogProbMetric: 28.5503 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 491/1000
2023-10-25 17:16:24.415 
Epoch 491/1000 
	 loss: 27.1880, MinusLogProbMetric: 27.1880, val_loss: 28.5130, val_MinusLogProbMetric: 28.5130

Epoch 491: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1880 - MinusLogProbMetric: 27.1880 - val_loss: 28.5130 - val_MinusLogProbMetric: 28.5130 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 492/1000
2023-10-25 17:16:59.341 
Epoch 492/1000 
	 loss: 27.1762, MinusLogProbMetric: 27.1762, val_loss: 28.5825, val_MinusLogProbMetric: 28.5825

Epoch 492: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1762 - MinusLogProbMetric: 27.1762 - val_loss: 28.5825 - val_MinusLogProbMetric: 28.5825 - lr: 1.2500e-04 - 35s/epoch - 178ms/step
Epoch 493/1000
2023-10-25 17:17:34.348 
Epoch 493/1000 
	 loss: 27.1866, MinusLogProbMetric: 27.1866, val_loss: 28.5579, val_MinusLogProbMetric: 28.5579

Epoch 493: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1866 - MinusLogProbMetric: 27.1866 - val_loss: 28.5579 - val_MinusLogProbMetric: 28.5579 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 494/1000
2023-10-25 17:18:09.714 
Epoch 494/1000 
	 loss: 27.1774, MinusLogProbMetric: 27.1774, val_loss: 28.6512, val_MinusLogProbMetric: 28.6512

Epoch 494: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1774 - MinusLogProbMetric: 27.1774 - val_loss: 28.6512 - val_MinusLogProbMetric: 28.6512 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 495/1000
2023-10-25 17:18:45.274 
Epoch 495/1000 
	 loss: 27.1877, MinusLogProbMetric: 27.1877, val_loss: 28.5092, val_MinusLogProbMetric: 28.5092

Epoch 495: val_loss did not improve from 28.48097
196/196 - 36s - loss: 27.1877 - MinusLogProbMetric: 27.1877 - val_loss: 28.5092 - val_MinusLogProbMetric: 28.5092 - lr: 1.2500e-04 - 36s/epoch - 181ms/step
Epoch 496/1000
2023-10-25 17:19:20.577 
Epoch 496/1000 
	 loss: 27.1757, MinusLogProbMetric: 27.1757, val_loss: 28.5689, val_MinusLogProbMetric: 28.5689

Epoch 496: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1757 - MinusLogProbMetric: 27.1757 - val_loss: 28.5689 - val_MinusLogProbMetric: 28.5689 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 497/1000
2023-10-25 17:19:55.855 
Epoch 497/1000 
	 loss: 27.1752, MinusLogProbMetric: 27.1752, val_loss: 28.5216, val_MinusLogProbMetric: 28.5216

Epoch 497: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1752 - MinusLogProbMetric: 27.1752 - val_loss: 28.5216 - val_MinusLogProbMetric: 28.5216 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 498/1000
2023-10-25 17:20:31.236 
Epoch 498/1000 
	 loss: 27.1838, MinusLogProbMetric: 27.1838, val_loss: 28.5258, val_MinusLogProbMetric: 28.5258

Epoch 498: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1838 - MinusLogProbMetric: 27.1838 - val_loss: 28.5258 - val_MinusLogProbMetric: 28.5258 - lr: 1.2500e-04 - 35s/epoch - 181ms/step
Epoch 499/1000
2023-10-25 17:21:06.336 
Epoch 499/1000 
	 loss: 27.1752, MinusLogProbMetric: 27.1752, val_loss: 28.5973, val_MinusLogProbMetric: 28.5973

Epoch 499: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1752 - MinusLogProbMetric: 27.1752 - val_loss: 28.5973 - val_MinusLogProbMetric: 28.5973 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 500/1000
2023-10-25 17:21:41.569 
Epoch 500/1000 
	 loss: 27.1741, MinusLogProbMetric: 27.1741, val_loss: 28.5271, val_MinusLogProbMetric: 28.5271

Epoch 500: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1741 - MinusLogProbMetric: 27.1741 - val_loss: 28.5271 - val_MinusLogProbMetric: 28.5271 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 501/1000
2023-10-25 17:22:16.809 
Epoch 501/1000 
	 loss: 27.1757, MinusLogProbMetric: 27.1757, val_loss: 28.5616, val_MinusLogProbMetric: 28.5616

Epoch 501: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1757 - MinusLogProbMetric: 27.1757 - val_loss: 28.5616 - val_MinusLogProbMetric: 28.5616 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 502/1000
2023-10-25 17:22:51.914 
Epoch 502/1000 
	 loss: 27.1704, MinusLogProbMetric: 27.1704, val_loss: 28.5442, val_MinusLogProbMetric: 28.5442

Epoch 502: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1704 - MinusLogProbMetric: 27.1704 - val_loss: 28.5442 - val_MinusLogProbMetric: 28.5442 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 503/1000
2023-10-25 17:23:27.190 
Epoch 503/1000 
	 loss: 27.1718, MinusLogProbMetric: 27.1718, val_loss: 28.5802, val_MinusLogProbMetric: 28.5802

Epoch 503: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1718 - MinusLogProbMetric: 27.1718 - val_loss: 28.5802 - val_MinusLogProbMetric: 28.5802 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 504/1000
2023-10-25 17:24:02.465 
Epoch 504/1000 
	 loss: 27.1763, MinusLogProbMetric: 27.1763, val_loss: 28.5910, val_MinusLogProbMetric: 28.5910

Epoch 504: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1763 - MinusLogProbMetric: 27.1763 - val_loss: 28.5910 - val_MinusLogProbMetric: 28.5910 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 505/1000
2023-10-25 17:24:38.102 
Epoch 505/1000 
	 loss: 27.1741, MinusLogProbMetric: 27.1741, val_loss: 28.5272, val_MinusLogProbMetric: 28.5272

Epoch 505: val_loss did not improve from 28.48097
196/196 - 36s - loss: 27.1741 - MinusLogProbMetric: 27.1741 - val_loss: 28.5272 - val_MinusLogProbMetric: 28.5272 - lr: 1.2500e-04 - 36s/epoch - 182ms/step
Epoch 506/1000
2023-10-25 17:25:13.246 
Epoch 506/1000 
	 loss: 27.1726, MinusLogProbMetric: 27.1726, val_loss: 28.5253, val_MinusLogProbMetric: 28.5253

Epoch 506: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1726 - MinusLogProbMetric: 27.1726 - val_loss: 28.5253 - val_MinusLogProbMetric: 28.5253 - lr: 1.2500e-04 - 35s/epoch - 179ms/step
Epoch 507/1000
2023-10-25 17:25:48.563 
Epoch 507/1000 
	 loss: 27.1690, MinusLogProbMetric: 27.1690, val_loss: 28.6332, val_MinusLogProbMetric: 28.6332

Epoch 507: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1690 - MinusLogProbMetric: 27.1690 - val_loss: 28.6332 - val_MinusLogProbMetric: 28.6332 - lr: 1.2500e-04 - 35s/epoch - 180ms/step
Epoch 508/1000
2023-10-25 17:26:23.613 
Epoch 508/1000 
	 loss: 27.1328, MinusLogProbMetric: 27.1328, val_loss: 28.5152, val_MinusLogProbMetric: 28.5152

Epoch 508: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1328 - MinusLogProbMetric: 27.1328 - val_loss: 28.5152 - val_MinusLogProbMetric: 28.5152 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 509/1000
2023-10-25 17:26:58.829 
Epoch 509/1000 
	 loss: 27.1263, MinusLogProbMetric: 27.1263, val_loss: 28.5162, val_MinusLogProbMetric: 28.5162

Epoch 509: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1263 - MinusLogProbMetric: 27.1263 - val_loss: 28.5162 - val_MinusLogProbMetric: 28.5162 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 510/1000
2023-10-25 17:27:34.173 
Epoch 510/1000 
	 loss: 27.1269, MinusLogProbMetric: 27.1269, val_loss: 28.5102, val_MinusLogProbMetric: 28.5102

Epoch 510: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1269 - MinusLogProbMetric: 27.1269 - val_loss: 28.5102 - val_MinusLogProbMetric: 28.5102 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 511/1000
2023-10-25 17:28:09.318 
Epoch 511/1000 
	 loss: 27.1279, MinusLogProbMetric: 27.1279, val_loss: 28.5271, val_MinusLogProbMetric: 28.5271

Epoch 511: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1279 - MinusLogProbMetric: 27.1279 - val_loss: 28.5271 - val_MinusLogProbMetric: 28.5271 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 512/1000
2023-10-25 17:28:44.378 
Epoch 512/1000 
	 loss: 27.1269, MinusLogProbMetric: 27.1269, val_loss: 28.5157, val_MinusLogProbMetric: 28.5157

Epoch 512: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1269 - MinusLogProbMetric: 27.1269 - val_loss: 28.5157 - val_MinusLogProbMetric: 28.5157 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 513/1000
2023-10-25 17:29:19.927 
Epoch 513/1000 
	 loss: 27.1273, MinusLogProbMetric: 27.1273, val_loss: 28.5357, val_MinusLogProbMetric: 28.5357

Epoch 513: val_loss did not improve from 28.48097
196/196 - 36s - loss: 27.1273 - MinusLogProbMetric: 27.1273 - val_loss: 28.5357 - val_MinusLogProbMetric: 28.5357 - lr: 6.2500e-05 - 36s/epoch - 181ms/step
Epoch 514/1000
2023-10-25 17:29:55.328 
Epoch 514/1000 
	 loss: 27.1265, MinusLogProbMetric: 27.1265, val_loss: 28.5274, val_MinusLogProbMetric: 28.5274

Epoch 514: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1265 - MinusLogProbMetric: 27.1265 - val_loss: 28.5274 - val_MinusLogProbMetric: 28.5274 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 515/1000
2023-10-25 17:30:30.416 
Epoch 515/1000 
	 loss: 27.1294, MinusLogProbMetric: 27.1294, val_loss: 28.5199, val_MinusLogProbMetric: 28.5199

Epoch 515: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1294 - MinusLogProbMetric: 27.1294 - val_loss: 28.5199 - val_MinusLogProbMetric: 28.5199 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 516/1000
2023-10-25 17:31:05.535 
Epoch 516/1000 
	 loss: 27.1266, MinusLogProbMetric: 27.1266, val_loss: 28.5173, val_MinusLogProbMetric: 28.5173

Epoch 516: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1266 - MinusLogProbMetric: 27.1266 - val_loss: 28.5173 - val_MinusLogProbMetric: 28.5173 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 517/1000
2023-10-25 17:31:40.853 
Epoch 517/1000 
	 loss: 27.1269, MinusLogProbMetric: 27.1269, val_loss: 28.5279, val_MinusLogProbMetric: 28.5279

Epoch 517: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1269 - MinusLogProbMetric: 27.1269 - val_loss: 28.5279 - val_MinusLogProbMetric: 28.5279 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 518/1000
2023-10-25 17:32:15.822 
Epoch 518/1000 
	 loss: 27.1259, MinusLogProbMetric: 27.1259, val_loss: 28.5113, val_MinusLogProbMetric: 28.5113

Epoch 518: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1259 - MinusLogProbMetric: 27.1259 - val_loss: 28.5113 - val_MinusLogProbMetric: 28.5113 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 519/1000
2023-10-25 17:32:51.009 
Epoch 519/1000 
	 loss: 27.1259, MinusLogProbMetric: 27.1259, val_loss: 28.5361, val_MinusLogProbMetric: 28.5361

Epoch 519: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1259 - MinusLogProbMetric: 27.1259 - val_loss: 28.5361 - val_MinusLogProbMetric: 28.5361 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 520/1000
2023-10-25 17:33:26.357 
Epoch 520/1000 
	 loss: 27.1281, MinusLogProbMetric: 27.1281, val_loss: 28.5185, val_MinusLogProbMetric: 28.5185

Epoch 520: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1281 - MinusLogProbMetric: 27.1281 - val_loss: 28.5185 - val_MinusLogProbMetric: 28.5185 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 521/1000
2023-10-25 17:34:01.636 
Epoch 521/1000 
	 loss: 27.1263, MinusLogProbMetric: 27.1263, val_loss: 28.5265, val_MinusLogProbMetric: 28.5265

Epoch 521: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1263 - MinusLogProbMetric: 27.1263 - val_loss: 28.5265 - val_MinusLogProbMetric: 28.5265 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 522/1000
2023-10-25 17:34:36.648 
Epoch 522/1000 
	 loss: 27.1269, MinusLogProbMetric: 27.1269, val_loss: 28.5406, val_MinusLogProbMetric: 28.5406

Epoch 522: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1269 - MinusLogProbMetric: 27.1269 - val_loss: 28.5406 - val_MinusLogProbMetric: 28.5406 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 523/1000
2023-10-25 17:35:11.809 
Epoch 523/1000 
	 loss: 27.1227, MinusLogProbMetric: 27.1227, val_loss: 28.5295, val_MinusLogProbMetric: 28.5295

Epoch 523: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1227 - MinusLogProbMetric: 27.1227 - val_loss: 28.5295 - val_MinusLogProbMetric: 28.5295 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 524/1000
2023-10-25 17:35:46.817 
Epoch 524/1000 
	 loss: 27.1279, MinusLogProbMetric: 27.1279, val_loss: 28.5351, val_MinusLogProbMetric: 28.5351

Epoch 524: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1279 - MinusLogProbMetric: 27.1279 - val_loss: 28.5351 - val_MinusLogProbMetric: 28.5351 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 525/1000
2023-10-25 17:36:21.781 
Epoch 525/1000 
	 loss: 27.1243, MinusLogProbMetric: 27.1243, val_loss: 28.5377, val_MinusLogProbMetric: 28.5377

Epoch 525: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1243 - MinusLogProbMetric: 27.1243 - val_loss: 28.5377 - val_MinusLogProbMetric: 28.5377 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 526/1000
2023-10-25 17:36:57.082 
Epoch 526/1000 
	 loss: 27.1302, MinusLogProbMetric: 27.1302, val_loss: 28.5187, val_MinusLogProbMetric: 28.5187

Epoch 526: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1302 - MinusLogProbMetric: 27.1302 - val_loss: 28.5187 - val_MinusLogProbMetric: 28.5187 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 527/1000
2023-10-25 17:37:32.319 
Epoch 527/1000 
	 loss: 27.1260, MinusLogProbMetric: 27.1260, val_loss: 28.5316, val_MinusLogProbMetric: 28.5316

Epoch 527: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1260 - MinusLogProbMetric: 27.1260 - val_loss: 28.5316 - val_MinusLogProbMetric: 28.5316 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 528/1000
2023-10-25 17:38:07.670 
Epoch 528/1000 
	 loss: 27.1227, MinusLogProbMetric: 27.1227, val_loss: 28.5223, val_MinusLogProbMetric: 28.5223

Epoch 528: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1227 - MinusLogProbMetric: 27.1227 - val_loss: 28.5223 - val_MinusLogProbMetric: 28.5223 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 529/1000
2023-10-25 17:38:43.066 
Epoch 529/1000 
	 loss: 27.1213, MinusLogProbMetric: 27.1213, val_loss: 28.5306, val_MinusLogProbMetric: 28.5306

Epoch 529: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1213 - MinusLogProbMetric: 27.1213 - val_loss: 28.5306 - val_MinusLogProbMetric: 28.5306 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 530/1000
2023-10-25 17:39:18.294 
Epoch 530/1000 
	 loss: 27.1192, MinusLogProbMetric: 27.1192, val_loss: 28.5213, val_MinusLogProbMetric: 28.5213

Epoch 530: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1192 - MinusLogProbMetric: 27.1192 - val_loss: 28.5213 - val_MinusLogProbMetric: 28.5213 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 531/1000
2023-10-25 17:39:53.692 
Epoch 531/1000 
	 loss: 27.1253, MinusLogProbMetric: 27.1253, val_loss: 28.5397, val_MinusLogProbMetric: 28.5397

Epoch 531: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1253 - MinusLogProbMetric: 27.1253 - val_loss: 28.5397 - val_MinusLogProbMetric: 28.5397 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 532/1000
2023-10-25 17:40:28.786 
Epoch 532/1000 
	 loss: 27.1269, MinusLogProbMetric: 27.1269, val_loss: 28.5289, val_MinusLogProbMetric: 28.5289

Epoch 532: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1269 - MinusLogProbMetric: 27.1269 - val_loss: 28.5289 - val_MinusLogProbMetric: 28.5289 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 533/1000
2023-10-25 17:41:03.915 
Epoch 533/1000 
	 loss: 27.1252, MinusLogProbMetric: 27.1252, val_loss: 28.5431, val_MinusLogProbMetric: 28.5431

Epoch 533: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1252 - MinusLogProbMetric: 27.1252 - val_loss: 28.5431 - val_MinusLogProbMetric: 28.5431 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 534/1000
2023-10-25 17:41:39.152 
Epoch 534/1000 
	 loss: 27.1247, MinusLogProbMetric: 27.1247, val_loss: 28.5111, val_MinusLogProbMetric: 28.5111

Epoch 534: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1247 - MinusLogProbMetric: 27.1247 - val_loss: 28.5111 - val_MinusLogProbMetric: 28.5111 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 535/1000
2023-10-25 17:42:14.603 
Epoch 535/1000 
	 loss: 27.1308, MinusLogProbMetric: 27.1308, val_loss: 28.5414, val_MinusLogProbMetric: 28.5414

Epoch 535: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1308 - MinusLogProbMetric: 27.1308 - val_loss: 28.5414 - val_MinusLogProbMetric: 28.5414 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 536/1000
2023-10-25 17:42:49.986 
Epoch 536/1000 
	 loss: 27.1223, MinusLogProbMetric: 27.1223, val_loss: 28.5305, val_MinusLogProbMetric: 28.5305

Epoch 536: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1223 - MinusLogProbMetric: 27.1223 - val_loss: 28.5305 - val_MinusLogProbMetric: 28.5305 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 537/1000
2023-10-25 17:43:25.130 
Epoch 537/1000 
	 loss: 27.1265, MinusLogProbMetric: 27.1265, val_loss: 28.5277, val_MinusLogProbMetric: 28.5277

Epoch 537: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1265 - MinusLogProbMetric: 27.1265 - val_loss: 28.5277 - val_MinusLogProbMetric: 28.5277 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 538/1000
2023-10-25 17:44:00.382 
Epoch 538/1000 
	 loss: 27.1223, MinusLogProbMetric: 27.1223, val_loss: 28.5436, val_MinusLogProbMetric: 28.5436

Epoch 538: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1223 - MinusLogProbMetric: 27.1223 - val_loss: 28.5436 - val_MinusLogProbMetric: 28.5436 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 539/1000
2023-10-25 17:44:35.833 
Epoch 539/1000 
	 loss: 27.1257, MinusLogProbMetric: 27.1257, val_loss: 28.5378, val_MinusLogProbMetric: 28.5378

Epoch 539: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1257 - MinusLogProbMetric: 27.1257 - val_loss: 28.5378 - val_MinusLogProbMetric: 28.5378 - lr: 6.2500e-05 - 35s/epoch - 181ms/step
Epoch 540/1000
2023-10-25 17:45:11.065 
Epoch 540/1000 
	 loss: 27.1251, MinusLogProbMetric: 27.1251, val_loss: 28.5654, val_MinusLogProbMetric: 28.5654

Epoch 540: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1251 - MinusLogProbMetric: 27.1251 - val_loss: 28.5654 - val_MinusLogProbMetric: 28.5654 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 541/1000
2023-10-25 17:45:46.347 
Epoch 541/1000 
	 loss: 27.1226, MinusLogProbMetric: 27.1226, val_loss: 28.5480, val_MinusLogProbMetric: 28.5480

Epoch 541: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1226 - MinusLogProbMetric: 27.1226 - val_loss: 28.5480 - val_MinusLogProbMetric: 28.5480 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 542/1000
2023-10-25 17:46:21.636 
Epoch 542/1000 
	 loss: 27.1234, MinusLogProbMetric: 27.1234, val_loss: 28.5298, val_MinusLogProbMetric: 28.5298

Epoch 542: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1234 - MinusLogProbMetric: 27.1234 - val_loss: 28.5298 - val_MinusLogProbMetric: 28.5298 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 543/1000
2023-10-25 17:46:56.887 
Epoch 543/1000 
	 loss: 27.1199, MinusLogProbMetric: 27.1199, val_loss: 28.5196, val_MinusLogProbMetric: 28.5196

Epoch 543: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1199 - MinusLogProbMetric: 27.1199 - val_loss: 28.5196 - val_MinusLogProbMetric: 28.5196 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 544/1000
2023-10-25 17:47:32.147 
Epoch 544/1000 
	 loss: 27.1247, MinusLogProbMetric: 27.1247, val_loss: 28.5407, val_MinusLogProbMetric: 28.5407

Epoch 544: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1247 - MinusLogProbMetric: 27.1247 - val_loss: 28.5407 - val_MinusLogProbMetric: 28.5407 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 545/1000
2023-10-25 17:48:07.508 
Epoch 545/1000 
	 loss: 27.1210, MinusLogProbMetric: 27.1210, val_loss: 28.5285, val_MinusLogProbMetric: 28.5285

Epoch 545: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1210 - MinusLogProbMetric: 27.1210 - val_loss: 28.5285 - val_MinusLogProbMetric: 28.5285 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 546/1000
2023-10-25 17:48:42.612 
Epoch 546/1000 
	 loss: 27.1185, MinusLogProbMetric: 27.1185, val_loss: 28.5226, val_MinusLogProbMetric: 28.5226

Epoch 546: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1185 - MinusLogProbMetric: 27.1185 - val_loss: 28.5226 - val_MinusLogProbMetric: 28.5226 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 547/1000
2023-10-25 17:49:17.906 
Epoch 547/1000 
	 loss: 27.1191, MinusLogProbMetric: 27.1191, val_loss: 28.5199, val_MinusLogProbMetric: 28.5199

Epoch 547: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1191 - MinusLogProbMetric: 27.1191 - val_loss: 28.5199 - val_MinusLogProbMetric: 28.5199 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 548/1000
2023-10-25 17:49:53.203 
Epoch 548/1000 
	 loss: 27.1189, MinusLogProbMetric: 27.1189, val_loss: 28.5214, val_MinusLogProbMetric: 28.5214

Epoch 548: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1189 - MinusLogProbMetric: 27.1189 - val_loss: 28.5214 - val_MinusLogProbMetric: 28.5214 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 549/1000
2023-10-25 17:50:28.157 
Epoch 549/1000 
	 loss: 27.1214, MinusLogProbMetric: 27.1214, val_loss: 28.5441, val_MinusLogProbMetric: 28.5441

Epoch 549: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1214 - MinusLogProbMetric: 27.1214 - val_loss: 28.5441 - val_MinusLogProbMetric: 28.5441 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 550/1000
2023-10-25 17:51:03.262 
Epoch 550/1000 
	 loss: 27.1201, MinusLogProbMetric: 27.1201, val_loss: 28.5339, val_MinusLogProbMetric: 28.5339

Epoch 550: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1201 - MinusLogProbMetric: 27.1201 - val_loss: 28.5339 - val_MinusLogProbMetric: 28.5339 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 551/1000
2023-10-25 17:51:38.558 
Epoch 551/1000 
	 loss: 27.1194, MinusLogProbMetric: 27.1194, val_loss: 28.5233, val_MinusLogProbMetric: 28.5233

Epoch 551: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1194 - MinusLogProbMetric: 27.1194 - val_loss: 28.5233 - val_MinusLogProbMetric: 28.5233 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 552/1000
2023-10-25 17:52:13.480 
Epoch 552/1000 
	 loss: 27.1161, MinusLogProbMetric: 27.1161, val_loss: 28.5056, val_MinusLogProbMetric: 28.5056

Epoch 552: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1161 - MinusLogProbMetric: 27.1161 - val_loss: 28.5056 - val_MinusLogProbMetric: 28.5056 - lr: 6.2500e-05 - 35s/epoch - 178ms/step
Epoch 553/1000
2023-10-25 17:52:48.730 
Epoch 553/1000 
	 loss: 27.1188, MinusLogProbMetric: 27.1188, val_loss: 28.5356, val_MinusLogProbMetric: 28.5356

Epoch 553: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1188 - MinusLogProbMetric: 27.1188 - val_loss: 28.5356 - val_MinusLogProbMetric: 28.5356 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 554/1000
2023-10-25 17:53:24.108 
Epoch 554/1000 
	 loss: 27.1215, MinusLogProbMetric: 27.1215, val_loss: 28.5179, val_MinusLogProbMetric: 28.5179

Epoch 554: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1215 - MinusLogProbMetric: 27.1215 - val_loss: 28.5179 - val_MinusLogProbMetric: 28.5179 - lr: 6.2500e-05 - 35s/epoch - 180ms/step
Epoch 555/1000
2023-10-25 17:53:59.219 
Epoch 555/1000 
	 loss: 27.1191, MinusLogProbMetric: 27.1191, val_loss: 28.5881, val_MinusLogProbMetric: 28.5881

Epoch 555: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1191 - MinusLogProbMetric: 27.1191 - val_loss: 28.5881 - val_MinusLogProbMetric: 28.5881 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 556/1000
2023-10-25 17:54:34.331 
Epoch 556/1000 
	 loss: 27.1237, MinusLogProbMetric: 27.1237, val_loss: 28.5449, val_MinusLogProbMetric: 28.5449

Epoch 556: val_loss did not improve from 28.48097
196/196 - 35s - loss: 27.1237 - MinusLogProbMetric: 27.1237 - val_loss: 28.5449 - val_MinusLogProbMetric: 28.5449 - lr: 6.2500e-05 - 35s/epoch - 179ms/step
Epoch 557/1000
2023-10-25 17:55:09.706 
Epoch 557/1000 
	 loss: 27.1172, MinusLogProbMetric: 27.1172, val_loss: 28.5359, val_MinusLogProbMetric: 28.5359

Epoch 557: val_loss did not improve from 28.48097
Restoring model weights from the end of the best epoch: 457.
196/196 - 36s - loss: 27.1172 - MinusLogProbMetric: 27.1172 - val_loss: 28.5359 - val_MinusLogProbMetric: 28.5359 - lr: 6.2500e-05 - 36s/epoch - 182ms/step
Epoch 557: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 520.
Model trained in 18464.60 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.81 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.12 s.
===========
Run 354/720 done in 18470.03 s.
===========

Directory ../../results/CsplineN_new/run_355/ already exists.
Skipping it.
===========
Run 355/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_356/ already exists.
Skipping it.
===========
Run 356/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_357/ already exists.
Skipping it.
===========
Run 357/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_358/ already exists.
Skipping it.
===========
Run 358/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_359/ already exists.
Skipping it.
===========
Run 359/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_360/ already exists.
Skipping it.
===========
Run 360/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_361/ already exists.
Skipping it.
===========
Run 361/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_362/ already exists.
Skipping it.
===========
Run 362/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_363/ already exists.
Skipping it.
===========
Run 363/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_364/ already exists.
Skipping it.
===========
Run 364/720 already exists. Skipping it.
===========

===========
Generating train data for run 365.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_145"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_146 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7f1e6dd7b310>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1796fd09d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1796fd09d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1e6de8e290>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1796c88e20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1796c89390>, <keras.callbacks.ModelCheckpoint object at 0x7f1796c89450>, <keras.callbacks.EarlyStopping object at 0x7f1796c896c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1796c896f0>, <keras.callbacks.TerminateOnNaN object at 0x7f1796c89330>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-25 17:55:18.408584
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 17:57:42.728 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 144s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 144s/epoch - 736ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0003333333333333333.
===========
Generating train data for run 365.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_156"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_157 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7f19e832b3a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1a0841ed70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1a0841ed70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f18e0632c20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f17d367ee30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f17d367fac0>, <keras.callbacks.ModelCheckpoint object at 0x7f17d367fb50>, <keras.callbacks.EarlyStopping object at 0x7f17d367ff40>, <keras.callbacks.ReduceLROnPlateau object at 0x7f17d367f6d0>, <keras.callbacks.TerminateOnNaN object at 0x7f17d367fb80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-25 17:57:53.653399
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:00:42.675 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 169s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 169s/epoch - 861ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0001111111111111111.
===========
Generating train data for run 365.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_167"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_168 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7f15fc6c3c40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f15fc6c3610>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f15fc6c3610>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f15fdbf07c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f15fc449330>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f15fc4498a0>, <keras.callbacks.ModelCheckpoint object at 0x7f15fc449960>, <keras.callbacks.EarlyStopping object at 0x7f15fc449bd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f15fc449c00>, <keras.callbacks.TerminateOnNaN object at 0x7f15fc449840>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-25 18:00:53.029060
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:03:24.742 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 152s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 152s/epoch - 773ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 3.703703703703703e-05.
===========
Generating train data for run 365.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_178"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_179 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7f1aa01bdf00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f17c53ec0a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f17c53ec0a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f17d2353e80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1910553760>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1910553cd0>, <keras.callbacks.ModelCheckpoint object at 0x7f1910553d90>, <keras.callbacks.EarlyStopping object at 0x7f1910553c70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1910553ca0>, <keras.callbacks.TerminateOnNaN object at 0x7f1910553fa0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-25 18:03:32.952398
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:06:11.733 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 159s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 159s/epoch - 810ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 1.2345679012345677e-05.
===========
Generating train data for run 365.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_189"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_190 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7f15f5e7f370>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f15fc8a52a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f15fc8a52a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f163cbdaf50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f163cb9c730>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f163cb9cca0>, <keras.callbacks.ModelCheckpoint object at 0x7f163cb9cd60>, <keras.callbacks.EarlyStopping object at 0x7f163cb9cfd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f163cb9d000>, <keras.callbacks.TerminateOnNaN object at 0x7f163cb9cc40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-25 18:06:22.067865
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:09:17.904 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 176s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 176s/epoch - 896ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 4.115226337448558e-06.
===========
Generating train data for run 365.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_200"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_201 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7f15f53c7d60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f15fd5fbdc0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f15fd5fbdc0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f15e9538100>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f163532bfa0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f15e9fcc550>, <keras.callbacks.ModelCheckpoint object at 0x7f15e9fcc610>, <keras.callbacks.EarlyStopping object at 0x7f15e9fcc880>, <keras.callbacks.ReduceLROnPlateau object at 0x7f15e9fcc8b0>, <keras.callbacks.TerminateOnNaN object at 0x7f15e9fcc4f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-25 18:09:28.888304
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:12:14.520 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 166s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 166s/epoch - 844ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 1.3717421124828526e-06.
===========
Generating train data for run 365.
===========
Train data generated in 0.38 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_211"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_212 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7f18f8154f10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f19105e0d00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f19105e0d00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f163cb9e830>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f17c71ef220>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f17c71ed4b0>, <keras.callbacks.ModelCheckpoint object at 0x7f17c71ecb80>, <keras.callbacks.EarlyStopping object at 0x7f17c71ee110>, <keras.callbacks.ReduceLROnPlateau object at 0x7f17c71eee30>, <keras.callbacks.TerminateOnNaN object at 0x7f17c71efa00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-25 18:12:23.691581
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:15:05.022 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 161s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 161s/epoch - 823ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 4.572473708276175e-07.
===========
Generating train data for run 365.
===========
Train data generated in 0.36 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_222"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_223 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7f1801795ae0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f15984225f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f15984225f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1e6dfded10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f15e8408340>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f15e840b970>, <keras.callbacks.ModelCheckpoint object at 0x7f15e84086d0>, <keras.callbacks.EarlyStopping object at 0x7f15e840ba00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f15e840bfa0>, <keras.callbacks.TerminateOnNaN object at 0x7f15e840b880>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-25 18:15:16.554867
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:17:48.844 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 152s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 152s/epoch - 775ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 1.524157902758725e-07.
===========
Generating train data for run 365.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_233"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_234 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7f18e04fd210>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1796b0fca0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1796b0fca0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f18304e34f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f15fdb2a4a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f15fdb2bca0>, <keras.callbacks.ModelCheckpoint object at 0x7f15fdb29510>, <keras.callbacks.EarlyStopping object at 0x7f15fdb2a470>, <keras.callbacks.ReduceLROnPlateau object at 0x7f15fdb2bb20>, <keras.callbacks.TerminateOnNaN object at 0x7f15fdb2b280>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-25 18:18:07.748461
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:20:50.073 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 162s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 162s/epoch - 827ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 5.0805263425290834e-08.
===========
Generating train data for run 365.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_244"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_245 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7f178fb8fca0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f18304e2860>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f18304e2860>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f15fdb296f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f178fbc2cb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f178fbc3220>, <keras.callbacks.ModelCheckpoint object at 0x7f178fbc32e0>, <keras.callbacks.EarlyStopping object at 0x7f178fbc3550>, <keras.callbacks.ReduceLROnPlateau object at 0x7f178fbc3580>, <keras.callbacks.TerminateOnNaN object at 0x7f178fbc31c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-25 18:21:01.011099
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:23:43.795 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 163s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 163s/epoch - 829ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 1.6935087808430278e-08.
===========
Generating train data for run 365.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_365/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_365
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_255"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_256 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7f15fdb09d80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f161d9a49a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f161d9a49a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f17d2d0fdf0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f14f07793c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_365/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f14f0779930>, <keras.callbacks.ModelCheckpoint object at 0x7f14f07799f0>, <keras.callbacks.EarlyStopping object at 0x7f14f0779c60>, <keras.callbacks.ReduceLROnPlateau object at 0x7f14f0779c90>, <keras.callbacks.TerminateOnNaN object at 0x7f14f07798d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_365/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 365/720 with hyperparameters:
timestamp = 2023-10-25 18:23:54.296705
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:26:41.542 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7777.1406, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 167s - loss: nan - MinusLogProbMetric: 7777.1406 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 167s/epoch - 852ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 5.645029269476759e-09.
===========
Run 365/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 366.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_366
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_266"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_267 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7f17e05d3970>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f163ca2fbe0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f163ca2fbe0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f17d245b4f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f17e05ca980>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f17e05caef0>, <keras.callbacks.ModelCheckpoint object at 0x7f17e05cafb0>, <keras.callbacks.EarlyStopping object at 0x7f17e05cb220>, <keras.callbacks.ReduceLROnPlateau object at 0x7f17e05cb250>, <keras.callbacks.TerminateOnNaN object at 0x7f17e05cae90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_366/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 366/720 with hyperparameters:
timestamp = 2023-10-25 18:26:51.157596
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 5: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:29:30.392 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6412.7593, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 159s - loss: nan - MinusLogProbMetric: 6412.7593 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 159s/epoch - 812ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0003333333333333333.
===========
Generating train data for run 366.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_366
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_277"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_278 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7f15286076a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f17d3857fa0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f17d3857fa0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1644c282e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f15286926b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1528692c20>, <keras.callbacks.ModelCheckpoint object at 0x7f1528692ce0>, <keras.callbacks.EarlyStopping object at 0x7f1528692f50>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1528692f80>, <keras.callbacks.TerminateOnNaN object at 0x7f1528692bc0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_366/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 366/720 with hyperparameters:
timestamp = 2023-10-25 18:29:40.517966
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 24: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:32:24.456 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5559.7876, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 164s - loss: nan - MinusLogProbMetric: 5559.7876 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 164s/epoch - 835ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0001111111111111111.
===========
Generating train data for run 366.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_366
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_288"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_289 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7f18a0541270>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f19a46c3fa0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f19a46c3fa0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f17d0884ca0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f15fc9ea860>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f15fc9eadd0>, <keras.callbacks.ModelCheckpoint object at 0x7f15fc9eae90>, <keras.callbacks.EarlyStopping object at 0x7f15fc9eb100>, <keras.callbacks.ReduceLROnPlateau object at 0x7f15fc9eb130>, <keras.callbacks.TerminateOnNaN object at 0x7f15fc9ead70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_366/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 366/720 with hyperparameters:
timestamp = 2023-10-25 18:32:35.192763
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 58: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:35:50.848 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5595.4771, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 195s - loss: nan - MinusLogProbMetric: 5595.4771 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 195s/epoch - 997ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 3.703703703703703e-05.
===========
Generating train data for run 366.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_366
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_299"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_300 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7f19a4203850>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f165c147d60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f165c147d60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f14b9b11180>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f19a472b760>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f19a472bcd0>, <keras.callbacks.ModelCheckpoint object at 0x7f19a472bd90>, <keras.callbacks.EarlyStopping object at 0x7f19a472bc70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f19a472bca0>, <keras.callbacks.TerminateOnNaN object at 0x7f19a472bfa0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_366/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 366/720 with hyperparameters:
timestamp = 2023-10-25 18:36:02.014790
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 9: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:38:38.195 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6893.3716, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 156s - loss: nan - MinusLogProbMetric: 6893.3716 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 156s/epoch - 795ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 1.2345679012345677e-05.
===========
Generating train data for run 366.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_366
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_310"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_311 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7f16f0755b10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f19103cea70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f19103cea70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f16f0756b60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f18f8499cf0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f18f8499030>, <keras.callbacks.ModelCheckpoint object at 0x7f18f8499c60>, <keras.callbacks.EarlyStopping object at 0x7f18f8498160>, <keras.callbacks.ReduceLROnPlateau object at 0x7f18f8498cd0>, <keras.callbacks.TerminateOnNaN object at 0x7f18f8499210>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_366/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 366/720 with hyperparameters:
timestamp = 2023-10-25 18:38:56.543447
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 54: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:41:58.991 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6787.2515, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 182s - loss: nan - MinusLogProbMetric: 6787.2515 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 182s/epoch - 930ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 4.115226337448558e-06.
===========
Generating train data for run 366.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_366
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_321"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_322 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7f178f334220>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f15dcb320b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f15dcb320b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f15e85025f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f178eaf7ca0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f178e928250>, <keras.callbacks.ModelCheckpoint object at 0x7f178e928310>, <keras.callbacks.EarlyStopping object at 0x7f178e928580>, <keras.callbacks.ReduceLROnPlateau object at 0x7f178e9285b0>, <keras.callbacks.TerminateOnNaN object at 0x7f178e9281f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_366/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 366/720 with hyperparameters:
timestamp = 2023-10-25 18:42:07.053300
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
2023-10-25 18:45:57.269 
Epoch 1/1000 
	 loss: 6774.1460, MinusLogProbMetric: 6774.1460, val_loss: 6567.9331, val_MinusLogProbMetric: 6567.9331

Epoch 1: val_loss improved from inf to 6567.93311, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 231s - loss: 6774.1460 - MinusLogProbMetric: 6774.1460 - val_loss: 6567.9331 - val_MinusLogProbMetric: 6567.9331 - lr: 4.1152e-06 - 231s/epoch - 1s/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 56: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:46:21.543 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 6522.2490, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 6567.93311
196/196 - 23s - loss: nan - MinusLogProbMetric: 6522.2490 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 23s/epoch - 116ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 1.3717421124828526e-06.
===========
Generating train data for run 366.
===========
Train data generated in 0.39 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_366
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_332"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_333 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7f1599fcf460>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f15e8b62fb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f15e8b62fb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f178fa75e10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f15f46d13c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f15f46d1930>, <keras.callbacks.ModelCheckpoint object at 0x7f15f46d19f0>, <keras.callbacks.EarlyStopping object at 0x7f15f46d1c60>, <keras.callbacks.ReduceLROnPlateau object at 0x7f15f46d1c90>, <keras.callbacks.TerminateOnNaN object at 0x7f15f46d18d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 366/720 with hyperparameters:
timestamp = 2023-10-25 18:46:30.371929
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 143: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:49:42.988 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6500.4365, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 193s - loss: nan - MinusLogProbMetric: 6500.4365 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 193s/epoch - 983ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 4.572473708276175e-07.
===========
Generating train data for run 366.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_366
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_343"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_344 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7f16761099c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1e885948b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1e885948b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f17d3be15a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f17e04190f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f17e0418040>, <keras.callbacks.ModelCheckpoint object at 0x7f17e041b160>, <keras.callbacks.EarlyStopping object at 0x7f17e041b5e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f17e041b640>, <keras.callbacks.TerminateOnNaN object at 0x7f17e041bf10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 366/720 with hyperparameters:
timestamp = 2023-10-25 18:49:54.566281
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 150: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:53:35.494 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6554.4756, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 221s - loss: nan - MinusLogProbMetric: 6554.4756 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 221s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 1.524157902758725e-07.
===========
Generating train data for run 366.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_366
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_354"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_355 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7f143a9d3850>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1458b336d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1458b336d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1481e58910>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f14585f0b80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f14585f10f0>, <keras.callbacks.ModelCheckpoint object at 0x7f14585f11b0>, <keras.callbacks.EarlyStopping object at 0x7f14585f1420>, <keras.callbacks.ReduceLROnPlateau object at 0x7f14585f1450>, <keras.callbacks.TerminateOnNaN object at 0x7f14585f1090>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 366/720 with hyperparameters:
timestamp = 2023-10-25 18:53:45.316842
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 150: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 18:57:24.874 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6565.2402, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 219s - loss: nan - MinusLogProbMetric: 6565.2402 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 219s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 5.0805263425290834e-08.
===========
Generating train data for run 366.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_366
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_365"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_366 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7f14bab1bb20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1599982080>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1599982080>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f159808e0e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f147824d810>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f147824dd80>, <keras.callbacks.ModelCheckpoint object at 0x7f147824de40>, <keras.callbacks.EarlyStopping object at 0x7f147824e0b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f147824e0e0>, <keras.callbacks.TerminateOnNaN object at 0x7f147824dd20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 366/720 with hyperparameters:
timestamp = 2023-10-25 18:57:35.885712
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 119: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 19:00:49.048 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6569.1836, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 193s - loss: nan - MinusLogProbMetric: 6569.1836 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 193s/epoch - 985ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 1.6935087808430278e-08.
===========
Generating train data for run 366.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_366
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_376"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_377 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7f19307b50c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f16f03b98d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f16f03b98d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f17e02481f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f14381e2620>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f14381e22f0>, <keras.callbacks.ModelCheckpoint object at 0x7f14381e1e10>, <keras.callbacks.EarlyStopping object at 0x7f14381e3a60>, <keras.callbacks.ReduceLROnPlateau object at 0x7f14381e1420>, <keras.callbacks.TerminateOnNaN object at 0x7f14381e0040>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 366/720 with hyperparameters:
timestamp = 2023-10-25 19:00:59.963260
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
2023-10-25 19:04:31.983 
Epoch 1/1000 
	 loss: 6569.9512, MinusLogProbMetric: 6569.9512, val_loss: 6566.6367, val_MinusLogProbMetric: 6566.6367

Epoch 1: val_loss improved from inf to 6566.63672, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 212s - loss: 6569.9512 - MinusLogProbMetric: 6569.9512 - val_loss: 6566.6367 - val_MinusLogProbMetric: 6566.6367 - lr: 1.6935e-08 - 212s/epoch - 1s/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 10: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 19:04:40.438 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 6562.4404, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 6566.63672
196/196 - 7s - loss: nan - MinusLogProbMetric: 6562.4404 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 7s/epoch - 38ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 5.645029269476759e-09.
===========
Run 366/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_367/ already exists.
Skipping it.
===========
Run 367/720 already exists. Skipping it.
===========

===========
Generating train data for run 368.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_368
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_387"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_388 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_42 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_42/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_42'")
self.model: <keras.engine.functional.Functional object at 0x7f178dfaf820>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f178e669a50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f178e669a50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f178df88e20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f178dfe6830>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f178dfe6da0>, <keras.callbacks.ModelCheckpoint object at 0x7f178dfe6e60>, <keras.callbacks.EarlyStopping object at 0x7f178dfe70d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f178dfe7100>, <keras.callbacks.TerminateOnNaN object at 0x7f178dfe6d40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_368/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 368/720 with hyperparameters:
timestamp = 2023-10-25 19:04:52.073978
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 11: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 19:08:43.107 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6238.3804, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 231s - loss: nan - MinusLogProbMetric: 6238.3804 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 231s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0003333333333333333.
===========
Generating train data for run 368.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_368
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_398"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_399 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_43 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_43/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_43'")
self.model: <keras.engine.functional.Functional object at 0x7f14787c8d00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f15f446a500>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f15f446a500>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f15986d9fc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f15986003a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1598600910>, <keras.callbacks.ModelCheckpoint object at 0x7f15986009d0>, <keras.callbacks.EarlyStopping object at 0x7f1598600c40>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1598600c70>, <keras.callbacks.TerminateOnNaN object at 0x7f15986008b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_368/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 368/720 with hyperparameters:
timestamp = 2023-10-25 19:08:54.898492
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 45: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 19:13:00.556 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 4975.7021, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 245s - loss: nan - MinusLogProbMetric: 4975.7021 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 245s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0001111111111111111.
===========
Generating train data for run 368.
===========
Train data generated in 0.36 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_368
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_409"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_410 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_44 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_44/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_44'")
self.model: <keras.engine.functional.Functional object at 0x7f14f0f17fd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f160c397a90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f160c397a90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f19a46a01f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f17c719feb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f17c71ac460>, <keras.callbacks.ModelCheckpoint object at 0x7f17c71ac520>, <keras.callbacks.EarlyStopping object at 0x7f17c71ac790>, <keras.callbacks.ReduceLROnPlateau object at 0x7f17c71ac7c0>, <keras.callbacks.TerminateOnNaN object at 0x7f17c71ac400>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_368/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 368/720 with hyperparameters:
timestamp = 2023-10-25 19:13:13.828319
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
2023-10-25 19:19:07.295 
Epoch 1/1000 
	 loss: 4374.2578, MinusLogProbMetric: 4374.2578, val_loss: 3138.6035, val_MinusLogProbMetric: 3138.6035

Epoch 1: val_loss improved from inf to 3138.60352, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 355s - loss: 4374.2578 - MinusLogProbMetric: 4374.2578 - val_loss: 3138.6035 - val_MinusLogProbMetric: 3138.6035 - lr: 1.1111e-04 - 355s/epoch - 2s/step
Epoch 2/1000
2023-10-25 19:20:38.954 
Epoch 2/1000 
	 loss: 2059.8499, MinusLogProbMetric: 2059.8499, val_loss: 1994.7448, val_MinusLogProbMetric: 1994.7448

Epoch 2: val_loss improved from 3138.60352 to 1994.74475, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 2059.8499 - MinusLogProbMetric: 2059.8499 - val_loss: 1994.7448 - val_MinusLogProbMetric: 1994.7448 - lr: 1.1111e-04 - 91s/epoch - 466ms/step
Epoch 3/1000
2023-10-25 19:22:10.166 
Epoch 3/1000 
	 loss: 1199.8169, MinusLogProbMetric: 1199.8169, val_loss: 893.0692, val_MinusLogProbMetric: 893.0692

Epoch 3: val_loss improved from 1994.74475 to 893.06921, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 1199.8169 - MinusLogProbMetric: 1199.8169 - val_loss: 893.0692 - val_MinusLogProbMetric: 893.0692 - lr: 1.1111e-04 - 91s/epoch - 466ms/step
Epoch 4/1000
2023-10-25 19:23:41.464 
Epoch 4/1000 
	 loss: 823.6104, MinusLogProbMetric: 823.6104, val_loss: 757.0874, val_MinusLogProbMetric: 757.0874

Epoch 4: val_loss improved from 893.06921 to 757.08740, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 823.6104 - MinusLogProbMetric: 823.6104 - val_loss: 757.0874 - val_MinusLogProbMetric: 757.0874 - lr: 1.1111e-04 - 91s/epoch - 465ms/step
Epoch 5/1000
2023-10-25 19:25:13.206 
Epoch 5/1000 
	 loss: 707.7331, MinusLogProbMetric: 707.7331, val_loss: 657.3683, val_MinusLogProbMetric: 657.3683

Epoch 5: val_loss improved from 757.08740 to 657.36835, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 707.7331 - MinusLogProbMetric: 707.7331 - val_loss: 657.3683 - val_MinusLogProbMetric: 657.3683 - lr: 1.1111e-04 - 92s/epoch - 469ms/step
Epoch 6/1000
2023-10-25 19:26:43.505 
Epoch 6/1000 
	 loss: 614.4360, MinusLogProbMetric: 614.4360, val_loss: 571.3664, val_MinusLogProbMetric: 571.3664

Epoch 6: val_loss improved from 657.36835 to 571.36639, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 614.4360 - MinusLogProbMetric: 614.4360 - val_loss: 571.3664 - val_MinusLogProbMetric: 571.3664 - lr: 1.1111e-04 - 90s/epoch - 460ms/step
Epoch 7/1000
2023-10-25 19:28:13.559 
Epoch 7/1000 
	 loss: 560.2106, MinusLogProbMetric: 560.2106, val_loss: 523.5009, val_MinusLogProbMetric: 523.5009

Epoch 7: val_loss improved from 571.36639 to 523.50092, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 560.2106 - MinusLogProbMetric: 560.2106 - val_loss: 523.5009 - val_MinusLogProbMetric: 523.5009 - lr: 1.1111e-04 - 90s/epoch - 460ms/step
Epoch 8/1000
2023-10-25 19:29:44.217 
Epoch 8/1000 
	 loss: 518.9111, MinusLogProbMetric: 518.9111, val_loss: 552.2151, val_MinusLogProbMetric: 552.2151

Epoch 8: val_loss did not improve from 523.50092
196/196 - 89s - loss: 518.9111 - MinusLogProbMetric: 518.9111 - val_loss: 552.2151 - val_MinusLogProbMetric: 552.2151 - lr: 1.1111e-04 - 89s/epoch - 454ms/step
Epoch 9/1000
2023-10-25 19:31:13.057 
Epoch 9/1000 
	 loss: 487.0829, MinusLogProbMetric: 487.0829, val_loss: 463.2081, val_MinusLogProbMetric: 463.2081

Epoch 9: val_loss improved from 523.50092 to 463.20810, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 487.0829 - MinusLogProbMetric: 487.0829 - val_loss: 463.2081 - val_MinusLogProbMetric: 463.2081 - lr: 1.1111e-04 - 90s/epoch - 461ms/step
Epoch 10/1000
2023-10-25 19:32:44.895 
Epoch 10/1000 
	 loss: 590.9509, MinusLogProbMetric: 590.9509, val_loss: 530.7512, val_MinusLogProbMetric: 530.7512

Epoch 10: val_loss did not improve from 463.20810
196/196 - 90s - loss: 590.9509 - MinusLogProbMetric: 590.9509 - val_loss: 530.7512 - val_MinusLogProbMetric: 530.7512 - lr: 1.1111e-04 - 90s/epoch - 461ms/step
Epoch 11/1000
2023-10-25 19:34:15.219 
Epoch 11/1000 
	 loss: 496.6063, MinusLogProbMetric: 496.6063, val_loss: 543.4500, val_MinusLogProbMetric: 543.4500

Epoch 11: val_loss did not improve from 463.20810
196/196 - 90s - loss: 496.6063 - MinusLogProbMetric: 496.6063 - val_loss: 543.4500 - val_MinusLogProbMetric: 543.4500 - lr: 1.1111e-04 - 90s/epoch - 461ms/step
Epoch 12/1000
2023-10-25 19:35:44.778 
Epoch 12/1000 
	 loss: 511.6332, MinusLogProbMetric: 511.6332, val_loss: 454.3486, val_MinusLogProbMetric: 454.3486

Epoch 12: val_loss improved from 463.20810 to 454.34860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 511.6332 - MinusLogProbMetric: 511.6332 - val_loss: 454.3486 - val_MinusLogProbMetric: 454.3486 - lr: 1.1111e-04 - 91s/epoch - 465ms/step
Epoch 13/1000
2023-10-25 19:37:15.162 
Epoch 13/1000 
	 loss: 442.2108, MinusLogProbMetric: 442.2108, val_loss: 552.6157, val_MinusLogProbMetric: 552.6157

Epoch 13: val_loss did not improve from 454.34860
196/196 - 89s - loss: 442.2108 - MinusLogProbMetric: 442.2108 - val_loss: 552.6157 - val_MinusLogProbMetric: 552.6157 - lr: 1.1111e-04 - 89s/epoch - 453ms/step
Epoch 14/1000
2023-10-25 19:38:43.870 
Epoch 14/1000 
	 loss: 582.0630, MinusLogProbMetric: 582.0630, val_loss: 666.9597, val_MinusLogProbMetric: 666.9597

Epoch 14: val_loss did not improve from 454.34860
196/196 - 89s - loss: 582.0630 - MinusLogProbMetric: 582.0630 - val_loss: 666.9597 - val_MinusLogProbMetric: 666.9597 - lr: 1.1111e-04 - 89s/epoch - 453ms/step
Epoch 15/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 150: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 19:39:54.016 
Epoch 15/1000 
	 loss: nan, MinusLogProbMetric: 1049.3645, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 15: val_loss did not improve from 454.34860
196/196 - 70s - loss: nan - MinusLogProbMetric: 1049.3645 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 70s/epoch - 358ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 3.703703703703703e-05.
===========
Generating train data for run 368.
===========
Train data generated in 0.46 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_368
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_420"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_421 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_45 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_45/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_45'")
self.model: <keras.engine.functional.Functional object at 0x7f197efb8310>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f17201b6680>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f17201b6680>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f19c844de40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1599c38e20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1599c3a530>, <keras.callbacks.ModelCheckpoint object at 0x7f1599c3b1c0>, <keras.callbacks.EarlyStopping object at 0x7f1599c398d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1599c3bc10>, <keras.callbacks.TerminateOnNaN object at 0x7f1599c38ca0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 368/720 with hyperparameters:
timestamp = 2023-10-25 19:40:10.313136
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
2023-10-25 19:45:14.715 
Epoch 1/1000 
	 loss: 446.6693, MinusLogProbMetric: 446.6693, val_loss: 369.5869, val_MinusLogProbMetric: 369.5869

Epoch 1: val_loss improved from inf to 369.58694, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 306s - loss: 446.6693 - MinusLogProbMetric: 446.6693 - val_loss: 369.5869 - val_MinusLogProbMetric: 369.5869 - lr: 3.7037e-05 - 306s/epoch - 2s/step
Epoch 2/1000
2023-10-25 19:46:43.688 
Epoch 2/1000 
	 loss: 475.4364, MinusLogProbMetric: 475.4364, val_loss: 620.2145, val_MinusLogProbMetric: 620.2145

Epoch 2: val_loss did not improve from 369.58694
196/196 - 87s - loss: 475.4364 - MinusLogProbMetric: 475.4364 - val_loss: 620.2145 - val_MinusLogProbMetric: 620.2145 - lr: 3.7037e-05 - 87s/epoch - 444ms/step
Epoch 3/1000
2023-10-25 19:48:10.779 
Epoch 3/1000 
	 loss: 706.2275, MinusLogProbMetric: 706.2275, val_loss: 589.0498, val_MinusLogProbMetric: 589.0498

Epoch 3: val_loss did not improve from 369.58694
196/196 - 87s - loss: 706.2275 - MinusLogProbMetric: 706.2275 - val_loss: 589.0498 - val_MinusLogProbMetric: 589.0498 - lr: 3.7037e-05 - 87s/epoch - 444ms/step
Epoch 4/1000
2023-10-25 19:49:38.727 
Epoch 4/1000 
	 loss: 472.2975, MinusLogProbMetric: 472.2975, val_loss: 397.7781, val_MinusLogProbMetric: 397.7781

Epoch 4: val_loss did not improve from 369.58694
196/196 - 88s - loss: 472.2975 - MinusLogProbMetric: 472.2975 - val_loss: 397.7781 - val_MinusLogProbMetric: 397.7781 - lr: 3.7037e-05 - 88s/epoch - 449ms/step
Epoch 5/1000
2023-10-25 19:51:06.159 
Epoch 5/1000 
	 loss: 825.2542, MinusLogProbMetric: 825.2542, val_loss: 899.6880, val_MinusLogProbMetric: 899.6880

Epoch 5: val_loss did not improve from 369.58694
196/196 - 87s - loss: 825.2542 - MinusLogProbMetric: 825.2542 - val_loss: 899.6880 - val_MinusLogProbMetric: 899.6880 - lr: 3.7037e-05 - 87s/epoch - 446ms/step
Epoch 6/1000
2023-10-25 19:52:33.620 
Epoch 6/1000 
	 loss: 984.0775, MinusLogProbMetric: 984.0775, val_loss: 740.6830, val_MinusLogProbMetric: 740.6830

Epoch 6: val_loss did not improve from 369.58694
196/196 - 87s - loss: 984.0775 - MinusLogProbMetric: 984.0775 - val_loss: 740.6830 - val_MinusLogProbMetric: 740.6830 - lr: 3.7037e-05 - 87s/epoch - 446ms/step
Epoch 7/1000
2023-10-25 19:54:00.129 
Epoch 7/1000 
	 loss: 865.1935, MinusLogProbMetric: 865.1935, val_loss: 768.6822, val_MinusLogProbMetric: 768.6822

Epoch 7: val_loss did not improve from 369.58694
196/196 - 87s - loss: 865.1935 - MinusLogProbMetric: 865.1935 - val_loss: 768.6822 - val_MinusLogProbMetric: 768.6822 - lr: 3.7037e-05 - 87s/epoch - 441ms/step
Epoch 8/1000
2023-10-25 19:55:26.853 
Epoch 8/1000 
	 loss: 666.8726, MinusLogProbMetric: 666.8726, val_loss: 568.7975, val_MinusLogProbMetric: 568.7975

Epoch 8: val_loss did not improve from 369.58694
196/196 - 87s - loss: 666.8726 - MinusLogProbMetric: 666.8726 - val_loss: 568.7975 - val_MinusLogProbMetric: 568.7975 - lr: 3.7037e-05 - 87s/epoch - 442ms/step
Epoch 9/1000
2023-10-25 19:56:52.549 
Epoch 9/1000 
	 loss: 564.7025, MinusLogProbMetric: 564.7025, val_loss: 549.4257, val_MinusLogProbMetric: 549.4257

Epoch 9: val_loss did not improve from 369.58694
196/196 - 86s - loss: 564.7025 - MinusLogProbMetric: 564.7025 - val_loss: 549.4257 - val_MinusLogProbMetric: 549.4257 - lr: 3.7037e-05 - 86s/epoch - 437ms/step
Epoch 10/1000
2023-10-25 19:58:19.963 
Epoch 10/1000 
	 loss: 550.9606, MinusLogProbMetric: 550.9606, val_loss: 494.4172, val_MinusLogProbMetric: 494.4172

Epoch 10: val_loss did not improve from 369.58694
196/196 - 87s - loss: 550.9606 - MinusLogProbMetric: 550.9606 - val_loss: 494.4172 - val_MinusLogProbMetric: 494.4172 - lr: 3.7037e-05 - 87s/epoch - 446ms/step
Epoch 11/1000
2023-10-25 19:59:46.920 
Epoch 11/1000 
	 loss: 470.7678, MinusLogProbMetric: 470.7678, val_loss: 493.9747, val_MinusLogProbMetric: 493.9747

Epoch 11: val_loss did not improve from 369.58694
196/196 - 87s - loss: 470.7678 - MinusLogProbMetric: 470.7678 - val_loss: 493.9747 - val_MinusLogProbMetric: 493.9747 - lr: 3.7037e-05 - 87s/epoch - 444ms/step
Epoch 12/1000
2023-10-25 20:01:13.785 
Epoch 12/1000 
	 loss: 428.4229, MinusLogProbMetric: 428.4229, val_loss: 396.7482, val_MinusLogProbMetric: 396.7482

Epoch 12: val_loss did not improve from 369.58694
196/196 - 87s - loss: 428.4229 - MinusLogProbMetric: 428.4229 - val_loss: 396.7482 - val_MinusLogProbMetric: 396.7482 - lr: 3.7037e-05 - 87s/epoch - 443ms/step
Epoch 13/1000
2023-10-25 20:02:41.463 
Epoch 13/1000 
	 loss: 483.4054, MinusLogProbMetric: 483.4054, val_loss: 422.1618, val_MinusLogProbMetric: 422.1618

Epoch 13: val_loss did not improve from 369.58694
196/196 - 88s - loss: 483.4054 - MinusLogProbMetric: 483.4054 - val_loss: 422.1618 - val_MinusLogProbMetric: 422.1618 - lr: 3.7037e-05 - 88s/epoch - 447ms/step
Epoch 14/1000
2023-10-25 20:04:08.266 
Epoch 14/1000 
	 loss: 417.1312, MinusLogProbMetric: 417.1312, val_loss: 500.7972, val_MinusLogProbMetric: 500.7972

Epoch 14: val_loss did not improve from 369.58694
196/196 - 87s - loss: 417.1312 - MinusLogProbMetric: 417.1312 - val_loss: 500.7972 - val_MinusLogProbMetric: 500.7972 - lr: 3.7037e-05 - 87s/epoch - 443ms/step
Epoch 15/1000
2023-10-25 20:05:34.696 
Epoch 15/1000 
	 loss: 439.0144, MinusLogProbMetric: 439.0144, val_loss: 409.3739, val_MinusLogProbMetric: 409.3739

Epoch 15: val_loss did not improve from 369.58694
196/196 - 86s - loss: 439.0144 - MinusLogProbMetric: 439.0144 - val_loss: 409.3739 - val_MinusLogProbMetric: 409.3739 - lr: 3.7037e-05 - 86s/epoch - 441ms/step
Epoch 16/1000
2023-10-25 20:07:01.388 
Epoch 16/1000 
	 loss: 377.5427, MinusLogProbMetric: 377.5427, val_loss: 355.2685, val_MinusLogProbMetric: 355.2685

Epoch 16: val_loss improved from 369.58694 to 355.26849, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 88s - loss: 377.5427 - MinusLogProbMetric: 377.5427 - val_loss: 355.2685 - val_MinusLogProbMetric: 355.2685 - lr: 3.7037e-05 - 88s/epoch - 451ms/step
Epoch 17/1000
2023-10-25 20:08:29.862 
Epoch 17/1000 
	 loss: 344.1116, MinusLogProbMetric: 344.1116, val_loss: 333.9458, val_MinusLogProbMetric: 333.9458

Epoch 17: val_loss improved from 355.26849 to 333.94577, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 88s - loss: 344.1116 - MinusLogProbMetric: 344.1116 - val_loss: 333.9458 - val_MinusLogProbMetric: 333.9458 - lr: 3.7037e-05 - 88s/epoch - 450ms/step
Epoch 18/1000
2023-10-25 20:09:58.532 
Epoch 18/1000 
	 loss: 325.1898, MinusLogProbMetric: 325.1898, val_loss: 328.6799, val_MinusLogProbMetric: 328.6799

Epoch 18: val_loss improved from 333.94577 to 328.67993, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 325.1898 - MinusLogProbMetric: 325.1898 - val_loss: 328.6799 - val_MinusLogProbMetric: 328.6799 - lr: 3.7037e-05 - 89s/epoch - 453ms/step
Epoch 19/1000
2023-10-25 20:11:27.529 
Epoch 19/1000 
	 loss: 306.8546, MinusLogProbMetric: 306.8546, val_loss: 298.4737, val_MinusLogProbMetric: 298.4737

Epoch 19: val_loss improved from 328.67993 to 298.47369, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 306.8546 - MinusLogProbMetric: 306.8546 - val_loss: 298.4737 - val_MinusLogProbMetric: 298.4737 - lr: 3.7037e-05 - 89s/epoch - 454ms/step
Epoch 20/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 81: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 20:12:09.199 
Epoch 20/1000 
	 loss: nan, MinusLogProbMetric: 295.3770, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 20: val_loss did not improve from 298.47369
196/196 - 40s - loss: nan - MinusLogProbMetric: 295.3770 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 40s/epoch - 203ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 1.2345679012345677e-05.
===========
Generating train data for run 368.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_368
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_431"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_432 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_46 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_46/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_46'")
self.model: <keras.engine.functional.Functional object at 0x7f178d833430>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f15a8b1cb50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f15a8b1cb50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1598b1fcd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f15e925dba0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f14783eb520>, <keras.callbacks.ModelCheckpoint object at 0x7f14783eb670>, <keras.callbacks.EarlyStopping object at 0x7f14783ebb80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f14783ebca0>, <keras.callbacks.TerminateOnNaN object at 0x7f14783eb5b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 368/720 with hyperparameters:
timestamp = 2023-10-25 20:12:39.110649
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
2023-10-25 20:17:56.101 
Epoch 1/1000 
	 loss: 399.5173, MinusLogProbMetric: 399.5173, val_loss: 372.8504, val_MinusLogProbMetric: 372.8504

Epoch 1: val_loss improved from inf to 372.85040, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 318s - loss: 399.5173 - MinusLogProbMetric: 399.5173 - val_loss: 372.8504 - val_MinusLogProbMetric: 372.8504 - lr: 1.2346e-05 - 318s/epoch - 2s/step
Epoch 2/1000
2023-10-25 20:19:26.270 
Epoch 2/1000 
	 loss: 379.4990, MinusLogProbMetric: 379.4990, val_loss: 405.3560, val_MinusLogProbMetric: 405.3560

Epoch 2: val_loss did not improve from 372.85040
196/196 - 88s - loss: 379.4990 - MinusLogProbMetric: 379.4990 - val_loss: 405.3560 - val_MinusLogProbMetric: 405.3560 - lr: 1.2346e-05 - 88s/epoch - 450ms/step
Epoch 3/1000
2023-10-25 20:20:53.661 
Epoch 3/1000 
	 loss: 339.5224, MinusLogProbMetric: 339.5224, val_loss: 315.3372, val_MinusLogProbMetric: 315.3372

Epoch 3: val_loss improved from 372.85040 to 315.33716, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 339.5224 - MinusLogProbMetric: 339.5224 - val_loss: 315.3372 - val_MinusLogProbMetric: 315.3372 - lr: 1.2346e-05 - 89s/epoch - 455ms/step
Epoch 4/1000
2023-10-25 20:22:23.044 
Epoch 4/1000 
	 loss: 303.8366, MinusLogProbMetric: 303.8366, val_loss: 294.0553, val_MinusLogProbMetric: 294.0553

Epoch 4: val_loss improved from 315.33716 to 294.05527, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 303.8366 - MinusLogProbMetric: 303.8366 - val_loss: 294.0553 - val_MinusLogProbMetric: 294.0553 - lr: 1.2346e-05 - 89s/epoch - 455ms/step
Epoch 5/1000
2023-10-25 20:23:52.566 
Epoch 5/1000 
	 loss: 290.4467, MinusLogProbMetric: 290.4467, val_loss: 284.1944, val_MinusLogProbMetric: 284.1944

Epoch 5: val_loss improved from 294.05527 to 284.19440, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 290.4467 - MinusLogProbMetric: 290.4467 - val_loss: 284.1944 - val_MinusLogProbMetric: 284.1944 - lr: 1.2346e-05 - 89s/epoch - 456ms/step
Epoch 6/1000
2023-10-25 20:25:21.320 
Epoch 6/1000 
	 loss: 278.5258, MinusLogProbMetric: 278.5258, val_loss: 294.0879, val_MinusLogProbMetric: 294.0879

Epoch 6: val_loss did not improve from 284.19440
196/196 - 87s - loss: 278.5258 - MinusLogProbMetric: 278.5258 - val_loss: 294.0879 - val_MinusLogProbMetric: 294.0879 - lr: 1.2346e-05 - 87s/epoch - 446ms/step
Epoch 7/1000
2023-10-25 20:26:49.278 
Epoch 7/1000 
	 loss: 291.7941, MinusLogProbMetric: 291.7941, val_loss: 276.6365, val_MinusLogProbMetric: 276.6365

Epoch 7: val_loss improved from 284.19440 to 276.63647, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 291.7941 - MinusLogProbMetric: 291.7941 - val_loss: 276.6365 - val_MinusLogProbMetric: 276.6365 - lr: 1.2346e-05 - 89s/epoch - 456ms/step
Epoch 8/1000
2023-10-25 20:28:18.728 
Epoch 8/1000 
	 loss: 286.9136, MinusLogProbMetric: 286.9136, val_loss: 265.9903, val_MinusLogProbMetric: 265.9903

Epoch 8: val_loss improved from 276.63647 to 265.99026, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 286.9136 - MinusLogProbMetric: 286.9136 - val_loss: 265.9903 - val_MinusLogProbMetric: 265.9903 - lr: 1.2346e-05 - 90s/epoch - 457ms/step
Epoch 9/1000
2023-10-25 20:29:47.627 
Epoch 9/1000 
	 loss: 259.9281, MinusLogProbMetric: 259.9281, val_loss: 252.2731, val_MinusLogProbMetric: 252.2731

Epoch 9: val_loss improved from 265.99026 to 252.27312, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 259.9281 - MinusLogProbMetric: 259.9281 - val_loss: 252.2731 - val_MinusLogProbMetric: 252.2731 - lr: 1.2346e-05 - 89s/epoch - 452ms/step
Epoch 10/1000
2023-10-25 20:31:16.059 
Epoch 10/1000 
	 loss: 247.2674, MinusLogProbMetric: 247.2674, val_loss: 242.0186, val_MinusLogProbMetric: 242.0186

Epoch 10: val_loss improved from 252.27312 to 242.01857, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 88s - loss: 247.2674 - MinusLogProbMetric: 247.2674 - val_loss: 242.0186 - val_MinusLogProbMetric: 242.0186 - lr: 1.2346e-05 - 88s/epoch - 451ms/step
Epoch 11/1000
2023-10-25 20:32:44.837 
Epoch 11/1000 
	 loss: 266.7296, MinusLogProbMetric: 266.7296, val_loss: 278.9830, val_MinusLogProbMetric: 278.9830

Epoch 11: val_loss did not improve from 242.01857
196/196 - 88s - loss: 266.7296 - MinusLogProbMetric: 266.7296 - val_loss: 278.9830 - val_MinusLogProbMetric: 278.9830 - lr: 1.2346e-05 - 88s/epoch - 447ms/step
Epoch 12/1000
2023-10-25 20:34:12.539 
Epoch 12/1000 
	 loss: 265.8998, MinusLogProbMetric: 265.8998, val_loss: 246.2086, val_MinusLogProbMetric: 246.2086

Epoch 12: val_loss did not improve from 242.01857
196/196 - 88s - loss: 265.8998 - MinusLogProbMetric: 265.8998 - val_loss: 246.2086 - val_MinusLogProbMetric: 246.2086 - lr: 1.2346e-05 - 88s/epoch - 447ms/step
Epoch 13/1000
2023-10-25 20:35:39.788 
Epoch 13/1000 
	 loss: 238.6176, MinusLogProbMetric: 238.6176, val_loss: 232.0058, val_MinusLogProbMetric: 232.0058

Epoch 13: val_loss improved from 242.01857 to 232.00578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 238.6176 - MinusLogProbMetric: 238.6176 - val_loss: 232.0058 - val_MinusLogProbMetric: 232.0058 - lr: 1.2346e-05 - 89s/epoch - 452ms/step
Epoch 14/1000
2023-10-25 20:37:08.099 
Epoch 14/1000 
	 loss: 228.0622, MinusLogProbMetric: 228.0622, val_loss: 223.4832, val_MinusLogProbMetric: 223.4832

Epoch 14: val_loss improved from 232.00578 to 223.48318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 228.0622 - MinusLogProbMetric: 228.0622 - val_loss: 223.4832 - val_MinusLogProbMetric: 223.4832 - lr: 1.2346e-05 - 89s/epoch - 452ms/step
Epoch 15/1000
2023-10-25 20:38:37.554 
Epoch 15/1000 
	 loss: 362.6571, MinusLogProbMetric: 362.6571, val_loss: 678.3593, val_MinusLogProbMetric: 678.3593

Epoch 15: val_loss did not improve from 223.48318
196/196 - 88s - loss: 362.6571 - MinusLogProbMetric: 362.6571 - val_loss: 678.3593 - val_MinusLogProbMetric: 678.3593 - lr: 1.2346e-05 - 88s/epoch - 448ms/step
Epoch 16/1000
2023-10-25 20:40:05.050 
Epoch 16/1000 
	 loss: 490.6574, MinusLogProbMetric: 490.6574, val_loss: 398.0568, val_MinusLogProbMetric: 398.0568

Epoch 16: val_loss did not improve from 223.48318
196/196 - 87s - loss: 490.6574 - MinusLogProbMetric: 490.6574 - val_loss: 398.0568 - val_MinusLogProbMetric: 398.0568 - lr: 1.2346e-05 - 87s/epoch - 446ms/step
Epoch 17/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 70: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 20:40:40.314 
Epoch 17/1000 
	 loss: nan, MinusLogProbMetric: 424.2388, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 17: val_loss did not improve from 223.48318
196/196 - 35s - loss: nan - MinusLogProbMetric: 424.2388 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 35s/epoch - 180ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 4.115226337448558e-06.
===========
Generating train data for run 368.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_368
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_442"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_443 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_47 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_47/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_47'")
self.model: <keras.engine.functional.Functional object at 0x7f1757411e40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1756b02140>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1756b02140>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1756bbafb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1756a860b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1756a86620>, <keras.callbacks.ModelCheckpoint object at 0x7f1756a866e0>, <keras.callbacks.EarlyStopping object at 0x7f1756a86950>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1756a86980>, <keras.callbacks.TerminateOnNaN object at 0x7f1756a865c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 368/720 with hyperparameters:
timestamp = 2023-10-25 20:40:55.686030
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
2023-10-25 20:46:28.184 
Epoch 1/1000 
	 loss: 219.2644, MinusLogProbMetric: 219.2644, val_loss: 211.4412, val_MinusLogProbMetric: 211.4412

Epoch 1: val_loss improved from inf to 211.44116, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 334s - loss: 219.2644 - MinusLogProbMetric: 219.2644 - val_loss: 211.4412 - val_MinusLogProbMetric: 211.4412 - lr: 4.1152e-06 - 334s/epoch - 2s/step
Epoch 2/1000
2023-10-25 20:47:57.170 
Epoch 2/1000 
	 loss: 207.7130, MinusLogProbMetric: 207.7130, val_loss: 201.9224, val_MinusLogProbMetric: 201.9224

Epoch 2: val_loss improved from 211.44116 to 201.92241, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 207.7130 - MinusLogProbMetric: 207.7130 - val_loss: 201.9224 - val_MinusLogProbMetric: 201.9224 - lr: 4.1152e-06 - 89s/epoch - 452ms/step
Epoch 3/1000
2023-10-25 20:49:26.562 
Epoch 3/1000 
	 loss: 199.1581, MinusLogProbMetric: 199.1581, val_loss: 193.4684, val_MinusLogProbMetric: 193.4684

Epoch 3: val_loss improved from 201.92241 to 193.46843, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 199.1581 - MinusLogProbMetric: 199.1581 - val_loss: 193.4684 - val_MinusLogProbMetric: 193.4684 - lr: 4.1152e-06 - 89s/epoch - 457ms/step
Epoch 4/1000
2023-10-25 20:50:55.638 
Epoch 4/1000 
	 loss: 188.8650, MinusLogProbMetric: 188.8650, val_loss: 185.1322, val_MinusLogProbMetric: 185.1322

Epoch 4: val_loss improved from 193.46843 to 185.13217, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 188.8650 - MinusLogProbMetric: 188.8650 - val_loss: 185.1322 - val_MinusLogProbMetric: 185.1322 - lr: 4.1152e-06 - 89s/epoch - 455ms/step
Epoch 5/1000
2023-10-25 20:52:25.143 
Epoch 5/1000 
	 loss: 181.8643, MinusLogProbMetric: 181.8643, val_loss: 178.6085, val_MinusLogProbMetric: 178.6085

Epoch 5: val_loss improved from 185.13217 to 178.60851, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 181.8643 - MinusLogProbMetric: 181.8643 - val_loss: 178.6085 - val_MinusLogProbMetric: 178.6085 - lr: 4.1152e-06 - 89s/epoch - 457ms/step
Epoch 6/1000
2023-10-25 20:53:53.823 
Epoch 6/1000 
	 loss: 176.5652, MinusLogProbMetric: 176.5652, val_loss: 171.7970, val_MinusLogProbMetric: 171.7970

Epoch 6: val_loss improved from 178.60851 to 171.79695, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 176.5652 - MinusLogProbMetric: 176.5652 - val_loss: 171.7970 - val_MinusLogProbMetric: 171.7970 - lr: 4.1152e-06 - 89s/epoch - 453ms/step
Epoch 7/1000
2023-10-25 20:55:22.721 
Epoch 7/1000 
	 loss: 170.8703, MinusLogProbMetric: 170.8703, val_loss: 169.2615, val_MinusLogProbMetric: 169.2615

Epoch 7: val_loss improved from 171.79695 to 169.26149, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 88s - loss: 170.8703 - MinusLogProbMetric: 170.8703 - val_loss: 169.2615 - val_MinusLogProbMetric: 169.2615 - lr: 4.1152e-06 - 88s/epoch - 451ms/step
Epoch 8/1000
2023-10-25 20:56:51.471 
Epoch 8/1000 
	 loss: 167.0257, MinusLogProbMetric: 167.0257, val_loss: 164.1718, val_MinusLogProbMetric: 164.1718

Epoch 8: val_loss improved from 169.26149 to 164.17184, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 167.0257 - MinusLogProbMetric: 167.0257 - val_loss: 164.1718 - val_MinusLogProbMetric: 164.1718 - lr: 4.1152e-06 - 89s/epoch - 455ms/step
Epoch 9/1000
2023-10-25 20:58:20.916 
Epoch 9/1000 
	 loss: 163.8414, MinusLogProbMetric: 163.8414, val_loss: 163.0207, val_MinusLogProbMetric: 163.0207

Epoch 9: val_loss improved from 164.17184 to 163.02066, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 163.8414 - MinusLogProbMetric: 163.8414 - val_loss: 163.0207 - val_MinusLogProbMetric: 163.0207 - lr: 4.1152e-06 - 89s/epoch - 454ms/step
Epoch 10/1000
2023-10-25 20:59:49.273 
Epoch 10/1000 
	 loss: 159.7527, MinusLogProbMetric: 159.7527, val_loss: 156.5794, val_MinusLogProbMetric: 156.5794

Epoch 10: val_loss improved from 163.02066 to 156.57938, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 159.7527 - MinusLogProbMetric: 159.7527 - val_loss: 156.5794 - val_MinusLogProbMetric: 156.5794 - lr: 4.1152e-06 - 89s/epoch - 452ms/step
Epoch 11/1000
2023-10-25 21:01:18.762 
Epoch 11/1000 
	 loss: 159.6073, MinusLogProbMetric: 159.6073, val_loss: 156.1164, val_MinusLogProbMetric: 156.1164

Epoch 11: val_loss improved from 156.57938 to 156.11638, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 159.6073 - MinusLogProbMetric: 159.6073 - val_loss: 156.1164 - val_MinusLogProbMetric: 156.1164 - lr: 4.1152e-06 - 89s/epoch - 456ms/step
Epoch 12/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 123: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 21:02:18.326 
Epoch 12/1000 
	 loss: nan, MinusLogProbMetric: 207.2720, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 12: val_loss did not improve from 156.11638
196/196 - 58s - loss: nan - MinusLogProbMetric: 207.2720 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 58s/epoch - 296ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 1.3717421124828526e-06.
===========
Generating train data for run 368.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_368
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_453"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_454 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_48 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_48/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_48'")
self.model: <keras.engine.functional.Functional object at 0x7f15e9f10910>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f17d2303520>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f17d2303520>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f16c84b53c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f16c84f4e20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f16c84f5390>, <keras.callbacks.ModelCheckpoint object at 0x7f16c84f5450>, <keras.callbacks.EarlyStopping object at 0x7f16c84f56c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f16c84f56f0>, <keras.callbacks.TerminateOnNaN object at 0x7f16c84f5330>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 368/720 with hyperparameters:
timestamp = 2023-10-25 21:02:31.708862
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
2023-10-25 21:08:06.088 
Epoch 1/1000 
	 loss: 154.2357, MinusLogProbMetric: 154.2357, val_loss: 152.4728, val_MinusLogProbMetric: 152.4728

Epoch 1: val_loss improved from inf to 152.47279, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 336s - loss: 154.2357 - MinusLogProbMetric: 154.2357 - val_loss: 152.4728 - val_MinusLogProbMetric: 152.4728 - lr: 1.3717e-06 - 336s/epoch - 2s/step
Epoch 2/1000
2023-10-25 21:09:38.521 
Epoch 2/1000 
	 loss: 151.1536, MinusLogProbMetric: 151.1536, val_loss: 152.4368, val_MinusLogProbMetric: 152.4368

Epoch 2: val_loss improved from 152.47279 to 152.43675, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 151.1536 - MinusLogProbMetric: 151.1536 - val_loss: 152.4368 - val_MinusLogProbMetric: 152.4368 - lr: 1.3717e-06 - 92s/epoch - 468ms/step
Epoch 3/1000
2023-10-25 21:11:09.878 
Epoch 3/1000 
	 loss: 149.6237, MinusLogProbMetric: 149.6237, val_loss: 148.6532, val_MinusLogProbMetric: 148.6532

Epoch 3: val_loss improved from 152.43675 to 148.65318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 149.6237 - MinusLogProbMetric: 149.6237 - val_loss: 148.6532 - val_MinusLogProbMetric: 148.6532 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 4/1000
2023-10-25 21:12:40.481 
Epoch 4/1000 
	 loss: 148.1378, MinusLogProbMetric: 148.1378, val_loss: 147.4785, val_MinusLogProbMetric: 147.4785

Epoch 4: val_loss improved from 148.65318 to 147.47845, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 148.1378 - MinusLogProbMetric: 148.1378 - val_loss: 147.4785 - val_MinusLogProbMetric: 147.4785 - lr: 1.3717e-06 - 91s/epoch - 462ms/step
Epoch 5/1000
2023-10-25 21:14:11.842 
Epoch 5/1000 
	 loss: 147.2267, MinusLogProbMetric: 147.2267, val_loss: 146.2625, val_MinusLogProbMetric: 146.2625

Epoch 5: val_loss improved from 147.47845 to 146.26247, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 147.2267 - MinusLogProbMetric: 147.2267 - val_loss: 146.2625 - val_MinusLogProbMetric: 146.2625 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 6/1000
2023-10-25 21:15:43.266 
Epoch 6/1000 
	 loss: 147.6017, MinusLogProbMetric: 147.6017, val_loss: 150.8307, val_MinusLogProbMetric: 150.8307

Epoch 6: val_loss did not improve from 146.26247
196/196 - 90s - loss: 147.6017 - MinusLogProbMetric: 147.6017 - val_loss: 150.8307 - val_MinusLogProbMetric: 150.8307 - lr: 1.3717e-06 - 90s/epoch - 458ms/step
Epoch 7/1000
2023-10-25 21:17:12.639 
Epoch 7/1000 
	 loss: 149.8929, MinusLogProbMetric: 149.8929, val_loss: 147.9342, val_MinusLogProbMetric: 147.9342

Epoch 7: val_loss did not improve from 146.26247
196/196 - 89s - loss: 149.8929 - MinusLogProbMetric: 149.8929 - val_loss: 147.9342 - val_MinusLogProbMetric: 147.9342 - lr: 1.3717e-06 - 89s/epoch - 456ms/step
Epoch 8/1000
2023-10-25 21:18:42.024 
Epoch 8/1000 
	 loss: 146.8684, MinusLogProbMetric: 146.8684, val_loss: 146.0764, val_MinusLogProbMetric: 146.0764

Epoch 8: val_loss improved from 146.26247 to 146.07635, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 146.8684 - MinusLogProbMetric: 146.8684 - val_loss: 146.0764 - val_MinusLogProbMetric: 146.0764 - lr: 1.3717e-06 - 91s/epoch - 464ms/step
Epoch 9/1000
2023-10-25 21:20:12.242 
Epoch 9/1000 
	 loss: 145.8223, MinusLogProbMetric: 145.8223, val_loss: 145.6112, val_MinusLogProbMetric: 145.6112

Epoch 9: val_loss improved from 146.07635 to 145.61122, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 145.8223 - MinusLogProbMetric: 145.8223 - val_loss: 145.6112 - val_MinusLogProbMetric: 145.6112 - lr: 1.3717e-06 - 91s/epoch - 463ms/step
Epoch 10/1000
2023-10-25 21:21:42.683 
Epoch 10/1000 
	 loss: 144.6104, MinusLogProbMetric: 144.6104, val_loss: 144.1942, val_MinusLogProbMetric: 144.1942

Epoch 10: val_loss improved from 145.61122 to 144.19421, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 144.6104 - MinusLogProbMetric: 144.6104 - val_loss: 144.1942 - val_MinusLogProbMetric: 144.1942 - lr: 1.3717e-06 - 90s/epoch - 460ms/step
Epoch 11/1000
2023-10-25 21:23:14.004 
Epoch 11/1000 
	 loss: 143.7210, MinusLogProbMetric: 143.7210, val_loss: 143.2964, val_MinusLogProbMetric: 143.2964

Epoch 11: val_loss improved from 144.19421 to 143.29636, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 143.7210 - MinusLogProbMetric: 143.7210 - val_loss: 143.2964 - val_MinusLogProbMetric: 143.2964 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 12/1000
2023-10-25 21:24:45.114 
Epoch 12/1000 
	 loss: 147.5237, MinusLogProbMetric: 147.5237, val_loss: 143.2310, val_MinusLogProbMetric: 143.2310

Epoch 12: val_loss improved from 143.29636 to 143.23096, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 147.5237 - MinusLogProbMetric: 147.5237 - val_loss: 143.2310 - val_MinusLogProbMetric: 143.2310 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 13/1000
2023-10-25 21:26:16.304 
Epoch 13/1000 
	 loss: 142.3443, MinusLogProbMetric: 142.3443, val_loss: 142.2272, val_MinusLogProbMetric: 142.2272

Epoch 13: val_loss improved from 143.23096 to 142.22717, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 142.3443 - MinusLogProbMetric: 142.3443 - val_loss: 142.2272 - val_MinusLogProbMetric: 142.2272 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 14/1000
2023-10-25 21:27:47.793 
Epoch 14/1000 
	 loss: 141.6579, MinusLogProbMetric: 141.6579, val_loss: 141.5261, val_MinusLogProbMetric: 141.5261

Epoch 14: val_loss improved from 142.22717 to 141.52606, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 141.6579 - MinusLogProbMetric: 141.6579 - val_loss: 141.5261 - val_MinusLogProbMetric: 141.5261 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 15/1000
2023-10-25 21:29:18.699 
Epoch 15/1000 
	 loss: 141.5056, MinusLogProbMetric: 141.5056, val_loss: 142.6287, val_MinusLogProbMetric: 142.6287

Epoch 15: val_loss did not improve from 141.52606
196/196 - 89s - loss: 141.5056 - MinusLogProbMetric: 141.5056 - val_loss: 142.6287 - val_MinusLogProbMetric: 142.6287 - lr: 1.3717e-06 - 89s/epoch - 455ms/step
Epoch 16/1000
2023-10-25 21:30:47.289 
Epoch 16/1000 
	 loss: 141.1830, MinusLogProbMetric: 141.1830, val_loss: 140.8273, val_MinusLogProbMetric: 140.8273

Epoch 16: val_loss improved from 141.52606 to 140.82735, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 141.1830 - MinusLogProbMetric: 141.1830 - val_loss: 140.8273 - val_MinusLogProbMetric: 140.8273 - lr: 1.3717e-06 - 90s/epoch - 461ms/step
Epoch 17/1000
2023-10-25 21:32:17.489 
Epoch 17/1000 
	 loss: 140.1804, MinusLogProbMetric: 140.1804, val_loss: 140.1122, val_MinusLogProbMetric: 140.1122

Epoch 17: val_loss improved from 140.82735 to 140.11223, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 140.1804 - MinusLogProbMetric: 140.1804 - val_loss: 140.1122 - val_MinusLogProbMetric: 140.1122 - lr: 1.3717e-06 - 90s/epoch - 461ms/step
Epoch 18/1000
2023-10-25 21:33:48.285 
Epoch 18/1000 
	 loss: 139.5181, MinusLogProbMetric: 139.5181, val_loss: 139.5909, val_MinusLogProbMetric: 139.5909

Epoch 18: val_loss improved from 140.11223 to 139.59085, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 139.5181 - MinusLogProbMetric: 139.5181 - val_loss: 139.5909 - val_MinusLogProbMetric: 139.5909 - lr: 1.3717e-06 - 91s/epoch - 463ms/step
Epoch 19/1000
2023-10-25 21:35:19.744 
Epoch 19/1000 
	 loss: 145.1434, MinusLogProbMetric: 145.1434, val_loss: 157.6201, val_MinusLogProbMetric: 157.6201

Epoch 19: val_loss did not improve from 139.59085
196/196 - 89s - loss: 145.1434 - MinusLogProbMetric: 145.1434 - val_loss: 157.6201 - val_MinusLogProbMetric: 157.6201 - lr: 1.3717e-06 - 89s/epoch - 457ms/step
Epoch 20/1000
2023-10-25 21:36:48.639 
Epoch 20/1000 
	 loss: 146.3178, MinusLogProbMetric: 146.3178, val_loss: 142.2379, val_MinusLogProbMetric: 142.2379

Epoch 20: val_loss did not improve from 139.59085
196/196 - 89s - loss: 146.3178 - MinusLogProbMetric: 146.3178 - val_loss: 142.2379 - val_MinusLogProbMetric: 142.2379 - lr: 1.3717e-06 - 89s/epoch - 454ms/step
Epoch 21/1000
2023-10-25 21:38:18.086 
Epoch 21/1000 
	 loss: 141.7088, MinusLogProbMetric: 141.7088, val_loss: 140.8734, val_MinusLogProbMetric: 140.8734

Epoch 21: val_loss did not improve from 139.59085
196/196 - 89s - loss: 141.7088 - MinusLogProbMetric: 141.7088 - val_loss: 140.8734 - val_MinusLogProbMetric: 140.8734 - lr: 1.3717e-06 - 89s/epoch - 456ms/step
Epoch 22/1000
2023-10-25 21:39:47.083 
Epoch 22/1000 
	 loss: 140.0667, MinusLogProbMetric: 140.0667, val_loss: 139.4635, val_MinusLogProbMetric: 139.4635

Epoch 22: val_loss improved from 139.59085 to 139.46347, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 140.0667 - MinusLogProbMetric: 140.0667 - val_loss: 139.4635 - val_MinusLogProbMetric: 139.4635 - lr: 1.3717e-06 - 91s/epoch - 463ms/step
Epoch 23/1000
2023-10-25 21:41:18.795 
Epoch 23/1000 
	 loss: 172.3999, MinusLogProbMetric: 172.3999, val_loss: 169.3061, val_MinusLogProbMetric: 169.3061

Epoch 23: val_loss did not improve from 139.46347
196/196 - 90s - loss: 172.3999 - MinusLogProbMetric: 172.3999 - val_loss: 169.3061 - val_MinusLogProbMetric: 169.3061 - lr: 1.3717e-06 - 90s/epoch - 459ms/step
Epoch 24/1000
2023-10-25 21:42:48.732 
Epoch 24/1000 
	 loss: 159.1673, MinusLogProbMetric: 159.1673, val_loss: 153.7750, val_MinusLogProbMetric: 153.7750

Epoch 24: val_loss did not improve from 139.46347
196/196 - 90s - loss: 159.1673 - MinusLogProbMetric: 159.1673 - val_loss: 153.7750 - val_MinusLogProbMetric: 153.7750 - lr: 1.3717e-06 - 90s/epoch - 459ms/step
Epoch 25/1000
2023-10-25 21:44:17.681 
Epoch 25/1000 
	 loss: 150.9343, MinusLogProbMetric: 150.9343, val_loss: 148.5597, val_MinusLogProbMetric: 148.5597

Epoch 25: val_loss did not improve from 139.46347
196/196 - 89s - loss: 150.9343 - MinusLogProbMetric: 150.9343 - val_loss: 148.5597 - val_MinusLogProbMetric: 148.5597 - lr: 1.3717e-06 - 89s/epoch - 454ms/step
Epoch 26/1000
2023-10-25 21:45:46.393 
Epoch 26/1000 
	 loss: 147.0237, MinusLogProbMetric: 147.0237, val_loss: 145.8210, val_MinusLogProbMetric: 145.8210

Epoch 26: val_loss did not improve from 139.46347
196/196 - 89s - loss: 147.0237 - MinusLogProbMetric: 147.0237 - val_loss: 145.8210 - val_MinusLogProbMetric: 145.8210 - lr: 1.3717e-06 - 89s/epoch - 453ms/step
Epoch 27/1000
2023-10-25 21:47:15.682 
Epoch 27/1000 
	 loss: 149.1989, MinusLogProbMetric: 149.1989, val_loss: 161.4720, val_MinusLogProbMetric: 161.4720

Epoch 27: val_loss did not improve from 139.46347
196/196 - 89s - loss: 149.1989 - MinusLogProbMetric: 149.1989 - val_loss: 161.4720 - val_MinusLogProbMetric: 161.4720 - lr: 1.3717e-06 - 89s/epoch - 456ms/step
Epoch 28/1000
2023-10-25 21:48:45.203 
Epoch 28/1000 
	 loss: 151.2897, MinusLogProbMetric: 151.2897, val_loss: 147.1315, val_MinusLogProbMetric: 147.1315

Epoch 28: val_loss did not improve from 139.46347
196/196 - 90s - loss: 151.2897 - MinusLogProbMetric: 151.2897 - val_loss: 147.1315 - val_MinusLogProbMetric: 147.1315 - lr: 1.3717e-06 - 90s/epoch - 457ms/step
Epoch 29/1000
2023-10-25 21:50:14.193 
Epoch 29/1000 
	 loss: 146.1078, MinusLogProbMetric: 146.1078, val_loss: 145.0148, val_MinusLogProbMetric: 145.0148

Epoch 29: val_loss did not improve from 139.46347
196/196 - 89s - loss: 146.1078 - MinusLogProbMetric: 146.1078 - val_loss: 145.0148 - val_MinusLogProbMetric: 145.0148 - lr: 1.3717e-06 - 89s/epoch - 454ms/step
Epoch 30/1000
2023-10-25 21:51:43.442 
Epoch 30/1000 
	 loss: 144.0605, MinusLogProbMetric: 144.0605, val_loss: 143.3035, val_MinusLogProbMetric: 143.3035

Epoch 30: val_loss did not improve from 139.46347
196/196 - 89s - loss: 144.0605 - MinusLogProbMetric: 144.0605 - val_loss: 143.3035 - val_MinusLogProbMetric: 143.3035 - lr: 1.3717e-06 - 89s/epoch - 455ms/step
Epoch 31/1000
2023-10-25 21:53:12.860 
Epoch 31/1000 
	 loss: 142.6061, MinusLogProbMetric: 142.6061, val_loss: 142.0537, val_MinusLogProbMetric: 142.0537

Epoch 31: val_loss did not improve from 139.46347
196/196 - 89s - loss: 142.6061 - MinusLogProbMetric: 142.6061 - val_loss: 142.0537 - val_MinusLogProbMetric: 142.0537 - lr: 1.3717e-06 - 89s/epoch - 456ms/step
Epoch 32/1000
2023-10-25 21:54:40.800 
Epoch 32/1000 
	 loss: 141.4135, MinusLogProbMetric: 141.4135, val_loss: 140.9051, val_MinusLogProbMetric: 140.9051

Epoch 32: val_loss did not improve from 139.46347
196/196 - 88s - loss: 141.4135 - MinusLogProbMetric: 141.4135 - val_loss: 140.9051 - val_MinusLogProbMetric: 140.9051 - lr: 1.3717e-06 - 88s/epoch - 449ms/step
Epoch 33/1000
2023-10-25 21:56:09.223 
Epoch 33/1000 
	 loss: 140.4301, MinusLogProbMetric: 140.4301, val_loss: 140.1116, val_MinusLogProbMetric: 140.1116

Epoch 33: val_loss did not improve from 139.46347
196/196 - 88s - loss: 140.4301 - MinusLogProbMetric: 140.4301 - val_loss: 140.1116 - val_MinusLogProbMetric: 140.1116 - lr: 1.3717e-06 - 88s/epoch - 451ms/step
Epoch 34/1000
2023-10-25 21:57:38.297 
Epoch 34/1000 
	 loss: 139.6850, MinusLogProbMetric: 139.6850, val_loss: 139.4745, val_MinusLogProbMetric: 139.4745

Epoch 34: val_loss did not improve from 139.46347
196/196 - 89s - loss: 139.6850 - MinusLogProbMetric: 139.6850 - val_loss: 139.4745 - val_MinusLogProbMetric: 139.4745 - lr: 1.3717e-06 - 89s/epoch - 454ms/step
Epoch 35/1000
2023-10-25 21:59:06.509 
Epoch 35/1000 
	 loss: 139.1157, MinusLogProbMetric: 139.1157, val_loss: 138.9413, val_MinusLogProbMetric: 138.9413

Epoch 35: val_loss improved from 139.46347 to 138.94130, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 139.1157 - MinusLogProbMetric: 139.1157 - val_loss: 138.9413 - val_MinusLogProbMetric: 138.9413 - lr: 1.3717e-06 - 90s/epoch - 460ms/step
Epoch 36/1000
2023-10-25 22:00:36.010 
Epoch 36/1000 
	 loss: 138.7732, MinusLogProbMetric: 138.7732, val_loss: 138.7410, val_MinusLogProbMetric: 138.7410

Epoch 36: val_loss improved from 138.94130 to 138.74101, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 138.7732 - MinusLogProbMetric: 138.7732 - val_loss: 138.7410 - val_MinusLogProbMetric: 138.7410 - lr: 1.3717e-06 - 89s/epoch - 455ms/step
Epoch 37/1000
2023-10-25 22:02:05.737 
Epoch 37/1000 
	 loss: 138.6834, MinusLogProbMetric: 138.6834, val_loss: 138.1033, val_MinusLogProbMetric: 138.1033

Epoch 37: val_loss improved from 138.74101 to 138.10329, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 138.6834 - MinusLogProbMetric: 138.6834 - val_loss: 138.1033 - val_MinusLogProbMetric: 138.1033 - lr: 1.3717e-06 - 90s/epoch - 459ms/step
Epoch 38/1000
2023-10-25 22:03:35.245 
Epoch 38/1000 
	 loss: 137.8091, MinusLogProbMetric: 137.8091, val_loss: 137.5983, val_MinusLogProbMetric: 137.5983

Epoch 38: val_loss improved from 138.10329 to 137.59833, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 137.8091 - MinusLogProbMetric: 137.8091 - val_loss: 137.5983 - val_MinusLogProbMetric: 137.5983 - lr: 1.3717e-06 - 90s/epoch - 457ms/step
Epoch 39/1000
2023-10-25 22:05:05.854 
Epoch 39/1000 
	 loss: 137.4253, MinusLogProbMetric: 137.4253, val_loss: 137.7037, val_MinusLogProbMetric: 137.7037

Epoch 39: val_loss did not improve from 137.59833
196/196 - 89s - loss: 137.4253 - MinusLogProbMetric: 137.4253 - val_loss: 137.7037 - val_MinusLogProbMetric: 137.7037 - lr: 1.3717e-06 - 89s/epoch - 453ms/step
Epoch 40/1000
2023-10-25 22:06:33.050 
Epoch 40/1000 
	 loss: 137.6681, MinusLogProbMetric: 137.6681, val_loss: 136.8676, val_MinusLogProbMetric: 136.8676

Epoch 40: val_loss improved from 137.59833 to 136.86758, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 137.6681 - MinusLogProbMetric: 137.6681 - val_loss: 136.8676 - val_MinusLogProbMetric: 136.8676 - lr: 1.3717e-06 - 89s/epoch - 454ms/step
Epoch 41/1000
2023-10-25 22:08:02.359 
Epoch 41/1000 
	 loss: 136.4915, MinusLogProbMetric: 136.4915, val_loss: 136.2903, val_MinusLogProbMetric: 136.2903

Epoch 41: val_loss improved from 136.86758 to 136.29025, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 136.4915 - MinusLogProbMetric: 136.4915 - val_loss: 136.2903 - val_MinusLogProbMetric: 136.2903 - lr: 1.3717e-06 - 89s/epoch - 456ms/step
Epoch 42/1000
2023-10-25 22:09:30.461 
Epoch 42/1000 
	 loss: 136.0213, MinusLogProbMetric: 136.0213, val_loss: 135.9314, val_MinusLogProbMetric: 135.9314

Epoch 42: val_loss improved from 136.29025 to 135.93135, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 88s - loss: 136.0213 - MinusLogProbMetric: 136.0213 - val_loss: 135.9314 - val_MinusLogProbMetric: 135.9314 - lr: 1.3717e-06 - 88s/epoch - 449ms/step
Epoch 43/1000
2023-10-25 22:11:00.080 
Epoch 43/1000 
	 loss: 136.1690, MinusLogProbMetric: 136.1690, val_loss: 136.7152, val_MinusLogProbMetric: 136.7152

Epoch 43: val_loss did not improve from 135.93135
196/196 - 88s - loss: 136.1690 - MinusLogProbMetric: 136.1690 - val_loss: 136.7152 - val_MinusLogProbMetric: 136.7152 - lr: 1.3717e-06 - 88s/epoch - 448ms/step
Epoch 44/1000
2023-10-25 22:12:26.427 
Epoch 44/1000 
	 loss: 135.8469, MinusLogProbMetric: 135.8469, val_loss: 135.4334, val_MinusLogProbMetric: 135.4334

Epoch 44: val_loss improved from 135.93135 to 135.43338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 88s - loss: 135.8469 - MinusLogProbMetric: 135.8469 - val_loss: 135.4334 - val_MinusLogProbMetric: 135.4334 - lr: 1.3717e-06 - 88s/epoch - 450ms/step
Epoch 45/1000
2023-10-25 22:13:56.192 
Epoch 45/1000 
	 loss: 135.0907, MinusLogProbMetric: 135.0907, val_loss: 135.0668, val_MinusLogProbMetric: 135.0668

Epoch 45: val_loss improved from 135.43338 to 135.06677, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 135.0907 - MinusLogProbMetric: 135.0907 - val_loss: 135.0668 - val_MinusLogProbMetric: 135.0668 - lr: 1.3717e-06 - 90s/epoch - 458ms/step
Epoch 46/1000
2023-10-25 22:15:25.948 
Epoch 46/1000 
	 loss: 139.7715, MinusLogProbMetric: 139.7715, val_loss: 136.1461, val_MinusLogProbMetric: 136.1461

Epoch 46: val_loss did not improve from 135.06677
196/196 - 88s - loss: 139.7715 - MinusLogProbMetric: 139.7715 - val_loss: 136.1461 - val_MinusLogProbMetric: 136.1461 - lr: 1.3717e-06 - 88s/epoch - 449ms/step
Epoch 47/1000
2023-10-25 22:16:53.813 
Epoch 47/1000 
	 loss: 135.1947, MinusLogProbMetric: 135.1947, val_loss: 134.6238, val_MinusLogProbMetric: 134.6238

Epoch 47: val_loss improved from 135.06677 to 134.62384, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 135.1947 - MinusLogProbMetric: 135.1947 - val_loss: 134.6238 - val_MinusLogProbMetric: 134.6238 - lr: 1.3717e-06 - 90s/epoch - 457ms/step
Epoch 48/1000
2023-10-25 22:18:23.407 
Epoch 48/1000 
	 loss: 135.6657, MinusLogProbMetric: 135.6657, val_loss: 133.8697, val_MinusLogProbMetric: 133.8697

Epoch 48: val_loss improved from 134.62384 to 133.86966, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 135.6657 - MinusLogProbMetric: 135.6657 - val_loss: 133.8697 - val_MinusLogProbMetric: 133.8697 - lr: 1.3717e-06 - 90s/epoch - 458ms/step
Epoch 49/1000
2023-10-25 22:19:53.482 
Epoch 49/1000 
	 loss: 133.7633, MinusLogProbMetric: 133.7633, val_loss: 133.7861, val_MinusLogProbMetric: 133.7861

Epoch 49: val_loss improved from 133.86966 to 133.78607, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 133.7633 - MinusLogProbMetric: 133.7633 - val_loss: 133.7861 - val_MinusLogProbMetric: 133.7861 - lr: 1.3717e-06 - 90s/epoch - 459ms/step
Epoch 50/1000
2023-10-25 22:21:23.341 
Epoch 50/1000 
	 loss: 133.8058, MinusLogProbMetric: 133.8058, val_loss: 133.5262, val_MinusLogProbMetric: 133.5262

Epoch 50: val_loss improved from 133.78607 to 133.52625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 133.8058 - MinusLogProbMetric: 133.8058 - val_loss: 133.5262 - val_MinusLogProbMetric: 133.5262 - lr: 1.3717e-06 - 90s/epoch - 459ms/step
Epoch 51/1000
2023-10-25 22:22:53.563 
Epoch 51/1000 
	 loss: 133.0895, MinusLogProbMetric: 133.0895, val_loss: 133.0203, val_MinusLogProbMetric: 133.0203

Epoch 51: val_loss improved from 133.52625 to 133.02034, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 133.0895 - MinusLogProbMetric: 133.0895 - val_loss: 133.0203 - val_MinusLogProbMetric: 133.0203 - lr: 1.3717e-06 - 90s/epoch - 461ms/step
Epoch 52/1000
2023-10-25 22:24:24.737 
Epoch 52/1000 
	 loss: 133.2686, MinusLogProbMetric: 133.2686, val_loss: 132.6981, val_MinusLogProbMetric: 132.6981

Epoch 52: val_loss improved from 133.02034 to 132.69814, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 133.2686 - MinusLogProbMetric: 133.2686 - val_loss: 132.6981 - val_MinusLogProbMetric: 132.6981 - lr: 1.3717e-06 - 91s/epoch - 464ms/step
Epoch 53/1000
2023-10-25 22:25:54.385 
Epoch 53/1000 
	 loss: 132.2621, MinusLogProbMetric: 132.2621, val_loss: 132.2635, val_MinusLogProbMetric: 132.2635

Epoch 53: val_loss improved from 132.69814 to 132.26352, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 132.2621 - MinusLogProbMetric: 132.2621 - val_loss: 132.2635 - val_MinusLogProbMetric: 132.2635 - lr: 1.3717e-06 - 90s/epoch - 458ms/step
Epoch 54/1000
2023-10-25 22:27:24.843 
Epoch 54/1000 
	 loss: 132.0030, MinusLogProbMetric: 132.0030, val_loss: 132.0058, val_MinusLogProbMetric: 132.0058

Epoch 54: val_loss improved from 132.26352 to 132.00580, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 132.0030 - MinusLogProbMetric: 132.0030 - val_loss: 132.0058 - val_MinusLogProbMetric: 132.0058 - lr: 1.3717e-06 - 90s/epoch - 461ms/step
Epoch 55/1000
2023-10-25 22:28:55.172 
Epoch 55/1000 
	 loss: 131.7049, MinusLogProbMetric: 131.7049, val_loss: 131.8035, val_MinusLogProbMetric: 131.8035

Epoch 55: val_loss improved from 132.00580 to 131.80354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 131.7049 - MinusLogProbMetric: 131.7049 - val_loss: 131.8035 - val_MinusLogProbMetric: 131.8035 - lr: 1.3717e-06 - 91s/epoch - 462ms/step
Epoch 56/1000
2023-10-25 22:30:16.955 
Epoch 56/1000 
	 loss: 131.9565, MinusLogProbMetric: 131.9565, val_loss: 131.9293, val_MinusLogProbMetric: 131.9293

Epoch 56: val_loss did not improve from 131.80354
196/196 - 80s - loss: 131.9565 - MinusLogProbMetric: 131.9565 - val_loss: 131.9293 - val_MinusLogProbMetric: 131.9293 - lr: 1.3717e-06 - 80s/epoch - 407ms/step
Epoch 57/1000
2023-10-25 22:31:47.248 
Epoch 57/1000 
	 loss: 131.5830, MinusLogProbMetric: 131.5830, val_loss: 131.4961, val_MinusLogProbMetric: 131.4961

Epoch 57: val_loss improved from 131.80354 to 131.49608, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 131.5830 - MinusLogProbMetric: 131.5830 - val_loss: 131.4961 - val_MinusLogProbMetric: 131.4961 - lr: 1.3717e-06 - 92s/epoch - 470ms/step
Epoch 58/1000
2023-10-25 22:33:18.732 
Epoch 58/1000 
	 loss: 131.2661, MinusLogProbMetric: 131.2661, val_loss: 131.0176, val_MinusLogProbMetric: 131.0176

Epoch 58: val_loss improved from 131.49608 to 131.01759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 131.2661 - MinusLogProbMetric: 131.2661 - val_loss: 131.0176 - val_MinusLogProbMetric: 131.0176 - lr: 1.3717e-06 - 91s/epoch - 467ms/step
Epoch 59/1000
2023-10-25 22:34:49.889 
Epoch 59/1000 
	 loss: 133.5031, MinusLogProbMetric: 133.5031, val_loss: 131.9403, val_MinusLogProbMetric: 131.9403

Epoch 59: val_loss did not improve from 131.01759
196/196 - 89s - loss: 133.5031 - MinusLogProbMetric: 133.5031 - val_loss: 131.9403 - val_MinusLogProbMetric: 131.9403 - lr: 1.3717e-06 - 89s/epoch - 456ms/step
Epoch 60/1000
2023-10-25 22:36:19.589 
Epoch 60/1000 
	 loss: 132.0897, MinusLogProbMetric: 132.0897, val_loss: 133.6988, val_MinusLogProbMetric: 133.6988

Epoch 60: val_loss did not improve from 131.01759
196/196 - 90s - loss: 132.0897 - MinusLogProbMetric: 132.0897 - val_loss: 133.6988 - val_MinusLogProbMetric: 133.6988 - lr: 1.3717e-06 - 90s/epoch - 458ms/step
Epoch 61/1000
2023-10-25 22:37:49.395 
Epoch 61/1000 
	 loss: 131.6260, MinusLogProbMetric: 131.6260, val_loss: 130.9374, val_MinusLogProbMetric: 130.9374

Epoch 61: val_loss improved from 131.01759 to 130.93739, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 131.6260 - MinusLogProbMetric: 131.6260 - val_loss: 130.9374 - val_MinusLogProbMetric: 130.9374 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 62/1000
2023-10-25 22:39:20.526 
Epoch 62/1000 
	 loss: 132.9812, MinusLogProbMetric: 132.9812, val_loss: 130.3670, val_MinusLogProbMetric: 130.3670

Epoch 62: val_loss improved from 130.93739 to 130.36703, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 132.9812 - MinusLogProbMetric: 132.9812 - val_loss: 130.3670 - val_MinusLogProbMetric: 130.3670 - lr: 1.3717e-06 - 91s/epoch - 465ms/step
Epoch 63/1000
2023-10-25 22:40:51.644 
Epoch 63/1000 
	 loss: 130.2220, MinusLogProbMetric: 130.2220, val_loss: 130.1781, val_MinusLogProbMetric: 130.1781

Epoch 63: val_loss improved from 130.36703 to 130.17805, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 130.2220 - MinusLogProbMetric: 130.2220 - val_loss: 130.1781 - val_MinusLogProbMetric: 130.1781 - lr: 1.3717e-06 - 92s/epoch - 467ms/step
Epoch 64/1000
2023-10-25 22:42:23.293 
Epoch 64/1000 
	 loss: 129.8521, MinusLogProbMetric: 129.8521, val_loss: 129.6880, val_MinusLogProbMetric: 129.6880

Epoch 64: val_loss improved from 130.17805 to 129.68796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 129.8521 - MinusLogProbMetric: 129.8521 - val_loss: 129.6880 - val_MinusLogProbMetric: 129.6880 - lr: 1.3717e-06 - 91s/epoch - 467ms/step
Epoch 65/1000
2023-10-25 22:43:54.356 
Epoch 65/1000 
	 loss: 129.3998, MinusLogProbMetric: 129.3998, val_loss: 129.4384, val_MinusLogProbMetric: 129.4384

Epoch 65: val_loss improved from 129.68796 to 129.43835, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 129.3998 - MinusLogProbMetric: 129.3998 - val_loss: 129.4384 - val_MinusLogProbMetric: 129.4384 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 66/1000
2023-10-25 22:45:25.271 
Epoch 66/1000 
	 loss: 129.2166, MinusLogProbMetric: 129.2166, val_loss: 129.1695, val_MinusLogProbMetric: 129.1695

Epoch 66: val_loss improved from 129.43835 to 129.16948, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 129.2166 - MinusLogProbMetric: 129.2166 - val_loss: 129.1695 - val_MinusLogProbMetric: 129.1695 - lr: 1.3717e-06 - 91s/epoch - 464ms/step
Epoch 67/1000
2023-10-25 22:46:57.604 
Epoch 67/1000 
	 loss: 133.5573, MinusLogProbMetric: 133.5573, val_loss: 129.4185, val_MinusLogProbMetric: 129.4185

Epoch 67: val_loss did not improve from 129.16948
196/196 - 90s - loss: 133.5573 - MinusLogProbMetric: 133.5573 - val_loss: 129.4185 - val_MinusLogProbMetric: 129.4185 - lr: 1.3717e-06 - 90s/epoch - 461ms/step
Epoch 68/1000
2023-10-25 22:48:27.396 
Epoch 68/1000 
	 loss: 129.6830, MinusLogProbMetric: 129.6830, val_loss: 135.2119, val_MinusLogProbMetric: 135.2119

Epoch 68: val_loss did not improve from 129.16948
196/196 - 90s - loss: 129.6830 - MinusLogProbMetric: 129.6830 - val_loss: 135.2119 - val_MinusLogProbMetric: 135.2119 - lr: 1.3717e-06 - 90s/epoch - 458ms/step
Epoch 69/1000
2023-10-25 22:49:57.448 
Epoch 69/1000 
	 loss: 145.9456, MinusLogProbMetric: 145.9456, val_loss: 128.9969, val_MinusLogProbMetric: 128.9969

Epoch 69: val_loss improved from 129.16948 to 128.99695, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 145.9456 - MinusLogProbMetric: 145.9456 - val_loss: 128.9969 - val_MinusLogProbMetric: 128.9969 - lr: 1.3717e-06 - 92s/epoch - 469ms/step
Epoch 70/1000
2023-10-25 22:51:28.401 
Epoch 70/1000 
	 loss: 128.7754, MinusLogProbMetric: 128.7754, val_loss: 128.5329, val_MinusLogProbMetric: 128.5329

Epoch 70: val_loss improved from 128.99695 to 128.53287, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 128.7754 - MinusLogProbMetric: 128.7754 - val_loss: 128.5329 - val_MinusLogProbMetric: 128.5329 - lr: 1.3717e-06 - 91s/epoch - 462ms/step
Epoch 71/1000
2023-10-25 22:53:00.074 
Epoch 71/1000 
	 loss: 128.2510, MinusLogProbMetric: 128.2510, val_loss: 128.2028, val_MinusLogProbMetric: 128.2028

Epoch 71: val_loss improved from 128.53287 to 128.20279, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 128.2510 - MinusLogProbMetric: 128.2510 - val_loss: 128.2028 - val_MinusLogProbMetric: 128.2028 - lr: 1.3717e-06 - 92s/epoch - 468ms/step
Epoch 72/1000
2023-10-25 22:54:30.849 
Epoch 72/1000 
	 loss: 128.0614, MinusLogProbMetric: 128.0614, val_loss: 128.0027, val_MinusLogProbMetric: 128.0027

Epoch 72: val_loss improved from 128.20279 to 128.00267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 128.0614 - MinusLogProbMetric: 128.0614 - val_loss: 128.0027 - val_MinusLogProbMetric: 128.0027 - lr: 1.3717e-06 - 91s/epoch - 464ms/step
Epoch 73/1000
2023-10-25 22:56:02.021 
Epoch 73/1000 
	 loss: 129.8083, MinusLogProbMetric: 129.8083, val_loss: 127.9430, val_MinusLogProbMetric: 127.9430

Epoch 73: val_loss improved from 128.00267 to 127.94300, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 129.8083 - MinusLogProbMetric: 129.8083 - val_loss: 127.9430 - val_MinusLogProbMetric: 127.9430 - lr: 1.3717e-06 - 91s/epoch - 465ms/step
Epoch 74/1000
2023-10-25 22:57:33.362 
Epoch 74/1000 
	 loss: 127.6493, MinusLogProbMetric: 127.6493, val_loss: 127.5261, val_MinusLogProbMetric: 127.5261

Epoch 74: val_loss improved from 127.94300 to 127.52610, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 127.6493 - MinusLogProbMetric: 127.6493 - val_loss: 127.5261 - val_MinusLogProbMetric: 127.5261 - lr: 1.3717e-06 - 91s/epoch - 467ms/step
Epoch 75/1000
2023-10-25 22:59:04.968 
Epoch 75/1000 
	 loss: 127.3362, MinusLogProbMetric: 127.3362, val_loss: 127.2990, val_MinusLogProbMetric: 127.2990

Epoch 75: val_loss improved from 127.52610 to 127.29896, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 127.3362 - MinusLogProbMetric: 127.3362 - val_loss: 127.2990 - val_MinusLogProbMetric: 127.2990 - lr: 1.3717e-06 - 92s/epoch - 468ms/step
Epoch 76/1000
2023-10-25 23:00:36.289 
Epoch 76/1000 
	 loss: 127.2370, MinusLogProbMetric: 127.2370, val_loss: 127.2140, val_MinusLogProbMetric: 127.2140

Epoch 76: val_loss improved from 127.29896 to 127.21395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 127.2370 - MinusLogProbMetric: 127.2370 - val_loss: 127.2140 - val_MinusLogProbMetric: 127.2140 - lr: 1.3717e-06 - 91s/epoch - 465ms/step
Epoch 77/1000
2023-10-25 23:02:07.648 
Epoch 77/1000 
	 loss: 126.9116, MinusLogProbMetric: 126.9116, val_loss: 126.9396, val_MinusLogProbMetric: 126.9396

Epoch 77: val_loss improved from 127.21395 to 126.93955, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 126.9116 - MinusLogProbMetric: 126.9116 - val_loss: 126.9396 - val_MinusLogProbMetric: 126.9396 - lr: 1.3717e-06 - 92s/epoch - 467ms/step
Epoch 78/1000
2023-10-25 23:03:39.059 
Epoch 78/1000 
	 loss: 126.6343, MinusLogProbMetric: 126.6343, val_loss: 126.7359, val_MinusLogProbMetric: 126.7359

Epoch 78: val_loss improved from 126.93955 to 126.73587, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 126.6343 - MinusLogProbMetric: 126.6343 - val_loss: 126.7359 - val_MinusLogProbMetric: 126.7359 - lr: 1.3717e-06 - 91s/epoch - 465ms/step
Epoch 79/1000
2023-10-25 23:05:12.068 
Epoch 79/1000 
	 loss: 126.6256, MinusLogProbMetric: 126.6256, val_loss: 127.2158, val_MinusLogProbMetric: 127.2158

Epoch 79: val_loss did not improve from 126.73587
196/196 - 91s - loss: 126.6256 - MinusLogProbMetric: 126.6256 - val_loss: 127.2158 - val_MinusLogProbMetric: 127.2158 - lr: 1.3717e-06 - 91s/epoch - 465ms/step
Epoch 80/1000
2023-10-25 23:06:42.460 
Epoch 80/1000 
	 loss: 127.1469, MinusLogProbMetric: 127.1469, val_loss: 126.5558, val_MinusLogProbMetric: 126.5558

Epoch 80: val_loss improved from 126.73587 to 126.55578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 127.1469 - MinusLogProbMetric: 127.1469 - val_loss: 126.5558 - val_MinusLogProbMetric: 126.5558 - lr: 1.3717e-06 - 92s/epoch - 471ms/step
Epoch 81/1000
2023-10-25 23:08:14.558 
Epoch 81/1000 
	 loss: 128.0394, MinusLogProbMetric: 128.0394, val_loss: 128.6959, val_MinusLogProbMetric: 128.6959

Epoch 81: val_loss did not improve from 126.55578
196/196 - 90s - loss: 128.0394 - MinusLogProbMetric: 128.0394 - val_loss: 128.6959 - val_MinusLogProbMetric: 128.6959 - lr: 1.3717e-06 - 90s/epoch - 460ms/step
Epoch 82/1000
2023-10-25 23:09:43.965 
Epoch 82/1000 
	 loss: 127.7594, MinusLogProbMetric: 127.7594, val_loss: 127.4950, val_MinusLogProbMetric: 127.4950

Epoch 82: val_loss did not improve from 126.55578
196/196 - 89s - loss: 127.7594 - MinusLogProbMetric: 127.7594 - val_loss: 127.4950 - val_MinusLogProbMetric: 127.4950 - lr: 1.3717e-06 - 89s/epoch - 456ms/step
Epoch 83/1000
2023-10-25 23:11:14.004 
Epoch 83/1000 
	 loss: 127.0269, MinusLogProbMetric: 127.0269, val_loss: 126.8489, val_MinusLogProbMetric: 126.8489

Epoch 83: val_loss did not improve from 126.55578
196/196 - 90s - loss: 127.0269 - MinusLogProbMetric: 127.0269 - val_loss: 126.8489 - val_MinusLogProbMetric: 126.8489 - lr: 1.3717e-06 - 90s/epoch - 459ms/step
Epoch 84/1000
2023-10-25 23:12:43.611 
Epoch 84/1000 
	 loss: 126.4906, MinusLogProbMetric: 126.4906, val_loss: 126.4258, val_MinusLogProbMetric: 126.4258

Epoch 84: val_loss improved from 126.55578 to 126.42581, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 126.4906 - MinusLogProbMetric: 126.4906 - val_loss: 126.4258 - val_MinusLogProbMetric: 126.4258 - lr: 1.3717e-06 - 92s/epoch - 467ms/step
Epoch 85/1000
2023-10-25 23:14:14.807 
Epoch 85/1000 
	 loss: 126.1534, MinusLogProbMetric: 126.1534, val_loss: 126.1886, val_MinusLogProbMetric: 126.1886

Epoch 85: val_loss improved from 126.42581 to 126.18864, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 126.1534 - MinusLogProbMetric: 126.1534 - val_loss: 126.1886 - val_MinusLogProbMetric: 126.1886 - lr: 1.3717e-06 - 91s/epoch - 463ms/step
Epoch 86/1000
2023-10-25 23:15:45.623 
Epoch 86/1000 
	 loss: 125.8901, MinusLogProbMetric: 125.8901, val_loss: 125.8990, val_MinusLogProbMetric: 125.8990

Epoch 86: val_loss improved from 126.18864 to 125.89903, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 125.8901 - MinusLogProbMetric: 125.8901 - val_loss: 125.8990 - val_MinusLogProbMetric: 125.8990 - lr: 1.3717e-06 - 91s/epoch - 465ms/step
Epoch 87/1000
2023-10-25 23:17:17.697 
Epoch 87/1000 
	 loss: 125.6350, MinusLogProbMetric: 125.6350, val_loss: 125.7548, val_MinusLogProbMetric: 125.7548

Epoch 87: val_loss improved from 125.89903 to 125.75477, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 125.6350 - MinusLogProbMetric: 125.6350 - val_loss: 125.7548 - val_MinusLogProbMetric: 125.7548 - lr: 1.3717e-06 - 92s/epoch - 470ms/step
Epoch 88/1000
2023-10-25 23:18:49.387 
Epoch 88/1000 
	 loss: 125.3910, MinusLogProbMetric: 125.3910, val_loss: 125.4294, val_MinusLogProbMetric: 125.4294

Epoch 88: val_loss improved from 125.75477 to 125.42937, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 125.3910 - MinusLogProbMetric: 125.3910 - val_loss: 125.4294 - val_MinusLogProbMetric: 125.4294 - lr: 1.3717e-06 - 92s/epoch - 467ms/step
Epoch 89/1000
2023-10-25 23:20:21.585 
Epoch 89/1000 
	 loss: 125.0829, MinusLogProbMetric: 125.0829, val_loss: 127.0361, val_MinusLogProbMetric: 127.0361

Epoch 89: val_loss did not improve from 125.42937
196/196 - 90s - loss: 125.0829 - MinusLogProbMetric: 125.0829 - val_loss: 127.0361 - val_MinusLogProbMetric: 127.0361 - lr: 1.3717e-06 - 90s/epoch - 461ms/step
Epoch 90/1000
2023-10-25 23:21:51.394 
Epoch 90/1000 
	 loss: 125.7314, MinusLogProbMetric: 125.7314, val_loss: 125.3817, val_MinusLogProbMetric: 125.3817

Epoch 90: val_loss improved from 125.42937 to 125.38172, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 125.7314 - MinusLogProbMetric: 125.7314 - val_loss: 125.3817 - val_MinusLogProbMetric: 125.3817 - lr: 1.3717e-06 - 92s/epoch - 468ms/step
Epoch 91/1000
2023-10-25 23:23:23.018 
Epoch 91/1000 
	 loss: 124.9089, MinusLogProbMetric: 124.9089, val_loss: 124.9962, val_MinusLogProbMetric: 124.9962

Epoch 91: val_loss improved from 125.38172 to 124.99621, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 124.9089 - MinusLogProbMetric: 124.9089 - val_loss: 124.9962 - val_MinusLogProbMetric: 124.9962 - lr: 1.3717e-06 - 91s/epoch - 467ms/step
Epoch 92/1000
2023-10-25 23:24:54.530 
Epoch 92/1000 
	 loss: 124.7012, MinusLogProbMetric: 124.7012, val_loss: 124.8004, val_MinusLogProbMetric: 124.8004

Epoch 92: val_loss improved from 124.99621 to 124.80039, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 124.7012 - MinusLogProbMetric: 124.7012 - val_loss: 124.8004 - val_MinusLogProbMetric: 124.8004 - lr: 1.3717e-06 - 92s/epoch - 468ms/step
Epoch 93/1000
2023-10-25 23:26:26.166 
Epoch 93/1000 
	 loss: 124.5352, MinusLogProbMetric: 124.5352, val_loss: 124.7066, val_MinusLogProbMetric: 124.7066

Epoch 93: val_loss improved from 124.80039 to 124.70661, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 124.5352 - MinusLogProbMetric: 124.5352 - val_loss: 124.7066 - val_MinusLogProbMetric: 124.7066 - lr: 1.3717e-06 - 92s/epoch - 467ms/step
Epoch 94/1000
2023-10-25 23:27:57.259 
Epoch 94/1000 
	 loss: 124.5534, MinusLogProbMetric: 124.5534, val_loss: 124.6769, val_MinusLogProbMetric: 124.6769

Epoch 94: val_loss improved from 124.70661 to 124.67688, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 124.5534 - MinusLogProbMetric: 124.5534 - val_loss: 124.6769 - val_MinusLogProbMetric: 124.6769 - lr: 1.3717e-06 - 91s/epoch - 464ms/step
Epoch 95/1000
2023-10-25 23:29:28.361 
Epoch 95/1000 
	 loss: 124.6833, MinusLogProbMetric: 124.6833, val_loss: 124.5352, val_MinusLogProbMetric: 124.5352

Epoch 95: val_loss improved from 124.67688 to 124.53516, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 124.6833 - MinusLogProbMetric: 124.6833 - val_loss: 124.5352 - val_MinusLogProbMetric: 124.5352 - lr: 1.3717e-06 - 91s/epoch - 464ms/step
Epoch 96/1000
2023-10-25 23:30:59.906 
Epoch 96/1000 
	 loss: 124.2140, MinusLogProbMetric: 124.2140, val_loss: 124.2944, val_MinusLogProbMetric: 124.2944

Epoch 96: val_loss improved from 124.53516 to 124.29436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 124.2140 - MinusLogProbMetric: 124.2140 - val_loss: 124.2944 - val_MinusLogProbMetric: 124.2944 - lr: 1.3717e-06 - 92s/epoch - 467ms/step
Epoch 97/1000
2023-10-25 23:32:31.015 
Epoch 97/1000 
	 loss: 123.8705, MinusLogProbMetric: 123.8705, val_loss: 123.9486, val_MinusLogProbMetric: 123.9486

Epoch 97: val_loss improved from 124.29436 to 123.94865, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 123.8705 - MinusLogProbMetric: 123.8705 - val_loss: 123.9486 - val_MinusLogProbMetric: 123.9486 - lr: 1.3717e-06 - 91s/epoch - 464ms/step
Epoch 98/1000
2023-10-25 23:34:02.285 
Epoch 98/1000 
	 loss: 123.8685, MinusLogProbMetric: 123.8685, val_loss: 123.7999, val_MinusLogProbMetric: 123.7999

Epoch 98: val_loss improved from 123.94865 to 123.79989, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 123.8685 - MinusLogProbMetric: 123.8685 - val_loss: 123.7999 - val_MinusLogProbMetric: 123.7999 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 99/1000
2023-10-25 23:35:34.024 
Epoch 99/1000 
	 loss: 124.0103, MinusLogProbMetric: 124.0103, val_loss: 132.3013, val_MinusLogProbMetric: 132.3013

Epoch 99: val_loss did not improve from 123.79989
196/196 - 90s - loss: 124.0103 - MinusLogProbMetric: 124.0103 - val_loss: 132.3013 - val_MinusLogProbMetric: 132.3013 - lr: 1.3717e-06 - 90s/epoch - 459ms/step
Epoch 100/1000
2023-10-25 23:37:04.603 
Epoch 100/1000 
	 loss: 129.8337, MinusLogProbMetric: 129.8337, val_loss: 127.1581, val_MinusLogProbMetric: 127.1581

Epoch 100: val_loss did not improve from 123.79989
196/196 - 91s - loss: 129.8337 - MinusLogProbMetric: 129.8337 - val_loss: 127.1581 - val_MinusLogProbMetric: 127.1581 - lr: 1.3717e-06 - 91s/epoch - 462ms/step
Epoch 101/1000
2023-10-25 23:38:33.936 
Epoch 101/1000 
	 loss: 126.0203, MinusLogProbMetric: 126.0203, val_loss: 125.6675, val_MinusLogProbMetric: 125.6675

Epoch 101: val_loss did not improve from 123.79989
196/196 - 89s - loss: 126.0203 - MinusLogProbMetric: 126.0203 - val_loss: 125.6675 - val_MinusLogProbMetric: 125.6675 - lr: 1.3717e-06 - 89s/epoch - 456ms/step
Epoch 102/1000
2023-10-25 23:40:03.443 
Epoch 102/1000 
	 loss: 125.0807, MinusLogProbMetric: 125.0807, val_loss: 124.9520, val_MinusLogProbMetric: 124.9520

Epoch 102: val_loss did not improve from 123.79989
196/196 - 90s - loss: 125.0807 - MinusLogProbMetric: 125.0807 - val_loss: 124.9520 - val_MinusLogProbMetric: 124.9520 - lr: 1.3717e-06 - 90s/epoch - 457ms/step
Epoch 103/1000
2023-10-25 23:41:32.687 
Epoch 103/1000 
	 loss: 124.5460, MinusLogProbMetric: 124.5460, val_loss: 124.5312, val_MinusLogProbMetric: 124.5312

Epoch 103: val_loss did not improve from 123.79989
196/196 - 89s - loss: 124.5460 - MinusLogProbMetric: 124.5460 - val_loss: 124.5312 - val_MinusLogProbMetric: 124.5312 - lr: 1.3717e-06 - 89s/epoch - 455ms/step
Epoch 104/1000
2023-10-25 23:43:02.648 
Epoch 104/1000 
	 loss: 124.2286, MinusLogProbMetric: 124.2286, val_loss: 124.2645, val_MinusLogProbMetric: 124.2645

Epoch 104: val_loss did not improve from 123.79989
196/196 - 90s - loss: 124.2286 - MinusLogProbMetric: 124.2286 - val_loss: 124.2645 - val_MinusLogProbMetric: 124.2645 - lr: 1.3717e-06 - 90s/epoch - 459ms/step
Epoch 105/1000
2023-10-25 23:44:31.502 
Epoch 105/1000 
	 loss: 123.9827, MinusLogProbMetric: 123.9827, val_loss: 124.1739, val_MinusLogProbMetric: 124.1739

Epoch 105: val_loss did not improve from 123.79989
196/196 - 89s - loss: 123.9827 - MinusLogProbMetric: 123.9827 - val_loss: 124.1739 - val_MinusLogProbMetric: 124.1739 - lr: 1.3717e-06 - 89s/epoch - 453ms/step
Epoch 106/1000
2023-10-25 23:46:01.083 
Epoch 106/1000 
	 loss: 123.8478, MinusLogProbMetric: 123.8478, val_loss: 123.9703, val_MinusLogProbMetric: 123.9703

Epoch 106: val_loss did not improve from 123.79989
196/196 - 90s - loss: 123.8478 - MinusLogProbMetric: 123.8478 - val_loss: 123.9703 - val_MinusLogProbMetric: 123.9703 - lr: 1.3717e-06 - 90s/epoch - 457ms/step
Epoch 107/1000
2023-10-25 23:47:29.271 
Epoch 107/1000 
	 loss: 123.6504, MinusLogProbMetric: 123.6504, val_loss: 123.7093, val_MinusLogProbMetric: 123.7093

Epoch 107: val_loss improved from 123.79989 to 123.70927, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 123.6504 - MinusLogProbMetric: 123.6504 - val_loss: 123.7093 - val_MinusLogProbMetric: 123.7093 - lr: 1.3717e-06 - 90s/epoch - 457ms/step
Epoch 108/1000
2023-10-25 23:48:59.655 
Epoch 108/1000 
	 loss: 123.3727, MinusLogProbMetric: 123.3727, val_loss: 123.4656, val_MinusLogProbMetric: 123.4656

Epoch 108: val_loss improved from 123.70927 to 123.46558, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 123.3727 - MinusLogProbMetric: 123.3727 - val_loss: 123.4656 - val_MinusLogProbMetric: 123.4656 - lr: 1.3717e-06 - 91s/epoch - 463ms/step
Epoch 109/1000
2023-10-25 23:50:30.948 
Epoch 109/1000 
	 loss: 123.1581, MinusLogProbMetric: 123.1581, val_loss: 123.3314, val_MinusLogProbMetric: 123.3314

Epoch 109: val_loss improved from 123.46558 to 123.33137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 123.1581 - MinusLogProbMetric: 123.1581 - val_loss: 123.3314 - val_MinusLogProbMetric: 123.3314 - lr: 1.3717e-06 - 92s/epoch - 467ms/step
Epoch 110/1000
2023-10-25 23:52:02.086 
Epoch 110/1000 
	 loss: 122.9842, MinusLogProbMetric: 122.9842, val_loss: 123.3736, val_MinusLogProbMetric: 123.3736

Epoch 110: val_loss did not improve from 123.33137
196/196 - 89s - loss: 122.9842 - MinusLogProbMetric: 122.9842 - val_loss: 123.3736 - val_MinusLogProbMetric: 123.3736 - lr: 1.3717e-06 - 89s/epoch - 455ms/step
Epoch 111/1000
2023-10-25 23:53:31.274 
Epoch 111/1000 
	 loss: 122.8844, MinusLogProbMetric: 122.8844, val_loss: 123.0990, val_MinusLogProbMetric: 123.0990

Epoch 111: val_loss improved from 123.33137 to 123.09895, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 122.8844 - MinusLogProbMetric: 122.8844 - val_loss: 123.0990 - val_MinusLogProbMetric: 123.0990 - lr: 1.3717e-06 - 91s/epoch - 464ms/step
Epoch 112/1000
2023-10-25 23:55:02.266 
Epoch 112/1000 
	 loss: 123.1967, MinusLogProbMetric: 123.1967, val_loss: 123.7142, val_MinusLogProbMetric: 123.7142

Epoch 112: val_loss did not improve from 123.09895
196/196 - 89s - loss: 123.1967 - MinusLogProbMetric: 123.1967 - val_loss: 123.7142 - val_MinusLogProbMetric: 123.7142 - lr: 1.3717e-06 - 89s/epoch - 455ms/step
Epoch 113/1000
2023-10-25 23:56:31.188 
Epoch 113/1000 
	 loss: 123.0913, MinusLogProbMetric: 123.0913, val_loss: 123.7946, val_MinusLogProbMetric: 123.7946

Epoch 113: val_loss did not improve from 123.09895
196/196 - 89s - loss: 123.0913 - MinusLogProbMetric: 123.0913 - val_loss: 123.7946 - val_MinusLogProbMetric: 123.7946 - lr: 1.3717e-06 - 89s/epoch - 454ms/step
Epoch 114/1000
2023-10-25 23:58:00.764 
Epoch 114/1000 
	 loss: 123.2364, MinusLogProbMetric: 123.2364, val_loss: 123.4485, val_MinusLogProbMetric: 123.4485

Epoch 114: val_loss did not improve from 123.09895
196/196 - 90s - loss: 123.2364 - MinusLogProbMetric: 123.2364 - val_loss: 123.4485 - val_MinusLogProbMetric: 123.4485 - lr: 1.3717e-06 - 90s/epoch - 457ms/step
Epoch 115/1000
2023-10-25 23:59:29.927 
Epoch 115/1000 
	 loss: 122.8674, MinusLogProbMetric: 122.8674, val_loss: 122.8511, val_MinusLogProbMetric: 122.8511

Epoch 115: val_loss improved from 123.09895 to 122.85112, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 122.8674 - MinusLogProbMetric: 122.8674 - val_loss: 122.8511 - val_MinusLogProbMetric: 122.8511 - lr: 1.3717e-06 - 91s/epoch - 464ms/step
Epoch 116/1000
2023-10-26 00:00:59.392 
Epoch 116/1000 
	 loss: 122.5210, MinusLogProbMetric: 122.5210, val_loss: 122.9209, val_MinusLogProbMetric: 122.9209

Epoch 116: val_loss did not improve from 122.85112
196/196 - 88s - loss: 122.5210 - MinusLogProbMetric: 122.5210 - val_loss: 122.9209 - val_MinusLogProbMetric: 122.9209 - lr: 1.3717e-06 - 88s/epoch - 447ms/step
Epoch 117/1000
2023-10-26 00:02:20.206 
Epoch 117/1000 
	 loss: 122.2688, MinusLogProbMetric: 122.2688, val_loss: 122.1702, val_MinusLogProbMetric: 122.1702

Epoch 117: val_loss improved from 122.85112 to 122.17017, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 82s - loss: 122.2688 - MinusLogProbMetric: 122.2688 - val_loss: 122.1702 - val_MinusLogProbMetric: 122.1702 - lr: 1.3717e-06 - 82s/epoch - 420ms/step
Epoch 118/1000
2023-10-26 00:03:50.676 
Epoch 118/1000 
	 loss: 123.1042, MinusLogProbMetric: 123.1042, val_loss: 128.1208, val_MinusLogProbMetric: 128.1208

Epoch 118: val_loss did not improve from 122.17017
196/196 - 89s - loss: 123.1042 - MinusLogProbMetric: 123.1042 - val_loss: 128.1208 - val_MinusLogProbMetric: 128.1208 - lr: 1.3717e-06 - 89s/epoch - 454ms/step
Epoch 119/1000
2023-10-26 00:05:10.376 
Epoch 119/1000 
	 loss: 122.3405, MinusLogProbMetric: 122.3405, val_loss: 121.9714, val_MinusLogProbMetric: 121.9714

Epoch 119: val_loss improved from 122.17017 to 121.97137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 81s - loss: 122.3405 - MinusLogProbMetric: 122.3405 - val_loss: 121.9714 - val_MinusLogProbMetric: 121.9714 - lr: 1.3717e-06 - 81s/epoch - 415ms/step
Epoch 120/1000
2023-10-26 00:06:40.767 
Epoch 120/1000 
	 loss: 156.5826, MinusLogProbMetric: 156.5826, val_loss: 141.2183, val_MinusLogProbMetric: 141.2183

Epoch 120: val_loss did not improve from 121.97137
196/196 - 89s - loss: 156.5826 - MinusLogProbMetric: 156.5826 - val_loss: 141.2183 - val_MinusLogProbMetric: 141.2183 - lr: 1.3717e-06 - 89s/epoch - 453ms/step
Epoch 121/1000
2023-10-26 00:08:10.008 
Epoch 121/1000 
	 loss: 132.9641, MinusLogProbMetric: 132.9641, val_loss: 127.8203, val_MinusLogProbMetric: 127.8203

Epoch 121: val_loss did not improve from 121.97137
196/196 - 89s - loss: 132.9641 - MinusLogProbMetric: 132.9641 - val_loss: 127.8203 - val_MinusLogProbMetric: 127.8203 - lr: 1.3717e-06 - 89s/epoch - 455ms/step
Epoch 122/1000
2023-10-26 00:09:39.829 
Epoch 122/1000 
	 loss: 126.0722, MinusLogProbMetric: 126.0722, val_loss: 125.1209, val_MinusLogProbMetric: 125.1209

Epoch 122: val_loss did not improve from 121.97137
196/196 - 90s - loss: 126.0722 - MinusLogProbMetric: 126.0722 - val_loss: 125.1209 - val_MinusLogProbMetric: 125.1209 - lr: 1.3717e-06 - 90s/epoch - 458ms/step
Epoch 123/1000
2023-10-26 00:11:09.313 
Epoch 123/1000 
	 loss: 124.1654, MinusLogProbMetric: 124.1654, val_loss: 123.5453, val_MinusLogProbMetric: 123.5453

Epoch 123: val_loss did not improve from 121.97137
196/196 - 89s - loss: 124.1654 - MinusLogProbMetric: 124.1654 - val_loss: 123.5453 - val_MinusLogProbMetric: 123.5453 - lr: 1.3717e-06 - 89s/epoch - 457ms/step
Epoch 124/1000
2023-10-26 00:12:38.452 
Epoch 124/1000 
	 loss: 122.9654, MinusLogProbMetric: 122.9654, val_loss: 122.7378, val_MinusLogProbMetric: 122.7378

Epoch 124: val_loss did not improve from 121.97137
196/196 - 89s - loss: 122.9654 - MinusLogProbMetric: 122.9654 - val_loss: 122.7378 - val_MinusLogProbMetric: 122.7378 - lr: 1.3717e-06 - 89s/epoch - 455ms/step
Epoch 125/1000
2023-10-26 00:14:08.314 
Epoch 125/1000 
	 loss: 122.2721, MinusLogProbMetric: 122.2721, val_loss: 122.2319, val_MinusLogProbMetric: 122.2319

Epoch 125: val_loss did not improve from 121.97137
196/196 - 90s - loss: 122.2721 - MinusLogProbMetric: 122.2721 - val_loss: 122.2319 - val_MinusLogProbMetric: 122.2319 - lr: 1.3717e-06 - 90s/epoch - 458ms/step
Epoch 126/1000
2023-10-26 00:15:37.908 
Epoch 126/1000 
	 loss: 121.8511, MinusLogProbMetric: 121.8511, val_loss: 121.7399, val_MinusLogProbMetric: 121.7399

Epoch 126: val_loss improved from 121.97137 to 121.73987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 121.8511 - MinusLogProbMetric: 121.8511 - val_loss: 121.7399 - val_MinusLogProbMetric: 121.7399 - lr: 1.3717e-06 - 91s/epoch - 467ms/step
Epoch 127/1000
2023-10-26 00:17:08.840 
Epoch 127/1000 
	 loss: 121.4229, MinusLogProbMetric: 121.4229, val_loss: 121.4163, val_MinusLogProbMetric: 121.4163

Epoch 127: val_loss improved from 121.73987 to 121.41625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 121.4229 - MinusLogProbMetric: 121.4229 - val_loss: 121.4163 - val_MinusLogProbMetric: 121.4163 - lr: 1.3717e-06 - 91s/epoch - 464ms/step
Epoch 128/1000
2023-10-26 00:18:40.998 
Epoch 128/1000 
	 loss: 121.1409, MinusLogProbMetric: 121.1409, val_loss: 121.2094, val_MinusLogProbMetric: 121.2094

Epoch 128: val_loss improved from 121.41625 to 121.20944, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 121.1409 - MinusLogProbMetric: 121.1409 - val_loss: 121.2094 - val_MinusLogProbMetric: 121.2094 - lr: 1.3717e-06 - 92s/epoch - 470ms/step
Epoch 129/1000
2023-10-26 00:20:12.437 
Epoch 129/1000 
	 loss: 120.8983, MinusLogProbMetric: 120.8983, val_loss: 120.9883, val_MinusLogProbMetric: 120.9883

Epoch 129: val_loss improved from 121.20944 to 120.98831, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 120.8983 - MinusLogProbMetric: 120.8983 - val_loss: 120.9883 - val_MinusLogProbMetric: 120.9883 - lr: 1.3717e-06 - 91s/epoch - 467ms/step
Epoch 130/1000
2023-10-26 00:21:43.994 
Epoch 130/1000 
	 loss: 120.6566, MinusLogProbMetric: 120.6566, val_loss: 120.7537, val_MinusLogProbMetric: 120.7537

Epoch 130: val_loss improved from 120.98831 to 120.75367, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 120.6566 - MinusLogProbMetric: 120.6566 - val_loss: 120.7537 - val_MinusLogProbMetric: 120.7537 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 131/1000
2023-10-26 00:23:14.745 
Epoch 131/1000 
	 loss: 120.4169, MinusLogProbMetric: 120.4169, val_loss: 120.5219, val_MinusLogProbMetric: 120.5219

Epoch 131: val_loss improved from 120.75367 to 120.52190, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 120.4169 - MinusLogProbMetric: 120.4169 - val_loss: 120.5219 - val_MinusLogProbMetric: 120.5219 - lr: 1.3717e-06 - 91s/epoch - 464ms/step
Epoch 132/1000
2023-10-26 00:24:45.468 
Epoch 132/1000 
	 loss: 120.2514, MinusLogProbMetric: 120.2514, val_loss: 120.3115, val_MinusLogProbMetric: 120.3115

Epoch 132: val_loss improved from 120.52190 to 120.31151, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 120.2514 - MinusLogProbMetric: 120.2514 - val_loss: 120.3115 - val_MinusLogProbMetric: 120.3115 - lr: 1.3717e-06 - 91s/epoch - 463ms/step
Epoch 133/1000
2023-10-26 00:26:16.464 
Epoch 133/1000 
	 loss: 120.0380, MinusLogProbMetric: 120.0380, val_loss: 120.1140, val_MinusLogProbMetric: 120.1140

Epoch 133: val_loss improved from 120.31151 to 120.11399, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 120.0380 - MinusLogProbMetric: 120.0380 - val_loss: 120.1140 - val_MinusLogProbMetric: 120.1140 - lr: 1.3717e-06 - 91s/epoch - 463ms/step
Epoch 134/1000
2023-10-26 00:27:46.863 
Epoch 134/1000 
	 loss: 119.8817, MinusLogProbMetric: 119.8817, val_loss: 120.0769, val_MinusLogProbMetric: 120.0769

Epoch 134: val_loss improved from 120.11399 to 120.07691, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 119.8817 - MinusLogProbMetric: 119.8817 - val_loss: 120.0769 - val_MinusLogProbMetric: 120.0769 - lr: 1.3717e-06 - 91s/epoch - 462ms/step
Epoch 135/1000
2023-10-26 00:29:18.202 
Epoch 135/1000 
	 loss: 122.7729, MinusLogProbMetric: 122.7729, val_loss: 120.5641, val_MinusLogProbMetric: 120.5641

Epoch 135: val_loss did not improve from 120.07691
196/196 - 90s - loss: 122.7729 - MinusLogProbMetric: 122.7729 - val_loss: 120.5641 - val_MinusLogProbMetric: 120.5641 - lr: 1.3717e-06 - 90s/epoch - 457ms/step
Epoch 136/1000
2023-10-26 00:30:47.436 
Epoch 136/1000 
	 loss: 119.9448, MinusLogProbMetric: 119.9448, val_loss: 119.8644, val_MinusLogProbMetric: 119.8644

Epoch 136: val_loss improved from 120.07691 to 119.86442, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 119.9448 - MinusLogProbMetric: 119.9448 - val_loss: 119.8644 - val_MinusLogProbMetric: 119.8644 - lr: 1.3717e-06 - 91s/epoch - 465ms/step
Epoch 137/1000
2023-10-26 00:32:18.351 
Epoch 137/1000 
	 loss: 119.5370, MinusLogProbMetric: 119.5370, val_loss: 119.6648, val_MinusLogProbMetric: 119.6648

Epoch 137: val_loss improved from 119.86442 to 119.66478, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 119.5370 - MinusLogProbMetric: 119.5370 - val_loss: 119.6648 - val_MinusLogProbMetric: 119.6648 - lr: 1.3717e-06 - 91s/epoch - 464ms/step
Epoch 138/1000
2023-10-26 00:33:49.624 
Epoch 138/1000 
	 loss: 119.2818, MinusLogProbMetric: 119.2818, val_loss: 119.4135, val_MinusLogProbMetric: 119.4135

Epoch 138: val_loss improved from 119.66478 to 119.41346, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 119.2818 - MinusLogProbMetric: 119.2818 - val_loss: 119.4135 - val_MinusLogProbMetric: 119.4135 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 139/1000
2023-10-26 00:35:21.527 
Epoch 139/1000 
	 loss: 119.1302, MinusLogProbMetric: 119.1302, val_loss: 119.2555, val_MinusLogProbMetric: 119.2555

Epoch 139: val_loss improved from 119.41346 to 119.25549, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 119.1302 - MinusLogProbMetric: 119.1302 - val_loss: 119.2555 - val_MinusLogProbMetric: 119.2555 - lr: 1.3717e-06 - 92s/epoch - 469ms/step
Epoch 140/1000
2023-10-26 00:36:53.540 
Epoch 140/1000 
	 loss: 119.0354, MinusLogProbMetric: 119.0354, val_loss: 119.1452, val_MinusLogProbMetric: 119.1452

Epoch 140: val_loss improved from 119.25549 to 119.14520, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 119.0354 - MinusLogProbMetric: 119.0354 - val_loss: 119.1452 - val_MinusLogProbMetric: 119.1452 - lr: 1.3717e-06 - 92s/epoch - 469ms/step
Epoch 141/1000
2023-10-26 00:38:24.891 
Epoch 141/1000 
	 loss: 118.7897, MinusLogProbMetric: 118.7897, val_loss: 118.9128, val_MinusLogProbMetric: 118.9128

Epoch 141: val_loss improved from 119.14520 to 118.91285, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 118.7897 - MinusLogProbMetric: 118.7897 - val_loss: 118.9128 - val_MinusLogProbMetric: 118.9128 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 142/1000
2023-10-26 00:39:55.689 
Epoch 142/1000 
	 loss: 118.6715, MinusLogProbMetric: 118.6715, val_loss: 118.8590, val_MinusLogProbMetric: 118.8590

Epoch 142: val_loss improved from 118.91285 to 118.85900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 118.6715 - MinusLogProbMetric: 118.6715 - val_loss: 118.8590 - val_MinusLogProbMetric: 118.8590 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 143/1000
2023-10-26 00:41:27.566 
Epoch 143/1000 
	 loss: 118.6088, MinusLogProbMetric: 118.6088, val_loss: 118.7230, val_MinusLogProbMetric: 118.7230

Epoch 143: val_loss improved from 118.85900 to 118.72295, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 118.6088 - MinusLogProbMetric: 118.6088 - val_loss: 118.7230 - val_MinusLogProbMetric: 118.7230 - lr: 1.3717e-06 - 91s/epoch - 467ms/step
Epoch 144/1000
2023-10-26 00:42:59.030 
Epoch 144/1000 
	 loss: 118.4154, MinusLogProbMetric: 118.4154, val_loss: 118.4893, val_MinusLogProbMetric: 118.4893

Epoch 144: val_loss improved from 118.72295 to 118.48928, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 118.4154 - MinusLogProbMetric: 118.4154 - val_loss: 118.4893 - val_MinusLogProbMetric: 118.4893 - lr: 1.3717e-06 - 91s/epoch - 467ms/step
Epoch 145/1000
2023-10-26 00:44:30.648 
Epoch 145/1000 
	 loss: 118.2677, MinusLogProbMetric: 118.2677, val_loss: 118.4774, val_MinusLogProbMetric: 118.4774

Epoch 145: val_loss improved from 118.48928 to 118.47741, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 118.2677 - MinusLogProbMetric: 118.2677 - val_loss: 118.4774 - val_MinusLogProbMetric: 118.4774 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 146/1000
2023-10-26 00:46:01.742 
Epoch 146/1000 
	 loss: 118.1299, MinusLogProbMetric: 118.1299, val_loss: 118.2794, val_MinusLogProbMetric: 118.2794

Epoch 146: val_loss improved from 118.47741 to 118.27943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 118.1299 - MinusLogProbMetric: 118.1299 - val_loss: 118.2794 - val_MinusLogProbMetric: 118.2794 - lr: 1.3717e-06 - 92s/epoch - 467ms/step
Epoch 147/1000
2023-10-26 00:47:33.731 
Epoch 147/1000 
	 loss: 118.0113, MinusLogProbMetric: 118.0113, val_loss: 118.2273, val_MinusLogProbMetric: 118.2273

Epoch 147: val_loss improved from 118.27943 to 118.22733, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 118.0113 - MinusLogProbMetric: 118.0113 - val_loss: 118.2273 - val_MinusLogProbMetric: 118.2273 - lr: 1.3717e-06 - 92s/epoch - 470ms/step
Epoch 148/1000
2023-10-26 00:49:05.495 
Epoch 148/1000 
	 loss: 117.8793, MinusLogProbMetric: 117.8793, val_loss: 118.1224, val_MinusLogProbMetric: 118.1224

Epoch 148: val_loss improved from 118.22733 to 118.12244, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 117.8793 - MinusLogProbMetric: 117.8793 - val_loss: 118.1224 - val_MinusLogProbMetric: 118.1224 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 149/1000
2023-10-26 00:50:36.318 
Epoch 149/1000 
	 loss: 118.1600, MinusLogProbMetric: 118.1600, val_loss: 118.3803, val_MinusLogProbMetric: 118.3803

Epoch 149: val_loss did not improve from 118.12244
196/196 - 89s - loss: 118.1600 - MinusLogProbMetric: 118.1600 - val_loss: 118.3803 - val_MinusLogProbMetric: 118.3803 - lr: 1.3717e-06 - 89s/epoch - 453ms/step
Epoch 150/1000
2023-10-26 00:52:03.147 
Epoch 150/1000 
	 loss: 117.8005, MinusLogProbMetric: 117.8005, val_loss: 118.0481, val_MinusLogProbMetric: 118.0481

Epoch 150: val_loss improved from 118.12244 to 118.04813, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 88s - loss: 117.8005 - MinusLogProbMetric: 117.8005 - val_loss: 118.0481 - val_MinusLogProbMetric: 118.0481 - lr: 1.3717e-06 - 88s/epoch - 450ms/step
Epoch 151/1000
2023-10-26 00:53:25.969 
Epoch 151/1000 
	 loss: 117.6014, MinusLogProbMetric: 117.6014, val_loss: 117.8740, val_MinusLogProbMetric: 117.8740

Epoch 151: val_loss improved from 118.04813 to 117.87395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 117.6014 - MinusLogProbMetric: 117.6014 - val_loss: 117.8740 - val_MinusLogProbMetric: 117.8740 - lr: 1.3717e-06 - 84s/epoch - 427ms/step
Epoch 152/1000
2023-10-26 00:54:58.582 
Epoch 152/1000 
	 loss: 117.3899, MinusLogProbMetric: 117.3899, val_loss: 117.5741, val_MinusLogProbMetric: 117.5741

Epoch 152: val_loss improved from 117.87395 to 117.57413, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 93s - loss: 117.3899 - MinusLogProbMetric: 117.3899 - val_loss: 117.5741 - val_MinusLogProbMetric: 117.5741 - lr: 1.3717e-06 - 93s/epoch - 473ms/step
Epoch 153/1000
2023-10-26 00:56:31.193 
Epoch 153/1000 
	 loss: 117.2711, MinusLogProbMetric: 117.2711, val_loss: 117.5430, val_MinusLogProbMetric: 117.5430

Epoch 153: val_loss improved from 117.57413 to 117.54305, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 93s - loss: 117.2711 - MinusLogProbMetric: 117.2711 - val_loss: 117.5430 - val_MinusLogProbMetric: 117.5430 - lr: 1.3717e-06 - 93s/epoch - 473ms/step
Epoch 154/1000
2023-10-26 00:58:03.028 
Epoch 154/1000 
	 loss: 117.1713, MinusLogProbMetric: 117.1713, val_loss: 117.3084, val_MinusLogProbMetric: 117.3084

Epoch 154: val_loss improved from 117.54305 to 117.30838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 117.1713 - MinusLogProbMetric: 117.1713 - val_loss: 117.3084 - val_MinusLogProbMetric: 117.3084 - lr: 1.3717e-06 - 91s/epoch - 467ms/step
Epoch 155/1000
2023-10-26 00:59:34.113 
Epoch 155/1000 
	 loss: 117.0825, MinusLogProbMetric: 117.0825, val_loss: 117.2642, val_MinusLogProbMetric: 117.2642

Epoch 155: val_loss improved from 117.30838 to 117.26424, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 117.0825 - MinusLogProbMetric: 117.0825 - val_loss: 117.2642 - val_MinusLogProbMetric: 117.2642 - lr: 1.3717e-06 - 91s/epoch - 467ms/step
Epoch 156/1000
2023-10-26 01:01:05.619 
Epoch 156/1000 
	 loss: 116.8403, MinusLogProbMetric: 116.8403, val_loss: 117.0849, val_MinusLogProbMetric: 117.0849

Epoch 156: val_loss improved from 117.26424 to 117.08486, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 116.8403 - MinusLogProbMetric: 116.8403 - val_loss: 117.0849 - val_MinusLogProbMetric: 117.0849 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 157/1000
2023-10-26 01:02:37.547 
Epoch 157/1000 
	 loss: 116.6683, MinusLogProbMetric: 116.6683, val_loss: 116.9648, val_MinusLogProbMetric: 116.9648

Epoch 157: val_loss improved from 117.08486 to 116.96479, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 116.6683 - MinusLogProbMetric: 116.6683 - val_loss: 116.9648 - val_MinusLogProbMetric: 116.9648 - lr: 1.3717e-06 - 92s/epoch - 468ms/step
Epoch 158/1000
2023-10-26 01:04:09.285 
Epoch 158/1000 
	 loss: 116.5415, MinusLogProbMetric: 116.5415, val_loss: 116.8011, val_MinusLogProbMetric: 116.8011

Epoch 158: val_loss improved from 116.96479 to 116.80111, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 116.5415 - MinusLogProbMetric: 116.5415 - val_loss: 116.8011 - val_MinusLogProbMetric: 116.8011 - lr: 1.3717e-06 - 92s/epoch - 468ms/step
Epoch 159/1000
2023-10-26 01:05:41.109 
Epoch 159/1000 
	 loss: 129.6525, MinusLogProbMetric: 129.6525, val_loss: 124.9021, val_MinusLogProbMetric: 124.9021

Epoch 159: val_loss did not improve from 116.80111
196/196 - 90s - loss: 129.6525 - MinusLogProbMetric: 129.6525 - val_loss: 124.9021 - val_MinusLogProbMetric: 124.9021 - lr: 1.3717e-06 - 90s/epoch - 459ms/step
Epoch 160/1000
2023-10-26 01:07:10.561 
Epoch 160/1000 
	 loss: 121.4224, MinusLogProbMetric: 121.4224, val_loss: 120.0980, val_MinusLogProbMetric: 120.0980

Epoch 160: val_loss did not improve from 116.80111
196/196 - 89s - loss: 121.4224 - MinusLogProbMetric: 121.4224 - val_loss: 120.0980 - val_MinusLogProbMetric: 120.0980 - lr: 1.3717e-06 - 89s/epoch - 456ms/step
Epoch 161/1000
2023-10-26 01:08:39.654 
Epoch 161/1000 
	 loss: 119.0335, MinusLogProbMetric: 119.0335, val_loss: 118.6788, val_MinusLogProbMetric: 118.6788

Epoch 161: val_loss did not improve from 116.80111
196/196 - 89s - loss: 119.0335 - MinusLogProbMetric: 119.0335 - val_loss: 118.6788 - val_MinusLogProbMetric: 118.6788 - lr: 1.3717e-06 - 89s/epoch - 455ms/step
Epoch 162/1000
2023-10-26 01:10:09.737 
Epoch 162/1000 
	 loss: 118.0562, MinusLogProbMetric: 118.0562, val_loss: 117.9465, val_MinusLogProbMetric: 117.9465

Epoch 162: val_loss did not improve from 116.80111
196/196 - 90s - loss: 118.0562 - MinusLogProbMetric: 118.0562 - val_loss: 117.9465 - val_MinusLogProbMetric: 117.9465 - lr: 1.3717e-06 - 90s/epoch - 460ms/step
Epoch 163/1000
2023-10-26 01:11:39.266 
Epoch 163/1000 
	 loss: 117.3722, MinusLogProbMetric: 117.3722, val_loss: 117.4317, val_MinusLogProbMetric: 117.4317

Epoch 163: val_loss did not improve from 116.80111
196/196 - 90s - loss: 117.3722 - MinusLogProbMetric: 117.3722 - val_loss: 117.4317 - val_MinusLogProbMetric: 117.4317 - lr: 1.3717e-06 - 90s/epoch - 457ms/step
Epoch 164/1000
2023-10-26 01:13:08.859 
Epoch 164/1000 
	 loss: 117.0387, MinusLogProbMetric: 117.0387, val_loss: 117.2344, val_MinusLogProbMetric: 117.2344

Epoch 164: val_loss did not improve from 116.80111
196/196 - 90s - loss: 117.0387 - MinusLogProbMetric: 117.0387 - val_loss: 117.2344 - val_MinusLogProbMetric: 117.2344 - lr: 1.3717e-06 - 90s/epoch - 457ms/step
Epoch 165/1000
2023-10-26 01:14:38.502 
Epoch 165/1000 
	 loss: 116.7583, MinusLogProbMetric: 116.7583, val_loss: 116.9006, val_MinusLogProbMetric: 116.9006

Epoch 165: val_loss did not improve from 116.80111
196/196 - 90s - loss: 116.7583 - MinusLogProbMetric: 116.7583 - val_loss: 116.9006 - val_MinusLogProbMetric: 116.9006 - lr: 1.3717e-06 - 90s/epoch - 457ms/step
Epoch 166/1000
2023-10-26 01:16:08.622 
Epoch 166/1000 
	 loss: 116.4963, MinusLogProbMetric: 116.4963, val_loss: 116.6141, val_MinusLogProbMetric: 116.6141

Epoch 166: val_loss improved from 116.80111 to 116.61407, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 116.4963 - MinusLogProbMetric: 116.4963 - val_loss: 116.6141 - val_MinusLogProbMetric: 116.6141 - lr: 1.3717e-06 - 92s/epoch - 472ms/step
Epoch 167/1000
2023-10-26 01:17:40.490 
Epoch 167/1000 
	 loss: 116.2962, MinusLogProbMetric: 116.2962, val_loss: 116.5040, val_MinusLogProbMetric: 116.5040

Epoch 167: val_loss improved from 116.61407 to 116.50401, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 116.2962 - MinusLogProbMetric: 116.2962 - val_loss: 116.5040 - val_MinusLogProbMetric: 116.5040 - lr: 1.3717e-06 - 92s/epoch - 468ms/step
Epoch 168/1000
2023-10-26 01:19:13.201 
Epoch 168/1000 
	 loss: 116.0979, MinusLogProbMetric: 116.0979, val_loss: 116.3316, val_MinusLogProbMetric: 116.3316

Epoch 168: val_loss improved from 116.50401 to 116.33161, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 116.0979 - MinusLogProbMetric: 116.0979 - val_loss: 116.3316 - val_MinusLogProbMetric: 116.3316 - lr: 1.3717e-06 - 92s/epoch - 471ms/step
Epoch 169/1000
2023-10-26 01:20:44.530 
Epoch 169/1000 
	 loss: 115.9595, MinusLogProbMetric: 115.9595, val_loss: 116.2463, val_MinusLogProbMetric: 116.2463

Epoch 169: val_loss improved from 116.33161 to 116.24630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 115.9595 - MinusLogProbMetric: 115.9595 - val_loss: 116.2463 - val_MinusLogProbMetric: 116.2463 - lr: 1.3717e-06 - 92s/epoch - 469ms/step
Epoch 170/1000
2023-10-26 01:22:16.788 
Epoch 170/1000 
	 loss: 115.8318, MinusLogProbMetric: 115.8318, val_loss: 116.0536, val_MinusLogProbMetric: 116.0536

Epoch 170: val_loss improved from 116.24630 to 116.05360, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 115.8318 - MinusLogProbMetric: 115.8318 - val_loss: 116.0536 - val_MinusLogProbMetric: 116.0536 - lr: 1.3717e-06 - 92s/epoch - 469ms/step
Epoch 171/1000
2023-10-26 01:23:48.398 
Epoch 171/1000 
	 loss: 115.6999, MinusLogProbMetric: 115.6999, val_loss: 115.9950, val_MinusLogProbMetric: 115.9950

Epoch 171: val_loss improved from 116.05360 to 115.99502, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 115.6999 - MinusLogProbMetric: 115.6999 - val_loss: 115.9950 - val_MinusLogProbMetric: 115.9950 - lr: 1.3717e-06 - 92s/epoch - 469ms/step
Epoch 172/1000
2023-10-26 01:25:19.298 
Epoch 172/1000 
	 loss: 115.6282, MinusLogProbMetric: 115.6282, val_loss: 115.8163, val_MinusLogProbMetric: 115.8163

Epoch 172: val_loss improved from 115.99502 to 115.81630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 115.6282 - MinusLogProbMetric: 115.6282 - val_loss: 115.8163 - val_MinusLogProbMetric: 115.8163 - lr: 1.3717e-06 - 91s/epoch - 464ms/step
Epoch 173/1000
2023-10-26 01:26:50.463 
Epoch 173/1000 
	 loss: 115.4741, MinusLogProbMetric: 115.4741, val_loss: 115.7561, val_MinusLogProbMetric: 115.7561

Epoch 173: val_loss improved from 115.81630 to 115.75611, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 115.4741 - MinusLogProbMetric: 115.4741 - val_loss: 115.7561 - val_MinusLogProbMetric: 115.7561 - lr: 1.3717e-06 - 91s/epoch - 465ms/step
Epoch 174/1000
2023-10-26 01:28:20.815 
Epoch 174/1000 
	 loss: 115.5122, MinusLogProbMetric: 115.5122, val_loss: 115.8185, val_MinusLogProbMetric: 115.8185

Epoch 174: val_loss did not improve from 115.75611
196/196 - 88s - loss: 115.5122 - MinusLogProbMetric: 115.5122 - val_loss: 115.8185 - val_MinusLogProbMetric: 115.8185 - lr: 1.3717e-06 - 88s/epoch - 450ms/step
Epoch 175/1000
2023-10-26 01:29:49.954 
Epoch 175/1000 
	 loss: 115.3676, MinusLogProbMetric: 115.3676, val_loss: 115.6260, val_MinusLogProbMetric: 115.6260

Epoch 175: val_loss improved from 115.75611 to 115.62598, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 115.3676 - MinusLogProbMetric: 115.3676 - val_loss: 115.6260 - val_MinusLogProbMetric: 115.6260 - lr: 1.3717e-06 - 91s/epoch - 464ms/step
Epoch 176/1000
2023-10-26 01:31:21.291 
Epoch 176/1000 
	 loss: 115.4248, MinusLogProbMetric: 115.4248, val_loss: 115.4255, val_MinusLogProbMetric: 115.4255

Epoch 176: val_loss improved from 115.62598 to 115.42550, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 115.4248 - MinusLogProbMetric: 115.4248 - val_loss: 115.4255 - val_MinusLogProbMetric: 115.4255 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 177/1000
2023-10-26 01:32:52.245 
Epoch 177/1000 
	 loss: 115.0844, MinusLogProbMetric: 115.0844, val_loss: 115.4314, val_MinusLogProbMetric: 115.4314

Epoch 177: val_loss did not improve from 115.42550
196/196 - 89s - loss: 115.0844 - MinusLogProbMetric: 115.0844 - val_loss: 115.4314 - val_MinusLogProbMetric: 115.4314 - lr: 1.3717e-06 - 89s/epoch - 455ms/step
Epoch 178/1000
2023-10-26 01:34:22.374 
Epoch 178/1000 
	 loss: 115.0653, MinusLogProbMetric: 115.0653, val_loss: 115.2797, val_MinusLogProbMetric: 115.2797

Epoch 178: val_loss improved from 115.42550 to 115.27966, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 115.0653 - MinusLogProbMetric: 115.0653 - val_loss: 115.2797 - val_MinusLogProbMetric: 115.2797 - lr: 1.3717e-06 - 92s/epoch - 471ms/step
Epoch 179/1000
2023-10-26 01:35:54.147 
Epoch 179/1000 
	 loss: 114.9028, MinusLogProbMetric: 114.9028, val_loss: 115.1653, val_MinusLogProbMetric: 115.1653

Epoch 179: val_loss improved from 115.27966 to 115.16531, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 114.9028 - MinusLogProbMetric: 114.9028 - val_loss: 115.1653 - val_MinusLogProbMetric: 115.1653 - lr: 1.3717e-06 - 92s/epoch - 469ms/step
Epoch 180/1000
2023-10-26 01:37:26.477 
Epoch 180/1000 
	 loss: 114.7737, MinusLogProbMetric: 114.7737, val_loss: 115.0488, val_MinusLogProbMetric: 115.0488

Epoch 180: val_loss improved from 115.16531 to 115.04881, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 114.7737 - MinusLogProbMetric: 114.7737 - val_loss: 115.0488 - val_MinusLogProbMetric: 115.0488 - lr: 1.3717e-06 - 92s/epoch - 470ms/step
Epoch 181/1000
2023-10-26 01:38:40.026 
Epoch 181/1000 
	 loss: 114.5635, MinusLogProbMetric: 114.5635, val_loss: 114.8133, val_MinusLogProbMetric: 114.8133

Epoch 181: val_loss improved from 115.04881 to 114.81326, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 74s - loss: 114.5635 - MinusLogProbMetric: 114.5635 - val_loss: 114.8133 - val_MinusLogProbMetric: 114.8133 - lr: 1.3717e-06 - 74s/epoch - 377ms/step
Epoch 182/1000
2023-10-26 01:39:57.241 
Epoch 182/1000 
	 loss: 114.5752, MinusLogProbMetric: 114.5752, val_loss: 114.8123, val_MinusLogProbMetric: 114.8123

Epoch 182: val_loss improved from 114.81326 to 114.81230, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 77s - loss: 114.5752 - MinusLogProbMetric: 114.5752 - val_loss: 114.8123 - val_MinusLogProbMetric: 114.8123 - lr: 1.3717e-06 - 77s/epoch - 392ms/step
Epoch 183/1000
2023-10-26 01:41:27.819 
Epoch 183/1000 
	 loss: 114.4059, MinusLogProbMetric: 114.4059, val_loss: 114.6478, val_MinusLogProbMetric: 114.6478

Epoch 183: val_loss improved from 114.81230 to 114.64784, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 114.4059 - MinusLogProbMetric: 114.4059 - val_loss: 114.6478 - val_MinusLogProbMetric: 114.6478 - lr: 1.3717e-06 - 90s/epoch - 462ms/step
Epoch 184/1000
2023-10-26 01:42:58.921 
Epoch 184/1000 
	 loss: 114.2571, MinusLogProbMetric: 114.2571, val_loss: 114.4799, val_MinusLogProbMetric: 114.4799

Epoch 184: val_loss improved from 114.64784 to 114.47988, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 114.2571 - MinusLogProbMetric: 114.2571 - val_loss: 114.4799 - val_MinusLogProbMetric: 114.4799 - lr: 1.3717e-06 - 91s/epoch - 467ms/step
Epoch 185/1000
2023-10-26 01:44:29.460 
Epoch 185/1000 
	 loss: 114.1270, MinusLogProbMetric: 114.1270, val_loss: 114.3655, val_MinusLogProbMetric: 114.3655

Epoch 185: val_loss improved from 114.47988 to 114.36549, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 114.1270 - MinusLogProbMetric: 114.1270 - val_loss: 114.3655 - val_MinusLogProbMetric: 114.3655 - lr: 1.3717e-06 - 90s/epoch - 461ms/step
Epoch 186/1000
2023-10-26 01:46:00.572 
Epoch 186/1000 
	 loss: 113.9972, MinusLogProbMetric: 113.9972, val_loss: 114.2670, val_MinusLogProbMetric: 114.2670

Epoch 186: val_loss improved from 114.36549 to 114.26701, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 113.9972 - MinusLogProbMetric: 113.9972 - val_loss: 114.2670 - val_MinusLogProbMetric: 114.2670 - lr: 1.3717e-06 - 91s/epoch - 465ms/step
Epoch 187/1000
2023-10-26 01:47:32.486 
Epoch 187/1000 
	 loss: 113.8710, MinusLogProbMetric: 113.8710, val_loss: 114.1888, val_MinusLogProbMetric: 114.1888

Epoch 187: val_loss improved from 114.26701 to 114.18876, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 113.8710 - MinusLogProbMetric: 113.8710 - val_loss: 114.1888 - val_MinusLogProbMetric: 114.1888 - lr: 1.3717e-06 - 92s/epoch - 469ms/step
Epoch 188/1000
2023-10-26 01:49:04.352 
Epoch 188/1000 
	 loss: 113.7698, MinusLogProbMetric: 113.7698, val_loss: 114.0516, val_MinusLogProbMetric: 114.0516

Epoch 188: val_loss improved from 114.18876 to 114.05163, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 113.7698 - MinusLogProbMetric: 113.7698 - val_loss: 114.0516 - val_MinusLogProbMetric: 114.0516 - lr: 1.3717e-06 - 92s/epoch - 467ms/step
Epoch 189/1000
2023-10-26 01:50:35.429 
Epoch 189/1000 
	 loss: 113.6880, MinusLogProbMetric: 113.6880, val_loss: 114.0306, val_MinusLogProbMetric: 114.0306

Epoch 189: val_loss improved from 114.05163 to 114.03056, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 113.6880 - MinusLogProbMetric: 113.6880 - val_loss: 114.0306 - val_MinusLogProbMetric: 114.0306 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 190/1000
2023-10-26 01:52:06.339 
Epoch 190/1000 
	 loss: 113.6769, MinusLogProbMetric: 113.6769, val_loss: 113.9748, val_MinusLogProbMetric: 113.9748

Epoch 190: val_loss improved from 114.03056 to 113.97476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 113.6769 - MinusLogProbMetric: 113.6769 - val_loss: 113.9748 - val_MinusLogProbMetric: 113.9748 - lr: 1.3717e-06 - 91s/epoch - 464ms/step
Epoch 191/1000
2023-10-26 01:53:36.798 
Epoch 191/1000 
	 loss: 113.5680, MinusLogProbMetric: 113.5680, val_loss: 113.8915, val_MinusLogProbMetric: 113.8915

Epoch 191: val_loss improved from 113.97476 to 113.89153, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 113.5680 - MinusLogProbMetric: 113.5680 - val_loss: 113.8915 - val_MinusLogProbMetric: 113.8915 - lr: 1.3717e-06 - 90s/epoch - 459ms/step
Epoch 192/1000
2023-10-26 01:55:07.054 
Epoch 192/1000 
	 loss: 113.4646, MinusLogProbMetric: 113.4646, val_loss: 113.7825, val_MinusLogProbMetric: 113.7825

Epoch 192: val_loss improved from 113.89153 to 113.78250, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 113.4646 - MinusLogProbMetric: 113.4646 - val_loss: 113.7825 - val_MinusLogProbMetric: 113.7825 - lr: 1.3717e-06 - 90s/epoch - 460ms/step
Epoch 193/1000
2023-10-26 01:56:36.083 
Epoch 193/1000 
	 loss: 113.3515, MinusLogProbMetric: 113.3515, val_loss: 113.6884, val_MinusLogProbMetric: 113.6884

Epoch 193: val_loss improved from 113.78250 to 113.68838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 113.3515 - MinusLogProbMetric: 113.3515 - val_loss: 113.6884 - val_MinusLogProbMetric: 113.6884 - lr: 1.3717e-06 - 89s/epoch - 456ms/step
Epoch 194/1000
2023-10-26 01:58:06.322 
Epoch 194/1000 
	 loss: 113.2890, MinusLogProbMetric: 113.2890, val_loss: 113.6065, val_MinusLogProbMetric: 113.6065

Epoch 194: val_loss improved from 113.68838 to 113.60645, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 113.2890 - MinusLogProbMetric: 113.2890 - val_loss: 113.6065 - val_MinusLogProbMetric: 113.6065 - lr: 1.3717e-06 - 90s/epoch - 460ms/step
Epoch 195/1000
2023-10-26 01:59:36.913 
Epoch 195/1000 
	 loss: 113.1681, MinusLogProbMetric: 113.1681, val_loss: 113.6107, val_MinusLogProbMetric: 113.6107

Epoch 195: val_loss did not improve from 113.60645
196/196 - 89s - loss: 113.1681 - MinusLogProbMetric: 113.1681 - val_loss: 113.6107 - val_MinusLogProbMetric: 113.6107 - lr: 1.3717e-06 - 89s/epoch - 452ms/step
Epoch 196/1000
2023-10-26 02:01:05.786 
Epoch 196/1000 
	 loss: 113.0806, MinusLogProbMetric: 113.0806, val_loss: 113.4268, val_MinusLogProbMetric: 113.4268

Epoch 196: val_loss improved from 113.60645 to 113.42678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 113.0806 - MinusLogProbMetric: 113.0806 - val_loss: 113.4268 - val_MinusLogProbMetric: 113.4268 - lr: 1.3717e-06 - 91s/epoch - 464ms/step
Epoch 197/1000
2023-10-26 02:02:35.995 
Epoch 197/1000 
	 loss: 113.0022, MinusLogProbMetric: 113.0022, val_loss: 113.3922, val_MinusLogProbMetric: 113.3922

Epoch 197: val_loss improved from 113.42678 to 113.39217, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 113.0022 - MinusLogProbMetric: 113.0022 - val_loss: 113.3922 - val_MinusLogProbMetric: 113.3922 - lr: 1.3717e-06 - 90s/epoch - 459ms/step
Epoch 198/1000
2023-10-26 02:04:06.444 
Epoch 198/1000 
	 loss: 112.8883, MinusLogProbMetric: 112.8883, val_loss: 113.2341, val_MinusLogProbMetric: 113.2341

Epoch 198: val_loss improved from 113.39217 to 113.23413, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 112.8883 - MinusLogProbMetric: 112.8883 - val_loss: 113.2341 - val_MinusLogProbMetric: 113.2341 - lr: 1.3717e-06 - 91s/epoch - 463ms/step
Epoch 199/1000
2023-10-26 02:05:36.660 
Epoch 199/1000 
	 loss: 112.7724, MinusLogProbMetric: 112.7724, val_loss: 113.0574, val_MinusLogProbMetric: 113.0574

Epoch 199: val_loss improved from 113.23413 to 113.05741, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 112.7724 - MinusLogProbMetric: 112.7724 - val_loss: 113.0574 - val_MinusLogProbMetric: 113.0574 - lr: 1.3717e-06 - 90s/epoch - 458ms/step
Epoch 200/1000
2023-10-26 02:07:07.009 
Epoch 200/1000 
	 loss: 112.7984, MinusLogProbMetric: 112.7984, val_loss: 113.1087, val_MinusLogProbMetric: 113.1087

Epoch 200: val_loss did not improve from 113.05741
196/196 - 89s - loss: 112.7984 - MinusLogProbMetric: 112.7984 - val_loss: 113.1087 - val_MinusLogProbMetric: 113.1087 - lr: 1.3717e-06 - 89s/epoch - 452ms/step
Epoch 201/1000
2023-10-26 02:08:35.893 
Epoch 201/1000 
	 loss: 112.6237, MinusLogProbMetric: 112.6237, val_loss: 112.9823, val_MinusLogProbMetric: 112.9823

Epoch 201: val_loss improved from 113.05741 to 112.98227, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 112.6237 - MinusLogProbMetric: 112.6237 - val_loss: 112.9823 - val_MinusLogProbMetric: 112.9823 - lr: 1.3717e-06 - 91s/epoch - 465ms/step
Epoch 202/1000
2023-10-26 02:10:05.855 
Epoch 202/1000 
	 loss: 112.4911, MinusLogProbMetric: 112.4911, val_loss: 112.8922, val_MinusLogProbMetric: 112.8922

Epoch 202: val_loss improved from 112.98227 to 112.89223, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 112.4911 - MinusLogProbMetric: 112.4911 - val_loss: 112.8922 - val_MinusLogProbMetric: 112.8922 - lr: 1.3717e-06 - 89s/epoch - 456ms/step
Epoch 203/1000
2023-10-26 02:11:36.827 
Epoch 203/1000 
	 loss: 112.3982, MinusLogProbMetric: 112.3982, val_loss: 112.7789, val_MinusLogProbMetric: 112.7789

Epoch 203: val_loss improved from 112.89223 to 112.77894, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 112.3982 - MinusLogProbMetric: 112.3982 - val_loss: 112.7789 - val_MinusLogProbMetric: 112.7789 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 204/1000
2023-10-26 02:13:08.383 
Epoch 204/1000 
	 loss: 119.7025, MinusLogProbMetric: 119.7025, val_loss: 281.6652, val_MinusLogProbMetric: 281.6652

Epoch 204: val_loss did not improve from 112.77894
196/196 - 89s - loss: 119.7025 - MinusLogProbMetric: 119.7025 - val_loss: 281.6652 - val_MinusLogProbMetric: 281.6652 - lr: 1.3717e-06 - 89s/epoch - 456ms/step
Epoch 205/1000
2023-10-26 02:14:36.992 
Epoch 205/1000 
	 loss: 126.0910, MinusLogProbMetric: 126.0910, val_loss: 113.3284, val_MinusLogProbMetric: 113.3284

Epoch 205: val_loss did not improve from 112.77894
196/196 - 89s - loss: 126.0910 - MinusLogProbMetric: 126.0910 - val_loss: 113.3284 - val_MinusLogProbMetric: 113.3284 - lr: 1.3717e-06 - 89s/epoch - 452ms/step
Epoch 206/1000
2023-10-26 02:16:06.422 
Epoch 206/1000 
	 loss: 112.4867, MinusLogProbMetric: 112.4867, val_loss: 112.6189, val_MinusLogProbMetric: 112.6189

Epoch 206: val_loss improved from 112.77894 to 112.61885, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 112.4867 - MinusLogProbMetric: 112.4867 - val_loss: 112.6189 - val_MinusLogProbMetric: 112.6189 - lr: 1.3717e-06 - 92s/epoch - 468ms/step
Epoch 207/1000
2023-10-26 02:17:37.770 
Epoch 207/1000 
	 loss: 112.2018, MinusLogProbMetric: 112.2018, val_loss: 112.3474, val_MinusLogProbMetric: 112.3474

Epoch 207: val_loss improved from 112.61885 to 112.34744, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 112.2018 - MinusLogProbMetric: 112.2018 - val_loss: 112.3474 - val_MinusLogProbMetric: 112.3474 - lr: 1.3717e-06 - 91s/epoch - 466ms/step
Epoch 208/1000
2023-10-26 02:19:09.680 
Epoch 208/1000 
	 loss: 112.4546, MinusLogProbMetric: 112.4546, val_loss: 112.7209, val_MinusLogProbMetric: 112.7209

Epoch 208: val_loss did not improve from 112.34744
196/196 - 90s - loss: 112.4546 - MinusLogProbMetric: 112.4546 - val_loss: 112.7209 - val_MinusLogProbMetric: 112.7209 - lr: 1.3717e-06 - 90s/epoch - 457ms/step
Epoch 209/1000
2023-10-26 02:20:38.308 
Epoch 209/1000 
	 loss: 112.0579, MinusLogProbMetric: 112.0579, val_loss: 112.1698, val_MinusLogProbMetric: 112.1698

Epoch 209: val_loss improved from 112.34744 to 112.16983, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 112.0579 - MinusLogProbMetric: 112.0579 - val_loss: 112.1698 - val_MinusLogProbMetric: 112.1698 - lr: 1.3717e-06 - 91s/epoch - 464ms/step
Epoch 210/1000
2023-10-26 02:22:09.387 
Epoch 210/1000 
	 loss: 111.8083, MinusLogProbMetric: 111.8083, val_loss: 112.0740, val_MinusLogProbMetric: 112.0740

Epoch 210: val_loss improved from 112.16983 to 112.07400, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 111.8083 - MinusLogProbMetric: 111.8083 - val_loss: 112.0740 - val_MinusLogProbMetric: 112.0740 - lr: 1.3717e-06 - 90s/epoch - 461ms/step
Epoch 211/1000
2023-10-26 02:23:39.096 
Epoch 211/1000 
	 loss: 111.7216, MinusLogProbMetric: 111.7216, val_loss: 111.9865, val_MinusLogProbMetric: 111.9865

Epoch 211: val_loss improved from 112.07400 to 111.98645, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 111.7216 - MinusLogProbMetric: 111.7216 - val_loss: 111.9865 - val_MinusLogProbMetric: 111.9865 - lr: 1.3717e-06 - 90s/epoch - 461ms/step
Epoch 212/1000
2023-10-26 02:25:09.915 
Epoch 212/1000 
	 loss: 111.7117, MinusLogProbMetric: 111.7117, val_loss: 111.9839, val_MinusLogProbMetric: 111.9839

Epoch 212: val_loss improved from 111.98645 to 111.98386, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 111.7117 - MinusLogProbMetric: 111.7117 - val_loss: 111.9839 - val_MinusLogProbMetric: 111.9839 - lr: 1.3717e-06 - 91s/epoch - 463ms/step
Epoch 213/1000
2023-10-26 02:26:40.718 
Epoch 213/1000 
	 loss: 111.6272, MinusLogProbMetric: 111.6272, val_loss: 111.8037, val_MinusLogProbMetric: 111.8037

Epoch 213: val_loss improved from 111.98386 to 111.80375, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 111.6272 - MinusLogProbMetric: 111.6272 - val_loss: 111.8037 - val_MinusLogProbMetric: 111.8037 - lr: 1.3717e-06 - 91s/epoch - 463ms/step
Epoch 214/1000
2023-10-26 02:28:11.365 
Epoch 214/1000 
	 loss: 111.4651, MinusLogProbMetric: 111.4651, val_loss: 111.6718, val_MinusLogProbMetric: 111.6718

Epoch 214: val_loss improved from 111.80375 to 111.67181, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 111.4651 - MinusLogProbMetric: 111.4651 - val_loss: 111.6718 - val_MinusLogProbMetric: 111.6718 - lr: 1.3717e-06 - 91s/epoch - 463ms/step
Epoch 215/1000
2023-10-26 02:29:41.742 
Epoch 215/1000 
	 loss: 202.5895, MinusLogProbMetric: 202.5895, val_loss: 256.5805, val_MinusLogProbMetric: 256.5805

Epoch 215: val_loss did not improve from 111.67181
196/196 - 88s - loss: 202.5895 - MinusLogProbMetric: 202.5895 - val_loss: 256.5805 - val_MinusLogProbMetric: 256.5805 - lr: 1.3717e-06 - 88s/epoch - 449ms/step
Epoch 216/1000
2023-10-26 02:31:09.759 
Epoch 216/1000 
	 loss: 219.4220, MinusLogProbMetric: 219.4220, val_loss: 196.1801, val_MinusLogProbMetric: 196.1801

Epoch 216: val_loss did not improve from 111.67181
196/196 - 88s - loss: 219.4220 - MinusLogProbMetric: 219.4220 - val_loss: 196.1801 - val_MinusLogProbMetric: 196.1801 - lr: 1.3717e-06 - 88s/epoch - 449ms/step
Epoch 217/1000
2023-10-26 02:32:36.709 
Epoch 217/1000 
	 loss: 186.7247, MinusLogProbMetric: 186.7247, val_loss: 178.9222, val_MinusLogProbMetric: 178.9222

Epoch 217: val_loss did not improve from 111.67181
196/196 - 87s - loss: 186.7247 - MinusLogProbMetric: 186.7247 - val_loss: 178.9222 - val_MinusLogProbMetric: 178.9222 - lr: 1.3717e-06 - 87s/epoch - 444ms/step
Epoch 218/1000
2023-10-26 02:34:05.356 
Epoch 218/1000 
	 loss: 174.8427, MinusLogProbMetric: 174.8427, val_loss: 170.7982, val_MinusLogProbMetric: 170.7982

Epoch 218: val_loss did not improve from 111.67181
196/196 - 89s - loss: 174.8427 - MinusLogProbMetric: 174.8427 - val_loss: 170.7982 - val_MinusLogProbMetric: 170.7982 - lr: 1.3717e-06 - 89s/epoch - 452ms/step
Epoch 219/1000
2023-10-26 02:35:32.915 
Epoch 219/1000 
	 loss: 168.3017, MinusLogProbMetric: 168.3017, val_loss: 165.4009, val_MinusLogProbMetric: 165.4009

Epoch 219: val_loss did not improve from 111.67181
196/196 - 88s - loss: 168.3017 - MinusLogProbMetric: 168.3017 - val_loss: 165.4009 - val_MinusLogProbMetric: 165.4009 - lr: 1.3717e-06 - 88s/epoch - 447ms/step
Epoch 220/1000
2023-10-26 02:37:00.589 
Epoch 220/1000 
	 loss: 163.6085, MinusLogProbMetric: 163.6085, val_loss: 161.5992, val_MinusLogProbMetric: 161.5992

Epoch 220: val_loss did not improve from 111.67181
196/196 - 88s - loss: 163.6085 - MinusLogProbMetric: 163.6085 - val_loss: 161.5992 - val_MinusLogProbMetric: 161.5992 - lr: 1.3717e-06 - 88s/epoch - 447ms/step
Epoch 221/1000
2023-10-26 02:38:28.965 
Epoch 221/1000 
	 loss: 160.2349, MinusLogProbMetric: 160.2349, val_loss: 158.5703, val_MinusLogProbMetric: 158.5703

Epoch 221: val_loss did not improve from 111.67181
196/196 - 88s - loss: 160.2349 - MinusLogProbMetric: 160.2349 - val_loss: 158.5703 - val_MinusLogProbMetric: 158.5703 - lr: 1.3717e-06 - 88s/epoch - 451ms/step
Epoch 222/1000
2023-10-26 02:39:57.319 
Epoch 222/1000 
	 loss: 157.9337, MinusLogProbMetric: 157.9337, val_loss: 156.6241, val_MinusLogProbMetric: 156.6241

Epoch 222: val_loss did not improve from 111.67181
196/196 - 88s - loss: 157.9337 - MinusLogProbMetric: 157.9337 - val_loss: 156.6241 - val_MinusLogProbMetric: 156.6241 - lr: 1.3717e-06 - 88s/epoch - 451ms/step
Epoch 223/1000
2023-10-26 02:41:26.392 
Epoch 223/1000 
	 loss: 156.2326, MinusLogProbMetric: 156.2326, val_loss: 154.6568, val_MinusLogProbMetric: 154.6568

Epoch 223: val_loss did not improve from 111.67181
196/196 - 89s - loss: 156.2326 - MinusLogProbMetric: 156.2326 - val_loss: 154.6568 - val_MinusLogProbMetric: 154.6568 - lr: 1.3717e-06 - 89s/epoch - 454ms/step
Epoch 224/1000
2023-10-26 02:42:54.715 
Epoch 224/1000 
	 loss: 154.6606, MinusLogProbMetric: 154.6606, val_loss: 152.5784, val_MinusLogProbMetric: 152.5784

Epoch 224: val_loss did not improve from 111.67181
196/196 - 88s - loss: 154.6606 - MinusLogProbMetric: 154.6606 - val_loss: 152.5784 - val_MinusLogProbMetric: 152.5784 - lr: 1.3717e-06 - 88s/epoch - 451ms/step
Epoch 225/1000
2023-10-26 02:44:23.704 
Epoch 225/1000 
	 loss: 151.4752, MinusLogProbMetric: 151.4752, val_loss: 150.0971, val_MinusLogProbMetric: 150.0971

Epoch 225: val_loss did not improve from 111.67181
196/196 - 89s - loss: 151.4752 - MinusLogProbMetric: 151.4752 - val_loss: 150.0971 - val_MinusLogProbMetric: 150.0971 - lr: 1.3717e-06 - 89s/epoch - 454ms/step
Epoch 226/1000
2023-10-26 02:45:52.064 
Epoch 226/1000 
	 loss: 149.3673, MinusLogProbMetric: 149.3673, val_loss: 148.5475, val_MinusLogProbMetric: 148.5475

Epoch 226: val_loss did not improve from 111.67181
196/196 - 88s - loss: 149.3673 - MinusLogProbMetric: 149.3673 - val_loss: 148.5475 - val_MinusLogProbMetric: 148.5475 - lr: 1.3717e-06 - 88s/epoch - 451ms/step
Epoch 227/1000
2023-10-26 02:47:20.302 
Epoch 227/1000 
	 loss: 147.6615, MinusLogProbMetric: 147.6615, val_loss: 147.0388, val_MinusLogProbMetric: 147.0388

Epoch 227: val_loss did not improve from 111.67181
196/196 - 88s - loss: 147.6615 - MinusLogProbMetric: 147.6615 - val_loss: 147.0388 - val_MinusLogProbMetric: 147.0388 - lr: 1.3717e-06 - 88s/epoch - 450ms/step
Epoch 228/1000
2023-10-26 02:48:48.976 
Epoch 228/1000 
	 loss: 146.2633, MinusLogProbMetric: 146.2633, val_loss: 145.5945, val_MinusLogProbMetric: 145.5945

Epoch 228: val_loss did not improve from 111.67181
196/196 - 89s - loss: 146.2633 - MinusLogProbMetric: 146.2633 - val_loss: 145.5945 - val_MinusLogProbMetric: 145.5945 - lr: 1.3717e-06 - 89s/epoch - 452ms/step
Epoch 229/1000
2023-10-26 02:50:18.718 
Epoch 229/1000 
	 loss: 144.9352, MinusLogProbMetric: 144.9352, val_loss: 144.3656, val_MinusLogProbMetric: 144.3656

Epoch 229: val_loss did not improve from 111.67181
196/196 - 90s - loss: 144.9352 - MinusLogProbMetric: 144.9352 - val_loss: 144.3656 - val_MinusLogProbMetric: 144.3656 - lr: 1.3717e-06 - 90s/epoch - 458ms/step
Epoch 230/1000
2023-10-26 02:51:48.414 
Epoch 230/1000 
	 loss: 143.7563, MinusLogProbMetric: 143.7563, val_loss: 143.2892, val_MinusLogProbMetric: 143.2892

Epoch 230: val_loss did not improve from 111.67181
196/196 - 90s - loss: 143.7563 - MinusLogProbMetric: 143.7563 - val_loss: 143.2892 - val_MinusLogProbMetric: 143.2892 - lr: 1.3717e-06 - 90s/epoch - 458ms/step
Epoch 231/1000
2023-10-26 02:53:17.667 
Epoch 231/1000 
	 loss: 142.7025, MinusLogProbMetric: 142.7025, val_loss: 142.3202, val_MinusLogProbMetric: 142.3202

Epoch 231: val_loss did not improve from 111.67181
196/196 - 89s - loss: 142.7025 - MinusLogProbMetric: 142.7025 - val_loss: 142.3202 - val_MinusLogProbMetric: 142.3202 - lr: 1.3717e-06 - 89s/epoch - 455ms/step
Epoch 232/1000
2023-10-26 02:54:47.547 
Epoch 232/1000 
	 loss: 142.5166, MinusLogProbMetric: 142.5166, val_loss: 144.6897, val_MinusLogProbMetric: 144.6897

Epoch 232: val_loss did not improve from 111.67181
196/196 - 90s - loss: 142.5166 - MinusLogProbMetric: 142.5166 - val_loss: 144.6897 - val_MinusLogProbMetric: 144.6897 - lr: 1.3717e-06 - 90s/epoch - 459ms/step
Epoch 233/1000
2023-10-26 02:56:17.653 
Epoch 233/1000 
	 loss: 143.5399, MinusLogProbMetric: 143.5399, val_loss: 142.0622, val_MinusLogProbMetric: 142.0622

Epoch 233: val_loss did not improve from 111.67181
196/196 - 90s - loss: 143.5399 - MinusLogProbMetric: 143.5399 - val_loss: 142.0622 - val_MinusLogProbMetric: 142.0622 - lr: 1.3717e-06 - 90s/epoch - 460ms/step
Epoch 234/1000
2023-10-26 02:57:47.780 
Epoch 234/1000 
	 loss: 141.2660, MinusLogProbMetric: 141.2660, val_loss: 140.5661, val_MinusLogProbMetric: 140.5661

Epoch 234: val_loss did not improve from 111.67181
196/196 - 90s - loss: 141.2660 - MinusLogProbMetric: 141.2660 - val_loss: 140.5661 - val_MinusLogProbMetric: 140.5661 - lr: 1.3717e-06 - 90s/epoch - 460ms/step
Epoch 235/1000
2023-10-26 02:59:17.399 
Epoch 235/1000 
	 loss: 140.0301, MinusLogProbMetric: 140.0301, val_loss: 139.5596, val_MinusLogProbMetric: 139.5596

Epoch 235: val_loss did not improve from 111.67181
196/196 - 90s - loss: 140.0301 - MinusLogProbMetric: 140.0301 - val_loss: 139.5596 - val_MinusLogProbMetric: 139.5596 - lr: 1.3717e-06 - 90s/epoch - 457ms/step
Epoch 236/1000
2023-10-26 03:00:47.083 
Epoch 236/1000 
	 loss: 139.2563, MinusLogProbMetric: 139.2563, val_loss: 140.2184, val_MinusLogProbMetric: 140.2184

Epoch 236: val_loss did not improve from 111.67181
196/196 - 90s - loss: 139.2563 - MinusLogProbMetric: 139.2563 - val_loss: 140.2184 - val_MinusLogProbMetric: 140.2184 - lr: 1.3717e-06 - 90s/epoch - 458ms/step
Epoch 237/1000
2023-10-26 03:02:16.599 
Epoch 237/1000 
	 loss: 138.3553, MinusLogProbMetric: 138.3553, val_loss: 137.7833, val_MinusLogProbMetric: 137.7833

Epoch 237: val_loss did not improve from 111.67181
196/196 - 90s - loss: 138.3553 - MinusLogProbMetric: 138.3553 - val_loss: 137.7833 - val_MinusLogProbMetric: 137.7833 - lr: 1.3717e-06 - 90s/epoch - 457ms/step
Epoch 238/1000
2023-10-26 03:03:45.767 
Epoch 238/1000 
	 loss: 137.2662, MinusLogProbMetric: 137.2662, val_loss: 137.1744, val_MinusLogProbMetric: 137.1744

Epoch 238: val_loss did not improve from 111.67181
196/196 - 89s - loss: 137.2662 - MinusLogProbMetric: 137.2662 - val_loss: 137.1744 - val_MinusLogProbMetric: 137.1744 - lr: 1.3717e-06 - 89s/epoch - 455ms/step
Epoch 239/1000
2023-10-26 03:05:15.089 
Epoch 239/1000 
	 loss: 136.7847, MinusLogProbMetric: 136.7847, val_loss: 136.5056, val_MinusLogProbMetric: 136.5056

Epoch 239: val_loss did not improve from 111.67181
196/196 - 89s - loss: 136.7847 - MinusLogProbMetric: 136.7847 - val_loss: 136.5056 - val_MinusLogProbMetric: 136.5056 - lr: 1.3717e-06 - 89s/epoch - 456ms/step
Epoch 240/1000
2023-10-26 03:06:43.980 
Epoch 240/1000 
	 loss: 135.9531, MinusLogProbMetric: 135.9531, val_loss: 135.7311, val_MinusLogProbMetric: 135.7311

Epoch 240: val_loss did not improve from 111.67181
196/196 - 89s - loss: 135.9531 - MinusLogProbMetric: 135.9531 - val_loss: 135.7311 - val_MinusLogProbMetric: 135.7311 - lr: 1.3717e-06 - 89s/epoch - 454ms/step
Epoch 241/1000
2023-10-26 03:08:13.196 
Epoch 241/1000 
	 loss: 135.3489, MinusLogProbMetric: 135.3489, val_loss: 134.9185, val_MinusLogProbMetric: 134.9185

Epoch 241: val_loss did not improve from 111.67181
196/196 - 89s - loss: 135.3489 - MinusLogProbMetric: 135.3489 - val_loss: 134.9185 - val_MinusLogProbMetric: 134.9185 - lr: 1.3717e-06 - 89s/epoch - 455ms/step
Epoch 242/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 13: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 03:08:25.140 
Epoch 242/1000 
	 loss: nan, MinusLogProbMetric: 134.4374, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 242: val_loss did not improve from 111.67181
196/196 - 12s - loss: nan - MinusLogProbMetric: 134.4374 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 12s/epoch - 61ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 4.572473708276175e-07.
===========
Generating train data for run 368.
===========
Train data generated in 0.51 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_368
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_464"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_465 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_49 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_49/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_49'")
self.model: <keras.engine.functional.Functional object at 0x7f1644cb23b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1598dc1300>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1598dc1300>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1a4c4c9a50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f166dbd5900>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f166dbd6c50>, <keras.callbacks.ModelCheckpoint object at 0x7f166dbd76a0>, <keras.callbacks.EarlyStopping object at 0x7f166dbd4e80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f166dbd4e20>, <keras.callbacks.TerminateOnNaN object at 0x7f166dbd4c40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 368/720 with hyperparameters:
timestamp = 2023-10-26 03:08:41.686475
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
2023-10-26 03:13:58.458 
Epoch 1/1000 
	 loss: 111.3552, MinusLogProbMetric: 111.3552, val_loss: 111.5080, val_MinusLogProbMetric: 111.5080

Epoch 1: val_loss improved from inf to 111.50801, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 318s - loss: 111.3552 - MinusLogProbMetric: 111.3552 - val_loss: 111.5080 - val_MinusLogProbMetric: 111.5080 - lr: 4.5725e-07 - 318s/epoch - 2s/step
Epoch 2/1000
2023-10-26 03:15:29.302 
Epoch 2/1000 
	 loss: 111.1248, MinusLogProbMetric: 111.1248, val_loss: 111.2160, val_MinusLogProbMetric: 111.2160

Epoch 2: val_loss improved from 111.50801 to 111.21601, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 111.1248 - MinusLogProbMetric: 111.1248 - val_loss: 111.2160 - val_MinusLogProbMetric: 111.2160 - lr: 4.5725e-07 - 90s/epoch - 460ms/step
Epoch 3/1000
2023-10-26 03:16:59.293 
Epoch 3/1000 
	 loss: 110.7167, MinusLogProbMetric: 110.7167, val_loss: 110.7424, val_MinusLogProbMetric: 110.7424

Epoch 3: val_loss improved from 111.21601 to 110.74240, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 110.7167 - MinusLogProbMetric: 110.7167 - val_loss: 110.7424 - val_MinusLogProbMetric: 110.7424 - lr: 4.5725e-07 - 90s/epoch - 460ms/step
Epoch 4/1000
2023-10-26 03:18:28.972 
Epoch 4/1000 
	 loss: 110.4260, MinusLogProbMetric: 110.4260, val_loss: 110.5421, val_MinusLogProbMetric: 110.5421

Epoch 4: val_loss improved from 110.74240 to 110.54212, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 110.4260 - MinusLogProbMetric: 110.4260 - val_loss: 110.5421 - val_MinusLogProbMetric: 110.5421 - lr: 4.5725e-07 - 90s/epoch - 458ms/step
Epoch 5/1000
2023-10-26 03:19:58.917 
Epoch 5/1000 
	 loss: 110.9475, MinusLogProbMetric: 110.9475, val_loss: 111.1538, val_MinusLogProbMetric: 111.1538

Epoch 5: val_loss did not improve from 110.54212
196/196 - 88s - loss: 110.9475 - MinusLogProbMetric: 110.9475 - val_loss: 111.1538 - val_MinusLogProbMetric: 111.1538 - lr: 4.5725e-07 - 88s/epoch - 448ms/step
Epoch 6/1000
2023-10-26 03:21:27.649 
Epoch 6/1000 
	 loss: 110.6312, MinusLogProbMetric: 110.6312, val_loss: 110.3753, val_MinusLogProbMetric: 110.3753

Epoch 6: val_loss improved from 110.54212 to 110.37526, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 110.6312 - MinusLogProbMetric: 110.6312 - val_loss: 110.3753 - val_MinusLogProbMetric: 110.3753 - lr: 4.5725e-07 - 91s/epoch - 462ms/step
Epoch 7/1000
2023-10-26 03:22:57.698 
Epoch 7/1000 
	 loss: 111.8810, MinusLogProbMetric: 111.8810, val_loss: 111.2809, val_MinusLogProbMetric: 111.2809

Epoch 7: val_loss did not improve from 110.37526
196/196 - 88s - loss: 111.8810 - MinusLogProbMetric: 111.8810 - val_loss: 111.2809 - val_MinusLogProbMetric: 111.2809 - lr: 4.5725e-07 - 88s/epoch - 450ms/step
Epoch 8/1000
2023-10-26 03:24:25.362 
Epoch 8/1000 
	 loss: 109.8119, MinusLogProbMetric: 109.8119, val_loss: 109.8262, val_MinusLogProbMetric: 109.8262

Epoch 8: val_loss improved from 110.37526 to 109.82619, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 109.8119 - MinusLogProbMetric: 109.8119 - val_loss: 109.8262 - val_MinusLogProbMetric: 109.8262 - lr: 4.5725e-07 - 89s/epoch - 455ms/step
Epoch 9/1000
2023-10-26 03:25:54.891 
Epoch 9/1000 
	 loss: 109.2880, MinusLogProbMetric: 109.2880, val_loss: 109.6299, val_MinusLogProbMetric: 109.6299

Epoch 9: val_loss improved from 109.82619 to 109.62990, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 109.2880 - MinusLogProbMetric: 109.2880 - val_loss: 109.6299 - val_MinusLogProbMetric: 109.6299 - lr: 4.5725e-07 - 90s/epoch - 458ms/step
Epoch 10/1000
2023-10-26 03:27:24.488 
Epoch 10/1000 
	 loss: 109.6474, MinusLogProbMetric: 109.6474, val_loss: 109.7504, val_MinusLogProbMetric: 109.7504

Epoch 10: val_loss did not improve from 109.62990
196/196 - 88s - loss: 109.6474 - MinusLogProbMetric: 109.6474 - val_loss: 109.7504 - val_MinusLogProbMetric: 109.7504 - lr: 4.5725e-07 - 88s/epoch - 448ms/step
Epoch 11/1000
2023-10-26 03:28:53.514 
Epoch 11/1000 
	 loss: 111.2201, MinusLogProbMetric: 111.2201, val_loss: 109.2595, val_MinusLogProbMetric: 109.2595

Epoch 11: val_loss improved from 109.62990 to 109.25954, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 111.2201 - MinusLogProbMetric: 111.2201 - val_loss: 109.2595 - val_MinusLogProbMetric: 109.2595 - lr: 4.5725e-07 - 91s/epoch - 462ms/step
Epoch 12/1000
2023-10-26 03:30:23.765 
Epoch 12/1000 
	 loss: 108.6816, MinusLogProbMetric: 108.6816, val_loss: 108.8930, val_MinusLogProbMetric: 108.8930

Epoch 12: val_loss improved from 109.25954 to 108.89301, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 108.6816 - MinusLogProbMetric: 108.6816 - val_loss: 108.8930 - val_MinusLogProbMetric: 108.8930 - lr: 4.5725e-07 - 91s/epoch - 464ms/step
Epoch 13/1000
2023-10-26 03:31:54.861 
Epoch 13/1000 
	 loss: 108.5730, MinusLogProbMetric: 108.5730, val_loss: 109.9607, val_MinusLogProbMetric: 109.9607

Epoch 13: val_loss did not improve from 108.89301
196/196 - 89s - loss: 108.5730 - MinusLogProbMetric: 108.5730 - val_loss: 109.9607 - val_MinusLogProbMetric: 109.9607 - lr: 4.5725e-07 - 89s/epoch - 454ms/step
Epoch 14/1000
2023-10-26 03:33:23.698 
Epoch 14/1000 
	 loss: 122.9571, MinusLogProbMetric: 122.9571, val_loss: 114.7119, val_MinusLogProbMetric: 114.7119

Epoch 14: val_loss did not improve from 108.89301
196/196 - 89s - loss: 122.9571 - MinusLogProbMetric: 122.9571 - val_loss: 114.7119 - val_MinusLogProbMetric: 114.7119 - lr: 4.5725e-07 - 89s/epoch - 453ms/step
Epoch 15/1000
2023-10-26 03:34:52.070 
Epoch 15/1000 
	 loss: 112.2375, MinusLogProbMetric: 112.2375, val_loss: 111.4128, val_MinusLogProbMetric: 111.4128

Epoch 15: val_loss did not improve from 108.89301
196/196 - 88s - loss: 112.2375 - MinusLogProbMetric: 112.2375 - val_loss: 111.4128 - val_MinusLogProbMetric: 111.4128 - lr: 4.5725e-07 - 88s/epoch - 451ms/step
Epoch 16/1000
2023-10-26 03:36:20.417 
Epoch 16/1000 
	 loss: 110.4330, MinusLogProbMetric: 110.4330, val_loss: 110.4603, val_MinusLogProbMetric: 110.4603

Epoch 16: val_loss did not improve from 108.89301
196/196 - 88s - loss: 110.4330 - MinusLogProbMetric: 110.4330 - val_loss: 110.4603 - val_MinusLogProbMetric: 110.4603 - lr: 4.5725e-07 - 88s/epoch - 451ms/step
Epoch 17/1000
2023-10-26 03:37:48.448 
Epoch 17/1000 
	 loss: 109.8418, MinusLogProbMetric: 109.8418, val_loss: 110.0239, val_MinusLogProbMetric: 110.0239

Epoch 17: val_loss did not improve from 108.89301
196/196 - 88s - loss: 109.8418 - MinusLogProbMetric: 109.8418 - val_loss: 110.0239 - val_MinusLogProbMetric: 110.0239 - lr: 4.5725e-07 - 88s/epoch - 449ms/step
Epoch 18/1000
2023-10-26 03:39:17.085 
Epoch 18/1000 
	 loss: 109.4096, MinusLogProbMetric: 109.4096, val_loss: 109.5507, val_MinusLogProbMetric: 109.5507

Epoch 18: val_loss did not improve from 108.89301
196/196 - 89s - loss: 109.4096 - MinusLogProbMetric: 109.4096 - val_loss: 109.5507 - val_MinusLogProbMetric: 109.5507 - lr: 4.5725e-07 - 89s/epoch - 452ms/step
Epoch 19/1000
2023-10-26 03:40:45.717 
Epoch 19/1000 
	 loss: 108.9612, MinusLogProbMetric: 108.9612, val_loss: 109.1038, val_MinusLogProbMetric: 109.1038

Epoch 19: val_loss did not improve from 108.89301
196/196 - 89s - loss: 108.9612 - MinusLogProbMetric: 108.9612 - val_loss: 109.1038 - val_MinusLogProbMetric: 109.1038 - lr: 4.5725e-07 - 89s/epoch - 452ms/step
Epoch 20/1000
2023-10-26 03:42:14.147 
Epoch 20/1000 
	 loss: 108.5973, MinusLogProbMetric: 108.5973, val_loss: 108.7328, val_MinusLogProbMetric: 108.7328

Epoch 20: val_loss improved from 108.89301 to 108.73277, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 108.5973 - MinusLogProbMetric: 108.5973 - val_loss: 108.7328 - val_MinusLogProbMetric: 108.7328 - lr: 4.5725e-07 - 90s/epoch - 459ms/step
Epoch 21/1000
2023-10-26 03:43:44.729 
Epoch 21/1000 
	 loss: 108.3306, MinusLogProbMetric: 108.3306, val_loss: 108.5169, val_MinusLogProbMetric: 108.5169

Epoch 21: val_loss improved from 108.73277 to 108.51685, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 108.3306 - MinusLogProbMetric: 108.3306 - val_loss: 108.5169 - val_MinusLogProbMetric: 108.5169 - lr: 4.5725e-07 - 91s/epoch - 465ms/step
Epoch 22/1000
2023-10-26 03:45:15.690 
Epoch 22/1000 
	 loss: 108.1274, MinusLogProbMetric: 108.1274, val_loss: 108.4098, val_MinusLogProbMetric: 108.4098

Epoch 22: val_loss improved from 108.51685 to 108.40983, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 108.1274 - MinusLogProbMetric: 108.1274 - val_loss: 108.4098 - val_MinusLogProbMetric: 108.4098 - lr: 4.5725e-07 - 90s/epoch - 460ms/step
Epoch 23/1000
2023-10-26 03:46:44.995 
Epoch 23/1000 
	 loss: 108.0303, MinusLogProbMetric: 108.0303, val_loss: 108.2527, val_MinusLogProbMetric: 108.2527

Epoch 23: val_loss improved from 108.40983 to 108.25267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 108.0303 - MinusLogProbMetric: 108.0303 - val_loss: 108.2527 - val_MinusLogProbMetric: 108.2527 - lr: 4.5725e-07 - 90s/epoch - 458ms/step
Epoch 24/1000
2023-10-26 03:48:15.286 
Epoch 24/1000 
	 loss: 107.9240, MinusLogProbMetric: 107.9240, val_loss: 108.1891, val_MinusLogProbMetric: 108.1891

Epoch 24: val_loss improved from 108.25267 to 108.18907, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 107.9240 - MinusLogProbMetric: 107.9240 - val_loss: 108.1891 - val_MinusLogProbMetric: 108.1891 - lr: 4.5725e-07 - 90s/epoch - 461ms/step
Epoch 25/1000
2023-10-26 03:49:45.681 
Epoch 25/1000 
	 loss: 107.9643, MinusLogProbMetric: 107.9643, val_loss: 108.1266, val_MinusLogProbMetric: 108.1266

Epoch 25: val_loss improved from 108.18907 to 108.12660, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 107.9643 - MinusLogProbMetric: 107.9643 - val_loss: 108.1266 - val_MinusLogProbMetric: 108.1266 - lr: 4.5725e-07 - 90s/epoch - 460ms/step
Epoch 26/1000
2023-10-26 03:51:16.163 
Epoch 26/1000 
	 loss: 107.6329, MinusLogProbMetric: 107.6329, val_loss: 108.0390, val_MinusLogProbMetric: 108.0390

Epoch 26: val_loss improved from 108.12660 to 108.03899, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 107.6329 - MinusLogProbMetric: 107.6329 - val_loss: 108.0390 - val_MinusLogProbMetric: 108.0390 - lr: 4.5725e-07 - 91s/epoch - 463ms/step
Epoch 27/1000
2023-10-26 03:52:46.375 
Epoch 27/1000 
	 loss: 107.4639, MinusLogProbMetric: 107.4639, val_loss: 107.6722, val_MinusLogProbMetric: 107.6722

Epoch 27: val_loss improved from 108.03899 to 107.67220, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 107.4639 - MinusLogProbMetric: 107.4639 - val_loss: 107.6722 - val_MinusLogProbMetric: 107.6722 - lr: 4.5725e-07 - 90s/epoch - 458ms/step
Epoch 28/1000
2023-10-26 03:54:15.204 
Epoch 28/1000 
	 loss: 107.3631, MinusLogProbMetric: 107.3631, val_loss: 107.6304, val_MinusLogProbMetric: 107.6304

Epoch 28: val_loss improved from 107.67220 to 107.63043, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 107.3631 - MinusLogProbMetric: 107.3631 - val_loss: 107.6304 - val_MinusLogProbMetric: 107.6304 - lr: 4.5725e-07 - 89s/epoch - 454ms/step
Epoch 29/1000
2023-10-26 03:55:44.451 
Epoch 29/1000 
	 loss: 107.2423, MinusLogProbMetric: 107.2423, val_loss: 107.4491, val_MinusLogProbMetric: 107.4491

Epoch 29: val_loss improved from 107.63043 to 107.44911, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 107.2423 - MinusLogProbMetric: 107.2423 - val_loss: 107.4491 - val_MinusLogProbMetric: 107.4491 - lr: 4.5725e-07 - 89s/epoch - 454ms/step
Epoch 30/1000
2023-10-26 03:57:13.135 
Epoch 30/1000 
	 loss: 107.1162, MinusLogProbMetric: 107.1162, val_loss: 107.3564, val_MinusLogProbMetric: 107.3564

Epoch 30: val_loss improved from 107.44911 to 107.35643, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 107.1162 - MinusLogProbMetric: 107.1162 - val_loss: 107.3564 - val_MinusLogProbMetric: 107.3564 - lr: 4.5725e-07 - 89s/epoch - 455ms/step
Epoch 31/1000
2023-10-26 03:58:42.774 
Epoch 31/1000 
	 loss: 107.0462, MinusLogProbMetric: 107.0462, val_loss: 110.5285, val_MinusLogProbMetric: 110.5285

Epoch 31: val_loss did not improve from 107.35643
196/196 - 88s - loss: 107.0462 - MinusLogProbMetric: 107.0462 - val_loss: 110.5285 - val_MinusLogProbMetric: 110.5285 - lr: 4.5725e-07 - 88s/epoch - 447ms/step
Epoch 32/1000
2023-10-26 04:00:09.830 
Epoch 32/1000 
	 loss: 108.4915, MinusLogProbMetric: 108.4915, val_loss: 108.5953, val_MinusLogProbMetric: 108.5953

Epoch 32: val_loss did not improve from 107.35643
196/196 - 87s - loss: 108.4915 - MinusLogProbMetric: 108.4915 - val_loss: 108.5953 - val_MinusLogProbMetric: 108.5953 - lr: 4.5725e-07 - 87s/epoch - 444ms/step
Epoch 33/1000
2023-10-26 04:01:37.293 
Epoch 33/1000 
	 loss: 108.0405, MinusLogProbMetric: 108.0405, val_loss: 108.3712, val_MinusLogProbMetric: 108.3712

Epoch 33: val_loss did not improve from 107.35643
196/196 - 87s - loss: 108.0405 - MinusLogProbMetric: 108.0405 - val_loss: 108.3712 - val_MinusLogProbMetric: 108.3712 - lr: 4.5725e-07 - 87s/epoch - 446ms/step
Epoch 34/1000
2023-10-26 04:03:04.311 
Epoch 34/1000 
	 loss: 108.1370, MinusLogProbMetric: 108.1370, val_loss: 108.3256, val_MinusLogProbMetric: 108.3256

Epoch 34: val_loss did not improve from 107.35643
196/196 - 87s - loss: 108.1370 - MinusLogProbMetric: 108.1370 - val_loss: 108.3256 - val_MinusLogProbMetric: 108.3256 - lr: 4.5725e-07 - 87s/epoch - 444ms/step
Epoch 35/1000
2023-10-26 04:04:32.652 
Epoch 35/1000 
	 loss: 107.7098, MinusLogProbMetric: 107.7098, val_loss: 108.0015, val_MinusLogProbMetric: 108.0015

Epoch 35: val_loss did not improve from 107.35643
196/196 - 88s - loss: 107.7098 - MinusLogProbMetric: 107.7098 - val_loss: 108.0015 - val_MinusLogProbMetric: 108.0015 - lr: 4.5725e-07 - 88s/epoch - 451ms/step
Epoch 36/1000
2023-10-26 04:06:00.532 
Epoch 36/1000 
	 loss: 107.6742, MinusLogProbMetric: 107.6742, val_loss: 108.1746, val_MinusLogProbMetric: 108.1746

Epoch 36: val_loss did not improve from 107.35643
196/196 - 88s - loss: 107.6742 - MinusLogProbMetric: 107.6742 - val_loss: 108.1746 - val_MinusLogProbMetric: 108.1746 - lr: 4.5725e-07 - 88s/epoch - 448ms/step
Epoch 37/1000
2023-10-26 04:07:28.381 
Epoch 37/1000 
	 loss: 107.4421, MinusLogProbMetric: 107.4421, val_loss: 107.6251, val_MinusLogProbMetric: 107.6251

Epoch 37: val_loss did not improve from 107.35643
196/196 - 88s - loss: 107.4421 - MinusLogProbMetric: 107.4421 - val_loss: 107.6251 - val_MinusLogProbMetric: 107.6251 - lr: 4.5725e-07 - 88s/epoch - 448ms/step
Epoch 38/1000
2023-10-26 04:08:56.464 
Epoch 38/1000 
	 loss: 107.1719, MinusLogProbMetric: 107.1719, val_loss: 107.6090, val_MinusLogProbMetric: 107.6090

Epoch 38: val_loss did not improve from 107.35643
196/196 - 88s - loss: 107.1719 - MinusLogProbMetric: 107.1719 - val_loss: 107.6090 - val_MinusLogProbMetric: 107.6090 - lr: 4.5725e-07 - 88s/epoch - 449ms/step
Epoch 39/1000
2023-10-26 04:10:24.427 
Epoch 39/1000 
	 loss: 107.2943, MinusLogProbMetric: 107.2943, val_loss: 108.8386, val_MinusLogProbMetric: 108.8386

Epoch 39: val_loss did not improve from 107.35643
196/196 - 88s - loss: 107.2943 - MinusLogProbMetric: 107.2943 - val_loss: 108.8386 - val_MinusLogProbMetric: 108.8386 - lr: 4.5725e-07 - 88s/epoch - 449ms/step
Epoch 40/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 116: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 04:11:19.053 
Epoch 40/1000 
	 loss: nan, MinusLogProbMetric: 108.5536, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 40: val_loss did not improve from 107.35643
196/196 - 55s - loss: nan - MinusLogProbMetric: 108.5536 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 55s/epoch - 279ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 1.524157902758725e-07.
===========
Generating train data for run 368.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_368
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_475"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_476 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_50 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_50/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_50'")
self.model: <keras.engine.functional.Functional object at 0x7f13db336b60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1644cf8070>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1644cf8070>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1402d91ea0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f13db13f460>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f13db13d720>, <keras.callbacks.ModelCheckpoint object at 0x7f13db13da20>, <keras.callbacks.EarlyStopping object at 0x7f13db13dc00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f13db13d570>, <keras.callbacks.TerminateOnNaN object at 0x7f13db13d6f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 368/720 with hyperparameters:
timestamp = 2023-10-26 04:11:33.662464
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
2023-10-26 04:17:13.339 
Epoch 1/1000 
	 loss: 106.9020, MinusLogProbMetric: 106.9020, val_loss: 107.1333, val_MinusLogProbMetric: 107.1333

Epoch 1: val_loss improved from inf to 107.13335, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 341s - loss: 106.9020 - MinusLogProbMetric: 106.9020 - val_loss: 107.1333 - val_MinusLogProbMetric: 107.1333 - lr: 1.5242e-07 - 341s/epoch - 2s/step
Epoch 2/1000
2023-10-26 04:18:44.912 
Epoch 2/1000 
	 loss: 106.7638, MinusLogProbMetric: 106.7638, val_loss: 107.1715, val_MinusLogProbMetric: 107.1715

Epoch 2: val_loss did not improve from 107.13335
196/196 - 90s - loss: 106.7638 - MinusLogProbMetric: 106.7638 - val_loss: 107.1715 - val_MinusLogProbMetric: 107.1715 - lr: 1.5242e-07 - 90s/epoch - 457ms/step
Epoch 3/1000
2023-10-26 04:20:13.944 
Epoch 3/1000 
	 loss: 106.6235, MinusLogProbMetric: 106.6235, val_loss: 107.0310, val_MinusLogProbMetric: 107.0310

Epoch 3: val_loss improved from 107.13335 to 107.03104, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 90s - loss: 106.6235 - MinusLogProbMetric: 106.6235 - val_loss: 107.0310 - val_MinusLogProbMetric: 107.0310 - lr: 1.5242e-07 - 90s/epoch - 461ms/step
Epoch 4/1000
2023-10-26 04:21:44.315 
Epoch 4/1000 
	 loss: 106.6191, MinusLogProbMetric: 106.6191, val_loss: 107.0046, val_MinusLogProbMetric: 107.0046

Epoch 4: val_loss improved from 107.03104 to 107.00459, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 91s - loss: 106.6191 - MinusLogProbMetric: 106.6191 - val_loss: 107.0046 - val_MinusLogProbMetric: 107.0046 - lr: 1.5242e-07 - 91s/epoch - 463ms/step
Epoch 5/1000
2023-10-26 04:23:09.672 
Epoch 5/1000 
	 loss: 106.5400, MinusLogProbMetric: 106.5400, val_loss: 107.0827, val_MinusLogProbMetric: 107.0827

Epoch 5: val_loss did not improve from 107.00459
196/196 - 84s - loss: 106.5400 - MinusLogProbMetric: 106.5400 - val_loss: 107.0827 - val_MinusLogProbMetric: 107.0827 - lr: 1.5242e-07 - 84s/epoch - 426ms/step
Epoch 6/1000
2023-10-26 04:24:21.862 
Epoch 6/1000 
	 loss: 106.4030, MinusLogProbMetric: 106.4030, val_loss: 106.5833, val_MinusLogProbMetric: 106.5833

Epoch 6: val_loss improved from 107.00459 to 106.58334, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 74s - loss: 106.4030 - MinusLogProbMetric: 106.4030 - val_loss: 106.5833 - val_MinusLogProbMetric: 106.5833 - lr: 1.5242e-07 - 74s/epoch - 378ms/step
Epoch 7/1000
2023-10-26 04:25:43.298 
Epoch 7/1000 
	 loss: 106.2434, MinusLogProbMetric: 106.2434, val_loss: 106.5209, val_MinusLogProbMetric: 106.5209

Epoch 7: val_loss improved from 106.58334 to 106.52085, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 81s - loss: 106.2434 - MinusLogProbMetric: 106.2434 - val_loss: 106.5209 - val_MinusLogProbMetric: 106.5209 - lr: 1.5242e-07 - 81s/epoch - 411ms/step
Epoch 8/1000
2023-10-26 04:26:54.407 
Epoch 8/1000 
	 loss: 106.1274, MinusLogProbMetric: 106.1274, val_loss: 106.5074, val_MinusLogProbMetric: 106.5074

Epoch 8: val_loss improved from 106.52085 to 106.50742, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 71s - loss: 106.1274 - MinusLogProbMetric: 106.1274 - val_loss: 106.5074 - val_MinusLogProbMetric: 106.5074 - lr: 1.5242e-07 - 71s/epoch - 365ms/step
Epoch 9/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 96: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 04:27:40.615 
Epoch 9/1000 
	 loss: nan, MinusLogProbMetric: 107.8164, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 9: val_loss did not improve from 106.50742
196/196 - 45s - loss: nan - MinusLogProbMetric: 107.8164 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 45s/epoch - 228ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 5.0805263425290834e-08.
===========
Generating train data for run 368.
===========
Train data generated in 0.38 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_368/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_368
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_486"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_487 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_51 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_51/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_51'")
self.model: <keras.engine.functional.Functional object at 0x7f1529c97d60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1eaa5f0550>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1eaa5f0550>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1401b960e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f14034a25c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f14034a1270>, <keras.callbacks.ModelCheckpoint object at 0x7f14034a2b00>, <keras.callbacks.EarlyStopping object at 0x7f14034a1450>, <keras.callbacks.ReduceLROnPlateau object at 0x7f14034a15a0>, <keras.callbacks.TerminateOnNaN object at 0x7f14034a1b40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 368/720 with hyperparameters:
timestamp = 2023-10-26 04:27:53.496023
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
2023-10-26 04:32:03.893 
Epoch 1/1000 
	 loss: 106.0393, MinusLogProbMetric: 106.0393, val_loss: 106.3918, val_MinusLogProbMetric: 106.3918

Epoch 1: val_loss improved from inf to 106.39182, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 252s - loss: 106.0393 - MinusLogProbMetric: 106.0393 - val_loss: 106.3918 - val_MinusLogProbMetric: 106.3918 - lr: 5.0805e-08 - 252s/epoch - 1s/step
Epoch 2/1000
2023-10-26 04:33:16.127 
Epoch 2/1000 
	 loss: 106.0060, MinusLogProbMetric: 106.0060, val_loss: 106.3569, val_MinusLogProbMetric: 106.3569

Epoch 2: val_loss improved from 106.39182 to 106.35693, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 71s - loss: 106.0060 - MinusLogProbMetric: 106.0060 - val_loss: 106.3569 - val_MinusLogProbMetric: 106.3569 - lr: 5.0805e-08 - 71s/epoch - 363ms/step
Epoch 3/1000
2023-10-26 04:34:33.800 
Epoch 3/1000 
	 loss: 105.9613, MinusLogProbMetric: 105.9613, val_loss: 106.3497, val_MinusLogProbMetric: 106.3497

Epoch 3: val_loss improved from 106.35693 to 106.34967, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 79s - loss: 105.9613 - MinusLogProbMetric: 105.9613 - val_loss: 106.3497 - val_MinusLogProbMetric: 106.3497 - lr: 5.0805e-08 - 79s/epoch - 401ms/step
Epoch 4/1000
2023-10-26 04:35:44.446 
Epoch 4/1000 
	 loss: 105.9181, MinusLogProbMetric: 105.9181, val_loss: 106.2320, val_MinusLogProbMetric: 106.2320

Epoch 4: val_loss improved from 106.34967 to 106.23203, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 70s - loss: 105.9181 - MinusLogProbMetric: 105.9181 - val_loss: 106.2320 - val_MinusLogProbMetric: 106.2320 - lr: 5.0805e-08 - 70s/epoch - 356ms/step
Epoch 5/1000
2023-10-26 04:37:03.256 
Epoch 5/1000 
	 loss: 105.9392, MinusLogProbMetric: 105.9392, val_loss: 106.2201, val_MinusLogProbMetric: 106.2201

Epoch 5: val_loss improved from 106.23203 to 106.22011, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 80s - loss: 105.9392 - MinusLogProbMetric: 105.9392 - val_loss: 106.2201 - val_MinusLogProbMetric: 106.2201 - lr: 5.0805e-08 - 80s/epoch - 406ms/step
Epoch 6/1000
2023-10-26 04:38:13.775 
Epoch 6/1000 
	 loss: 105.8301, MinusLogProbMetric: 105.8301, val_loss: 106.1882, val_MinusLogProbMetric: 106.1882

Epoch 6: val_loss improved from 106.22011 to 106.18822, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 70s - loss: 105.8301 - MinusLogProbMetric: 105.8301 - val_loss: 106.1882 - val_MinusLogProbMetric: 106.1882 - lr: 5.0805e-08 - 70s/epoch - 357ms/step
Epoch 7/1000
2023-10-26 04:39:34.832 
Epoch 7/1000 
	 loss: 105.7898, MinusLogProbMetric: 105.7898, val_loss: 106.1246, val_MinusLogProbMetric: 106.1246

Epoch 7: val_loss improved from 106.18822 to 106.12457, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 81s - loss: 105.7898 - MinusLogProbMetric: 105.7898 - val_loss: 106.1246 - val_MinusLogProbMetric: 106.1246 - lr: 5.0805e-08 - 81s/epoch - 412ms/step
Epoch 8/1000
2023-10-26 04:40:43.764 
Epoch 8/1000 
	 loss: 105.7673, MinusLogProbMetric: 105.7673, val_loss: 106.1875, val_MinusLogProbMetric: 106.1875

Epoch 8: val_loss did not improve from 106.12457
196/196 - 68s - loss: 105.7673 - MinusLogProbMetric: 105.7673 - val_loss: 106.1875 - val_MinusLogProbMetric: 106.1875 - lr: 5.0805e-08 - 68s/epoch - 345ms/step
Epoch 9/1000
2023-10-26 04:42:06.059 
Epoch 9/1000 
	 loss: 105.8458, MinusLogProbMetric: 105.8458, val_loss: 106.0616, val_MinusLogProbMetric: 106.0616

Epoch 9: val_loss improved from 106.12457 to 106.06160, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 105.8458 - MinusLogProbMetric: 105.8458 - val_loss: 106.0616 - val_MinusLogProbMetric: 106.0616 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 10/1000
2023-10-26 04:43:15.339 
Epoch 10/1000 
	 loss: 105.6908, MinusLogProbMetric: 105.6908, val_loss: 106.0306, val_MinusLogProbMetric: 106.0306

Epoch 10: val_loss improved from 106.06160 to 106.03061, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 69s - loss: 105.6908 - MinusLogProbMetric: 105.6908 - val_loss: 106.0306 - val_MinusLogProbMetric: 106.0306 - lr: 5.0805e-08 - 69s/epoch - 352ms/step
Epoch 11/1000
2023-10-26 04:44:38.464 
Epoch 11/1000 
	 loss: 105.6648, MinusLogProbMetric: 105.6648, val_loss: 105.9796, val_MinusLogProbMetric: 105.9796

Epoch 11: val_loss improved from 106.03061 to 105.97962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 105.6648 - MinusLogProbMetric: 105.6648 - val_loss: 105.9796 - val_MinusLogProbMetric: 105.9796 - lr: 5.0805e-08 - 83s/epoch - 426ms/step
Epoch 12/1000
2023-10-26 04:45:46.903 
Epoch 12/1000 
	 loss: 105.6339, MinusLogProbMetric: 105.6339, val_loss: 105.9811, val_MinusLogProbMetric: 105.9811

Epoch 12: val_loss did not improve from 105.97962
196/196 - 67s - loss: 105.6339 - MinusLogProbMetric: 105.6339 - val_loss: 105.9811 - val_MinusLogProbMetric: 105.9811 - lr: 5.0805e-08 - 67s/epoch - 341ms/step
Epoch 13/1000
2023-10-26 04:47:00.711 
Epoch 13/1000 
	 loss: 105.6110, MinusLogProbMetric: 105.6110, val_loss: 105.9500, val_MinusLogProbMetric: 105.9500

Epoch 13: val_loss improved from 105.97962 to 105.94997, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 75s - loss: 105.6110 - MinusLogProbMetric: 105.6110 - val_loss: 105.9500 - val_MinusLogProbMetric: 105.9500 - lr: 5.0805e-08 - 75s/epoch - 383ms/step
Epoch 14/1000
2023-10-26 04:48:09.252 
Epoch 14/1000 
	 loss: 105.5899, MinusLogProbMetric: 105.5899, val_loss: 105.9205, val_MinusLogProbMetric: 105.9205

Epoch 14: val_loss improved from 105.94997 to 105.92052, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 68s - loss: 105.5899 - MinusLogProbMetric: 105.5899 - val_loss: 105.9205 - val_MinusLogProbMetric: 105.9205 - lr: 5.0805e-08 - 68s/epoch - 349ms/step
Epoch 15/1000
2023-10-26 04:49:27.800 
Epoch 15/1000 
	 loss: 105.6038, MinusLogProbMetric: 105.6038, val_loss: 105.9280, val_MinusLogProbMetric: 105.9280

Epoch 15: val_loss did not improve from 105.92052
196/196 - 77s - loss: 105.6038 - MinusLogProbMetric: 105.6038 - val_loss: 105.9280 - val_MinusLogProbMetric: 105.9280 - lr: 5.0805e-08 - 77s/epoch - 395ms/step
Epoch 16/1000
2023-10-26 04:50:37.661 
Epoch 16/1000 
	 loss: 105.5449, MinusLogProbMetric: 105.5449, val_loss: 105.9127, val_MinusLogProbMetric: 105.9127

Epoch 16: val_loss improved from 105.92052 to 105.91266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 71s - loss: 105.5449 - MinusLogProbMetric: 105.5449 - val_loss: 105.9127 - val_MinusLogProbMetric: 105.9127 - lr: 5.0805e-08 - 71s/epoch - 363ms/step
Epoch 17/1000
2023-10-26 04:52:00.386 
Epoch 17/1000 
	 loss: 105.5252, MinusLogProbMetric: 105.5252, val_loss: 105.9220, val_MinusLogProbMetric: 105.9220

Epoch 17: val_loss did not improve from 105.91266
196/196 - 82s - loss: 105.5252 - MinusLogProbMetric: 105.5252 - val_loss: 105.9220 - val_MinusLogProbMetric: 105.9220 - lr: 5.0805e-08 - 82s/epoch - 416ms/step
Epoch 18/1000
2023-10-26 04:53:04.538 
Epoch 18/1000 
	 loss: 105.5000, MinusLogProbMetric: 105.5000, val_loss: 105.8984, val_MinusLogProbMetric: 105.8984

Epoch 18: val_loss improved from 105.91266 to 105.89840, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 65s - loss: 105.5000 - MinusLogProbMetric: 105.5000 - val_loss: 105.8984 - val_MinusLogProbMetric: 105.8984 - lr: 5.0805e-08 - 65s/epoch - 334ms/step
Epoch 19/1000
2023-10-26 04:54:11.975 
Epoch 19/1000 
	 loss: 105.4898, MinusLogProbMetric: 105.4898, val_loss: 105.8567, val_MinusLogProbMetric: 105.8567

Epoch 19: val_loss improved from 105.89840 to 105.85667, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 67s - loss: 105.4898 - MinusLogProbMetric: 105.4898 - val_loss: 105.8567 - val_MinusLogProbMetric: 105.8567 - lr: 5.0805e-08 - 67s/epoch - 344ms/step
Epoch 20/1000
2023-10-26 04:55:20.425 
Epoch 20/1000 
	 loss: 105.4816, MinusLogProbMetric: 105.4816, val_loss: 105.8033, val_MinusLogProbMetric: 105.8033

Epoch 20: val_loss improved from 105.85667 to 105.80331, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 68s - loss: 105.4816 - MinusLogProbMetric: 105.4816 - val_loss: 105.8033 - val_MinusLogProbMetric: 105.8033 - lr: 5.0805e-08 - 68s/epoch - 349ms/step
Epoch 21/1000
2023-10-26 04:56:25.203 
Epoch 21/1000 
	 loss: 105.4280, MinusLogProbMetric: 105.4280, val_loss: 105.8069, val_MinusLogProbMetric: 105.8069

Epoch 21: val_loss did not improve from 105.80331
196/196 - 64s - loss: 105.4280 - MinusLogProbMetric: 105.4280 - val_loss: 105.8069 - val_MinusLogProbMetric: 105.8069 - lr: 5.0805e-08 - 64s/epoch - 324ms/step
Epoch 22/1000
2023-10-26 04:57:41.245 
Epoch 22/1000 
	 loss: 105.4195, MinusLogProbMetric: 105.4195, val_loss: 105.7907, val_MinusLogProbMetric: 105.7907

Epoch 22: val_loss improved from 105.80331 to 105.79075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 77s - loss: 105.4195 - MinusLogProbMetric: 105.4195 - val_loss: 105.7907 - val_MinusLogProbMetric: 105.7907 - lr: 5.0805e-08 - 77s/epoch - 394ms/step
Epoch 23/1000
2023-10-26 04:58:58.304 
Epoch 23/1000 
	 loss: 105.3820, MinusLogProbMetric: 105.3820, val_loss: 105.7610, val_MinusLogProbMetric: 105.7610

Epoch 23: val_loss improved from 105.79075 to 105.76100, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 77s - loss: 105.3820 - MinusLogProbMetric: 105.3820 - val_loss: 105.7610 - val_MinusLogProbMetric: 105.7610 - lr: 5.0805e-08 - 77s/epoch - 393ms/step
Epoch 24/1000
2023-10-26 05:00:08.184 
Epoch 24/1000 
	 loss: 105.3956, MinusLogProbMetric: 105.3956, val_loss: 105.8645, val_MinusLogProbMetric: 105.8645

Epoch 24: val_loss did not improve from 105.76100
196/196 - 69s - loss: 105.3956 - MinusLogProbMetric: 105.3956 - val_loss: 105.8645 - val_MinusLogProbMetric: 105.8645 - lr: 5.0805e-08 - 69s/epoch - 350ms/step
Epoch 25/1000
2023-10-26 05:01:20.299 
Epoch 25/1000 
	 loss: 105.4077, MinusLogProbMetric: 105.4077, val_loss: 105.8558, val_MinusLogProbMetric: 105.8558

Epoch 25: val_loss did not improve from 105.76100
196/196 - 72s - loss: 105.4077 - MinusLogProbMetric: 105.4077 - val_loss: 105.8558 - val_MinusLogProbMetric: 105.8558 - lr: 5.0805e-08 - 72s/epoch - 368ms/step
Epoch 26/1000
2023-10-26 05:02:28.130 
Epoch 26/1000 
	 loss: 105.4029, MinusLogProbMetric: 105.4029, val_loss: 105.8144, val_MinusLogProbMetric: 105.8144

Epoch 26: val_loss did not improve from 105.76100
196/196 - 68s - loss: 105.4029 - MinusLogProbMetric: 105.4029 - val_loss: 105.8144 - val_MinusLogProbMetric: 105.8144 - lr: 5.0805e-08 - 68s/epoch - 346ms/step
Epoch 27/1000
2023-10-26 05:03:33.159 
Epoch 27/1000 
	 loss: 105.3708, MinusLogProbMetric: 105.3708, val_loss: 105.8019, val_MinusLogProbMetric: 105.8019

Epoch 27: val_loss did not improve from 105.76100
196/196 - 65s - loss: 105.3708 - MinusLogProbMetric: 105.3708 - val_loss: 105.8019 - val_MinusLogProbMetric: 105.8019 - lr: 5.0805e-08 - 65s/epoch - 332ms/step
Epoch 28/1000
2023-10-26 05:04:47.108 
Epoch 28/1000 
	 loss: 105.3543, MinusLogProbMetric: 105.3543, val_loss: 105.7819, val_MinusLogProbMetric: 105.7819

Epoch 28: val_loss did not improve from 105.76100
196/196 - 74s - loss: 105.3543 - MinusLogProbMetric: 105.3543 - val_loss: 105.7819 - val_MinusLogProbMetric: 105.7819 - lr: 5.0805e-08 - 74s/epoch - 377ms/step
Epoch 29/1000
2023-10-26 05:05:56.953 
Epoch 29/1000 
	 loss: 105.3213, MinusLogProbMetric: 105.3213, val_loss: 105.7542, val_MinusLogProbMetric: 105.7542

Epoch 29: val_loss improved from 105.76100 to 105.75417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 71s - loss: 105.3213 - MinusLogProbMetric: 105.3213 - val_loss: 105.7542 - val_MinusLogProbMetric: 105.7542 - lr: 5.0805e-08 - 71s/epoch - 362ms/step
Epoch 30/1000
2023-10-26 05:07:04.617 
Epoch 30/1000 
	 loss: 105.3027, MinusLogProbMetric: 105.3027, val_loss: 105.7484, val_MinusLogProbMetric: 105.7484

Epoch 30: val_loss improved from 105.75417 to 105.74838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 68s - loss: 105.3027 - MinusLogProbMetric: 105.3027 - val_loss: 105.7484 - val_MinusLogProbMetric: 105.7484 - lr: 5.0805e-08 - 68s/epoch - 349ms/step
Epoch 31/1000
2023-10-26 05:08:22.826 
Epoch 31/1000 
	 loss: 105.3156, MinusLogProbMetric: 105.3156, val_loss: 105.7578, val_MinusLogProbMetric: 105.7578

Epoch 31: val_loss did not improve from 105.74838
196/196 - 76s - loss: 105.3156 - MinusLogProbMetric: 105.3156 - val_loss: 105.7578 - val_MinusLogProbMetric: 105.7578 - lr: 5.0805e-08 - 76s/epoch - 389ms/step
Epoch 32/1000
2023-10-26 05:09:28.611 
Epoch 32/1000 
	 loss: 105.3131, MinusLogProbMetric: 105.3131, val_loss: 105.7164, val_MinusLogProbMetric: 105.7164

Epoch 32: val_loss improved from 105.74838 to 105.71638, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 67s - loss: 105.3131 - MinusLogProbMetric: 105.3131 - val_loss: 105.7164 - val_MinusLogProbMetric: 105.7164 - lr: 5.0805e-08 - 67s/epoch - 341ms/step
Epoch 33/1000
2023-10-26 05:10:37.613 
Epoch 33/1000 
	 loss: 105.2655, MinusLogProbMetric: 105.2655, val_loss: 105.7352, val_MinusLogProbMetric: 105.7352

Epoch 33: val_loss did not improve from 105.71638
196/196 - 68s - loss: 105.2655 - MinusLogProbMetric: 105.2655 - val_loss: 105.7352 - val_MinusLogProbMetric: 105.7352 - lr: 5.0805e-08 - 68s/epoch - 346ms/step
Epoch 34/1000
2023-10-26 05:11:49.670 
Epoch 34/1000 
	 loss: 105.2349, MinusLogProbMetric: 105.2349, val_loss: 105.6882, val_MinusLogProbMetric: 105.6882

Epoch 34: val_loss improved from 105.71638 to 105.68822, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 73s - loss: 105.2349 - MinusLogProbMetric: 105.2349 - val_loss: 105.6882 - val_MinusLogProbMetric: 105.6882 - lr: 5.0805e-08 - 73s/epoch - 373ms/step
Epoch 35/1000
2023-10-26 05:12:55.344 
Epoch 35/1000 
	 loss: 105.2169, MinusLogProbMetric: 105.2169, val_loss: 105.6475, val_MinusLogProbMetric: 105.6475

Epoch 35: val_loss improved from 105.68822 to 105.64748, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 66s - loss: 105.2169 - MinusLogProbMetric: 105.2169 - val_loss: 105.6475 - val_MinusLogProbMetric: 105.6475 - lr: 5.0805e-08 - 66s/epoch - 335ms/step
Epoch 36/1000
2023-10-26 05:14:10.747 
Epoch 36/1000 
	 loss: 105.2244, MinusLogProbMetric: 105.2244, val_loss: 105.6697, val_MinusLogProbMetric: 105.6697

Epoch 36: val_loss did not improve from 105.64748
196/196 - 74s - loss: 105.2244 - MinusLogProbMetric: 105.2244 - val_loss: 105.6697 - val_MinusLogProbMetric: 105.6697 - lr: 5.0805e-08 - 74s/epoch - 379ms/step
Epoch 37/1000
2023-10-26 05:15:23.757 
Epoch 37/1000 
	 loss: 105.2018, MinusLogProbMetric: 105.2018, val_loss: 105.6045, val_MinusLogProbMetric: 105.6045

Epoch 37: val_loss improved from 105.64748 to 105.60450, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 74s - loss: 105.2018 - MinusLogProbMetric: 105.2018 - val_loss: 105.6045 - val_MinusLogProbMetric: 105.6045 - lr: 5.0805e-08 - 74s/epoch - 378ms/step
Epoch 38/1000
2023-10-26 05:16:29.195 
Epoch 38/1000 
	 loss: 105.1858, MinusLogProbMetric: 105.1858, val_loss: 105.6408, val_MinusLogProbMetric: 105.6408

Epoch 38: val_loss did not improve from 105.60450
196/196 - 64s - loss: 105.1858 - MinusLogProbMetric: 105.1858 - val_loss: 105.6408 - val_MinusLogProbMetric: 105.6408 - lr: 5.0805e-08 - 64s/epoch - 328ms/step
Epoch 39/1000
2023-10-26 05:17:45.155 
Epoch 39/1000 
	 loss: 105.1632, MinusLogProbMetric: 105.1632, val_loss: 105.6027, val_MinusLogProbMetric: 105.6027

Epoch 39: val_loss improved from 105.60450 to 105.60273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 77s - loss: 105.1632 - MinusLogProbMetric: 105.1632 - val_loss: 105.6027 - val_MinusLogProbMetric: 105.6027 - lr: 5.0805e-08 - 77s/epoch - 393ms/step
Epoch 40/1000
2023-10-26 05:18:52.332 
Epoch 40/1000 
	 loss: 105.1403, MinusLogProbMetric: 105.1403, val_loss: 105.4941, val_MinusLogProbMetric: 105.4941

Epoch 40: val_loss improved from 105.60273 to 105.49413, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 67s - loss: 105.1403 - MinusLogProbMetric: 105.1403 - val_loss: 105.4941 - val_MinusLogProbMetric: 105.4941 - lr: 5.0805e-08 - 67s/epoch - 343ms/step
Epoch 41/1000
2023-10-26 05:19:59.208 
Epoch 41/1000 
	 loss: 105.1114, MinusLogProbMetric: 105.1114, val_loss: 105.5313, val_MinusLogProbMetric: 105.5313

Epoch 41: val_loss did not improve from 105.49413
196/196 - 66s - loss: 105.1114 - MinusLogProbMetric: 105.1114 - val_loss: 105.5313 - val_MinusLogProbMetric: 105.5313 - lr: 5.0805e-08 - 66s/epoch - 335ms/step
Epoch 42/1000
2023-10-26 05:21:14.970 
Epoch 42/1000 
	 loss: 105.1212, MinusLogProbMetric: 105.1212, val_loss: 105.4927, val_MinusLogProbMetric: 105.4927

Epoch 42: val_loss improved from 105.49413 to 105.49265, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 77s - loss: 105.1212 - MinusLogProbMetric: 105.1212 - val_loss: 105.4927 - val_MinusLogProbMetric: 105.4927 - lr: 5.0805e-08 - 77s/epoch - 394ms/step
Epoch 43/1000
2023-10-26 05:22:24.205 
Epoch 43/1000 
	 loss: 105.0909, MinusLogProbMetric: 105.0909, val_loss: 105.6077, val_MinusLogProbMetric: 105.6077

Epoch 43: val_loss did not improve from 105.49265
196/196 - 68s - loss: 105.0909 - MinusLogProbMetric: 105.0909 - val_loss: 105.6077 - val_MinusLogProbMetric: 105.6077 - lr: 5.0805e-08 - 68s/epoch - 345ms/step
Epoch 44/1000
2023-10-26 05:23:28.624 
Epoch 44/1000 
	 loss: 105.1846, MinusLogProbMetric: 105.1846, val_loss: 105.5123, val_MinusLogProbMetric: 105.5123

Epoch 44: val_loss did not improve from 105.49265
196/196 - 64s - loss: 105.1846 - MinusLogProbMetric: 105.1846 - val_loss: 105.5123 - val_MinusLogProbMetric: 105.5123 - lr: 5.0805e-08 - 64s/epoch - 329ms/step
Epoch 45/1000
2023-10-26 05:24:45.143 
Epoch 45/1000 
	 loss: 105.1054, MinusLogProbMetric: 105.1054, val_loss: 105.4983, val_MinusLogProbMetric: 105.4983

Epoch 45: val_loss did not improve from 105.49265
196/196 - 77s - loss: 105.1054 - MinusLogProbMetric: 105.1054 - val_loss: 105.4983 - val_MinusLogProbMetric: 105.4983 - lr: 5.0805e-08 - 77s/epoch - 390ms/step
Epoch 46/1000
2023-10-26 05:25:52.300 
Epoch 46/1000 
	 loss: 105.0824, MinusLogProbMetric: 105.0824, val_loss: 105.4967, val_MinusLogProbMetric: 105.4967

Epoch 46: val_loss did not improve from 105.49265
196/196 - 67s - loss: 105.0824 - MinusLogProbMetric: 105.0824 - val_loss: 105.4967 - val_MinusLogProbMetric: 105.4967 - lr: 5.0805e-08 - 67s/epoch - 343ms/step
Epoch 47/1000
2023-10-26 05:26:56.703 
Epoch 47/1000 
	 loss: 105.0767, MinusLogProbMetric: 105.0767, val_loss: 105.4491, val_MinusLogProbMetric: 105.4491

Epoch 47: val_loss improved from 105.49265 to 105.44909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 66s - loss: 105.0767 - MinusLogProbMetric: 105.0767 - val_loss: 105.4491 - val_MinusLogProbMetric: 105.4491 - lr: 5.0805e-08 - 66s/epoch - 335ms/step
Epoch 48/1000
2023-10-26 05:28:04.025 
Epoch 48/1000 
	 loss: 105.0720, MinusLogProbMetric: 105.0720, val_loss: 105.8202, val_MinusLogProbMetric: 105.8202

Epoch 48: val_loss did not improve from 105.44909
196/196 - 66s - loss: 105.0720 - MinusLogProbMetric: 105.0720 - val_loss: 105.8202 - val_MinusLogProbMetric: 105.8202 - lr: 5.0805e-08 - 66s/epoch - 337ms/step
Epoch 49/1000
2023-10-26 05:29:15.937 
Epoch 49/1000 
	 loss: 105.2270, MinusLogProbMetric: 105.2270, val_loss: 105.5117, val_MinusLogProbMetric: 105.5117

Epoch 49: val_loss did not improve from 105.44909
196/196 - 72s - loss: 105.2270 - MinusLogProbMetric: 105.2270 - val_loss: 105.5117 - val_MinusLogProbMetric: 105.5117 - lr: 5.0805e-08 - 72s/epoch - 367ms/step
Epoch 50/1000
2023-10-26 05:30:21.156 
Epoch 50/1000 
	 loss: 105.0601, MinusLogProbMetric: 105.0601, val_loss: 105.4608, val_MinusLogProbMetric: 105.4608

Epoch 50: val_loss did not improve from 105.44909
196/196 - 65s - loss: 105.0601 - MinusLogProbMetric: 105.0601 - val_loss: 105.4608 - val_MinusLogProbMetric: 105.4608 - lr: 5.0805e-08 - 65s/epoch - 333ms/step
Epoch 51/1000
2023-10-26 05:31:30.118 
Epoch 51/1000 
	 loss: 105.2950, MinusLogProbMetric: 105.2950, val_loss: 105.5852, val_MinusLogProbMetric: 105.5852

Epoch 51: val_loss did not improve from 105.44909
196/196 - 69s - loss: 105.2950 - MinusLogProbMetric: 105.2950 - val_loss: 105.5852 - val_MinusLogProbMetric: 105.5852 - lr: 5.0805e-08 - 69s/epoch - 352ms/step
Epoch 52/1000
2023-10-26 05:32:42.259 
Epoch 52/1000 
	 loss: 105.1338, MinusLogProbMetric: 105.1338, val_loss: 105.5245, val_MinusLogProbMetric: 105.5245

Epoch 52: val_loss did not improve from 105.44909
196/196 - 72s - loss: 105.1338 - MinusLogProbMetric: 105.1338 - val_loss: 105.5245 - val_MinusLogProbMetric: 105.5245 - lr: 5.0805e-08 - 72s/epoch - 368ms/step
Epoch 53/1000
2023-10-26 05:33:47.620 
Epoch 53/1000 
	 loss: 105.0783, MinusLogProbMetric: 105.0783, val_loss: 105.4478, val_MinusLogProbMetric: 105.4478

Epoch 53: val_loss improved from 105.44909 to 105.44778, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 67s - loss: 105.0783 - MinusLogProbMetric: 105.0783 - val_loss: 105.4478 - val_MinusLogProbMetric: 105.4478 - lr: 5.0805e-08 - 67s/epoch - 339ms/step
Epoch 54/1000
2023-10-26 05:34:52.775 
Epoch 54/1000 
	 loss: 105.0574, MinusLogProbMetric: 105.0574, val_loss: 105.4581, val_MinusLogProbMetric: 105.4581

Epoch 54: val_loss did not improve from 105.44778
196/196 - 64s - loss: 105.0574 - MinusLogProbMetric: 105.0574 - val_loss: 105.4581 - val_MinusLogProbMetric: 105.4581 - lr: 5.0805e-08 - 64s/epoch - 327ms/step
Epoch 55/1000
2023-10-26 05:36:03.095 
Epoch 55/1000 
	 loss: 105.0231, MinusLogProbMetric: 105.0231, val_loss: 105.4113, val_MinusLogProbMetric: 105.4113

Epoch 55: val_loss improved from 105.44778 to 105.41129, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 72s - loss: 105.0231 - MinusLogProbMetric: 105.0231 - val_loss: 105.4113 - val_MinusLogProbMetric: 105.4113 - lr: 5.0805e-08 - 72s/epoch - 366ms/step
Epoch 56/1000
2023-10-26 05:37:15.669 
Epoch 56/1000 
	 loss: 105.0290, MinusLogProbMetric: 105.0290, val_loss: 105.3759, val_MinusLogProbMetric: 105.3759

Epoch 56: val_loss improved from 105.41129 to 105.37587, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 73s - loss: 105.0290 - MinusLogProbMetric: 105.0290 - val_loss: 105.3759 - val_MinusLogProbMetric: 105.3759 - lr: 5.0805e-08 - 73s/epoch - 371ms/step
Epoch 57/1000
2023-10-26 05:38:23.360 
Epoch 57/1000 
	 loss: 105.0010, MinusLogProbMetric: 105.0010, val_loss: 105.4116, val_MinusLogProbMetric: 105.4116

Epoch 57: val_loss did not improve from 105.37587
196/196 - 66s - loss: 105.0010 - MinusLogProbMetric: 105.0010 - val_loss: 105.4116 - val_MinusLogProbMetric: 105.4116 - lr: 5.0805e-08 - 66s/epoch - 337ms/step
Epoch 58/1000
2023-10-26 05:39:27.246 
Epoch 58/1000 
	 loss: 104.9947, MinusLogProbMetric: 104.9947, val_loss: 105.3830, val_MinusLogProbMetric: 105.3830

Epoch 58: val_loss did not improve from 105.37587
196/196 - 64s - loss: 104.9947 - MinusLogProbMetric: 104.9947 - val_loss: 105.3830 - val_MinusLogProbMetric: 105.3830 - lr: 5.0805e-08 - 64s/epoch - 326ms/step
Epoch 59/1000
2023-10-26 05:40:35.604 
Epoch 59/1000 
	 loss: 105.0251, MinusLogProbMetric: 105.0251, val_loss: 105.4901, val_MinusLogProbMetric: 105.4901

Epoch 59: val_loss did not improve from 105.37587
196/196 - 68s - loss: 105.0251 - MinusLogProbMetric: 105.0251 - val_loss: 105.4901 - val_MinusLogProbMetric: 105.4901 - lr: 5.0805e-08 - 68s/epoch - 349ms/step
Epoch 60/1000
2023-10-26 05:41:39.643 
Epoch 60/1000 
	 loss: 105.0629, MinusLogProbMetric: 105.0629, val_loss: 105.3885, val_MinusLogProbMetric: 105.3885

Epoch 60: val_loss did not improve from 105.37587
196/196 - 64s - loss: 105.0629 - MinusLogProbMetric: 105.0629 - val_loss: 105.3885 - val_MinusLogProbMetric: 105.3885 - lr: 5.0805e-08 - 64s/epoch - 327ms/step
Epoch 61/1000
2023-10-26 05:42:44.646 
Epoch 61/1000 
	 loss: 105.0463, MinusLogProbMetric: 105.0463, val_loss: 105.4824, val_MinusLogProbMetric: 105.4824

Epoch 61: val_loss did not improve from 105.37587
196/196 - 65s - loss: 105.0463 - MinusLogProbMetric: 105.0463 - val_loss: 105.4824 - val_MinusLogProbMetric: 105.4824 - lr: 5.0805e-08 - 65s/epoch - 332ms/step
Epoch 62/1000
2023-10-26 05:44:04.939 
Epoch 62/1000 
	 loss: 105.1376, MinusLogProbMetric: 105.1376, val_loss: 105.4132, val_MinusLogProbMetric: 105.4132

Epoch 62: val_loss did not improve from 105.37587
196/196 - 80s - loss: 105.1376 - MinusLogProbMetric: 105.1376 - val_loss: 105.4132 - val_MinusLogProbMetric: 105.4132 - lr: 5.0805e-08 - 80s/epoch - 410ms/step
Epoch 63/1000
2023-10-26 05:45:25.046 
Epoch 63/1000 
	 loss: 105.0559, MinusLogProbMetric: 105.0559, val_loss: 105.3803, val_MinusLogProbMetric: 105.3803

Epoch 63: val_loss did not improve from 105.37587
196/196 - 80s - loss: 105.0559 - MinusLogProbMetric: 105.0559 - val_loss: 105.3803 - val_MinusLogProbMetric: 105.3803 - lr: 5.0805e-08 - 80s/epoch - 409ms/step
Epoch 64/1000
2023-10-26 05:46:47.426 
Epoch 64/1000 
	 loss: 105.0105, MinusLogProbMetric: 105.0105, val_loss: 105.5020, val_MinusLogProbMetric: 105.5020

Epoch 64: val_loss did not improve from 105.37587
196/196 - 82s - loss: 105.0105 - MinusLogProbMetric: 105.0105 - val_loss: 105.5020 - val_MinusLogProbMetric: 105.5020 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 65/1000
2023-10-26 05:48:09.590 
Epoch 65/1000 
	 loss: 105.0249, MinusLogProbMetric: 105.0249, val_loss: 105.3539, val_MinusLogProbMetric: 105.3539

Epoch 65: val_loss improved from 105.37587 to 105.35387, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 105.0249 - MinusLogProbMetric: 105.0249 - val_loss: 105.3539 - val_MinusLogProbMetric: 105.3539 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 66/1000
2023-10-26 05:49:33.804 
Epoch 66/1000 
	 loss: 104.9915, MinusLogProbMetric: 104.9915, val_loss: 105.3655, val_MinusLogProbMetric: 105.3655

Epoch 66: val_loss did not improve from 105.35387
196/196 - 82s - loss: 104.9915 - MinusLogProbMetric: 104.9915 - val_loss: 105.3655 - val_MinusLogProbMetric: 105.3655 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 67/1000
2023-10-26 05:50:56.135 
Epoch 67/1000 
	 loss: 105.0470, MinusLogProbMetric: 105.0470, val_loss: 105.4492, val_MinusLogProbMetric: 105.4492

Epoch 67: val_loss did not improve from 105.35387
196/196 - 82s - loss: 105.0470 - MinusLogProbMetric: 105.0470 - val_loss: 105.4492 - val_MinusLogProbMetric: 105.4492 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 68/1000
2023-10-26 05:52:18.254 
Epoch 68/1000 
	 loss: 105.1567, MinusLogProbMetric: 105.1567, val_loss: 105.4356, val_MinusLogProbMetric: 105.4356

Epoch 68: val_loss did not improve from 105.35387
196/196 - 82s - loss: 105.1567 - MinusLogProbMetric: 105.1567 - val_loss: 105.4356 - val_MinusLogProbMetric: 105.4356 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 69/1000
2023-10-26 05:53:40.067 
Epoch 69/1000 
	 loss: 105.1150, MinusLogProbMetric: 105.1150, val_loss: 105.4055, val_MinusLogProbMetric: 105.4055

Epoch 69: val_loss did not improve from 105.35387
196/196 - 82s - loss: 105.1150 - MinusLogProbMetric: 105.1150 - val_loss: 105.4055 - val_MinusLogProbMetric: 105.4055 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 70/1000
2023-10-26 05:55:02.329 
Epoch 70/1000 
	 loss: 105.3311, MinusLogProbMetric: 105.3311, val_loss: 105.7240, val_MinusLogProbMetric: 105.7240

Epoch 70: val_loss did not improve from 105.35387
196/196 - 82s - loss: 105.3311 - MinusLogProbMetric: 105.3311 - val_loss: 105.7240 - val_MinusLogProbMetric: 105.7240 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 71/1000
2023-10-26 05:56:24.249 
Epoch 71/1000 
	 loss: 105.0307, MinusLogProbMetric: 105.0307, val_loss: 105.3145, val_MinusLogProbMetric: 105.3145

Epoch 71: val_loss improved from 105.35387 to 105.31454, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 105.0307 - MinusLogProbMetric: 105.0307 - val_loss: 105.3145 - val_MinusLogProbMetric: 105.3145 - lr: 5.0805e-08 - 83s/epoch - 426ms/step
Epoch 72/1000
2023-10-26 05:57:47.938 
Epoch 72/1000 
	 loss: 104.9975, MinusLogProbMetric: 104.9975, val_loss: 105.3465, val_MinusLogProbMetric: 105.3465

Epoch 72: val_loss did not improve from 105.31454
196/196 - 82s - loss: 104.9975 - MinusLogProbMetric: 104.9975 - val_loss: 105.3465 - val_MinusLogProbMetric: 105.3465 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 73/1000
2023-10-26 05:59:10.430 
Epoch 73/1000 
	 loss: 105.0392, MinusLogProbMetric: 105.0392, val_loss: 105.7941, val_MinusLogProbMetric: 105.7941

Epoch 73: val_loss did not improve from 105.31454
196/196 - 82s - loss: 105.0392 - MinusLogProbMetric: 105.0392 - val_loss: 105.7941 - val_MinusLogProbMetric: 105.7941 - lr: 5.0805e-08 - 82s/epoch - 421ms/step
Epoch 74/1000
2023-10-26 06:00:32.970 
Epoch 74/1000 
	 loss: 105.6138, MinusLogProbMetric: 105.6138, val_loss: 105.8827, val_MinusLogProbMetric: 105.8827

Epoch 74: val_loss did not improve from 105.31454
196/196 - 83s - loss: 105.6138 - MinusLogProbMetric: 105.6138 - val_loss: 105.8827 - val_MinusLogProbMetric: 105.8827 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 75/1000
2023-10-26 06:01:55.618 
Epoch 75/1000 
	 loss: 114.4349, MinusLogProbMetric: 114.4349, val_loss: 110.7681, val_MinusLogProbMetric: 110.7681

Epoch 75: val_loss did not improve from 105.31454
196/196 - 83s - loss: 114.4349 - MinusLogProbMetric: 114.4349 - val_loss: 110.7681 - val_MinusLogProbMetric: 110.7681 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 76/1000
2023-10-26 06:03:17.816 
Epoch 76/1000 
	 loss: 107.6103, MinusLogProbMetric: 107.6103, val_loss: 106.3828, val_MinusLogProbMetric: 106.3828

Epoch 76: val_loss did not improve from 105.31454
196/196 - 82s - loss: 107.6103 - MinusLogProbMetric: 107.6103 - val_loss: 106.3828 - val_MinusLogProbMetric: 106.3828 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 77/1000
2023-10-26 06:04:39.905 
Epoch 77/1000 
	 loss: 105.9659, MinusLogProbMetric: 105.9659, val_loss: 106.1755, val_MinusLogProbMetric: 106.1755

Epoch 77: val_loss did not improve from 105.31454
196/196 - 82s - loss: 105.9659 - MinusLogProbMetric: 105.9659 - val_loss: 106.1755 - val_MinusLogProbMetric: 106.1755 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 78/1000
2023-10-26 06:06:02.634 
Epoch 78/1000 
	 loss: 105.8216, MinusLogProbMetric: 105.8216, val_loss: 106.0902, val_MinusLogProbMetric: 106.0902

Epoch 78: val_loss did not improve from 105.31454
196/196 - 83s - loss: 105.8216 - MinusLogProbMetric: 105.8216 - val_loss: 106.0902 - val_MinusLogProbMetric: 106.0902 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 79/1000
2023-10-26 06:07:24.382 
Epoch 79/1000 
	 loss: 105.8045, MinusLogProbMetric: 105.8045, val_loss: 105.9734, val_MinusLogProbMetric: 105.9734

Epoch 79: val_loss did not improve from 105.31454
196/196 - 82s - loss: 105.8045 - MinusLogProbMetric: 105.8045 - val_loss: 105.9734 - val_MinusLogProbMetric: 105.9734 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 80/1000
2023-10-26 06:08:46.469 
Epoch 80/1000 
	 loss: 106.4898, MinusLogProbMetric: 106.4898, val_loss: 107.0977, val_MinusLogProbMetric: 107.0977

Epoch 80: val_loss did not improve from 105.31454
196/196 - 82s - loss: 106.4898 - MinusLogProbMetric: 106.4898 - val_loss: 107.0977 - val_MinusLogProbMetric: 107.0977 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 81/1000
2023-10-26 06:10:08.130 
Epoch 81/1000 
	 loss: 106.5514, MinusLogProbMetric: 106.5514, val_loss: 106.6025, val_MinusLogProbMetric: 106.6025

Epoch 81: val_loss did not improve from 105.31454
196/196 - 82s - loss: 106.5514 - MinusLogProbMetric: 106.5514 - val_loss: 106.6025 - val_MinusLogProbMetric: 106.6025 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 82/1000
2023-10-26 06:11:30.033 
Epoch 82/1000 
	 loss: 108.4215, MinusLogProbMetric: 108.4215, val_loss: 109.0233, val_MinusLogProbMetric: 109.0233

Epoch 82: val_loss did not improve from 105.31454
196/196 - 82s - loss: 108.4215 - MinusLogProbMetric: 108.4215 - val_loss: 109.0233 - val_MinusLogProbMetric: 109.0233 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 83/1000
2023-10-26 06:12:52.180 
Epoch 83/1000 
	 loss: 108.4009, MinusLogProbMetric: 108.4009, val_loss: 108.1372, val_MinusLogProbMetric: 108.1372

Epoch 83: val_loss did not improve from 105.31454
196/196 - 82s - loss: 108.4009 - MinusLogProbMetric: 108.4009 - val_loss: 108.1372 - val_MinusLogProbMetric: 108.1372 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 84/1000
2023-10-26 06:14:13.710 
Epoch 84/1000 
	 loss: 107.8201, MinusLogProbMetric: 107.8201, val_loss: 107.9058, val_MinusLogProbMetric: 107.9058

Epoch 84: val_loss did not improve from 105.31454
196/196 - 82s - loss: 107.8201 - MinusLogProbMetric: 107.8201 - val_loss: 107.9058 - val_MinusLogProbMetric: 107.9058 - lr: 5.0805e-08 - 82s/epoch - 416ms/step
Epoch 85/1000
2023-10-26 06:15:36.051 
Epoch 85/1000 
	 loss: 107.0658, MinusLogProbMetric: 107.0658, val_loss: 106.4917, val_MinusLogProbMetric: 106.4917

Epoch 85: val_loss did not improve from 105.31454
196/196 - 82s - loss: 107.0658 - MinusLogProbMetric: 107.0658 - val_loss: 106.4917 - val_MinusLogProbMetric: 106.4917 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 86/1000
2023-10-26 06:16:58.940 
Epoch 86/1000 
	 loss: 107.4787, MinusLogProbMetric: 107.4787, val_loss: 108.1446, val_MinusLogProbMetric: 108.1446

Epoch 86: val_loss did not improve from 105.31454
196/196 - 83s - loss: 107.4787 - MinusLogProbMetric: 107.4787 - val_loss: 108.1446 - val_MinusLogProbMetric: 108.1446 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 87/1000
2023-10-26 06:18:21.942 
Epoch 87/1000 
	 loss: 106.2056, MinusLogProbMetric: 106.2056, val_loss: 105.4827, val_MinusLogProbMetric: 105.4827

Epoch 87: val_loss did not improve from 105.31454
196/196 - 83s - loss: 106.2056 - MinusLogProbMetric: 106.2056 - val_loss: 105.4827 - val_MinusLogProbMetric: 105.4827 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 88/1000
2023-10-26 06:19:43.105 
Epoch 88/1000 
	 loss: 105.0439, MinusLogProbMetric: 105.0439, val_loss: 105.2591, val_MinusLogProbMetric: 105.2591

Epoch 88: val_loss improved from 105.31454 to 105.25909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 105.0439 - MinusLogProbMetric: 105.0439 - val_loss: 105.2591 - val_MinusLogProbMetric: 105.2591 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 89/1000
2023-10-26 06:21:07.719 
Epoch 89/1000 
	 loss: 104.8958, MinusLogProbMetric: 104.8958, val_loss: 105.1549, val_MinusLogProbMetric: 105.1549

Epoch 89: val_loss improved from 105.25909 to 105.15493, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 104.8958 - MinusLogProbMetric: 104.8958 - val_loss: 105.1549 - val_MinusLogProbMetric: 105.1549 - lr: 5.0805e-08 - 85s/epoch - 432ms/step
Epoch 90/1000
2023-10-26 06:22:31.530 
Epoch 90/1000 
	 loss: 104.8352, MinusLogProbMetric: 104.8352, val_loss: 105.1155, val_MinusLogProbMetric: 105.1155

Epoch 90: val_loss improved from 105.15493 to 105.11553, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 104.8352 - MinusLogProbMetric: 104.8352 - val_loss: 105.1155 - val_MinusLogProbMetric: 105.1155 - lr: 5.0805e-08 - 83s/epoch - 426ms/step
Epoch 91/1000
2023-10-26 06:23:56.337 
Epoch 91/1000 
	 loss: 104.7924, MinusLogProbMetric: 104.7924, val_loss: 105.0511, val_MinusLogProbMetric: 105.0511

Epoch 91: val_loss improved from 105.11553 to 105.05113, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 104.7924 - MinusLogProbMetric: 104.7924 - val_loss: 105.0511 - val_MinusLogProbMetric: 105.0511 - lr: 5.0805e-08 - 85s/epoch - 434ms/step
Epoch 92/1000
2023-10-26 06:25:20.989 
Epoch 92/1000 
	 loss: 104.7682, MinusLogProbMetric: 104.7682, val_loss: 105.0402, val_MinusLogProbMetric: 105.0402

Epoch 92: val_loss improved from 105.05113 to 105.04018, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 104.7682 - MinusLogProbMetric: 104.7682 - val_loss: 105.0402 - val_MinusLogProbMetric: 105.0402 - lr: 5.0805e-08 - 85s/epoch - 431ms/step
Epoch 93/1000
2023-10-26 06:26:45.438 
Epoch 93/1000 
	 loss: 104.7447, MinusLogProbMetric: 104.7447, val_loss: 105.0475, val_MinusLogProbMetric: 105.0475

Epoch 93: val_loss did not improve from 105.04018
196/196 - 83s - loss: 104.7447 - MinusLogProbMetric: 104.7447 - val_loss: 105.0475 - val_MinusLogProbMetric: 105.0475 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 94/1000
2023-10-26 06:28:07.357 
Epoch 94/1000 
	 loss: 104.7175, MinusLogProbMetric: 104.7175, val_loss: 105.0129, val_MinusLogProbMetric: 105.0129

Epoch 94: val_loss improved from 105.04018 to 105.01290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 104.7175 - MinusLogProbMetric: 104.7175 - val_loss: 105.0129 - val_MinusLogProbMetric: 105.0129 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 95/1000
2023-10-26 06:29:31.861 
Epoch 95/1000 
	 loss: 104.6945, MinusLogProbMetric: 104.6945, val_loss: 105.0101, val_MinusLogProbMetric: 105.0101

Epoch 95: val_loss improved from 105.01290 to 105.01014, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 104.6945 - MinusLogProbMetric: 104.6945 - val_loss: 105.0101 - val_MinusLogProbMetric: 105.0101 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 96/1000
2023-10-26 06:30:55.898 
Epoch 96/1000 
	 loss: 104.6735, MinusLogProbMetric: 104.6735, val_loss: 104.9929, val_MinusLogProbMetric: 104.9929

Epoch 96: val_loss improved from 105.01014 to 104.99294, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 104.6735 - MinusLogProbMetric: 104.6735 - val_loss: 104.9929 - val_MinusLogProbMetric: 104.9929 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 97/1000
2023-10-26 06:32:20.193 
Epoch 97/1000 
	 loss: 104.6623, MinusLogProbMetric: 104.6623, val_loss: 104.9715, val_MinusLogProbMetric: 104.9715

Epoch 97: val_loss improved from 104.99294 to 104.97145, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 104.6623 - MinusLogProbMetric: 104.6623 - val_loss: 104.9715 - val_MinusLogProbMetric: 104.9715 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 98/1000
2023-10-26 06:33:44.109 
Epoch 98/1000 
	 loss: 104.6555, MinusLogProbMetric: 104.6555, val_loss: 104.9829, val_MinusLogProbMetric: 104.9829

Epoch 98: val_loss did not improve from 104.97145
196/196 - 82s - loss: 104.6555 - MinusLogProbMetric: 104.6555 - val_loss: 104.9829 - val_MinusLogProbMetric: 104.9829 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 99/1000
2023-10-26 06:35:06.735 
Epoch 99/1000 
	 loss: 104.6436, MinusLogProbMetric: 104.6436, val_loss: 104.9542, val_MinusLogProbMetric: 104.9542

Epoch 99: val_loss improved from 104.97145 to 104.95422, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 104.6436 - MinusLogProbMetric: 104.6436 - val_loss: 104.9542 - val_MinusLogProbMetric: 104.9542 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 100/1000
2023-10-26 06:36:30.420 
Epoch 100/1000 
	 loss: 104.6424, MinusLogProbMetric: 104.6424, val_loss: 104.9474, val_MinusLogProbMetric: 104.9474

Epoch 100: val_loss improved from 104.95422 to 104.94736, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 104.6424 - MinusLogProbMetric: 104.6424 - val_loss: 104.9474 - val_MinusLogProbMetric: 104.9474 - lr: 5.0805e-08 - 84s/epoch - 426ms/step
Epoch 101/1000
2023-10-26 06:37:54.279 
Epoch 101/1000 
	 loss: 104.6411, MinusLogProbMetric: 104.6411, val_loss: 104.9492, val_MinusLogProbMetric: 104.9492

Epoch 101: val_loss did not improve from 104.94736
196/196 - 82s - loss: 104.6411 - MinusLogProbMetric: 104.6411 - val_loss: 104.9492 - val_MinusLogProbMetric: 104.9492 - lr: 5.0805e-08 - 82s/epoch - 421ms/step
Epoch 102/1000
2023-10-26 06:39:16.628 
Epoch 102/1000 
	 loss: 104.6161, MinusLogProbMetric: 104.6161, val_loss: 104.9473, val_MinusLogProbMetric: 104.9473

Epoch 102: val_loss improved from 104.94736 to 104.94734, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 104.6161 - MinusLogProbMetric: 104.6161 - val_loss: 104.9473 - val_MinusLogProbMetric: 104.9473 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 103/1000
2023-10-26 06:40:40.592 
Epoch 103/1000 
	 loss: 104.5949, MinusLogProbMetric: 104.5949, val_loss: 104.9304, val_MinusLogProbMetric: 104.9304

Epoch 103: val_loss improved from 104.94734 to 104.93043, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 104.5949 - MinusLogProbMetric: 104.5949 - val_loss: 104.9304 - val_MinusLogProbMetric: 104.9304 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 104/1000
2023-10-26 06:42:03.874 
Epoch 104/1000 
	 loss: 104.5986, MinusLogProbMetric: 104.5986, val_loss: 104.9270, val_MinusLogProbMetric: 104.9270

Epoch 104: val_loss improved from 104.93043 to 104.92701, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 104.5986 - MinusLogProbMetric: 104.5986 - val_loss: 104.9270 - val_MinusLogProbMetric: 104.9270 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 105/1000
2023-10-26 06:43:28.229 
Epoch 105/1000 
	 loss: 104.6132, MinusLogProbMetric: 104.6132, val_loss: 104.8986, val_MinusLogProbMetric: 104.8986

Epoch 105: val_loss improved from 104.92701 to 104.89864, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 104.6132 - MinusLogProbMetric: 104.6132 - val_loss: 104.8986 - val_MinusLogProbMetric: 104.8986 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 106/1000
2023-10-26 06:44:52.916 
Epoch 106/1000 
	 loss: 104.6024, MinusLogProbMetric: 104.6024, val_loss: 104.9128, val_MinusLogProbMetric: 104.9128

Epoch 106: val_loss did not improve from 104.89864
196/196 - 83s - loss: 104.6024 - MinusLogProbMetric: 104.6024 - val_loss: 104.9128 - val_MinusLogProbMetric: 104.9128 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 107/1000
2023-10-26 06:46:15.682 
Epoch 107/1000 
	 loss: 104.5910, MinusLogProbMetric: 104.5910, val_loss: 104.8995, val_MinusLogProbMetric: 104.8995

Epoch 107: val_loss did not improve from 104.89864
196/196 - 83s - loss: 104.5910 - MinusLogProbMetric: 104.5910 - val_loss: 104.8995 - val_MinusLogProbMetric: 104.8995 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 108/1000
2023-10-26 06:47:38.053 
Epoch 108/1000 
	 loss: 104.5800, MinusLogProbMetric: 104.5800, val_loss: 104.8799, val_MinusLogProbMetric: 104.8799

Epoch 108: val_loss improved from 104.89864 to 104.87991, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 104.5800 - MinusLogProbMetric: 104.5800 - val_loss: 104.8799 - val_MinusLogProbMetric: 104.8799 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 109/1000
2023-10-26 06:49:01.744 
Epoch 109/1000 
	 loss: 104.5894, MinusLogProbMetric: 104.5894, val_loss: 104.8698, val_MinusLogProbMetric: 104.8698

Epoch 109: val_loss improved from 104.87991 to 104.86976, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 104.5894 - MinusLogProbMetric: 104.5894 - val_loss: 104.8698 - val_MinusLogProbMetric: 104.8698 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 110/1000
2023-10-26 06:50:25.688 
Epoch 110/1000 
	 loss: 104.5635, MinusLogProbMetric: 104.5635, val_loss: 104.8733, val_MinusLogProbMetric: 104.8733

Epoch 110: val_loss did not improve from 104.86976
196/196 - 82s - loss: 104.5635 - MinusLogProbMetric: 104.5635 - val_loss: 104.8733 - val_MinusLogProbMetric: 104.8733 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 111/1000
2023-10-26 06:51:46.466 
Epoch 111/1000 
	 loss: 104.5755, MinusLogProbMetric: 104.5755, val_loss: 104.8675, val_MinusLogProbMetric: 104.8675

Epoch 111: val_loss improved from 104.86976 to 104.86749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 82s - loss: 104.5755 - MinusLogProbMetric: 104.5755 - val_loss: 104.8675 - val_MinusLogProbMetric: 104.8675 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 112/1000
2023-10-26 06:53:07.509 
Epoch 112/1000 
	 loss: 104.5870, MinusLogProbMetric: 104.5870, val_loss: 104.8677, val_MinusLogProbMetric: 104.8677

Epoch 112: val_loss did not improve from 104.86749
196/196 - 80s - loss: 104.5870 - MinusLogProbMetric: 104.5870 - val_loss: 104.8677 - val_MinusLogProbMetric: 104.8677 - lr: 5.0805e-08 - 80s/epoch - 407ms/step
Epoch 113/1000
2023-10-26 06:54:26.933 
Epoch 113/1000 
	 loss: 104.5815, MinusLogProbMetric: 104.5815, val_loss: 104.8399, val_MinusLogProbMetric: 104.8399

Epoch 113: val_loss improved from 104.86749 to 104.83993, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 81s - loss: 104.5815 - MinusLogProbMetric: 104.5815 - val_loss: 104.8399 - val_MinusLogProbMetric: 104.8399 - lr: 5.0805e-08 - 81s/epoch - 413ms/step
Epoch 114/1000
2023-10-26 06:55:48.820 
Epoch 114/1000 
	 loss: 104.5747, MinusLogProbMetric: 104.5747, val_loss: 104.8434, val_MinusLogProbMetric: 104.8434

Epoch 114: val_loss did not improve from 104.83993
196/196 - 80s - loss: 104.5747 - MinusLogProbMetric: 104.5747 - val_loss: 104.8434 - val_MinusLogProbMetric: 104.8434 - lr: 5.0805e-08 - 80s/epoch - 410ms/step
Epoch 115/1000
2023-10-26 06:57:06.988 
Epoch 115/1000 
	 loss: 104.5734, MinusLogProbMetric: 104.5734, val_loss: 104.8339, val_MinusLogProbMetric: 104.8339

Epoch 115: val_loss improved from 104.83993 to 104.83387, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 80s - loss: 104.5734 - MinusLogProbMetric: 104.5734 - val_loss: 104.8339 - val_MinusLogProbMetric: 104.8339 - lr: 5.0805e-08 - 80s/epoch - 407ms/step
Epoch 116/1000
2023-10-26 06:58:30.403 
Epoch 116/1000 
	 loss: 104.5549, MinusLogProbMetric: 104.5549, val_loss: 104.8311, val_MinusLogProbMetric: 104.8311

Epoch 116: val_loss improved from 104.83387 to 104.83110, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 104.5549 - MinusLogProbMetric: 104.5549 - val_loss: 104.8311 - val_MinusLogProbMetric: 104.8311 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 117/1000
2023-10-26 06:59:54.019 
Epoch 117/1000 
	 loss: 104.5439, MinusLogProbMetric: 104.5439, val_loss: 104.8053, val_MinusLogProbMetric: 104.8053

Epoch 117: val_loss improved from 104.83110 to 104.80527, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 104.5439 - MinusLogProbMetric: 104.5439 - val_loss: 104.8053 - val_MinusLogProbMetric: 104.8053 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 118/1000
2023-10-26 07:01:18.447 
Epoch 118/1000 
	 loss: 104.5395, MinusLogProbMetric: 104.5395, val_loss: 104.8187, val_MinusLogProbMetric: 104.8187

Epoch 118: val_loss did not improve from 104.80527
196/196 - 83s - loss: 104.5395 - MinusLogProbMetric: 104.5395 - val_loss: 104.8187 - val_MinusLogProbMetric: 104.8187 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 119/1000
2023-10-26 07:02:41.869 
Epoch 119/1000 
	 loss: 104.5597, MinusLogProbMetric: 104.5597, val_loss: 104.8516, val_MinusLogProbMetric: 104.8516

Epoch 119: val_loss did not improve from 104.80527
196/196 - 83s - loss: 104.5597 - MinusLogProbMetric: 104.5597 - val_loss: 104.8516 - val_MinusLogProbMetric: 104.8516 - lr: 5.0805e-08 - 83s/epoch - 426ms/step
Epoch 120/1000
2023-10-26 07:04:05.710 
Epoch 120/1000 
	 loss: 104.6143, MinusLogProbMetric: 104.6143, val_loss: 104.8542, val_MinusLogProbMetric: 104.8542

Epoch 120: val_loss did not improve from 104.80527
196/196 - 84s - loss: 104.6143 - MinusLogProbMetric: 104.6143 - val_loss: 104.8542 - val_MinusLogProbMetric: 104.8542 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 121/1000
2023-10-26 07:05:30.815 
Epoch 121/1000 
	 loss: 104.6535, MinusLogProbMetric: 104.6535, val_loss: 104.8311, val_MinusLogProbMetric: 104.8311

Epoch 121: val_loss did not improve from 104.80527
196/196 - 85s - loss: 104.6535 - MinusLogProbMetric: 104.6535 - val_loss: 104.8311 - val_MinusLogProbMetric: 104.8311 - lr: 5.0805e-08 - 85s/epoch - 434ms/step
Epoch 122/1000
2023-10-26 07:06:54.534 
Epoch 122/1000 
	 loss: 104.5732, MinusLogProbMetric: 104.5732, val_loss: 104.8366, val_MinusLogProbMetric: 104.8366

Epoch 122: val_loss did not improve from 104.80527
196/196 - 84s - loss: 104.5732 - MinusLogProbMetric: 104.5732 - val_loss: 104.8366 - val_MinusLogProbMetric: 104.8366 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 123/1000
2023-10-26 07:08:18.379 
Epoch 123/1000 
	 loss: 104.5599, MinusLogProbMetric: 104.5599, val_loss: 104.7836, val_MinusLogProbMetric: 104.7836

Epoch 123: val_loss improved from 104.80527 to 104.78363, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 104.5599 - MinusLogProbMetric: 104.5599 - val_loss: 104.7836 - val_MinusLogProbMetric: 104.7836 - lr: 5.0805e-08 - 86s/epoch - 437ms/step
Epoch 124/1000
2023-10-26 07:09:43.170 
Epoch 124/1000 
	 loss: 104.6557, MinusLogProbMetric: 104.6557, val_loss: 104.9679, val_MinusLogProbMetric: 104.9679

Epoch 124: val_loss did not improve from 104.78363
196/196 - 83s - loss: 104.6557 - MinusLogProbMetric: 104.6557 - val_loss: 104.9679 - val_MinusLogProbMetric: 104.9679 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 125/1000
2023-10-26 07:11:07.376 
Epoch 125/1000 
	 loss: 104.6978, MinusLogProbMetric: 104.6978, val_loss: 104.9135, val_MinusLogProbMetric: 104.9135

Epoch 125: val_loss did not improve from 104.78363
196/196 - 84s - loss: 104.6978 - MinusLogProbMetric: 104.6978 - val_loss: 104.9135 - val_MinusLogProbMetric: 104.9135 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 126/1000
2023-10-26 07:12:31.567 
Epoch 126/1000 
	 loss: 104.6495, MinusLogProbMetric: 104.6495, val_loss: 104.8857, val_MinusLogProbMetric: 104.8857

Epoch 126: val_loss did not improve from 104.78363
196/196 - 84s - loss: 104.6495 - MinusLogProbMetric: 104.6495 - val_loss: 104.8857 - val_MinusLogProbMetric: 104.8857 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 127/1000
2023-10-26 07:13:55.217 
Epoch 127/1000 
	 loss: 104.6375, MinusLogProbMetric: 104.6375, val_loss: 104.8761, val_MinusLogProbMetric: 104.8761

Epoch 127: val_loss did not improve from 104.78363
196/196 - 84s - loss: 104.6375 - MinusLogProbMetric: 104.6375 - val_loss: 104.8761 - val_MinusLogProbMetric: 104.8761 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 128/1000
2023-10-26 07:15:19.470 
Epoch 128/1000 
	 loss: 104.6082, MinusLogProbMetric: 104.6082, val_loss: 104.8648, val_MinusLogProbMetric: 104.8648

Epoch 128: val_loss did not improve from 104.78363
196/196 - 84s - loss: 104.6082 - MinusLogProbMetric: 104.6082 - val_loss: 104.8648 - val_MinusLogProbMetric: 104.8648 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 129/1000
2023-10-26 07:16:43.752 
Epoch 129/1000 
	 loss: 104.6168, MinusLogProbMetric: 104.6168, val_loss: 104.8609, val_MinusLogProbMetric: 104.8609

Epoch 129: val_loss did not improve from 104.78363
196/196 - 84s - loss: 104.6168 - MinusLogProbMetric: 104.6168 - val_loss: 104.8609 - val_MinusLogProbMetric: 104.8609 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 130/1000
2023-10-26 07:18:08.832 
Epoch 130/1000 
	 loss: 104.5598, MinusLogProbMetric: 104.5598, val_loss: 104.8017, val_MinusLogProbMetric: 104.8017

Epoch 130: val_loss did not improve from 104.78363
196/196 - 85s - loss: 104.5598 - MinusLogProbMetric: 104.5598 - val_loss: 104.8017 - val_MinusLogProbMetric: 104.8017 - lr: 5.0805e-08 - 85s/epoch - 434ms/step
Epoch 131/1000
2023-10-26 07:19:33.299 
Epoch 131/1000 
	 loss: 104.5387, MinusLogProbMetric: 104.5387, val_loss: 104.7789, val_MinusLogProbMetric: 104.7789

Epoch 131: val_loss improved from 104.78363 to 104.77888, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 104.5387 - MinusLogProbMetric: 104.5387 - val_loss: 104.7789 - val_MinusLogProbMetric: 104.7789 - lr: 5.0805e-08 - 86s/epoch - 440ms/step
Epoch 132/1000
2023-10-26 07:20:58.930 
Epoch 132/1000 
	 loss: 104.5482, MinusLogProbMetric: 104.5482, val_loss: 104.7605, val_MinusLogProbMetric: 104.7605

Epoch 132: val_loss improved from 104.77888 to 104.76054, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 104.5482 - MinusLogProbMetric: 104.5482 - val_loss: 104.7605 - val_MinusLogProbMetric: 104.7605 - lr: 5.0805e-08 - 86s/epoch - 438ms/step
Epoch 133/1000
2023-10-26 07:22:25.355 
Epoch 133/1000 
	 loss: 104.5220, MinusLogProbMetric: 104.5220, val_loss: 104.7881, val_MinusLogProbMetric: 104.7881

Epoch 133: val_loss did not improve from 104.76054
196/196 - 84s - loss: 104.5220 - MinusLogProbMetric: 104.5220 - val_loss: 104.7881 - val_MinusLogProbMetric: 104.7881 - lr: 5.0805e-08 - 84s/epoch - 431ms/step
Epoch 134/1000
2023-10-26 07:23:49.240 
Epoch 134/1000 
	 loss: 104.5248, MinusLogProbMetric: 104.5248, val_loss: 104.7594, val_MinusLogProbMetric: 104.7594

Epoch 134: val_loss improved from 104.76054 to 104.75942, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 104.5248 - MinusLogProbMetric: 104.5248 - val_loss: 104.7594 - val_MinusLogProbMetric: 104.7594 - lr: 5.0805e-08 - 85s/epoch - 436ms/step
Epoch 135/1000
2023-10-26 07:25:15.563 
Epoch 135/1000 
	 loss: 104.5274, MinusLogProbMetric: 104.5274, val_loss: 104.7944, val_MinusLogProbMetric: 104.7944

Epoch 135: val_loss did not improve from 104.75942
196/196 - 85s - loss: 104.5274 - MinusLogProbMetric: 104.5274 - val_loss: 104.7944 - val_MinusLogProbMetric: 104.7944 - lr: 5.0805e-08 - 85s/epoch - 432ms/step
Epoch 136/1000
2023-10-26 07:26:40.247 
Epoch 136/1000 
	 loss: 104.5884, MinusLogProbMetric: 104.5884, val_loss: 104.7981, val_MinusLogProbMetric: 104.7981

Epoch 136: val_loss did not improve from 104.75942
196/196 - 85s - loss: 104.5884 - MinusLogProbMetric: 104.5884 - val_loss: 104.7981 - val_MinusLogProbMetric: 104.7981 - lr: 5.0805e-08 - 85s/epoch - 432ms/step
Epoch 137/1000
2023-10-26 07:28:03.844 
Epoch 137/1000 
	 loss: 104.5677, MinusLogProbMetric: 104.5677, val_loss: 104.7914, val_MinusLogProbMetric: 104.7914

Epoch 137: val_loss did not improve from 104.75942
196/196 - 84s - loss: 104.5677 - MinusLogProbMetric: 104.5677 - val_loss: 104.7914 - val_MinusLogProbMetric: 104.7914 - lr: 5.0805e-08 - 84s/epoch - 426ms/step
Epoch 138/1000
2023-10-26 07:29:28.907 
Epoch 138/1000 
	 loss: 104.5555, MinusLogProbMetric: 104.5555, val_loss: 104.7764, val_MinusLogProbMetric: 104.7764

Epoch 138: val_loss did not improve from 104.75942
196/196 - 85s - loss: 104.5555 - MinusLogProbMetric: 104.5555 - val_loss: 104.7764 - val_MinusLogProbMetric: 104.7764 - lr: 5.0805e-08 - 85s/epoch - 434ms/step
Epoch 139/1000
2023-10-26 07:30:50.694 
Epoch 139/1000 
	 loss: 104.5518, MinusLogProbMetric: 104.5518, val_loss: 104.7768, val_MinusLogProbMetric: 104.7768

Epoch 139: val_loss did not improve from 104.75942
196/196 - 82s - loss: 104.5518 - MinusLogProbMetric: 104.5518 - val_loss: 104.7768 - val_MinusLogProbMetric: 104.7768 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 140/1000
2023-10-26 07:32:12.290 
Epoch 140/1000 
	 loss: 104.5230, MinusLogProbMetric: 104.5230, val_loss: 104.7798, val_MinusLogProbMetric: 104.7798

Epoch 140: val_loss did not improve from 104.75942
196/196 - 82s - loss: 104.5230 - MinusLogProbMetric: 104.5230 - val_loss: 104.7798 - val_MinusLogProbMetric: 104.7798 - lr: 5.0805e-08 - 82s/epoch - 416ms/step
Epoch 141/1000
2023-10-26 07:33:35.687 
Epoch 141/1000 
	 loss: 104.4867, MinusLogProbMetric: 104.4867, val_loss: 104.7669, val_MinusLogProbMetric: 104.7669

Epoch 141: val_loss did not improve from 104.75942
196/196 - 83s - loss: 104.4867 - MinusLogProbMetric: 104.4867 - val_loss: 104.7669 - val_MinusLogProbMetric: 104.7669 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 142/1000
2023-10-26 07:34:58.329 
Epoch 142/1000 
	 loss: 104.4823, MinusLogProbMetric: 104.4823, val_loss: 104.7563, val_MinusLogProbMetric: 104.7563

Epoch 142: val_loss improved from 104.75942 to 104.75634, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 104.4823 - MinusLogProbMetric: 104.4823 - val_loss: 104.7563 - val_MinusLogProbMetric: 104.7563 - lr: 5.0805e-08 - 85s/epoch - 431ms/step
Epoch 143/1000
2023-10-26 07:36:23.528 
Epoch 143/1000 
	 loss: 104.4928, MinusLogProbMetric: 104.4928, val_loss: 104.7194, val_MinusLogProbMetric: 104.7194

Epoch 143: val_loss improved from 104.75634 to 104.71944, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 104.4928 - MinusLogProbMetric: 104.4928 - val_loss: 104.7194 - val_MinusLogProbMetric: 104.7194 - lr: 5.0805e-08 - 85s/epoch - 433ms/step
Epoch 144/1000
2023-10-26 07:37:49.133 
Epoch 144/1000 
	 loss: 104.4664, MinusLogProbMetric: 104.4664, val_loss: 104.7430, val_MinusLogProbMetric: 104.7430

Epoch 144: val_loss did not improve from 104.71944
196/196 - 84s - loss: 104.4664 - MinusLogProbMetric: 104.4664 - val_loss: 104.7430 - val_MinusLogProbMetric: 104.7430 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 145/1000
2023-10-26 07:39:12.484 
Epoch 145/1000 
	 loss: 104.4919, MinusLogProbMetric: 104.4919, val_loss: 104.7591, val_MinusLogProbMetric: 104.7591

Epoch 145: val_loss did not improve from 104.71944
196/196 - 83s - loss: 104.4919 - MinusLogProbMetric: 104.4919 - val_loss: 104.7591 - val_MinusLogProbMetric: 104.7591 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 146/1000
2023-10-26 07:40:36.225 
Epoch 146/1000 
	 loss: 104.4675, MinusLogProbMetric: 104.4675, val_loss: 104.7348, val_MinusLogProbMetric: 104.7348

Epoch 146: val_loss did not improve from 104.71944
196/196 - 84s - loss: 104.4675 - MinusLogProbMetric: 104.4675 - val_loss: 104.7348 - val_MinusLogProbMetric: 104.7348 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 147/1000
2023-10-26 07:41:59.248 
Epoch 147/1000 
	 loss: 104.4546, MinusLogProbMetric: 104.4546, val_loss: 104.7276, val_MinusLogProbMetric: 104.7276

Epoch 147: val_loss did not improve from 104.71944
196/196 - 83s - loss: 104.4546 - MinusLogProbMetric: 104.4546 - val_loss: 104.7276 - val_MinusLogProbMetric: 104.7276 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 148/1000
2023-10-26 07:43:22.146 
Epoch 148/1000 
	 loss: 104.4443, MinusLogProbMetric: 104.4443, val_loss: 104.7122, val_MinusLogProbMetric: 104.7122

Epoch 148: val_loss improved from 104.71944 to 104.71219, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 104.4443 - MinusLogProbMetric: 104.4443 - val_loss: 104.7122 - val_MinusLogProbMetric: 104.7122 - lr: 5.0805e-08 - 85s/epoch - 433ms/step
Epoch 149/1000
2023-10-26 07:44:47.282 
Epoch 149/1000 
	 loss: 104.4471, MinusLogProbMetric: 104.4471, val_loss: 104.6941, val_MinusLogProbMetric: 104.6941

Epoch 149: val_loss improved from 104.71219 to 104.69411, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 104.4471 - MinusLogProbMetric: 104.4471 - val_loss: 104.6941 - val_MinusLogProbMetric: 104.6941 - lr: 5.0805e-08 - 85s/epoch - 432ms/step
Epoch 150/1000
2023-10-26 07:46:12.853 
Epoch 150/1000 
	 loss: 104.4186, MinusLogProbMetric: 104.4186, val_loss: 104.7036, val_MinusLogProbMetric: 104.7036

Epoch 150: val_loss did not improve from 104.69411
196/196 - 84s - loss: 104.4186 - MinusLogProbMetric: 104.4186 - val_loss: 104.7036 - val_MinusLogProbMetric: 104.7036 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 151/1000
2023-10-26 07:47:36.004 
Epoch 151/1000 
	 loss: 104.4225, MinusLogProbMetric: 104.4225, val_loss: 104.6752, val_MinusLogProbMetric: 104.6752

Epoch 151: val_loss improved from 104.69411 to 104.67523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 104.4225 - MinusLogProbMetric: 104.4225 - val_loss: 104.6752 - val_MinusLogProbMetric: 104.6752 - lr: 5.0805e-08 - 85s/epoch - 435ms/step
Epoch 152/1000
2023-10-26 07:49:00.886 
Epoch 152/1000 
	 loss: 104.4131, MinusLogProbMetric: 104.4131, val_loss: 104.6754, val_MinusLogProbMetric: 104.6754

Epoch 152: val_loss did not improve from 104.67523
196/196 - 83s - loss: 104.4131 - MinusLogProbMetric: 104.4131 - val_loss: 104.6754 - val_MinusLogProbMetric: 104.6754 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 153/1000
2023-10-26 07:50:23.704 
Epoch 153/1000 
	 loss: 104.4086, MinusLogProbMetric: 104.4086, val_loss: 104.6593, val_MinusLogProbMetric: 104.6593

Epoch 153: val_loss improved from 104.67523 to 104.65930, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 104.4086 - MinusLogProbMetric: 104.4086 - val_loss: 104.6593 - val_MinusLogProbMetric: 104.6593 - lr: 5.0805e-08 - 84s/epoch - 431ms/step
Epoch 154/1000
2023-10-26 07:51:43.109 
Epoch 154/1000 
	 loss: 104.4138, MinusLogProbMetric: 104.4138, val_loss: 104.6529, val_MinusLogProbMetric: 104.6529

Epoch 154: val_loss improved from 104.65930 to 104.65288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 80s - loss: 104.4138 - MinusLogProbMetric: 104.4138 - val_loss: 104.6529 - val_MinusLogProbMetric: 104.6529 - lr: 5.0805e-08 - 80s/epoch - 407ms/step
Epoch 155/1000
2023-10-26 07:53:06.414 
Epoch 155/1000 
	 loss: 104.4197, MinusLogProbMetric: 104.4197, val_loss: 104.6675, val_MinusLogProbMetric: 104.6675

Epoch 155: val_loss did not improve from 104.65288
196/196 - 81s - loss: 104.4197 - MinusLogProbMetric: 104.4197 - val_loss: 104.6675 - val_MinusLogProbMetric: 104.6675 - lr: 5.0805e-08 - 81s/epoch - 415ms/step
Epoch 156/1000
2023-10-26 07:54:29.004 
Epoch 156/1000 
	 loss: 104.4516, MinusLogProbMetric: 104.4516, val_loss: 104.6241, val_MinusLogProbMetric: 104.6241

Epoch 156: val_loss improved from 104.65288 to 104.62406, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 104.4516 - MinusLogProbMetric: 104.4516 - val_loss: 104.6241 - val_MinusLogProbMetric: 104.6241 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 157/1000
2023-10-26 07:55:52.621 
Epoch 157/1000 
	 loss: 104.4388, MinusLogProbMetric: 104.4388, val_loss: 104.6331, val_MinusLogProbMetric: 104.6331

Epoch 157: val_loss did not improve from 104.62406
196/196 - 82s - loss: 104.4388 - MinusLogProbMetric: 104.4388 - val_loss: 104.6331 - val_MinusLogProbMetric: 104.6331 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 158/1000
2023-10-26 07:57:15.714 
Epoch 158/1000 
	 loss: 104.4411, MinusLogProbMetric: 104.4411, val_loss: 104.6253, val_MinusLogProbMetric: 104.6253

Epoch 158: val_loss did not improve from 104.62406
196/196 - 83s - loss: 104.4411 - MinusLogProbMetric: 104.4411 - val_loss: 104.6253 - val_MinusLogProbMetric: 104.6253 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 159/1000
2023-10-26 07:58:32.596 
Epoch 159/1000 
	 loss: 104.4222, MinusLogProbMetric: 104.4222, val_loss: 104.6315, val_MinusLogProbMetric: 104.6315

Epoch 159: val_loss did not improve from 104.62406
196/196 - 77s - loss: 104.4222 - MinusLogProbMetric: 104.4222 - val_loss: 104.6315 - val_MinusLogProbMetric: 104.6315 - lr: 5.0805e-08 - 77s/epoch - 392ms/step
Epoch 160/1000
2023-10-26 07:59:43.102 
Epoch 160/1000 
	 loss: 104.4262, MinusLogProbMetric: 104.4262, val_loss: 104.6084, val_MinusLogProbMetric: 104.6084

Epoch 160: val_loss improved from 104.62406 to 104.60835, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 72s - loss: 104.4262 - MinusLogProbMetric: 104.4262 - val_loss: 104.6084 - val_MinusLogProbMetric: 104.6084 - lr: 5.0805e-08 - 72s/epoch - 366ms/step
Epoch 161/1000
2023-10-26 08:01:03.093 
Epoch 161/1000 
	 loss: 104.4109, MinusLogProbMetric: 104.4109, val_loss: 104.5805, val_MinusLogProbMetric: 104.5805

Epoch 161: val_loss improved from 104.60835 to 104.58050, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 80s - loss: 104.4109 - MinusLogProbMetric: 104.4109 - val_loss: 104.5805 - val_MinusLogProbMetric: 104.5805 - lr: 5.0805e-08 - 80s/epoch - 410ms/step
Epoch 162/1000
2023-10-26 08:02:26.407 
Epoch 162/1000 
	 loss: 104.4028, MinusLogProbMetric: 104.4028, val_loss: 104.5769, val_MinusLogProbMetric: 104.5769

Epoch 162: val_loss improved from 104.58050 to 104.57685, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 104.4028 - MinusLogProbMetric: 104.4028 - val_loss: 104.5769 - val_MinusLogProbMetric: 104.5769 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 163/1000
2023-10-26 08:03:50.744 
Epoch 163/1000 
	 loss: 104.3944, MinusLogProbMetric: 104.3944, val_loss: 104.5779, val_MinusLogProbMetric: 104.5779

Epoch 163: val_loss did not improve from 104.57685
196/196 - 83s - loss: 104.3944 - MinusLogProbMetric: 104.3944 - val_loss: 104.5779 - val_MinusLogProbMetric: 104.5779 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 164/1000
2023-10-26 08:05:14.494 
Epoch 164/1000 
	 loss: 104.3948, MinusLogProbMetric: 104.3948, val_loss: 104.5972, val_MinusLogProbMetric: 104.5972

Epoch 164: val_loss did not improve from 104.57685
196/196 - 84s - loss: 104.3948 - MinusLogProbMetric: 104.3948 - val_loss: 104.5972 - val_MinusLogProbMetric: 104.5972 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 165/1000
2023-10-26 08:06:37.884 
Epoch 165/1000 
	 loss: 104.3713, MinusLogProbMetric: 104.3713, val_loss: 104.5877, val_MinusLogProbMetric: 104.5877

Epoch 165: val_loss did not improve from 104.57685
196/196 - 83s - loss: 104.3713 - MinusLogProbMetric: 104.3713 - val_loss: 104.5877 - val_MinusLogProbMetric: 104.5877 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 166/1000
2023-10-26 08:08:02.294 
Epoch 166/1000 
	 loss: 104.3484, MinusLogProbMetric: 104.3484, val_loss: 104.5838, val_MinusLogProbMetric: 104.5838

Epoch 166: val_loss did not improve from 104.57685
196/196 - 84s - loss: 104.3484 - MinusLogProbMetric: 104.3484 - val_loss: 104.5838 - val_MinusLogProbMetric: 104.5838 - lr: 5.0805e-08 - 84s/epoch - 431ms/step
Epoch 167/1000
2023-10-26 08:09:26.131 
Epoch 167/1000 
	 loss: 104.3106, MinusLogProbMetric: 104.3106, val_loss: 104.5843, val_MinusLogProbMetric: 104.5843

Epoch 167: val_loss did not improve from 104.57685
196/196 - 84s - loss: 104.3106 - MinusLogProbMetric: 104.3106 - val_loss: 104.5843 - val_MinusLogProbMetric: 104.5843 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 168/1000
2023-10-26 08:10:41.516 
Epoch 168/1000 
	 loss: 104.3172, MinusLogProbMetric: 104.3172, val_loss: 104.5903, val_MinusLogProbMetric: 104.5903

Epoch 168: val_loss did not improve from 104.57685
196/196 - 75s - loss: 104.3172 - MinusLogProbMetric: 104.3172 - val_loss: 104.5903 - val_MinusLogProbMetric: 104.5903 - lr: 5.0805e-08 - 75s/epoch - 385ms/step
Epoch 169/1000
2023-10-26 08:11:57.526 
Epoch 169/1000 
	 loss: 104.3172, MinusLogProbMetric: 104.3172, val_loss: 104.5674, val_MinusLogProbMetric: 104.5674

Epoch 169: val_loss improved from 104.57685 to 104.56741, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 78s - loss: 104.3172 - MinusLogProbMetric: 104.3172 - val_loss: 104.5674 - val_MinusLogProbMetric: 104.5674 - lr: 5.0805e-08 - 78s/epoch - 397ms/step
Epoch 170/1000
2023-10-26 08:13:15.331 
Epoch 170/1000 
	 loss: 104.3146, MinusLogProbMetric: 104.3146, val_loss: 104.5603, val_MinusLogProbMetric: 104.5603

Epoch 170: val_loss improved from 104.56741 to 104.56033, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 78s - loss: 104.3146 - MinusLogProbMetric: 104.3146 - val_loss: 104.5603 - val_MinusLogProbMetric: 104.5603 - lr: 5.0805e-08 - 78s/epoch - 395ms/step
Epoch 171/1000
2023-10-26 08:14:33.139 
Epoch 171/1000 
	 loss: 104.2990, MinusLogProbMetric: 104.2990, val_loss: 104.5561, val_MinusLogProbMetric: 104.5561

Epoch 171: val_loss improved from 104.56033 to 104.55614, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 78s - loss: 104.2990 - MinusLogProbMetric: 104.2990 - val_loss: 104.5561 - val_MinusLogProbMetric: 104.5561 - lr: 5.0805e-08 - 78s/epoch - 398ms/step
Epoch 172/1000
2023-10-26 08:15:50.751 
Epoch 172/1000 
	 loss: 104.2872, MinusLogProbMetric: 104.2872, val_loss: 104.5629, val_MinusLogProbMetric: 104.5629

Epoch 172: val_loss did not improve from 104.55614
196/196 - 76s - loss: 104.2872 - MinusLogProbMetric: 104.2872 - val_loss: 104.5629 - val_MinusLogProbMetric: 104.5629 - lr: 5.0805e-08 - 76s/epoch - 387ms/step
Epoch 173/1000
2023-10-26 08:17:04.486 
Epoch 173/1000 
	 loss: 104.2875, MinusLogProbMetric: 104.2875, val_loss: 104.5495, val_MinusLogProbMetric: 104.5495

Epoch 173: val_loss improved from 104.55614 to 104.54949, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 75s - loss: 104.2875 - MinusLogProbMetric: 104.2875 - val_loss: 104.5495 - val_MinusLogProbMetric: 104.5495 - lr: 5.0805e-08 - 75s/epoch - 384ms/step
Epoch 174/1000
2023-10-26 08:18:23.612 
Epoch 174/1000 
	 loss: 104.2935, MinusLogProbMetric: 104.2935, val_loss: 104.5369, val_MinusLogProbMetric: 104.5369

Epoch 174: val_loss improved from 104.54949 to 104.53695, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 80s - loss: 104.2935 - MinusLogProbMetric: 104.2935 - val_loss: 104.5369 - val_MinusLogProbMetric: 104.5369 - lr: 5.0805e-08 - 80s/epoch - 406ms/step
Epoch 175/1000
2023-10-26 08:19:44.706 
Epoch 175/1000 
	 loss: 104.2755, MinusLogProbMetric: 104.2755, val_loss: 104.5426, val_MinusLogProbMetric: 104.5426

Epoch 175: val_loss did not improve from 104.53695
196/196 - 79s - loss: 104.2755 - MinusLogProbMetric: 104.2755 - val_loss: 104.5426 - val_MinusLogProbMetric: 104.5426 - lr: 5.0805e-08 - 79s/epoch - 404ms/step
Epoch 176/1000
2023-10-26 08:21:02.068 
Epoch 176/1000 
	 loss: 104.2810, MinusLogProbMetric: 104.2810, val_loss: 104.5364, val_MinusLogProbMetric: 104.5364

Epoch 176: val_loss improved from 104.53695 to 104.53643, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 79s - loss: 104.2810 - MinusLogProbMetric: 104.2810 - val_loss: 104.5364 - val_MinusLogProbMetric: 104.5364 - lr: 5.0805e-08 - 79s/epoch - 405ms/step
Epoch 177/1000
2023-10-26 08:22:14.806 
Epoch 177/1000 
	 loss: 104.2690, MinusLogProbMetric: 104.2690, val_loss: 104.5281, val_MinusLogProbMetric: 104.5281

Epoch 177: val_loss improved from 104.53643 to 104.52812, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 73s - loss: 104.2690 - MinusLogProbMetric: 104.2690 - val_loss: 104.5281 - val_MinusLogProbMetric: 104.5281 - lr: 5.0805e-08 - 73s/epoch - 370ms/step
Epoch 178/1000
2023-10-26 08:23:34.183 
Epoch 178/1000 
	 loss: 104.2639, MinusLogProbMetric: 104.2639, val_loss: 104.5344, val_MinusLogProbMetric: 104.5344

Epoch 178: val_loss did not improve from 104.52812
196/196 - 78s - loss: 104.2639 - MinusLogProbMetric: 104.2639 - val_loss: 104.5344 - val_MinusLogProbMetric: 104.5344 - lr: 5.0805e-08 - 78s/epoch - 396ms/step
Epoch 179/1000
2023-10-26 08:24:48.938 
Epoch 179/1000 
	 loss: 104.2618, MinusLogProbMetric: 104.2618, val_loss: 104.5175, val_MinusLogProbMetric: 104.5175

Epoch 179: val_loss improved from 104.52812 to 104.51747, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 76s - loss: 104.2618 - MinusLogProbMetric: 104.2618 - val_loss: 104.5175 - val_MinusLogProbMetric: 104.5175 - lr: 5.0805e-08 - 76s/epoch - 389ms/step
Epoch 180/1000
2023-10-26 08:26:08.033 
Epoch 180/1000 
	 loss: 104.2606, MinusLogProbMetric: 104.2606, val_loss: 104.5309, val_MinusLogProbMetric: 104.5309

Epoch 180: val_loss did not improve from 104.51747
196/196 - 78s - loss: 104.2606 - MinusLogProbMetric: 104.2606 - val_loss: 104.5309 - val_MinusLogProbMetric: 104.5309 - lr: 5.0805e-08 - 78s/epoch - 396ms/step
Epoch 181/1000
2023-10-26 08:27:25.583 
Epoch 181/1000 
	 loss: 104.2577, MinusLogProbMetric: 104.2577, val_loss: 104.5108, val_MinusLogProbMetric: 104.5108

Epoch 181: val_loss improved from 104.51747 to 104.51079, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 79s - loss: 104.2577 - MinusLogProbMetric: 104.2577 - val_loss: 104.5108 - val_MinusLogProbMetric: 104.5108 - lr: 5.0805e-08 - 79s/epoch - 405ms/step
Epoch 182/1000
2023-10-26 08:28:42.418 
Epoch 182/1000 
	 loss: 104.2495, MinusLogProbMetric: 104.2495, val_loss: 104.5055, val_MinusLogProbMetric: 104.5055

Epoch 182: val_loss improved from 104.51079 to 104.50552, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 77s - loss: 104.2495 - MinusLogProbMetric: 104.2495 - val_loss: 104.5055 - val_MinusLogProbMetric: 104.5055 - lr: 5.0805e-08 - 77s/epoch - 394ms/step
Epoch 183/1000
2023-10-26 08:30:07.344 
Epoch 183/1000 
	 loss: 104.2563, MinusLogProbMetric: 104.2563, val_loss: 104.5212, val_MinusLogProbMetric: 104.5212

Epoch 183: val_loss did not improve from 104.50552
196/196 - 83s - loss: 104.2563 - MinusLogProbMetric: 104.2563 - val_loss: 104.5212 - val_MinusLogProbMetric: 104.5212 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 184/1000
2023-10-26 08:31:31.380 
Epoch 184/1000 
	 loss: 104.2415, MinusLogProbMetric: 104.2415, val_loss: 104.5138, val_MinusLogProbMetric: 104.5138

Epoch 184: val_loss did not improve from 104.50552
196/196 - 84s - loss: 104.2415 - MinusLogProbMetric: 104.2415 - val_loss: 104.5138 - val_MinusLogProbMetric: 104.5138 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 185/1000
2023-10-26 08:32:55.304 
Epoch 185/1000 
	 loss: 104.2429, MinusLogProbMetric: 104.2429, val_loss: 104.5074, val_MinusLogProbMetric: 104.5074

Epoch 185: val_loss did not improve from 104.50552
196/196 - 84s - loss: 104.2429 - MinusLogProbMetric: 104.2429 - val_loss: 104.5074 - val_MinusLogProbMetric: 104.5074 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 186/1000
2023-10-26 08:34:18.020 
Epoch 186/1000 
	 loss: 104.2693, MinusLogProbMetric: 104.2693, val_loss: 104.5233, val_MinusLogProbMetric: 104.5233

Epoch 186: val_loss did not improve from 104.50552
196/196 - 83s - loss: 104.2693 - MinusLogProbMetric: 104.2693 - val_loss: 104.5233 - val_MinusLogProbMetric: 104.5233 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 187/1000
2023-10-26 08:35:42.574 
Epoch 187/1000 
	 loss: 104.2422, MinusLogProbMetric: 104.2422, val_loss: 104.5003, val_MinusLogProbMetric: 104.5003

Epoch 187: val_loss improved from 104.50552 to 104.50026, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 104.2422 - MinusLogProbMetric: 104.2422 - val_loss: 104.5003 - val_MinusLogProbMetric: 104.5003 - lr: 5.0805e-08 - 86s/epoch - 440ms/step
Epoch 188/1000
2023-10-26 08:37:09.046 
Epoch 188/1000 
	 loss: 104.2382, MinusLogProbMetric: 104.2382, val_loss: 104.4949, val_MinusLogProbMetric: 104.4949

Epoch 188: val_loss improved from 104.50026 to 104.49495, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 104.2382 - MinusLogProbMetric: 104.2382 - val_loss: 104.4949 - val_MinusLogProbMetric: 104.4949 - lr: 5.0805e-08 - 87s/epoch - 443ms/step
Epoch 189/1000
2023-10-26 08:38:35.983 
Epoch 189/1000 
	 loss: 104.2331, MinusLogProbMetric: 104.2331, val_loss: 104.4934, val_MinusLogProbMetric: 104.4934

Epoch 189: val_loss improved from 104.49495 to 104.49341, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 104.2331 - MinusLogProbMetric: 104.2331 - val_loss: 104.4934 - val_MinusLogProbMetric: 104.4934 - lr: 5.0805e-08 - 87s/epoch - 445ms/step
Epoch 190/1000
2023-10-26 08:40:02.311 
Epoch 190/1000 
	 loss: 104.2290, MinusLogProbMetric: 104.2290, val_loss: 104.4900, val_MinusLogProbMetric: 104.4900

Epoch 190: val_loss improved from 104.49341 to 104.49001, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 104.2290 - MinusLogProbMetric: 104.2290 - val_loss: 104.4900 - val_MinusLogProbMetric: 104.4900 - lr: 5.0805e-08 - 86s/epoch - 439ms/step
Epoch 191/1000
2023-10-26 08:41:30.238 
Epoch 191/1000 
	 loss: 104.2258, MinusLogProbMetric: 104.2258, val_loss: 104.4836, val_MinusLogProbMetric: 104.4836

Epoch 191: val_loss improved from 104.49001 to 104.48357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 88s - loss: 104.2258 - MinusLogProbMetric: 104.2258 - val_loss: 104.4836 - val_MinusLogProbMetric: 104.4836 - lr: 5.0805e-08 - 88s/epoch - 447ms/step
Epoch 192/1000
2023-10-26 08:42:57.077 
Epoch 192/1000 
	 loss: 104.2163, MinusLogProbMetric: 104.2163, val_loss: 104.4784, val_MinusLogProbMetric: 104.4784

Epoch 192: val_loss improved from 104.48357 to 104.47842, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 104.2163 - MinusLogProbMetric: 104.2163 - val_loss: 104.4784 - val_MinusLogProbMetric: 104.4784 - lr: 5.0805e-08 - 87s/epoch - 443ms/step
Epoch 193/1000
2023-10-26 08:44:22.961 
Epoch 193/1000 
	 loss: 104.2146, MinusLogProbMetric: 104.2146, val_loss: 104.4717, val_MinusLogProbMetric: 104.4717

Epoch 193: val_loss improved from 104.47842 to 104.47172, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 104.2146 - MinusLogProbMetric: 104.2146 - val_loss: 104.4717 - val_MinusLogProbMetric: 104.4717 - lr: 5.0805e-08 - 86s/epoch - 440ms/step
Epoch 194/1000
2023-10-26 08:45:47.917 
Epoch 194/1000 
	 loss: 104.2070, MinusLogProbMetric: 104.2070, val_loss: 104.4719, val_MinusLogProbMetric: 104.4719

Epoch 194: val_loss did not improve from 104.47172
196/196 - 83s - loss: 104.2070 - MinusLogProbMetric: 104.2070 - val_loss: 104.4719 - val_MinusLogProbMetric: 104.4719 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 195/1000
2023-10-26 08:47:11.985 
Epoch 195/1000 
	 loss: 104.2078, MinusLogProbMetric: 104.2078, val_loss: 104.4647, val_MinusLogProbMetric: 104.4647

Epoch 195: val_loss improved from 104.47172 to 104.46467, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 104.2078 - MinusLogProbMetric: 104.2078 - val_loss: 104.4647 - val_MinusLogProbMetric: 104.4647 - lr: 5.0805e-08 - 86s/epoch - 437ms/step
Epoch 196/1000
2023-10-26 08:48:37.784 
Epoch 196/1000 
	 loss: 104.2010, MinusLogProbMetric: 104.2010, val_loss: 104.4580, val_MinusLogProbMetric: 104.4580

Epoch 196: val_loss improved from 104.46467 to 104.45802, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 104.2010 - MinusLogProbMetric: 104.2010 - val_loss: 104.4580 - val_MinusLogProbMetric: 104.4580 - lr: 5.0805e-08 - 86s/epoch - 437ms/step
Epoch 197/1000
2023-10-26 08:50:00.816 
Epoch 197/1000 
	 loss: 104.1883, MinusLogProbMetric: 104.1883, val_loss: 104.4676, val_MinusLogProbMetric: 104.4676

Epoch 197: val_loss did not improve from 104.45802
196/196 - 82s - loss: 104.1883 - MinusLogProbMetric: 104.1883 - val_loss: 104.4676 - val_MinusLogProbMetric: 104.4676 - lr: 5.0805e-08 - 82s/epoch - 416ms/step
Epoch 198/1000
2023-10-26 08:51:26.580 
Epoch 198/1000 
	 loss: 104.1873, MinusLogProbMetric: 104.1873, val_loss: 104.4545, val_MinusLogProbMetric: 104.4545

Epoch 198: val_loss improved from 104.45802 to 104.45450, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 104.1873 - MinusLogProbMetric: 104.1873 - val_loss: 104.4545 - val_MinusLogProbMetric: 104.4545 - lr: 5.0805e-08 - 87s/epoch - 446ms/step
Epoch 199/1000
2023-10-26 08:52:52.334 
Epoch 199/1000 
	 loss: 104.1843, MinusLogProbMetric: 104.1843, val_loss: 104.4536, val_MinusLogProbMetric: 104.4536

Epoch 199: val_loss improved from 104.45450 to 104.45357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 104.1843 - MinusLogProbMetric: 104.1843 - val_loss: 104.4536 - val_MinusLogProbMetric: 104.4536 - lr: 5.0805e-08 - 86s/epoch - 437ms/step
Epoch 200/1000
2023-10-26 08:54:18.945 
Epoch 200/1000 
	 loss: 104.1846, MinusLogProbMetric: 104.1846, val_loss: 104.4408, val_MinusLogProbMetric: 104.4408

Epoch 200: val_loss improved from 104.45357 to 104.44081, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 104.1846 - MinusLogProbMetric: 104.1846 - val_loss: 104.4408 - val_MinusLogProbMetric: 104.4408 - lr: 5.0805e-08 - 87s/epoch - 443ms/step
Epoch 201/1000
2023-10-26 08:55:45.507 
Epoch 201/1000 
	 loss: 104.1741, MinusLogProbMetric: 104.1741, val_loss: 104.4237, val_MinusLogProbMetric: 104.4237

Epoch 201: val_loss improved from 104.44081 to 104.42371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 104.1741 - MinusLogProbMetric: 104.1741 - val_loss: 104.4237 - val_MinusLogProbMetric: 104.4237 - lr: 5.0805e-08 - 87s/epoch - 442ms/step
Epoch 202/1000
2023-10-26 08:57:10.804 
Epoch 202/1000 
	 loss: 104.1778, MinusLogProbMetric: 104.1778, val_loss: 104.4241, val_MinusLogProbMetric: 104.4241

Epoch 202: val_loss did not improve from 104.42371
196/196 - 83s - loss: 104.1778 - MinusLogProbMetric: 104.1778 - val_loss: 104.4241 - val_MinusLogProbMetric: 104.4241 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 203/1000
2023-10-26 08:58:36.164 
Epoch 203/1000 
	 loss: 104.1712, MinusLogProbMetric: 104.1712, val_loss: 104.4273, val_MinusLogProbMetric: 104.4273

Epoch 203: val_loss did not improve from 104.42371
196/196 - 85s - loss: 104.1712 - MinusLogProbMetric: 104.1712 - val_loss: 104.4273 - val_MinusLogProbMetric: 104.4273 - lr: 5.0805e-08 - 85s/epoch - 435ms/step
Epoch 204/1000
2023-10-26 09:00:01.514 
Epoch 204/1000 
	 loss: 104.1720, MinusLogProbMetric: 104.1720, val_loss: 104.4348, val_MinusLogProbMetric: 104.4348

Epoch 204: val_loss did not improve from 104.42371
196/196 - 85s - loss: 104.1720 - MinusLogProbMetric: 104.1720 - val_loss: 104.4348 - val_MinusLogProbMetric: 104.4348 - lr: 5.0805e-08 - 85s/epoch - 435ms/step
Epoch 205/1000
2023-10-26 09:01:27.101 
Epoch 205/1000 
	 loss: 104.1647, MinusLogProbMetric: 104.1647, val_loss: 104.3922, val_MinusLogProbMetric: 104.3922

Epoch 205: val_loss improved from 104.42371 to 104.39217, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 104.1647 - MinusLogProbMetric: 104.1647 - val_loss: 104.3922 - val_MinusLogProbMetric: 104.3922 - lr: 5.0805e-08 - 87s/epoch - 443ms/step
Epoch 206/1000
2023-10-26 09:02:53.235 
Epoch 206/1000 
	 loss: 104.1580, MinusLogProbMetric: 104.1580, val_loss: 104.4262, val_MinusLogProbMetric: 104.4262

Epoch 206: val_loss did not improve from 104.39217
196/196 - 85s - loss: 104.1580 - MinusLogProbMetric: 104.1580 - val_loss: 104.4262 - val_MinusLogProbMetric: 104.4262 - lr: 5.0805e-08 - 85s/epoch - 433ms/step
Epoch 207/1000
2023-10-26 09:04:16.806 
Epoch 207/1000 
	 loss: 104.1574, MinusLogProbMetric: 104.1574, val_loss: 104.4146, val_MinusLogProbMetric: 104.4146

Epoch 207: val_loss did not improve from 104.39217
196/196 - 84s - loss: 104.1574 - MinusLogProbMetric: 104.1574 - val_loss: 104.4146 - val_MinusLogProbMetric: 104.4146 - lr: 5.0805e-08 - 84s/epoch - 426ms/step
Epoch 208/1000
2023-10-26 09:05:40.358 
Epoch 208/1000 
	 loss: 104.1437, MinusLogProbMetric: 104.1437, val_loss: 104.4208, val_MinusLogProbMetric: 104.4208

Epoch 208: val_loss did not improve from 104.39217
196/196 - 84s - loss: 104.1437 - MinusLogProbMetric: 104.1437 - val_loss: 104.4208 - val_MinusLogProbMetric: 104.4208 - lr: 5.0805e-08 - 84s/epoch - 426ms/step
Epoch 209/1000
2023-10-26 09:07:04.267 
Epoch 209/1000 
	 loss: 104.1386, MinusLogProbMetric: 104.1386, val_loss: 104.4081, val_MinusLogProbMetric: 104.4081

Epoch 209: val_loss did not improve from 104.39217
196/196 - 84s - loss: 104.1386 - MinusLogProbMetric: 104.1386 - val_loss: 104.4081 - val_MinusLogProbMetric: 104.4081 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 210/1000
2023-10-26 09:08:28.569 
Epoch 210/1000 
	 loss: 104.1480, MinusLogProbMetric: 104.1480, val_loss: 104.4118, val_MinusLogProbMetric: 104.4118

Epoch 210: val_loss did not improve from 104.39217
196/196 - 84s - loss: 104.1480 - MinusLogProbMetric: 104.1480 - val_loss: 104.4118 - val_MinusLogProbMetric: 104.4118 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 211/1000
2023-10-26 09:09:53.076 
Epoch 211/1000 
	 loss: 104.1417, MinusLogProbMetric: 104.1417, val_loss: 104.3971, val_MinusLogProbMetric: 104.3971

Epoch 211: val_loss did not improve from 104.39217
196/196 - 85s - loss: 104.1417 - MinusLogProbMetric: 104.1417 - val_loss: 104.3971 - val_MinusLogProbMetric: 104.3971 - lr: 5.0805e-08 - 85s/epoch - 431ms/step
Epoch 212/1000
2023-10-26 09:11:17.617 
Epoch 212/1000 
	 loss: 104.1513, MinusLogProbMetric: 104.1513, val_loss: 104.3795, val_MinusLogProbMetric: 104.3795

Epoch 212: val_loss improved from 104.39217 to 104.37950, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 104.1513 - MinusLogProbMetric: 104.1513 - val_loss: 104.3795 - val_MinusLogProbMetric: 104.3795 - lr: 5.0805e-08 - 87s/epoch - 442ms/step
Epoch 213/1000
2023-10-26 09:12:44.654 
Epoch 213/1000 
	 loss: 104.1489, MinusLogProbMetric: 104.1489, val_loss: 104.3817, val_MinusLogProbMetric: 104.3817

Epoch 213: val_loss did not improve from 104.37950
196/196 - 85s - loss: 104.1489 - MinusLogProbMetric: 104.1489 - val_loss: 104.3817 - val_MinusLogProbMetric: 104.3817 - lr: 5.0805e-08 - 85s/epoch - 433ms/step
Epoch 214/1000
2023-10-26 09:14:06.668 
Epoch 214/1000 
	 loss: 104.1416, MinusLogProbMetric: 104.1416, val_loss: 104.3744, val_MinusLogProbMetric: 104.3744

Epoch 214: val_loss improved from 104.37950 to 104.37435, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 104.1416 - MinusLogProbMetric: 104.1416 - val_loss: 104.3744 - val_MinusLogProbMetric: 104.3744 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 215/1000
2023-10-26 09:15:34.980 
Epoch 215/1000 
	 loss: 104.1403, MinusLogProbMetric: 104.1403, val_loss: 104.3839, val_MinusLogProbMetric: 104.3839

Epoch 215: val_loss did not improve from 104.37435
196/196 - 86s - loss: 104.1403 - MinusLogProbMetric: 104.1403 - val_loss: 104.3839 - val_MinusLogProbMetric: 104.3839 - lr: 5.0805e-08 - 86s/epoch - 441ms/step
Epoch 216/1000
2023-10-26 09:17:00.242 
Epoch 216/1000 
	 loss: 104.1348, MinusLogProbMetric: 104.1348, val_loss: 104.3922, val_MinusLogProbMetric: 104.3922

Epoch 216: val_loss did not improve from 104.37435
196/196 - 85s - loss: 104.1348 - MinusLogProbMetric: 104.1348 - val_loss: 104.3922 - val_MinusLogProbMetric: 104.3922 - lr: 5.0805e-08 - 85s/epoch - 435ms/step
Epoch 217/1000
2023-10-26 09:18:24.169 
Epoch 217/1000 
	 loss: 104.1295, MinusLogProbMetric: 104.1295, val_loss: 104.3896, val_MinusLogProbMetric: 104.3896

Epoch 217: val_loss did not improve from 104.37435
196/196 - 84s - loss: 104.1295 - MinusLogProbMetric: 104.1295 - val_loss: 104.3896 - val_MinusLogProbMetric: 104.3896 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 218/1000
2023-10-26 09:19:47.284 
Epoch 218/1000 
	 loss: 104.1212, MinusLogProbMetric: 104.1212, val_loss: 104.3807, val_MinusLogProbMetric: 104.3807

Epoch 218: val_loss did not improve from 104.37435
196/196 - 83s - loss: 104.1212 - MinusLogProbMetric: 104.1212 - val_loss: 104.3807 - val_MinusLogProbMetric: 104.3807 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 219/1000
2023-10-26 09:21:13.544 
Epoch 219/1000 
	 loss: 104.1214, MinusLogProbMetric: 104.1214, val_loss: 104.3603, val_MinusLogProbMetric: 104.3603

Epoch 219: val_loss improved from 104.37435 to 104.36030, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 88s - loss: 104.1214 - MinusLogProbMetric: 104.1214 - val_loss: 104.3603 - val_MinusLogProbMetric: 104.3603 - lr: 5.0805e-08 - 88s/epoch - 449ms/step
Epoch 220/1000
2023-10-26 09:22:40.209 
Epoch 220/1000 
	 loss: 104.1148, MinusLogProbMetric: 104.1148, val_loss: 104.3849, val_MinusLogProbMetric: 104.3849

Epoch 220: val_loss did not improve from 104.36030
196/196 - 85s - loss: 104.1148 - MinusLogProbMetric: 104.1148 - val_loss: 104.3849 - val_MinusLogProbMetric: 104.3849 - lr: 5.0805e-08 - 85s/epoch - 433ms/step
Epoch 221/1000
2023-10-26 09:24:05.487 
Epoch 221/1000 
	 loss: 104.1172, MinusLogProbMetric: 104.1172, val_loss: 104.3868, val_MinusLogProbMetric: 104.3868

Epoch 221: val_loss did not improve from 104.36030
196/196 - 85s - loss: 104.1172 - MinusLogProbMetric: 104.1172 - val_loss: 104.3868 - val_MinusLogProbMetric: 104.3868 - lr: 5.0805e-08 - 85s/epoch - 435ms/step
Epoch 222/1000
2023-10-26 09:25:30.475 
Epoch 222/1000 
	 loss: 104.1120, MinusLogProbMetric: 104.1120, val_loss: 104.3654, val_MinusLogProbMetric: 104.3654

Epoch 222: val_loss did not improve from 104.36030
196/196 - 85s - loss: 104.1120 - MinusLogProbMetric: 104.1120 - val_loss: 104.3654 - val_MinusLogProbMetric: 104.3654 - lr: 5.0805e-08 - 85s/epoch - 434ms/step
Epoch 223/1000
2023-10-26 09:26:54.753 
Epoch 223/1000 
	 loss: 104.1173, MinusLogProbMetric: 104.1173, val_loss: 104.3722, val_MinusLogProbMetric: 104.3722

Epoch 223: val_loss did not improve from 104.36030
196/196 - 84s - loss: 104.1173 - MinusLogProbMetric: 104.1173 - val_loss: 104.3722 - val_MinusLogProbMetric: 104.3722 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 224/1000
2023-10-26 09:28:20.044 
Epoch 224/1000 
	 loss: 104.1073, MinusLogProbMetric: 104.1073, val_loss: 104.3606, val_MinusLogProbMetric: 104.3606

Epoch 224: val_loss did not improve from 104.36030
196/196 - 85s - loss: 104.1073 - MinusLogProbMetric: 104.1073 - val_loss: 104.3606 - val_MinusLogProbMetric: 104.3606 - lr: 5.0805e-08 - 85s/epoch - 435ms/step
Epoch 225/1000
2023-10-26 09:29:44.567 
Epoch 225/1000 
	 loss: 104.0981, MinusLogProbMetric: 104.0981, val_loss: 104.3490, val_MinusLogProbMetric: 104.3490

Epoch 225: val_loss improved from 104.36030 to 104.34904, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 104.0981 - MinusLogProbMetric: 104.0981 - val_loss: 104.3490 - val_MinusLogProbMetric: 104.3490 - lr: 5.0805e-08 - 86s/epoch - 438ms/step
Epoch 226/1000
2023-10-26 09:31:11.801 
Epoch 226/1000 
	 loss: 104.1006, MinusLogProbMetric: 104.1006, val_loss: 104.3606, val_MinusLogProbMetric: 104.3606

Epoch 226: val_loss did not improve from 104.34904
196/196 - 86s - loss: 104.1006 - MinusLogProbMetric: 104.1006 - val_loss: 104.3606 - val_MinusLogProbMetric: 104.3606 - lr: 5.0805e-08 - 86s/epoch - 438ms/step
Epoch 227/1000
2023-10-26 09:32:38.259 
Epoch 227/1000 
	 loss: 104.0991, MinusLogProbMetric: 104.0991, val_loss: 104.3389, val_MinusLogProbMetric: 104.3389

Epoch 227: val_loss improved from 104.34904 to 104.33891, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 88s - loss: 104.0991 - MinusLogProbMetric: 104.0991 - val_loss: 104.3389 - val_MinusLogProbMetric: 104.3389 - lr: 5.0805e-08 - 88s/epoch - 449ms/step
Epoch 228/1000
2023-10-26 09:34:03.557 
Epoch 228/1000 
	 loss: 104.0834, MinusLogProbMetric: 104.0834, val_loss: 104.3507, val_MinusLogProbMetric: 104.3507

Epoch 228: val_loss did not improve from 104.33891
196/196 - 84s - loss: 104.0834 - MinusLogProbMetric: 104.0834 - val_loss: 104.3507 - val_MinusLogProbMetric: 104.3507 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 229/1000
2023-10-26 09:35:28.571 
Epoch 229/1000 
	 loss: 104.0858, MinusLogProbMetric: 104.0858, val_loss: 104.3388, val_MinusLogProbMetric: 104.3388

Epoch 229: val_loss improved from 104.33891 to 104.33884, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 104.0858 - MinusLogProbMetric: 104.0858 - val_loss: 104.3388 - val_MinusLogProbMetric: 104.3388 - lr: 5.0805e-08 - 86s/epoch - 440ms/step
Epoch 230/1000
2023-10-26 09:36:55.189 
Epoch 230/1000 
	 loss: 104.0812, MinusLogProbMetric: 104.0812, val_loss: 104.3353, val_MinusLogProbMetric: 104.3353

Epoch 230: val_loss improved from 104.33884 to 104.33528, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 104.0812 - MinusLogProbMetric: 104.0812 - val_loss: 104.3353 - val_MinusLogProbMetric: 104.3353 - lr: 5.0805e-08 - 87s/epoch - 444ms/step
Epoch 231/1000
2023-10-26 09:38:20.134 
Epoch 231/1000 
	 loss: 104.0704, MinusLogProbMetric: 104.0704, val_loss: 104.3288, val_MinusLogProbMetric: 104.3288

Epoch 231: val_loss improved from 104.33528 to 104.32880, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 104.0704 - MinusLogProbMetric: 104.0704 - val_loss: 104.3288 - val_MinusLogProbMetric: 104.3288 - lr: 5.0805e-08 - 85s/epoch - 434ms/step
Epoch 232/1000
2023-10-26 09:39:44.649 
Epoch 232/1000 
	 loss: 104.0790, MinusLogProbMetric: 104.0790, val_loss: 104.3286, val_MinusLogProbMetric: 104.3286

Epoch 232: val_loss improved from 104.32880 to 104.32861, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 104.0790 - MinusLogProbMetric: 104.0790 - val_loss: 104.3286 - val_MinusLogProbMetric: 104.3286 - lr: 5.0805e-08 - 84s/epoch - 431ms/step
Epoch 233/1000
2023-10-26 09:41:10.414 
Epoch 233/1000 
	 loss: 104.0869, MinusLogProbMetric: 104.0869, val_loss: 104.3307, val_MinusLogProbMetric: 104.3307

Epoch 233: val_loss did not improve from 104.32861
196/196 - 84s - loss: 104.0869 - MinusLogProbMetric: 104.0869 - val_loss: 104.3307 - val_MinusLogProbMetric: 104.3307 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 234/1000
2023-10-26 09:42:35.362 
Epoch 234/1000 
	 loss: 104.0589, MinusLogProbMetric: 104.0589, val_loss: 104.3186, val_MinusLogProbMetric: 104.3186

Epoch 234: val_loss improved from 104.32861 to 104.31862, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 104.0589 - MinusLogProbMetric: 104.0589 - val_loss: 104.3186 - val_MinusLogProbMetric: 104.3186 - lr: 5.0805e-08 - 87s/epoch - 442ms/step
Epoch 235/1000
2023-10-26 09:44:02.387 
Epoch 235/1000 
	 loss: 104.0802, MinusLogProbMetric: 104.0802, val_loss: 104.3285, val_MinusLogProbMetric: 104.3285

Epoch 235: val_loss did not improve from 104.31862
196/196 - 85s - loss: 104.0802 - MinusLogProbMetric: 104.0802 - val_loss: 104.3285 - val_MinusLogProbMetric: 104.3285 - lr: 5.0805e-08 - 85s/epoch - 436ms/step
Epoch 236/1000
2023-10-26 09:45:25.880 
Epoch 236/1000 
	 loss: 104.0573, MinusLogProbMetric: 104.0573, val_loss: 104.3340, val_MinusLogProbMetric: 104.3340

Epoch 236: val_loss did not improve from 104.31862
196/196 - 83s - loss: 104.0573 - MinusLogProbMetric: 104.0573 - val_loss: 104.3340 - val_MinusLogProbMetric: 104.3340 - lr: 5.0805e-08 - 83s/epoch - 426ms/step
Epoch 237/1000
2023-10-26 09:46:51.178 
Epoch 237/1000 
	 loss: 104.0650, MinusLogProbMetric: 104.0650, val_loss: 104.3298, val_MinusLogProbMetric: 104.3298

Epoch 237: val_loss did not improve from 104.31862
196/196 - 85s - loss: 104.0650 - MinusLogProbMetric: 104.0650 - val_loss: 104.3298 - val_MinusLogProbMetric: 104.3298 - lr: 5.0805e-08 - 85s/epoch - 435ms/step
Epoch 238/1000
2023-10-26 09:48:14.756 
Epoch 238/1000 
	 loss: 104.0636, MinusLogProbMetric: 104.0636, val_loss: 104.3179, val_MinusLogProbMetric: 104.3179

Epoch 238: val_loss improved from 104.31862 to 104.31788, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 104.0636 - MinusLogProbMetric: 104.0636 - val_loss: 104.3179 - val_MinusLogProbMetric: 104.3179 - lr: 5.0805e-08 - 86s/epoch - 436ms/step
Epoch 239/1000
2023-10-26 09:49:41.531 
Epoch 239/1000 
	 loss: 104.0926, MinusLogProbMetric: 104.0926, val_loss: 104.3391, val_MinusLogProbMetric: 104.3391

Epoch 239: val_loss did not improve from 104.31788
196/196 - 85s - loss: 104.0926 - MinusLogProbMetric: 104.0926 - val_loss: 104.3391 - val_MinusLogProbMetric: 104.3391 - lr: 5.0805e-08 - 85s/epoch - 433ms/step
Epoch 240/1000
2023-10-26 09:51:06.823 
Epoch 240/1000 
	 loss: 104.0960, MinusLogProbMetric: 104.0960, val_loss: 104.3559, val_MinusLogProbMetric: 104.3559

Epoch 240: val_loss did not improve from 104.31788
196/196 - 85s - loss: 104.0960 - MinusLogProbMetric: 104.0960 - val_loss: 104.3559 - val_MinusLogProbMetric: 104.3559 - lr: 5.0805e-08 - 85s/epoch - 435ms/step
Epoch 241/1000
2023-10-26 09:52:31.799 
Epoch 241/1000 
	 loss: 104.0614, MinusLogProbMetric: 104.0614, val_loss: 104.3273, val_MinusLogProbMetric: 104.3273

Epoch 241: val_loss did not improve from 104.31788
196/196 - 85s - loss: 104.0614 - MinusLogProbMetric: 104.0614 - val_loss: 104.3273 - val_MinusLogProbMetric: 104.3273 - lr: 5.0805e-08 - 85s/epoch - 434ms/step
Epoch 242/1000
2023-10-26 09:53:55.613 
Epoch 242/1000 
	 loss: 104.0505, MinusLogProbMetric: 104.0505, val_loss: 104.3208, val_MinusLogProbMetric: 104.3208

Epoch 242: val_loss did not improve from 104.31788
196/196 - 84s - loss: 104.0505 - MinusLogProbMetric: 104.0505 - val_loss: 104.3208 - val_MinusLogProbMetric: 104.3208 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 243/1000
2023-10-26 09:55:20.148 
Epoch 243/1000 
	 loss: 104.0396, MinusLogProbMetric: 104.0396, val_loss: 104.3295, val_MinusLogProbMetric: 104.3295

Epoch 243: val_loss did not improve from 104.31788
196/196 - 85s - loss: 104.0396 - MinusLogProbMetric: 104.0396 - val_loss: 104.3295 - val_MinusLogProbMetric: 104.3295 - lr: 5.0805e-08 - 85s/epoch - 431ms/step
Epoch 244/1000
2023-10-26 09:56:46.232 
Epoch 244/1000 
	 loss: 104.0343, MinusLogProbMetric: 104.0343, val_loss: 104.2970, val_MinusLogProbMetric: 104.2970

Epoch 244: val_loss improved from 104.31788 to 104.29696, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 88s - loss: 104.0343 - MinusLogProbMetric: 104.0343 - val_loss: 104.2970 - val_MinusLogProbMetric: 104.2970 - lr: 5.0805e-08 - 88s/epoch - 448ms/step
Epoch 245/1000
2023-10-26 09:58:13.057 
Epoch 245/1000 
	 loss: 104.0390, MinusLogProbMetric: 104.0390, val_loss: 104.3119, val_MinusLogProbMetric: 104.3119

Epoch 245: val_loss did not improve from 104.29696
196/196 - 85s - loss: 104.0390 - MinusLogProbMetric: 104.0390 - val_loss: 104.3119 - val_MinusLogProbMetric: 104.3119 - lr: 5.0805e-08 - 85s/epoch - 434ms/step
Epoch 246/1000
2023-10-26 09:59:35.936 
Epoch 246/1000 
	 loss: 104.0397, MinusLogProbMetric: 104.0397, val_loss: 104.3297, val_MinusLogProbMetric: 104.3297

Epoch 246: val_loss did not improve from 104.29696
196/196 - 83s - loss: 104.0397 - MinusLogProbMetric: 104.0397 - val_loss: 104.3297 - val_MinusLogProbMetric: 104.3297 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 247/1000
2023-10-26 10:00:55.030 
Epoch 247/1000 
	 loss: 104.0392, MinusLogProbMetric: 104.0392, val_loss: 104.3031, val_MinusLogProbMetric: 104.3031

Epoch 247: val_loss did not improve from 104.29696
196/196 - 79s - loss: 104.0392 - MinusLogProbMetric: 104.0392 - val_loss: 104.3031 - val_MinusLogProbMetric: 104.3031 - lr: 5.0805e-08 - 79s/epoch - 404ms/step
Epoch 248/1000
2023-10-26 10:02:08.284 
Epoch 248/1000 
	 loss: 104.0261, MinusLogProbMetric: 104.0261, val_loss: 104.2998, val_MinusLogProbMetric: 104.2998

Epoch 248: val_loss did not improve from 104.29696
196/196 - 73s - loss: 104.0261 - MinusLogProbMetric: 104.0261 - val_loss: 104.2998 - val_MinusLogProbMetric: 104.2998 - lr: 5.0805e-08 - 73s/epoch - 374ms/step
Epoch 249/1000
2023-10-26 10:03:34.912 
Epoch 249/1000 
	 loss: 104.0285, MinusLogProbMetric: 104.0285, val_loss: 104.2753, val_MinusLogProbMetric: 104.2753

Epoch 249: val_loss improved from 104.29696 to 104.27528, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 104.0285 - MinusLogProbMetric: 104.0285 - val_loss: 104.2753 - val_MinusLogProbMetric: 104.2753 - lr: 5.0805e-08 - 89s/epoch - 452ms/step
Epoch 250/1000
2023-10-26 10:05:03.235 
Epoch 250/1000 
	 loss: 104.0070, MinusLogProbMetric: 104.0070, val_loss: 104.2923, val_MinusLogProbMetric: 104.2923

Epoch 250: val_loss did not improve from 104.27528
196/196 - 86s - loss: 104.0070 - MinusLogProbMetric: 104.0070 - val_loss: 104.2923 - val_MinusLogProbMetric: 104.2923 - lr: 5.0805e-08 - 86s/epoch - 441ms/step
Epoch 251/1000
2023-10-26 10:06:27.626 
Epoch 251/1000 
	 loss: 104.0034, MinusLogProbMetric: 104.0034, val_loss: 104.2620, val_MinusLogProbMetric: 104.2620

Epoch 251: val_loss improved from 104.27528 to 104.26203, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 104.0034 - MinusLogProbMetric: 104.0034 - val_loss: 104.2620 - val_MinusLogProbMetric: 104.2620 - lr: 5.0805e-08 - 86s/epoch - 439ms/step
Epoch 252/1000
2023-10-26 10:07:55.531 
Epoch 252/1000 
	 loss: 103.9961, MinusLogProbMetric: 103.9961, val_loss: 104.2696, val_MinusLogProbMetric: 104.2696

Epoch 252: val_loss did not improve from 104.26203
196/196 - 86s - loss: 103.9961 - MinusLogProbMetric: 103.9961 - val_loss: 104.2696 - val_MinusLogProbMetric: 104.2696 - lr: 5.0805e-08 - 86s/epoch - 440ms/step
Epoch 253/1000
2023-10-26 10:09:20.909 
Epoch 253/1000 
	 loss: 103.9921, MinusLogProbMetric: 103.9921, val_loss: 104.2842, val_MinusLogProbMetric: 104.2842

Epoch 253: val_loss did not improve from 104.26203
196/196 - 85s - loss: 103.9921 - MinusLogProbMetric: 103.9921 - val_loss: 104.2842 - val_MinusLogProbMetric: 104.2842 - lr: 5.0805e-08 - 85s/epoch - 436ms/step
Epoch 254/1000
2023-10-26 10:10:46.149 
Epoch 254/1000 
	 loss: 103.9952, MinusLogProbMetric: 103.9952, val_loss: 104.2785, val_MinusLogProbMetric: 104.2785

Epoch 254: val_loss did not improve from 104.26203
196/196 - 85s - loss: 103.9952 - MinusLogProbMetric: 103.9952 - val_loss: 104.2785 - val_MinusLogProbMetric: 104.2785 - lr: 5.0805e-08 - 85s/epoch - 435ms/step
Epoch 255/1000
2023-10-26 10:12:11.926 
Epoch 255/1000 
	 loss: 103.9872, MinusLogProbMetric: 103.9872, val_loss: 104.2649, val_MinusLogProbMetric: 104.2649

Epoch 255: val_loss did not improve from 104.26203
196/196 - 86s - loss: 103.9872 - MinusLogProbMetric: 103.9872 - val_loss: 104.2649 - val_MinusLogProbMetric: 104.2649 - lr: 5.0805e-08 - 86s/epoch - 438ms/step
Epoch 256/1000
2023-10-26 10:13:38.991 
Epoch 256/1000 
	 loss: 103.9913, MinusLogProbMetric: 103.9913, val_loss: 104.2646, val_MinusLogProbMetric: 104.2646

Epoch 256: val_loss did not improve from 104.26203
196/196 - 87s - loss: 103.9913 - MinusLogProbMetric: 103.9913 - val_loss: 104.2646 - val_MinusLogProbMetric: 104.2646 - lr: 5.0805e-08 - 87s/epoch - 444ms/step
Epoch 257/1000
2023-10-26 10:15:05.306 
Epoch 257/1000 
	 loss: 103.9828, MinusLogProbMetric: 103.9828, val_loss: 104.2626, val_MinusLogProbMetric: 104.2626

Epoch 257: val_loss did not improve from 104.26203
196/196 - 86s - loss: 103.9828 - MinusLogProbMetric: 103.9828 - val_loss: 104.2626 - val_MinusLogProbMetric: 104.2626 - lr: 5.0805e-08 - 86s/epoch - 440ms/step
Epoch 258/1000
2023-10-26 10:16:29.993 
Epoch 258/1000 
	 loss: 103.9762, MinusLogProbMetric: 103.9762, val_loss: 104.2690, val_MinusLogProbMetric: 104.2690

Epoch 258: val_loss did not improve from 104.26203
196/196 - 85s - loss: 103.9762 - MinusLogProbMetric: 103.9762 - val_loss: 104.2690 - val_MinusLogProbMetric: 104.2690 - lr: 5.0805e-08 - 85s/epoch - 432ms/step
Epoch 259/1000
2023-10-26 10:17:57.066 
Epoch 259/1000 
	 loss: 103.9775, MinusLogProbMetric: 103.9775, val_loss: 104.2601, val_MinusLogProbMetric: 104.2601

Epoch 259: val_loss improved from 104.26203 to 104.26006, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 89s - loss: 103.9775 - MinusLogProbMetric: 103.9775 - val_loss: 104.2601 - val_MinusLogProbMetric: 104.2601 - lr: 5.0805e-08 - 89s/epoch - 453ms/step
Epoch 260/1000
2023-10-26 10:19:28.505 
Epoch 260/1000 
	 loss: 103.9714, MinusLogProbMetric: 103.9714, val_loss: 104.2566, val_MinusLogProbMetric: 104.2566

Epoch 260: val_loss improved from 104.26006 to 104.25662, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 92s - loss: 103.9714 - MinusLogProbMetric: 103.9714 - val_loss: 104.2566 - val_MinusLogProbMetric: 104.2566 - lr: 5.0805e-08 - 92s/epoch - 468ms/step
Epoch 261/1000
2023-10-26 10:20:57.264 
Epoch 261/1000 
	 loss: 103.9703, MinusLogProbMetric: 103.9703, val_loss: 104.2470, val_MinusLogProbMetric: 104.2470

Epoch 261: val_loss improved from 104.25662 to 104.24696, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 88s - loss: 103.9703 - MinusLogProbMetric: 103.9703 - val_loss: 104.2470 - val_MinusLogProbMetric: 104.2470 - lr: 5.0805e-08 - 88s/epoch - 451ms/step
Epoch 262/1000
2023-10-26 10:22:24.641 
Epoch 262/1000 
	 loss: 103.9565, MinusLogProbMetric: 103.9565, val_loss: 104.2516, val_MinusLogProbMetric: 104.2516

Epoch 262: val_loss did not improve from 104.24696
196/196 - 86s - loss: 103.9565 - MinusLogProbMetric: 103.9565 - val_loss: 104.2516 - val_MinusLogProbMetric: 104.2516 - lr: 5.0805e-08 - 86s/epoch - 437ms/step
Epoch 263/1000
2023-10-26 10:23:49.715 
Epoch 263/1000 
	 loss: 103.9600, MinusLogProbMetric: 103.9600, val_loss: 104.2462, val_MinusLogProbMetric: 104.2462

Epoch 263: val_loss improved from 104.24696 to 104.24617, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 103.9600 - MinusLogProbMetric: 103.9600 - val_loss: 104.2462 - val_MinusLogProbMetric: 104.2462 - lr: 5.0805e-08 - 87s/epoch - 445ms/step
Epoch 264/1000
2023-10-26 10:25:18.285 
Epoch 264/1000 
	 loss: 103.9495, MinusLogProbMetric: 103.9495, val_loss: 104.2307, val_MinusLogProbMetric: 104.2307

Epoch 264: val_loss improved from 104.24617 to 104.23074, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 88s - loss: 103.9495 - MinusLogProbMetric: 103.9495 - val_loss: 104.2307 - val_MinusLogProbMetric: 104.2307 - lr: 5.0805e-08 - 88s/epoch - 451ms/step
Epoch 265/1000
2023-10-26 10:26:45.029 
Epoch 265/1000 
	 loss: 103.9504, MinusLogProbMetric: 103.9504, val_loss: 104.2334, val_MinusLogProbMetric: 104.2334

Epoch 265: val_loss did not improve from 104.23074
196/196 - 85s - loss: 103.9504 - MinusLogProbMetric: 103.9504 - val_loss: 104.2334 - val_MinusLogProbMetric: 104.2334 - lr: 5.0805e-08 - 85s/epoch - 432ms/step
Epoch 266/1000
2023-10-26 10:28:10.223 
Epoch 266/1000 
	 loss: 103.9486, MinusLogProbMetric: 103.9486, val_loss: 104.2350, val_MinusLogProbMetric: 104.2350

Epoch 266: val_loss did not improve from 104.23074
196/196 - 85s - loss: 103.9486 - MinusLogProbMetric: 103.9486 - val_loss: 104.2350 - val_MinusLogProbMetric: 104.2350 - lr: 5.0805e-08 - 85s/epoch - 435ms/step
Epoch 267/1000
2023-10-26 10:29:33.528 
Epoch 267/1000 
	 loss: 103.9509, MinusLogProbMetric: 103.9509, val_loss: 104.2355, val_MinusLogProbMetric: 104.2355

Epoch 267: val_loss did not improve from 104.23074
196/196 - 83s - loss: 103.9509 - MinusLogProbMetric: 103.9509 - val_loss: 104.2355 - val_MinusLogProbMetric: 104.2355 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 268/1000
2023-10-26 10:30:57.457 
Epoch 268/1000 
	 loss: 103.9527, MinusLogProbMetric: 103.9527, val_loss: 104.2405, val_MinusLogProbMetric: 104.2405

Epoch 268: val_loss did not improve from 104.23074
196/196 - 84s - loss: 103.9527 - MinusLogProbMetric: 103.9527 - val_loss: 104.2405 - val_MinusLogProbMetric: 104.2405 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 269/1000
2023-10-26 10:32:21.522 
Epoch 269/1000 
	 loss: 103.9394, MinusLogProbMetric: 103.9394, val_loss: 104.2186, val_MinusLogProbMetric: 104.2186

Epoch 269: val_loss improved from 104.23074 to 104.21855, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 103.9394 - MinusLogProbMetric: 103.9394 - val_loss: 104.2186 - val_MinusLogProbMetric: 104.2186 - lr: 5.0805e-08 - 86s/epoch - 440ms/step
Epoch 270/1000
2023-10-26 10:33:47.175 
Epoch 270/1000 
	 loss: 103.9213, MinusLogProbMetric: 103.9213, val_loss: 104.2200, val_MinusLogProbMetric: 104.2200

Epoch 270: val_loss did not improve from 104.21855
196/196 - 84s - loss: 103.9213 - MinusLogProbMetric: 103.9213 - val_loss: 104.2200 - val_MinusLogProbMetric: 104.2200 - lr: 5.0805e-08 - 84s/epoch - 426ms/step
Epoch 271/1000
2023-10-26 10:35:12.745 
Epoch 271/1000 
	 loss: 103.9262, MinusLogProbMetric: 103.9262, val_loss: 104.2039, val_MinusLogProbMetric: 104.2039

Epoch 271: val_loss improved from 104.21855 to 104.20392, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 103.9262 - MinusLogProbMetric: 103.9262 - val_loss: 104.2039 - val_MinusLogProbMetric: 104.2039 - lr: 5.0805e-08 - 87s/epoch - 445ms/step
Epoch 272/1000
2023-10-26 10:36:38.732 
Epoch 272/1000 
	 loss: 103.9231, MinusLogProbMetric: 103.9231, val_loss: 104.2153, val_MinusLogProbMetric: 104.2153

Epoch 272: val_loss did not improve from 104.20392
196/196 - 84s - loss: 103.9231 - MinusLogProbMetric: 103.9231 - val_loss: 104.2153 - val_MinusLogProbMetric: 104.2153 - lr: 5.0805e-08 - 84s/epoch - 431ms/step
Epoch 273/1000
2023-10-26 10:38:02.071 
Epoch 273/1000 
	 loss: 104.0387, MinusLogProbMetric: 104.0387, val_loss: 104.4074, val_MinusLogProbMetric: 104.4074

Epoch 273: val_loss did not improve from 104.20392
196/196 - 83s - loss: 104.0387 - MinusLogProbMetric: 104.0387 - val_loss: 104.4074 - val_MinusLogProbMetric: 104.4074 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 274/1000
2023-10-26 10:39:26.265 
Epoch 274/1000 
	 loss: 104.0804, MinusLogProbMetric: 104.0804, val_loss: 104.3291, val_MinusLogProbMetric: 104.3291

Epoch 274: val_loss did not improve from 104.20392
196/196 - 84s - loss: 104.0804 - MinusLogProbMetric: 104.0804 - val_loss: 104.3291 - val_MinusLogProbMetric: 104.3291 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 275/1000
2023-10-26 10:40:51.616 
Epoch 275/1000 
	 loss: 104.0430, MinusLogProbMetric: 104.0430, val_loss: 104.2961, val_MinusLogProbMetric: 104.2961

Epoch 275: val_loss did not improve from 104.20392
196/196 - 85s - loss: 104.0430 - MinusLogProbMetric: 104.0430 - val_loss: 104.2961 - val_MinusLogProbMetric: 104.2961 - lr: 5.0805e-08 - 85s/epoch - 435ms/step
Epoch 276/1000
2023-10-26 10:42:16.990 
Epoch 276/1000 
	 loss: 104.0027, MinusLogProbMetric: 104.0027, val_loss: 104.2823, val_MinusLogProbMetric: 104.2823

Epoch 276: val_loss did not improve from 104.20392
196/196 - 85s - loss: 104.0027 - MinusLogProbMetric: 104.0027 - val_loss: 104.2823 - val_MinusLogProbMetric: 104.2823 - lr: 5.0805e-08 - 85s/epoch - 436ms/step
Epoch 277/1000
2023-10-26 10:43:41.093 
Epoch 277/1000 
	 loss: 103.9827, MinusLogProbMetric: 103.9827, val_loss: 104.2725, val_MinusLogProbMetric: 104.2725

Epoch 277: val_loss did not improve from 104.20392
196/196 - 84s - loss: 103.9827 - MinusLogProbMetric: 103.9827 - val_loss: 104.2725 - val_MinusLogProbMetric: 104.2725 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 278/1000
2023-10-26 10:45:05.646 
Epoch 278/1000 
	 loss: 103.9715, MinusLogProbMetric: 103.9715, val_loss: 104.2448, val_MinusLogProbMetric: 104.2448

Epoch 278: val_loss did not improve from 104.20392
196/196 - 85s - loss: 103.9715 - MinusLogProbMetric: 103.9715 - val_loss: 104.2448 - val_MinusLogProbMetric: 104.2448 - lr: 5.0805e-08 - 85s/epoch - 431ms/step
Epoch 279/1000
2023-10-26 10:46:28.058 
Epoch 279/1000 
	 loss: 103.9651, MinusLogProbMetric: 103.9651, val_loss: 104.2224, val_MinusLogProbMetric: 104.2224

Epoch 279: val_loss did not improve from 104.20392
196/196 - 82s - loss: 103.9651 - MinusLogProbMetric: 103.9651 - val_loss: 104.2224 - val_MinusLogProbMetric: 104.2224 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 280/1000
2023-10-26 10:47:51.944 
Epoch 280/1000 
	 loss: 103.9748, MinusLogProbMetric: 103.9748, val_loss: 104.2532, val_MinusLogProbMetric: 104.2532

Epoch 280: val_loss did not improve from 104.20392
196/196 - 84s - loss: 103.9748 - MinusLogProbMetric: 103.9748 - val_loss: 104.2532 - val_MinusLogProbMetric: 104.2532 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 281/1000
2023-10-26 10:49:16.882 
Epoch 281/1000 
	 loss: 103.9477, MinusLogProbMetric: 103.9477, val_loss: 104.2526, val_MinusLogProbMetric: 104.2526

Epoch 281: val_loss did not improve from 104.20392
196/196 - 85s - loss: 103.9477 - MinusLogProbMetric: 103.9477 - val_loss: 104.2526 - val_MinusLogProbMetric: 104.2526 - lr: 5.0805e-08 - 85s/epoch - 433ms/step
Epoch 282/1000
2023-10-26 10:50:41.184 
Epoch 282/1000 
	 loss: 103.9519, MinusLogProbMetric: 103.9519, val_loss: 104.2481, val_MinusLogProbMetric: 104.2481

Epoch 282: val_loss did not improve from 104.20392
196/196 - 84s - loss: 103.9519 - MinusLogProbMetric: 103.9519 - val_loss: 104.2481 - val_MinusLogProbMetric: 104.2481 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 283/1000
2023-10-26 10:52:04.013 
Epoch 283/1000 
	 loss: 103.9479, MinusLogProbMetric: 103.9479, val_loss: 104.2206, val_MinusLogProbMetric: 104.2206

Epoch 283: val_loss did not improve from 104.20392
196/196 - 83s - loss: 103.9479 - MinusLogProbMetric: 103.9479 - val_loss: 104.2206 - val_MinusLogProbMetric: 104.2206 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 284/1000
2023-10-26 10:53:27.478 
Epoch 284/1000 
	 loss: 103.9409, MinusLogProbMetric: 103.9409, val_loss: 104.2369, val_MinusLogProbMetric: 104.2369

Epoch 284: val_loss did not improve from 104.20392
196/196 - 83s - loss: 103.9409 - MinusLogProbMetric: 103.9409 - val_loss: 104.2369 - val_MinusLogProbMetric: 104.2369 - lr: 5.0805e-08 - 83s/epoch - 426ms/step
Epoch 285/1000
2023-10-26 10:54:51.083 
Epoch 285/1000 
	 loss: 103.9407, MinusLogProbMetric: 103.9407, val_loss: 104.2143, val_MinusLogProbMetric: 104.2143

Epoch 285: val_loss did not improve from 104.20392
196/196 - 84s - loss: 103.9407 - MinusLogProbMetric: 103.9407 - val_loss: 104.2143 - val_MinusLogProbMetric: 104.2143 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 286/1000
2023-10-26 10:56:14.531 
Epoch 286/1000 
	 loss: 103.9395, MinusLogProbMetric: 103.9395, val_loss: 104.2058, val_MinusLogProbMetric: 104.2058

Epoch 286: val_loss did not improve from 104.20392
196/196 - 83s - loss: 103.9395 - MinusLogProbMetric: 103.9395 - val_loss: 104.2058 - val_MinusLogProbMetric: 104.2058 - lr: 5.0805e-08 - 83s/epoch - 426ms/step
Epoch 287/1000
2023-10-26 10:57:39.859 
Epoch 287/1000 
	 loss: 103.9363, MinusLogProbMetric: 103.9363, val_loss: 104.1838, val_MinusLogProbMetric: 104.1838

Epoch 287: val_loss improved from 104.20392 to 104.18383, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 103.9363 - MinusLogProbMetric: 103.9363 - val_loss: 104.1838 - val_MinusLogProbMetric: 104.1838 - lr: 5.0805e-08 - 87s/epoch - 443ms/step
Epoch 288/1000
2023-10-26 10:59:02.896 
Epoch 288/1000 
	 loss: 103.9258, MinusLogProbMetric: 103.9258, val_loss: 104.1990, val_MinusLogProbMetric: 104.1990

Epoch 288: val_loss did not improve from 104.18383
196/196 - 81s - loss: 103.9258 - MinusLogProbMetric: 103.9258 - val_loss: 104.1990 - val_MinusLogProbMetric: 104.1990 - lr: 5.0805e-08 - 81s/epoch - 416ms/step
Epoch 289/1000
2023-10-26 11:00:26.995 
Epoch 289/1000 
	 loss: 103.9210, MinusLogProbMetric: 103.9210, val_loss: 104.1942, val_MinusLogProbMetric: 104.1942

Epoch 289: val_loss did not improve from 104.18383
196/196 - 84s - loss: 103.9210 - MinusLogProbMetric: 103.9210 - val_loss: 104.1942 - val_MinusLogProbMetric: 104.1942 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 290/1000
2023-10-26 11:01:51.990 
Epoch 290/1000 
	 loss: 103.9148, MinusLogProbMetric: 103.9148, val_loss: 104.2085, val_MinusLogProbMetric: 104.2085

Epoch 290: val_loss did not improve from 104.18383
196/196 - 85s - loss: 103.9148 - MinusLogProbMetric: 103.9148 - val_loss: 104.2085 - val_MinusLogProbMetric: 104.2085 - lr: 5.0805e-08 - 85s/epoch - 434ms/step
Epoch 291/1000
2023-10-26 11:03:17.589 
Epoch 291/1000 
	 loss: 103.9046, MinusLogProbMetric: 103.9046, val_loss: 104.1830, val_MinusLogProbMetric: 104.1830

Epoch 291: val_loss improved from 104.18383 to 104.18300, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 103.9046 - MinusLogProbMetric: 103.9046 - val_loss: 104.1830 - val_MinusLogProbMetric: 104.1830 - lr: 5.0805e-08 - 87s/epoch - 446ms/step
Epoch 292/1000
2023-10-26 11:04:43.195 
Epoch 292/1000 
	 loss: 103.9114, MinusLogProbMetric: 103.9114, val_loss: 104.1981, val_MinusLogProbMetric: 104.1981

Epoch 292: val_loss did not improve from 104.18300
196/196 - 84s - loss: 103.9114 - MinusLogProbMetric: 103.9114 - val_loss: 104.1981 - val_MinusLogProbMetric: 104.1981 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 293/1000
2023-10-26 11:06:06.030 
Epoch 293/1000 
	 loss: 103.9010, MinusLogProbMetric: 103.9010, val_loss: 104.1598, val_MinusLogProbMetric: 104.1598

Epoch 293: val_loss improved from 104.18300 to 104.15984, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 103.9010 - MinusLogProbMetric: 103.9010 - val_loss: 104.1598 - val_MinusLogProbMetric: 104.1598 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 294/1000
2023-10-26 11:07:32.541 
Epoch 294/1000 
	 loss: 103.8932, MinusLogProbMetric: 103.8932, val_loss: 104.1703, val_MinusLogProbMetric: 104.1703

Epoch 294: val_loss did not improve from 104.15984
196/196 - 85s - loss: 103.8932 - MinusLogProbMetric: 103.8932 - val_loss: 104.1703 - val_MinusLogProbMetric: 104.1703 - lr: 5.0805e-08 - 85s/epoch - 434ms/step
Epoch 295/1000
2023-10-26 11:08:56.462 
Epoch 295/1000 
	 loss: 103.8875, MinusLogProbMetric: 103.8875, val_loss: 104.1830, val_MinusLogProbMetric: 104.1830

Epoch 295: val_loss did not improve from 104.15984
196/196 - 84s - loss: 103.8875 - MinusLogProbMetric: 103.8875 - val_loss: 104.1830 - val_MinusLogProbMetric: 104.1830 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 296/1000
2023-10-26 11:10:19.748 
Epoch 296/1000 
	 loss: 103.8913, MinusLogProbMetric: 103.8913, val_loss: 104.1628, val_MinusLogProbMetric: 104.1628

Epoch 296: val_loss did not improve from 104.15984
196/196 - 83s - loss: 103.8913 - MinusLogProbMetric: 103.8913 - val_loss: 104.1628 - val_MinusLogProbMetric: 104.1628 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 297/1000
2023-10-26 11:11:44.540 
Epoch 297/1000 
	 loss: 103.8828, MinusLogProbMetric: 103.8828, val_loss: 104.1470, val_MinusLogProbMetric: 104.1470

Epoch 297: val_loss improved from 104.15984 to 104.14696, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 103.8828 - MinusLogProbMetric: 103.8828 - val_loss: 104.1470 - val_MinusLogProbMetric: 104.1470 - lr: 5.0805e-08 - 87s/epoch - 441ms/step
Epoch 298/1000
2023-10-26 11:13:10.130 
Epoch 298/1000 
	 loss: 103.8803, MinusLogProbMetric: 103.8803, val_loss: 104.1654, val_MinusLogProbMetric: 104.1654

Epoch 298: val_loss did not improve from 104.14696
196/196 - 84s - loss: 103.8803 - MinusLogProbMetric: 103.8803 - val_loss: 104.1654 - val_MinusLogProbMetric: 104.1654 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 299/1000
2023-10-26 11:14:32.492 
Epoch 299/1000 
	 loss: 103.8697, MinusLogProbMetric: 103.8697, val_loss: 104.1626, val_MinusLogProbMetric: 104.1626

Epoch 299: val_loss did not improve from 104.14696
196/196 - 82s - loss: 103.8697 - MinusLogProbMetric: 103.8697 - val_loss: 104.1626 - val_MinusLogProbMetric: 104.1626 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 300/1000
2023-10-26 11:15:55.630 
Epoch 300/1000 
	 loss: 103.8678, MinusLogProbMetric: 103.8678, val_loss: 104.1650, val_MinusLogProbMetric: 104.1650

Epoch 300: val_loss did not improve from 104.14696
196/196 - 83s - loss: 103.8678 - MinusLogProbMetric: 103.8678 - val_loss: 104.1650 - val_MinusLogProbMetric: 104.1650 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 301/1000
2023-10-26 11:17:18.797 
Epoch 301/1000 
	 loss: 103.8554, MinusLogProbMetric: 103.8554, val_loss: 104.1587, val_MinusLogProbMetric: 104.1587

Epoch 301: val_loss did not improve from 104.14696
196/196 - 83s - loss: 103.8554 - MinusLogProbMetric: 103.8554 - val_loss: 104.1587 - val_MinusLogProbMetric: 104.1587 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 302/1000
2023-10-26 11:18:42.924 
Epoch 302/1000 
	 loss: 103.8606, MinusLogProbMetric: 103.8606, val_loss: 104.1574, val_MinusLogProbMetric: 104.1574

Epoch 302: val_loss did not improve from 104.14696
196/196 - 84s - loss: 103.8606 - MinusLogProbMetric: 103.8606 - val_loss: 104.1574 - val_MinusLogProbMetric: 104.1574 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 303/1000
2023-10-26 11:20:07.568 
Epoch 303/1000 
	 loss: 103.8589, MinusLogProbMetric: 103.8589, val_loss: 104.1440, val_MinusLogProbMetric: 104.1440

Epoch 303: val_loss improved from 104.14696 to 104.14395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 103.8589 - MinusLogProbMetric: 103.8589 - val_loss: 104.1440 - val_MinusLogProbMetric: 104.1440 - lr: 5.0805e-08 - 86s/epoch - 441ms/step
Epoch 304/1000
2023-10-26 11:21:32.982 
Epoch 304/1000 
	 loss: 103.8534, MinusLogProbMetric: 103.8534, val_loss: 104.1309, val_MinusLogProbMetric: 104.1309

Epoch 304: val_loss improved from 104.14395 to 104.13088, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 103.8534 - MinusLogProbMetric: 103.8534 - val_loss: 104.1309 - val_MinusLogProbMetric: 104.1309 - lr: 5.0805e-08 - 86s/epoch - 438ms/step
Epoch 305/1000
2023-10-26 11:23:00.601 
Epoch 305/1000 
	 loss: 103.8511, MinusLogProbMetric: 103.8511, val_loss: 104.1291, val_MinusLogProbMetric: 104.1291

Epoch 305: val_loss improved from 104.13088 to 104.12914, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 103.8511 - MinusLogProbMetric: 103.8511 - val_loss: 104.1291 - val_MinusLogProbMetric: 104.1291 - lr: 5.0805e-08 - 87s/epoch - 444ms/step
Epoch 306/1000
2023-10-26 11:24:27.729 
Epoch 306/1000 
	 loss: 103.8495, MinusLogProbMetric: 103.8495, val_loss: 104.1105, val_MinusLogProbMetric: 104.1105

Epoch 306: val_loss improved from 104.12914 to 104.11054, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 88s - loss: 103.8495 - MinusLogProbMetric: 103.8495 - val_loss: 104.1105 - val_MinusLogProbMetric: 104.1105 - lr: 5.0805e-08 - 88s/epoch - 448ms/step
Epoch 307/1000
2023-10-26 11:25:54.150 
Epoch 307/1000 
	 loss: 103.8376, MinusLogProbMetric: 103.8376, val_loss: 104.1147, val_MinusLogProbMetric: 104.1147

Epoch 307: val_loss did not improve from 104.11054
196/196 - 84s - loss: 103.8376 - MinusLogProbMetric: 103.8376 - val_loss: 104.1147 - val_MinusLogProbMetric: 104.1147 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 308/1000
2023-10-26 11:27:18.402 
Epoch 308/1000 
	 loss: 103.8357, MinusLogProbMetric: 103.8357, val_loss: 104.1257, val_MinusLogProbMetric: 104.1257

Epoch 308: val_loss did not improve from 104.11054
196/196 - 84s - loss: 103.8357 - MinusLogProbMetric: 103.8357 - val_loss: 104.1257 - val_MinusLogProbMetric: 104.1257 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 309/1000
2023-10-26 11:28:42.054 
Epoch 309/1000 
	 loss: 103.8393, MinusLogProbMetric: 103.8393, val_loss: 104.1013, val_MinusLogProbMetric: 104.1013

Epoch 309: val_loss improved from 104.11054 to 104.10132, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 103.8393 - MinusLogProbMetric: 103.8393 - val_loss: 104.1013 - val_MinusLogProbMetric: 104.1013 - lr: 5.0805e-08 - 85s/epoch - 434ms/step
Epoch 310/1000
2023-10-26 11:30:09.785 
Epoch 310/1000 
	 loss: 103.8201, MinusLogProbMetric: 103.8201, val_loss: 104.1089, val_MinusLogProbMetric: 104.1089

Epoch 310: val_loss did not improve from 104.10132
196/196 - 86s - loss: 103.8201 - MinusLogProbMetric: 103.8201 - val_loss: 104.1089 - val_MinusLogProbMetric: 104.1089 - lr: 5.0805e-08 - 86s/epoch - 440ms/step
Epoch 311/1000
2023-10-26 11:31:36.175 
Epoch 311/1000 
	 loss: 103.8283, MinusLogProbMetric: 103.8283, val_loss: 104.1205, val_MinusLogProbMetric: 104.1205

Epoch 311: val_loss did not improve from 104.10132
196/196 - 86s - loss: 103.8283 - MinusLogProbMetric: 103.8283 - val_loss: 104.1205 - val_MinusLogProbMetric: 104.1205 - lr: 5.0805e-08 - 86s/epoch - 441ms/step
Epoch 312/1000
2023-10-26 11:33:00.994 
Epoch 312/1000 
	 loss: 103.8222, MinusLogProbMetric: 103.8222, val_loss: 104.0982, val_MinusLogProbMetric: 104.0982

Epoch 312: val_loss improved from 104.10132 to 104.09818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 103.8222 - MinusLogProbMetric: 103.8222 - val_loss: 104.0982 - val_MinusLogProbMetric: 104.0982 - lr: 5.0805e-08 - 87s/epoch - 442ms/step
Epoch 313/1000
2023-10-26 11:34:27.997 
Epoch 313/1000 
	 loss: 103.8268, MinusLogProbMetric: 103.8268, val_loss: 104.1063, val_MinusLogProbMetric: 104.1063

Epoch 313: val_loss did not improve from 104.09818
196/196 - 85s - loss: 103.8268 - MinusLogProbMetric: 103.8268 - val_loss: 104.1063 - val_MinusLogProbMetric: 104.1063 - lr: 5.0805e-08 - 85s/epoch - 435ms/step
Epoch 314/1000
2023-10-26 11:35:52.510 
Epoch 314/1000 
	 loss: 103.8135, MinusLogProbMetric: 103.8135, val_loss: 104.0998, val_MinusLogProbMetric: 104.0998

Epoch 314: val_loss did not improve from 104.09818
196/196 - 85s - loss: 103.8135 - MinusLogProbMetric: 103.8135 - val_loss: 104.0998 - val_MinusLogProbMetric: 104.0998 - lr: 5.0805e-08 - 85s/epoch - 431ms/step
Epoch 315/1000
2023-10-26 11:37:18.086 
Epoch 315/1000 
	 loss: 103.8185, MinusLogProbMetric: 103.8185, val_loss: 104.0908, val_MinusLogProbMetric: 104.0908

Epoch 315: val_loss improved from 104.09818 to 104.09081, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 103.8185 - MinusLogProbMetric: 103.8185 - val_loss: 104.0908 - val_MinusLogProbMetric: 104.0908 - lr: 5.0805e-08 - 87s/epoch - 446ms/step
Epoch 316/1000
2023-10-26 11:38:46.384 
Epoch 316/1000 
	 loss: 103.8088, MinusLogProbMetric: 103.8088, val_loss: 104.0974, val_MinusLogProbMetric: 104.0974

Epoch 316: val_loss did not improve from 104.09081
196/196 - 86s - loss: 103.8088 - MinusLogProbMetric: 103.8088 - val_loss: 104.0974 - val_MinusLogProbMetric: 104.0974 - lr: 5.0805e-08 - 86s/epoch - 441ms/step
Epoch 317/1000
2023-10-26 11:40:11.439 
Epoch 317/1000 
	 loss: 103.8099, MinusLogProbMetric: 103.8099, val_loss: 104.0835, val_MinusLogProbMetric: 104.0835

Epoch 317: val_loss improved from 104.09081 to 104.08345, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 103.8099 - MinusLogProbMetric: 103.8099 - val_loss: 104.0835 - val_MinusLogProbMetric: 104.0835 - lr: 5.0805e-08 - 87s/epoch - 445ms/step
Epoch 318/1000
2023-10-26 11:41:36.995 
Epoch 318/1000 
	 loss: 103.8050, MinusLogProbMetric: 103.8050, val_loss: 104.0856, val_MinusLogProbMetric: 104.0856

Epoch 318: val_loss did not improve from 104.08345
196/196 - 83s - loss: 103.8050 - MinusLogProbMetric: 103.8050 - val_loss: 104.0856 - val_MinusLogProbMetric: 104.0856 - lr: 5.0805e-08 - 83s/epoch - 426ms/step
Epoch 319/1000
2023-10-26 11:43:01.551 
Epoch 319/1000 
	 loss: 103.8084, MinusLogProbMetric: 103.8084, val_loss: 104.0883, val_MinusLogProbMetric: 104.0883

Epoch 319: val_loss did not improve from 104.08345
196/196 - 85s - loss: 103.8084 - MinusLogProbMetric: 103.8084 - val_loss: 104.0883 - val_MinusLogProbMetric: 104.0883 - lr: 5.0805e-08 - 85s/epoch - 431ms/step
Epoch 320/1000
2023-10-26 11:44:27.169 
Epoch 320/1000 
	 loss: 103.7927, MinusLogProbMetric: 103.7927, val_loss: 104.0782, val_MinusLogProbMetric: 104.0782

Epoch 320: val_loss improved from 104.08345 to 104.07816, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 103.7927 - MinusLogProbMetric: 103.7927 - val_loss: 104.0782 - val_MinusLogProbMetric: 104.0782 - lr: 5.0805e-08 - 87s/epoch - 445ms/step
Epoch 321/1000
2023-10-26 11:45:52.760 
Epoch 321/1000 
	 loss: 103.8031, MinusLogProbMetric: 103.8031, val_loss: 104.0700, val_MinusLogProbMetric: 104.0700

Epoch 321: val_loss improved from 104.07816 to 104.07005, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 103.8031 - MinusLogProbMetric: 103.8031 - val_loss: 104.0700 - val_MinusLogProbMetric: 104.0700 - lr: 5.0805e-08 - 86s/epoch - 437ms/step
Epoch 322/1000
2023-10-26 11:47:17.938 
Epoch 322/1000 
	 loss: 103.8010, MinusLogProbMetric: 103.8010, val_loss: 104.0653, val_MinusLogProbMetric: 104.0653

Epoch 322: val_loss improved from 104.07005 to 104.06533, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 103.8010 - MinusLogProbMetric: 103.8010 - val_loss: 104.0653 - val_MinusLogProbMetric: 104.0653 - lr: 5.0805e-08 - 86s/epoch - 437ms/step
Epoch 323/1000
2023-10-26 11:48:45.153 
Epoch 323/1000 
	 loss: 103.7972, MinusLogProbMetric: 103.7972, val_loss: 104.0763, val_MinusLogProbMetric: 104.0763

Epoch 323: val_loss did not improve from 104.06533
196/196 - 85s - loss: 103.7972 - MinusLogProbMetric: 103.7972 - val_loss: 104.0763 - val_MinusLogProbMetric: 104.0763 - lr: 5.0805e-08 - 85s/epoch - 435ms/step
Epoch 324/1000
2023-10-26 11:50:09.513 
Epoch 324/1000 
	 loss: 103.7823, MinusLogProbMetric: 103.7823, val_loss: 104.0560, val_MinusLogProbMetric: 104.0560

Epoch 324: val_loss improved from 104.06533 to 104.05598, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 103.7823 - MinusLogProbMetric: 103.7823 - val_loss: 104.0560 - val_MinusLogProbMetric: 104.0560 - lr: 5.0805e-08 - 86s/epoch - 439ms/step
Epoch 325/1000
2023-10-26 11:51:36.133 
Epoch 325/1000 
	 loss: 103.7813, MinusLogProbMetric: 103.7813, val_loss: 104.0803, val_MinusLogProbMetric: 104.0803

Epoch 325: val_loss did not improve from 104.05598
196/196 - 85s - loss: 103.7813 - MinusLogProbMetric: 103.7813 - val_loss: 104.0803 - val_MinusLogProbMetric: 104.0803 - lr: 5.0805e-08 - 85s/epoch - 434ms/step
Epoch 326/1000
2023-10-26 11:53:00.886 
Epoch 326/1000 
	 loss: 103.7853, MinusLogProbMetric: 103.7853, val_loss: 104.0742, val_MinusLogProbMetric: 104.0742

Epoch 326: val_loss did not improve from 104.05598
196/196 - 85s - loss: 103.7853 - MinusLogProbMetric: 103.7853 - val_loss: 104.0742 - val_MinusLogProbMetric: 104.0742 - lr: 5.0805e-08 - 85s/epoch - 432ms/step
Epoch 327/1000
2023-10-26 11:54:25.614 
Epoch 327/1000 
	 loss: 103.7745, MinusLogProbMetric: 103.7745, val_loss: 104.0681, val_MinusLogProbMetric: 104.0681

Epoch 327: val_loss did not improve from 104.05598
196/196 - 85s - loss: 103.7745 - MinusLogProbMetric: 103.7745 - val_loss: 104.0681 - val_MinusLogProbMetric: 104.0681 - lr: 5.0805e-08 - 85s/epoch - 432ms/step
Epoch 328/1000
2023-10-26 11:55:49.697 
Epoch 328/1000 
	 loss: 103.7633, MinusLogProbMetric: 103.7633, val_loss: 104.0540, val_MinusLogProbMetric: 104.0540

Epoch 328: val_loss improved from 104.05598 to 104.05403, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 103.7633 - MinusLogProbMetric: 103.7633 - val_loss: 104.0540 - val_MinusLogProbMetric: 104.0540 - lr: 5.0805e-08 - 86s/epoch - 437ms/step
Epoch 329/1000
2023-10-26 11:57:15.630 
Epoch 329/1000 
	 loss: 103.7717, MinusLogProbMetric: 103.7717, val_loss: 104.0434, val_MinusLogProbMetric: 104.0434

Epoch 329: val_loss improved from 104.05403 to 104.04338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 103.7717 - MinusLogProbMetric: 103.7717 - val_loss: 104.0434 - val_MinusLogProbMetric: 104.0434 - lr: 5.0805e-08 - 86s/epoch - 439ms/step
Epoch 330/1000
2023-10-26 11:58:42.180 
Epoch 330/1000 
	 loss: 103.7727, MinusLogProbMetric: 103.7727, val_loss: 104.0329, val_MinusLogProbMetric: 104.0329

Epoch 330: val_loss improved from 104.04338 to 104.03285, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 86s - loss: 103.7727 - MinusLogProbMetric: 103.7727 - val_loss: 104.0329 - val_MinusLogProbMetric: 104.0329 - lr: 5.0805e-08 - 86s/epoch - 441ms/step
Epoch 331/1000
2023-10-26 12:00:07.477 
Epoch 331/1000 
	 loss: 103.7593, MinusLogProbMetric: 103.7593, val_loss: 104.0256, val_MinusLogProbMetric: 104.0256

Epoch 331: val_loss improved from 104.03285 to 104.02557, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 103.7593 - MinusLogProbMetric: 103.7593 - val_loss: 104.0256 - val_MinusLogProbMetric: 104.0256 - lr: 5.0805e-08 - 85s/epoch - 435ms/step
Epoch 332/1000
2023-10-26 12:01:33.952 
Epoch 332/1000 
	 loss: 103.7511, MinusLogProbMetric: 103.7511, val_loss: 104.0337, val_MinusLogProbMetric: 104.0337

Epoch 332: val_loss did not improve from 104.02557
196/196 - 85s - loss: 103.7511 - MinusLogProbMetric: 103.7511 - val_loss: 104.0337 - val_MinusLogProbMetric: 104.0337 - lr: 5.0805e-08 - 85s/epoch - 433ms/step
Epoch 333/1000
2023-10-26 12:02:56.502 
Epoch 333/1000 
	 loss: 103.7478, MinusLogProbMetric: 103.7478, val_loss: 104.0295, val_MinusLogProbMetric: 104.0295

Epoch 333: val_loss did not improve from 104.02557
196/196 - 83s - loss: 103.7478 - MinusLogProbMetric: 103.7478 - val_loss: 104.0295 - val_MinusLogProbMetric: 104.0295 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 334/1000
2023-10-26 12:04:20.935 
Epoch 334/1000 
	 loss: 103.7561, MinusLogProbMetric: 103.7561, val_loss: 104.0433, val_MinusLogProbMetric: 104.0433

Epoch 334: val_loss did not improve from 104.02557
196/196 - 84s - loss: 103.7561 - MinusLogProbMetric: 103.7561 - val_loss: 104.0433 - val_MinusLogProbMetric: 104.0433 - lr: 5.0805e-08 - 84s/epoch - 431ms/step
Epoch 335/1000
2023-10-26 12:05:46.672 
Epoch 335/1000 
	 loss: 103.7624, MinusLogProbMetric: 103.7624, val_loss: 104.0117, val_MinusLogProbMetric: 104.0117

Epoch 335: val_loss improved from 104.02557 to 104.01174, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 88s - loss: 103.7624 - MinusLogProbMetric: 103.7624 - val_loss: 104.0117 - val_MinusLogProbMetric: 104.0117 - lr: 5.0805e-08 - 88s/epoch - 447ms/step
Epoch 336/1000
2023-10-26 12:07:13.728 
Epoch 336/1000 
	 loss: 103.7536, MinusLogProbMetric: 103.7536, val_loss: 104.0164, val_MinusLogProbMetric: 104.0164

Epoch 336: val_loss did not improve from 104.01174
196/196 - 85s - loss: 103.7536 - MinusLogProbMetric: 103.7536 - val_loss: 104.0164 - val_MinusLogProbMetric: 104.0164 - lr: 5.0805e-08 - 85s/epoch - 434ms/step
Epoch 337/1000
2023-10-26 12:08:37.059 
Epoch 337/1000 
	 loss: 103.7415, MinusLogProbMetric: 103.7415, val_loss: 104.0118, val_MinusLogProbMetric: 104.0118

Epoch 337: val_loss did not improve from 104.01174
196/196 - 83s - loss: 103.7415 - MinusLogProbMetric: 103.7415 - val_loss: 104.0118 - val_MinusLogProbMetric: 104.0118 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 338/1000
2023-10-26 12:10:02.040 
Epoch 338/1000 
	 loss: 103.7447, MinusLogProbMetric: 103.7447, val_loss: 104.0028, val_MinusLogProbMetric: 104.0028

Epoch 338: val_loss improved from 104.01174 to 104.00278, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 103.7447 - MinusLogProbMetric: 103.7447 - val_loss: 104.0028 - val_MinusLogProbMetric: 104.0028 - lr: 5.0805e-08 - 87s/epoch - 442ms/step
Epoch 339/1000
2023-10-26 12:11:26.954 
Epoch 339/1000 
	 loss: 103.7454, MinusLogProbMetric: 103.7454, val_loss: 104.0184, val_MinusLogProbMetric: 104.0184

Epoch 339: val_loss did not improve from 104.00278
196/196 - 83s - loss: 103.7454 - MinusLogProbMetric: 103.7454 - val_loss: 104.0184 - val_MinusLogProbMetric: 104.0184 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 340/1000
2023-10-26 12:12:48.791 
Epoch 340/1000 
	 loss: 103.7267, MinusLogProbMetric: 103.7267, val_loss: 103.9982, val_MinusLogProbMetric: 103.9982

Epoch 340: val_loss improved from 104.00278 to 103.99825, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 103.7267 - MinusLogProbMetric: 103.7267 - val_loss: 103.9982 - val_MinusLogProbMetric: 103.9982 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 341/1000
2023-10-26 12:14:14.677 
Epoch 341/1000 
	 loss: 103.7258, MinusLogProbMetric: 103.7258, val_loss: 103.9989, val_MinusLogProbMetric: 103.9989

Epoch 341: val_loss did not improve from 103.99825
196/196 - 84s - loss: 103.7258 - MinusLogProbMetric: 103.7258 - val_loss: 103.9989 - val_MinusLogProbMetric: 103.9989 - lr: 5.0805e-08 - 84s/epoch - 431ms/step
Epoch 342/1000
2023-10-26 12:15:39.072 
Epoch 342/1000 
	 loss: 103.7238, MinusLogProbMetric: 103.7238, val_loss: 104.0010, val_MinusLogProbMetric: 104.0010

Epoch 342: val_loss did not improve from 103.99825
196/196 - 84s - loss: 103.7238 - MinusLogProbMetric: 103.7238 - val_loss: 104.0010 - val_MinusLogProbMetric: 104.0010 - lr: 5.0805e-08 - 84s/epoch - 431ms/step
Epoch 343/1000
2023-10-26 12:17:02.227 
Epoch 343/1000 
	 loss: 103.7194, MinusLogProbMetric: 103.7194, val_loss: 103.9788, val_MinusLogProbMetric: 103.9788

Epoch 343: val_loss improved from 103.99825 to 103.97878, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 103.7194 - MinusLogProbMetric: 103.7194 - val_loss: 103.9788 - val_MinusLogProbMetric: 103.9788 - lr: 5.0805e-08 - 85s/epoch - 434ms/step
Epoch 344/1000
2023-10-26 12:18:27.006 
Epoch 344/1000 
	 loss: 103.7142, MinusLogProbMetric: 103.7142, val_loss: 103.9717, val_MinusLogProbMetric: 103.9717

Epoch 344: val_loss improved from 103.97878 to 103.97168, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 103.7142 - MinusLogProbMetric: 103.7142 - val_loss: 103.9717 - val_MinusLogProbMetric: 103.9717 - lr: 5.0805e-08 - 85s/epoch - 432ms/step
Epoch 345/1000
2023-10-26 12:19:55.112 
Epoch 345/1000 
	 loss: 103.7090, MinusLogProbMetric: 103.7090, val_loss: 103.9714, val_MinusLogProbMetric: 103.9714

Epoch 345: val_loss improved from 103.97168 to 103.97137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 88s - loss: 103.7090 - MinusLogProbMetric: 103.7090 - val_loss: 103.9714 - val_MinusLogProbMetric: 103.9714 - lr: 5.0805e-08 - 88s/epoch - 450ms/step
Epoch 346/1000
2023-10-26 12:21:22.091 
Epoch 346/1000 
	 loss: 103.6968, MinusLogProbMetric: 103.6968, val_loss: 103.9657, val_MinusLogProbMetric: 103.9657

Epoch 346: val_loss improved from 103.97137 to 103.96571, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 87s - loss: 103.6968 - MinusLogProbMetric: 103.6968 - val_loss: 103.9657 - val_MinusLogProbMetric: 103.9657 - lr: 5.0805e-08 - 87s/epoch - 444ms/step
Epoch 347/1000
2023-10-26 12:22:47.125 
Epoch 347/1000 
	 loss: 103.6945, MinusLogProbMetric: 103.6945, val_loss: 103.9695, val_MinusLogProbMetric: 103.9695

Epoch 347: val_loss did not improve from 103.96571
196/196 - 83s - loss: 103.6945 - MinusLogProbMetric: 103.6945 - val_loss: 103.9695 - val_MinusLogProbMetric: 103.9695 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 348/1000
2023-10-26 12:24:12.773 
Epoch 348/1000 
	 loss: 103.6892, MinusLogProbMetric: 103.6892, val_loss: 103.9613, val_MinusLogProbMetric: 103.9613

Epoch 348: val_loss improved from 103.96571 to 103.96127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 88s - loss: 103.6892 - MinusLogProbMetric: 103.6892 - val_loss: 103.9613 - val_MinusLogProbMetric: 103.9613 - lr: 5.0805e-08 - 88s/epoch - 447ms/step
Epoch 349/1000
2023-10-26 12:25:38.181 
Epoch 349/1000 
	 loss: 103.6831, MinusLogProbMetric: 103.6831, val_loss: 103.9741, val_MinusLogProbMetric: 103.9741

Epoch 349: val_loss did not improve from 103.96127
196/196 - 83s - loss: 103.6831 - MinusLogProbMetric: 103.6831 - val_loss: 103.9741 - val_MinusLogProbMetric: 103.9741 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 350/1000
2023-10-26 12:26:43.726 
Epoch 350/1000 
	 loss: 103.6914, MinusLogProbMetric: 103.6914, val_loss: 103.9691, val_MinusLogProbMetric: 103.9691

Epoch 350: val_loss did not improve from 103.96127
196/196 - 66s - loss: 103.6914 - MinusLogProbMetric: 103.6914 - val_loss: 103.9691 - val_MinusLogProbMetric: 103.9691 - lr: 5.0805e-08 - 66s/epoch - 334ms/step
Epoch 351/1000
2023-10-26 12:27:56.489 
Epoch 351/1000 
	 loss: 103.6830, MinusLogProbMetric: 103.6830, val_loss: 103.9571, val_MinusLogProbMetric: 103.9571

Epoch 351: val_loss improved from 103.96127 to 103.95712, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 74s - loss: 103.6830 - MinusLogProbMetric: 103.6830 - val_loss: 103.9571 - val_MinusLogProbMetric: 103.9571 - lr: 5.0805e-08 - 74s/epoch - 380ms/step
Epoch 352/1000
2023-10-26 12:29:01.978 
Epoch 352/1000 
	 loss: 103.6747, MinusLogProbMetric: 103.6747, val_loss: 103.9475, val_MinusLogProbMetric: 103.9475

Epoch 352: val_loss improved from 103.95712 to 103.94749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 65s - loss: 103.6747 - MinusLogProbMetric: 103.6747 - val_loss: 103.9475 - val_MinusLogProbMetric: 103.9475 - lr: 5.0805e-08 - 65s/epoch - 332ms/step
Epoch 353/1000
2023-10-26 12:30:13.384 
Epoch 353/1000 
	 loss: 103.6744, MinusLogProbMetric: 103.6744, val_loss: 103.9643, val_MinusLogProbMetric: 103.9643

Epoch 353: val_loss did not improve from 103.94749
196/196 - 70s - loss: 103.6744 - MinusLogProbMetric: 103.6744 - val_loss: 103.9643 - val_MinusLogProbMetric: 103.9643 - lr: 5.0805e-08 - 70s/epoch - 358ms/step
Epoch 354/1000
2023-10-26 12:31:18.727 
Epoch 354/1000 
	 loss: 103.6743, MinusLogProbMetric: 103.6743, val_loss: 103.9542, val_MinusLogProbMetric: 103.9542

Epoch 354: val_loss did not improve from 103.94749
196/196 - 65s - loss: 103.6743 - MinusLogProbMetric: 103.6743 - val_loss: 103.9542 - val_MinusLogProbMetric: 103.9542 - lr: 5.0805e-08 - 65s/epoch - 333ms/step
Epoch 355/1000
2023-10-26 12:32:30.145 
Epoch 355/1000 
	 loss: 103.6687, MinusLogProbMetric: 103.6687, val_loss: 103.9401, val_MinusLogProbMetric: 103.9401

Epoch 355: val_loss improved from 103.94749 to 103.94013, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 73s - loss: 103.6687 - MinusLogProbMetric: 103.6687 - val_loss: 103.9401 - val_MinusLogProbMetric: 103.9401 - lr: 5.0805e-08 - 73s/epoch - 371ms/step
Epoch 356/1000
2023-10-26 12:33:34.766 
Epoch 356/1000 
	 loss: 103.6686, MinusLogProbMetric: 103.6686, val_loss: 103.9269, val_MinusLogProbMetric: 103.9269

Epoch 356: val_loss improved from 103.94013 to 103.92688, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 65s - loss: 103.6686 - MinusLogProbMetric: 103.6686 - val_loss: 103.9269 - val_MinusLogProbMetric: 103.9269 - lr: 5.0805e-08 - 65s/epoch - 329ms/step
Epoch 357/1000
2023-10-26 12:34:48.357 
Epoch 357/1000 
	 loss: 103.6620, MinusLogProbMetric: 103.6620, val_loss: 103.9519, val_MinusLogProbMetric: 103.9519

Epoch 357: val_loss did not improve from 103.92688
196/196 - 72s - loss: 103.6620 - MinusLogProbMetric: 103.6620 - val_loss: 103.9519 - val_MinusLogProbMetric: 103.9519 - lr: 5.0805e-08 - 72s/epoch - 369ms/step
Epoch 358/1000
2023-10-26 12:35:53.773 
Epoch 358/1000 
	 loss: 103.6532, MinusLogProbMetric: 103.6532, val_loss: 103.9336, val_MinusLogProbMetric: 103.9336

Epoch 358: val_loss did not improve from 103.92688
196/196 - 65s - loss: 103.6532 - MinusLogProbMetric: 103.6532 - val_loss: 103.9336 - val_MinusLogProbMetric: 103.9336 - lr: 5.0805e-08 - 65s/epoch - 334ms/step
Epoch 359/1000
2023-10-26 12:37:05.036 
Epoch 359/1000 
	 loss: 103.6495, MinusLogProbMetric: 103.6495, val_loss: 103.9232, val_MinusLogProbMetric: 103.9232

Epoch 359: val_loss improved from 103.92688 to 103.92319, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 73s - loss: 103.6495 - MinusLogProbMetric: 103.6495 - val_loss: 103.9232 - val_MinusLogProbMetric: 103.9232 - lr: 5.0805e-08 - 73s/epoch - 372ms/step
Epoch 360/1000
2023-10-26 12:38:10.351 
Epoch 360/1000 
	 loss: 103.6507, MinusLogProbMetric: 103.6507, val_loss: 103.9210, val_MinusLogProbMetric: 103.9210

Epoch 360: val_loss improved from 103.92319 to 103.92097, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 65s - loss: 103.6507 - MinusLogProbMetric: 103.6507 - val_loss: 103.9210 - val_MinusLogProbMetric: 103.9210 - lr: 5.0805e-08 - 65s/epoch - 330ms/step
Epoch 361/1000
2023-10-26 12:39:21.838 
Epoch 361/1000 
	 loss: 103.6449, MinusLogProbMetric: 103.6449, val_loss: 103.9246, val_MinusLogProbMetric: 103.9246

Epoch 361: val_loss did not improve from 103.92097
196/196 - 70s - loss: 103.6449 - MinusLogProbMetric: 103.6449 - val_loss: 103.9246 - val_MinusLogProbMetric: 103.9246 - lr: 5.0805e-08 - 70s/epoch - 359ms/step
Epoch 362/1000
2023-10-26 12:40:24.038 
Epoch 362/1000 
	 loss: 103.6451, MinusLogProbMetric: 103.6451, val_loss: 103.9083, val_MinusLogProbMetric: 103.9083

Epoch 362: val_loss improved from 103.92097 to 103.90833, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 63s - loss: 103.6451 - MinusLogProbMetric: 103.6451 - val_loss: 103.9083 - val_MinusLogProbMetric: 103.9083 - lr: 5.0805e-08 - 63s/epoch - 323ms/step
Epoch 363/1000
2023-10-26 12:41:37.621 
Epoch 363/1000 
	 loss: 103.6389, MinusLogProbMetric: 103.6389, val_loss: 103.9057, val_MinusLogProbMetric: 103.9057

Epoch 363: val_loss improved from 103.90833 to 103.90573, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 74s - loss: 103.6389 - MinusLogProbMetric: 103.6389 - val_loss: 103.9057 - val_MinusLogProbMetric: 103.9057 - lr: 5.0805e-08 - 74s/epoch - 378ms/step
Epoch 364/1000
2023-10-26 12:42:41.699 
Epoch 364/1000 
	 loss: 103.6361, MinusLogProbMetric: 103.6361, val_loss: 103.9200, val_MinusLogProbMetric: 103.9200

Epoch 364: val_loss did not improve from 103.90573
196/196 - 62s - loss: 103.6361 - MinusLogProbMetric: 103.6361 - val_loss: 103.9200 - val_MinusLogProbMetric: 103.9200 - lr: 5.0805e-08 - 62s/epoch - 318ms/step
Epoch 365/1000
2023-10-26 12:43:51.553 
Epoch 365/1000 
	 loss: 103.6355, MinusLogProbMetric: 103.6355, val_loss: 103.9214, val_MinusLogProbMetric: 103.9214

Epoch 365: val_loss did not improve from 103.90573
196/196 - 70s - loss: 103.6355 - MinusLogProbMetric: 103.6355 - val_loss: 103.9214 - val_MinusLogProbMetric: 103.9214 - lr: 5.0805e-08 - 70s/epoch - 356ms/step
Epoch 366/1000
2023-10-26 12:44:56.851 
Epoch 366/1000 
	 loss: 103.6272, MinusLogProbMetric: 103.6272, val_loss: 103.9011, val_MinusLogProbMetric: 103.9011

Epoch 366: val_loss improved from 103.90573 to 103.90111, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 66s - loss: 103.6272 - MinusLogProbMetric: 103.6272 - val_loss: 103.9011 - val_MinusLogProbMetric: 103.9011 - lr: 5.0805e-08 - 66s/epoch - 339ms/step
Epoch 367/1000
2023-10-26 12:46:02.430 
Epoch 367/1000 
	 loss: 103.6238, MinusLogProbMetric: 103.6238, val_loss: 103.8896, val_MinusLogProbMetric: 103.8896

Epoch 367: val_loss improved from 103.90111 to 103.88964, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 66s - loss: 103.6238 - MinusLogProbMetric: 103.6238 - val_loss: 103.8896 - val_MinusLogProbMetric: 103.8896 - lr: 5.0805e-08 - 66s/epoch - 334ms/step
Epoch 368/1000
2023-10-26 12:47:07.457 
Epoch 368/1000 
	 loss: 103.6212, MinusLogProbMetric: 103.6212, val_loss: 103.8921, val_MinusLogProbMetric: 103.8921

Epoch 368: val_loss did not improve from 103.88964
196/196 - 64s - loss: 103.6212 - MinusLogProbMetric: 103.6212 - val_loss: 103.8921 - val_MinusLogProbMetric: 103.8921 - lr: 5.0805e-08 - 64s/epoch - 326ms/step
Epoch 369/1000
2023-10-26 12:48:12.190 
Epoch 369/1000 
	 loss: 103.6130, MinusLogProbMetric: 103.6130, val_loss: 103.9003, val_MinusLogProbMetric: 103.9003

Epoch 369: val_loss did not improve from 103.88964
196/196 - 65s - loss: 103.6130 - MinusLogProbMetric: 103.6130 - val_loss: 103.9003 - val_MinusLogProbMetric: 103.9003 - lr: 5.0805e-08 - 65s/epoch - 330ms/step
Epoch 370/1000
2023-10-26 12:49:16.469 
Epoch 370/1000 
	 loss: 103.6171, MinusLogProbMetric: 103.6171, val_loss: 103.8969, val_MinusLogProbMetric: 103.8969

Epoch 370: val_loss did not improve from 103.88964
196/196 - 64s - loss: 103.6171 - MinusLogProbMetric: 103.6171 - val_loss: 103.8969 - val_MinusLogProbMetric: 103.8969 - lr: 5.0805e-08 - 64s/epoch - 328ms/step
Epoch 371/1000
2023-10-26 12:50:20.475 
Epoch 371/1000 
	 loss: 103.6149, MinusLogProbMetric: 103.6149, val_loss: 103.9096, val_MinusLogProbMetric: 103.9096

Epoch 371: val_loss did not improve from 103.88964
196/196 - 64s - loss: 103.6149 - MinusLogProbMetric: 103.6149 - val_loss: 103.9096 - val_MinusLogProbMetric: 103.9096 - lr: 5.0805e-08 - 64s/epoch - 327ms/step
Epoch 372/1000
2023-10-26 12:51:25.174 
Epoch 372/1000 
	 loss: 103.6131, MinusLogProbMetric: 103.6131, val_loss: 103.8972, val_MinusLogProbMetric: 103.8972

Epoch 372: val_loss did not improve from 103.88964
196/196 - 65s - loss: 103.6131 - MinusLogProbMetric: 103.6131 - val_loss: 103.8972 - val_MinusLogProbMetric: 103.8972 - lr: 5.0805e-08 - 65s/epoch - 330ms/step
Epoch 373/1000
2023-10-26 12:52:27.423 
Epoch 373/1000 
	 loss: 103.6098, MinusLogProbMetric: 103.6098, val_loss: 103.8994, val_MinusLogProbMetric: 103.8994

Epoch 373: val_loss did not improve from 103.88964
196/196 - 62s - loss: 103.6098 - MinusLogProbMetric: 103.6098 - val_loss: 103.8994 - val_MinusLogProbMetric: 103.8994 - lr: 5.0805e-08 - 62s/epoch - 318ms/step
Epoch 374/1000
2023-10-26 12:53:42.942 
Epoch 374/1000 
	 loss: 103.6010, MinusLogProbMetric: 103.6010, val_loss: 103.8967, val_MinusLogProbMetric: 103.8967

Epoch 374: val_loss did not improve from 103.88964
196/196 - 76s - loss: 103.6010 - MinusLogProbMetric: 103.6010 - val_loss: 103.8967 - val_MinusLogProbMetric: 103.8967 - lr: 5.0805e-08 - 76s/epoch - 385ms/step
Epoch 375/1000
2023-10-26 12:54:45.895 
Epoch 375/1000 
	 loss: 103.5992, MinusLogProbMetric: 103.5992, val_loss: 103.8866, val_MinusLogProbMetric: 103.8866

Epoch 375: val_loss improved from 103.88964 to 103.88655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 64s - loss: 103.5992 - MinusLogProbMetric: 103.5992 - val_loss: 103.8866 - val_MinusLogProbMetric: 103.8866 - lr: 5.0805e-08 - 64s/epoch - 328ms/step
Epoch 376/1000
2023-10-26 12:55:53.121 
Epoch 376/1000 
	 loss: 103.5964, MinusLogProbMetric: 103.5964, val_loss: 103.8860, val_MinusLogProbMetric: 103.8860

Epoch 376: val_loss improved from 103.88655 to 103.88603, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 68s - loss: 103.5964 - MinusLogProbMetric: 103.5964 - val_loss: 103.8860 - val_MinusLogProbMetric: 103.8860 - lr: 5.0805e-08 - 68s/epoch - 345ms/step
Epoch 377/1000
2023-10-26 12:57:05.557 
Epoch 377/1000 
	 loss: 103.5857, MinusLogProbMetric: 103.5857, val_loss: 103.8855, val_MinusLogProbMetric: 103.8855

Epoch 377: val_loss improved from 103.88603 to 103.88554, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 72s - loss: 103.5857 - MinusLogProbMetric: 103.5857 - val_loss: 103.8855 - val_MinusLogProbMetric: 103.8855 - lr: 5.0805e-08 - 72s/epoch - 367ms/step
Epoch 378/1000
2023-10-26 12:58:09.452 
Epoch 378/1000 
	 loss: 103.5827, MinusLogProbMetric: 103.5827, val_loss: 103.8966, val_MinusLogProbMetric: 103.8966

Epoch 378: val_loss did not improve from 103.88554
196/196 - 63s - loss: 103.5827 - MinusLogProbMetric: 103.5827 - val_loss: 103.8966 - val_MinusLogProbMetric: 103.8966 - lr: 5.0805e-08 - 63s/epoch - 320ms/step
Epoch 379/1000
2023-10-26 12:59:14.805 
Epoch 379/1000 
	 loss: 103.6005, MinusLogProbMetric: 103.6005, val_loss: 103.8698, val_MinusLogProbMetric: 103.8698

Epoch 379: val_loss improved from 103.88554 to 103.86979, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 67s - loss: 103.6005 - MinusLogProbMetric: 103.6005 - val_loss: 103.8698 - val_MinusLogProbMetric: 103.8698 - lr: 5.0805e-08 - 67s/epoch - 340ms/step
Epoch 380/1000
2023-10-26 13:00:18.491 
Epoch 380/1000 
	 loss: 103.5920, MinusLogProbMetric: 103.5920, val_loss: 103.8786, val_MinusLogProbMetric: 103.8786

Epoch 380: val_loss did not improve from 103.86979
196/196 - 62s - loss: 103.5920 - MinusLogProbMetric: 103.5920 - val_loss: 103.8786 - val_MinusLogProbMetric: 103.8786 - lr: 5.0805e-08 - 62s/epoch - 319ms/step
Epoch 381/1000
2023-10-26 13:01:27.499 
Epoch 381/1000 
	 loss: 103.5905, MinusLogProbMetric: 103.5905, val_loss: 103.8562, val_MinusLogProbMetric: 103.8562

Epoch 381: val_loss improved from 103.86979 to 103.85622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 71s - loss: 103.5905 - MinusLogProbMetric: 103.5905 - val_loss: 103.8562 - val_MinusLogProbMetric: 103.8562 - lr: 5.0805e-08 - 71s/epoch - 360ms/step
Epoch 382/1000
2023-10-26 13:02:39.829 
Epoch 382/1000 
	 loss: 103.5783, MinusLogProbMetric: 103.5783, val_loss: 103.8389, val_MinusLogProbMetric: 103.8389

Epoch 382: val_loss improved from 103.85622 to 103.83891, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 72s - loss: 103.5783 - MinusLogProbMetric: 103.5783 - val_loss: 103.8389 - val_MinusLogProbMetric: 103.8389 - lr: 5.0805e-08 - 72s/epoch - 367ms/step
Epoch 383/1000
2023-10-26 13:03:43.518 
Epoch 383/1000 
	 loss: 103.5734, MinusLogProbMetric: 103.5734, val_loss: 103.8619, val_MinusLogProbMetric: 103.8619

Epoch 383: val_loss did not improve from 103.83891
196/196 - 63s - loss: 103.5734 - MinusLogProbMetric: 103.5734 - val_loss: 103.8619 - val_MinusLogProbMetric: 103.8619 - lr: 5.0805e-08 - 63s/epoch - 319ms/step
Epoch 384/1000
2023-10-26 13:04:57.495 
Epoch 384/1000 
	 loss: 103.5770, MinusLogProbMetric: 103.5770, val_loss: 103.8688, val_MinusLogProbMetric: 103.8688

Epoch 384: val_loss did not improve from 103.83891
196/196 - 74s - loss: 103.5770 - MinusLogProbMetric: 103.5770 - val_loss: 103.8688 - val_MinusLogProbMetric: 103.8688 - lr: 5.0805e-08 - 74s/epoch - 377ms/step
Epoch 385/1000
2023-10-26 13:06:05.072 
Epoch 385/1000 
	 loss: 103.5737, MinusLogProbMetric: 103.5737, val_loss: 103.8519, val_MinusLogProbMetric: 103.8519

Epoch 385: val_loss did not improve from 103.83891
196/196 - 68s - loss: 103.5737 - MinusLogProbMetric: 103.5737 - val_loss: 103.8519 - val_MinusLogProbMetric: 103.8519 - lr: 5.0805e-08 - 68s/epoch - 345ms/step
Epoch 386/1000
2023-10-26 13:07:10.465 
Epoch 386/1000 
	 loss: 103.5730, MinusLogProbMetric: 103.5730, val_loss: 103.8598, val_MinusLogProbMetric: 103.8598

Epoch 386: val_loss did not improve from 103.83891
196/196 - 65s - loss: 103.5730 - MinusLogProbMetric: 103.5730 - val_loss: 103.8598 - val_MinusLogProbMetric: 103.8598 - lr: 5.0805e-08 - 65s/epoch - 334ms/step
Epoch 387/1000
2023-10-26 13:08:26.404 
Epoch 387/1000 
	 loss: 103.5736, MinusLogProbMetric: 103.5736, val_loss: 103.8449, val_MinusLogProbMetric: 103.8449

Epoch 387: val_loss did not improve from 103.83891
196/196 - 76s - loss: 103.5736 - MinusLogProbMetric: 103.5736 - val_loss: 103.8449 - val_MinusLogProbMetric: 103.8449 - lr: 5.0805e-08 - 76s/epoch - 387ms/step
Epoch 388/1000
2023-10-26 13:09:29.820 
Epoch 388/1000 
	 loss: 103.5621, MinusLogProbMetric: 103.5621, val_loss: 103.8652, val_MinusLogProbMetric: 103.8652

Epoch 388: val_loss did not improve from 103.83891
196/196 - 63s - loss: 103.5621 - MinusLogProbMetric: 103.5621 - val_loss: 103.8652 - val_MinusLogProbMetric: 103.8652 - lr: 5.0805e-08 - 63s/epoch - 324ms/step
Epoch 389/1000
2023-10-26 13:10:37.761 
Epoch 389/1000 
	 loss: 103.5658, MinusLogProbMetric: 103.5658, val_loss: 103.8519, val_MinusLogProbMetric: 103.8519

Epoch 389: val_loss did not improve from 103.83891
196/196 - 68s - loss: 103.5658 - MinusLogProbMetric: 103.5658 - val_loss: 103.8519 - val_MinusLogProbMetric: 103.8519 - lr: 5.0805e-08 - 68s/epoch - 347ms/step
Epoch 390/1000
2023-10-26 13:11:42.322 
Epoch 390/1000 
	 loss: 103.5538, MinusLogProbMetric: 103.5538, val_loss: 103.8366, val_MinusLogProbMetric: 103.8366

Epoch 390: val_loss improved from 103.83891 to 103.83665, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 66s - loss: 103.5538 - MinusLogProbMetric: 103.5538 - val_loss: 103.8366 - val_MinusLogProbMetric: 103.8366 - lr: 5.0805e-08 - 66s/epoch - 335ms/step
Epoch 391/1000
2023-10-26 13:12:45.896 
Epoch 391/1000 
	 loss: 103.5538, MinusLogProbMetric: 103.5538, val_loss: 103.8433, val_MinusLogProbMetric: 103.8433

Epoch 391: val_loss did not improve from 103.83665
196/196 - 62s - loss: 103.5538 - MinusLogProbMetric: 103.5538 - val_loss: 103.8433 - val_MinusLogProbMetric: 103.8433 - lr: 5.0805e-08 - 62s/epoch - 319ms/step
Epoch 392/1000
2023-10-26 13:14:01.647 
Epoch 392/1000 
	 loss: 103.5543, MinusLogProbMetric: 103.5543, val_loss: 103.8284, val_MinusLogProbMetric: 103.8284

Epoch 392: val_loss improved from 103.83665 to 103.82842, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 77s - loss: 103.5543 - MinusLogProbMetric: 103.5543 - val_loss: 103.8284 - val_MinusLogProbMetric: 103.8284 - lr: 5.0805e-08 - 77s/epoch - 392ms/step
Epoch 393/1000
2023-10-26 13:15:05.969 
Epoch 393/1000 
	 loss: 103.5380, MinusLogProbMetric: 103.5380, val_loss: 103.8266, val_MinusLogProbMetric: 103.8266

Epoch 393: val_loss improved from 103.82842 to 103.82660, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 64s - loss: 103.5380 - MinusLogProbMetric: 103.5380 - val_loss: 103.8266 - val_MinusLogProbMetric: 103.8266 - lr: 5.0805e-08 - 64s/epoch - 328ms/step
Epoch 394/1000
2023-10-26 13:16:16.680 
Epoch 394/1000 
	 loss: 103.5448, MinusLogProbMetric: 103.5448, val_loss: 103.8374, val_MinusLogProbMetric: 103.8374

Epoch 394: val_loss did not improve from 103.82660
196/196 - 70s - loss: 103.5448 - MinusLogProbMetric: 103.5448 - val_loss: 103.8374 - val_MinusLogProbMetric: 103.8374 - lr: 5.0805e-08 - 70s/epoch - 355ms/step
Epoch 395/1000
2023-10-26 13:17:27.258 
Epoch 395/1000 
	 loss: 103.5363, MinusLogProbMetric: 103.5363, val_loss: 103.8381, val_MinusLogProbMetric: 103.8381

Epoch 395: val_loss did not improve from 103.82660
196/196 - 71s - loss: 103.5363 - MinusLogProbMetric: 103.5363 - val_loss: 103.8381 - val_MinusLogProbMetric: 103.8381 - lr: 5.0805e-08 - 71s/epoch - 360ms/step
Epoch 396/1000
2023-10-26 13:18:30.168 
Epoch 396/1000 
	 loss: 103.5405, MinusLogProbMetric: 103.5405, val_loss: 103.8192, val_MinusLogProbMetric: 103.8192

Epoch 396: val_loss improved from 103.82660 to 103.81918, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 64s - loss: 103.5405 - MinusLogProbMetric: 103.5405 - val_loss: 103.8192 - val_MinusLogProbMetric: 103.8192 - lr: 5.0805e-08 - 64s/epoch - 327ms/step
Epoch 397/1000
2023-10-26 13:19:46.605 
Epoch 397/1000 
	 loss: 103.5421, MinusLogProbMetric: 103.5421, val_loss: 103.7952, val_MinusLogProbMetric: 103.7952

Epoch 397: val_loss improved from 103.81918 to 103.79522, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 77s - loss: 103.5421 - MinusLogProbMetric: 103.5421 - val_loss: 103.7952 - val_MinusLogProbMetric: 103.7952 - lr: 5.0805e-08 - 77s/epoch - 392ms/step
Epoch 398/1000
2023-10-26 13:20:52.701 
Epoch 398/1000 
	 loss: 103.5309, MinusLogProbMetric: 103.5309, val_loss: 103.7931, val_MinusLogProbMetric: 103.7931

Epoch 398: val_loss improved from 103.79522 to 103.79314, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 66s - loss: 103.5309 - MinusLogProbMetric: 103.5309 - val_loss: 103.7931 - val_MinusLogProbMetric: 103.7931 - lr: 5.0805e-08 - 66s/epoch - 336ms/step
Epoch 399/1000
2023-10-26 13:22:00.745 
Epoch 399/1000 
	 loss: 103.5284, MinusLogProbMetric: 103.5284, val_loss: 103.7911, val_MinusLogProbMetric: 103.7911

Epoch 399: val_loss improved from 103.79314 to 103.79110, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 68s - loss: 103.5284 - MinusLogProbMetric: 103.5284 - val_loss: 103.7911 - val_MinusLogProbMetric: 103.7911 - lr: 5.0805e-08 - 68s/epoch - 347ms/step
Epoch 400/1000
2023-10-26 13:23:15.465 
Epoch 400/1000 
	 loss: 103.5315, MinusLogProbMetric: 103.5315, val_loss: 103.7911, val_MinusLogProbMetric: 103.7911

Epoch 400: val_loss improved from 103.79110 to 103.79109, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 74s - loss: 103.5315 - MinusLogProbMetric: 103.5315 - val_loss: 103.7911 - val_MinusLogProbMetric: 103.7911 - lr: 5.0805e-08 - 74s/epoch - 379ms/step
Epoch 401/1000
2023-10-26 13:24:18.333 
Epoch 401/1000 
	 loss: 103.5307, MinusLogProbMetric: 103.5307, val_loss: 103.8191, val_MinusLogProbMetric: 103.8191

Epoch 401: val_loss did not improve from 103.79109
196/196 - 62s - loss: 103.5307 - MinusLogProbMetric: 103.5307 - val_loss: 103.8191 - val_MinusLogProbMetric: 103.8191 - lr: 5.0805e-08 - 62s/epoch - 317ms/step
Epoch 402/1000
2023-10-26 13:25:27.973 
Epoch 402/1000 
	 loss: 103.5206, MinusLogProbMetric: 103.5206, val_loss: 103.7930, val_MinusLogProbMetric: 103.7930

Epoch 402: val_loss did not improve from 103.79109
196/196 - 70s - loss: 103.5206 - MinusLogProbMetric: 103.5206 - val_loss: 103.7930 - val_MinusLogProbMetric: 103.7930 - lr: 5.0805e-08 - 70s/epoch - 355ms/step
Epoch 403/1000
2023-10-26 13:26:36.676 
Epoch 403/1000 
	 loss: 103.5273, MinusLogProbMetric: 103.5273, val_loss: 103.8015, val_MinusLogProbMetric: 103.8015

Epoch 403: val_loss did not improve from 103.79109
196/196 - 69s - loss: 103.5273 - MinusLogProbMetric: 103.5273 - val_loss: 103.8015 - val_MinusLogProbMetric: 103.8015 - lr: 5.0805e-08 - 69s/epoch - 351ms/step
Epoch 404/1000
2023-10-26 13:27:39.130 
Epoch 404/1000 
	 loss: 103.5106, MinusLogProbMetric: 103.5106, val_loss: 103.8068, val_MinusLogProbMetric: 103.8068

Epoch 404: val_loss did not improve from 103.79109
196/196 - 62s - loss: 103.5106 - MinusLogProbMetric: 103.5106 - val_loss: 103.8068 - val_MinusLogProbMetric: 103.8068 - lr: 5.0805e-08 - 62s/epoch - 319ms/step
Epoch 405/1000
2023-10-26 13:28:54.287 
Epoch 405/1000 
	 loss: 103.5076, MinusLogProbMetric: 103.5076, val_loss: 103.8040, val_MinusLogProbMetric: 103.8040

Epoch 405: val_loss did not improve from 103.79109
196/196 - 75s - loss: 103.5076 - MinusLogProbMetric: 103.5076 - val_loss: 103.8040 - val_MinusLogProbMetric: 103.8040 - lr: 5.0805e-08 - 75s/epoch - 383ms/step
Epoch 406/1000
2023-10-26 13:29:59.899 
Epoch 406/1000 
	 loss: 103.5167, MinusLogProbMetric: 103.5167, val_loss: 103.7899, val_MinusLogProbMetric: 103.7899

Epoch 406: val_loss improved from 103.79109 to 103.78994, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 67s - loss: 103.5167 - MinusLogProbMetric: 103.5167 - val_loss: 103.7899 - val_MinusLogProbMetric: 103.7899 - lr: 5.0805e-08 - 67s/epoch - 341ms/step
Epoch 407/1000
2023-10-26 13:31:03.440 
Epoch 407/1000 
	 loss: 103.5067, MinusLogProbMetric: 103.5067, val_loss: 103.7924, val_MinusLogProbMetric: 103.7924

Epoch 407: val_loss did not improve from 103.78994
196/196 - 62s - loss: 103.5067 - MinusLogProbMetric: 103.5067 - val_loss: 103.7924 - val_MinusLogProbMetric: 103.7924 - lr: 5.0805e-08 - 62s/epoch - 318ms/step
Epoch 408/1000
2023-10-26 13:32:16.045 
Epoch 408/1000 
	 loss: 103.4957, MinusLogProbMetric: 103.4957, val_loss: 103.7622, val_MinusLogProbMetric: 103.7622

Epoch 408: val_loss improved from 103.78994 to 103.76219, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 74s - loss: 103.4957 - MinusLogProbMetric: 103.4957 - val_loss: 103.7622 - val_MinusLogProbMetric: 103.7622 - lr: 5.0805e-08 - 74s/epoch - 376ms/step
Epoch 409/1000
2023-10-26 13:33:20.752 
Epoch 409/1000 
	 loss: 103.4785, MinusLogProbMetric: 103.4785, val_loss: 103.7864, val_MinusLogProbMetric: 103.7864

Epoch 409: val_loss did not improve from 103.76219
196/196 - 64s - loss: 103.4785 - MinusLogProbMetric: 103.4785 - val_loss: 103.7864 - val_MinusLogProbMetric: 103.7864 - lr: 5.0805e-08 - 64s/epoch - 324ms/step
Epoch 410/1000
2023-10-26 13:34:26.157 
Epoch 410/1000 
	 loss: 103.4894, MinusLogProbMetric: 103.4894, val_loss: 103.7808, val_MinusLogProbMetric: 103.7808

Epoch 410: val_loss did not improve from 103.76219
196/196 - 65s - loss: 103.4894 - MinusLogProbMetric: 103.4894 - val_loss: 103.7808 - val_MinusLogProbMetric: 103.7808 - lr: 5.0805e-08 - 65s/epoch - 334ms/step
Epoch 411/1000
2023-10-26 13:35:37.226 
Epoch 411/1000 
	 loss: 103.4829, MinusLogProbMetric: 103.4829, val_loss: 103.7922, val_MinusLogProbMetric: 103.7922

Epoch 411: val_loss did not improve from 103.76219
196/196 - 71s - loss: 103.4829 - MinusLogProbMetric: 103.4829 - val_loss: 103.7922 - val_MinusLogProbMetric: 103.7922 - lr: 5.0805e-08 - 71s/epoch - 363ms/step
Epoch 412/1000
2023-10-26 13:36:39.701 
Epoch 412/1000 
	 loss: 103.4905, MinusLogProbMetric: 103.4905, val_loss: 103.7582, val_MinusLogProbMetric: 103.7582

Epoch 412: val_loss improved from 103.76219 to 103.75819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 64s - loss: 103.4905 - MinusLogProbMetric: 103.4905 - val_loss: 103.7582 - val_MinusLogProbMetric: 103.7582 - lr: 5.0805e-08 - 64s/epoch - 325ms/step
Epoch 413/1000
2023-10-26 13:37:53.798 
Epoch 413/1000 
	 loss: 103.5339, MinusLogProbMetric: 103.5339, val_loss: 103.8241, val_MinusLogProbMetric: 103.8241

Epoch 413: val_loss did not improve from 103.75819
196/196 - 73s - loss: 103.5339 - MinusLogProbMetric: 103.5339 - val_loss: 103.8241 - val_MinusLogProbMetric: 103.8241 - lr: 5.0805e-08 - 73s/epoch - 372ms/step
Epoch 414/1000
2023-10-26 13:39:03.872 
Epoch 414/1000 
	 loss: 103.5351, MinusLogProbMetric: 103.5351, val_loss: 103.8187, val_MinusLogProbMetric: 103.8187

Epoch 414: val_loss did not improve from 103.75819
196/196 - 70s - loss: 103.5351 - MinusLogProbMetric: 103.5351 - val_loss: 103.8187 - val_MinusLogProbMetric: 103.8187 - lr: 5.0805e-08 - 70s/epoch - 358ms/step
Epoch 415/1000
2023-10-26 13:40:09.593 
Epoch 415/1000 
	 loss: 103.5393, MinusLogProbMetric: 103.5393, val_loss: 103.8121, val_MinusLogProbMetric: 103.8121

Epoch 415: val_loss did not improve from 103.75819
196/196 - 66s - loss: 103.5393 - MinusLogProbMetric: 103.5393 - val_loss: 103.8121 - val_MinusLogProbMetric: 103.8121 - lr: 5.0805e-08 - 66s/epoch - 335ms/step
Epoch 416/1000
2023-10-26 13:41:24.125 
Epoch 416/1000 
	 loss: 103.5370, MinusLogProbMetric: 103.5370, val_loss: 103.8086, val_MinusLogProbMetric: 103.8086

Epoch 416: val_loss did not improve from 103.75819
196/196 - 75s - loss: 103.5370 - MinusLogProbMetric: 103.5370 - val_loss: 103.8086 - val_MinusLogProbMetric: 103.8086 - lr: 5.0805e-08 - 75s/epoch - 380ms/step
Epoch 417/1000
2023-10-26 13:42:33.495 
Epoch 417/1000 
	 loss: 103.5378, MinusLogProbMetric: 103.5378, val_loss: 103.8074, val_MinusLogProbMetric: 103.8074

Epoch 417: val_loss did not improve from 103.75819
196/196 - 69s - loss: 103.5378 - MinusLogProbMetric: 103.5378 - val_loss: 103.8074 - val_MinusLogProbMetric: 103.8074 - lr: 5.0805e-08 - 69s/epoch - 354ms/step
Epoch 418/1000
2023-10-26 13:43:40.675 
Epoch 418/1000 
	 loss: 103.5207, MinusLogProbMetric: 103.5207, val_loss: 103.7772, val_MinusLogProbMetric: 103.7772

Epoch 418: val_loss did not improve from 103.75819
196/196 - 67s - loss: 103.5207 - MinusLogProbMetric: 103.5207 - val_loss: 103.7772 - val_MinusLogProbMetric: 103.7772 - lr: 5.0805e-08 - 67s/epoch - 343ms/step
Epoch 419/1000
2023-10-26 13:44:55.087 
Epoch 419/1000 
	 loss: 103.5250, MinusLogProbMetric: 103.5250, val_loss: 103.7779, val_MinusLogProbMetric: 103.7779

Epoch 419: val_loss did not improve from 103.75819
196/196 - 74s - loss: 103.5250 - MinusLogProbMetric: 103.5250 - val_loss: 103.7779 - val_MinusLogProbMetric: 103.7779 - lr: 5.0805e-08 - 74s/epoch - 380ms/step
Epoch 420/1000
2023-10-26 13:46:00.707 
Epoch 420/1000 
	 loss: 103.5088, MinusLogProbMetric: 103.5088, val_loss: 103.7751, val_MinusLogProbMetric: 103.7751

Epoch 420: val_loss did not improve from 103.75819
196/196 - 66s - loss: 103.5088 - MinusLogProbMetric: 103.5088 - val_loss: 103.7751 - val_MinusLogProbMetric: 103.7751 - lr: 5.0805e-08 - 66s/epoch - 335ms/step
Epoch 421/1000
2023-10-26 13:47:07.619 
Epoch 421/1000 
	 loss: 103.5038, MinusLogProbMetric: 103.5038, val_loss: 103.7674, val_MinusLogProbMetric: 103.7674

Epoch 421: val_loss did not improve from 103.75819
196/196 - 67s - loss: 103.5038 - MinusLogProbMetric: 103.5038 - val_loss: 103.7674 - val_MinusLogProbMetric: 103.7674 - lr: 5.0805e-08 - 67s/epoch - 341ms/step
Epoch 422/1000
2023-10-26 13:48:26.047 
Epoch 422/1000 
	 loss: 103.5000, MinusLogProbMetric: 103.5000, val_loss: 103.7742, val_MinusLogProbMetric: 103.7742

Epoch 422: val_loss did not improve from 103.75819
196/196 - 78s - loss: 103.5000 - MinusLogProbMetric: 103.5000 - val_loss: 103.7742 - val_MinusLogProbMetric: 103.7742 - lr: 5.0805e-08 - 78s/epoch - 400ms/step
Epoch 423/1000
2023-10-26 13:49:31.048 
Epoch 423/1000 
	 loss: 103.4834, MinusLogProbMetric: 103.4834, val_loss: 103.7671, val_MinusLogProbMetric: 103.7671

Epoch 423: val_loss did not improve from 103.75819
196/196 - 65s - loss: 103.4834 - MinusLogProbMetric: 103.4834 - val_loss: 103.7671 - val_MinusLogProbMetric: 103.7671 - lr: 5.0805e-08 - 65s/epoch - 332ms/step
Epoch 424/1000
2023-10-26 13:50:37.332 
Epoch 424/1000 
	 loss: 103.4909, MinusLogProbMetric: 103.4909, val_loss: 103.7670, val_MinusLogProbMetric: 103.7670

Epoch 424: val_loss did not improve from 103.75819
196/196 - 66s - loss: 103.4909 - MinusLogProbMetric: 103.4909 - val_loss: 103.7670 - val_MinusLogProbMetric: 103.7670 - lr: 5.0805e-08 - 66s/epoch - 338ms/step
Epoch 425/1000
2023-10-26 13:51:55.155 
Epoch 425/1000 
	 loss: 103.4943, MinusLogProbMetric: 103.4943, val_loss: 103.7709, val_MinusLogProbMetric: 103.7709

Epoch 425: val_loss did not improve from 103.75819
196/196 - 78s - loss: 103.4943 - MinusLogProbMetric: 103.4943 - val_loss: 103.7709 - val_MinusLogProbMetric: 103.7709 - lr: 5.0805e-08 - 78s/epoch - 397ms/step
Epoch 426/1000
2023-10-26 13:52:58.065 
Epoch 426/1000 
	 loss: 103.4895, MinusLogProbMetric: 103.4895, val_loss: 103.7793, val_MinusLogProbMetric: 103.7793

Epoch 426: val_loss did not improve from 103.75819
196/196 - 63s - loss: 103.4895 - MinusLogProbMetric: 103.4895 - val_loss: 103.7793 - val_MinusLogProbMetric: 103.7793 - lr: 5.0805e-08 - 63s/epoch - 321ms/step
Epoch 427/1000
2023-10-26 13:54:03.240 
Epoch 427/1000 
	 loss: 103.4815, MinusLogProbMetric: 103.4815, val_loss: 103.7643, val_MinusLogProbMetric: 103.7643

Epoch 427: val_loss did not improve from 103.75819
196/196 - 65s - loss: 103.4815 - MinusLogProbMetric: 103.4815 - val_loss: 103.7643 - val_MinusLogProbMetric: 103.7643 - lr: 5.0805e-08 - 65s/epoch - 333ms/step
Epoch 428/1000
2023-10-26 13:55:19.108 
Epoch 428/1000 
	 loss: 103.4896, MinusLogProbMetric: 103.4896, val_loss: 103.7552, val_MinusLogProbMetric: 103.7552

Epoch 428: val_loss improved from 103.75819 to 103.75517, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 77s - loss: 103.4896 - MinusLogProbMetric: 103.4896 - val_loss: 103.7552 - val_MinusLogProbMetric: 103.7552 - lr: 5.0805e-08 - 77s/epoch - 393ms/step
Epoch 429/1000
2023-10-26 13:56:24.914 
Epoch 429/1000 
	 loss: 103.4694, MinusLogProbMetric: 103.4694, val_loss: 103.7621, val_MinusLogProbMetric: 103.7621

Epoch 429: val_loss did not improve from 103.75517
196/196 - 65s - loss: 103.4694 - MinusLogProbMetric: 103.4694 - val_loss: 103.7621 - val_MinusLogProbMetric: 103.7621 - lr: 5.0805e-08 - 65s/epoch - 330ms/step
Epoch 430/1000
2023-10-26 13:57:34.357 
Epoch 430/1000 
	 loss: 103.4909, MinusLogProbMetric: 103.4909, val_loss: 103.7634, val_MinusLogProbMetric: 103.7634

Epoch 430: val_loss did not improve from 103.75517
196/196 - 69s - loss: 103.4909 - MinusLogProbMetric: 103.4909 - val_loss: 103.7634 - val_MinusLogProbMetric: 103.7634 - lr: 5.0805e-08 - 69s/epoch - 354ms/step
Epoch 431/1000
2023-10-26 13:58:49.703 
Epoch 431/1000 
	 loss: 103.4924, MinusLogProbMetric: 103.4924, val_loss: 103.7550, val_MinusLogProbMetric: 103.7550

Epoch 431: val_loss improved from 103.75517 to 103.75495, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 77s - loss: 103.4924 - MinusLogProbMetric: 103.4924 - val_loss: 103.7550 - val_MinusLogProbMetric: 103.7550 - lr: 5.0805e-08 - 77s/epoch - 391ms/step
Epoch 432/1000
2023-10-26 14:00:01.371 
Epoch 432/1000 
	 loss: 103.4683, MinusLogProbMetric: 103.4683, val_loss: 103.7581, val_MinusLogProbMetric: 103.7581

Epoch 432: val_loss did not improve from 103.75495
196/196 - 70s - loss: 103.4683 - MinusLogProbMetric: 103.4683 - val_loss: 103.7581 - val_MinusLogProbMetric: 103.7581 - lr: 5.0805e-08 - 70s/epoch - 359ms/step
Epoch 433/1000
2023-10-26 14:01:11.094 
Epoch 433/1000 
	 loss: 103.4799, MinusLogProbMetric: 103.4799, val_loss: 103.7190, val_MinusLogProbMetric: 103.7190

Epoch 433: val_loss improved from 103.75495 to 103.71899, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 71s - loss: 103.4799 - MinusLogProbMetric: 103.4799 - val_loss: 103.7190 - val_MinusLogProbMetric: 103.7190 - lr: 5.0805e-08 - 71s/epoch - 362ms/step
Epoch 434/1000
2023-10-26 14:02:24.166 
Epoch 434/1000 
	 loss: 103.4652, MinusLogProbMetric: 103.4652, val_loss: 103.7519, val_MinusLogProbMetric: 103.7519

Epoch 434: val_loss did not improve from 103.71899
196/196 - 72s - loss: 103.4652 - MinusLogProbMetric: 103.4652 - val_loss: 103.7519 - val_MinusLogProbMetric: 103.7519 - lr: 5.0805e-08 - 72s/epoch - 366ms/step
Epoch 435/1000
2023-10-26 14:03:45.895 
Epoch 435/1000 
	 loss: 103.4732, MinusLogProbMetric: 103.4732, val_loss: 103.7496, val_MinusLogProbMetric: 103.7496

Epoch 435: val_loss did not improve from 103.71899
196/196 - 82s - loss: 103.4732 - MinusLogProbMetric: 103.4732 - val_loss: 103.7496 - val_MinusLogProbMetric: 103.7496 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 436/1000
2023-10-26 14:05:08.699 
Epoch 436/1000 
	 loss: 103.4587, MinusLogProbMetric: 103.4587, val_loss: 103.7347, val_MinusLogProbMetric: 103.7347

Epoch 436: val_loss did not improve from 103.71899
196/196 - 83s - loss: 103.4587 - MinusLogProbMetric: 103.4587 - val_loss: 103.7347 - val_MinusLogProbMetric: 103.7347 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 437/1000
2023-10-26 14:06:30.958 
Epoch 437/1000 
	 loss: 103.4615, MinusLogProbMetric: 103.4615, val_loss: 103.7387, val_MinusLogProbMetric: 103.7387

Epoch 437: val_loss did not improve from 103.71899
196/196 - 82s - loss: 103.4615 - MinusLogProbMetric: 103.4615 - val_loss: 103.7387 - val_MinusLogProbMetric: 103.7387 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 438/1000
2023-10-26 14:07:53.576 
Epoch 438/1000 
	 loss: 103.4663, MinusLogProbMetric: 103.4663, val_loss: 103.7383, val_MinusLogProbMetric: 103.7383

Epoch 438: val_loss did not improve from 103.71899
196/196 - 83s - loss: 103.4663 - MinusLogProbMetric: 103.4663 - val_loss: 103.7383 - val_MinusLogProbMetric: 103.7383 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 439/1000
2023-10-26 14:09:15.406 
Epoch 439/1000 
	 loss: 103.4557, MinusLogProbMetric: 103.4557, val_loss: 103.7404, val_MinusLogProbMetric: 103.7404

Epoch 439: val_loss did not improve from 103.71899
196/196 - 82s - loss: 103.4557 - MinusLogProbMetric: 103.4557 - val_loss: 103.7404 - val_MinusLogProbMetric: 103.7404 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 440/1000
2023-10-26 14:10:28.452 
Epoch 440/1000 
	 loss: 103.4443, MinusLogProbMetric: 103.4443, val_loss: 103.7362, val_MinusLogProbMetric: 103.7362

Epoch 440: val_loss did not improve from 103.71899
196/196 - 73s - loss: 103.4443 - MinusLogProbMetric: 103.4443 - val_loss: 103.7362 - val_MinusLogProbMetric: 103.7362 - lr: 5.0805e-08 - 73s/epoch - 373ms/step
Epoch 441/1000
2023-10-26 14:11:42.848 
Epoch 441/1000 
	 loss: 103.4380, MinusLogProbMetric: 103.4380, val_loss: 103.7244, val_MinusLogProbMetric: 103.7244

Epoch 441: val_loss did not improve from 103.71899
196/196 - 74s - loss: 103.4380 - MinusLogProbMetric: 103.4380 - val_loss: 103.7244 - val_MinusLogProbMetric: 103.7244 - lr: 5.0805e-08 - 74s/epoch - 380ms/step
Epoch 442/1000
2023-10-26 14:13:04.353 
Epoch 442/1000 
	 loss: 103.4465, MinusLogProbMetric: 103.4465, val_loss: 103.7196, val_MinusLogProbMetric: 103.7196

Epoch 442: val_loss did not improve from 103.71899
196/196 - 82s - loss: 103.4465 - MinusLogProbMetric: 103.4465 - val_loss: 103.7196 - val_MinusLogProbMetric: 103.7196 - lr: 5.0805e-08 - 82s/epoch - 416ms/step
Epoch 443/1000
2023-10-26 14:14:26.483 
Epoch 443/1000 
	 loss: 103.4479, MinusLogProbMetric: 103.4479, val_loss: 103.7300, val_MinusLogProbMetric: 103.7300

Epoch 443: val_loss did not improve from 103.71899
196/196 - 82s - loss: 103.4479 - MinusLogProbMetric: 103.4479 - val_loss: 103.7300 - val_MinusLogProbMetric: 103.7300 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 444/1000
2023-10-26 14:15:47.895 
Epoch 444/1000 
	 loss: 103.4468, MinusLogProbMetric: 103.4468, val_loss: 103.7349, val_MinusLogProbMetric: 103.7349

Epoch 444: val_loss did not improve from 103.71899
196/196 - 81s - loss: 103.4468 - MinusLogProbMetric: 103.4468 - val_loss: 103.7349 - val_MinusLogProbMetric: 103.7349 - lr: 5.0805e-08 - 81s/epoch - 415ms/step
Epoch 445/1000
2023-10-26 14:17:09.733 
Epoch 445/1000 
	 loss: 103.4380, MinusLogProbMetric: 103.4380, val_loss: 103.7104, val_MinusLogProbMetric: 103.7104

Epoch 445: val_loss improved from 103.71899 to 103.71044, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 103.4380 - MinusLogProbMetric: 103.4380 - val_loss: 103.7104 - val_MinusLogProbMetric: 103.7104 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 446/1000
2023-10-26 14:18:27.234 
Epoch 446/1000 
	 loss: 103.4401, MinusLogProbMetric: 103.4401, val_loss: 103.7135, val_MinusLogProbMetric: 103.7135

Epoch 446: val_loss did not improve from 103.71044
196/196 - 76s - loss: 103.4401 - MinusLogProbMetric: 103.4401 - val_loss: 103.7135 - val_MinusLogProbMetric: 103.7135 - lr: 5.0805e-08 - 76s/epoch - 388ms/step
Epoch 447/1000
2023-10-26 14:19:38.991 
Epoch 447/1000 
	 loss: 103.4471, MinusLogProbMetric: 103.4471, val_loss: 103.7210, val_MinusLogProbMetric: 103.7210

Epoch 447: val_loss did not improve from 103.71044
196/196 - 72s - loss: 103.4471 - MinusLogProbMetric: 103.4471 - val_loss: 103.7210 - val_MinusLogProbMetric: 103.7210 - lr: 5.0805e-08 - 72s/epoch - 366ms/step
Epoch 448/1000
2023-10-26 14:21:00.085 
Epoch 448/1000 
	 loss: 103.4289, MinusLogProbMetric: 103.4289, val_loss: 103.7301, val_MinusLogProbMetric: 103.7301

Epoch 448: val_loss did not improve from 103.71044
196/196 - 81s - loss: 103.4289 - MinusLogProbMetric: 103.4289 - val_loss: 103.7301 - val_MinusLogProbMetric: 103.7301 - lr: 5.0805e-08 - 81s/epoch - 414ms/step
Epoch 449/1000
2023-10-26 14:22:22.124 
Epoch 449/1000 
	 loss: 103.4182, MinusLogProbMetric: 103.4182, val_loss: 103.7091, val_MinusLogProbMetric: 103.7091

Epoch 449: val_loss improved from 103.71044 to 103.70910, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 103.4182 - MinusLogProbMetric: 103.4182 - val_loss: 103.7091 - val_MinusLogProbMetric: 103.7091 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 450/1000
2023-10-26 14:23:46.838 
Epoch 450/1000 
	 loss: 103.4149, MinusLogProbMetric: 103.4149, val_loss: 103.6807, val_MinusLogProbMetric: 103.6807

Epoch 450: val_loss improved from 103.70910 to 103.68066, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 103.4149 - MinusLogProbMetric: 103.4149 - val_loss: 103.6807 - val_MinusLogProbMetric: 103.6807 - lr: 5.0805e-08 - 85s/epoch - 432ms/step
Epoch 451/1000
2023-10-26 14:25:09.990 
Epoch 451/1000 
	 loss: 103.4033, MinusLogProbMetric: 103.4033, val_loss: 103.6612, val_MinusLogProbMetric: 103.6612

Epoch 451: val_loss improved from 103.68066 to 103.66124, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 103.4033 - MinusLogProbMetric: 103.4033 - val_loss: 103.6612 - val_MinusLogProbMetric: 103.6612 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 452/1000
2023-10-26 14:26:32.736 
Epoch 452/1000 
	 loss: 103.3736, MinusLogProbMetric: 103.3736, val_loss: 103.6566, val_MinusLogProbMetric: 103.6566

Epoch 452: val_loss improved from 103.66124 to 103.65661, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 103.3736 - MinusLogProbMetric: 103.3736 - val_loss: 103.6566 - val_MinusLogProbMetric: 103.6566 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 453/1000
2023-10-26 14:27:56.267 
Epoch 453/1000 
	 loss: 103.3585, MinusLogProbMetric: 103.3585, val_loss: 103.6523, val_MinusLogProbMetric: 103.6523

Epoch 453: val_loss improved from 103.65661 to 103.65231, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 103.3585 - MinusLogProbMetric: 103.3585 - val_loss: 103.6523 - val_MinusLogProbMetric: 103.6523 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 454/1000
2023-10-26 14:29:19.649 
Epoch 454/1000 
	 loss: 103.3766, MinusLogProbMetric: 103.3766, val_loss: 103.6723, val_MinusLogProbMetric: 103.6723

Epoch 454: val_loss did not improve from 103.65231
196/196 - 82s - loss: 103.3766 - MinusLogProbMetric: 103.3766 - val_loss: 103.6723 - val_MinusLogProbMetric: 103.6723 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 455/1000
2023-10-26 14:30:41.704 
Epoch 455/1000 
	 loss: 103.3672, MinusLogProbMetric: 103.3672, val_loss: 103.6378, val_MinusLogProbMetric: 103.6378

Epoch 455: val_loss improved from 103.65231 to 103.63779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 103.3672 - MinusLogProbMetric: 103.3672 - val_loss: 103.6378 - val_MinusLogProbMetric: 103.6378 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 456/1000
2023-10-26 14:32:05.213 
Epoch 456/1000 
	 loss: 103.3493, MinusLogProbMetric: 103.3493, val_loss: 103.6492, val_MinusLogProbMetric: 103.6492

Epoch 456: val_loss did not improve from 103.63779
196/196 - 82s - loss: 103.3493 - MinusLogProbMetric: 103.3493 - val_loss: 103.6492 - val_MinusLogProbMetric: 103.6492 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 457/1000
2023-10-26 14:33:26.952 
Epoch 457/1000 
	 loss: 103.3617, MinusLogProbMetric: 103.3617, val_loss: 103.6598, val_MinusLogProbMetric: 103.6598

Epoch 457: val_loss did not improve from 103.63779
196/196 - 82s - loss: 103.3617 - MinusLogProbMetric: 103.3617 - val_loss: 103.6598 - val_MinusLogProbMetric: 103.6598 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 458/1000
2023-10-26 14:34:49.728 
Epoch 458/1000 
	 loss: 103.3548, MinusLogProbMetric: 103.3548, val_loss: 103.6577, val_MinusLogProbMetric: 103.6577

Epoch 458: val_loss did not improve from 103.63779
196/196 - 83s - loss: 103.3548 - MinusLogProbMetric: 103.3548 - val_loss: 103.6577 - val_MinusLogProbMetric: 103.6577 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 459/1000
2023-10-26 14:36:09.492 
Epoch 459/1000 
	 loss: 103.3602, MinusLogProbMetric: 103.3602, val_loss: 103.6433, val_MinusLogProbMetric: 103.6433

Epoch 459: val_loss did not improve from 103.63779
196/196 - 80s - loss: 103.3602 - MinusLogProbMetric: 103.3602 - val_loss: 103.6433 - val_MinusLogProbMetric: 103.6433 - lr: 5.0805e-08 - 80s/epoch - 407ms/step
Epoch 460/1000
2023-10-26 14:37:30.234 
Epoch 460/1000 
	 loss: 103.3407, MinusLogProbMetric: 103.3407, val_loss: 103.6506, val_MinusLogProbMetric: 103.6506

Epoch 460: val_loss did not improve from 103.63779
196/196 - 81s - loss: 103.3407 - MinusLogProbMetric: 103.3407 - val_loss: 103.6506 - val_MinusLogProbMetric: 103.6506 - lr: 5.0805e-08 - 81s/epoch - 412ms/step
Epoch 461/1000
2023-10-26 14:38:51.136 
Epoch 461/1000 
	 loss: 103.3490, MinusLogProbMetric: 103.3490, val_loss: 103.6617, val_MinusLogProbMetric: 103.6617

Epoch 461: val_loss did not improve from 103.63779
196/196 - 81s - loss: 103.3490 - MinusLogProbMetric: 103.3490 - val_loss: 103.6617 - val_MinusLogProbMetric: 103.6617 - lr: 5.0805e-08 - 81s/epoch - 413ms/step
Epoch 462/1000
2023-10-26 14:40:12.215 
Epoch 462/1000 
	 loss: 103.3425, MinusLogProbMetric: 103.3425, val_loss: 103.6364, val_MinusLogProbMetric: 103.6364

Epoch 462: val_loss improved from 103.63779 to 103.63638, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 103.3425 - MinusLogProbMetric: 103.3425 - val_loss: 103.6364 - val_MinusLogProbMetric: 103.6364 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 463/1000
2023-10-26 14:41:34.194 
Epoch 463/1000 
	 loss: 103.3535, MinusLogProbMetric: 103.3535, val_loss: 103.6205, val_MinusLogProbMetric: 103.6205

Epoch 463: val_loss improved from 103.63638 to 103.62048, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 82s - loss: 103.3535 - MinusLogProbMetric: 103.3535 - val_loss: 103.6205 - val_MinusLogProbMetric: 103.6205 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 464/1000
2023-10-26 14:42:57.705 
Epoch 464/1000 
	 loss: 103.3364, MinusLogProbMetric: 103.3364, val_loss: 103.6227, val_MinusLogProbMetric: 103.6227

Epoch 464: val_loss did not improve from 103.62048
196/196 - 82s - loss: 103.3364 - MinusLogProbMetric: 103.3364 - val_loss: 103.6227 - val_MinusLogProbMetric: 103.6227 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 465/1000
2023-10-26 14:44:17.684 
Epoch 465/1000 
	 loss: 103.3327, MinusLogProbMetric: 103.3327, val_loss: 103.6165, val_MinusLogProbMetric: 103.6165

Epoch 465: val_loss improved from 103.62048 to 103.61650, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 81s - loss: 103.3327 - MinusLogProbMetric: 103.3327 - val_loss: 103.6165 - val_MinusLogProbMetric: 103.6165 - lr: 5.0805e-08 - 81s/epoch - 416ms/step
Epoch 466/1000
2023-10-26 14:45:41.336 
Epoch 466/1000 
	 loss: 103.3229, MinusLogProbMetric: 103.3229, val_loss: 103.6239, val_MinusLogProbMetric: 103.6239

Epoch 466: val_loss did not improve from 103.61650
196/196 - 82s - loss: 103.3229 - MinusLogProbMetric: 103.3229 - val_loss: 103.6239 - val_MinusLogProbMetric: 103.6239 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 467/1000
2023-10-26 14:47:02.801 
Epoch 467/1000 
	 loss: 103.3154, MinusLogProbMetric: 103.3154, val_loss: 103.6148, val_MinusLogProbMetric: 103.6148

Epoch 467: val_loss improved from 103.61650 to 103.61475, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 103.3154 - MinusLogProbMetric: 103.3154 - val_loss: 103.6148 - val_MinusLogProbMetric: 103.6148 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 468/1000
2023-10-26 14:48:27.314 
Epoch 468/1000 
	 loss: 103.3212, MinusLogProbMetric: 103.3212, val_loss: 103.6048, val_MinusLogProbMetric: 103.6048

Epoch 468: val_loss improved from 103.61475 to 103.60479, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 103.3212 - MinusLogProbMetric: 103.3212 - val_loss: 103.6048 - val_MinusLogProbMetric: 103.6048 - lr: 5.0805e-08 - 84s/epoch - 431ms/step
Epoch 469/1000
2023-10-26 14:49:51.076 
Epoch 469/1000 
	 loss: 103.3120, MinusLogProbMetric: 103.3120, val_loss: 103.6086, val_MinusLogProbMetric: 103.6086

Epoch 469: val_loss did not improve from 103.60479
196/196 - 82s - loss: 103.3120 - MinusLogProbMetric: 103.3120 - val_loss: 103.6086 - val_MinusLogProbMetric: 103.6086 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 470/1000
2023-10-26 14:51:13.480 
Epoch 470/1000 
	 loss: 103.3223, MinusLogProbMetric: 103.3223, val_loss: 103.6233, val_MinusLogProbMetric: 103.6233

Epoch 470: val_loss did not improve from 103.60479
196/196 - 82s - loss: 103.3223 - MinusLogProbMetric: 103.3223 - val_loss: 103.6233 - val_MinusLogProbMetric: 103.6233 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 471/1000
2023-10-26 14:52:35.514 
Epoch 471/1000 
	 loss: 103.3155, MinusLogProbMetric: 103.3155, val_loss: 103.5968, val_MinusLogProbMetric: 103.5968

Epoch 471: val_loss improved from 103.60479 to 103.59680, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 103.3155 - MinusLogProbMetric: 103.3155 - val_loss: 103.5968 - val_MinusLogProbMetric: 103.5968 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 472/1000
2023-10-26 14:53:59.672 
Epoch 472/1000 
	 loss: 103.3065, MinusLogProbMetric: 103.3065, val_loss: 103.6077, val_MinusLogProbMetric: 103.6077

Epoch 472: val_loss did not improve from 103.59680
196/196 - 82s - loss: 103.3065 - MinusLogProbMetric: 103.3065 - val_loss: 103.6077 - val_MinusLogProbMetric: 103.6077 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 473/1000
2023-10-26 14:55:20.934 
Epoch 473/1000 
	 loss: 103.2987, MinusLogProbMetric: 103.2987, val_loss: 103.5898, val_MinusLogProbMetric: 103.5898

Epoch 473: val_loss improved from 103.59680 to 103.58976, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 103.2987 - MinusLogProbMetric: 103.2987 - val_loss: 103.5898 - val_MinusLogProbMetric: 103.5898 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 474/1000
2023-10-26 14:56:45.431 
Epoch 474/1000 
	 loss: 103.3020, MinusLogProbMetric: 103.3020, val_loss: 103.5904, val_MinusLogProbMetric: 103.5904

Epoch 474: val_loss did not improve from 103.58976
196/196 - 83s - loss: 103.3020 - MinusLogProbMetric: 103.3020 - val_loss: 103.5904 - val_MinusLogProbMetric: 103.5904 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 475/1000
2023-10-26 14:58:06.953 
Epoch 475/1000 
	 loss: 103.6588, MinusLogProbMetric: 103.6588, val_loss: 104.1833, val_MinusLogProbMetric: 104.1833

Epoch 475: val_loss did not improve from 103.58976
196/196 - 82s - loss: 103.6588 - MinusLogProbMetric: 103.6588 - val_loss: 104.1833 - val_MinusLogProbMetric: 104.1833 - lr: 5.0805e-08 - 82s/epoch - 416ms/step
Epoch 476/1000
2023-10-26 14:59:29.191 
Epoch 476/1000 
	 loss: 103.8069, MinusLogProbMetric: 103.8069, val_loss: 104.0027, val_MinusLogProbMetric: 104.0027

Epoch 476: val_loss did not improve from 103.58976
196/196 - 82s - loss: 103.8069 - MinusLogProbMetric: 103.8069 - val_loss: 104.0027 - val_MinusLogProbMetric: 104.0027 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 477/1000
2023-10-26 15:00:51.373 
Epoch 477/1000 
	 loss: 103.7159, MinusLogProbMetric: 103.7159, val_loss: 103.9270, val_MinusLogProbMetric: 103.9270

Epoch 477: val_loss did not improve from 103.58976
196/196 - 82s - loss: 103.7159 - MinusLogProbMetric: 103.7159 - val_loss: 103.9270 - val_MinusLogProbMetric: 103.9270 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 478/1000
2023-10-26 15:02:06.651 
Epoch 478/1000 
	 loss: 103.6513, MinusLogProbMetric: 103.6513, val_loss: 103.8788, val_MinusLogProbMetric: 103.8788

Epoch 478: val_loss did not improve from 103.58976
196/196 - 75s - loss: 103.6513 - MinusLogProbMetric: 103.6513 - val_loss: 103.8788 - val_MinusLogProbMetric: 103.8788 - lr: 5.0805e-08 - 75s/epoch - 384ms/step
Epoch 479/1000
2023-10-26 15:03:18.537 
Epoch 479/1000 
	 loss: 103.6189, MinusLogProbMetric: 103.6189, val_loss: 103.8574, val_MinusLogProbMetric: 103.8574

Epoch 479: val_loss did not improve from 103.58976
196/196 - 72s - loss: 103.6189 - MinusLogProbMetric: 103.6189 - val_loss: 103.8574 - val_MinusLogProbMetric: 103.8574 - lr: 5.0805e-08 - 72s/epoch - 367ms/step
Epoch 480/1000
2023-10-26 15:04:39.764 
Epoch 480/1000 
	 loss: 103.5842, MinusLogProbMetric: 103.5842, val_loss: 103.8108, val_MinusLogProbMetric: 103.8108

Epoch 480: val_loss did not improve from 103.58976
196/196 - 81s - loss: 103.5842 - MinusLogProbMetric: 103.5842 - val_loss: 103.8108 - val_MinusLogProbMetric: 103.8108 - lr: 5.0805e-08 - 81s/epoch - 414ms/step
Epoch 481/1000
2023-10-26 15:06:01.536 
Epoch 481/1000 
	 loss: 103.5544, MinusLogProbMetric: 103.5544, val_loss: 103.7828, val_MinusLogProbMetric: 103.7828

Epoch 481: val_loss did not improve from 103.58976
196/196 - 82s - loss: 103.5544 - MinusLogProbMetric: 103.5544 - val_loss: 103.7828 - val_MinusLogProbMetric: 103.7828 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 482/1000
2023-10-26 15:07:23.183 
Epoch 482/1000 
	 loss: 103.5333, MinusLogProbMetric: 103.5333, val_loss: 103.7838, val_MinusLogProbMetric: 103.7838

Epoch 482: val_loss did not improve from 103.58976
196/196 - 82s - loss: 103.5333 - MinusLogProbMetric: 103.5333 - val_loss: 103.7838 - val_MinusLogProbMetric: 103.7838 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 483/1000
2023-10-26 15:08:45.985 
Epoch 483/1000 
	 loss: 103.5273, MinusLogProbMetric: 103.5273, val_loss: 103.7589, val_MinusLogProbMetric: 103.7589

Epoch 483: val_loss did not improve from 103.58976
196/196 - 83s - loss: 103.5273 - MinusLogProbMetric: 103.5273 - val_loss: 103.7589 - val_MinusLogProbMetric: 103.7589 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 484/1000
2023-10-26 15:09:54.650 
Epoch 484/1000 
	 loss: 103.5075, MinusLogProbMetric: 103.5075, val_loss: 103.7497, val_MinusLogProbMetric: 103.7497

Epoch 484: val_loss did not improve from 103.58976
196/196 - 69s - loss: 103.5075 - MinusLogProbMetric: 103.5075 - val_loss: 103.7497 - val_MinusLogProbMetric: 103.7497 - lr: 5.0805e-08 - 69s/epoch - 350ms/step
Epoch 485/1000
2023-10-26 15:11:16.265 
Epoch 485/1000 
	 loss: 103.5010, MinusLogProbMetric: 103.5010, val_loss: 103.7227, val_MinusLogProbMetric: 103.7227

Epoch 485: val_loss did not improve from 103.58976
196/196 - 82s - loss: 103.5010 - MinusLogProbMetric: 103.5010 - val_loss: 103.7227 - val_MinusLogProbMetric: 103.7227 - lr: 5.0805e-08 - 82s/epoch - 416ms/step
Epoch 486/1000
2023-10-26 15:12:38.044 
Epoch 486/1000 
	 loss: 103.4719, MinusLogProbMetric: 103.4719, val_loss: 103.7355, val_MinusLogProbMetric: 103.7355

Epoch 486: val_loss did not improve from 103.58976
196/196 - 82s - loss: 103.4719 - MinusLogProbMetric: 103.4719 - val_loss: 103.7355 - val_MinusLogProbMetric: 103.7355 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 487/1000
2023-10-26 15:13:59.273 
Epoch 487/1000 
	 loss: 103.4705, MinusLogProbMetric: 103.4705, val_loss: 103.7489, val_MinusLogProbMetric: 103.7489

Epoch 487: val_loss did not improve from 103.58976
196/196 - 81s - loss: 103.4705 - MinusLogProbMetric: 103.4705 - val_loss: 103.7489 - val_MinusLogProbMetric: 103.7489 - lr: 5.0805e-08 - 81s/epoch - 414ms/step
Epoch 488/1000
2023-10-26 15:15:19.322 
Epoch 488/1000 
	 loss: 103.4765, MinusLogProbMetric: 103.4765, val_loss: 103.7239, val_MinusLogProbMetric: 103.7239

Epoch 488: val_loss did not improve from 103.58976
196/196 - 80s - loss: 103.4765 - MinusLogProbMetric: 103.4765 - val_loss: 103.7239 - val_MinusLogProbMetric: 103.7239 - lr: 5.0805e-08 - 80s/epoch - 408ms/step
Epoch 489/1000
2023-10-26 15:16:40.182 
Epoch 489/1000 
	 loss: 103.4530, MinusLogProbMetric: 103.4530, val_loss: 103.7216, val_MinusLogProbMetric: 103.7216

Epoch 489: val_loss did not improve from 103.58976
196/196 - 81s - loss: 103.4530 - MinusLogProbMetric: 103.4530 - val_loss: 103.7216 - val_MinusLogProbMetric: 103.7216 - lr: 5.0805e-08 - 81s/epoch - 413ms/step
Epoch 490/1000
2023-10-26 15:18:01.751 
Epoch 490/1000 
	 loss: 103.4448, MinusLogProbMetric: 103.4448, val_loss: 103.6987, val_MinusLogProbMetric: 103.6987

Epoch 490: val_loss did not improve from 103.58976
196/196 - 82s - loss: 103.4448 - MinusLogProbMetric: 103.4448 - val_loss: 103.6987 - val_MinusLogProbMetric: 103.6987 - lr: 5.0805e-08 - 82s/epoch - 416ms/step
Epoch 491/1000
2023-10-26 15:19:22.473 
Epoch 491/1000 
	 loss: 103.4403, MinusLogProbMetric: 103.4403, val_loss: 103.7273, val_MinusLogProbMetric: 103.7273

Epoch 491: val_loss did not improve from 103.58976
196/196 - 81s - loss: 103.4403 - MinusLogProbMetric: 103.4403 - val_loss: 103.7273 - val_MinusLogProbMetric: 103.7273 - lr: 5.0805e-08 - 81s/epoch - 412ms/step
Epoch 492/1000
2023-10-26 15:20:43.463 
Epoch 492/1000 
	 loss: 103.4368, MinusLogProbMetric: 103.4368, val_loss: 103.6853, val_MinusLogProbMetric: 103.6853

Epoch 492: val_loss did not improve from 103.58976
196/196 - 81s - loss: 103.4368 - MinusLogProbMetric: 103.4368 - val_loss: 103.6853 - val_MinusLogProbMetric: 103.6853 - lr: 5.0805e-08 - 81s/epoch - 413ms/step
Epoch 493/1000
2023-10-26 15:22:04.218 
Epoch 493/1000 
	 loss: 103.4181, MinusLogProbMetric: 103.4181, val_loss: 103.6723, val_MinusLogProbMetric: 103.6723

Epoch 493: val_loss did not improve from 103.58976
196/196 - 81s - loss: 103.4181 - MinusLogProbMetric: 103.4181 - val_loss: 103.6723 - val_MinusLogProbMetric: 103.6723 - lr: 5.0805e-08 - 81s/epoch - 412ms/step
Epoch 494/1000
2023-10-26 15:23:25.218 
Epoch 494/1000 
	 loss: 103.4114, MinusLogProbMetric: 103.4114, val_loss: 103.6764, val_MinusLogProbMetric: 103.6764

Epoch 494: val_loss did not improve from 103.58976
196/196 - 81s - loss: 103.4114 - MinusLogProbMetric: 103.4114 - val_loss: 103.6764 - val_MinusLogProbMetric: 103.6764 - lr: 5.0805e-08 - 81s/epoch - 413ms/step
Epoch 495/1000
2023-10-26 15:24:45.876 
Epoch 495/1000 
	 loss: 103.4046, MinusLogProbMetric: 103.4046, val_loss: 103.6503, val_MinusLogProbMetric: 103.6503

Epoch 495: val_loss did not improve from 103.58976
196/196 - 81s - loss: 103.4046 - MinusLogProbMetric: 103.4046 - val_loss: 103.6503 - val_MinusLogProbMetric: 103.6503 - lr: 5.0805e-08 - 81s/epoch - 412ms/step
Epoch 496/1000
2023-10-26 15:26:07.186 
Epoch 496/1000 
	 loss: 103.3885, MinusLogProbMetric: 103.3885, val_loss: 103.6499, val_MinusLogProbMetric: 103.6499

Epoch 496: val_loss did not improve from 103.58976
196/196 - 81s - loss: 103.3885 - MinusLogProbMetric: 103.3885 - val_loss: 103.6499 - val_MinusLogProbMetric: 103.6499 - lr: 5.0805e-08 - 81s/epoch - 415ms/step
Epoch 497/1000
2023-10-26 15:27:15.517 
Epoch 497/1000 
	 loss: 103.3848, MinusLogProbMetric: 103.3848, val_loss: 103.6573, val_MinusLogProbMetric: 103.6573

Epoch 497: val_loss did not improve from 103.58976
196/196 - 68s - loss: 103.3848 - MinusLogProbMetric: 103.3848 - val_loss: 103.6573 - val_MinusLogProbMetric: 103.6573 - lr: 5.0805e-08 - 68s/epoch - 349ms/step
Epoch 498/1000
2023-10-26 15:28:29.505 
Epoch 498/1000 
	 loss: 103.3843, MinusLogProbMetric: 103.3843, val_loss: 103.6520, val_MinusLogProbMetric: 103.6520

Epoch 498: val_loss did not improve from 103.58976
196/196 - 74s - loss: 103.3843 - MinusLogProbMetric: 103.3843 - val_loss: 103.6520 - val_MinusLogProbMetric: 103.6520 - lr: 5.0805e-08 - 74s/epoch - 377ms/step
Epoch 499/1000
2023-10-26 15:29:47.544 
Epoch 499/1000 
	 loss: 103.3899, MinusLogProbMetric: 103.3899, val_loss: 103.6313, val_MinusLogProbMetric: 103.6313

Epoch 499: val_loss did not improve from 103.58976
196/196 - 78s - loss: 103.3899 - MinusLogProbMetric: 103.3899 - val_loss: 103.6313 - val_MinusLogProbMetric: 103.6313 - lr: 5.0805e-08 - 78s/epoch - 398ms/step
Epoch 500/1000
2023-10-26 15:30:55.501 
Epoch 500/1000 
	 loss: 103.3884, MinusLogProbMetric: 103.3884, val_loss: 103.6246, val_MinusLogProbMetric: 103.6246

Epoch 500: val_loss did not improve from 103.58976
196/196 - 68s - loss: 103.3884 - MinusLogProbMetric: 103.3884 - val_loss: 103.6246 - val_MinusLogProbMetric: 103.6246 - lr: 5.0805e-08 - 68s/epoch - 347ms/step
Epoch 501/1000
2023-10-26 15:32:11.185 
Epoch 501/1000 
	 loss: 103.3749, MinusLogProbMetric: 103.3749, val_loss: 103.6263, val_MinusLogProbMetric: 103.6263

Epoch 501: val_loss did not improve from 103.58976
196/196 - 76s - loss: 103.3749 - MinusLogProbMetric: 103.3749 - val_loss: 103.6263 - val_MinusLogProbMetric: 103.6263 - lr: 5.0805e-08 - 76s/epoch - 386ms/step
Epoch 502/1000
2023-10-26 15:33:18.506 
Epoch 502/1000 
	 loss: 103.3682, MinusLogProbMetric: 103.3682, val_loss: 103.6119, val_MinusLogProbMetric: 103.6119

Epoch 502: val_loss did not improve from 103.58976
196/196 - 67s - loss: 103.3682 - MinusLogProbMetric: 103.3682 - val_loss: 103.6119 - val_MinusLogProbMetric: 103.6119 - lr: 5.0805e-08 - 67s/epoch - 343ms/step
Epoch 503/1000
2023-10-26 15:34:35.115 
Epoch 503/1000 
	 loss: 103.3642, MinusLogProbMetric: 103.3642, val_loss: 103.6053, val_MinusLogProbMetric: 103.6053

Epoch 503: val_loss did not improve from 103.58976
196/196 - 77s - loss: 103.3642 - MinusLogProbMetric: 103.3642 - val_loss: 103.6053 - val_MinusLogProbMetric: 103.6053 - lr: 5.0805e-08 - 77s/epoch - 391ms/step
Epoch 504/1000
2023-10-26 15:35:44.510 
Epoch 504/1000 
	 loss: 103.3522, MinusLogProbMetric: 103.3522, val_loss: 103.6134, val_MinusLogProbMetric: 103.6134

Epoch 504: val_loss did not improve from 103.58976
196/196 - 69s - loss: 103.3522 - MinusLogProbMetric: 103.3522 - val_loss: 103.6134 - val_MinusLogProbMetric: 103.6134 - lr: 5.0805e-08 - 69s/epoch - 354ms/step
Epoch 505/1000
2023-10-26 15:37:00.484 
Epoch 505/1000 
	 loss: 103.3527, MinusLogProbMetric: 103.3527, val_loss: 103.6293, val_MinusLogProbMetric: 103.6293

Epoch 505: val_loss did not improve from 103.58976
196/196 - 76s - loss: 103.3527 - MinusLogProbMetric: 103.3527 - val_loss: 103.6293 - val_MinusLogProbMetric: 103.6293 - lr: 5.0805e-08 - 76s/epoch - 388ms/step
Epoch 506/1000
2023-10-26 15:38:12.330 
Epoch 506/1000 
	 loss: 103.3547, MinusLogProbMetric: 103.3547, val_loss: 103.6078, val_MinusLogProbMetric: 103.6078

Epoch 506: val_loss did not improve from 103.58976
196/196 - 72s - loss: 103.3547 - MinusLogProbMetric: 103.3547 - val_loss: 103.6078 - val_MinusLogProbMetric: 103.6078 - lr: 5.0805e-08 - 72s/epoch - 367ms/step
Epoch 507/1000
2023-10-26 15:39:26.460 
Epoch 507/1000 
	 loss: 103.3619, MinusLogProbMetric: 103.3619, val_loss: 103.6334, val_MinusLogProbMetric: 103.6334

Epoch 507: val_loss did not improve from 103.58976
196/196 - 74s - loss: 103.3619 - MinusLogProbMetric: 103.3619 - val_loss: 103.6334 - val_MinusLogProbMetric: 103.6334 - lr: 5.0805e-08 - 74s/epoch - 378ms/step
Epoch 508/1000
2023-10-26 15:40:36.965 
Epoch 508/1000 
	 loss: 103.3570, MinusLogProbMetric: 103.3570, val_loss: 103.6470, val_MinusLogProbMetric: 103.6470

Epoch 508: val_loss did not improve from 103.58976
196/196 - 71s - loss: 103.3570 - MinusLogProbMetric: 103.3570 - val_loss: 103.6470 - val_MinusLogProbMetric: 103.6470 - lr: 5.0805e-08 - 71s/epoch - 360ms/step
Epoch 509/1000
2023-10-26 15:41:50.392 
Epoch 509/1000 
	 loss: 103.3696, MinusLogProbMetric: 103.3696, val_loss: 103.6208, val_MinusLogProbMetric: 103.6208

Epoch 509: val_loss did not improve from 103.58976
196/196 - 73s - loss: 103.3696 - MinusLogProbMetric: 103.3696 - val_loss: 103.6208 - val_MinusLogProbMetric: 103.6208 - lr: 5.0805e-08 - 73s/epoch - 375ms/step
Epoch 510/1000
2023-10-26 15:43:06.267 
Epoch 510/1000 
	 loss: 103.3486, MinusLogProbMetric: 103.3486, val_loss: 103.6317, val_MinusLogProbMetric: 103.6317

Epoch 510: val_loss did not improve from 103.58976
196/196 - 76s - loss: 103.3486 - MinusLogProbMetric: 103.3486 - val_loss: 103.6317 - val_MinusLogProbMetric: 103.6317 - lr: 5.0805e-08 - 76s/epoch - 387ms/step
Epoch 511/1000
2023-10-26 15:44:21.161 
Epoch 511/1000 
	 loss: 103.3586, MinusLogProbMetric: 103.3586, val_loss: 103.6193, val_MinusLogProbMetric: 103.6193

Epoch 511: val_loss did not improve from 103.58976
196/196 - 75s - loss: 103.3586 - MinusLogProbMetric: 103.3586 - val_loss: 103.6193 - val_MinusLogProbMetric: 103.6193 - lr: 5.0805e-08 - 75s/epoch - 382ms/step
Epoch 512/1000
2023-10-26 15:45:33.513 
Epoch 512/1000 
	 loss: 103.3488, MinusLogProbMetric: 103.3488, val_loss: 103.6542, val_MinusLogProbMetric: 103.6542

Epoch 512: val_loss did not improve from 103.58976
196/196 - 72s - loss: 103.3488 - MinusLogProbMetric: 103.3488 - val_loss: 103.6542 - val_MinusLogProbMetric: 103.6542 - lr: 5.0805e-08 - 72s/epoch - 369ms/step
Epoch 513/1000
2023-10-26 15:46:46.401 
Epoch 513/1000 
	 loss: 103.3464, MinusLogProbMetric: 103.3464, val_loss: 103.6247, val_MinusLogProbMetric: 103.6247

Epoch 513: val_loss did not improve from 103.58976
196/196 - 73s - loss: 103.3464 - MinusLogProbMetric: 103.3464 - val_loss: 103.6247 - val_MinusLogProbMetric: 103.6247 - lr: 5.0805e-08 - 73s/epoch - 372ms/step
Epoch 514/1000
2023-10-26 15:47:58.440 
Epoch 514/1000 
	 loss: 103.3450, MinusLogProbMetric: 103.3450, val_loss: 103.5997, val_MinusLogProbMetric: 103.5997

Epoch 514: val_loss did not improve from 103.58976
196/196 - 72s - loss: 103.3450 - MinusLogProbMetric: 103.3450 - val_loss: 103.5997 - val_MinusLogProbMetric: 103.5997 - lr: 5.0805e-08 - 72s/epoch - 368ms/step
Epoch 515/1000
2023-10-26 15:49:14.034 
Epoch 515/1000 
	 loss: 103.3361, MinusLogProbMetric: 103.3361, val_loss: 103.5895, val_MinusLogProbMetric: 103.5895

Epoch 515: val_loss improved from 103.58976 to 103.58949, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 77s - loss: 103.3361 - MinusLogProbMetric: 103.3361 - val_loss: 103.5895 - val_MinusLogProbMetric: 103.5895 - lr: 5.0805e-08 - 77s/epoch - 393ms/step
Epoch 516/1000
2023-10-26 15:50:30.180 
Epoch 516/1000 
	 loss: 103.3445, MinusLogProbMetric: 103.3445, val_loss: 103.6044, val_MinusLogProbMetric: 103.6044

Epoch 516: val_loss did not improve from 103.58949
196/196 - 75s - loss: 103.3445 - MinusLogProbMetric: 103.3445 - val_loss: 103.6044 - val_MinusLogProbMetric: 103.6044 - lr: 5.0805e-08 - 75s/epoch - 381ms/step
Epoch 517/1000
2023-10-26 15:51:41.510 
Epoch 517/1000 
	 loss: 103.3331, MinusLogProbMetric: 103.3331, val_loss: 103.5603, val_MinusLogProbMetric: 103.5603

Epoch 517: val_loss improved from 103.58949 to 103.56026, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 73s - loss: 103.3331 - MinusLogProbMetric: 103.3331 - val_loss: 103.5603 - val_MinusLogProbMetric: 103.5603 - lr: 5.0805e-08 - 73s/epoch - 373ms/step
Epoch 518/1000
2023-10-26 15:52:55.833 
Epoch 518/1000 
	 loss: 103.3184, MinusLogProbMetric: 103.3184, val_loss: 103.5795, val_MinusLogProbMetric: 103.5795

Epoch 518: val_loss did not improve from 103.56026
196/196 - 73s - loss: 103.3184 - MinusLogProbMetric: 103.3184 - val_loss: 103.5795 - val_MinusLogProbMetric: 103.5795 - lr: 5.0805e-08 - 73s/epoch - 370ms/step
Epoch 519/1000
2023-10-26 15:54:10.729 
Epoch 519/1000 
	 loss: 103.3380, MinusLogProbMetric: 103.3380, val_loss: 103.5786, val_MinusLogProbMetric: 103.5786

Epoch 519: val_loss did not improve from 103.56026
196/196 - 75s - loss: 103.3380 - MinusLogProbMetric: 103.3380 - val_loss: 103.5786 - val_MinusLogProbMetric: 103.5786 - lr: 5.0805e-08 - 75s/epoch - 382ms/step
Epoch 520/1000
2023-10-26 15:55:28.744 
Epoch 520/1000 
	 loss: 103.3288, MinusLogProbMetric: 103.3288, val_loss: 103.5911, val_MinusLogProbMetric: 103.5911

Epoch 520: val_loss did not improve from 103.56026
196/196 - 78s - loss: 103.3288 - MinusLogProbMetric: 103.3288 - val_loss: 103.5911 - val_MinusLogProbMetric: 103.5911 - lr: 5.0805e-08 - 78s/epoch - 398ms/step
Epoch 521/1000
2023-10-26 15:56:51.129 
Epoch 521/1000 
	 loss: 103.3276, MinusLogProbMetric: 103.3276, val_loss: 103.5772, val_MinusLogProbMetric: 103.5772

Epoch 521: val_loss did not improve from 103.56026
196/196 - 82s - loss: 103.3276 - MinusLogProbMetric: 103.3276 - val_loss: 103.5772 - val_MinusLogProbMetric: 103.5772 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 522/1000
2023-10-26 15:58:09.066 
Epoch 522/1000 
	 loss: 103.3230, MinusLogProbMetric: 103.3230, val_loss: 103.5813, val_MinusLogProbMetric: 103.5813

Epoch 522: val_loss did not improve from 103.56026
196/196 - 78s - loss: 103.3230 - MinusLogProbMetric: 103.3230 - val_loss: 103.5813 - val_MinusLogProbMetric: 103.5813 - lr: 5.0805e-08 - 78s/epoch - 398ms/step
Epoch 523/1000
2023-10-26 15:59:16.605 
Epoch 523/1000 
	 loss: 103.3436, MinusLogProbMetric: 103.3436, val_loss: 103.5798, val_MinusLogProbMetric: 103.5798

Epoch 523: val_loss did not improve from 103.56026
196/196 - 68s - loss: 103.3436 - MinusLogProbMetric: 103.3436 - val_loss: 103.5798 - val_MinusLogProbMetric: 103.5798 - lr: 5.0805e-08 - 68s/epoch - 344ms/step
Epoch 524/1000
2023-10-26 16:00:25.512 
Epoch 524/1000 
	 loss: 103.3146, MinusLogProbMetric: 103.3146, val_loss: 103.5663, val_MinusLogProbMetric: 103.5663

Epoch 524: val_loss did not improve from 103.56026
196/196 - 69s - loss: 103.3146 - MinusLogProbMetric: 103.3146 - val_loss: 103.5663 - val_MinusLogProbMetric: 103.5663 - lr: 5.0805e-08 - 69s/epoch - 352ms/step
Epoch 525/1000
2023-10-26 16:01:41.637 
Epoch 525/1000 
	 loss: 103.3294, MinusLogProbMetric: 103.3294, val_loss: 103.6144, val_MinusLogProbMetric: 103.6144

Epoch 525: val_loss did not improve from 103.56026
196/196 - 76s - loss: 103.3294 - MinusLogProbMetric: 103.3294 - val_loss: 103.6144 - val_MinusLogProbMetric: 103.6144 - lr: 5.0805e-08 - 76s/epoch - 388ms/step
Epoch 526/1000
2023-10-26 16:02:46.851 
Epoch 526/1000 
	 loss: 103.3193, MinusLogProbMetric: 103.3193, val_loss: 103.5820, val_MinusLogProbMetric: 103.5820

Epoch 526: val_loss did not improve from 103.56026
196/196 - 65s - loss: 103.3193 - MinusLogProbMetric: 103.3193 - val_loss: 103.5820 - val_MinusLogProbMetric: 103.5820 - lr: 5.0805e-08 - 65s/epoch - 333ms/step
Epoch 527/1000
2023-10-26 16:03:59.614 
Epoch 527/1000 
	 loss: 103.3097, MinusLogProbMetric: 103.3097, val_loss: 103.5884, val_MinusLogProbMetric: 103.5884

Epoch 527: val_loss did not improve from 103.56026
196/196 - 73s - loss: 103.3097 - MinusLogProbMetric: 103.3097 - val_loss: 103.5884 - val_MinusLogProbMetric: 103.5884 - lr: 5.0805e-08 - 73s/epoch - 371ms/step
Epoch 528/1000
2023-10-26 16:05:07.536 
Epoch 528/1000 
	 loss: 103.3219, MinusLogProbMetric: 103.3219, val_loss: 103.5944, val_MinusLogProbMetric: 103.5944

Epoch 528: val_loss did not improve from 103.56026
196/196 - 68s - loss: 103.3219 - MinusLogProbMetric: 103.3219 - val_loss: 103.5944 - val_MinusLogProbMetric: 103.5944 - lr: 5.0805e-08 - 68s/epoch - 347ms/step
Epoch 529/1000
2023-10-26 16:06:20.056 
Epoch 529/1000 
	 loss: 103.3115, MinusLogProbMetric: 103.3115, val_loss: 103.5697, val_MinusLogProbMetric: 103.5697

Epoch 529: val_loss did not improve from 103.56026
196/196 - 73s - loss: 103.3115 - MinusLogProbMetric: 103.3115 - val_loss: 103.5697 - val_MinusLogProbMetric: 103.5697 - lr: 5.0805e-08 - 73s/epoch - 370ms/step
Epoch 530/1000
2023-10-26 16:07:40.250 
Epoch 530/1000 
	 loss: 103.3091, MinusLogProbMetric: 103.3091, val_loss: 103.6155, val_MinusLogProbMetric: 103.6155

Epoch 530: val_loss did not improve from 103.56026
196/196 - 80s - loss: 103.3091 - MinusLogProbMetric: 103.3091 - val_loss: 103.6155 - val_MinusLogProbMetric: 103.6155 - lr: 5.0805e-08 - 80s/epoch - 409ms/step
Epoch 531/1000
2023-10-26 16:08:59.559 
Epoch 531/1000 
	 loss: 103.3067, MinusLogProbMetric: 103.3067, val_loss: 103.5872, val_MinusLogProbMetric: 103.5872

Epoch 531: val_loss did not improve from 103.56026
196/196 - 79s - loss: 103.3067 - MinusLogProbMetric: 103.3067 - val_loss: 103.5872 - val_MinusLogProbMetric: 103.5872 - lr: 5.0805e-08 - 79s/epoch - 405ms/step
Epoch 532/1000
2023-10-26 16:10:22.879 
Epoch 532/1000 
	 loss: 103.3022, MinusLogProbMetric: 103.3022, val_loss: 103.5314, val_MinusLogProbMetric: 103.5314

Epoch 532: val_loss improved from 103.56026 to 103.53142, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 103.3022 - MinusLogProbMetric: 103.3022 - val_loss: 103.5314 - val_MinusLogProbMetric: 103.5314 - lr: 5.0805e-08 - 85s/epoch - 433ms/step
Epoch 533/1000
2023-10-26 16:11:43.984 
Epoch 533/1000 
	 loss: 103.2956, MinusLogProbMetric: 103.2956, val_loss: 103.5318, val_MinusLogProbMetric: 103.5318

Epoch 533: val_loss did not improve from 103.53142
196/196 - 80s - loss: 103.2956 - MinusLogProbMetric: 103.2956 - val_loss: 103.5318 - val_MinusLogProbMetric: 103.5318 - lr: 5.0805e-08 - 80s/epoch - 406ms/step
Epoch 534/1000
2023-10-26 16:13:06.700 
Epoch 534/1000 
	 loss: 103.2823, MinusLogProbMetric: 103.2823, val_loss: 103.5596, val_MinusLogProbMetric: 103.5596

Epoch 534: val_loss did not improve from 103.53142
196/196 - 83s - loss: 103.2823 - MinusLogProbMetric: 103.2823 - val_loss: 103.5596 - val_MinusLogProbMetric: 103.5596 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 535/1000
2023-10-26 16:14:26.456 
Epoch 535/1000 
	 loss: 103.2435, MinusLogProbMetric: 103.2435, val_loss: 103.5245, val_MinusLogProbMetric: 103.5245

Epoch 535: val_loss improved from 103.53142 to 103.52450, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 81s - loss: 103.2435 - MinusLogProbMetric: 103.2435 - val_loss: 103.5245 - val_MinusLogProbMetric: 103.5245 - lr: 5.0805e-08 - 81s/epoch - 414ms/step
Epoch 536/1000
2023-10-26 16:15:51.406 
Epoch 536/1000 
	 loss: 103.2460, MinusLogProbMetric: 103.2460, val_loss: 103.5010, val_MinusLogProbMetric: 103.5010

Epoch 536: val_loss improved from 103.52450 to 103.50105, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 103.2460 - MinusLogProbMetric: 103.2460 - val_loss: 103.5010 - val_MinusLogProbMetric: 103.5010 - lr: 5.0805e-08 - 85s/epoch - 434ms/step
Epoch 537/1000
2023-10-26 16:17:14.531 
Epoch 537/1000 
	 loss: 103.2451, MinusLogProbMetric: 103.2451, val_loss: 103.5017, val_MinusLogProbMetric: 103.5017

Epoch 537: val_loss did not improve from 103.50105
196/196 - 82s - loss: 103.2451 - MinusLogProbMetric: 103.2451 - val_loss: 103.5017 - val_MinusLogProbMetric: 103.5017 - lr: 5.0805e-08 - 82s/epoch - 416ms/step
Epoch 538/1000
2023-10-26 16:18:34.369 
Epoch 538/1000 
	 loss: 103.2310, MinusLogProbMetric: 103.2310, val_loss: 103.4933, val_MinusLogProbMetric: 103.4933

Epoch 538: val_loss improved from 103.50105 to 103.49332, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 82s - loss: 103.2310 - MinusLogProbMetric: 103.2310 - val_loss: 103.4933 - val_MinusLogProbMetric: 103.4933 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 539/1000
2023-10-26 16:19:58.407 
Epoch 539/1000 
	 loss: 103.2217, MinusLogProbMetric: 103.2217, val_loss: 103.5051, val_MinusLogProbMetric: 103.5051

Epoch 539: val_loss did not improve from 103.49332
196/196 - 82s - loss: 103.2217 - MinusLogProbMetric: 103.2217 - val_loss: 103.5051 - val_MinusLogProbMetric: 103.5051 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 540/1000
2023-10-26 16:21:20.018 
Epoch 540/1000 
	 loss: 103.2220, MinusLogProbMetric: 103.2220, val_loss: 103.5158, val_MinusLogProbMetric: 103.5158

Epoch 540: val_loss did not improve from 103.49332
196/196 - 82s - loss: 103.2220 - MinusLogProbMetric: 103.2220 - val_loss: 103.5158 - val_MinusLogProbMetric: 103.5158 - lr: 5.0805e-08 - 82s/epoch - 416ms/step
Epoch 541/1000
2023-10-26 16:22:40.619 
Epoch 541/1000 
	 loss: 103.2222, MinusLogProbMetric: 103.2222, val_loss: 103.4842, val_MinusLogProbMetric: 103.4842

Epoch 541: val_loss improved from 103.49332 to 103.48419, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 103.2222 - MinusLogProbMetric: 103.2222 - val_loss: 103.4842 - val_MinusLogProbMetric: 103.4842 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 542/1000
2023-10-26 16:24:02.604 
Epoch 542/1000 
	 loss: 103.2128, MinusLogProbMetric: 103.2128, val_loss: 103.4782, val_MinusLogProbMetric: 103.4782

Epoch 542: val_loss improved from 103.48419 to 103.47823, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 81s - loss: 103.2128 - MinusLogProbMetric: 103.2128 - val_loss: 103.4782 - val_MinusLogProbMetric: 103.4782 - lr: 5.0805e-08 - 81s/epoch - 416ms/step
Epoch 543/1000
2023-10-26 16:25:14.272 
Epoch 543/1000 
	 loss: 103.1976, MinusLogProbMetric: 103.1976, val_loss: 103.4622, val_MinusLogProbMetric: 103.4622

Epoch 543: val_loss improved from 103.47823 to 103.46219, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 72s - loss: 103.1976 - MinusLogProbMetric: 103.1976 - val_loss: 103.4622 - val_MinusLogProbMetric: 103.4622 - lr: 5.0805e-08 - 72s/epoch - 365ms/step
Epoch 544/1000
2023-10-26 16:26:31.638 
Epoch 544/1000 
	 loss: 103.1992, MinusLogProbMetric: 103.1992, val_loss: 103.4664, val_MinusLogProbMetric: 103.4664

Epoch 544: val_loss did not improve from 103.46219
196/196 - 76s - loss: 103.1992 - MinusLogProbMetric: 103.1992 - val_loss: 103.4664 - val_MinusLogProbMetric: 103.4664 - lr: 5.0805e-08 - 76s/epoch - 388ms/step
Epoch 545/1000
2023-10-26 16:27:51.781 
Epoch 545/1000 
	 loss: 103.1998, MinusLogProbMetric: 103.1998, val_loss: 103.4522, val_MinusLogProbMetric: 103.4522

Epoch 545: val_loss improved from 103.46219 to 103.45223, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 82s - loss: 103.1998 - MinusLogProbMetric: 103.1998 - val_loss: 103.4522 - val_MinusLogProbMetric: 103.4522 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 546/1000
2023-10-26 16:29:15.449 
Epoch 546/1000 
	 loss: 103.1966, MinusLogProbMetric: 103.1966, val_loss: 103.4840, val_MinusLogProbMetric: 103.4840

Epoch 546: val_loss did not improve from 103.45223
196/196 - 82s - loss: 103.1966 - MinusLogProbMetric: 103.1966 - val_loss: 103.4840 - val_MinusLogProbMetric: 103.4840 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 547/1000
2023-10-26 16:30:33.610 
Epoch 547/1000 
	 loss: 103.1920, MinusLogProbMetric: 103.1920, val_loss: 103.4527, val_MinusLogProbMetric: 103.4527

Epoch 547: val_loss did not improve from 103.45223
196/196 - 78s - loss: 103.1920 - MinusLogProbMetric: 103.1920 - val_loss: 103.4527 - val_MinusLogProbMetric: 103.4527 - lr: 5.0805e-08 - 78s/epoch - 399ms/step
Epoch 548/1000
2023-10-26 16:31:55.615 
Epoch 548/1000 
	 loss: 103.1877, MinusLogProbMetric: 103.1877, val_loss: 103.4510, val_MinusLogProbMetric: 103.4510

Epoch 548: val_loss improved from 103.45223 to 103.45096, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 103.1877 - MinusLogProbMetric: 103.1877 - val_loss: 103.4510 - val_MinusLogProbMetric: 103.4510 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 549/1000
2023-10-26 16:33:19.210 
Epoch 549/1000 
	 loss: 103.1818, MinusLogProbMetric: 103.1818, val_loss: 103.4612, val_MinusLogProbMetric: 103.4612

Epoch 549: val_loss did not improve from 103.45096
196/196 - 82s - loss: 103.1818 - MinusLogProbMetric: 103.1818 - val_loss: 103.4612 - val_MinusLogProbMetric: 103.4612 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 550/1000
2023-10-26 16:35:24.981 
Epoch 550/1000 
	 loss: 103.1849, MinusLogProbMetric: 103.1849, val_loss: 103.4571, val_MinusLogProbMetric: 103.4571

Epoch 550: val_loss did not improve from 103.45096
196/196 - 126s - loss: 103.1849 - MinusLogProbMetric: 103.1849 - val_loss: 103.4571 - val_MinusLogProbMetric: 103.4571 - lr: 5.0805e-08 - 126s/epoch - 642ms/step
Epoch 551/1000
2023-10-26 16:36:46.243 
Epoch 551/1000 
	 loss: 103.1780, MinusLogProbMetric: 103.1780, val_loss: 103.4267, val_MinusLogProbMetric: 103.4267

Epoch 551: val_loss improved from 103.45096 to 103.42666, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 103.1780 - MinusLogProbMetric: 103.1780 - val_loss: 103.4267 - val_MinusLogProbMetric: 103.4267 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 552/1000
2023-10-26 16:38:03.253 
Epoch 552/1000 
	 loss: 103.1774, MinusLogProbMetric: 103.1774, val_loss: 103.4310, val_MinusLogProbMetric: 103.4310

Epoch 552: val_loss did not improve from 103.42666
196/196 - 75s - loss: 103.1774 - MinusLogProbMetric: 103.1774 - val_loss: 103.4310 - val_MinusLogProbMetric: 103.4310 - lr: 5.0805e-08 - 75s/epoch - 383ms/step
Epoch 553/1000
2023-10-26 16:39:21.305 
Epoch 553/1000 
	 loss: 103.1797, MinusLogProbMetric: 103.1797, val_loss: 103.4320, val_MinusLogProbMetric: 103.4320

Epoch 553: val_loss did not improve from 103.42666
196/196 - 78s - loss: 103.1797 - MinusLogProbMetric: 103.1797 - val_loss: 103.4320 - val_MinusLogProbMetric: 103.4320 - lr: 5.0805e-08 - 78s/epoch - 398ms/step
Epoch 554/1000
2023-10-26 16:40:41.212 
Epoch 554/1000 
	 loss: 103.1723, MinusLogProbMetric: 103.1723, val_loss: 103.4438, val_MinusLogProbMetric: 103.4438

Epoch 554: val_loss did not improve from 103.42666
196/196 - 80s - loss: 103.1723 - MinusLogProbMetric: 103.1723 - val_loss: 103.4438 - val_MinusLogProbMetric: 103.4438 - lr: 5.0805e-08 - 80s/epoch - 408ms/step
Epoch 555/1000
2023-10-26 16:41:59.916 
Epoch 555/1000 
	 loss: 103.1732, MinusLogProbMetric: 103.1732, val_loss: 103.4336, val_MinusLogProbMetric: 103.4336

Epoch 555: val_loss did not improve from 103.42666
196/196 - 79s - loss: 103.1732 - MinusLogProbMetric: 103.1732 - val_loss: 103.4336 - val_MinusLogProbMetric: 103.4336 - lr: 5.0805e-08 - 79s/epoch - 402ms/step
Epoch 556/1000
2023-10-26 16:43:21.492 
Epoch 556/1000 
	 loss: 103.1628, MinusLogProbMetric: 103.1628, val_loss: 103.4405, val_MinusLogProbMetric: 103.4405

Epoch 556: val_loss did not improve from 103.42666
196/196 - 82s - loss: 103.1628 - MinusLogProbMetric: 103.1628 - val_loss: 103.4405 - val_MinusLogProbMetric: 103.4405 - lr: 5.0805e-08 - 82s/epoch - 416ms/step
Epoch 557/1000
2023-10-26 16:44:41.981 
Epoch 557/1000 
	 loss: 103.1665, MinusLogProbMetric: 103.1665, val_loss: 103.4277, val_MinusLogProbMetric: 103.4277

Epoch 557: val_loss did not improve from 103.42666
196/196 - 80s - loss: 103.1665 - MinusLogProbMetric: 103.1665 - val_loss: 103.4277 - val_MinusLogProbMetric: 103.4277 - lr: 5.0805e-08 - 80s/epoch - 411ms/step
Epoch 558/1000
2023-10-26 16:46:00.285 
Epoch 558/1000 
	 loss: 103.1544, MinusLogProbMetric: 103.1544, val_loss: 103.4422, val_MinusLogProbMetric: 103.4422

Epoch 558: val_loss did not improve from 103.42666
196/196 - 78s - loss: 103.1544 - MinusLogProbMetric: 103.1544 - val_loss: 103.4422 - val_MinusLogProbMetric: 103.4422 - lr: 5.0805e-08 - 78s/epoch - 399ms/step
Epoch 559/1000
2023-10-26 16:47:17.293 
Epoch 559/1000 
	 loss: 103.1549, MinusLogProbMetric: 103.1549, val_loss: 103.4265, val_MinusLogProbMetric: 103.4265

Epoch 559: val_loss improved from 103.42666 to 103.42654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 79s - loss: 103.1549 - MinusLogProbMetric: 103.1549 - val_loss: 103.4265 - val_MinusLogProbMetric: 103.4265 - lr: 5.0805e-08 - 79s/epoch - 402ms/step
Epoch 560/1000
2023-10-26 16:48:40.275 
Epoch 560/1000 
	 loss: 103.1610, MinusLogProbMetric: 103.1610, val_loss: 103.4235, val_MinusLogProbMetric: 103.4235

Epoch 560: val_loss improved from 103.42654 to 103.42348, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 82s - loss: 103.1610 - MinusLogProbMetric: 103.1610 - val_loss: 103.4235 - val_MinusLogProbMetric: 103.4235 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 561/1000
2023-10-26 16:49:56.200 
Epoch 561/1000 
	 loss: 103.1472, MinusLogProbMetric: 103.1472, val_loss: 103.4175, val_MinusLogProbMetric: 103.4175

Epoch 561: val_loss improved from 103.42348 to 103.41749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 77s - loss: 103.1472 - MinusLogProbMetric: 103.1472 - val_loss: 103.4175 - val_MinusLogProbMetric: 103.4175 - lr: 5.0805e-08 - 77s/epoch - 391ms/step
Epoch 562/1000
2023-10-26 16:51:17.204 
Epoch 562/1000 
	 loss: 103.1527, MinusLogProbMetric: 103.1527, val_loss: 103.4051, val_MinusLogProbMetric: 103.4051

Epoch 562: val_loss improved from 103.41749 to 103.40506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 81s - loss: 103.1527 - MinusLogProbMetric: 103.1527 - val_loss: 103.4051 - val_MinusLogProbMetric: 103.4051 - lr: 5.0805e-08 - 81s/epoch - 412ms/step
Epoch 563/1000
2023-10-26 16:52:40.839 
Epoch 563/1000 
	 loss: 103.1363, MinusLogProbMetric: 103.1363, val_loss: 103.3942, val_MinusLogProbMetric: 103.3942

Epoch 563: val_loss improved from 103.40506 to 103.39422, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 103.1363 - MinusLogProbMetric: 103.1363 - val_loss: 103.3942 - val_MinusLogProbMetric: 103.3942 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 564/1000
2023-10-26 16:53:56.172 
Epoch 564/1000 
	 loss: 103.1277, MinusLogProbMetric: 103.1277, val_loss: 103.3964, val_MinusLogProbMetric: 103.3964

Epoch 564: val_loss did not improve from 103.39422
196/196 - 73s - loss: 103.1277 - MinusLogProbMetric: 103.1277 - val_loss: 103.3964 - val_MinusLogProbMetric: 103.3964 - lr: 5.0805e-08 - 73s/epoch - 374ms/step
Epoch 565/1000
2023-10-26 16:55:17.042 
Epoch 565/1000 
	 loss: 103.1325, MinusLogProbMetric: 103.1325, val_loss: 103.3924, val_MinusLogProbMetric: 103.3924

Epoch 565: val_loss improved from 103.39422 to 103.39240, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 103.1325 - MinusLogProbMetric: 103.1325 - val_loss: 103.3924 - val_MinusLogProbMetric: 103.3924 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 566/1000
2023-10-26 16:56:40.269 
Epoch 566/1000 
	 loss: 103.1308, MinusLogProbMetric: 103.1308, val_loss: 103.4038, val_MinusLogProbMetric: 103.4038

Epoch 566: val_loss did not improve from 103.39240
196/196 - 81s - loss: 103.1308 - MinusLogProbMetric: 103.1308 - val_loss: 103.4038 - val_MinusLogProbMetric: 103.4038 - lr: 5.0805e-08 - 81s/epoch - 414ms/step
Epoch 567/1000
2023-10-26 16:57:59.898 
Epoch 567/1000 
	 loss: 103.1324, MinusLogProbMetric: 103.1324, val_loss: 103.3764, val_MinusLogProbMetric: 103.3764

Epoch 567: val_loss improved from 103.39240 to 103.37636, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 81s - loss: 103.1324 - MinusLogProbMetric: 103.1324 - val_loss: 103.3764 - val_MinusLogProbMetric: 103.3764 - lr: 5.0805e-08 - 81s/epoch - 414ms/step
Epoch 568/1000
2023-10-26 16:59:22.818 
Epoch 568/1000 
	 loss: 103.1284, MinusLogProbMetric: 103.1284, val_loss: 103.3865, val_MinusLogProbMetric: 103.3865

Epoch 568: val_loss did not improve from 103.37636
196/196 - 81s - loss: 103.1284 - MinusLogProbMetric: 103.1284 - val_loss: 103.3865 - val_MinusLogProbMetric: 103.3865 - lr: 5.0805e-08 - 81s/epoch - 415ms/step
Epoch 569/1000
2023-10-26 17:00:42.548 
Epoch 569/1000 
	 loss: 103.1244, MinusLogProbMetric: 103.1244, val_loss: 103.3739, val_MinusLogProbMetric: 103.3739

Epoch 569: val_loss improved from 103.37636 to 103.37393, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 81s - loss: 103.1244 - MinusLogProbMetric: 103.1244 - val_loss: 103.3739 - val_MinusLogProbMetric: 103.3739 - lr: 5.0805e-08 - 81s/epoch - 415ms/step
Epoch 570/1000
2023-10-26 17:02:02.376 
Epoch 570/1000 
	 loss: 103.1223, MinusLogProbMetric: 103.1223, val_loss: 103.4434, val_MinusLogProbMetric: 103.4434

Epoch 570: val_loss did not improve from 103.37393
196/196 - 78s - loss: 103.1223 - MinusLogProbMetric: 103.1223 - val_loss: 103.4434 - val_MinusLogProbMetric: 103.4434 - lr: 5.0805e-08 - 78s/epoch - 399ms/step
Epoch 571/1000
2023-10-26 17:03:21.104 
Epoch 571/1000 
	 loss: 103.1400, MinusLogProbMetric: 103.1400, val_loss: 103.3916, val_MinusLogProbMetric: 103.3916

Epoch 571: val_loss did not improve from 103.37393
196/196 - 79s - loss: 103.1400 - MinusLogProbMetric: 103.1400 - val_loss: 103.3916 - val_MinusLogProbMetric: 103.3916 - lr: 5.0805e-08 - 79s/epoch - 402ms/step
Epoch 572/1000
2023-10-26 17:04:36.203 
Epoch 572/1000 
	 loss: 103.1401, MinusLogProbMetric: 103.1401, val_loss: 103.3781, val_MinusLogProbMetric: 103.3781

Epoch 572: val_loss did not improve from 103.37393
196/196 - 75s - loss: 103.1401 - MinusLogProbMetric: 103.1401 - val_loss: 103.3781 - val_MinusLogProbMetric: 103.3781 - lr: 5.0805e-08 - 75s/epoch - 383ms/step
Epoch 573/1000
2023-10-26 17:05:55.137 
Epoch 573/1000 
	 loss: 103.1242, MinusLogProbMetric: 103.1242, val_loss: 103.3847, val_MinusLogProbMetric: 103.3847

Epoch 573: val_loss did not improve from 103.37393
196/196 - 79s - loss: 103.1242 - MinusLogProbMetric: 103.1242 - val_loss: 103.3847 - val_MinusLogProbMetric: 103.3847 - lr: 5.0805e-08 - 79s/epoch - 403ms/step
Epoch 574/1000
2023-10-26 17:07:13.861 
Epoch 574/1000 
	 loss: 103.1252, MinusLogProbMetric: 103.1252, val_loss: 103.3835, val_MinusLogProbMetric: 103.3835

Epoch 574: val_loss did not improve from 103.37393
196/196 - 79s - loss: 103.1252 - MinusLogProbMetric: 103.1252 - val_loss: 103.3835 - val_MinusLogProbMetric: 103.3835 - lr: 5.0805e-08 - 79s/epoch - 402ms/step
Epoch 575/1000
2023-10-26 17:08:33.228 
Epoch 575/1000 
	 loss: 103.1319, MinusLogProbMetric: 103.1319, val_loss: 103.4439, val_MinusLogProbMetric: 103.4439

Epoch 575: val_loss did not improve from 103.37393
196/196 - 79s - loss: 103.1319 - MinusLogProbMetric: 103.1319 - val_loss: 103.4439 - val_MinusLogProbMetric: 103.4439 - lr: 5.0805e-08 - 79s/epoch - 405ms/step
Epoch 576/1000
2023-10-26 17:09:50.744 
Epoch 576/1000 
	 loss: 103.1589, MinusLogProbMetric: 103.1589, val_loss: 103.4211, val_MinusLogProbMetric: 103.4211

Epoch 576: val_loss did not improve from 103.37393
196/196 - 78s - loss: 103.1589 - MinusLogProbMetric: 103.1589 - val_loss: 103.4211 - val_MinusLogProbMetric: 103.4211 - lr: 5.0805e-08 - 78s/epoch - 395ms/step
Epoch 577/1000
2023-10-26 17:11:11.562 
Epoch 577/1000 
	 loss: 103.1395, MinusLogProbMetric: 103.1395, val_loss: 103.3985, val_MinusLogProbMetric: 103.3985

Epoch 577: val_loss did not improve from 103.37393
196/196 - 81s - loss: 103.1395 - MinusLogProbMetric: 103.1395 - val_loss: 103.3985 - val_MinusLogProbMetric: 103.3985 - lr: 5.0805e-08 - 81s/epoch - 412ms/step
Epoch 578/1000
2023-10-26 17:12:31.276 
Epoch 578/1000 
	 loss: 103.1322, MinusLogProbMetric: 103.1322, val_loss: 103.3921, val_MinusLogProbMetric: 103.3921

Epoch 578: val_loss did not improve from 103.37393
196/196 - 80s - loss: 103.1322 - MinusLogProbMetric: 103.1322 - val_loss: 103.3921 - val_MinusLogProbMetric: 103.3921 - lr: 5.0805e-08 - 80s/epoch - 407ms/step
Epoch 579/1000
2023-10-26 17:13:48.217 
Epoch 579/1000 
	 loss: 103.1277, MinusLogProbMetric: 103.1277, val_loss: 103.4007, val_MinusLogProbMetric: 103.4007

Epoch 579: val_loss did not improve from 103.37393
196/196 - 77s - loss: 103.1277 - MinusLogProbMetric: 103.1277 - val_loss: 103.4007 - val_MinusLogProbMetric: 103.4007 - lr: 5.0805e-08 - 77s/epoch - 393ms/step
Epoch 580/1000
2023-10-26 17:15:07.014 
Epoch 580/1000 
	 loss: 103.1332, MinusLogProbMetric: 103.1332, val_loss: 103.4027, val_MinusLogProbMetric: 103.4027

Epoch 580: val_loss did not improve from 103.37393
196/196 - 79s - loss: 103.1332 - MinusLogProbMetric: 103.1332 - val_loss: 103.4027 - val_MinusLogProbMetric: 103.4027 - lr: 5.0805e-08 - 79s/epoch - 402ms/step
Epoch 581/1000
2023-10-26 17:16:27.916 
Epoch 581/1000 
	 loss: 103.1206, MinusLogProbMetric: 103.1206, val_loss: 103.3851, val_MinusLogProbMetric: 103.3851

Epoch 581: val_loss did not improve from 103.37393
196/196 - 81s - loss: 103.1206 - MinusLogProbMetric: 103.1206 - val_loss: 103.3851 - val_MinusLogProbMetric: 103.3851 - lr: 5.0805e-08 - 81s/epoch - 413ms/step
Epoch 582/1000
2023-10-26 17:17:49.240 
Epoch 582/1000 
	 loss: 103.1231, MinusLogProbMetric: 103.1231, val_loss: 103.3669, val_MinusLogProbMetric: 103.3669

Epoch 582: val_loss improved from 103.37393 to 103.36694, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 103.1231 - MinusLogProbMetric: 103.1231 - val_loss: 103.3669 - val_MinusLogProbMetric: 103.3669 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 583/1000
2023-10-26 17:19:04.948 
Epoch 583/1000 
	 loss: 103.1135, MinusLogProbMetric: 103.1135, val_loss: 103.3698, val_MinusLogProbMetric: 103.3698

Epoch 583: val_loss did not improve from 103.36694
196/196 - 74s - loss: 103.1135 - MinusLogProbMetric: 103.1135 - val_loss: 103.3698 - val_MinusLogProbMetric: 103.3698 - lr: 5.0805e-08 - 74s/epoch - 380ms/step
Epoch 584/1000
2023-10-26 17:20:22.764 
Epoch 584/1000 
	 loss: 103.1115, MinusLogProbMetric: 103.1115, val_loss: 103.3775, val_MinusLogProbMetric: 103.3775

Epoch 584: val_loss did not improve from 103.36694
196/196 - 78s - loss: 103.1115 - MinusLogProbMetric: 103.1115 - val_loss: 103.3775 - val_MinusLogProbMetric: 103.3775 - lr: 5.0805e-08 - 78s/epoch - 397ms/step
Epoch 585/1000
2023-10-26 17:21:42.084 
Epoch 585/1000 
	 loss: 103.1095, MinusLogProbMetric: 103.1095, val_loss: 103.3701, val_MinusLogProbMetric: 103.3701

Epoch 585: val_loss did not improve from 103.36694
196/196 - 79s - loss: 103.1095 - MinusLogProbMetric: 103.1095 - val_loss: 103.3701 - val_MinusLogProbMetric: 103.3701 - lr: 5.0805e-08 - 79s/epoch - 405ms/step
Epoch 586/1000
2023-10-26 17:23:00.849 
Epoch 586/1000 
	 loss: 103.0983, MinusLogProbMetric: 103.0983, val_loss: 103.3724, val_MinusLogProbMetric: 103.3724

Epoch 586: val_loss did not improve from 103.36694
196/196 - 79s - loss: 103.0983 - MinusLogProbMetric: 103.0983 - val_loss: 103.3724 - val_MinusLogProbMetric: 103.3724 - lr: 5.0805e-08 - 79s/epoch - 402ms/step
Epoch 587/1000
2023-10-26 17:24:17.828 
Epoch 587/1000 
	 loss: 103.1010, MinusLogProbMetric: 103.1010, val_loss: 103.3472, val_MinusLogProbMetric: 103.3472

Epoch 587: val_loss improved from 103.36694 to 103.34722, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 79s - loss: 103.1010 - MinusLogProbMetric: 103.1010 - val_loss: 103.3472 - val_MinusLogProbMetric: 103.3472 - lr: 5.0805e-08 - 79s/epoch - 403ms/step
Epoch 588/1000
2023-10-26 17:25:39.997 
Epoch 588/1000 
	 loss: 103.0999, MinusLogProbMetric: 103.0999, val_loss: 103.3478, val_MinusLogProbMetric: 103.3478

Epoch 588: val_loss did not improve from 103.34722
196/196 - 80s - loss: 103.0999 - MinusLogProbMetric: 103.0999 - val_loss: 103.3478 - val_MinusLogProbMetric: 103.3478 - lr: 5.0805e-08 - 80s/epoch - 409ms/step
Epoch 589/1000
2023-10-26 17:26:58.689 
Epoch 589/1000 
	 loss: 103.0911, MinusLogProbMetric: 103.0911, val_loss: 103.3407, val_MinusLogProbMetric: 103.3407

Epoch 589: val_loss improved from 103.34722 to 103.34067, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 80s - loss: 103.0911 - MinusLogProbMetric: 103.0911 - val_loss: 103.3407 - val_MinusLogProbMetric: 103.3407 - lr: 5.0805e-08 - 80s/epoch - 410ms/step
Epoch 590/1000
2023-10-26 17:28:18.435 
Epoch 590/1000 
	 loss: 103.0813, MinusLogProbMetric: 103.0813, val_loss: 103.3396, val_MinusLogProbMetric: 103.3396

Epoch 590: val_loss improved from 103.34067 to 103.33963, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 80s - loss: 103.0813 - MinusLogProbMetric: 103.0813 - val_loss: 103.3396 - val_MinusLogProbMetric: 103.3396 - lr: 5.0805e-08 - 80s/epoch - 406ms/step
Epoch 591/1000
2023-10-26 17:29:40.524 
Epoch 591/1000 
	 loss: 103.1003, MinusLogProbMetric: 103.1003, val_loss: 103.3441, val_MinusLogProbMetric: 103.3441

Epoch 591: val_loss did not improve from 103.33963
196/196 - 81s - loss: 103.1003 - MinusLogProbMetric: 103.1003 - val_loss: 103.3441 - val_MinusLogProbMetric: 103.3441 - lr: 5.0805e-08 - 81s/epoch - 411ms/step
Epoch 592/1000
2023-10-26 17:31:01.617 
Epoch 592/1000 
	 loss: 103.0822, MinusLogProbMetric: 103.0822, val_loss: 103.3496, val_MinusLogProbMetric: 103.3496

Epoch 592: val_loss did not improve from 103.33963
196/196 - 81s - loss: 103.0822 - MinusLogProbMetric: 103.0822 - val_loss: 103.3496 - val_MinusLogProbMetric: 103.3496 - lr: 5.0805e-08 - 81s/epoch - 414ms/step
Epoch 593/1000
2023-10-26 17:32:23.291 
Epoch 593/1000 
	 loss: 103.0781, MinusLogProbMetric: 103.0781, val_loss: 103.3490, val_MinusLogProbMetric: 103.3490

Epoch 593: val_loss did not improve from 103.33963
196/196 - 82s - loss: 103.0781 - MinusLogProbMetric: 103.0781 - val_loss: 103.3490 - val_MinusLogProbMetric: 103.3490 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 594/1000
2023-10-26 17:33:43.299 
Epoch 594/1000 
	 loss: 103.0835, MinusLogProbMetric: 103.0835, val_loss: 103.3381, val_MinusLogProbMetric: 103.3381

Epoch 594: val_loss improved from 103.33963 to 103.33808, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 81s - loss: 103.0835 - MinusLogProbMetric: 103.0835 - val_loss: 103.3381 - val_MinusLogProbMetric: 103.3381 - lr: 5.0805e-08 - 81s/epoch - 415ms/step
Epoch 595/1000
2023-10-26 17:35:03.220 
Epoch 595/1000 
	 loss: 103.0752, MinusLogProbMetric: 103.0752, val_loss: 103.3155, val_MinusLogProbMetric: 103.3155

Epoch 595: val_loss improved from 103.33808 to 103.31549, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 80s - loss: 103.0752 - MinusLogProbMetric: 103.0752 - val_loss: 103.3155 - val_MinusLogProbMetric: 103.3155 - lr: 5.0805e-08 - 80s/epoch - 409ms/step
Epoch 596/1000
2023-10-26 17:36:25.591 
Epoch 596/1000 
	 loss: 103.0684, MinusLogProbMetric: 103.0684, val_loss: 103.3543, val_MinusLogProbMetric: 103.3543

Epoch 596: val_loss did not improve from 103.31549
196/196 - 81s - loss: 103.0684 - MinusLogProbMetric: 103.0684 - val_loss: 103.3543 - val_MinusLogProbMetric: 103.3543 - lr: 5.0805e-08 - 81s/epoch - 413ms/step
Epoch 597/1000
2023-10-26 17:37:48.154 
Epoch 597/1000 
	 loss: 103.0575, MinusLogProbMetric: 103.0575, val_loss: 103.3426, val_MinusLogProbMetric: 103.3426

Epoch 597: val_loss did not improve from 103.31549
196/196 - 83s - loss: 103.0575 - MinusLogProbMetric: 103.0575 - val_loss: 103.3426 - val_MinusLogProbMetric: 103.3426 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 598/1000
2023-10-26 17:39:10.154 
Epoch 598/1000 
	 loss: 103.0625, MinusLogProbMetric: 103.0625, val_loss: 103.3379, val_MinusLogProbMetric: 103.3379

Epoch 598: val_loss did not improve from 103.31549
196/196 - 82s - loss: 103.0625 - MinusLogProbMetric: 103.0625 - val_loss: 103.3379 - val_MinusLogProbMetric: 103.3379 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 599/1000
2023-10-26 17:40:31.783 
Epoch 599/1000 
	 loss: 103.0643, MinusLogProbMetric: 103.0643, val_loss: 103.3342, val_MinusLogProbMetric: 103.3342

Epoch 599: val_loss did not improve from 103.31549
196/196 - 82s - loss: 103.0643 - MinusLogProbMetric: 103.0643 - val_loss: 103.3342 - val_MinusLogProbMetric: 103.3342 - lr: 5.0805e-08 - 82s/epoch - 416ms/step
Epoch 600/1000
2023-10-26 17:41:53.537 
Epoch 600/1000 
	 loss: 103.0559, MinusLogProbMetric: 103.0559, val_loss: 103.3366, val_MinusLogProbMetric: 103.3366

Epoch 600: val_loss did not improve from 103.31549
196/196 - 82s - loss: 103.0559 - MinusLogProbMetric: 103.0559 - val_loss: 103.3366 - val_MinusLogProbMetric: 103.3366 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 601/1000
2023-10-26 17:43:16.165 
Epoch 601/1000 
	 loss: 103.0512, MinusLogProbMetric: 103.0512, val_loss: 103.3284, val_MinusLogProbMetric: 103.3284

Epoch 601: val_loss did not improve from 103.31549
196/196 - 83s - loss: 103.0512 - MinusLogProbMetric: 103.0512 - val_loss: 103.3284 - val_MinusLogProbMetric: 103.3284 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 602/1000
2023-10-26 17:44:27.599 
Epoch 602/1000 
	 loss: 103.0644, MinusLogProbMetric: 103.0644, val_loss: 103.3036, val_MinusLogProbMetric: 103.3036

Epoch 602: val_loss improved from 103.31549 to 103.30362, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 73s - loss: 103.0644 - MinusLogProbMetric: 103.0644 - val_loss: 103.3036 - val_MinusLogProbMetric: 103.3036 - lr: 5.0805e-08 - 73s/epoch - 370ms/step
Epoch 603/1000
2023-10-26 17:45:45.977 
Epoch 603/1000 
	 loss: 103.0432, MinusLogProbMetric: 103.0432, val_loss: 103.3312, val_MinusLogProbMetric: 103.3312

Epoch 603: val_loss did not improve from 103.30362
196/196 - 77s - loss: 103.0432 - MinusLogProbMetric: 103.0432 - val_loss: 103.3312 - val_MinusLogProbMetric: 103.3312 - lr: 5.0805e-08 - 77s/epoch - 394ms/step
Epoch 604/1000
2023-10-26 17:47:08.018 
Epoch 604/1000 
	 loss: 103.0484, MinusLogProbMetric: 103.0484, val_loss: 103.3181, val_MinusLogProbMetric: 103.3181

Epoch 604: val_loss did not improve from 103.30362
196/196 - 82s - loss: 103.0484 - MinusLogProbMetric: 103.0484 - val_loss: 103.3181 - val_MinusLogProbMetric: 103.3181 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 605/1000
2023-10-26 17:48:29.629 
Epoch 605/1000 
	 loss: 103.0397, MinusLogProbMetric: 103.0397, val_loss: 103.3147, val_MinusLogProbMetric: 103.3147

Epoch 605: val_loss did not improve from 103.30362
196/196 - 82s - loss: 103.0397 - MinusLogProbMetric: 103.0397 - val_loss: 103.3147 - val_MinusLogProbMetric: 103.3147 - lr: 5.0805e-08 - 82s/epoch - 416ms/step
Epoch 606/1000
2023-10-26 17:49:50.348 
Epoch 606/1000 
	 loss: 103.0441, MinusLogProbMetric: 103.0441, val_loss: 103.3094, val_MinusLogProbMetric: 103.3094

Epoch 606: val_loss did not improve from 103.30362
196/196 - 81s - loss: 103.0441 - MinusLogProbMetric: 103.0441 - val_loss: 103.3094 - val_MinusLogProbMetric: 103.3094 - lr: 5.0805e-08 - 81s/epoch - 412ms/step
Epoch 607/1000
2023-10-26 17:51:12.704 
Epoch 607/1000 
	 loss: 103.0392, MinusLogProbMetric: 103.0392, val_loss: 103.2924, val_MinusLogProbMetric: 103.2924

Epoch 607: val_loss improved from 103.30362 to 103.29241, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 103.0392 - MinusLogProbMetric: 103.0392 - val_loss: 103.2924 - val_MinusLogProbMetric: 103.2924 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 608/1000
2023-10-26 17:52:36.676 
Epoch 608/1000 
	 loss: 103.0312, MinusLogProbMetric: 103.0312, val_loss: 103.3040, val_MinusLogProbMetric: 103.3040

Epoch 608: val_loss did not improve from 103.29241
196/196 - 82s - loss: 103.0312 - MinusLogProbMetric: 103.0312 - val_loss: 103.3040 - val_MinusLogProbMetric: 103.3040 - lr: 5.0805e-08 - 82s/epoch - 421ms/step
Epoch 609/1000
2023-10-26 17:53:59.462 
Epoch 609/1000 
	 loss: 103.0354, MinusLogProbMetric: 103.0354, val_loss: 103.3079, val_MinusLogProbMetric: 103.3079

Epoch 609: val_loss did not improve from 103.29241
196/196 - 83s - loss: 103.0354 - MinusLogProbMetric: 103.0354 - val_loss: 103.3079 - val_MinusLogProbMetric: 103.3079 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 610/1000
2023-10-26 17:55:21.927 
Epoch 610/1000 
	 loss: 103.0196, MinusLogProbMetric: 103.0196, val_loss: 103.3099, val_MinusLogProbMetric: 103.3099

Epoch 610: val_loss did not improve from 103.29241
196/196 - 82s - loss: 103.0196 - MinusLogProbMetric: 103.0196 - val_loss: 103.3099 - val_MinusLogProbMetric: 103.3099 - lr: 5.0805e-08 - 82s/epoch - 421ms/step
Epoch 611/1000
2023-10-26 17:56:44.179 
Epoch 611/1000 
	 loss: 103.0247, MinusLogProbMetric: 103.0247, val_loss: 103.2976, val_MinusLogProbMetric: 103.2976

Epoch 611: val_loss did not improve from 103.29241
196/196 - 82s - loss: 103.0247 - MinusLogProbMetric: 103.0247 - val_loss: 103.2976 - val_MinusLogProbMetric: 103.2976 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 612/1000
2023-10-26 17:58:05.961 
Epoch 612/1000 
	 loss: 103.0229, MinusLogProbMetric: 103.0229, val_loss: 103.2944, val_MinusLogProbMetric: 103.2944

Epoch 612: val_loss did not improve from 103.29241
196/196 - 82s - loss: 103.0229 - MinusLogProbMetric: 103.0229 - val_loss: 103.2944 - val_MinusLogProbMetric: 103.2944 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 613/1000
2023-10-26 17:59:28.034 
Epoch 613/1000 
	 loss: 103.0171, MinusLogProbMetric: 103.0171, val_loss: 103.2744, val_MinusLogProbMetric: 103.2744

Epoch 613: val_loss improved from 103.29241 to 103.27440, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 103.0171 - MinusLogProbMetric: 103.0171 - val_loss: 103.2744 - val_MinusLogProbMetric: 103.2744 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 614/1000
2023-10-26 18:00:51.345 
Epoch 614/1000 
	 loss: 103.0171, MinusLogProbMetric: 103.0171, val_loss: 103.2905, val_MinusLogProbMetric: 103.2905

Epoch 614: val_loss did not improve from 103.27440
196/196 - 81s - loss: 103.0171 - MinusLogProbMetric: 103.0171 - val_loss: 103.2905 - val_MinusLogProbMetric: 103.2905 - lr: 5.0805e-08 - 81s/epoch - 415ms/step
Epoch 615/1000
2023-10-26 18:02:13.880 
Epoch 615/1000 
	 loss: 103.0090, MinusLogProbMetric: 103.0090, val_loss: 103.2907, val_MinusLogProbMetric: 103.2907

Epoch 615: val_loss did not improve from 103.27440
196/196 - 83s - loss: 103.0090 - MinusLogProbMetric: 103.0090 - val_loss: 103.2907 - val_MinusLogProbMetric: 103.2907 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 616/1000
2023-10-26 18:03:36.250 
Epoch 616/1000 
	 loss: 103.0054, MinusLogProbMetric: 103.0054, val_loss: 103.2967, val_MinusLogProbMetric: 103.2967

Epoch 616: val_loss did not improve from 103.27440
196/196 - 82s - loss: 103.0054 - MinusLogProbMetric: 103.0054 - val_loss: 103.2967 - val_MinusLogProbMetric: 103.2967 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 617/1000
2023-10-26 18:04:58.230 
Epoch 617/1000 
	 loss: 103.0068, MinusLogProbMetric: 103.0068, val_loss: 103.2528, val_MinusLogProbMetric: 103.2528

Epoch 617: val_loss improved from 103.27440 to 103.25284, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 103.0068 - MinusLogProbMetric: 103.0068 - val_loss: 103.2528 - val_MinusLogProbMetric: 103.2528 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 618/1000
2023-10-26 18:06:18.566 
Epoch 618/1000 
	 loss: 103.0100, MinusLogProbMetric: 103.0100, val_loss: 103.2826, val_MinusLogProbMetric: 103.2826

Epoch 618: val_loss did not improve from 103.25284
196/196 - 79s - loss: 103.0100 - MinusLogProbMetric: 103.0100 - val_loss: 103.2826 - val_MinusLogProbMetric: 103.2826 - lr: 5.0805e-08 - 79s/epoch - 404ms/step
Epoch 619/1000
2023-10-26 18:07:31.807 
Epoch 619/1000 
	 loss: 103.0065, MinusLogProbMetric: 103.0065, val_loss: 103.2839, val_MinusLogProbMetric: 103.2839

Epoch 619: val_loss did not improve from 103.25284
196/196 - 73s - loss: 103.0065 - MinusLogProbMetric: 103.0065 - val_loss: 103.2839 - val_MinusLogProbMetric: 103.2839 - lr: 5.0805e-08 - 73s/epoch - 374ms/step
Epoch 620/1000
2023-10-26 18:08:50.473 
Epoch 620/1000 
	 loss: 103.0013, MinusLogProbMetric: 103.0013, val_loss: 103.2720, val_MinusLogProbMetric: 103.2720

Epoch 620: val_loss did not improve from 103.25284
196/196 - 79s - loss: 103.0013 - MinusLogProbMetric: 103.0013 - val_loss: 103.2720 - val_MinusLogProbMetric: 103.2720 - lr: 5.0805e-08 - 79s/epoch - 401ms/step
Epoch 621/1000
2023-10-26 18:10:11.640 
Epoch 621/1000 
	 loss: 102.9908, MinusLogProbMetric: 102.9908, val_loss: 103.2739, val_MinusLogProbMetric: 103.2739

Epoch 621: val_loss did not improve from 103.25284
196/196 - 81s - loss: 102.9908 - MinusLogProbMetric: 102.9908 - val_loss: 103.2739 - val_MinusLogProbMetric: 103.2739 - lr: 5.0805e-08 - 81s/epoch - 414ms/step
Epoch 622/1000
2023-10-26 18:11:32.988 
Epoch 622/1000 
	 loss: 103.0040, MinusLogProbMetric: 103.0040, val_loss: 103.2737, val_MinusLogProbMetric: 103.2737

Epoch 622: val_loss did not improve from 103.25284
196/196 - 81s - loss: 103.0040 - MinusLogProbMetric: 103.0040 - val_loss: 103.2737 - val_MinusLogProbMetric: 103.2737 - lr: 5.0805e-08 - 81s/epoch - 415ms/step
Epoch 623/1000
2023-10-26 18:12:54.641 
Epoch 623/1000 
	 loss: 102.9908, MinusLogProbMetric: 102.9908, val_loss: 103.2539, val_MinusLogProbMetric: 103.2539

Epoch 623: val_loss did not improve from 103.25284
196/196 - 82s - loss: 102.9908 - MinusLogProbMetric: 102.9908 - val_loss: 103.2539 - val_MinusLogProbMetric: 103.2539 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 624/1000
2023-10-26 18:14:17.254 
Epoch 624/1000 
	 loss: 102.9848, MinusLogProbMetric: 102.9848, val_loss: 103.2584, val_MinusLogProbMetric: 103.2584

Epoch 624: val_loss did not improve from 103.25284
196/196 - 83s - loss: 102.9848 - MinusLogProbMetric: 102.9848 - val_loss: 103.2584 - val_MinusLogProbMetric: 103.2584 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 625/1000
2023-10-26 18:15:39.690 
Epoch 625/1000 
	 loss: 102.9917, MinusLogProbMetric: 102.9917, val_loss: 103.2395, val_MinusLogProbMetric: 103.2395

Epoch 625: val_loss improved from 103.25284 to 103.23951, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 102.9917 - MinusLogProbMetric: 102.9917 - val_loss: 103.2395 - val_MinusLogProbMetric: 103.2395 - lr: 5.0805e-08 - 85s/epoch - 431ms/step
Epoch 626/1000
2023-10-26 18:16:58.239 
Epoch 626/1000 
	 loss: 102.9784, MinusLogProbMetric: 102.9784, val_loss: 103.2499, val_MinusLogProbMetric: 103.2499

Epoch 626: val_loss did not improve from 103.23951
196/196 - 76s - loss: 102.9784 - MinusLogProbMetric: 102.9784 - val_loss: 103.2499 - val_MinusLogProbMetric: 103.2499 - lr: 5.0805e-08 - 76s/epoch - 390ms/step
Epoch 627/1000
2023-10-26 18:18:16.954 
Epoch 627/1000 
	 loss: 102.9843, MinusLogProbMetric: 102.9843, val_loss: 103.2581, val_MinusLogProbMetric: 103.2581

Epoch 627: val_loss did not improve from 103.23951
196/196 - 79s - loss: 102.9843 - MinusLogProbMetric: 102.9843 - val_loss: 103.2581 - val_MinusLogProbMetric: 103.2581 - lr: 5.0805e-08 - 79s/epoch - 402ms/step
Epoch 628/1000
2023-10-26 18:19:26.166 
Epoch 628/1000 
	 loss: 102.9837, MinusLogProbMetric: 102.9837, val_loss: 103.2577, val_MinusLogProbMetric: 103.2577

Epoch 628: val_loss did not improve from 103.23951
196/196 - 69s - loss: 102.9837 - MinusLogProbMetric: 102.9837 - val_loss: 103.2577 - val_MinusLogProbMetric: 103.2577 - lr: 5.0805e-08 - 69s/epoch - 353ms/step
Epoch 629/1000
2023-10-26 18:20:48.493 
Epoch 629/1000 
	 loss: 102.9689, MinusLogProbMetric: 102.9689, val_loss: 103.2474, val_MinusLogProbMetric: 103.2474

Epoch 629: val_loss did not improve from 103.23951
196/196 - 82s - loss: 102.9689 - MinusLogProbMetric: 102.9689 - val_loss: 103.2474 - val_MinusLogProbMetric: 103.2474 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 630/1000
2023-10-26 18:22:11.348 
Epoch 630/1000 
	 loss: 102.9703, MinusLogProbMetric: 102.9703, val_loss: 103.2417, val_MinusLogProbMetric: 103.2417

Epoch 630: val_loss did not improve from 103.23951
196/196 - 83s - loss: 102.9703 - MinusLogProbMetric: 102.9703 - val_loss: 103.2417 - val_MinusLogProbMetric: 103.2417 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 631/1000
2023-10-26 18:23:33.591 
Epoch 631/1000 
	 loss: 102.9637, MinusLogProbMetric: 102.9637, val_loss: 103.2480, val_MinusLogProbMetric: 103.2480

Epoch 631: val_loss did not improve from 103.23951
196/196 - 82s - loss: 102.9637 - MinusLogProbMetric: 102.9637 - val_loss: 103.2480 - val_MinusLogProbMetric: 103.2480 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 632/1000
2023-10-26 18:24:56.243 
Epoch 632/1000 
	 loss: 102.9578, MinusLogProbMetric: 102.9578, val_loss: 103.2343, val_MinusLogProbMetric: 103.2343

Epoch 632: val_loss improved from 103.23951 to 103.23430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.9578 - MinusLogProbMetric: 102.9578 - val_loss: 103.2343 - val_MinusLogProbMetric: 103.2343 - lr: 5.0805e-08 - 84s/epoch - 431ms/step
Epoch 633/1000
2023-10-26 18:26:20.267 
Epoch 633/1000 
	 loss: 102.9747, MinusLogProbMetric: 102.9747, val_loss: 103.2237, val_MinusLogProbMetric: 103.2237

Epoch 633: val_loss improved from 103.23430 to 103.22369, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.9747 - MinusLogProbMetric: 102.9747 - val_loss: 103.2237 - val_MinusLogProbMetric: 103.2237 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 634/1000
2023-10-26 18:27:45.079 
Epoch 634/1000 
	 loss: 102.9558, MinusLogProbMetric: 102.9558, val_loss: 103.2096, val_MinusLogProbMetric: 103.2096

Epoch 634: val_loss improved from 103.22369 to 103.20965, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 102.9558 - MinusLogProbMetric: 102.9558 - val_loss: 103.2096 - val_MinusLogProbMetric: 103.2096 - lr: 5.0805e-08 - 85s/epoch - 435ms/step
Epoch 635/1000
2023-10-26 18:29:09.248 
Epoch 635/1000 
	 loss: 102.9575, MinusLogProbMetric: 102.9575, val_loss: 103.2051, val_MinusLogProbMetric: 103.2051

Epoch 635: val_loss improved from 103.20965 to 103.20506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.9575 - MinusLogProbMetric: 102.9575 - val_loss: 103.2051 - val_MinusLogProbMetric: 103.2051 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 636/1000
2023-10-26 18:30:32.675 
Epoch 636/1000 
	 loss: 102.9556, MinusLogProbMetric: 102.9556, val_loss: 103.2213, val_MinusLogProbMetric: 103.2213

Epoch 636: val_loss did not improve from 103.20506
196/196 - 81s - loss: 102.9556 - MinusLogProbMetric: 102.9556 - val_loss: 103.2213 - val_MinusLogProbMetric: 103.2213 - lr: 5.0805e-08 - 81s/epoch - 416ms/step
Epoch 637/1000
2023-10-26 18:31:54.952 
Epoch 637/1000 
	 loss: 102.9516, MinusLogProbMetric: 102.9516, val_loss: 103.2044, val_MinusLogProbMetric: 103.2044

Epoch 637: val_loss improved from 103.20506 to 103.20436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.9516 - MinusLogProbMetric: 102.9516 - val_loss: 103.2044 - val_MinusLogProbMetric: 103.2044 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 638/1000
2023-10-26 18:33:18.467 
Epoch 638/1000 
	 loss: 102.9447, MinusLogProbMetric: 102.9447, val_loss: 103.2178, val_MinusLogProbMetric: 103.2178

Epoch 638: val_loss did not improve from 103.20436
196/196 - 82s - loss: 102.9447 - MinusLogProbMetric: 102.9447 - val_loss: 103.2178 - val_MinusLogProbMetric: 103.2178 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 639/1000
2023-10-26 18:34:40.212 
Epoch 639/1000 
	 loss: 102.9367, MinusLogProbMetric: 102.9367, val_loss: 103.2097, val_MinusLogProbMetric: 103.2097

Epoch 639: val_loss did not improve from 103.20436
196/196 - 82s - loss: 102.9367 - MinusLogProbMetric: 102.9367 - val_loss: 103.2097 - val_MinusLogProbMetric: 103.2097 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 640/1000
2023-10-26 18:36:02.240 
Epoch 640/1000 
	 loss: 102.9352, MinusLogProbMetric: 102.9352, val_loss: 103.2093, val_MinusLogProbMetric: 103.2093

Epoch 640: val_loss did not improve from 103.20436
196/196 - 82s - loss: 102.9352 - MinusLogProbMetric: 102.9352 - val_loss: 103.2093 - val_MinusLogProbMetric: 103.2093 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 641/1000
2023-10-26 18:37:24.566 
Epoch 641/1000 
	 loss: 102.9304, MinusLogProbMetric: 102.9304, val_loss: 103.1923, val_MinusLogProbMetric: 103.1923

Epoch 641: val_loss improved from 103.20436 to 103.19230, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.9304 - MinusLogProbMetric: 102.9304 - val_loss: 103.1923 - val_MinusLogProbMetric: 103.1923 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 642/1000
2023-10-26 18:38:48.651 
Epoch 642/1000 
	 loss: 102.9318, MinusLogProbMetric: 102.9318, val_loss: 103.2028, val_MinusLogProbMetric: 103.2028

Epoch 642: val_loss did not improve from 103.19230
196/196 - 83s - loss: 102.9318 - MinusLogProbMetric: 102.9318 - val_loss: 103.2028 - val_MinusLogProbMetric: 103.2028 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 643/1000
2023-10-26 18:40:10.447 
Epoch 643/1000 
	 loss: 102.9295, MinusLogProbMetric: 102.9295, val_loss: 103.2247, val_MinusLogProbMetric: 103.2247

Epoch 643: val_loss did not improve from 103.19230
196/196 - 82s - loss: 102.9295 - MinusLogProbMetric: 102.9295 - val_loss: 103.2247 - val_MinusLogProbMetric: 103.2247 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 644/1000
2023-10-26 18:41:32.968 
Epoch 644/1000 
	 loss: 102.9278, MinusLogProbMetric: 102.9278, val_loss: 103.2030, val_MinusLogProbMetric: 103.2030

Epoch 644: val_loss did not improve from 103.19230
196/196 - 83s - loss: 102.9278 - MinusLogProbMetric: 102.9278 - val_loss: 103.2030 - val_MinusLogProbMetric: 103.2030 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 645/1000
2023-10-26 18:42:55.566 
Epoch 645/1000 
	 loss: 102.9309, MinusLogProbMetric: 102.9309, val_loss: 103.2149, val_MinusLogProbMetric: 103.2149

Epoch 645: val_loss did not improve from 103.19230
196/196 - 83s - loss: 102.9309 - MinusLogProbMetric: 102.9309 - val_loss: 103.2149 - val_MinusLogProbMetric: 103.2149 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 646/1000
2023-10-26 18:44:17.696 
Epoch 646/1000 
	 loss: 102.9166, MinusLogProbMetric: 102.9166, val_loss: 103.2028, val_MinusLogProbMetric: 103.2028

Epoch 646: val_loss did not improve from 103.19230
196/196 - 82s - loss: 102.9166 - MinusLogProbMetric: 102.9166 - val_loss: 103.2028 - val_MinusLogProbMetric: 103.2028 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 647/1000
2023-10-26 18:45:40.602 
Epoch 647/1000 
	 loss: 102.9299, MinusLogProbMetric: 102.9299, val_loss: 103.1943, val_MinusLogProbMetric: 103.1943

Epoch 647: val_loss did not improve from 103.19230
196/196 - 83s - loss: 102.9299 - MinusLogProbMetric: 102.9299 - val_loss: 103.1943 - val_MinusLogProbMetric: 103.1943 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 648/1000
2023-10-26 18:47:02.860 
Epoch 648/1000 
	 loss: 102.9088, MinusLogProbMetric: 102.9088, val_loss: 103.1990, val_MinusLogProbMetric: 103.1990

Epoch 648: val_loss did not improve from 103.19230
196/196 - 82s - loss: 102.9088 - MinusLogProbMetric: 102.9088 - val_loss: 103.1990 - val_MinusLogProbMetric: 103.1990 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 649/1000
2023-10-26 18:48:25.311 
Epoch 649/1000 
	 loss: 102.9136, MinusLogProbMetric: 102.9136, val_loss: 103.1762, val_MinusLogProbMetric: 103.1762

Epoch 649: val_loss improved from 103.19230 to 103.17625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.9136 - MinusLogProbMetric: 102.9136 - val_loss: 103.1762 - val_MinusLogProbMetric: 103.1762 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 650/1000
2023-10-26 18:49:48.867 
Epoch 650/1000 
	 loss: 102.9192, MinusLogProbMetric: 102.9192, val_loss: 103.1752, val_MinusLogProbMetric: 103.1752

Epoch 650: val_loss improved from 103.17625 to 103.17519, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.9192 - MinusLogProbMetric: 102.9192 - val_loss: 103.1752 - val_MinusLogProbMetric: 103.1752 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 651/1000
2023-10-26 18:51:12.503 
Epoch 651/1000 
	 loss: 102.9110, MinusLogProbMetric: 102.9110, val_loss: 103.1651, val_MinusLogProbMetric: 103.1651

Epoch 651: val_loss improved from 103.17519 to 103.16510, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.9110 - MinusLogProbMetric: 102.9110 - val_loss: 103.1651 - val_MinusLogProbMetric: 103.1651 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 652/1000
2023-10-26 18:52:36.940 
Epoch 652/1000 
	 loss: 102.9072, MinusLogProbMetric: 102.9072, val_loss: 103.1810, val_MinusLogProbMetric: 103.1810

Epoch 652: val_loss did not improve from 103.16510
196/196 - 83s - loss: 102.9072 - MinusLogProbMetric: 102.9072 - val_loss: 103.1810 - val_MinusLogProbMetric: 103.1810 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 653/1000
2023-10-26 18:53:58.886 
Epoch 653/1000 
	 loss: 102.9013, MinusLogProbMetric: 102.9013, val_loss: 103.1732, val_MinusLogProbMetric: 103.1732

Epoch 653: val_loss did not improve from 103.16510
196/196 - 82s - loss: 102.9013 - MinusLogProbMetric: 102.9013 - val_loss: 103.1732 - val_MinusLogProbMetric: 103.1732 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 654/1000
2023-10-26 18:55:21.163 
Epoch 654/1000 
	 loss: 102.9019, MinusLogProbMetric: 102.9019, val_loss: 103.1736, val_MinusLogProbMetric: 103.1736

Epoch 654: val_loss did not improve from 103.16510
196/196 - 82s - loss: 102.9019 - MinusLogProbMetric: 102.9019 - val_loss: 103.1736 - val_MinusLogProbMetric: 103.1736 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 655/1000
2023-10-26 18:56:42.883 
Epoch 655/1000 
	 loss: 102.8869, MinusLogProbMetric: 102.8869, val_loss: 103.1430, val_MinusLogProbMetric: 103.1430

Epoch 655: val_loss improved from 103.16510 to 103.14304, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.8869 - MinusLogProbMetric: 102.8869 - val_loss: 103.1430 - val_MinusLogProbMetric: 103.1430 - lr: 5.0805e-08 - 84s/epoch - 426ms/step
Epoch 656/1000
2023-10-26 18:58:07.146 
Epoch 656/1000 
	 loss: 102.9015, MinusLogProbMetric: 102.9015, val_loss: 103.1478, val_MinusLogProbMetric: 103.1478

Epoch 656: val_loss did not improve from 103.14304
196/196 - 82s - loss: 102.9015 - MinusLogProbMetric: 102.9015 - val_loss: 103.1478 - val_MinusLogProbMetric: 103.1478 - lr: 5.0805e-08 - 82s/epoch - 421ms/step
Epoch 657/1000
2023-10-26 18:59:29.143 
Epoch 657/1000 
	 loss: 102.8965, MinusLogProbMetric: 102.8965, val_loss: 103.1514, val_MinusLogProbMetric: 103.1514

Epoch 657: val_loss did not improve from 103.14304
196/196 - 82s - loss: 102.8965 - MinusLogProbMetric: 102.8965 - val_loss: 103.1514 - val_MinusLogProbMetric: 103.1514 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 658/1000
2023-10-26 19:00:51.059 
Epoch 658/1000 
	 loss: 102.8925, MinusLogProbMetric: 102.8925, val_loss: 103.1371, val_MinusLogProbMetric: 103.1371

Epoch 658: val_loss improved from 103.14304 to 103.13710, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.8925 - MinusLogProbMetric: 102.8925 - val_loss: 103.1371 - val_MinusLogProbMetric: 103.1371 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 659/1000
2023-10-26 19:02:15.459 
Epoch 659/1000 
	 loss: 102.8823, MinusLogProbMetric: 102.8823, val_loss: 103.1613, val_MinusLogProbMetric: 103.1613

Epoch 659: val_loss did not improve from 103.13710
196/196 - 82s - loss: 102.8823 - MinusLogProbMetric: 102.8823 - val_loss: 103.1613 - val_MinusLogProbMetric: 103.1613 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 660/1000
2023-10-26 19:03:37.818 
Epoch 660/1000 
	 loss: 102.8924, MinusLogProbMetric: 102.8924, val_loss: 103.1429, val_MinusLogProbMetric: 103.1429

Epoch 660: val_loss did not improve from 103.13710
196/196 - 82s - loss: 102.8924 - MinusLogProbMetric: 102.8924 - val_loss: 103.1429 - val_MinusLogProbMetric: 103.1429 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 661/1000
2023-10-26 19:05:00.699 
Epoch 661/1000 
	 loss: 102.8902, MinusLogProbMetric: 102.8902, val_loss: 103.1457, val_MinusLogProbMetric: 103.1457

Epoch 661: val_loss did not improve from 103.13710
196/196 - 83s - loss: 102.8902 - MinusLogProbMetric: 102.8902 - val_loss: 103.1457 - val_MinusLogProbMetric: 103.1457 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 662/1000
2023-10-26 19:06:21.986 
Epoch 662/1000 
	 loss: 102.8831, MinusLogProbMetric: 102.8831, val_loss: 103.1424, val_MinusLogProbMetric: 103.1424

Epoch 662: val_loss did not improve from 103.13710
196/196 - 81s - loss: 102.8831 - MinusLogProbMetric: 102.8831 - val_loss: 103.1424 - val_MinusLogProbMetric: 103.1424 - lr: 5.0805e-08 - 81s/epoch - 415ms/step
Epoch 663/1000
2023-10-26 19:07:40.116 
Epoch 663/1000 
	 loss: 102.8833, MinusLogProbMetric: 102.8833, val_loss: 103.1460, val_MinusLogProbMetric: 103.1460

Epoch 663: val_loss did not improve from 103.13710
196/196 - 78s - loss: 102.8833 - MinusLogProbMetric: 102.8833 - val_loss: 103.1460 - val_MinusLogProbMetric: 103.1460 - lr: 5.0805e-08 - 78s/epoch - 399ms/step
Epoch 664/1000
2023-10-26 19:08:53.743 
Epoch 664/1000 
	 loss: 102.8753, MinusLogProbMetric: 102.8753, val_loss: 103.1525, val_MinusLogProbMetric: 103.1525

Epoch 664: val_loss did not improve from 103.13710
196/196 - 74s - loss: 102.8753 - MinusLogProbMetric: 102.8753 - val_loss: 103.1525 - val_MinusLogProbMetric: 103.1525 - lr: 5.0805e-08 - 74s/epoch - 376ms/step
Epoch 665/1000
2023-10-26 19:10:14.405 
Epoch 665/1000 
	 loss: 102.8773, MinusLogProbMetric: 102.8773, val_loss: 103.1570, val_MinusLogProbMetric: 103.1570

Epoch 665: val_loss did not improve from 103.13710
196/196 - 81s - loss: 102.8773 - MinusLogProbMetric: 102.8773 - val_loss: 103.1570 - val_MinusLogProbMetric: 103.1570 - lr: 5.0805e-08 - 81s/epoch - 412ms/step
Epoch 666/1000
2023-10-26 19:11:35.043 
Epoch 666/1000 
	 loss: 102.8711, MinusLogProbMetric: 102.8711, val_loss: 103.1590, val_MinusLogProbMetric: 103.1590

Epoch 666: val_loss did not improve from 103.13710
196/196 - 81s - loss: 102.8711 - MinusLogProbMetric: 102.8711 - val_loss: 103.1590 - val_MinusLogProbMetric: 103.1590 - lr: 5.0805e-08 - 81s/epoch - 411ms/step
Epoch 667/1000
2023-10-26 19:12:58.065 
Epoch 667/1000 
	 loss: 102.8756, MinusLogProbMetric: 102.8756, val_loss: 103.1665, val_MinusLogProbMetric: 103.1665

Epoch 667: val_loss did not improve from 103.13710
196/196 - 83s - loss: 102.8756 - MinusLogProbMetric: 102.8756 - val_loss: 103.1665 - val_MinusLogProbMetric: 103.1665 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 668/1000
2023-10-26 19:14:20.977 
Epoch 668/1000 
	 loss: 102.8720, MinusLogProbMetric: 102.8720, val_loss: 103.1359, val_MinusLogProbMetric: 103.1359

Epoch 668: val_loss improved from 103.13710 to 103.13587, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.8720 - MinusLogProbMetric: 102.8720 - val_loss: 103.1359 - val_MinusLogProbMetric: 103.1359 - lr: 5.0805e-08 - 84s/epoch - 431ms/step
Epoch 669/1000
2023-10-26 19:15:45.479 
Epoch 669/1000 
	 loss: 102.8644, MinusLogProbMetric: 102.8644, val_loss: 103.1455, val_MinusLogProbMetric: 103.1455

Epoch 669: val_loss did not improve from 103.13587
196/196 - 83s - loss: 102.8644 - MinusLogProbMetric: 102.8644 - val_loss: 103.1455 - val_MinusLogProbMetric: 103.1455 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 670/1000
2023-10-26 19:17:08.473 
Epoch 670/1000 
	 loss: 102.8605, MinusLogProbMetric: 102.8605, val_loss: 103.1593, val_MinusLogProbMetric: 103.1593

Epoch 670: val_loss did not improve from 103.13587
196/196 - 83s - loss: 102.8605 - MinusLogProbMetric: 102.8605 - val_loss: 103.1593 - val_MinusLogProbMetric: 103.1593 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 671/1000
2023-10-26 19:18:30.837 
Epoch 671/1000 
	 loss: 102.8608, MinusLogProbMetric: 102.8608, val_loss: 103.1479, val_MinusLogProbMetric: 103.1479

Epoch 671: val_loss did not improve from 103.13587
196/196 - 82s - loss: 102.8608 - MinusLogProbMetric: 102.8608 - val_loss: 103.1479 - val_MinusLogProbMetric: 103.1479 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 672/1000
2023-10-26 19:19:53.630 
Epoch 672/1000 
	 loss: 102.8552, MinusLogProbMetric: 102.8552, val_loss: 103.1266, val_MinusLogProbMetric: 103.1266

Epoch 672: val_loss improved from 103.13587 to 103.12663, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 102.8552 - MinusLogProbMetric: 102.8552 - val_loss: 103.1266 - val_MinusLogProbMetric: 103.1266 - lr: 5.0805e-08 - 85s/epoch - 432ms/step
Epoch 673/1000
2023-10-26 19:21:18.936 
Epoch 673/1000 
	 loss: 102.8500, MinusLogProbMetric: 102.8500, val_loss: 103.1420, val_MinusLogProbMetric: 103.1420

Epoch 673: val_loss did not improve from 103.12663
196/196 - 84s - loss: 102.8500 - MinusLogProbMetric: 102.8500 - val_loss: 103.1420 - val_MinusLogProbMetric: 103.1420 - lr: 5.0805e-08 - 84s/epoch - 426ms/step
Epoch 674/1000
2023-10-26 19:22:41.968 
Epoch 674/1000 
	 loss: 102.8576, MinusLogProbMetric: 102.8576, val_loss: 103.1178, val_MinusLogProbMetric: 103.1178

Epoch 674: val_loss improved from 103.12663 to 103.11781, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 102.8576 - MinusLogProbMetric: 102.8576 - val_loss: 103.1178 - val_MinusLogProbMetric: 103.1178 - lr: 5.0805e-08 - 85s/epoch - 431ms/step
Epoch 675/1000
2023-10-26 19:24:05.789 
Epoch 675/1000 
	 loss: 102.8474, MinusLogProbMetric: 102.8474, val_loss: 103.1273, val_MinusLogProbMetric: 103.1273

Epoch 675: val_loss did not improve from 103.11781
196/196 - 82s - loss: 102.8474 - MinusLogProbMetric: 102.8474 - val_loss: 103.1273 - val_MinusLogProbMetric: 103.1273 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 676/1000
2023-10-26 19:25:28.910 
Epoch 676/1000 
	 loss: 102.8544, MinusLogProbMetric: 102.8544, val_loss: 103.1168, val_MinusLogProbMetric: 103.1168

Epoch 676: val_loss improved from 103.11781 to 103.11678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 102.8544 - MinusLogProbMetric: 102.8544 - val_loss: 103.1168 - val_MinusLogProbMetric: 103.1168 - lr: 5.0805e-08 - 85s/epoch - 432ms/step
Epoch 677/1000
2023-10-26 19:26:52.881 
Epoch 677/1000 
	 loss: 102.8442, MinusLogProbMetric: 102.8442, val_loss: 103.1135, val_MinusLogProbMetric: 103.1135

Epoch 677: val_loss improved from 103.11678 to 103.11346, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.8442 - MinusLogProbMetric: 102.8442 - val_loss: 103.1135 - val_MinusLogProbMetric: 103.1135 - lr: 5.0805e-08 - 84s/epoch - 431ms/step
Epoch 678/1000
2023-10-26 19:28:17.647 
Epoch 678/1000 
	 loss: 102.8485, MinusLogProbMetric: 102.8485, val_loss: 103.1466, val_MinusLogProbMetric: 103.1466

Epoch 678: val_loss did not improve from 103.11346
196/196 - 83s - loss: 102.8485 - MinusLogProbMetric: 102.8485 - val_loss: 103.1466 - val_MinusLogProbMetric: 103.1466 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 679/1000
2023-10-26 19:29:40.568 
Epoch 679/1000 
	 loss: 102.8414, MinusLogProbMetric: 102.8414, val_loss: 103.1166, val_MinusLogProbMetric: 103.1166

Epoch 679: val_loss did not improve from 103.11346
196/196 - 83s - loss: 102.8414 - MinusLogProbMetric: 102.8414 - val_loss: 103.1166 - val_MinusLogProbMetric: 103.1166 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 680/1000
2023-10-26 19:30:47.538 
Epoch 680/1000 
	 loss: 102.8469, MinusLogProbMetric: 102.8469, val_loss: 103.0938, val_MinusLogProbMetric: 103.0938

Epoch 680: val_loss improved from 103.11346 to 103.09384, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 68s - loss: 102.8469 - MinusLogProbMetric: 102.8469 - val_loss: 103.0938 - val_MinusLogProbMetric: 103.0938 - lr: 5.0805e-08 - 68s/epoch - 349ms/step
Epoch 681/1000
2023-10-26 19:32:09.800 
Epoch 681/1000 
	 loss: 102.8426, MinusLogProbMetric: 102.8426, val_loss: 103.1116, val_MinusLogProbMetric: 103.1116

Epoch 681: val_loss did not improve from 103.09384
196/196 - 81s - loss: 102.8426 - MinusLogProbMetric: 102.8426 - val_loss: 103.1116 - val_MinusLogProbMetric: 103.1116 - lr: 5.0805e-08 - 81s/epoch - 412ms/step
Epoch 682/1000
2023-10-26 19:33:32.334 
Epoch 682/1000 
	 loss: 102.8354, MinusLogProbMetric: 102.8354, val_loss: 103.1024, val_MinusLogProbMetric: 103.1024

Epoch 682: val_loss did not improve from 103.09384
196/196 - 83s - loss: 102.8354 - MinusLogProbMetric: 102.8354 - val_loss: 103.1024 - val_MinusLogProbMetric: 103.1024 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 683/1000
2023-10-26 19:34:55.455 
Epoch 683/1000 
	 loss: 102.8357, MinusLogProbMetric: 102.8357, val_loss: 103.1166, val_MinusLogProbMetric: 103.1166

Epoch 683: val_loss did not improve from 103.09384
196/196 - 83s - loss: 102.8357 - MinusLogProbMetric: 102.8357 - val_loss: 103.1166 - val_MinusLogProbMetric: 103.1166 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 684/1000
2023-10-26 19:36:17.530 
Epoch 684/1000 
	 loss: 102.8344, MinusLogProbMetric: 102.8344, val_loss: 103.0985, val_MinusLogProbMetric: 103.0985

Epoch 684: val_loss did not improve from 103.09384
196/196 - 82s - loss: 102.8344 - MinusLogProbMetric: 102.8344 - val_loss: 103.0985 - val_MinusLogProbMetric: 103.0985 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 685/1000
2023-10-26 19:37:39.768 
Epoch 685/1000 
	 loss: 102.8360, MinusLogProbMetric: 102.8360, val_loss: 103.1001, val_MinusLogProbMetric: 103.1001

Epoch 685: val_loss did not improve from 103.09384
196/196 - 82s - loss: 102.8360 - MinusLogProbMetric: 102.8360 - val_loss: 103.1001 - val_MinusLogProbMetric: 103.1001 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 686/1000
2023-10-26 19:39:02.221 
Epoch 686/1000 
	 loss: 102.8270, MinusLogProbMetric: 102.8270, val_loss: 103.1093, val_MinusLogProbMetric: 103.1093

Epoch 686: val_loss did not improve from 103.09384
196/196 - 82s - loss: 102.8270 - MinusLogProbMetric: 102.8270 - val_loss: 103.1093 - val_MinusLogProbMetric: 103.1093 - lr: 5.0805e-08 - 82s/epoch - 421ms/step
Epoch 687/1000
2023-10-26 19:40:24.370 
Epoch 687/1000 
	 loss: 102.8386, MinusLogProbMetric: 102.8386, val_loss: 103.0909, val_MinusLogProbMetric: 103.0909

Epoch 687: val_loss improved from 103.09384 to 103.09093, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.8386 - MinusLogProbMetric: 102.8386 - val_loss: 103.0909 - val_MinusLogProbMetric: 103.0909 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 688/1000
2023-10-26 19:41:48.136 
Epoch 688/1000 
	 loss: 102.8294, MinusLogProbMetric: 102.8294, val_loss: 103.1136, val_MinusLogProbMetric: 103.1136

Epoch 688: val_loss did not improve from 103.09093
196/196 - 82s - loss: 102.8294 - MinusLogProbMetric: 102.8294 - val_loss: 103.1136 - val_MinusLogProbMetric: 103.1136 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 689/1000
2023-10-26 19:43:10.112 
Epoch 689/1000 
	 loss: 102.8209, MinusLogProbMetric: 102.8209, val_loss: 103.0781, val_MinusLogProbMetric: 103.0781

Epoch 689: val_loss improved from 103.09093 to 103.07809, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.8209 - MinusLogProbMetric: 102.8209 - val_loss: 103.0781 - val_MinusLogProbMetric: 103.0781 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 690/1000
2023-10-26 19:44:33.225 
Epoch 690/1000 
	 loss: 102.8105, MinusLogProbMetric: 102.8105, val_loss: 103.0627, val_MinusLogProbMetric: 103.0627

Epoch 690: val_loss improved from 103.07809 to 103.06268, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.8105 - MinusLogProbMetric: 102.8105 - val_loss: 103.0627 - val_MinusLogProbMetric: 103.0627 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 691/1000
2023-10-26 19:45:57.241 
Epoch 691/1000 
	 loss: 102.8042, MinusLogProbMetric: 102.8042, val_loss: 103.0791, val_MinusLogProbMetric: 103.0791

Epoch 691: val_loss did not improve from 103.06268
196/196 - 82s - loss: 102.8042 - MinusLogProbMetric: 102.8042 - val_loss: 103.0791 - val_MinusLogProbMetric: 103.0791 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 692/1000
2023-10-26 19:47:19.429 
Epoch 692/1000 
	 loss: 102.8190, MinusLogProbMetric: 102.8190, val_loss: 103.0968, val_MinusLogProbMetric: 103.0968

Epoch 692: val_loss did not improve from 103.06268
196/196 - 82s - loss: 102.8190 - MinusLogProbMetric: 102.8190 - val_loss: 103.0968 - val_MinusLogProbMetric: 103.0968 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 693/1000
2023-10-26 19:48:42.175 
Epoch 693/1000 
	 loss: 102.8062, MinusLogProbMetric: 102.8062, val_loss: 103.0770, val_MinusLogProbMetric: 103.0770

Epoch 693: val_loss did not improve from 103.06268
196/196 - 83s - loss: 102.8062 - MinusLogProbMetric: 102.8062 - val_loss: 103.0770 - val_MinusLogProbMetric: 103.0770 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 694/1000
2023-10-26 19:50:04.221 
Epoch 694/1000 
	 loss: 102.8131, MinusLogProbMetric: 102.8131, val_loss: 103.0816, val_MinusLogProbMetric: 103.0816

Epoch 694: val_loss did not improve from 103.06268
196/196 - 82s - loss: 102.8131 - MinusLogProbMetric: 102.8131 - val_loss: 103.0816 - val_MinusLogProbMetric: 103.0816 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 695/1000
2023-10-26 19:51:26.245 
Epoch 695/1000 
	 loss: 102.8105, MinusLogProbMetric: 102.8105, val_loss: 103.0699, val_MinusLogProbMetric: 103.0699

Epoch 695: val_loss did not improve from 103.06268
196/196 - 82s - loss: 102.8105 - MinusLogProbMetric: 102.8105 - val_loss: 103.0699 - val_MinusLogProbMetric: 103.0699 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 696/1000
2023-10-26 19:52:48.034 
Epoch 696/1000 
	 loss: 102.7983, MinusLogProbMetric: 102.7983, val_loss: 103.0771, val_MinusLogProbMetric: 103.0771

Epoch 696: val_loss did not improve from 103.06268
196/196 - 82s - loss: 102.7983 - MinusLogProbMetric: 102.7983 - val_loss: 103.0771 - val_MinusLogProbMetric: 103.0771 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 697/1000
2023-10-26 19:54:10.127 
Epoch 697/1000 
	 loss: 102.7990, MinusLogProbMetric: 102.7990, val_loss: 103.0958, val_MinusLogProbMetric: 103.0958

Epoch 697: val_loss did not improve from 103.06268
196/196 - 82s - loss: 102.7990 - MinusLogProbMetric: 102.7990 - val_loss: 103.0958 - val_MinusLogProbMetric: 103.0958 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 698/1000
2023-10-26 19:55:32.662 
Epoch 698/1000 
	 loss: 102.8022, MinusLogProbMetric: 102.8022, val_loss: 103.0795, val_MinusLogProbMetric: 103.0795

Epoch 698: val_loss did not improve from 103.06268
196/196 - 83s - loss: 102.8022 - MinusLogProbMetric: 102.8022 - val_loss: 103.0795 - val_MinusLogProbMetric: 103.0795 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 699/1000
2023-10-26 19:56:54.995 
Epoch 699/1000 
	 loss: 102.7983, MinusLogProbMetric: 102.7983, val_loss: 103.0711, val_MinusLogProbMetric: 103.0711

Epoch 699: val_loss did not improve from 103.06268
196/196 - 82s - loss: 102.7983 - MinusLogProbMetric: 102.7983 - val_loss: 103.0711 - val_MinusLogProbMetric: 103.0711 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 700/1000
2023-10-26 19:58:17.334 
Epoch 700/1000 
	 loss: 102.7963, MinusLogProbMetric: 102.7963, val_loss: 103.0631, val_MinusLogProbMetric: 103.0631

Epoch 700: val_loss did not improve from 103.06268
196/196 - 82s - loss: 102.7963 - MinusLogProbMetric: 102.7963 - val_loss: 103.0631 - val_MinusLogProbMetric: 103.0631 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 701/1000
2023-10-26 19:59:39.204 
Epoch 701/1000 
	 loss: 102.7948, MinusLogProbMetric: 102.7948, val_loss: 103.1015, val_MinusLogProbMetric: 103.1015

Epoch 701: val_loss did not improve from 103.06268
196/196 - 82s - loss: 102.7948 - MinusLogProbMetric: 102.7948 - val_loss: 103.1015 - val_MinusLogProbMetric: 103.1015 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 702/1000
2023-10-26 20:01:00.498 
Epoch 702/1000 
	 loss: 102.7818, MinusLogProbMetric: 102.7818, val_loss: 103.0800, val_MinusLogProbMetric: 103.0800

Epoch 702: val_loss did not improve from 103.06268
196/196 - 81s - loss: 102.7818 - MinusLogProbMetric: 102.7818 - val_loss: 103.0800 - val_MinusLogProbMetric: 103.0800 - lr: 5.0805e-08 - 81s/epoch - 415ms/step
Epoch 703/1000
2023-10-26 20:02:22.348 
Epoch 703/1000 
	 loss: 102.7844, MinusLogProbMetric: 102.7844, val_loss: 103.0653, val_MinusLogProbMetric: 103.0653

Epoch 703: val_loss did not improve from 103.06268
196/196 - 82s - loss: 102.7844 - MinusLogProbMetric: 102.7844 - val_loss: 103.0653 - val_MinusLogProbMetric: 103.0653 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 704/1000
2023-10-26 20:03:44.452 
Epoch 704/1000 
	 loss: 102.7704, MinusLogProbMetric: 102.7704, val_loss: 103.0842, val_MinusLogProbMetric: 103.0842

Epoch 704: val_loss did not improve from 103.06268
196/196 - 82s - loss: 102.7704 - MinusLogProbMetric: 102.7704 - val_loss: 103.0842 - val_MinusLogProbMetric: 103.0842 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 705/1000
2023-10-26 20:05:06.952 
Epoch 705/1000 
	 loss: 102.7837, MinusLogProbMetric: 102.7837, val_loss: 103.0520, val_MinusLogProbMetric: 103.0520

Epoch 705: val_loss improved from 103.06268 to 103.05203, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 102.7837 - MinusLogProbMetric: 102.7837 - val_loss: 103.0520 - val_MinusLogProbMetric: 103.0520 - lr: 5.0805e-08 - 85s/epoch - 431ms/step
Epoch 706/1000
2023-10-26 20:06:31.801 
Epoch 706/1000 
	 loss: 102.7779, MinusLogProbMetric: 102.7779, val_loss: 103.0737, val_MinusLogProbMetric: 103.0737

Epoch 706: val_loss did not improve from 103.05203
196/196 - 83s - loss: 102.7779 - MinusLogProbMetric: 102.7779 - val_loss: 103.0737 - val_MinusLogProbMetric: 103.0737 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 707/1000
2023-10-26 20:07:54.388 
Epoch 707/1000 
	 loss: 102.7766, MinusLogProbMetric: 102.7766, val_loss: 103.0415, val_MinusLogProbMetric: 103.0415

Epoch 707: val_loss improved from 103.05203 to 103.04147, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.7766 - MinusLogProbMetric: 102.7766 - val_loss: 103.0415 - val_MinusLogProbMetric: 103.0415 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 708/1000
2023-10-26 20:09:18.541 
Epoch 708/1000 
	 loss: 102.7802, MinusLogProbMetric: 102.7802, val_loss: 103.0416, val_MinusLogProbMetric: 103.0416

Epoch 708: val_loss did not improve from 103.04147
196/196 - 83s - loss: 102.7802 - MinusLogProbMetric: 102.7802 - val_loss: 103.0416 - val_MinusLogProbMetric: 103.0416 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 709/1000
2023-10-26 20:10:40.857 
Epoch 709/1000 
	 loss: 102.7746, MinusLogProbMetric: 102.7746, val_loss: 103.0493, val_MinusLogProbMetric: 103.0493

Epoch 709: val_loss did not improve from 103.04147
196/196 - 82s - loss: 102.7746 - MinusLogProbMetric: 102.7746 - val_loss: 103.0493 - val_MinusLogProbMetric: 103.0493 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 710/1000
2023-10-26 20:12:02.012 
Epoch 710/1000 
	 loss: 102.7795, MinusLogProbMetric: 102.7795, val_loss: 103.0613, val_MinusLogProbMetric: 103.0613

Epoch 710: val_loss did not improve from 103.04147
196/196 - 81s - loss: 102.7795 - MinusLogProbMetric: 102.7795 - val_loss: 103.0613 - val_MinusLogProbMetric: 103.0613 - lr: 5.0805e-08 - 81s/epoch - 414ms/step
Epoch 711/1000
2023-10-26 20:13:24.758 
Epoch 711/1000 
	 loss: 102.7812, MinusLogProbMetric: 102.7812, val_loss: 103.0688, val_MinusLogProbMetric: 103.0688

Epoch 711: val_loss did not improve from 103.04147
196/196 - 83s - loss: 102.7812 - MinusLogProbMetric: 102.7812 - val_loss: 103.0688 - val_MinusLogProbMetric: 103.0688 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 712/1000
2023-10-26 20:14:46.927 
Epoch 712/1000 
	 loss: 102.7748, MinusLogProbMetric: 102.7748, val_loss: 103.0534, val_MinusLogProbMetric: 103.0534

Epoch 712: val_loss did not improve from 103.04147
196/196 - 82s - loss: 102.7748 - MinusLogProbMetric: 102.7748 - val_loss: 103.0534 - val_MinusLogProbMetric: 103.0534 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 713/1000
2023-10-26 20:16:09.753 
Epoch 713/1000 
	 loss: 102.7551, MinusLogProbMetric: 102.7551, val_loss: 103.0411, val_MinusLogProbMetric: 103.0411

Epoch 713: val_loss improved from 103.04147 to 103.04113, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 102.7551 - MinusLogProbMetric: 102.7551 - val_loss: 103.0411 - val_MinusLogProbMetric: 103.0411 - lr: 5.0805e-08 - 85s/epoch - 433ms/step
Epoch 714/1000
2023-10-26 20:17:34.084 
Epoch 714/1000 
	 loss: 102.7717, MinusLogProbMetric: 102.7717, val_loss: 103.0128, val_MinusLogProbMetric: 103.0128

Epoch 714: val_loss improved from 103.04113 to 103.01281, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.7717 - MinusLogProbMetric: 102.7717 - val_loss: 103.0128 - val_MinusLogProbMetric: 103.0128 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 715/1000
2023-10-26 20:18:56.993 
Epoch 715/1000 
	 loss: 102.7634, MinusLogProbMetric: 102.7634, val_loss: 103.0433, val_MinusLogProbMetric: 103.0433

Epoch 715: val_loss did not improve from 103.01281
196/196 - 81s - loss: 102.7634 - MinusLogProbMetric: 102.7634 - val_loss: 103.0433 - val_MinusLogProbMetric: 103.0433 - lr: 5.0805e-08 - 81s/epoch - 415ms/step
Epoch 716/1000
2023-10-26 20:20:19.508 
Epoch 716/1000 
	 loss: 102.7590, MinusLogProbMetric: 102.7590, val_loss: 103.0297, val_MinusLogProbMetric: 103.0297

Epoch 716: val_loss did not improve from 103.01281
196/196 - 83s - loss: 102.7590 - MinusLogProbMetric: 102.7590 - val_loss: 103.0297 - val_MinusLogProbMetric: 103.0297 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 717/1000
2023-10-26 20:21:42.159 
Epoch 717/1000 
	 loss: 102.7607, MinusLogProbMetric: 102.7607, val_loss: 103.0451, val_MinusLogProbMetric: 103.0451

Epoch 717: val_loss did not improve from 103.01281
196/196 - 83s - loss: 102.7607 - MinusLogProbMetric: 102.7607 - val_loss: 103.0451 - val_MinusLogProbMetric: 103.0451 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 718/1000
2023-10-26 20:23:04.856 
Epoch 718/1000 
	 loss: 102.7574, MinusLogProbMetric: 102.7574, val_loss: 103.0474, val_MinusLogProbMetric: 103.0474

Epoch 718: val_loss did not improve from 103.01281
196/196 - 83s - loss: 102.7574 - MinusLogProbMetric: 102.7574 - val_loss: 103.0474 - val_MinusLogProbMetric: 103.0474 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 719/1000
2023-10-26 20:24:26.965 
Epoch 719/1000 
	 loss: 102.7549, MinusLogProbMetric: 102.7549, val_loss: 103.0289, val_MinusLogProbMetric: 103.0289

Epoch 719: val_loss did not improve from 103.01281
196/196 - 82s - loss: 102.7549 - MinusLogProbMetric: 102.7549 - val_loss: 103.0289 - val_MinusLogProbMetric: 103.0289 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 720/1000
2023-10-26 20:25:48.637 
Epoch 720/1000 
	 loss: 102.7523, MinusLogProbMetric: 102.7523, val_loss: 103.0245, val_MinusLogProbMetric: 103.0245

Epoch 720: val_loss did not improve from 103.01281
196/196 - 82s - loss: 102.7523 - MinusLogProbMetric: 102.7523 - val_loss: 103.0245 - val_MinusLogProbMetric: 103.0245 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 721/1000
2023-10-26 20:27:10.807 
Epoch 721/1000 
	 loss: 102.7524, MinusLogProbMetric: 102.7524, val_loss: 103.0219, val_MinusLogProbMetric: 103.0219

Epoch 721: val_loss did not improve from 103.01281
196/196 - 82s - loss: 102.7524 - MinusLogProbMetric: 102.7524 - val_loss: 103.0219 - val_MinusLogProbMetric: 103.0219 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 722/1000
2023-10-26 20:28:33.264 
Epoch 722/1000 
	 loss: 102.7565, MinusLogProbMetric: 102.7565, val_loss: 103.0229, val_MinusLogProbMetric: 103.0229

Epoch 722: val_loss did not improve from 103.01281
196/196 - 82s - loss: 102.7565 - MinusLogProbMetric: 102.7565 - val_loss: 103.0229 - val_MinusLogProbMetric: 103.0229 - lr: 5.0805e-08 - 82s/epoch - 421ms/step
Epoch 723/1000
2023-10-26 20:29:55.097 
Epoch 723/1000 
	 loss: 102.7496, MinusLogProbMetric: 102.7496, val_loss: 103.0207, val_MinusLogProbMetric: 103.0207

Epoch 723: val_loss did not improve from 103.01281
196/196 - 82s - loss: 102.7496 - MinusLogProbMetric: 102.7496 - val_loss: 103.0207 - val_MinusLogProbMetric: 103.0207 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 724/1000
2023-10-26 20:31:16.832 
Epoch 724/1000 
	 loss: 102.7395, MinusLogProbMetric: 102.7395, val_loss: 103.0108, val_MinusLogProbMetric: 103.0108

Epoch 724: val_loss improved from 103.01281 to 103.01084, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.7395 - MinusLogProbMetric: 102.7395 - val_loss: 103.0108 - val_MinusLogProbMetric: 103.0108 - lr: 5.0805e-08 - 84s/epoch - 426ms/step
Epoch 725/1000
2023-10-26 20:32:41.587 
Epoch 725/1000 
	 loss: 102.7416, MinusLogProbMetric: 102.7416, val_loss: 102.9979, val_MinusLogProbMetric: 102.9979

Epoch 725: val_loss improved from 103.01084 to 102.99790, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 102.7416 - MinusLogProbMetric: 102.7416 - val_loss: 102.9979 - val_MinusLogProbMetric: 102.9979 - lr: 5.0805e-08 - 85s/epoch - 433ms/step
Epoch 726/1000
2023-10-26 20:34:05.820 
Epoch 726/1000 
	 loss: 102.7299, MinusLogProbMetric: 102.7299, val_loss: 102.9906, val_MinusLogProbMetric: 102.9906

Epoch 726: val_loss improved from 102.99790 to 102.99062, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.7299 - MinusLogProbMetric: 102.7299 - val_loss: 102.9906 - val_MinusLogProbMetric: 102.9906 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 727/1000
2023-10-26 20:35:29.226 
Epoch 727/1000 
	 loss: 102.7265, MinusLogProbMetric: 102.7265, val_loss: 102.9881, val_MinusLogProbMetric: 102.9881

Epoch 727: val_loss improved from 102.99062 to 102.98805, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.7265 - MinusLogProbMetric: 102.7265 - val_loss: 102.9881 - val_MinusLogProbMetric: 102.9881 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 728/1000
2023-10-26 20:36:52.876 
Epoch 728/1000 
	 loss: 102.7245, MinusLogProbMetric: 102.7245, val_loss: 103.0170, val_MinusLogProbMetric: 103.0170

Epoch 728: val_loss did not improve from 102.98805
196/196 - 82s - loss: 102.7245 - MinusLogProbMetric: 102.7245 - val_loss: 103.0170 - val_MinusLogProbMetric: 103.0170 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 729/1000
2023-10-26 20:38:14.979 
Epoch 729/1000 
	 loss: 102.7373, MinusLogProbMetric: 102.7373, val_loss: 102.9966, val_MinusLogProbMetric: 102.9966

Epoch 729: val_loss did not improve from 102.98805
196/196 - 82s - loss: 102.7373 - MinusLogProbMetric: 102.7373 - val_loss: 102.9966 - val_MinusLogProbMetric: 102.9966 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 730/1000
2023-10-26 20:39:37.621 
Epoch 730/1000 
	 loss: 102.7251, MinusLogProbMetric: 102.7251, val_loss: 103.0117, val_MinusLogProbMetric: 103.0117

Epoch 730: val_loss did not improve from 102.98805
196/196 - 83s - loss: 102.7251 - MinusLogProbMetric: 102.7251 - val_loss: 103.0117 - val_MinusLogProbMetric: 103.0117 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 731/1000
2023-10-26 20:40:59.673 
Epoch 731/1000 
	 loss: 102.7226, MinusLogProbMetric: 102.7226, val_loss: 102.9951, val_MinusLogProbMetric: 102.9951

Epoch 731: val_loss did not improve from 102.98805
196/196 - 82s - loss: 102.7226 - MinusLogProbMetric: 102.7226 - val_loss: 102.9951 - val_MinusLogProbMetric: 102.9951 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 732/1000
2023-10-26 20:42:21.169 
Epoch 732/1000 
	 loss: 102.7264, MinusLogProbMetric: 102.7264, val_loss: 103.0020, val_MinusLogProbMetric: 103.0020

Epoch 732: val_loss did not improve from 102.98805
196/196 - 81s - loss: 102.7264 - MinusLogProbMetric: 102.7264 - val_loss: 103.0020 - val_MinusLogProbMetric: 103.0020 - lr: 5.0805e-08 - 81s/epoch - 416ms/step
Epoch 733/1000
2023-10-26 20:43:42.619 
Epoch 733/1000 
	 loss: 102.7204, MinusLogProbMetric: 102.7204, val_loss: 102.9824, val_MinusLogProbMetric: 102.9824

Epoch 733: val_loss improved from 102.98805 to 102.98236, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.7204 - MinusLogProbMetric: 102.7204 - val_loss: 102.9824 - val_MinusLogProbMetric: 102.9824 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 734/1000
2023-10-26 20:45:06.862 
Epoch 734/1000 
	 loss: 102.7165, MinusLogProbMetric: 102.7165, val_loss: 102.9672, val_MinusLogProbMetric: 102.9672

Epoch 734: val_loss improved from 102.98236 to 102.96722, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.7165 - MinusLogProbMetric: 102.7165 - val_loss: 102.9672 - val_MinusLogProbMetric: 102.9672 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 735/1000
2023-10-26 20:46:30.567 
Epoch 735/1000 
	 loss: 102.7174, MinusLogProbMetric: 102.7174, val_loss: 103.0214, val_MinusLogProbMetric: 103.0214

Epoch 735: val_loss did not improve from 102.96722
196/196 - 82s - loss: 102.7174 - MinusLogProbMetric: 102.7174 - val_loss: 103.0214 - val_MinusLogProbMetric: 103.0214 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 736/1000
2023-10-26 20:47:52.704 
Epoch 736/1000 
	 loss: 102.7189, MinusLogProbMetric: 102.7189, val_loss: 103.0004, val_MinusLogProbMetric: 103.0004

Epoch 736: val_loss did not improve from 102.96722
196/196 - 82s - loss: 102.7189 - MinusLogProbMetric: 102.7189 - val_loss: 103.0004 - val_MinusLogProbMetric: 103.0004 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 737/1000
2023-10-26 20:49:14.511 
Epoch 737/1000 
	 loss: 102.7182, MinusLogProbMetric: 102.7182, val_loss: 102.9729, val_MinusLogProbMetric: 102.9729

Epoch 737: val_loss did not improve from 102.96722
196/196 - 82s - loss: 102.7182 - MinusLogProbMetric: 102.7182 - val_loss: 102.9729 - val_MinusLogProbMetric: 102.9729 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 738/1000
2023-10-26 20:50:36.623 
Epoch 738/1000 
	 loss: 102.7162, MinusLogProbMetric: 102.7162, val_loss: 102.9954, val_MinusLogProbMetric: 102.9954

Epoch 738: val_loss did not improve from 102.96722
196/196 - 82s - loss: 102.7162 - MinusLogProbMetric: 102.7162 - val_loss: 102.9954 - val_MinusLogProbMetric: 102.9954 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 739/1000
2023-10-26 20:51:58.436 
Epoch 739/1000 
	 loss: 102.6999, MinusLogProbMetric: 102.6999, val_loss: 102.9766, val_MinusLogProbMetric: 102.9766

Epoch 739: val_loss did not improve from 102.96722
196/196 - 82s - loss: 102.6999 - MinusLogProbMetric: 102.6999 - val_loss: 102.9766 - val_MinusLogProbMetric: 102.9766 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 740/1000
2023-10-26 20:53:21.139 
Epoch 740/1000 
	 loss: 102.7038, MinusLogProbMetric: 102.7038, val_loss: 102.9578, val_MinusLogProbMetric: 102.9578

Epoch 740: val_loss improved from 102.96722 to 102.95779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.7038 - MinusLogProbMetric: 102.7038 - val_loss: 102.9578 - val_MinusLogProbMetric: 102.9578 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 741/1000
2023-10-26 20:54:44.914 
Epoch 741/1000 
	 loss: 102.7001, MinusLogProbMetric: 102.7001, val_loss: 102.9842, val_MinusLogProbMetric: 102.9842

Epoch 741: val_loss did not improve from 102.95779
196/196 - 82s - loss: 102.7001 - MinusLogProbMetric: 102.7001 - val_loss: 102.9842 - val_MinusLogProbMetric: 102.9842 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 742/1000
2023-10-26 20:56:07.157 
Epoch 742/1000 
	 loss: 102.6948, MinusLogProbMetric: 102.6948, val_loss: 102.9827, val_MinusLogProbMetric: 102.9827

Epoch 742: val_loss did not improve from 102.95779
196/196 - 82s - loss: 102.6948 - MinusLogProbMetric: 102.6948 - val_loss: 102.9827 - val_MinusLogProbMetric: 102.9827 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 743/1000
2023-10-26 20:57:28.891 
Epoch 743/1000 
	 loss: 102.7040, MinusLogProbMetric: 102.7040, val_loss: 102.9654, val_MinusLogProbMetric: 102.9654

Epoch 743: val_loss did not improve from 102.95779
196/196 - 82s - loss: 102.7040 - MinusLogProbMetric: 102.7040 - val_loss: 102.9654 - val_MinusLogProbMetric: 102.9654 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 744/1000
2023-10-26 20:58:50.812 
Epoch 744/1000 
	 loss: 102.6994, MinusLogProbMetric: 102.6994, val_loss: 102.9884, val_MinusLogProbMetric: 102.9884

Epoch 744: val_loss did not improve from 102.95779
196/196 - 82s - loss: 102.6994 - MinusLogProbMetric: 102.6994 - val_loss: 102.9884 - val_MinusLogProbMetric: 102.9884 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 745/1000
2023-10-26 21:00:13.092 
Epoch 745/1000 
	 loss: 102.6916, MinusLogProbMetric: 102.6916, val_loss: 102.9656, val_MinusLogProbMetric: 102.9656

Epoch 745: val_loss did not improve from 102.95779
196/196 - 82s - loss: 102.6916 - MinusLogProbMetric: 102.6916 - val_loss: 102.9656 - val_MinusLogProbMetric: 102.9656 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 746/1000
2023-10-26 21:01:35.081 
Epoch 746/1000 
	 loss: 102.7031, MinusLogProbMetric: 102.7031, val_loss: 103.0035, val_MinusLogProbMetric: 103.0035

Epoch 746: val_loss did not improve from 102.95779
196/196 - 82s - loss: 102.7031 - MinusLogProbMetric: 102.7031 - val_loss: 103.0035 - val_MinusLogProbMetric: 103.0035 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 747/1000
2023-10-26 21:02:56.478 
Epoch 747/1000 
	 loss: 102.6944, MinusLogProbMetric: 102.6944, val_loss: 102.9615, val_MinusLogProbMetric: 102.9615

Epoch 747: val_loss did not improve from 102.95779
196/196 - 81s - loss: 102.6944 - MinusLogProbMetric: 102.6944 - val_loss: 102.9615 - val_MinusLogProbMetric: 102.9615 - lr: 5.0805e-08 - 81s/epoch - 415ms/step
Epoch 748/1000
2023-10-26 21:04:18.390 
Epoch 748/1000 
	 loss: 102.6911, MinusLogProbMetric: 102.6911, val_loss: 102.9649, val_MinusLogProbMetric: 102.9649

Epoch 748: val_loss did not improve from 102.95779
196/196 - 82s - loss: 102.6911 - MinusLogProbMetric: 102.6911 - val_loss: 102.9649 - val_MinusLogProbMetric: 102.9649 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 749/1000
2023-10-26 21:05:40.680 
Epoch 749/1000 
	 loss: 102.7022, MinusLogProbMetric: 102.7022, val_loss: 102.9875, val_MinusLogProbMetric: 102.9875

Epoch 749: val_loss did not improve from 102.95779
196/196 - 82s - loss: 102.7022 - MinusLogProbMetric: 102.7022 - val_loss: 102.9875 - val_MinusLogProbMetric: 102.9875 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 750/1000
2023-10-26 21:07:02.071 
Epoch 750/1000 
	 loss: 102.6772, MinusLogProbMetric: 102.6772, val_loss: 102.9353, val_MinusLogProbMetric: 102.9353

Epoch 750: val_loss improved from 102.95779 to 102.93534, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.6772 - MinusLogProbMetric: 102.6772 - val_loss: 102.9353 - val_MinusLogProbMetric: 102.9353 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 751/1000
2023-10-26 21:08:25.594 
Epoch 751/1000 
	 loss: 102.6515, MinusLogProbMetric: 102.6515, val_loss: 102.9368, val_MinusLogProbMetric: 102.9368

Epoch 751: val_loss did not improve from 102.93534
196/196 - 82s - loss: 102.6515 - MinusLogProbMetric: 102.6515 - val_loss: 102.9368 - val_MinusLogProbMetric: 102.9368 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 752/1000
2023-10-26 21:09:46.898 
Epoch 752/1000 
	 loss: 102.7047, MinusLogProbMetric: 102.7047, val_loss: 103.0531, val_MinusLogProbMetric: 103.0531

Epoch 752: val_loss did not improve from 102.93534
196/196 - 81s - loss: 102.7047 - MinusLogProbMetric: 102.7047 - val_loss: 103.0531 - val_MinusLogProbMetric: 103.0531 - lr: 5.0805e-08 - 81s/epoch - 415ms/step
Epoch 753/1000
2023-10-26 21:11:09.777 
Epoch 753/1000 
	 loss: 102.7563, MinusLogProbMetric: 102.7563, val_loss: 102.9996, val_MinusLogProbMetric: 102.9996

Epoch 753: val_loss did not improve from 102.93534
196/196 - 83s - loss: 102.7563 - MinusLogProbMetric: 102.7563 - val_loss: 102.9996 - val_MinusLogProbMetric: 102.9996 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 754/1000
2023-10-26 21:12:31.894 
Epoch 754/1000 
	 loss: 102.7164, MinusLogProbMetric: 102.7164, val_loss: 102.9810, val_MinusLogProbMetric: 102.9810

Epoch 754: val_loss did not improve from 102.93534
196/196 - 82s - loss: 102.7164 - MinusLogProbMetric: 102.7164 - val_loss: 102.9810 - val_MinusLogProbMetric: 102.9810 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 755/1000
2023-10-26 21:13:53.367 
Epoch 755/1000 
	 loss: 102.7053, MinusLogProbMetric: 102.7053, val_loss: 102.9650, val_MinusLogProbMetric: 102.9650

Epoch 755: val_loss did not improve from 102.93534
196/196 - 81s - loss: 102.7053 - MinusLogProbMetric: 102.7053 - val_loss: 102.9650 - val_MinusLogProbMetric: 102.9650 - lr: 5.0805e-08 - 81s/epoch - 416ms/step
Epoch 756/1000
2023-10-26 21:15:16.174 
Epoch 756/1000 
	 loss: 102.7036, MinusLogProbMetric: 102.7036, val_loss: 102.9678, val_MinusLogProbMetric: 102.9678

Epoch 756: val_loss did not improve from 102.93534
196/196 - 83s - loss: 102.7036 - MinusLogProbMetric: 102.7036 - val_loss: 102.9678 - val_MinusLogProbMetric: 102.9678 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 757/1000
2023-10-26 21:16:37.536 
Epoch 757/1000 
	 loss: 102.7059, MinusLogProbMetric: 102.7059, val_loss: 102.9695, val_MinusLogProbMetric: 102.9695

Epoch 757: val_loss did not improve from 102.93534
196/196 - 81s - loss: 102.7059 - MinusLogProbMetric: 102.7059 - val_loss: 102.9695 - val_MinusLogProbMetric: 102.9695 - lr: 5.0805e-08 - 81s/epoch - 415ms/step
Epoch 758/1000
2023-10-26 21:17:59.911 
Epoch 758/1000 
	 loss: 102.6825, MinusLogProbMetric: 102.6825, val_loss: 102.9711, val_MinusLogProbMetric: 102.9711

Epoch 758: val_loss did not improve from 102.93534
196/196 - 82s - loss: 102.6825 - MinusLogProbMetric: 102.6825 - val_loss: 102.9711 - val_MinusLogProbMetric: 102.9711 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 759/1000
2023-10-26 21:19:22.438 
Epoch 759/1000 
	 loss: 102.6942, MinusLogProbMetric: 102.6942, val_loss: 102.9454, val_MinusLogProbMetric: 102.9454

Epoch 759: val_loss did not improve from 102.93534
196/196 - 83s - loss: 102.6942 - MinusLogProbMetric: 102.6942 - val_loss: 102.9454 - val_MinusLogProbMetric: 102.9454 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 760/1000
2023-10-26 21:20:44.537 
Epoch 760/1000 
	 loss: 102.6874, MinusLogProbMetric: 102.6874, val_loss: 102.9564, val_MinusLogProbMetric: 102.9564

Epoch 760: val_loss did not improve from 102.93534
196/196 - 82s - loss: 102.6874 - MinusLogProbMetric: 102.6874 - val_loss: 102.9564 - val_MinusLogProbMetric: 102.9564 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 761/1000
2023-10-26 21:22:06.575 
Epoch 761/1000 
	 loss: 102.6771, MinusLogProbMetric: 102.6771, val_loss: 102.9484, val_MinusLogProbMetric: 102.9484

Epoch 761: val_loss did not improve from 102.93534
196/196 - 82s - loss: 102.6771 - MinusLogProbMetric: 102.6771 - val_loss: 102.9484 - val_MinusLogProbMetric: 102.9484 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 762/1000
2023-10-26 21:23:28.866 
Epoch 762/1000 
	 loss: 102.6750, MinusLogProbMetric: 102.6750, val_loss: 102.9457, val_MinusLogProbMetric: 102.9457

Epoch 762: val_loss did not improve from 102.93534
196/196 - 82s - loss: 102.6750 - MinusLogProbMetric: 102.6750 - val_loss: 102.9457 - val_MinusLogProbMetric: 102.9457 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 763/1000
2023-10-26 21:24:51.071 
Epoch 763/1000 
	 loss: 102.6878, MinusLogProbMetric: 102.6878, val_loss: 102.9314, val_MinusLogProbMetric: 102.9314

Epoch 763: val_loss improved from 102.93534 to 102.93138, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.6878 - MinusLogProbMetric: 102.6878 - val_loss: 102.9314 - val_MinusLogProbMetric: 102.9314 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 764/1000
2023-10-26 21:26:15.487 
Epoch 764/1000 
	 loss: 102.6675, MinusLogProbMetric: 102.6675, val_loss: 102.9589, val_MinusLogProbMetric: 102.9589

Epoch 764: val_loss did not improve from 102.93138
196/196 - 82s - loss: 102.6675 - MinusLogProbMetric: 102.6675 - val_loss: 102.9589 - val_MinusLogProbMetric: 102.9589 - lr: 5.0805e-08 - 82s/epoch - 421ms/step
Epoch 765/1000
2023-10-26 21:27:37.825 
Epoch 765/1000 
	 loss: 102.6640, MinusLogProbMetric: 102.6640, val_loss: 102.9327, val_MinusLogProbMetric: 102.9327

Epoch 765: val_loss did not improve from 102.93138
196/196 - 82s - loss: 102.6640 - MinusLogProbMetric: 102.6640 - val_loss: 102.9327 - val_MinusLogProbMetric: 102.9327 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 766/1000
2023-10-26 21:28:59.646 
Epoch 766/1000 
	 loss: 102.6689, MinusLogProbMetric: 102.6689, val_loss: 102.9402, val_MinusLogProbMetric: 102.9402

Epoch 766: val_loss did not improve from 102.93138
196/196 - 82s - loss: 102.6689 - MinusLogProbMetric: 102.6689 - val_loss: 102.9402 - val_MinusLogProbMetric: 102.9402 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 767/1000
2023-10-26 21:30:21.699 
Epoch 767/1000 
	 loss: 102.6664, MinusLogProbMetric: 102.6664, val_loss: 102.9135, val_MinusLogProbMetric: 102.9135

Epoch 767: val_loss improved from 102.93138 to 102.91348, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.6664 - MinusLogProbMetric: 102.6664 - val_loss: 102.9135 - val_MinusLogProbMetric: 102.9135 - lr: 5.0805e-08 - 84s/epoch - 426ms/step
Epoch 768/1000
2023-10-26 21:31:45.476 
Epoch 768/1000 
	 loss: 102.6584, MinusLogProbMetric: 102.6584, val_loss: 102.9196, val_MinusLogProbMetric: 102.9196

Epoch 768: val_loss did not improve from 102.91348
196/196 - 82s - loss: 102.6584 - MinusLogProbMetric: 102.6584 - val_loss: 102.9196 - val_MinusLogProbMetric: 102.9196 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 769/1000
2023-10-26 21:33:07.994 
Epoch 769/1000 
	 loss: 102.6590, MinusLogProbMetric: 102.6590, val_loss: 102.9261, val_MinusLogProbMetric: 102.9261

Epoch 769: val_loss did not improve from 102.91348
196/196 - 83s - loss: 102.6590 - MinusLogProbMetric: 102.6590 - val_loss: 102.9261 - val_MinusLogProbMetric: 102.9261 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 770/1000
2023-10-26 21:34:29.898 
Epoch 770/1000 
	 loss: 102.6648, MinusLogProbMetric: 102.6648, val_loss: 102.9215, val_MinusLogProbMetric: 102.9215

Epoch 770: val_loss did not improve from 102.91348
196/196 - 82s - loss: 102.6648 - MinusLogProbMetric: 102.6648 - val_loss: 102.9215 - val_MinusLogProbMetric: 102.9215 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 771/1000
2023-10-26 21:35:51.717 
Epoch 771/1000 
	 loss: 102.6534, MinusLogProbMetric: 102.6534, val_loss: 102.9443, val_MinusLogProbMetric: 102.9443

Epoch 771: val_loss did not improve from 102.91348
196/196 - 82s - loss: 102.6534 - MinusLogProbMetric: 102.6534 - val_loss: 102.9443 - val_MinusLogProbMetric: 102.9443 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 772/1000
2023-10-26 21:37:13.214 
Epoch 772/1000 
	 loss: 102.6531, MinusLogProbMetric: 102.6531, val_loss: 102.9185, val_MinusLogProbMetric: 102.9185

Epoch 772: val_loss did not improve from 102.91348
196/196 - 81s - loss: 102.6531 - MinusLogProbMetric: 102.6531 - val_loss: 102.9185 - val_MinusLogProbMetric: 102.9185 - lr: 5.0805e-08 - 81s/epoch - 416ms/step
Epoch 773/1000
2023-10-26 21:38:34.976 
Epoch 773/1000 
	 loss: 102.6415, MinusLogProbMetric: 102.6415, val_loss: 102.9220, val_MinusLogProbMetric: 102.9220

Epoch 773: val_loss did not improve from 102.91348
196/196 - 82s - loss: 102.6415 - MinusLogProbMetric: 102.6415 - val_loss: 102.9220 - val_MinusLogProbMetric: 102.9220 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 774/1000
2023-10-26 21:39:57.261 
Epoch 774/1000 
	 loss: 102.6459, MinusLogProbMetric: 102.6459, val_loss: 102.9225, val_MinusLogProbMetric: 102.9225

Epoch 774: val_loss did not improve from 102.91348
196/196 - 82s - loss: 102.6459 - MinusLogProbMetric: 102.6459 - val_loss: 102.9225 - val_MinusLogProbMetric: 102.9225 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 775/1000
2023-10-26 21:41:19.444 
Epoch 775/1000 
	 loss: 102.6402, MinusLogProbMetric: 102.6402, val_loss: 102.9060, val_MinusLogProbMetric: 102.9060

Epoch 775: val_loss improved from 102.91348 to 102.90598, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.6402 - MinusLogProbMetric: 102.6402 - val_loss: 102.9060 - val_MinusLogProbMetric: 102.9060 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 776/1000
2023-10-26 21:42:43.604 
Epoch 776/1000 
	 loss: 102.6413, MinusLogProbMetric: 102.6413, val_loss: 102.9128, val_MinusLogProbMetric: 102.9128

Epoch 776: val_loss did not improve from 102.90598
196/196 - 82s - loss: 102.6413 - MinusLogProbMetric: 102.6413 - val_loss: 102.9128 - val_MinusLogProbMetric: 102.9128 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 777/1000
2023-10-26 21:44:05.643 
Epoch 777/1000 
	 loss: 102.6462, MinusLogProbMetric: 102.6462, val_loss: 102.9153, val_MinusLogProbMetric: 102.9153

Epoch 777: val_loss did not improve from 102.90598
196/196 - 82s - loss: 102.6462 - MinusLogProbMetric: 102.6462 - val_loss: 102.9153 - val_MinusLogProbMetric: 102.9153 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 778/1000
2023-10-26 21:45:27.885 
Epoch 778/1000 
	 loss: 102.6397, MinusLogProbMetric: 102.6397, val_loss: 102.9117, val_MinusLogProbMetric: 102.9117

Epoch 778: val_loss did not improve from 102.90598
196/196 - 82s - loss: 102.6397 - MinusLogProbMetric: 102.6397 - val_loss: 102.9117 - val_MinusLogProbMetric: 102.9117 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 779/1000
2023-10-26 21:46:49.906 
Epoch 779/1000 
	 loss: 102.6465, MinusLogProbMetric: 102.6465, val_loss: 102.8999, val_MinusLogProbMetric: 102.8999

Epoch 779: val_loss improved from 102.90598 to 102.89995, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.6465 - MinusLogProbMetric: 102.6465 - val_loss: 102.8999 - val_MinusLogProbMetric: 102.8999 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 780/1000
2023-10-26 21:48:13.845 
Epoch 780/1000 
	 loss: 102.6366, MinusLogProbMetric: 102.6366, val_loss: 102.9029, val_MinusLogProbMetric: 102.9029

Epoch 780: val_loss did not improve from 102.89995
196/196 - 82s - loss: 102.6366 - MinusLogProbMetric: 102.6366 - val_loss: 102.9029 - val_MinusLogProbMetric: 102.9029 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 781/1000
2023-10-26 21:49:35.897 
Epoch 781/1000 
	 loss: 102.6323, MinusLogProbMetric: 102.6323, val_loss: 102.8970, val_MinusLogProbMetric: 102.8970

Epoch 781: val_loss improved from 102.89995 to 102.89698, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.6323 - MinusLogProbMetric: 102.6323 - val_loss: 102.8970 - val_MinusLogProbMetric: 102.8970 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 782/1000
2023-10-26 21:51:00.378 
Epoch 782/1000 
	 loss: 102.6279, MinusLogProbMetric: 102.6279, val_loss: 102.8854, val_MinusLogProbMetric: 102.8854

Epoch 782: val_loss improved from 102.89698 to 102.88544, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.6279 - MinusLogProbMetric: 102.6279 - val_loss: 102.8854 - val_MinusLogProbMetric: 102.8854 - lr: 5.0805e-08 - 84s/epoch - 431ms/step
Epoch 783/1000
2023-10-26 21:52:24.066 
Epoch 783/1000 
	 loss: 102.6278, MinusLogProbMetric: 102.6278, val_loss: 102.8925, val_MinusLogProbMetric: 102.8925

Epoch 783: val_loss did not improve from 102.88544
196/196 - 82s - loss: 102.6278 - MinusLogProbMetric: 102.6278 - val_loss: 102.8925 - val_MinusLogProbMetric: 102.8925 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 784/1000
2023-10-26 21:53:45.580 
Epoch 784/1000 
	 loss: 102.6203, MinusLogProbMetric: 102.6203, val_loss: 102.8857, val_MinusLogProbMetric: 102.8857

Epoch 784: val_loss did not improve from 102.88544
196/196 - 82s - loss: 102.6203 - MinusLogProbMetric: 102.6203 - val_loss: 102.8857 - val_MinusLogProbMetric: 102.8857 - lr: 5.0805e-08 - 82s/epoch - 416ms/step
Epoch 785/1000
2023-10-26 21:55:07.200 
Epoch 785/1000 
	 loss: 102.6134, MinusLogProbMetric: 102.6134, val_loss: 102.8850, val_MinusLogProbMetric: 102.8850

Epoch 785: val_loss improved from 102.88544 to 102.88497, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.6134 - MinusLogProbMetric: 102.6134 - val_loss: 102.8850 - val_MinusLogProbMetric: 102.8850 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 786/1000
2023-10-26 21:56:30.922 
Epoch 786/1000 
	 loss: 102.6132, MinusLogProbMetric: 102.6132, val_loss: 102.8921, val_MinusLogProbMetric: 102.8921

Epoch 786: val_loss did not improve from 102.88497
196/196 - 82s - loss: 102.6132 - MinusLogProbMetric: 102.6132 - val_loss: 102.8921 - val_MinusLogProbMetric: 102.8921 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 787/1000
2023-10-26 21:57:49.797 
Epoch 787/1000 
	 loss: 102.6274, MinusLogProbMetric: 102.6274, val_loss: 102.9109, val_MinusLogProbMetric: 102.9109

Epoch 787: val_loss did not improve from 102.88497
196/196 - 79s - loss: 102.6274 - MinusLogProbMetric: 102.6274 - val_loss: 102.9109 - val_MinusLogProbMetric: 102.9109 - lr: 5.0805e-08 - 79s/epoch - 402ms/step
Epoch 788/1000
2023-10-26 21:58:54.892 
Epoch 788/1000 
	 loss: 102.6417, MinusLogProbMetric: 102.6417, val_loss: 102.9008, val_MinusLogProbMetric: 102.9008

Epoch 788: val_loss did not improve from 102.88497
196/196 - 65s - loss: 102.6417 - MinusLogProbMetric: 102.6417 - val_loss: 102.9008 - val_MinusLogProbMetric: 102.9008 - lr: 5.0805e-08 - 65s/epoch - 332ms/step
Epoch 789/1000
2023-10-26 22:00:15.647 
Epoch 789/1000 
	 loss: 102.6295, MinusLogProbMetric: 102.6295, val_loss: 102.8790, val_MinusLogProbMetric: 102.8790

Epoch 789: val_loss improved from 102.88497 to 102.87897, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 82s - loss: 102.6295 - MinusLogProbMetric: 102.6295 - val_loss: 102.8790 - val_MinusLogProbMetric: 102.8790 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 790/1000
2023-10-26 22:01:37.868 
Epoch 790/1000 
	 loss: 102.6229, MinusLogProbMetric: 102.6229, val_loss: 102.8978, val_MinusLogProbMetric: 102.8978

Epoch 790: val_loss did not improve from 102.87897
196/196 - 81s - loss: 102.6229 - MinusLogProbMetric: 102.6229 - val_loss: 102.8978 - val_MinusLogProbMetric: 102.8978 - lr: 5.0805e-08 - 81s/epoch - 412ms/step
Epoch 791/1000
2023-10-26 22:03:00.023 
Epoch 791/1000 
	 loss: 102.6265, MinusLogProbMetric: 102.6265, val_loss: 102.8941, val_MinusLogProbMetric: 102.8941

Epoch 791: val_loss did not improve from 102.87897
196/196 - 82s - loss: 102.6265 - MinusLogProbMetric: 102.6265 - val_loss: 102.8941 - val_MinusLogProbMetric: 102.8941 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 792/1000
2023-10-26 22:04:22.933 
Epoch 792/1000 
	 loss: 102.6152, MinusLogProbMetric: 102.6152, val_loss: 102.8843, val_MinusLogProbMetric: 102.8843

Epoch 792: val_loss did not improve from 102.87897
196/196 - 83s - loss: 102.6152 - MinusLogProbMetric: 102.6152 - val_loss: 102.8843 - val_MinusLogProbMetric: 102.8843 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 793/1000
2023-10-26 22:05:45.105 
Epoch 793/1000 
	 loss: 102.5884, MinusLogProbMetric: 102.5884, val_loss: 102.8392, val_MinusLogProbMetric: 102.8392

Epoch 793: val_loss improved from 102.87897 to 102.83916, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.5884 - MinusLogProbMetric: 102.5884 - val_loss: 102.8392 - val_MinusLogProbMetric: 102.8392 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 794/1000
2023-10-26 22:07:08.946 
Epoch 794/1000 
	 loss: 102.5602, MinusLogProbMetric: 102.5602, val_loss: 102.8294, val_MinusLogProbMetric: 102.8294

Epoch 794: val_loss improved from 102.83916 to 102.82936, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.5602 - MinusLogProbMetric: 102.5602 - val_loss: 102.8294 - val_MinusLogProbMetric: 102.8294 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 795/1000
2023-10-26 22:08:32.859 
Epoch 795/1000 
	 loss: 102.5653, MinusLogProbMetric: 102.5653, val_loss: 102.8260, val_MinusLogProbMetric: 102.8260

Epoch 795: val_loss improved from 102.82936 to 102.82599, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.5653 - MinusLogProbMetric: 102.5653 - val_loss: 102.8260 - val_MinusLogProbMetric: 102.8260 - lr: 5.0805e-08 - 83s/epoch - 426ms/step
Epoch 796/1000
2023-10-26 22:09:56.118 
Epoch 796/1000 
	 loss: 102.5497, MinusLogProbMetric: 102.5497, val_loss: 102.8172, val_MinusLogProbMetric: 102.8172

Epoch 796: val_loss improved from 102.82599 to 102.81724, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.5497 - MinusLogProbMetric: 102.5497 - val_loss: 102.8172 - val_MinusLogProbMetric: 102.8172 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 797/1000
2023-10-26 22:11:18.751 
Epoch 797/1000 
	 loss: 102.5481, MinusLogProbMetric: 102.5481, val_loss: 102.8205, val_MinusLogProbMetric: 102.8205

Epoch 797: val_loss did not improve from 102.81724
196/196 - 81s - loss: 102.5481 - MinusLogProbMetric: 102.5481 - val_loss: 102.8205 - val_MinusLogProbMetric: 102.8205 - lr: 5.0805e-08 - 81s/epoch - 414ms/step
Epoch 798/1000
2023-10-26 22:12:40.501 
Epoch 798/1000 
	 loss: 102.5399, MinusLogProbMetric: 102.5399, val_loss: 102.8154, val_MinusLogProbMetric: 102.8154

Epoch 798: val_loss improved from 102.81724 to 102.81544, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.5399 - MinusLogProbMetric: 102.5399 - val_loss: 102.8154 - val_MinusLogProbMetric: 102.8154 - lr: 5.0805e-08 - 84s/epoch - 426ms/step
Epoch 799/1000
2023-10-26 22:14:04.974 
Epoch 799/1000 
	 loss: 102.5391, MinusLogProbMetric: 102.5391, val_loss: 102.8224, val_MinusLogProbMetric: 102.8224

Epoch 799: val_loss did not improve from 102.81544
196/196 - 83s - loss: 102.5391 - MinusLogProbMetric: 102.5391 - val_loss: 102.8224 - val_MinusLogProbMetric: 102.8224 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 800/1000
2023-10-26 22:15:27.415 
Epoch 800/1000 
	 loss: 102.5382, MinusLogProbMetric: 102.5382, val_loss: 102.8110, val_MinusLogProbMetric: 102.8110

Epoch 800: val_loss improved from 102.81544 to 102.81100, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.5382 - MinusLogProbMetric: 102.5382 - val_loss: 102.8110 - val_MinusLogProbMetric: 102.8110 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 801/1000
2023-10-26 22:16:50.673 
Epoch 801/1000 
	 loss: 102.5322, MinusLogProbMetric: 102.5322, val_loss: 102.8061, val_MinusLogProbMetric: 102.8061

Epoch 801: val_loss improved from 102.81100 to 102.80608, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.5322 - MinusLogProbMetric: 102.5322 - val_loss: 102.8061 - val_MinusLogProbMetric: 102.8061 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 802/1000
2023-10-26 22:18:14.762 
Epoch 802/1000 
	 loss: 102.5310, MinusLogProbMetric: 102.5310, val_loss: 102.8039, val_MinusLogProbMetric: 102.8039

Epoch 802: val_loss improved from 102.80608 to 102.80389, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.5310 - MinusLogProbMetric: 102.5310 - val_loss: 102.8039 - val_MinusLogProbMetric: 102.8039 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 803/1000
2023-10-26 22:19:38.483 
Epoch 803/1000 
	 loss: 102.5291, MinusLogProbMetric: 102.5291, val_loss: 102.7993, val_MinusLogProbMetric: 102.7993

Epoch 803: val_loss improved from 102.80389 to 102.79932, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.5291 - MinusLogProbMetric: 102.5291 - val_loss: 102.7993 - val_MinusLogProbMetric: 102.7993 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 804/1000
2023-10-26 22:21:02.611 
Epoch 804/1000 
	 loss: 102.5286, MinusLogProbMetric: 102.5286, val_loss: 102.8038, val_MinusLogProbMetric: 102.8038

Epoch 804: val_loss did not improve from 102.79932
196/196 - 82s - loss: 102.5286 - MinusLogProbMetric: 102.5286 - val_loss: 102.8038 - val_MinusLogProbMetric: 102.8038 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 805/1000
2023-10-26 22:22:25.619 
Epoch 805/1000 
	 loss: 102.5226, MinusLogProbMetric: 102.5226, val_loss: 102.7941, val_MinusLogProbMetric: 102.7941

Epoch 805: val_loss improved from 102.79932 to 102.79409, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 102.5226 - MinusLogProbMetric: 102.5226 - val_loss: 102.7941 - val_MinusLogProbMetric: 102.7941 - lr: 5.0805e-08 - 85s/epoch - 432ms/step
Epoch 806/1000
2023-10-26 22:23:49.364 
Epoch 806/1000 
	 loss: 102.5108, MinusLogProbMetric: 102.5108, val_loss: 102.7876, val_MinusLogProbMetric: 102.7876

Epoch 806: val_loss improved from 102.79409 to 102.78757, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.5108 - MinusLogProbMetric: 102.5108 - val_loss: 102.7876 - val_MinusLogProbMetric: 102.7876 - lr: 5.0805e-08 - 84s/epoch - 426ms/step
Epoch 807/1000
2023-10-26 22:25:13.202 
Epoch 807/1000 
	 loss: 102.5103, MinusLogProbMetric: 102.5103, val_loss: 102.8072, val_MinusLogProbMetric: 102.8072

Epoch 807: val_loss did not improve from 102.78757
196/196 - 82s - loss: 102.5103 - MinusLogProbMetric: 102.5103 - val_loss: 102.8072 - val_MinusLogProbMetric: 102.8072 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 808/1000
2023-10-26 22:26:35.373 
Epoch 808/1000 
	 loss: 102.5088, MinusLogProbMetric: 102.5088, val_loss: 102.8077, val_MinusLogProbMetric: 102.8077

Epoch 808: val_loss did not improve from 102.78757
196/196 - 82s - loss: 102.5088 - MinusLogProbMetric: 102.5088 - val_loss: 102.8077 - val_MinusLogProbMetric: 102.8077 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 809/1000
2023-10-26 22:27:57.382 
Epoch 809/1000 
	 loss: 102.4993, MinusLogProbMetric: 102.4993, val_loss: 102.7859, val_MinusLogProbMetric: 102.7859

Epoch 809: val_loss improved from 102.78757 to 102.78587, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.4993 - MinusLogProbMetric: 102.4993 - val_loss: 102.7859 - val_MinusLogProbMetric: 102.7859 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 810/1000
2023-10-26 22:29:19.658 
Epoch 810/1000 
	 loss: 102.5050, MinusLogProbMetric: 102.5050, val_loss: 102.7896, val_MinusLogProbMetric: 102.7896

Epoch 810: val_loss did not improve from 102.78587
196/196 - 81s - loss: 102.5050 - MinusLogProbMetric: 102.5050 - val_loss: 102.7896 - val_MinusLogProbMetric: 102.7896 - lr: 5.0805e-08 - 81s/epoch - 413ms/step
Epoch 811/1000
2023-10-26 22:30:41.742 
Epoch 811/1000 
	 loss: 102.5064, MinusLogProbMetric: 102.5064, val_loss: 102.7843, val_MinusLogProbMetric: 102.7843

Epoch 811: val_loss improved from 102.78587 to 102.78435, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.5064 - MinusLogProbMetric: 102.5064 - val_loss: 102.7843 - val_MinusLogProbMetric: 102.7843 - lr: 5.0805e-08 - 84s/epoch - 426ms/step
Epoch 812/1000
2023-10-26 22:32:05.783 
Epoch 812/1000 
	 loss: 102.5089, MinusLogProbMetric: 102.5089, val_loss: 102.7738, val_MinusLogProbMetric: 102.7738

Epoch 812: val_loss improved from 102.78435 to 102.77381, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.5089 - MinusLogProbMetric: 102.5089 - val_loss: 102.7738 - val_MinusLogProbMetric: 102.7738 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 813/1000
2023-10-26 22:33:30.558 
Epoch 813/1000 
	 loss: 102.4964, MinusLogProbMetric: 102.4964, val_loss: 102.7692, val_MinusLogProbMetric: 102.7692

Epoch 813: val_loss improved from 102.77381 to 102.76916, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 102.4964 - MinusLogProbMetric: 102.4964 - val_loss: 102.7692 - val_MinusLogProbMetric: 102.7692 - lr: 5.0805e-08 - 85s/epoch - 434ms/step
Epoch 814/1000
2023-10-26 22:34:54.597 
Epoch 814/1000 
	 loss: 102.4960, MinusLogProbMetric: 102.4960, val_loss: 102.7704, val_MinusLogProbMetric: 102.7704

Epoch 814: val_loss did not improve from 102.76916
196/196 - 82s - loss: 102.4960 - MinusLogProbMetric: 102.4960 - val_loss: 102.7704 - val_MinusLogProbMetric: 102.7704 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 815/1000
2023-10-26 22:36:16.290 
Epoch 815/1000 
	 loss: 102.4847, MinusLogProbMetric: 102.4847, val_loss: 102.7688, val_MinusLogProbMetric: 102.7688

Epoch 815: val_loss improved from 102.76916 to 102.76877, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.4847 - MinusLogProbMetric: 102.4847 - val_loss: 102.7688 - val_MinusLogProbMetric: 102.7688 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 816/1000
2023-10-26 22:37:39.419 
Epoch 816/1000 
	 loss: 102.4870, MinusLogProbMetric: 102.4870, val_loss: 102.7566, val_MinusLogProbMetric: 102.7566

Epoch 816: val_loss improved from 102.76877 to 102.75662, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.4870 - MinusLogProbMetric: 102.4870 - val_loss: 102.7566 - val_MinusLogProbMetric: 102.7566 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 817/1000
2023-10-26 22:39:03.735 
Epoch 817/1000 
	 loss: 102.4960, MinusLogProbMetric: 102.4960, val_loss: 102.7557, val_MinusLogProbMetric: 102.7557

Epoch 817: val_loss improved from 102.75662 to 102.75568, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.4960 - MinusLogProbMetric: 102.4960 - val_loss: 102.7557 - val_MinusLogProbMetric: 102.7557 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 818/1000
2023-10-26 22:40:26.144 
Epoch 818/1000 
	 loss: 102.4870, MinusLogProbMetric: 102.4870, val_loss: 102.7710, val_MinusLogProbMetric: 102.7710

Epoch 818: val_loss did not improve from 102.75568
196/196 - 81s - loss: 102.4870 - MinusLogProbMetric: 102.4870 - val_loss: 102.7710 - val_MinusLogProbMetric: 102.7710 - lr: 5.0805e-08 - 81s/epoch - 413ms/step
Epoch 819/1000
2023-10-26 22:41:48.248 
Epoch 819/1000 
	 loss: 102.4893, MinusLogProbMetric: 102.4893, val_loss: 102.7579, val_MinusLogProbMetric: 102.7579

Epoch 819: val_loss did not improve from 102.75568
196/196 - 82s - loss: 102.4893 - MinusLogProbMetric: 102.4893 - val_loss: 102.7579 - val_MinusLogProbMetric: 102.7579 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 820/1000
2023-10-26 22:43:10.645 
Epoch 820/1000 
	 loss: 102.4792, MinusLogProbMetric: 102.4792, val_loss: 102.7605, val_MinusLogProbMetric: 102.7605

Epoch 820: val_loss did not improve from 102.75568
196/196 - 82s - loss: 102.4792 - MinusLogProbMetric: 102.4792 - val_loss: 102.7605 - val_MinusLogProbMetric: 102.7605 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 821/1000
2023-10-26 22:44:32.794 
Epoch 821/1000 
	 loss: 102.4863, MinusLogProbMetric: 102.4863, val_loss: 102.7581, val_MinusLogProbMetric: 102.7581

Epoch 821: val_loss did not improve from 102.75568
196/196 - 82s - loss: 102.4863 - MinusLogProbMetric: 102.4863 - val_loss: 102.7581 - val_MinusLogProbMetric: 102.7581 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 822/1000
2023-10-26 22:45:54.976 
Epoch 822/1000 
	 loss: 102.4776, MinusLogProbMetric: 102.4776, val_loss: 102.7551, val_MinusLogProbMetric: 102.7551

Epoch 822: val_loss improved from 102.75568 to 102.75511, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.4776 - MinusLogProbMetric: 102.4776 - val_loss: 102.7551 - val_MinusLogProbMetric: 102.7551 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 823/1000
2023-10-26 22:47:19.722 
Epoch 823/1000 
	 loss: 102.4743, MinusLogProbMetric: 102.4743, val_loss: 102.7598, val_MinusLogProbMetric: 102.7598

Epoch 823: val_loss did not improve from 102.75511
196/196 - 83s - loss: 102.4743 - MinusLogProbMetric: 102.4743 - val_loss: 102.7598 - val_MinusLogProbMetric: 102.7598 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 824/1000
2023-10-26 22:48:41.989 
Epoch 824/1000 
	 loss: 102.4711, MinusLogProbMetric: 102.4711, val_loss: 102.7505, val_MinusLogProbMetric: 102.7505

Epoch 824: val_loss improved from 102.75511 to 102.75047, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.4711 - MinusLogProbMetric: 102.4711 - val_loss: 102.7505 - val_MinusLogProbMetric: 102.7505 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 825/1000
2023-10-26 22:50:05.701 
Epoch 825/1000 
	 loss: 102.4727, MinusLogProbMetric: 102.4727, val_loss: 102.7568, val_MinusLogProbMetric: 102.7568

Epoch 825: val_loss did not improve from 102.75047
196/196 - 82s - loss: 102.4727 - MinusLogProbMetric: 102.4727 - val_loss: 102.7568 - val_MinusLogProbMetric: 102.7568 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 826/1000
2023-10-26 22:51:28.112 
Epoch 826/1000 
	 loss: 102.4697, MinusLogProbMetric: 102.4697, val_loss: 102.7346, val_MinusLogProbMetric: 102.7346

Epoch 826: val_loss improved from 102.75047 to 102.73458, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.4697 - MinusLogProbMetric: 102.4697 - val_loss: 102.7346 - val_MinusLogProbMetric: 102.7346 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 827/1000
2023-10-26 22:52:52.568 
Epoch 827/1000 
	 loss: 102.4684, MinusLogProbMetric: 102.4684, val_loss: 102.7431, val_MinusLogProbMetric: 102.7431

Epoch 827: val_loss did not improve from 102.73458
196/196 - 83s - loss: 102.4684 - MinusLogProbMetric: 102.4684 - val_loss: 102.7431 - val_MinusLogProbMetric: 102.7431 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 828/1000
2023-10-26 22:54:14.696 
Epoch 828/1000 
	 loss: 102.4591, MinusLogProbMetric: 102.4591, val_loss: 102.7376, val_MinusLogProbMetric: 102.7376

Epoch 828: val_loss did not improve from 102.73458
196/196 - 82s - loss: 102.4591 - MinusLogProbMetric: 102.4591 - val_loss: 102.7376 - val_MinusLogProbMetric: 102.7376 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 829/1000
2023-10-26 22:55:37.144 
Epoch 829/1000 
	 loss: 102.4611, MinusLogProbMetric: 102.4611, val_loss: 102.7354, val_MinusLogProbMetric: 102.7354

Epoch 829: val_loss did not improve from 102.73458
196/196 - 82s - loss: 102.4611 - MinusLogProbMetric: 102.4611 - val_loss: 102.7354 - val_MinusLogProbMetric: 102.7354 - lr: 5.0805e-08 - 82s/epoch - 421ms/step
Epoch 830/1000
2023-10-26 22:56:59.257 
Epoch 830/1000 
	 loss: 102.4638, MinusLogProbMetric: 102.4638, val_loss: 102.7468, val_MinusLogProbMetric: 102.7468

Epoch 830: val_loss did not improve from 102.73458
196/196 - 82s - loss: 102.4638 - MinusLogProbMetric: 102.4638 - val_loss: 102.7468 - val_MinusLogProbMetric: 102.7468 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 831/1000
2023-10-26 22:58:21.790 
Epoch 831/1000 
	 loss: 102.4563, MinusLogProbMetric: 102.4563, val_loss: 102.7318, val_MinusLogProbMetric: 102.7318

Epoch 831: val_loss improved from 102.73458 to 102.73185, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.4563 - MinusLogProbMetric: 102.4563 - val_loss: 102.7318 - val_MinusLogProbMetric: 102.7318 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 832/1000
2023-10-26 22:59:45.776 
Epoch 832/1000 
	 loss: 102.4543, MinusLogProbMetric: 102.4543, val_loss: 102.7421, val_MinusLogProbMetric: 102.7421

Epoch 832: val_loss did not improve from 102.73185
196/196 - 82s - loss: 102.4543 - MinusLogProbMetric: 102.4543 - val_loss: 102.7421 - val_MinusLogProbMetric: 102.7421 - lr: 5.0805e-08 - 82s/epoch - 421ms/step
Epoch 833/1000
2023-10-26 23:01:07.181 
Epoch 833/1000 
	 loss: 102.4640, MinusLogProbMetric: 102.4640, val_loss: 102.7388, val_MinusLogProbMetric: 102.7388

Epoch 833: val_loss did not improve from 102.73185
196/196 - 81s - loss: 102.4640 - MinusLogProbMetric: 102.4640 - val_loss: 102.7388 - val_MinusLogProbMetric: 102.7388 - lr: 5.0805e-08 - 81s/epoch - 415ms/step
Epoch 834/1000
2023-10-26 23:02:29.055 
Epoch 834/1000 
	 loss: 102.4436, MinusLogProbMetric: 102.4436, val_loss: 102.7291, val_MinusLogProbMetric: 102.7291

Epoch 834: val_loss improved from 102.73185 to 102.72906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.4436 - MinusLogProbMetric: 102.4436 - val_loss: 102.7291 - val_MinusLogProbMetric: 102.7291 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 835/1000
2023-10-26 23:03:52.522 
Epoch 835/1000 
	 loss: 102.4457, MinusLogProbMetric: 102.4457, val_loss: 102.7148, val_MinusLogProbMetric: 102.7148

Epoch 835: val_loss improved from 102.72906 to 102.71476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.4457 - MinusLogProbMetric: 102.4457 - val_loss: 102.7148 - val_MinusLogProbMetric: 102.7148 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 836/1000
2023-10-26 23:05:16.051 
Epoch 836/1000 
	 loss: 102.4425, MinusLogProbMetric: 102.4425, val_loss: 102.7237, val_MinusLogProbMetric: 102.7237

Epoch 836: val_loss did not improve from 102.71476
196/196 - 82s - loss: 102.4425 - MinusLogProbMetric: 102.4425 - val_loss: 102.7237 - val_MinusLogProbMetric: 102.7237 - lr: 5.0805e-08 - 82s/epoch - 416ms/step
Epoch 837/1000
2023-10-26 23:06:38.707 
Epoch 837/1000 
	 loss: 102.4310, MinusLogProbMetric: 102.4310, val_loss: 102.7305, val_MinusLogProbMetric: 102.7305

Epoch 837: val_loss did not improve from 102.71476
196/196 - 83s - loss: 102.4310 - MinusLogProbMetric: 102.4310 - val_loss: 102.7305 - val_MinusLogProbMetric: 102.7305 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 838/1000
2023-10-26 23:08:01.734 
Epoch 838/1000 
	 loss: 102.4374, MinusLogProbMetric: 102.4374, val_loss: 102.7117, val_MinusLogProbMetric: 102.7117

Epoch 838: val_loss improved from 102.71476 to 102.71169, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 102.4374 - MinusLogProbMetric: 102.4374 - val_loss: 102.7117 - val_MinusLogProbMetric: 102.7117 - lr: 5.0805e-08 - 85s/epoch - 432ms/step
Epoch 839/1000
2023-10-26 23:09:24.851 
Epoch 839/1000 
	 loss: 102.4301, MinusLogProbMetric: 102.4301, val_loss: 102.7073, val_MinusLogProbMetric: 102.7073

Epoch 839: val_loss improved from 102.71169 to 102.70731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.4301 - MinusLogProbMetric: 102.4301 - val_loss: 102.7073 - val_MinusLogProbMetric: 102.7073 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 840/1000
2023-10-26 23:10:48.524 
Epoch 840/1000 
	 loss: 102.4406, MinusLogProbMetric: 102.4406, val_loss: 102.7149, val_MinusLogProbMetric: 102.7149

Epoch 840: val_loss did not improve from 102.70731
196/196 - 82s - loss: 102.4406 - MinusLogProbMetric: 102.4406 - val_loss: 102.7149 - val_MinusLogProbMetric: 102.7149 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 841/1000
2023-10-26 23:12:09.899 
Epoch 841/1000 
	 loss: 102.4307, MinusLogProbMetric: 102.4307, val_loss: 102.6972, val_MinusLogProbMetric: 102.6972

Epoch 841: val_loss improved from 102.70731 to 102.69720, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.4307 - MinusLogProbMetric: 102.4307 - val_loss: 102.6972 - val_MinusLogProbMetric: 102.6972 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 842/1000
2023-10-26 23:13:34.626 
Epoch 842/1000 
	 loss: 102.4334, MinusLogProbMetric: 102.4334, val_loss: 102.7085, val_MinusLogProbMetric: 102.7085

Epoch 842: val_loss did not improve from 102.69720
196/196 - 83s - loss: 102.4334 - MinusLogProbMetric: 102.4334 - val_loss: 102.7085 - val_MinusLogProbMetric: 102.7085 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 843/1000
2023-10-26 23:14:56.251 
Epoch 843/1000 
	 loss: 102.4312, MinusLogProbMetric: 102.4312, val_loss: 102.7132, val_MinusLogProbMetric: 102.7132

Epoch 843: val_loss did not improve from 102.69720
196/196 - 82s - loss: 102.4312 - MinusLogProbMetric: 102.4312 - val_loss: 102.7132 - val_MinusLogProbMetric: 102.7132 - lr: 5.0805e-08 - 82s/epoch - 416ms/step
Epoch 844/1000
2023-10-26 23:16:18.152 
Epoch 844/1000 
	 loss: 102.4258, MinusLogProbMetric: 102.4258, val_loss: 102.6964, val_MinusLogProbMetric: 102.6964

Epoch 844: val_loss improved from 102.69720 to 102.69645, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.4258 - MinusLogProbMetric: 102.4258 - val_loss: 102.6964 - val_MinusLogProbMetric: 102.6964 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 845/1000
2023-10-26 23:17:42.395 
Epoch 845/1000 
	 loss: 102.4121, MinusLogProbMetric: 102.4121, val_loss: 102.7063, val_MinusLogProbMetric: 102.7063

Epoch 845: val_loss did not improve from 102.69645
196/196 - 82s - loss: 102.4121 - MinusLogProbMetric: 102.4121 - val_loss: 102.7063 - val_MinusLogProbMetric: 102.7063 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 846/1000
2023-10-26 23:19:04.390 
Epoch 846/1000 
	 loss: 102.4185, MinusLogProbMetric: 102.4185, val_loss: 102.7037, val_MinusLogProbMetric: 102.7037

Epoch 846: val_loss did not improve from 102.69645
196/196 - 82s - loss: 102.4185 - MinusLogProbMetric: 102.4185 - val_loss: 102.7037 - val_MinusLogProbMetric: 102.7037 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 847/1000
2023-10-26 23:20:26.936 
Epoch 847/1000 
	 loss: 102.4169, MinusLogProbMetric: 102.4169, val_loss: 102.6907, val_MinusLogProbMetric: 102.6907

Epoch 847: val_loss improved from 102.69645 to 102.69067, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.4169 - MinusLogProbMetric: 102.4169 - val_loss: 102.6907 - val_MinusLogProbMetric: 102.6907 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 848/1000
2023-10-26 23:21:50.685 
Epoch 848/1000 
	 loss: 102.4186, MinusLogProbMetric: 102.4186, val_loss: 102.6953, val_MinusLogProbMetric: 102.6953

Epoch 848: val_loss did not improve from 102.69067
196/196 - 82s - loss: 102.4186 - MinusLogProbMetric: 102.4186 - val_loss: 102.6953 - val_MinusLogProbMetric: 102.6953 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 849/1000
2023-10-26 23:23:12.480 
Epoch 849/1000 
	 loss: 102.4050, MinusLogProbMetric: 102.4050, val_loss: 102.6892, val_MinusLogProbMetric: 102.6892

Epoch 849: val_loss improved from 102.69067 to 102.68923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.4050 - MinusLogProbMetric: 102.4050 - val_loss: 102.6892 - val_MinusLogProbMetric: 102.6892 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 850/1000
2023-10-26 23:24:36.128 
Epoch 850/1000 
	 loss: 102.4077, MinusLogProbMetric: 102.4077, val_loss: 102.6855, val_MinusLogProbMetric: 102.6855

Epoch 850: val_loss improved from 102.68923 to 102.68554, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.4077 - MinusLogProbMetric: 102.4077 - val_loss: 102.6855 - val_MinusLogProbMetric: 102.6855 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 851/1000
2023-10-26 23:25:59.866 
Epoch 851/1000 
	 loss: 102.4052, MinusLogProbMetric: 102.4052, val_loss: 102.6865, val_MinusLogProbMetric: 102.6865

Epoch 851: val_loss did not improve from 102.68554
196/196 - 82s - loss: 102.4052 - MinusLogProbMetric: 102.4052 - val_loss: 102.6865 - val_MinusLogProbMetric: 102.6865 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 852/1000
2023-10-26 23:27:22.035 
Epoch 852/1000 
	 loss: 102.4010, MinusLogProbMetric: 102.4010, val_loss: 102.6590, val_MinusLogProbMetric: 102.6590

Epoch 852: val_loss improved from 102.68554 to 102.65901, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.4010 - MinusLogProbMetric: 102.4010 - val_loss: 102.6590 - val_MinusLogProbMetric: 102.6590 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 853/1000
2023-10-26 23:28:46.873 
Epoch 853/1000 
	 loss: 102.3986, MinusLogProbMetric: 102.3986, val_loss: 102.6885, val_MinusLogProbMetric: 102.6885

Epoch 853: val_loss did not improve from 102.65901
196/196 - 83s - loss: 102.3986 - MinusLogProbMetric: 102.3986 - val_loss: 102.6885 - val_MinusLogProbMetric: 102.6885 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 854/1000
2023-10-26 23:30:08.721 
Epoch 854/1000 
	 loss: 102.3894, MinusLogProbMetric: 102.3894, val_loss: 102.6625, val_MinusLogProbMetric: 102.6625

Epoch 854: val_loss did not improve from 102.65901
196/196 - 82s - loss: 102.3894 - MinusLogProbMetric: 102.3894 - val_loss: 102.6625 - val_MinusLogProbMetric: 102.6625 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 855/1000
2023-10-26 23:31:29.295 
Epoch 855/1000 
	 loss: 102.3907, MinusLogProbMetric: 102.3907, val_loss: 102.6692, val_MinusLogProbMetric: 102.6692

Epoch 855: val_loss did not improve from 102.65901
196/196 - 81s - loss: 102.3907 - MinusLogProbMetric: 102.3907 - val_loss: 102.6692 - val_MinusLogProbMetric: 102.6692 - lr: 5.0805e-08 - 81s/epoch - 411ms/step
Epoch 856/1000
2023-10-26 23:32:51.381 
Epoch 856/1000 
	 loss: 102.3847, MinusLogProbMetric: 102.3847, val_loss: 102.6727, val_MinusLogProbMetric: 102.6727

Epoch 856: val_loss did not improve from 102.65901
196/196 - 82s - loss: 102.3847 - MinusLogProbMetric: 102.3847 - val_loss: 102.6727 - val_MinusLogProbMetric: 102.6727 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 857/1000
2023-10-26 23:34:13.677 
Epoch 857/1000 
	 loss: 102.3870, MinusLogProbMetric: 102.3870, val_loss: 102.6756, val_MinusLogProbMetric: 102.6756

Epoch 857: val_loss did not improve from 102.65901
196/196 - 82s - loss: 102.3870 - MinusLogProbMetric: 102.3870 - val_loss: 102.6756 - val_MinusLogProbMetric: 102.6756 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 858/1000
2023-10-26 23:35:35.348 
Epoch 858/1000 
	 loss: 102.3847, MinusLogProbMetric: 102.3847, val_loss: 102.6821, val_MinusLogProbMetric: 102.6821

Epoch 858: val_loss did not improve from 102.65901
196/196 - 82s - loss: 102.3847 - MinusLogProbMetric: 102.3847 - val_loss: 102.6821 - val_MinusLogProbMetric: 102.6821 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 859/1000
2023-10-26 23:36:58.087 
Epoch 859/1000 
	 loss: 102.3778, MinusLogProbMetric: 102.3778, val_loss: 102.6492, val_MinusLogProbMetric: 102.6492

Epoch 859: val_loss improved from 102.65901 to 102.64917, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.3778 - MinusLogProbMetric: 102.3778 - val_loss: 102.6492 - val_MinusLogProbMetric: 102.6492 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 860/1000
2023-10-26 23:38:21.448 
Epoch 860/1000 
	 loss: 102.3782, MinusLogProbMetric: 102.3782, val_loss: 102.6679, val_MinusLogProbMetric: 102.6679

Epoch 860: val_loss did not improve from 102.64917
196/196 - 82s - loss: 102.3782 - MinusLogProbMetric: 102.3782 - val_loss: 102.6679 - val_MinusLogProbMetric: 102.6679 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 861/1000
2023-10-26 23:39:43.690 
Epoch 861/1000 
	 loss: 102.3827, MinusLogProbMetric: 102.3827, val_loss: 102.6427, val_MinusLogProbMetric: 102.6427

Epoch 861: val_loss improved from 102.64917 to 102.64270, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.3827 - MinusLogProbMetric: 102.3827 - val_loss: 102.6427 - val_MinusLogProbMetric: 102.6427 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 862/1000
2023-10-26 23:41:06.701 
Epoch 862/1000 
	 loss: 102.3745, MinusLogProbMetric: 102.3745, val_loss: 102.6304, val_MinusLogProbMetric: 102.6304

Epoch 862: val_loss improved from 102.64270 to 102.63042, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.3745 - MinusLogProbMetric: 102.3745 - val_loss: 102.6304 - val_MinusLogProbMetric: 102.6304 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 863/1000
2023-10-26 23:42:30.357 
Epoch 863/1000 
	 loss: 102.3697, MinusLogProbMetric: 102.3697, val_loss: 102.6509, val_MinusLogProbMetric: 102.6509

Epoch 863: val_loss did not improve from 102.63042
196/196 - 82s - loss: 102.3697 - MinusLogProbMetric: 102.3697 - val_loss: 102.6509 - val_MinusLogProbMetric: 102.6509 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 864/1000
2023-10-26 23:43:52.268 
Epoch 864/1000 
	 loss: 102.3709, MinusLogProbMetric: 102.3709, val_loss: 102.6513, val_MinusLogProbMetric: 102.6513

Epoch 864: val_loss did not improve from 102.63042
196/196 - 82s - loss: 102.3709 - MinusLogProbMetric: 102.3709 - val_loss: 102.6513 - val_MinusLogProbMetric: 102.6513 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 865/1000
2023-10-26 23:45:14.521 
Epoch 865/1000 
	 loss: 102.3640, MinusLogProbMetric: 102.3640, val_loss: 102.6516, val_MinusLogProbMetric: 102.6516

Epoch 865: val_loss did not improve from 102.63042
196/196 - 82s - loss: 102.3640 - MinusLogProbMetric: 102.3640 - val_loss: 102.6516 - val_MinusLogProbMetric: 102.6516 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 866/1000
2023-10-26 23:46:36.889 
Epoch 866/1000 
	 loss: 102.3629, MinusLogProbMetric: 102.3629, val_loss: 102.6439, val_MinusLogProbMetric: 102.6439

Epoch 866: val_loss did not improve from 102.63042
196/196 - 82s - loss: 102.3629 - MinusLogProbMetric: 102.3629 - val_loss: 102.6439 - val_MinusLogProbMetric: 102.6439 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 867/1000
2023-10-26 23:47:59.620 
Epoch 867/1000 
	 loss: 102.3584, MinusLogProbMetric: 102.3584, val_loss: 102.6459, val_MinusLogProbMetric: 102.6459

Epoch 867: val_loss did not improve from 102.63042
196/196 - 83s - loss: 102.3584 - MinusLogProbMetric: 102.3584 - val_loss: 102.6459 - val_MinusLogProbMetric: 102.6459 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 868/1000
2023-10-26 23:49:21.915 
Epoch 868/1000 
	 loss: 102.3562, MinusLogProbMetric: 102.3562, val_loss: 102.6479, val_MinusLogProbMetric: 102.6479

Epoch 868: val_loss did not improve from 102.63042
196/196 - 82s - loss: 102.3562 - MinusLogProbMetric: 102.3562 - val_loss: 102.6479 - val_MinusLogProbMetric: 102.6479 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 869/1000
2023-10-26 23:50:44.217 
Epoch 869/1000 
	 loss: 102.3542, MinusLogProbMetric: 102.3542, val_loss: 102.6795, val_MinusLogProbMetric: 102.6795

Epoch 869: val_loss did not improve from 102.63042
196/196 - 82s - loss: 102.3542 - MinusLogProbMetric: 102.3542 - val_loss: 102.6795 - val_MinusLogProbMetric: 102.6795 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 870/1000
2023-10-26 23:52:06.166 
Epoch 870/1000 
	 loss: 102.3637, MinusLogProbMetric: 102.3637, val_loss: 102.6592, val_MinusLogProbMetric: 102.6592

Epoch 870: val_loss did not improve from 102.63042
196/196 - 82s - loss: 102.3637 - MinusLogProbMetric: 102.3637 - val_loss: 102.6592 - val_MinusLogProbMetric: 102.6592 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 871/1000
2023-10-26 23:53:28.756 
Epoch 871/1000 
	 loss: 102.3575, MinusLogProbMetric: 102.3575, val_loss: 102.6539, val_MinusLogProbMetric: 102.6539

Epoch 871: val_loss did not improve from 102.63042
196/196 - 83s - loss: 102.3575 - MinusLogProbMetric: 102.3575 - val_loss: 102.6539 - val_MinusLogProbMetric: 102.6539 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 872/1000
2023-10-26 23:54:50.735 
Epoch 872/1000 
	 loss: 102.3456, MinusLogProbMetric: 102.3456, val_loss: 102.6583, val_MinusLogProbMetric: 102.6583

Epoch 872: val_loss did not improve from 102.63042
196/196 - 82s - loss: 102.3456 - MinusLogProbMetric: 102.3456 - val_loss: 102.6583 - val_MinusLogProbMetric: 102.6583 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 873/1000
2023-10-26 23:56:12.438 
Epoch 873/1000 
	 loss: 102.3412, MinusLogProbMetric: 102.3412, val_loss: 102.6482, val_MinusLogProbMetric: 102.6482

Epoch 873: val_loss did not improve from 102.63042
196/196 - 82s - loss: 102.3412 - MinusLogProbMetric: 102.3412 - val_loss: 102.6482 - val_MinusLogProbMetric: 102.6482 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 874/1000
2023-10-26 23:57:35.022 
Epoch 874/1000 
	 loss: 102.3439, MinusLogProbMetric: 102.3439, val_loss: 102.6525, val_MinusLogProbMetric: 102.6525

Epoch 874: val_loss did not improve from 102.63042
196/196 - 83s - loss: 102.3439 - MinusLogProbMetric: 102.3439 - val_loss: 102.6525 - val_MinusLogProbMetric: 102.6525 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 875/1000
2023-10-26 23:58:57.322 
Epoch 875/1000 
	 loss: 102.3396, MinusLogProbMetric: 102.3396, val_loss: 102.6385, val_MinusLogProbMetric: 102.6385

Epoch 875: val_loss did not improve from 102.63042
196/196 - 82s - loss: 102.3396 - MinusLogProbMetric: 102.3396 - val_loss: 102.6385 - val_MinusLogProbMetric: 102.6385 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 876/1000
2023-10-27 00:00:19.125 
Epoch 876/1000 
	 loss: 102.3342, MinusLogProbMetric: 102.3342, val_loss: 102.6379, val_MinusLogProbMetric: 102.6380

Epoch 876: val_loss did not improve from 102.63042
196/196 - 82s - loss: 102.3342 - MinusLogProbMetric: 102.3342 - val_loss: 102.6379 - val_MinusLogProbMetric: 102.6380 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 877/1000
2023-10-27 00:01:41.791 
Epoch 877/1000 
	 loss: 102.3310, MinusLogProbMetric: 102.3310, val_loss: 102.6283, val_MinusLogProbMetric: 102.6283

Epoch 877: val_loss improved from 102.63042 to 102.62835, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.3310 - MinusLogProbMetric: 102.3310 - val_loss: 102.6283 - val_MinusLogProbMetric: 102.6283 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 878/1000
2023-10-27 00:03:05.737 
Epoch 878/1000 
	 loss: 102.3288, MinusLogProbMetric: 102.3288, val_loss: 102.6312, val_MinusLogProbMetric: 102.6312

Epoch 878: val_loss did not improve from 102.62835
196/196 - 82s - loss: 102.3288 - MinusLogProbMetric: 102.3288 - val_loss: 102.6312 - val_MinusLogProbMetric: 102.6312 - lr: 5.0805e-08 - 82s/epoch - 421ms/step
Epoch 879/1000
2023-10-27 00:04:27.750 
Epoch 879/1000 
	 loss: 102.3226, MinusLogProbMetric: 102.3226, val_loss: 102.6337, val_MinusLogProbMetric: 102.6337

Epoch 879: val_loss did not improve from 102.62835
196/196 - 82s - loss: 102.3226 - MinusLogProbMetric: 102.3226 - val_loss: 102.6337 - val_MinusLogProbMetric: 102.6337 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 880/1000
2023-10-27 00:05:49.965 
Epoch 880/1000 
	 loss: 102.3151, MinusLogProbMetric: 102.3151, val_loss: 102.6249, val_MinusLogProbMetric: 102.6249

Epoch 880: val_loss improved from 102.62835 to 102.62491, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.3151 - MinusLogProbMetric: 102.3151 - val_loss: 102.6249 - val_MinusLogProbMetric: 102.6249 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 881/1000
2023-10-27 00:07:13.760 
Epoch 881/1000 
	 loss: 102.3162, MinusLogProbMetric: 102.3162, val_loss: 102.6154, val_MinusLogProbMetric: 102.6154

Epoch 881: val_loss improved from 102.62491 to 102.61543, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.3162 - MinusLogProbMetric: 102.3162 - val_loss: 102.6154 - val_MinusLogProbMetric: 102.6154 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 882/1000
2023-10-27 00:08:37.906 
Epoch 882/1000 
	 loss: 102.3165, MinusLogProbMetric: 102.3165, val_loss: 102.6092, val_MinusLogProbMetric: 102.6092

Epoch 882: val_loss improved from 102.61543 to 102.60919, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.3165 - MinusLogProbMetric: 102.3165 - val_loss: 102.6092 - val_MinusLogProbMetric: 102.6092 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 883/1000
2023-10-27 00:10:02.397 
Epoch 883/1000 
	 loss: 102.3106, MinusLogProbMetric: 102.3106, val_loss: 102.6113, val_MinusLogProbMetric: 102.6113

Epoch 883: val_loss did not improve from 102.60919
196/196 - 82s - loss: 102.3106 - MinusLogProbMetric: 102.3106 - val_loss: 102.6113 - val_MinusLogProbMetric: 102.6113 - lr: 5.0805e-08 - 82s/epoch - 421ms/step
Epoch 884/1000
2023-10-27 00:11:24.640 
Epoch 884/1000 
	 loss: 102.3100, MinusLogProbMetric: 102.3100, val_loss: 102.6047, val_MinusLogProbMetric: 102.6047

Epoch 884: val_loss improved from 102.60919 to 102.60471, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.3100 - MinusLogProbMetric: 102.3100 - val_loss: 102.6047 - val_MinusLogProbMetric: 102.6047 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 885/1000
2023-10-27 00:12:48.937 
Epoch 885/1000 
	 loss: 102.3030, MinusLogProbMetric: 102.3030, val_loss: 102.6042, val_MinusLogProbMetric: 102.6042

Epoch 885: val_loss improved from 102.60471 to 102.60421, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.3030 - MinusLogProbMetric: 102.3030 - val_loss: 102.6042 - val_MinusLogProbMetric: 102.6042 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 886/1000
2023-10-27 00:14:12.965 
Epoch 886/1000 
	 loss: 102.3049, MinusLogProbMetric: 102.3049, val_loss: 102.6022, val_MinusLogProbMetric: 102.6022

Epoch 886: val_loss improved from 102.60421 to 102.60221, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.3049 - MinusLogProbMetric: 102.3049 - val_loss: 102.6022 - val_MinusLogProbMetric: 102.6022 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 887/1000
2023-10-27 00:15:26.641 
Epoch 887/1000 
	 loss: 102.2943, MinusLogProbMetric: 102.2943, val_loss: 102.5983, val_MinusLogProbMetric: 102.5983

Epoch 887: val_loss improved from 102.60221 to 102.59829, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 74s - loss: 102.2943 - MinusLogProbMetric: 102.2943 - val_loss: 102.5983 - val_MinusLogProbMetric: 102.5983 - lr: 5.0805e-08 - 74s/epoch - 376ms/step
Epoch 888/1000
2023-10-27 00:16:51.229 
Epoch 888/1000 
	 loss: 102.2921, MinusLogProbMetric: 102.2921, val_loss: 102.6023, val_MinusLogProbMetric: 102.6023

Epoch 888: val_loss did not improve from 102.59829
196/196 - 83s - loss: 102.2921 - MinusLogProbMetric: 102.2921 - val_loss: 102.6023 - val_MinusLogProbMetric: 102.6023 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 889/1000
2023-10-27 00:18:13.684 
Epoch 889/1000 
	 loss: 102.2980, MinusLogProbMetric: 102.2980, val_loss: 102.5840, val_MinusLogProbMetric: 102.5840

Epoch 889: val_loss improved from 102.59829 to 102.58403, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.2980 - MinusLogProbMetric: 102.2980 - val_loss: 102.5840 - val_MinusLogProbMetric: 102.5840 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 890/1000
2023-10-27 00:19:37.867 
Epoch 890/1000 
	 loss: 102.2902, MinusLogProbMetric: 102.2902, val_loss: 102.5835, val_MinusLogProbMetric: 102.5835

Epoch 890: val_loss improved from 102.58403 to 102.58351, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.2902 - MinusLogProbMetric: 102.2902 - val_loss: 102.5835 - val_MinusLogProbMetric: 102.5835 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 891/1000
2023-10-27 00:21:01.591 
Epoch 891/1000 
	 loss: 102.2898, MinusLogProbMetric: 102.2898, val_loss: 102.5910, val_MinusLogProbMetric: 102.5910

Epoch 891: val_loss did not improve from 102.58351
196/196 - 82s - loss: 102.2898 - MinusLogProbMetric: 102.2898 - val_loss: 102.5910 - val_MinusLogProbMetric: 102.5910 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 892/1000
2023-10-27 00:22:24.684 
Epoch 892/1000 
	 loss: 102.2904, MinusLogProbMetric: 102.2904, val_loss: 102.5951, val_MinusLogProbMetric: 102.5951

Epoch 892: val_loss did not improve from 102.58351
196/196 - 83s - loss: 102.2904 - MinusLogProbMetric: 102.2904 - val_loss: 102.5951 - val_MinusLogProbMetric: 102.5951 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 893/1000
2023-10-27 00:23:46.788 
Epoch 893/1000 
	 loss: 102.2864, MinusLogProbMetric: 102.2864, val_loss: 102.5923, val_MinusLogProbMetric: 102.5923

Epoch 893: val_loss did not improve from 102.58351
196/196 - 82s - loss: 102.2864 - MinusLogProbMetric: 102.2864 - val_loss: 102.5923 - val_MinusLogProbMetric: 102.5923 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 894/1000
2023-10-27 00:25:09.590 
Epoch 894/1000 
	 loss: 102.2870, MinusLogProbMetric: 102.2870, val_loss: 102.5753, val_MinusLogProbMetric: 102.5753

Epoch 894: val_loss improved from 102.58351 to 102.57526, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 102.2870 - MinusLogProbMetric: 102.2870 - val_loss: 102.5753 - val_MinusLogProbMetric: 102.5753 - lr: 5.0805e-08 - 85s/epoch - 432ms/step
Epoch 895/1000
2023-10-27 00:26:34.647 
Epoch 895/1000 
	 loss: 102.2829, MinusLogProbMetric: 102.2829, val_loss: 102.5802, val_MinusLogProbMetric: 102.5802

Epoch 895: val_loss did not improve from 102.57526
196/196 - 83s - loss: 102.2829 - MinusLogProbMetric: 102.2829 - val_loss: 102.5802 - val_MinusLogProbMetric: 102.5802 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 896/1000
2023-10-27 00:27:58.143 
Epoch 896/1000 
	 loss: 102.2768, MinusLogProbMetric: 102.2768, val_loss: 102.5811, val_MinusLogProbMetric: 102.5811

Epoch 896: val_loss did not improve from 102.57526
196/196 - 83s - loss: 102.2768 - MinusLogProbMetric: 102.2768 - val_loss: 102.5811 - val_MinusLogProbMetric: 102.5811 - lr: 5.0805e-08 - 83s/epoch - 426ms/step
Epoch 897/1000
2023-10-27 00:29:20.363 
Epoch 897/1000 
	 loss: 102.2713, MinusLogProbMetric: 102.2713, val_loss: 102.5665, val_MinusLogProbMetric: 102.5665

Epoch 897: val_loss improved from 102.57526 to 102.56654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.2713 - MinusLogProbMetric: 102.2713 - val_loss: 102.5665 - val_MinusLogProbMetric: 102.5665 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 898/1000
2023-10-27 00:30:44.157 
Epoch 898/1000 
	 loss: 102.2680, MinusLogProbMetric: 102.2680, val_loss: 102.5677, val_MinusLogProbMetric: 102.5677

Epoch 898: val_loss did not improve from 102.56654
196/196 - 82s - loss: 102.2680 - MinusLogProbMetric: 102.2680 - val_loss: 102.5677 - val_MinusLogProbMetric: 102.5677 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 899/1000
2023-10-27 00:32:06.558 
Epoch 899/1000 
	 loss: 102.2716, MinusLogProbMetric: 102.2716, val_loss: 102.5526, val_MinusLogProbMetric: 102.5526

Epoch 899: val_loss improved from 102.56654 to 102.55264, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 102.2716 - MinusLogProbMetric: 102.2716 - val_loss: 102.5526 - val_MinusLogProbMetric: 102.5526 - lr: 5.0805e-08 - 85s/epoch - 431ms/step
Epoch 900/1000
2023-10-27 00:33:31.978 
Epoch 900/1000 
	 loss: 102.2752, MinusLogProbMetric: 102.2752, val_loss: 102.5518, val_MinusLogProbMetric: 102.5518

Epoch 900: val_loss improved from 102.55264 to 102.55184, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 102.2752 - MinusLogProbMetric: 102.2752 - val_loss: 102.5518 - val_MinusLogProbMetric: 102.5518 - lr: 5.0805e-08 - 85s/epoch - 433ms/step
Epoch 901/1000
2023-10-27 00:34:57.212 
Epoch 901/1000 
	 loss: 102.2681, MinusLogProbMetric: 102.2681, val_loss: 102.5382, val_MinusLogProbMetric: 102.5382

Epoch 901: val_loss improved from 102.55184 to 102.53815, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 102.2681 - MinusLogProbMetric: 102.2681 - val_loss: 102.5382 - val_MinusLogProbMetric: 102.5382 - lr: 5.0805e-08 - 85s/epoch - 434ms/step
Epoch 902/1000
2023-10-27 00:36:20.548 
Epoch 902/1000 
	 loss: 102.2635, MinusLogProbMetric: 102.2635, val_loss: 102.5433, val_MinusLogProbMetric: 102.5433

Epoch 902: val_loss did not improve from 102.53815
196/196 - 82s - loss: 102.2635 - MinusLogProbMetric: 102.2635 - val_loss: 102.5433 - val_MinusLogProbMetric: 102.5433 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 903/1000
2023-10-27 00:37:43.323 
Epoch 903/1000 
	 loss: 102.2612, MinusLogProbMetric: 102.2612, val_loss: 102.5595, val_MinusLogProbMetric: 102.5595

Epoch 903: val_loss did not improve from 102.53815
196/196 - 83s - loss: 102.2612 - MinusLogProbMetric: 102.2612 - val_loss: 102.5595 - val_MinusLogProbMetric: 102.5595 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 904/1000
2023-10-27 00:39:07.017 
Epoch 904/1000 
	 loss: 102.2577, MinusLogProbMetric: 102.2577, val_loss: 102.5544, val_MinusLogProbMetric: 102.5544

Epoch 904: val_loss did not improve from 102.53815
196/196 - 84s - loss: 102.2577 - MinusLogProbMetric: 102.2577 - val_loss: 102.5544 - val_MinusLogProbMetric: 102.5544 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 905/1000
2023-10-27 00:40:29.937 
Epoch 905/1000 
	 loss: 102.2527, MinusLogProbMetric: 102.2527, val_loss: 102.5475, val_MinusLogProbMetric: 102.5475

Epoch 905: val_loss did not improve from 102.53815
196/196 - 83s - loss: 102.2527 - MinusLogProbMetric: 102.2527 - val_loss: 102.5475 - val_MinusLogProbMetric: 102.5475 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 906/1000
2023-10-27 00:41:52.992 
Epoch 906/1000 
	 loss: 102.2568, MinusLogProbMetric: 102.2568, val_loss: 102.5460, val_MinusLogProbMetric: 102.5460

Epoch 906: val_loss did not improve from 102.53815
196/196 - 83s - loss: 102.2568 - MinusLogProbMetric: 102.2568 - val_loss: 102.5460 - val_MinusLogProbMetric: 102.5460 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 907/1000
2023-10-27 00:43:15.935 
Epoch 907/1000 
	 loss: 102.2496, MinusLogProbMetric: 102.2496, val_loss: 102.5464, val_MinusLogProbMetric: 102.5464

Epoch 907: val_loss did not improve from 102.53815
196/196 - 83s - loss: 102.2496 - MinusLogProbMetric: 102.2496 - val_loss: 102.5464 - val_MinusLogProbMetric: 102.5464 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 908/1000
2023-10-27 00:44:39.816 
Epoch 908/1000 
	 loss: 102.2483, MinusLogProbMetric: 102.2483, val_loss: 102.5500, val_MinusLogProbMetric: 102.5500

Epoch 908: val_loss did not improve from 102.53815
196/196 - 84s - loss: 102.2483 - MinusLogProbMetric: 102.2483 - val_loss: 102.5500 - val_MinusLogProbMetric: 102.5500 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 909/1000
2023-10-27 00:46:02.699 
Epoch 909/1000 
	 loss: 102.2442, MinusLogProbMetric: 102.2442, val_loss: 102.5218, val_MinusLogProbMetric: 102.5218

Epoch 909: val_loss improved from 102.53815 to 102.52176, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.2442 - MinusLogProbMetric: 102.2442 - val_loss: 102.5218 - val_MinusLogProbMetric: 102.5218 - lr: 5.0805e-08 - 84s/epoch - 431ms/step
Epoch 910/1000
2023-10-27 00:47:27.143 
Epoch 910/1000 
	 loss: 102.2449, MinusLogProbMetric: 102.2449, val_loss: 102.5380, val_MinusLogProbMetric: 102.5380

Epoch 910: val_loss did not improve from 102.52176
196/196 - 83s - loss: 102.2449 - MinusLogProbMetric: 102.2449 - val_loss: 102.5380 - val_MinusLogProbMetric: 102.5380 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 911/1000
2023-10-27 00:48:49.743 
Epoch 911/1000 
	 loss: 102.2443, MinusLogProbMetric: 102.2443, val_loss: 102.5274, val_MinusLogProbMetric: 102.5274

Epoch 911: val_loss did not improve from 102.52176
196/196 - 83s - loss: 102.2443 - MinusLogProbMetric: 102.2443 - val_loss: 102.5274 - val_MinusLogProbMetric: 102.5274 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 912/1000
2023-10-27 00:50:12.779 
Epoch 912/1000 
	 loss: 102.2331, MinusLogProbMetric: 102.2331, val_loss: 102.5396, val_MinusLogProbMetric: 102.5396

Epoch 912: val_loss did not improve from 102.52176
196/196 - 83s - loss: 102.2331 - MinusLogProbMetric: 102.2331 - val_loss: 102.5396 - val_MinusLogProbMetric: 102.5396 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 913/1000
2023-10-27 00:51:35.946 
Epoch 913/1000 
	 loss: 102.2367, MinusLogProbMetric: 102.2367, val_loss: 102.5381, val_MinusLogProbMetric: 102.5381

Epoch 913: val_loss did not improve from 102.52176
196/196 - 83s - loss: 102.2367 - MinusLogProbMetric: 102.2367 - val_loss: 102.5381 - val_MinusLogProbMetric: 102.5381 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 914/1000
2023-10-27 00:52:58.767 
Epoch 914/1000 
	 loss: 102.2301, MinusLogProbMetric: 102.2301, val_loss: 102.5441, val_MinusLogProbMetric: 102.5441

Epoch 914: val_loss did not improve from 102.52176
196/196 - 83s - loss: 102.2301 - MinusLogProbMetric: 102.2301 - val_loss: 102.5441 - val_MinusLogProbMetric: 102.5441 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 915/1000
2023-10-27 00:54:22.272 
Epoch 915/1000 
	 loss: 102.2333, MinusLogProbMetric: 102.2333, val_loss: 102.5375, val_MinusLogProbMetric: 102.5375

Epoch 915: val_loss did not improve from 102.52176
196/196 - 84s - loss: 102.2333 - MinusLogProbMetric: 102.2333 - val_loss: 102.5375 - val_MinusLogProbMetric: 102.5375 - lr: 5.0805e-08 - 84s/epoch - 426ms/step
Epoch 916/1000
2023-10-27 00:55:45.956 
Epoch 916/1000 
	 loss: 102.2298, MinusLogProbMetric: 102.2298, val_loss: 102.5294, val_MinusLogProbMetric: 102.5294

Epoch 916: val_loss did not improve from 102.52176
196/196 - 84s - loss: 102.2298 - MinusLogProbMetric: 102.2298 - val_loss: 102.5294 - val_MinusLogProbMetric: 102.5294 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 917/1000
2023-10-27 00:57:09.096 
Epoch 917/1000 
	 loss: 102.2284, MinusLogProbMetric: 102.2284, val_loss: 102.5350, val_MinusLogProbMetric: 102.5350

Epoch 917: val_loss did not improve from 102.52176
196/196 - 83s - loss: 102.2284 - MinusLogProbMetric: 102.2284 - val_loss: 102.5350 - val_MinusLogProbMetric: 102.5350 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 918/1000
2023-10-27 00:58:32.227 
Epoch 918/1000 
	 loss: 102.2238, MinusLogProbMetric: 102.2238, val_loss: 102.5109, val_MinusLogProbMetric: 102.5109

Epoch 918: val_loss improved from 102.52176 to 102.51086, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 102.2238 - MinusLogProbMetric: 102.2238 - val_loss: 102.5109 - val_MinusLogProbMetric: 102.5109 - lr: 5.0805e-08 - 85s/epoch - 434ms/step
Epoch 919/1000
2023-10-27 00:59:57.045 
Epoch 919/1000 
	 loss: 102.2197, MinusLogProbMetric: 102.2197, val_loss: 102.4932, val_MinusLogProbMetric: 102.4932

Epoch 919: val_loss improved from 102.51086 to 102.49319, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.2197 - MinusLogProbMetric: 102.2197 - val_loss: 102.4932 - val_MinusLogProbMetric: 102.4932 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 920/1000
2023-10-27 01:01:21.580 
Epoch 920/1000 
	 loss: 102.2250, MinusLogProbMetric: 102.2250, val_loss: 102.5292, val_MinusLogProbMetric: 102.5292

Epoch 920: val_loss did not improve from 102.49319
196/196 - 83s - loss: 102.2250 - MinusLogProbMetric: 102.2250 - val_loss: 102.5292 - val_MinusLogProbMetric: 102.5292 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 921/1000
2023-10-27 01:02:45.201 
Epoch 921/1000 
	 loss: 102.2166, MinusLogProbMetric: 102.2166, val_loss: 102.5184, val_MinusLogProbMetric: 102.5184

Epoch 921: val_loss did not improve from 102.49319
196/196 - 84s - loss: 102.2166 - MinusLogProbMetric: 102.2166 - val_loss: 102.5184 - val_MinusLogProbMetric: 102.5184 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 922/1000
2023-10-27 01:04:08.458 
Epoch 922/1000 
	 loss: 102.2159, MinusLogProbMetric: 102.2159, val_loss: 102.5217, val_MinusLogProbMetric: 102.5217

Epoch 922: val_loss did not improve from 102.49319
196/196 - 83s - loss: 102.2159 - MinusLogProbMetric: 102.2159 - val_loss: 102.5217 - val_MinusLogProbMetric: 102.5217 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 923/1000
2023-10-27 01:05:31.630 
Epoch 923/1000 
	 loss: 102.2127, MinusLogProbMetric: 102.2127, val_loss: 102.5119, val_MinusLogProbMetric: 102.5119

Epoch 923: val_loss did not improve from 102.49319
196/196 - 83s - loss: 102.2127 - MinusLogProbMetric: 102.2127 - val_loss: 102.5119 - val_MinusLogProbMetric: 102.5119 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 924/1000
2023-10-27 01:06:54.948 
Epoch 924/1000 
	 loss: 102.2099, MinusLogProbMetric: 102.2099, val_loss: 102.5232, val_MinusLogProbMetric: 102.5232

Epoch 924: val_loss did not improve from 102.49319
196/196 - 83s - loss: 102.2099 - MinusLogProbMetric: 102.2099 - val_loss: 102.5232 - val_MinusLogProbMetric: 102.5232 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 925/1000
2023-10-27 01:08:18.264 
Epoch 925/1000 
	 loss: 102.2077, MinusLogProbMetric: 102.2077, val_loss: 102.5112, val_MinusLogProbMetric: 102.5112

Epoch 925: val_loss did not improve from 102.49319
196/196 - 83s - loss: 102.2077 - MinusLogProbMetric: 102.2077 - val_loss: 102.5112 - val_MinusLogProbMetric: 102.5112 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 926/1000
2023-10-27 01:09:41.017 
Epoch 926/1000 
	 loss: 102.2076, MinusLogProbMetric: 102.2076, val_loss: 102.5157, val_MinusLogProbMetric: 102.5157

Epoch 926: val_loss did not improve from 102.49319
196/196 - 83s - loss: 102.2076 - MinusLogProbMetric: 102.2076 - val_loss: 102.5157 - val_MinusLogProbMetric: 102.5157 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 927/1000
2023-10-27 01:11:03.846 
Epoch 927/1000 
	 loss: 102.2071, MinusLogProbMetric: 102.2071, val_loss: 102.5055, val_MinusLogProbMetric: 102.5055

Epoch 927: val_loss did not improve from 102.49319
196/196 - 83s - loss: 102.2071 - MinusLogProbMetric: 102.2071 - val_loss: 102.5055 - val_MinusLogProbMetric: 102.5055 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 928/1000
2023-10-27 01:12:26.167 
Epoch 928/1000 
	 loss: 102.2020, MinusLogProbMetric: 102.2020, val_loss: 102.5009, val_MinusLogProbMetric: 102.5009

Epoch 928: val_loss did not improve from 102.49319
196/196 - 82s - loss: 102.2020 - MinusLogProbMetric: 102.2020 - val_loss: 102.5009 - val_MinusLogProbMetric: 102.5009 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 929/1000
2023-10-27 01:13:49.415 
Epoch 929/1000 
	 loss: 102.2006, MinusLogProbMetric: 102.2006, val_loss: 102.4979, val_MinusLogProbMetric: 102.4979

Epoch 929: val_loss did not improve from 102.49319
196/196 - 83s - loss: 102.2006 - MinusLogProbMetric: 102.2006 - val_loss: 102.4979 - val_MinusLogProbMetric: 102.4979 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 930/1000
2023-10-27 01:15:12.840 
Epoch 930/1000 
	 loss: 102.1983, MinusLogProbMetric: 102.1983, val_loss: 102.4825, val_MinusLogProbMetric: 102.4825

Epoch 930: val_loss improved from 102.49319 to 102.48251, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 102.1983 - MinusLogProbMetric: 102.1983 - val_loss: 102.4825 - val_MinusLogProbMetric: 102.4825 - lr: 5.0805e-08 - 85s/epoch - 433ms/step
Epoch 931/1000
2023-10-27 01:16:36.663 
Epoch 931/1000 
	 loss: 102.1949, MinusLogProbMetric: 102.1949, val_loss: 102.5046, val_MinusLogProbMetric: 102.5046

Epoch 931: val_loss did not improve from 102.48251
196/196 - 82s - loss: 102.1949 - MinusLogProbMetric: 102.1949 - val_loss: 102.5046 - val_MinusLogProbMetric: 102.5046 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 932/1000
2023-10-27 01:17:59.742 
Epoch 932/1000 
	 loss: 102.2580, MinusLogProbMetric: 102.2580, val_loss: 102.6807, val_MinusLogProbMetric: 102.6807

Epoch 932: val_loss did not improve from 102.48251
196/196 - 83s - loss: 102.2580 - MinusLogProbMetric: 102.2580 - val_loss: 102.6807 - val_MinusLogProbMetric: 102.6807 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 933/1000
2023-10-27 01:19:22.436 
Epoch 933/1000 
	 loss: 102.3600, MinusLogProbMetric: 102.3600, val_loss: 102.6116, val_MinusLogProbMetric: 102.6116

Epoch 933: val_loss did not improve from 102.48251
196/196 - 83s - loss: 102.3600 - MinusLogProbMetric: 102.3600 - val_loss: 102.6116 - val_MinusLogProbMetric: 102.6116 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 934/1000
2023-10-27 01:20:45.412 
Epoch 934/1000 
	 loss: 102.3331, MinusLogProbMetric: 102.3331, val_loss: 102.5902, val_MinusLogProbMetric: 102.5902

Epoch 934: val_loss did not improve from 102.48251
196/196 - 83s - loss: 102.3331 - MinusLogProbMetric: 102.3331 - val_loss: 102.5902 - val_MinusLogProbMetric: 102.5902 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 935/1000
2023-10-27 01:22:08.384 
Epoch 935/1000 
	 loss: 102.3155, MinusLogProbMetric: 102.3155, val_loss: 102.5674, val_MinusLogProbMetric: 102.5674

Epoch 935: val_loss did not improve from 102.48251
196/196 - 83s - loss: 102.3155 - MinusLogProbMetric: 102.3155 - val_loss: 102.5674 - val_MinusLogProbMetric: 102.5674 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 936/1000
2023-10-27 01:23:31.472 
Epoch 936/1000 
	 loss: 102.3104, MinusLogProbMetric: 102.3104, val_loss: 102.5363, val_MinusLogProbMetric: 102.5363

Epoch 936: val_loss did not improve from 102.48251
196/196 - 83s - loss: 102.3104 - MinusLogProbMetric: 102.3104 - val_loss: 102.5363 - val_MinusLogProbMetric: 102.5363 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 937/1000
2023-10-27 01:24:54.547 
Epoch 937/1000 
	 loss: 102.3083, MinusLogProbMetric: 102.3083, val_loss: 102.5580, val_MinusLogProbMetric: 102.5580

Epoch 937: val_loss did not improve from 102.48251
196/196 - 83s - loss: 102.3083 - MinusLogProbMetric: 102.3083 - val_loss: 102.5580 - val_MinusLogProbMetric: 102.5580 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 938/1000
2023-10-27 01:26:17.746 
Epoch 938/1000 
	 loss: 102.2992, MinusLogProbMetric: 102.2992, val_loss: 102.5424, val_MinusLogProbMetric: 102.5424

Epoch 938: val_loss did not improve from 102.48251
196/196 - 83s - loss: 102.2992 - MinusLogProbMetric: 102.2992 - val_loss: 102.5424 - val_MinusLogProbMetric: 102.5424 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 939/1000
2023-10-27 01:27:40.469 
Epoch 939/1000 
	 loss: 102.2951, MinusLogProbMetric: 102.2951, val_loss: 102.5401, val_MinusLogProbMetric: 102.5401

Epoch 939: val_loss did not improve from 102.48251
196/196 - 83s - loss: 102.2951 - MinusLogProbMetric: 102.2951 - val_loss: 102.5401 - val_MinusLogProbMetric: 102.5401 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 940/1000
2023-10-27 01:29:02.798 
Epoch 940/1000 
	 loss: 102.2904, MinusLogProbMetric: 102.2904, val_loss: 102.5342, val_MinusLogProbMetric: 102.5342

Epoch 940: val_loss did not improve from 102.48251
196/196 - 82s - loss: 102.2904 - MinusLogProbMetric: 102.2904 - val_loss: 102.5342 - val_MinusLogProbMetric: 102.5342 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 941/1000
2023-10-27 01:30:25.754 
Epoch 941/1000 
	 loss: 102.2795, MinusLogProbMetric: 102.2795, val_loss: 102.5323, val_MinusLogProbMetric: 102.5323

Epoch 941: val_loss did not improve from 102.48251
196/196 - 83s - loss: 102.2795 - MinusLogProbMetric: 102.2795 - val_loss: 102.5323 - val_MinusLogProbMetric: 102.5323 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 942/1000
2023-10-27 01:31:48.612 
Epoch 942/1000 
	 loss: 102.2732, MinusLogProbMetric: 102.2732, val_loss: 102.5278, val_MinusLogProbMetric: 102.5278

Epoch 942: val_loss did not improve from 102.48251
196/196 - 83s - loss: 102.2732 - MinusLogProbMetric: 102.2732 - val_loss: 102.5278 - val_MinusLogProbMetric: 102.5278 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 943/1000
2023-10-27 01:33:11.176 
Epoch 943/1000 
	 loss: 102.2791, MinusLogProbMetric: 102.2791, val_loss: 102.5255, val_MinusLogProbMetric: 102.5255

Epoch 943: val_loss did not improve from 102.48251
196/196 - 83s - loss: 102.2791 - MinusLogProbMetric: 102.2791 - val_loss: 102.5255 - val_MinusLogProbMetric: 102.5255 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 944/1000
2023-10-27 01:34:33.121 
Epoch 944/1000 
	 loss: 102.2597, MinusLogProbMetric: 102.2597, val_loss: 102.5171, val_MinusLogProbMetric: 102.5171

Epoch 944: val_loss did not improve from 102.48251
196/196 - 82s - loss: 102.2597 - MinusLogProbMetric: 102.2597 - val_loss: 102.5171 - val_MinusLogProbMetric: 102.5171 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 945/1000
2023-10-27 01:35:56.707 
Epoch 945/1000 
	 loss: 102.2609, MinusLogProbMetric: 102.2609, val_loss: 102.5302, val_MinusLogProbMetric: 102.5302

Epoch 945: val_loss did not improve from 102.48251
196/196 - 84s - loss: 102.2609 - MinusLogProbMetric: 102.2609 - val_loss: 102.5302 - val_MinusLogProbMetric: 102.5302 - lr: 5.0805e-08 - 84s/epoch - 426ms/step
Epoch 946/1000
2023-10-27 01:37:19.124 
Epoch 946/1000 
	 loss: 102.2589, MinusLogProbMetric: 102.2589, val_loss: 102.5375, val_MinusLogProbMetric: 102.5375

Epoch 946: val_loss did not improve from 102.48251
196/196 - 82s - loss: 102.2589 - MinusLogProbMetric: 102.2589 - val_loss: 102.5375 - val_MinusLogProbMetric: 102.5375 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 947/1000
2023-10-27 01:38:41.953 
Epoch 947/1000 
	 loss: 102.2667, MinusLogProbMetric: 102.2667, val_loss: 102.5195, val_MinusLogProbMetric: 102.5195

Epoch 947: val_loss did not improve from 102.48251
196/196 - 83s - loss: 102.2667 - MinusLogProbMetric: 102.2667 - val_loss: 102.5195 - val_MinusLogProbMetric: 102.5195 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 948/1000
2023-10-27 01:40:04.719 
Epoch 948/1000 
	 loss: 102.2670, MinusLogProbMetric: 102.2670, val_loss: 102.4997, val_MinusLogProbMetric: 102.4997

Epoch 948: val_loss did not improve from 102.48251
196/196 - 83s - loss: 102.2670 - MinusLogProbMetric: 102.2670 - val_loss: 102.4997 - val_MinusLogProbMetric: 102.4997 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 949/1000
2023-10-27 01:41:27.186 
Epoch 949/1000 
	 loss: 102.2543, MinusLogProbMetric: 102.2543, val_loss: 102.5155, val_MinusLogProbMetric: 102.5155

Epoch 949: val_loss did not improve from 102.48251
196/196 - 82s - loss: 102.2543 - MinusLogProbMetric: 102.2543 - val_loss: 102.5155 - val_MinusLogProbMetric: 102.5155 - lr: 5.0805e-08 - 82s/epoch - 421ms/step
Epoch 950/1000
2023-10-27 01:42:50.406 
Epoch 950/1000 
	 loss: 102.2561, MinusLogProbMetric: 102.2561, val_loss: 102.5124, val_MinusLogProbMetric: 102.5124

Epoch 950: val_loss did not improve from 102.48251
196/196 - 83s - loss: 102.2561 - MinusLogProbMetric: 102.2561 - val_loss: 102.5124 - val_MinusLogProbMetric: 102.5124 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 951/1000
2023-10-27 01:44:12.815 
Epoch 951/1000 
	 loss: 102.2536, MinusLogProbMetric: 102.2536, val_loss: 102.4930, val_MinusLogProbMetric: 102.4930

Epoch 951: val_loss did not improve from 102.48251
196/196 - 82s - loss: 102.2536 - MinusLogProbMetric: 102.2536 - val_loss: 102.4930 - val_MinusLogProbMetric: 102.4930 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 952/1000
2023-10-27 01:45:34.519 
Epoch 952/1000 
	 loss: 102.2522, MinusLogProbMetric: 102.2522, val_loss: 102.4983, val_MinusLogProbMetric: 102.4983

Epoch 952: val_loss did not improve from 102.48251
196/196 - 82s - loss: 102.2522 - MinusLogProbMetric: 102.2522 - val_loss: 102.4983 - val_MinusLogProbMetric: 102.4983 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 953/1000
2023-10-27 01:46:55.522 
Epoch 953/1000 
	 loss: 102.2428, MinusLogProbMetric: 102.2428, val_loss: 102.5084, val_MinusLogProbMetric: 102.5084

Epoch 953: val_loss did not improve from 102.48251
196/196 - 81s - loss: 102.2428 - MinusLogProbMetric: 102.2428 - val_loss: 102.5084 - val_MinusLogProbMetric: 102.5084 - lr: 5.0805e-08 - 81s/epoch - 413ms/step
Epoch 954/1000
2023-10-27 01:48:14.501 
Epoch 954/1000 
	 loss: 102.2264, MinusLogProbMetric: 102.2264, val_loss: 102.5013, val_MinusLogProbMetric: 102.5013

Epoch 954: val_loss did not improve from 102.48251
196/196 - 79s - loss: 102.2264 - MinusLogProbMetric: 102.2264 - val_loss: 102.5013 - val_MinusLogProbMetric: 102.5013 - lr: 5.0805e-08 - 79s/epoch - 403ms/step
Epoch 955/1000
2023-10-27 01:49:36.542 
Epoch 955/1000 
	 loss: 102.2290, MinusLogProbMetric: 102.2290, val_loss: 102.5031, val_MinusLogProbMetric: 102.5031

Epoch 955: val_loss did not improve from 102.48251
196/196 - 82s - loss: 102.2290 - MinusLogProbMetric: 102.2290 - val_loss: 102.5031 - val_MinusLogProbMetric: 102.5031 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 956/1000
2023-10-27 01:50:59.158 
Epoch 956/1000 
	 loss: 102.2253, MinusLogProbMetric: 102.2253, val_loss: 102.4768, val_MinusLogProbMetric: 102.4768

Epoch 956: val_loss improved from 102.48251 to 102.47681, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 102.2253 - MinusLogProbMetric: 102.2253 - val_loss: 102.4768 - val_MinusLogProbMetric: 102.4768 - lr: 5.0805e-08 - 85s/epoch - 432ms/step
Epoch 957/1000
2023-10-27 01:52:23.680 
Epoch 957/1000 
	 loss: 102.2199, MinusLogProbMetric: 102.2199, val_loss: 102.4806, val_MinusLogProbMetric: 102.4806

Epoch 957: val_loss did not improve from 102.47681
196/196 - 83s - loss: 102.2199 - MinusLogProbMetric: 102.2199 - val_loss: 102.4806 - val_MinusLogProbMetric: 102.4806 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 958/1000
2023-10-27 01:53:46.078 
Epoch 958/1000 
	 loss: 102.2168, MinusLogProbMetric: 102.2168, val_loss: 102.4650, val_MinusLogProbMetric: 102.4650

Epoch 958: val_loss improved from 102.47681 to 102.46502, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.2168 - MinusLogProbMetric: 102.2168 - val_loss: 102.4650 - val_MinusLogProbMetric: 102.4650 - lr: 5.0805e-08 - 84s/epoch - 430ms/step
Epoch 959/1000
2023-10-27 01:55:10.664 
Epoch 959/1000 
	 loss: 102.2200, MinusLogProbMetric: 102.2200, val_loss: 102.4823, val_MinusLogProbMetric: 102.4823

Epoch 959: val_loss did not improve from 102.46502
196/196 - 83s - loss: 102.2200 - MinusLogProbMetric: 102.2200 - val_loss: 102.4823 - val_MinusLogProbMetric: 102.4823 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 960/1000
2023-10-27 01:56:33.633 
Epoch 960/1000 
	 loss: 102.2088, MinusLogProbMetric: 102.2088, val_loss: 102.4905, val_MinusLogProbMetric: 102.4905

Epoch 960: val_loss did not improve from 102.46502
196/196 - 83s - loss: 102.2088 - MinusLogProbMetric: 102.2088 - val_loss: 102.4905 - val_MinusLogProbMetric: 102.4905 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 961/1000
2023-10-27 01:57:55.473 
Epoch 961/1000 
	 loss: 102.2122, MinusLogProbMetric: 102.2122, val_loss: 102.4819, val_MinusLogProbMetric: 102.4819

Epoch 961: val_loss did not improve from 102.46502
196/196 - 82s - loss: 102.2122 - MinusLogProbMetric: 102.2122 - val_loss: 102.4819 - val_MinusLogProbMetric: 102.4819 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 962/1000
2023-10-27 01:59:17.567 
Epoch 962/1000 
	 loss: 102.2099, MinusLogProbMetric: 102.2099, val_loss: 102.4754, val_MinusLogProbMetric: 102.4754

Epoch 962: val_loss did not improve from 102.46502
196/196 - 82s - loss: 102.2099 - MinusLogProbMetric: 102.2099 - val_loss: 102.4754 - val_MinusLogProbMetric: 102.4754 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 963/1000
2023-10-27 02:00:39.839 
Epoch 963/1000 
	 loss: 102.2067, MinusLogProbMetric: 102.2067, val_loss: 102.4634, val_MinusLogProbMetric: 102.4634

Epoch 963: val_loss improved from 102.46502 to 102.46339, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.2067 - MinusLogProbMetric: 102.2067 - val_loss: 102.4634 - val_MinusLogProbMetric: 102.4634 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 964/1000
2023-10-27 02:02:04.967 
Epoch 964/1000 
	 loss: 102.2001, MinusLogProbMetric: 102.2001, val_loss: 102.4720, val_MinusLogProbMetric: 102.4720

Epoch 964: val_loss did not improve from 102.46339
196/196 - 83s - loss: 102.2001 - MinusLogProbMetric: 102.2001 - val_loss: 102.4720 - val_MinusLogProbMetric: 102.4720 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 965/1000
2023-10-27 02:03:25.283 
Epoch 965/1000 
	 loss: 102.2025, MinusLogProbMetric: 102.2025, val_loss: 102.4656, val_MinusLogProbMetric: 102.4656

Epoch 965: val_loss did not improve from 102.46339
196/196 - 80s - loss: 102.2025 - MinusLogProbMetric: 102.2025 - val_loss: 102.4656 - val_MinusLogProbMetric: 102.4656 - lr: 5.0805e-08 - 80s/epoch - 410ms/step
Epoch 966/1000
2023-10-27 02:04:35.545 
Epoch 966/1000 
	 loss: 102.2014, MinusLogProbMetric: 102.2014, val_loss: 102.4657, val_MinusLogProbMetric: 102.4657

Epoch 966: val_loss did not improve from 102.46339
196/196 - 70s - loss: 102.2014 - MinusLogProbMetric: 102.2014 - val_loss: 102.4657 - val_MinusLogProbMetric: 102.4657 - lr: 5.0805e-08 - 70s/epoch - 358ms/step
Epoch 967/1000
2023-10-27 02:05:57.636 
Epoch 967/1000 
	 loss: 102.1951, MinusLogProbMetric: 102.1951, val_loss: 102.4606, val_MinusLogProbMetric: 102.4606

Epoch 967: val_loss improved from 102.46339 to 102.46059, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.1951 - MinusLogProbMetric: 102.1951 - val_loss: 102.4606 - val_MinusLogProbMetric: 102.4606 - lr: 5.0805e-08 - 84s/epoch - 426ms/step
Epoch 968/1000
2023-10-27 02:07:20.628 
Epoch 968/1000 
	 loss: 102.1986, MinusLogProbMetric: 102.1986, val_loss: 102.4664, val_MinusLogProbMetric: 102.4664

Epoch 968: val_loss did not improve from 102.46059
196/196 - 81s - loss: 102.1986 - MinusLogProbMetric: 102.1986 - val_loss: 102.4664 - val_MinusLogProbMetric: 102.4664 - lr: 5.0805e-08 - 81s/epoch - 416ms/step
Epoch 969/1000
2023-10-27 02:08:42.374 
Epoch 969/1000 
	 loss: 102.1917, MinusLogProbMetric: 102.1917, val_loss: 102.4668, val_MinusLogProbMetric: 102.4668

Epoch 969: val_loss did not improve from 102.46059
196/196 - 82s - loss: 102.1917 - MinusLogProbMetric: 102.1917 - val_loss: 102.4668 - val_MinusLogProbMetric: 102.4668 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 970/1000
2023-10-27 02:10:04.440 
Epoch 970/1000 
	 loss: 102.1915, MinusLogProbMetric: 102.1915, val_loss: 102.4584, val_MinusLogProbMetric: 102.4584

Epoch 970: val_loss improved from 102.46059 to 102.45840, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.1915 - MinusLogProbMetric: 102.1915 - val_loss: 102.4584 - val_MinusLogProbMetric: 102.4584 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 971/1000
2023-10-27 02:11:27.785 
Epoch 971/1000 
	 loss: 102.1821, MinusLogProbMetric: 102.1821, val_loss: 102.4435, val_MinusLogProbMetric: 102.4435

Epoch 971: val_loss improved from 102.45840 to 102.44354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.1821 - MinusLogProbMetric: 102.1821 - val_loss: 102.4435 - val_MinusLogProbMetric: 102.4435 - lr: 5.0805e-08 - 83s/epoch - 426ms/step
Epoch 972/1000
2023-10-27 02:12:51.532 
Epoch 972/1000 
	 loss: 102.1847, MinusLogProbMetric: 102.1847, val_loss: 102.4429, val_MinusLogProbMetric: 102.4429

Epoch 972: val_loss improved from 102.44354 to 102.44294, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.1847 - MinusLogProbMetric: 102.1847 - val_loss: 102.4429 - val_MinusLogProbMetric: 102.4429 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 973/1000
2023-10-27 02:14:14.705 
Epoch 973/1000 
	 loss: 102.1811, MinusLogProbMetric: 102.1811, val_loss: 102.4385, val_MinusLogProbMetric: 102.4385

Epoch 973: val_loss improved from 102.44294 to 102.43848, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.1811 - MinusLogProbMetric: 102.1811 - val_loss: 102.4385 - val_MinusLogProbMetric: 102.4385 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 974/1000
2023-10-27 02:15:38.653 
Epoch 974/1000 
	 loss: 102.1809, MinusLogProbMetric: 102.1809, val_loss: 102.4392, val_MinusLogProbMetric: 102.4392

Epoch 974: val_loss did not improve from 102.43848
196/196 - 82s - loss: 102.1809 - MinusLogProbMetric: 102.1809 - val_loss: 102.4392 - val_MinusLogProbMetric: 102.4392 - lr: 5.0805e-08 - 82s/epoch - 420ms/step
Epoch 975/1000
2023-10-27 02:17:00.640 
Epoch 975/1000 
	 loss: 102.1849, MinusLogProbMetric: 102.1849, val_loss: 102.4328, val_MinusLogProbMetric: 102.4328

Epoch 975: val_loss improved from 102.43848 to 102.43279, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.1849 - MinusLogProbMetric: 102.1849 - val_loss: 102.4328 - val_MinusLogProbMetric: 102.4328 - lr: 5.0805e-08 - 84s/epoch - 426ms/step
Epoch 976/1000
2023-10-27 02:18:24.098 
Epoch 976/1000 
	 loss: 102.1744, MinusLogProbMetric: 102.1744, val_loss: 102.4303, val_MinusLogProbMetric: 102.4303

Epoch 976: val_loss improved from 102.43279 to 102.43028, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.1744 - MinusLogProbMetric: 102.1744 - val_loss: 102.4303 - val_MinusLogProbMetric: 102.4303 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 977/1000
2023-10-27 02:19:45.065 
Epoch 977/1000 
	 loss: 102.1720, MinusLogProbMetric: 102.1720, val_loss: 102.4237, val_MinusLogProbMetric: 102.4237

Epoch 977: val_loss improved from 102.43028 to 102.42371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 81s - loss: 102.1720 - MinusLogProbMetric: 102.1720 - val_loss: 102.4237 - val_MinusLogProbMetric: 102.4237 - lr: 5.0805e-08 - 81s/epoch - 416ms/step
Epoch 978/1000
2023-10-27 02:21:09.454 
Epoch 978/1000 
	 loss: 102.1768, MinusLogProbMetric: 102.1768, val_loss: 102.4265, val_MinusLogProbMetric: 102.4265

Epoch 978: val_loss did not improve from 102.42371
196/196 - 83s - loss: 102.1768 - MinusLogProbMetric: 102.1768 - val_loss: 102.4265 - val_MinusLogProbMetric: 102.4265 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 979/1000
2023-10-27 02:22:32.049 
Epoch 979/1000 
	 loss: 102.1704, MinusLogProbMetric: 102.1704, val_loss: 102.4183, val_MinusLogProbMetric: 102.4183

Epoch 979: val_loss improved from 102.42371 to 102.41828, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 85s - loss: 102.1704 - MinusLogProbMetric: 102.1704 - val_loss: 102.4183 - val_MinusLogProbMetric: 102.4183 - lr: 5.0805e-08 - 85s/epoch - 431ms/step
Epoch 980/1000
2023-10-27 02:23:55.942 
Epoch 980/1000 
	 loss: 102.1692, MinusLogProbMetric: 102.1692, val_loss: 102.4295, val_MinusLogProbMetric: 102.4295

Epoch 980: val_loss did not improve from 102.41828
196/196 - 82s - loss: 102.1692 - MinusLogProbMetric: 102.1692 - val_loss: 102.4295 - val_MinusLogProbMetric: 102.4295 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 981/1000
2023-10-27 02:25:18.454 
Epoch 981/1000 
	 loss: 102.1640, MinusLogProbMetric: 102.1640, val_loss: 102.4254, val_MinusLogProbMetric: 102.4254

Epoch 981: val_loss did not improve from 102.41828
196/196 - 83s - loss: 102.1640 - MinusLogProbMetric: 102.1640 - val_loss: 102.4254 - val_MinusLogProbMetric: 102.4254 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 982/1000
2023-10-27 02:26:40.466 
Epoch 982/1000 
	 loss: 102.1505, MinusLogProbMetric: 102.1505, val_loss: 102.4129, val_MinusLogProbMetric: 102.4129

Epoch 982: val_loss improved from 102.41828 to 102.41287, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.1505 - MinusLogProbMetric: 102.1505 - val_loss: 102.4129 - val_MinusLogProbMetric: 102.4129 - lr: 5.0805e-08 - 84s/epoch - 427ms/step
Epoch 983/1000
2023-10-27 02:28:04.400 
Epoch 983/1000 
	 loss: 102.1453, MinusLogProbMetric: 102.1453, val_loss: 102.4119, val_MinusLogProbMetric: 102.4119

Epoch 983: val_loss improved from 102.41287 to 102.41194, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.1453 - MinusLogProbMetric: 102.1453 - val_loss: 102.4119 - val_MinusLogProbMetric: 102.4119 - lr: 5.0805e-08 - 84s/epoch - 429ms/step
Epoch 984/1000
2023-10-27 02:29:27.980 
Epoch 984/1000 
	 loss: 102.1425, MinusLogProbMetric: 102.1425, val_loss: 102.4298, val_MinusLogProbMetric: 102.4298

Epoch 984: val_loss did not improve from 102.41194
196/196 - 82s - loss: 102.1425 - MinusLogProbMetric: 102.1425 - val_loss: 102.4298 - val_MinusLogProbMetric: 102.4298 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 985/1000
2023-10-27 02:30:50.579 
Epoch 985/1000 
	 loss: 102.1469, MinusLogProbMetric: 102.1469, val_loss: 102.4327, val_MinusLogProbMetric: 102.4327

Epoch 985: val_loss did not improve from 102.41194
196/196 - 83s - loss: 102.1469 - MinusLogProbMetric: 102.1469 - val_loss: 102.4327 - val_MinusLogProbMetric: 102.4327 - lr: 5.0805e-08 - 83s/epoch - 421ms/step
Epoch 986/1000
2023-10-27 02:32:12.765 
Epoch 986/1000 
	 loss: 102.1431, MinusLogProbMetric: 102.1431, val_loss: 102.4223, val_MinusLogProbMetric: 102.4223

Epoch 986: val_loss did not improve from 102.41194
196/196 - 82s - loss: 102.1431 - MinusLogProbMetric: 102.1431 - val_loss: 102.4223 - val_MinusLogProbMetric: 102.4223 - lr: 5.0805e-08 - 82s/epoch - 419ms/step
Epoch 987/1000
2023-10-27 02:33:33.743 
Epoch 987/1000 
	 loss: 102.1432, MinusLogProbMetric: 102.1432, val_loss: 102.4247, val_MinusLogProbMetric: 102.4247

Epoch 987: val_loss did not improve from 102.41194
196/196 - 81s - loss: 102.1432 - MinusLogProbMetric: 102.1432 - val_loss: 102.4247 - val_MinusLogProbMetric: 102.4247 - lr: 5.0805e-08 - 81s/epoch - 413ms/step
Epoch 988/1000
2023-10-27 02:34:55.749 
Epoch 988/1000 
	 loss: 102.1474, MinusLogProbMetric: 102.1474, val_loss: 102.3982, val_MinusLogProbMetric: 102.3982

Epoch 988: val_loss improved from 102.41194 to 102.39822, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 84s - loss: 102.1474 - MinusLogProbMetric: 102.1474 - val_loss: 102.3982 - val_MinusLogProbMetric: 102.3982 - lr: 5.0805e-08 - 84s/epoch - 428ms/step
Epoch 989/1000
2023-10-27 02:36:19.421 
Epoch 989/1000 
	 loss: 102.1369, MinusLogProbMetric: 102.1369, val_loss: 102.4004, val_MinusLogProbMetric: 102.4004

Epoch 989: val_loss did not improve from 102.39822
196/196 - 82s - loss: 102.1369 - MinusLogProbMetric: 102.1369 - val_loss: 102.4004 - val_MinusLogProbMetric: 102.4004 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 990/1000
2023-10-27 02:37:41.248 
Epoch 990/1000 
	 loss: 102.1351, MinusLogProbMetric: 102.1351, val_loss: 102.4043, val_MinusLogProbMetric: 102.4043

Epoch 990: val_loss did not improve from 102.39822
196/196 - 82s - loss: 102.1351 - MinusLogProbMetric: 102.1351 - val_loss: 102.4043 - val_MinusLogProbMetric: 102.4043 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 991/1000
2023-10-27 02:39:03.163 
Epoch 991/1000 
	 loss: 102.1334, MinusLogProbMetric: 102.1334, val_loss: 102.4047, val_MinusLogProbMetric: 102.4047

Epoch 991: val_loss did not improve from 102.39822
196/196 - 82s - loss: 102.1334 - MinusLogProbMetric: 102.1334 - val_loss: 102.4047 - val_MinusLogProbMetric: 102.4047 - lr: 5.0805e-08 - 82s/epoch - 418ms/step
Epoch 992/1000
2023-10-27 02:40:25.654 
Epoch 992/1000 
	 loss: 102.1322, MinusLogProbMetric: 102.1322, val_loss: 102.4047, val_MinusLogProbMetric: 102.4047

Epoch 992: val_loss did not improve from 102.39822
196/196 - 82s - loss: 102.1322 - MinusLogProbMetric: 102.1322 - val_loss: 102.4047 - val_MinusLogProbMetric: 102.4047 - lr: 5.0805e-08 - 82s/epoch - 421ms/step
Epoch 993/1000
2023-10-27 02:41:47.112 
Epoch 993/1000 
	 loss: 102.1393, MinusLogProbMetric: 102.1393, val_loss: 102.3909, val_MinusLogProbMetric: 102.3909

Epoch 993: val_loss improved from 102.39822 to 102.39088, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.1393 - MinusLogProbMetric: 102.1393 - val_loss: 102.3909 - val_MinusLogProbMetric: 102.3909 - lr: 5.0805e-08 - 83s/epoch - 423ms/step
Epoch 994/1000
2023-10-27 02:43:10.435 
Epoch 994/1000 
	 loss: 102.1364, MinusLogProbMetric: 102.1364, val_loss: 102.3945, val_MinusLogProbMetric: 102.3945

Epoch 994: val_loss did not improve from 102.39088
196/196 - 82s - loss: 102.1364 - MinusLogProbMetric: 102.1364 - val_loss: 102.3945 - val_MinusLogProbMetric: 102.3945 - lr: 5.0805e-08 - 82s/epoch - 417ms/step
Epoch 995/1000
2023-10-27 02:44:31.935 
Epoch 995/1000 
	 loss: 102.1373, MinusLogProbMetric: 102.1373, val_loss: 102.3909, val_MinusLogProbMetric: 102.3909

Epoch 995: val_loss did not improve from 102.39088
196/196 - 81s - loss: 102.1373 - MinusLogProbMetric: 102.1373 - val_loss: 102.3909 - val_MinusLogProbMetric: 102.3909 - lr: 5.0805e-08 - 81s/epoch - 416ms/step
Epoch 996/1000
2023-10-27 02:45:53.224 
Epoch 996/1000 
	 loss: 102.1283, MinusLogProbMetric: 102.1283, val_loss: 102.3882, val_MinusLogProbMetric: 102.3882

Epoch 996: val_loss improved from 102.39088 to 102.38816, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.1283 - MinusLogProbMetric: 102.1283 - val_loss: 102.3882 - val_MinusLogProbMetric: 102.3882 - lr: 5.0805e-08 - 83s/epoch - 425ms/step
Epoch 997/1000
2023-10-27 02:47:16.331 
Epoch 997/1000 
	 loss: 102.1244, MinusLogProbMetric: 102.1244, val_loss: 102.3819, val_MinusLogProbMetric: 102.3819

Epoch 997: val_loss improved from 102.38816 to 102.38193, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.1244 - MinusLogProbMetric: 102.1244 - val_loss: 102.3819 - val_MinusLogProbMetric: 102.3819 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 998/1000
2023-10-27 02:48:39.375 
Epoch 998/1000 
	 loss: 102.1256, MinusLogProbMetric: 102.1256, val_loss: 102.3798, val_MinusLogProbMetric: 102.3798

Epoch 998: val_loss improved from 102.38193 to 102.37981, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.1256 - MinusLogProbMetric: 102.1256 - val_loss: 102.3798 - val_MinusLogProbMetric: 102.3798 - lr: 5.0805e-08 - 83s/epoch - 422ms/step
Epoch 999/1000
2023-10-27 02:50:02.609 
Epoch 999/1000 
	 loss: 102.1262, MinusLogProbMetric: 102.1262, val_loss: 102.3782, val_MinusLogProbMetric: 102.3782

Epoch 999: val_loss improved from 102.37981 to 102.37821, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 83s - loss: 102.1262 - MinusLogProbMetric: 102.1262 - val_loss: 102.3782 - val_MinusLogProbMetric: 102.3782 - lr: 5.0805e-08 - 83s/epoch - 424ms/step
Epoch 1000/1000
2023-10-27 02:51:18.412 
Epoch 1000/1000 
	 loss: 102.1234, MinusLogProbMetric: 102.1234, val_loss: 102.3737, val_MinusLogProbMetric: 102.3737

Epoch 1000: val_loss improved from 102.37821 to 102.37373, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_368/weights/best_weights.h5
196/196 - 76s - loss: 102.1234 - MinusLogProbMetric: 102.1234 - val_loss: 102.3737 - val_MinusLogProbMetric: 102.3737 - lr: 5.0805e-08 - 76s/epoch - 386ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 541.
Model trained in 80606.30 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.68 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 2.08 s.
===========
Run 368/720 done in 114401.33 s.
===========

Directory ../../results/CsplineN_new/run_369/ already exists.
Skipping it.
===========
Run 369/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_370/ already exists.
Skipping it.
===========
Run 370/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_371/ already exists.
Skipping it.
===========
Run 371/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_372/ already exists.
Skipping it.
===========
Run 372/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_373/ already exists.
Skipping it.
===========
Run 373/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_374/ already exists.
Skipping it.
===========
Run 374/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_375/ already exists.
Skipping it.
===========
Run 375/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_376/ already exists.
Skipping it.
===========
Run 376/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_377/ already exists.
Skipping it.
===========
Run 377/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_378/ already exists.
Skipping it.
===========
Run 378/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_379/ already exists.
Skipping it.
===========
Run 379/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_380/ already exists.
Skipping it.
===========
Run 380/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_381/ already exists.
Skipping it.
===========
Run 381/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_382/ already exists.
Skipping it.
===========
Run 382/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_383/ already exists.
Skipping it.
===========
Run 383/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_384/ already exists.
Skipping it.
===========
Run 384/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_385/ already exists.
Skipping it.
===========
Run 385/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_386/ already exists.
Skipping it.
===========
Run 386/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_387/ already exists.
Skipping it.
===========
Run 387/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_388/ already exists.
Skipping it.
===========
Run 388/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_389/ already exists.
Skipping it.
===========
Run 389/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_390/ already exists.
Skipping it.
===========
Run 390/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_391/ already exists.
Skipping it.
===========
Run 391/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_392/ already exists.
Skipping it.
===========
Run 392/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_393/ already exists.
Skipping it.
===========
Run 393/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_394/ already exists.
Skipping it.
===========
Run 394/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_395/ already exists.
Skipping it.
===========
Run 395/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_396/ already exists.
Skipping it.
===========
Run 396/720 already exists. Skipping it.
===========

===========
Generating train data for run 397.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_397
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_497"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_498 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_52 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_52/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_52'")
self.model: <keras.engine.functional.Functional object at 0x7f175752e2c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f18cc49d9f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f18cc49d9f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f17d36669b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f17c660c190>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f17c660ed10>, <keras.callbacks.ModelCheckpoint object at 0x7f17c660d600>, <keras.callbacks.EarlyStopping object at 0x7f17c660e4a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f17c660dc90>, <keras.callbacks.TerminateOnNaN object at 0x7f17c660f850>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_397/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 397/720 with hyperparameters:
timestamp = 2023-10-27 02:51:30.245010
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 02:53:51.897 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7537.2446, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 142s - loss: nan - MinusLogProbMetric: 7537.2446 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 142s/epoch - 722ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 0.0003333333333333333.
===========
Generating train data for run 397.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_397
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_508"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_509 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_53 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_53/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_53'")
self.model: <keras.engine.functional.Functional object at 0x7f13e036ad40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f17d0c02fb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f17d0c02fb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f13d8b68b50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f13da98a770>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f13da98ace0>, <keras.callbacks.ModelCheckpoint object at 0x7f13da98ada0>, <keras.callbacks.EarlyStopping object at 0x7f13da98b010>, <keras.callbacks.ReduceLROnPlateau object at 0x7f13da98b040>, <keras.callbacks.TerminateOnNaN object at 0x7f13da98ac80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_397/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 397/720 with hyperparameters:
timestamp = 2023-10-27 02:54:01.882382
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 02:56:47.840 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7537.2446, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 166s - loss: nan - MinusLogProbMetric: 7537.2446 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 166s/epoch - 845ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 0.0001111111111111111.
===========
Generating train data for run 397.
===========
Train data generated in 0.36 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_397
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_519"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_520 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_54 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_54/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_54'")
self.model: <keras.engine.functional.Functional object at 0x7f15dc4979a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1757dac640>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1757dac640>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1795a1f400>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f14f11b8be0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1ac01469e0>, <keras.callbacks.ModelCheckpoint object at 0x7f1ac01454b0>, <keras.callbacks.EarlyStopping object at 0x7f1ac0145960>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1ac01445e0>, <keras.callbacks.TerminateOnNaN object at 0x7f1ac0144c70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_397/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 397/720 with hyperparameters:
timestamp = 2023-10-27 02:56:58.656399
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 02:59:22.118 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7537.2446, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 143s - loss: nan - MinusLogProbMetric: 7537.2446 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 143s/epoch - 730ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 3.703703703703703e-05.
===========
Generating train data for run 397.
===========
Train data generated in 0.43 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_397
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_530"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_531 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_55 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_55/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_55'")
self.model: <keras.engine.functional.Functional object at 0x7f13e1516380>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1797096f50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1797096f50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1ac0144a30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1634486920>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1634484f40>, <keras.callbacks.ModelCheckpoint object at 0x7f1634485ae0>, <keras.callbacks.EarlyStopping object at 0x7f1634485780>, <keras.callbacks.ReduceLROnPlateau object at 0x7f16344858a0>, <keras.callbacks.TerminateOnNaN object at 0x7f1634485d80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_397/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 397/720 with hyperparameters:
timestamp = 2023-10-27 02:59:32.387294
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 03:02:03.053 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7537.2446, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 150s - loss: nan - MinusLogProbMetric: 7537.2446 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 150s/epoch - 768ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 1.2345679012345677e-05.
===========
Generating train data for run 397.
===========
Train data generated in 0.41 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_397
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_541"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_542 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_56 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_56/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_56'")
self.model: <keras.engine.functional.Functional object at 0x7f1aa0754400>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f18e027bbb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f18e027bbb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1403198760>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f13db6ace20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f13db6ad810>, <keras.callbacks.ModelCheckpoint object at 0x7f13db6ae380>, <keras.callbacks.EarlyStopping object at 0x7f13db6afd90>, <keras.callbacks.ReduceLROnPlateau object at 0x7f13db6af460>, <keras.callbacks.TerminateOnNaN object at 0x7f13db6acb80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_397/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 397/720 with hyperparameters:
timestamp = 2023-10-27 03:02:14.254285
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 03:04:30.296 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7537.2446, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 136s - loss: nan - MinusLogProbMetric: 7537.2446 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 136s/epoch - 693ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 4.115226337448558e-06.
===========
Generating train data for run 397.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_397
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_552"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_553 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_57 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_57/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_57'")
self.model: <keras.engine.functional.Functional object at 0x7f1675fbbf10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f145af1ce80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f145af1ce80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f17d2f63ca0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f13db9c0250>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f13db9c07c0>, <keras.callbacks.ModelCheckpoint object at 0x7f13db9c0880>, <keras.callbacks.EarlyStopping object at 0x7f13db9c0af0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f13db9c0b20>, <keras.callbacks.TerminateOnNaN object at 0x7f13db9c0760>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_397/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 397/720 with hyperparameters:
timestamp = 2023-10-27 03:04:40.336598
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 03:06:48.914 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7537.2446, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 128s - loss: nan - MinusLogProbMetric: 7537.2446 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 128s/epoch - 655ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 1.3717421124828526e-06.
===========
Generating train data for run 397.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_397
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_563"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_564 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_58 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_58/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_58'")
self.model: <keras.engine.functional.Functional object at 0x7f143b3b0850>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f17c46965c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f17c46965c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1e6e0a26e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f18e0246f80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f18e02461d0>, <keras.callbacks.ModelCheckpoint object at 0x7f18e0246e90>, <keras.callbacks.EarlyStopping object at 0x7f18e0246170>, <keras.callbacks.ReduceLROnPlateau object at 0x7f18e0247d00>, <keras.callbacks.TerminateOnNaN object at 0x7f18e0247d60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_397/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 397/720 with hyperparameters:
timestamp = 2023-10-27 03:07:13.237471
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 03:09:27.843 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7537.2446, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 134s - loss: nan - MinusLogProbMetric: 7537.2446 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 134s/epoch - 685ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 4.572473708276175e-07.
===========
Generating train data for run 397.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_397
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_574"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_575 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_59 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_59/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_59'")
self.model: <keras.engine.functional.Functional object at 0x7f17e012f3d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1801661f30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1801661f30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f14f120c5e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1400205f60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f14002064d0>, <keras.callbacks.ModelCheckpoint object at 0x7f1400206590>, <keras.callbacks.EarlyStopping object at 0x7f1400206800>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1400206830>, <keras.callbacks.TerminateOnNaN object at 0x7f1400206470>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_397/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 397/720 with hyperparameters:
timestamp = 2023-10-27 03:09:37.528734
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 03:11:51.693 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7537.2446, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 134s - loss: nan - MinusLogProbMetric: 7537.2446 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 134s/epoch - 683ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 1.524157902758725e-07.
===========
Generating train data for run 397.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_397
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_585"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_586 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_60 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_60/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_60'")
self.model: <keras.engine.functional.Functional object at 0x7f17564b7280>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f13705139d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f13705139d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f13c0d4b1f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1755a41ed0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1755a42440>, <keras.callbacks.ModelCheckpoint object at 0x7f1755a42500>, <keras.callbacks.EarlyStopping object at 0x7f1755a42770>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1755a427a0>, <keras.callbacks.TerminateOnNaN object at 0x7f1755a423e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_397/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 397/720 with hyperparameters:
timestamp = 2023-10-27 03:12:00.084676
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 03:14:26.398 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7537.2446, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 146s - loss: nan - MinusLogProbMetric: 7537.2446 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 146s/epoch - 745ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 5.0805263425290834e-08.
===========
Generating train data for run 397.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_397
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_596"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_597 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_61 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_61/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_61'")
self.model: <keras.engine.functional.Functional object at 0x7f13b2d8f160>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f13f3e4fd00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f13f3e4fd00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f13dbd97880>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f14015219f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1401521f60>, <keras.callbacks.ModelCheckpoint object at 0x7f1401522020>, <keras.callbacks.EarlyStopping object at 0x7f1401522290>, <keras.callbacks.ReduceLROnPlateau object at 0x7f14015222c0>, <keras.callbacks.TerminateOnNaN object at 0x7f1401521f00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_397/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 397/720 with hyperparameters:
timestamp = 2023-10-27 03:14:35.761883
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 03:16:40.785 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7537.2446, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 125s - loss: nan - MinusLogProbMetric: 7537.2446 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 125s/epoch - 638ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 1.6935087808430278e-08.
===========
Generating train data for run 397.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_397/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_397
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_607"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_608 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_62 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_62/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_62'")
self.model: <keras.engine.functional.Functional object at 0x7f16442578e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f13f1f1fe50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f13f1f1fe50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f13b9a68250>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f13b9a48b50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_397/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f13b9a490c0>, <keras.callbacks.ModelCheckpoint object at 0x7f13b9a49180>, <keras.callbacks.EarlyStopping object at 0x7f13b9a493f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f13b9a49420>, <keras.callbacks.TerminateOnNaN object at 0x7f13b9a49060>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_397/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 397/720 with hyperparameters:
timestamp = 2023-10-27 03:16:51.140674
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 03:19:11.291 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7537.2446, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 140s - loss: nan - MinusLogProbMetric: 7537.2446 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 140s/epoch - 714ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 5.645029269476759e-09.
===========
Run 397/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 398.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_398/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_398/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_398/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_398
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_618"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_619 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_63 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_63/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_63'")
self.model: <keras.engine.functional.Functional object at 0x7f1eaac831f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f14784e5ba0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f14784e5ba0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1644993310>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f17d36894e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_398/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f17d3689a50>, <keras.callbacks.ModelCheckpoint object at 0x7f17d3689b10>, <keras.callbacks.EarlyStopping object at 0x7f17d3689d80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f17d3689db0>, <keras.callbacks.TerminateOnNaN object at 0x7f17d36899f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_398/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 398/720 with hyperparameters:
timestamp = 2023-10-27 03:19:21.767510
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 13: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 03:21:37.274 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5408.1953, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 135s - loss: nan - MinusLogProbMetric: 5408.1953 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 135s/epoch - 691ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 0.0003333333333333333.
===========
Generating train data for run 398.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_398/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_398/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_398/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_398
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_629"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_630 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_64 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_64/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_64'")
self.model: <keras.engine.functional.Functional object at 0x7f165462fee0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f17d368a140>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f17d368a140>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f13c94bbd90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f13a0424100>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_398/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f13a0424670>, <keras.callbacks.ModelCheckpoint object at 0x7f13a0424730>, <keras.callbacks.EarlyStopping object at 0x7f13a04249a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f13a04249d0>, <keras.callbacks.TerminateOnNaN object at 0x7f13a0424610>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_398/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 398/720 with hyperparameters:
timestamp = 2023-10-27 03:21:47.267106
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
2023-10-27 03:25:13.936 
Epoch 1/1000 
	 loss: 1905.6168, MinusLogProbMetric: 1905.6168, val_loss: 940.2242, val_MinusLogProbMetric: 940.2242

Epoch 1: val_loss improved from inf to 940.22418, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_398/weights/best_weights.h5
196/196 - 207s - loss: 1905.6168 - MinusLogProbMetric: 1905.6168 - val_loss: 940.2242 - val_MinusLogProbMetric: 940.2242 - lr: 3.3333e-04 - 207s/epoch - 1s/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 122: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 03:26:00.369 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 990.3716, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 940.22418
196/196 - 45s - loss: nan - MinusLogProbMetric: 990.3716 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 45s/epoch - 231ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 0.0001111111111111111.
===========
Generating train data for run 398.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_398/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_398/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_398/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_398
self.data_kwargs: {'seed': 933}
self.x_data: [[5.283829   5.4811573  5.1471066  ... 2.068061   5.718564   1.3303564 ]
 [5.43722    7.8859186  6.61763    ... 0.8890271  7.930606   1.5775324 ]
 [6.9704323  2.6496117  6.1891685  ... 3.1566796  5.0643883  2.1436493 ]
 ...
 [6.465608   2.802359   6.2687764  ... 2.9286766  4.6154056  2.872694  ]
 [2.1362271  4.161193   8.446204   ... 5.4958386  0.87911683 4.614837  ]
 [6.587626   2.9718995  6.1274     ... 2.7110476  1.5571065  2.6203594 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_640"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_641 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_65 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_65/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_65'")
self.model: <keras.engine.functional.Functional object at 0x7f161decff10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f143a357d00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f143a357d00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f15dc127820>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f161de67460>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_398/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f161de679d0>, <keras.callbacks.ModelCheckpoint object at 0x7f161de67a90>, <keras.callbacks.EarlyStopping object at 0x7f161de67d00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f161de67d30>, <keras.callbacks.TerminateOnNaN object at 0x7f161de67970>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 5.3406906 ,  6.7156234 ,  6.143737  , ...,  1.1317147 ,
         6.7873054 ,  1.5391355 ],
       [ 1.3717952 ,  4.67478   ,  8.32236   , ...,  6.8291035 ,
        -1.8190882 ,  2.7221673 ],
       [ 2.1061342 ,  3.5083172 ,  6.2700667 , ...,  6.0540094 ,
        -0.78157866,  3.4469461 ],
       ...,
       [ 6.564865  ,  2.8767831 ,  6.206916  , ...,  3.6518402 ,
         3.1125357 ,  2.1379552 ],
       [ 5.691388  ,  8.012384  ,  5.3085465 , ...,  0.9096228 ,
         7.301206  ,  1.2701175 ],
       [ 6.87658   ,  2.975816  ,  6.1492    , ...,  3.4653115 ,
         2.9466043 ,  1.6093981 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 398/720 with hyperparameters:
timestamp = 2023-10-27 03:26:11.904707
ndims = 64
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.283829    5.4811573   5.1471066   4.359749    4.0882564   7.106916
  4.279815    8.769963    9.320983    3.7448802   7.5869823   5.289087
  5.7163315   9.246931    0.52656376  0.49675518  1.1537591   6.845315
  8.344608    8.775934    9.652934    7.963877    4.634178    7.3299294
  0.5719544   5.667658    0.7845129   9.099658    5.2270074   3.445206
  2.8781133   6.8099585   4.435907    7.0755777  -0.41389915  6.1743
  6.9032993   5.880037    9.551433    6.8428054   3.2568927   4.360301
  7.255477    0.6672695   6.9963293   7.1711373   2.1693141   1.8113687
  3.1912436   3.6322427   4.5161633   4.419193    9.739075    1.0817299
  1.689117    1.3588885   6.4803176   3.3257213   4.550867    2.2172434
  2.6351552   2.068061    5.718564    1.3303564 ]
Epoch 1/1000
LLVM ERROR: pthread_create failed: Resource temporarily unavailable
