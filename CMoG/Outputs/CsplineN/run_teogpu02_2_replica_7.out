2023-10-30 13:59:40.397631: Importing os...
2023-10-30 13:59:40.397721: Importing sys...
2023-10-30 13:59:40.397741: Importing and initializing argparse...
Visible devices: [2]
2023-10-30 13:59:40.419625: Importing timer from timeit...
2023-10-30 13:59:40.420323: Setting env variables for tf import (only device [2] will be available)...
2023-10-30 13:59:40.420377: Importing numpy...
2023-10-30 13:59:40.598341: Importing pandas...
2023-10-30 13:59:40.780757: Importing shutil...
2023-10-30 13:59:40.780782: Importing subprocess...
2023-10-30 13:59:40.780788: Importing tensorflow...
Tensorflow version: 2.12.0
2023-10-30 13:59:42.791268: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-10-30 13:59:43.124545: Importing textwrap...
2023-10-30 13:59:43.124570: Importing timeit...
2023-10-30 13:59:43.124579: Importing traceback...
2023-10-30 13:59:43.124585: Importing typing...
2023-10-30 13:59:43.124593: Setting tf configs...
2023-10-30 13:59:43.379103: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-10-30 13:59:44.411576: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_319/ already exists.
Skipping it.
===========
Run 319/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_321/ already exists.
Skipping it.
===========
Run 321/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_322/ already exists.
Skipping it.
===========
Run 322/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_323/ already exists.
Skipping it.
===========
Run 323/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_324/ already exists.
Skipping it.
===========
Run 324/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_325/ already exists.
Skipping it.
===========
Run 325/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_326/ already exists.
Skipping it.
===========
Run 326/720 already exists. Skipping it.
===========

===========
Generating train data for run 327.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  1817280   
 r)                                                              
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7f39e436fd90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f39e456ce20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f39e456ce20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3a10421810>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f394c43ac50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f394c43b1c0>, <keras.callbacks.ModelCheckpoint object at 0x7f394c43b310>, <keras.callbacks.EarlyStopping object at 0x7f394c43b520>, <keras.callbacks.ReduceLROnPlateau object at 0x7f394c43b550>, <keras.callbacks.TerminateOnNaN object at 0x7f394c43b280>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-30 13:59:51.852955
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:02:15.337 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 143s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 143s/epoch - 731ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0003333333333333333.
===========
Generating train data for run 327.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7f3da942fd00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3da8c94160>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3da8c94160>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3da9f17d90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3da8cf5a20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3da8cf5f90>, <keras.callbacks.ModelCheckpoint object at 0x7f3da8cf6050>, <keras.callbacks.EarlyStopping object at 0x7f3da8cf62c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3da8cf62f0>, <keras.callbacks.TerminateOnNaN object at 0x7f3da8cf5f30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-30 14:02:24.110528
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:04:51.206 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 147s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 147s/epoch - 750ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0001111111111111111.
===========
Generating train data for run 327.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7f38247c6ad0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f37b02cef20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f37b02cef20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f35e43e2e00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3824765210>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3824765780>, <keras.callbacks.ModelCheckpoint object at 0x7f3824765840>, <keras.callbacks.EarlyStopping object at 0x7f3824765ab0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3824765ae0>, <keras.callbacks.TerminateOnNaN object at 0x7f3824765720>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-30 14:04:58.213204
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:07:33.946 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 156s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 156s/epoch - 794ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 3.703703703703703e-05.
===========
Generating train data for run 327.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_43"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_44 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7f371c3d2b30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36d0d19ab0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36d0d19ab0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f36d1b298d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36d1b98ca0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36d1b99210>, <keras.callbacks.ModelCheckpoint object at 0x7f36d1b992d0>, <keras.callbacks.EarlyStopping object at 0x7f36d1b99540>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36d1b99570>, <keras.callbacks.TerminateOnNaN object at 0x7f36d1b991b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-30 14:07:42.075295
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:10:10.597 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 148s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 148s/epoch - 757ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.2345679012345677e-05.
===========
Generating train data for run 327.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_54"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_55 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7f38242da5f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36bc573fa0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36bc573fa0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f36bc529990>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36ce1041c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36ce104730>, <keras.callbacks.ModelCheckpoint object at 0x7f36ce1047f0>, <keras.callbacks.EarlyStopping object at 0x7f36ce104a60>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36ce104a90>, <keras.callbacks.TerminateOnNaN object at 0x7f36ce1046d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-30 14:10:19.418286
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x7f36d0f1b250> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:12:45.662 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 146s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 146s/epoch - 745ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.115226337448558e-06.
===========
Generating train data for run 327.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_65"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_66 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7f36d128bb20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f38704b6800>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f38704b6800>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f392c30f640>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36d1292740>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36d1292cb0>, <keras.callbacks.ModelCheckpoint object at 0x7f36d1292d70>, <keras.callbacks.EarlyStopping object at 0x7f36d1292fe0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36d1293010>, <keras.callbacks.TerminateOnNaN object at 0x7f36d1292c50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-30 14:12:53.231189
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_train_function.<locals>.train_function at 0x7f36cda22320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:15:19.423 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 146s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 146s/epoch - 744ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.3717421124828526e-06.
===========
Generating train data for run 327.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_76"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_77 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7f38241b2fe0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f38ac07fb50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f38ac07fb50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f38241ad060>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f382414b1c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f382414b730>, <keras.callbacks.ModelCheckpoint object at 0x7f382414b7f0>, <keras.callbacks.EarlyStopping object at 0x7f382414ba60>, <keras.callbacks.ReduceLROnPlateau object at 0x7f382414ba90>, <keras.callbacks.TerminateOnNaN object at 0x7f382414b6d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-30 14:15:27.981409
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:17:50.198 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 142s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 142s/epoch - 725ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.572473708276175e-07.
===========
Generating train data for run 327.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_87"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_88 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7f36c6407250>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36c6407700>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36c6407700>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f36cfa1bca0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36c66ba500>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36c66baa70>, <keras.callbacks.ModelCheckpoint object at 0x7f36c66bab30>, <keras.callbacks.EarlyStopping object at 0x7f36c66bada0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36c66badd0>, <keras.callbacks.TerminateOnNaN object at 0x7f36c66baa10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-30 14:17:58.446460
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:20:23.428 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 145s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 145s/epoch - 739ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.524157902758725e-07.
===========
Generating train data for run 327.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_98"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_99 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7f36ce3cb280>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f374c66bdf0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f374c66bdf0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f364450a7d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36cde161d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36cde16740>, <keras.callbacks.ModelCheckpoint object at 0x7f36cde16800>, <keras.callbacks.EarlyStopping object at 0x7f36cde16a70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36cde16aa0>, <keras.callbacks.TerminateOnNaN object at 0x7f36cde166e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-30 14:20:31.013425
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:22:56.196 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 145s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 145s/epoch - 740ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.0805263425290834e-08.
===========
Generating train data for run 327.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_109"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_110 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7f36bfb472e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3d77adb0a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3d77adb0a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3d77b72c20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3d779aa0e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3d779aa650>, <keras.callbacks.ModelCheckpoint object at 0x7f3d779aa710>, <keras.callbacks.EarlyStopping object at 0x7f3d779aa980>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3d779aa9b0>, <keras.callbacks.TerminateOnNaN object at 0x7f3d779aa5f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-30 14:23:04.540071
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:25:30.693 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 146s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 146s/epoch - 745ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.6935087808430278e-08.
===========
Generating train data for run 327.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_120"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_121 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7f36c4107220>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f382c4effa0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f382c4effa0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f38246677c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36c40e26e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36c40e2c50>, <keras.callbacks.ModelCheckpoint object at 0x7f36c40e2d10>, <keras.callbacks.EarlyStopping object at 0x7f36c40e2f80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36c40e2fb0>, <keras.callbacks.TerminateOnNaN object at 0x7f36c40e2bf0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-30 14:25:39.208107
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:28:02.624 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 143s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 143s/epoch - 731ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.645029269476759e-09.
===========
Run 327/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_328/ already exists.
Skipping it.
===========
Run 328/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_329/ already exists.
Skipping it.
===========
Run 329/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_330/ already exists.
Skipping it.
===========
Run 330/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_331/ already exists.
Skipping it.
===========
Run 331/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_332/ already exists.
Skipping it.
===========
Run 332/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_333/ already exists.
Skipping it.
===========
Run 333/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_334/ already exists.
Skipping it.
===========
Run 334/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_335/ already exists.
Skipping it.
===========
Run 335/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_336/ already exists.
Skipping it.
===========
Run 336/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_337/ already exists.
Skipping it.
===========
Run 337/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_338/ already exists.
Skipping it.
===========
Run 338/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_339/ already exists.
Skipping it.
===========
Run 339/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_340/ already exists.
Skipping it.
===========
Run 340/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_341/ already exists.
Skipping it.
===========
Run 341/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_342/ already exists.
Skipping it.
===========
Run 342/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_343/ already exists.
Skipping it.
===========
Run 343/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_344/ already exists.
Skipping it.
===========
Run 344/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_345/ already exists.
Skipping it.
===========
Run 345/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_346/ already exists.
Skipping it.
===========
Run 346/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_347/ already exists.
Skipping it.
===========
Run 347/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_348/ already exists.
Skipping it.
===========
Run 348/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_349/ already exists.
Skipping it.
===========
Run 349/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_350/ already exists.
Skipping it.
===========
Run 350/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_351/ already exists.
Skipping it.
===========
Run 351/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_352/ already exists.
Skipping it.
===========
Run 352/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_353/ already exists.
Skipping it.
===========
Run 353/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_354/ already exists.
Skipping it.
===========
Run 354/720 already exists. Skipping it.
===========

===========
Generating train data for run 355.
===========
Train data generated in 0.13 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_126"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_127 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7f364434f700>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36047367d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36047367d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f38ec1b8250>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f387c329de0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f387c32a350>, <keras.callbacks.ModelCheckpoint object at 0x7f387c32a410>, <keras.callbacks.EarlyStopping object at 0x7f387c32a680>, <keras.callbacks.ReduceLROnPlateau object at 0x7f387c32a6b0>, <keras.callbacks.TerminateOnNaN object at 0x7f387c32a2f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-30 14:28:07.548432
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:29:13.769 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 66s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 66s/epoch - 337ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0003333333333333333.
===========
Generating train data for run 355.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_132"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_133 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7f3d776d6560>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3d76db3430>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3d76db3430>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f35dc260700>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3d76caa710>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3d76caac80>, <keras.callbacks.ModelCheckpoint object at 0x7f3d76caad40>, <keras.callbacks.EarlyStopping object at 0x7f3d76caafb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3d76caafe0>, <keras.callbacks.TerminateOnNaN object at 0x7f3d76caac20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-30 14:29:18.402663
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:30:27.054 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 69s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 69s/epoch - 350ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0001111111111111111.
===========
Generating train data for run 355.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_138"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_139 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7f3647c352d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36b4c0f490>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36b4c0f490>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f364492c5e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36b533ba90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36b533bfa0>, <keras.callbacks.ModelCheckpoint object at 0x7f36b533bfd0>, <keras.callbacks.EarlyStopping object at 0x7f36b53f4340>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36b53f43a0>, <keras.callbacks.TerminateOnNaN object at 0x7f36b53f4370>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-30 14:30:31.543515
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:31:39.435 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 68s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 68s/epoch - 346ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 3.703703703703703e-05.
===========
Generating train data for run 355.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_144"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_145 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7f3d7570bd60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f366f5360e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f366f5360e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f36bddde830>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3647fed4e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3647feda50>, <keras.callbacks.ModelCheckpoint object at 0x7f3647fedb10>, <keras.callbacks.EarlyStopping object at 0x7f3647fedd80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3647feddb0>, <keras.callbacks.TerminateOnNaN object at 0x7f3647fed9f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-30 14:31:44.149161
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:32:48.729 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 64s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 64s/epoch - 329ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.2345679012345677e-05.
===========
Generating train data for run 355.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_150"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_151 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7f3d764a8640>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3d7693bd30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3d7693bd30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f35dd051210>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3d760db5b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3d760dbb20>, <keras.callbacks.ModelCheckpoint object at 0x7f3d760dbbe0>, <keras.callbacks.EarlyStopping object at 0x7f3d760dbe50>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3d760dbe80>, <keras.callbacks.TerminateOnNaN object at 0x7f3d760dbac0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-30 14:32:53.449143
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:34:04.601 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 71s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 71s/epoch - 362ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.115226337448558e-06.
===========
Generating train data for run 355.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_156"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_157 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7f35dd610eb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f366f9b66b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f366f9b66b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3645f56c80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36063cfe80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3606348430>, <keras.callbacks.ModelCheckpoint object at 0x7f36063484f0>, <keras.callbacks.EarlyStopping object at 0x7f3606348760>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3606348790>, <keras.callbacks.TerminateOnNaN object at 0x7f36063483d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-30 14:34:09.245978
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:35:18.384 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 69s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 69s/epoch - 352ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.3717421124828526e-06.
===========
Generating train data for run 355.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_162"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_163 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7f36b6ec0550>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3647100850>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3647100850>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f36055775e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f35e74b5000>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f35e74b4190>, <keras.callbacks.ModelCheckpoint object at 0x7f35e74b4520>, <keras.callbacks.EarlyStopping object at 0x7f35e74b4040>, <keras.callbacks.ReduceLROnPlateau object at 0x7f35e74b40d0>, <keras.callbacks.TerminateOnNaN object at 0x7f35e74b4700>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-30 14:35:23.162511
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:36:30.384 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 67s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 67s/epoch - 342ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.572473708276175e-07.
===========
Generating train data for run 355.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_168"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_169 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7f3d6d565030>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3606b83d60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3606b83d60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3588b103d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3d54b257e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3d54b25d50>, <keras.callbacks.ModelCheckpoint object at 0x7f3d54b25e10>, <keras.callbacks.EarlyStopping object at 0x7f3d54b26080>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3d54b260b0>, <keras.callbacks.TerminateOnNaN object at 0x7f3d54b25cf0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-30 14:36:34.943475
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:37:42.415 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 67s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 67s/epoch - 343ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.524157902758725e-07.
===========
Generating train data for run 355.
===========
Train data generated in 0.12 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_174"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_175 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7f35a498bd00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f35a48de4d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f35a48de4d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f35e70ca1a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3606e016c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3606e025c0>, <keras.callbacks.ModelCheckpoint object at 0x7f3606e02650>, <keras.callbacks.EarlyStopping object at 0x7f3606e01d20>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3606e017e0>, <keras.callbacks.TerminateOnNaN object at 0x7f3606e02350>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-30 14:37:46.272658
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:38:56.871 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 70s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 70s/epoch - 360ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.0805263425290834e-08.
===========
Generating train data for run 355.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_180"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_181 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7f360608fc40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3d779d9600>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3d779d9600>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3d75881870>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3d76cfa4d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3d76cfbbb0>, <keras.callbacks.ModelCheckpoint object at 0x7f3d76cfa440>, <keras.callbacks.EarlyStopping object at 0x7f3d76cf8a90>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3d76cf8460>, <keras.callbacks.TerminateOnNaN object at 0x7f3d76cfbb80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-30 14:39:01.196656
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:40:08.191 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 67s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 67s/epoch - 342ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.6935087808430278e-08.
===========
Generating train data for run 355.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_355/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_355
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_186"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_187 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7f3d43aebd60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3d43d52260>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3d43d52260>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f358978dd20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3d43b3f550>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_355/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3d43b3fac0>, <keras.callbacks.ModelCheckpoint object at 0x7f3d43b3fb80>, <keras.callbacks.EarlyStopping object at 0x7f3d43b3fdf0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3d43b3fe20>, <keras.callbacks.TerminateOnNaN object at 0x7f3d43b3fa60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_355/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 355/720 with hyperparameters:
timestamp = 2023-10-30 14:40:12.788828
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:41:21.177 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6409.8218, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 68s - loss: nan - MinusLogProbMetric: 6409.8218 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 68s/epoch - 348ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.645029269476759e-09.
===========
Run 355/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_356/ already exists.
Skipping it.
===========
Run 356/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_357/ already exists.
Skipping it.
===========
Run 357/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_358/ already exists.
Skipping it.
===========
Run 358/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_359/ already exists.
Skipping it.
===========
Run 359/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_360/ already exists.
Skipping it.
===========
Run 360/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_361/ already exists.
Skipping it.
===========
Run 361/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_362/ already exists.
Skipping it.
===========
Run 362/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_363/ already exists.
Skipping it.
===========
Run 363/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_364/ already exists.
Skipping it.
===========
Run 364/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_365/ already exists.
Skipping it.
===========
Run 365/720 already exists. Skipping it.
===========

===========
Generating train data for run 366.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_366
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_197"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_198 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7f3d75965ff0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36b565d960>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36b565d960>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f366f9d6fb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36b5e076d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36b5e07c40>, <keras.callbacks.ModelCheckpoint object at 0x7f36b5e07d00>, <keras.callbacks.EarlyStopping object at 0x7f36b5e07f70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36b5e07fa0>, <keras.callbacks.TerminateOnNaN object at 0x7f36b5e07be0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_366/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 366/720 with hyperparameters:
timestamp = 2023-10-30 14:41:27.098544
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 4: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:43:30.259 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6683.5234, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 123s - loss: nan - MinusLogProbMetric: 6683.5234 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 123s/epoch - 628ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0003333333333333333.
===========
Generating train data for run 366.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_366
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_208"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_209 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7f37746b4790>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3784631ea0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3784631ea0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f389c6d90f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f371c6c1c60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f371c6c2980>, <keras.callbacks.ModelCheckpoint object at 0x7f371c6c0eb0>, <keras.callbacks.EarlyStopping object at 0x7f371c6c0d60>, <keras.callbacks.ReduceLROnPlateau object at 0x7f371c6c2620>, <keras.callbacks.TerminateOnNaN object at 0x7f371c6c20e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_366/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 366/720 with hyperparameters:
timestamp = 2023-10-30 14:43:37.891272
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 16: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:45:44.164 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5991.1509, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 126s - loss: nan - MinusLogProbMetric: 5991.1509 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 126s/epoch - 644ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0001111111111111111.
===========
Generating train data for run 366.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_366
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_219"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_220 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7f3606b49cf0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f382c5d2f20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f382c5d2f20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3da86937c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f35ddcbd780>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f35ddcbce20>, <keras.callbacks.ModelCheckpoint object at 0x7f35ddcbd150>, <keras.callbacks.EarlyStopping object at 0x7f35ddcbdbd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f35ddcbded0>, <keras.callbacks.TerminateOnNaN object at 0x7f35ddcbcfa0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_366/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 366/720 with hyperparameters:
timestamp = 2023-10-30 14:45:51.593886
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
2023-10-30 14:48:33.252 
Epoch 1/1000 
	 loss: 3202.3840, MinusLogProbMetric: 3202.3840, val_loss: 1177.5201, val_MinusLogProbMetric: 1177.5201

Epoch 1: val_loss improved from inf to 1177.52014, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 162s - loss: 3202.3840 - MinusLogProbMetric: 3202.3840 - val_loss: 1177.5201 - val_MinusLogProbMetric: 1177.5201 - lr: 1.1111e-04 - 162s/epoch - 828ms/step
Epoch 2/1000
2023-10-30 14:49:24.886 
Epoch 2/1000 
	 loss: 1011.1071, MinusLogProbMetric: 1011.1071, val_loss: 829.8029, val_MinusLogProbMetric: 829.8029

Epoch 2: val_loss improved from 1177.52014 to 829.80292, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 51s - loss: 1011.1071 - MinusLogProbMetric: 1011.1071 - val_loss: 829.8029 - val_MinusLogProbMetric: 829.8029 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 3/1000
2023-10-30 14:50:13.330 
Epoch 3/1000 
	 loss: 764.6223, MinusLogProbMetric: 764.6223, val_loss: 793.0084, val_MinusLogProbMetric: 793.0084

Epoch 3: val_loss improved from 829.80292 to 793.00836, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 48s - loss: 764.6223 - MinusLogProbMetric: 764.6223 - val_loss: 793.0084 - val_MinusLogProbMetric: 793.0084 - lr: 1.1111e-04 - 48s/epoch - 247ms/step
Epoch 4/1000
2023-10-30 14:51:02.179 
Epoch 4/1000 
	 loss: 656.3903, MinusLogProbMetric: 656.3903, val_loss: 780.4241, val_MinusLogProbMetric: 780.4241

Epoch 4: val_loss improved from 793.00836 to 780.42413, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 49s - loss: 656.3903 - MinusLogProbMetric: 656.3903 - val_loss: 780.4241 - val_MinusLogProbMetric: 780.4241 - lr: 1.1111e-04 - 49s/epoch - 249ms/step
Epoch 5/1000
2023-10-30 14:51:51.569 
Epoch 5/1000 
	 loss: 1008.8308, MinusLogProbMetric: 1008.8308, val_loss: 761.7479, val_MinusLogProbMetric: 761.7479

Epoch 5: val_loss improved from 780.42413 to 761.74786, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 49s - loss: 1008.8308 - MinusLogProbMetric: 1008.8308 - val_loss: 761.7479 - val_MinusLogProbMetric: 761.7479 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 6/1000
2023-10-30 14:52:39.962 
Epoch 6/1000 
	 loss: 788.9563, MinusLogProbMetric: 788.9563, val_loss: 960.3593, val_MinusLogProbMetric: 960.3593

Epoch 6: val_loss did not improve from 761.74786
196/196 - 48s - loss: 788.9563 - MinusLogProbMetric: 788.9563 - val_loss: 960.3593 - val_MinusLogProbMetric: 960.3593 - lr: 1.1111e-04 - 48s/epoch - 243ms/step
Epoch 7/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 82: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 14:53:02.389 
Epoch 7/1000 
	 loss: nan, MinusLogProbMetric: 834.4155, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 7: val_loss did not improve from 761.74786
196/196 - 22s - loss: nan - MinusLogProbMetric: 834.4155 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 22s/epoch - 114ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 3.703703703703703e-05.
===========
Generating train data for run 366.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_366
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_230"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_231 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7f3784452bf0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f378c3ab5b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f378c3ab5b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f390c64f670>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3880e1ea70>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3880e1f640>, <keras.callbacks.ModelCheckpoint object at 0x7f3880e1e260>, <keras.callbacks.EarlyStopping object at 0x7f3880e1ece0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3880e1e020>, <keras.callbacks.TerminateOnNaN object at 0x7f3880e1e650>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 366/720 with hyperparameters:
timestamp = 2023-10-30 14:53:08.033161
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
2023-10-30 14:55:49.916 
Epoch 1/1000 
	 loss: 967.0769, MinusLogProbMetric: 967.0769, val_loss: 1025.5747, val_MinusLogProbMetric: 1025.5747

Epoch 1: val_loss improved from inf to 1025.57471, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 163s - loss: 967.0769 - MinusLogProbMetric: 967.0769 - val_loss: 1025.5747 - val_MinusLogProbMetric: 1025.5747 - lr: 3.7037e-05 - 163s/epoch - 829ms/step
Epoch 2/1000
2023-10-30 14:56:38.372 
Epoch 2/1000 
	 loss: 705.1910, MinusLogProbMetric: 705.1910, val_loss: 600.3462, val_MinusLogProbMetric: 600.3462

Epoch 2: val_loss improved from 1025.57471 to 600.34619, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 48s - loss: 705.1910 - MinusLogProbMetric: 705.1910 - val_loss: 600.3462 - val_MinusLogProbMetric: 600.3462 - lr: 3.7037e-05 - 48s/epoch - 246ms/step
Epoch 3/1000
2023-10-30 14:57:27.956 
Epoch 3/1000 
	 loss: 556.9882, MinusLogProbMetric: 556.9882, val_loss: 517.4585, val_MinusLogProbMetric: 517.4585

Epoch 3: val_loss improved from 600.34619 to 517.45850, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 556.9882 - MinusLogProbMetric: 556.9882 - val_loss: 517.4585 - val_MinusLogProbMetric: 517.4585 - lr: 3.7037e-05 - 50s/epoch - 253ms/step
Epoch 4/1000
2023-10-30 14:58:17.055 
Epoch 4/1000 
	 loss: 525.6039, MinusLogProbMetric: 525.6039, val_loss: 478.5528, val_MinusLogProbMetric: 478.5528

Epoch 4: val_loss improved from 517.45850 to 478.55280, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 49s - loss: 525.6039 - MinusLogProbMetric: 525.6039 - val_loss: 478.5528 - val_MinusLogProbMetric: 478.5528 - lr: 3.7037e-05 - 49s/epoch - 251ms/step
Epoch 5/1000
2023-10-30 14:59:06.419 
Epoch 5/1000 
	 loss: 537.1003, MinusLogProbMetric: 537.1003, val_loss: 529.4158, val_MinusLogProbMetric: 529.4158

Epoch 5: val_loss did not improve from 478.55280
196/196 - 48s - loss: 537.1003 - MinusLogProbMetric: 537.1003 - val_loss: 529.4158 - val_MinusLogProbMetric: 529.4158 - lr: 3.7037e-05 - 48s/epoch - 247ms/step
Epoch 6/1000
2023-10-30 14:59:54.993 
Epoch 6/1000 
	 loss: 656.9215, MinusLogProbMetric: 656.9215, val_loss: 633.4258, val_MinusLogProbMetric: 633.4258

Epoch 6: val_loss did not improve from 478.55280
196/196 - 49s - loss: 656.9215 - MinusLogProbMetric: 656.9215 - val_loss: 633.4258 - val_MinusLogProbMetric: 633.4258 - lr: 3.7037e-05 - 49s/epoch - 248ms/step
Epoch 7/1000
2023-10-30 15:00:43.180 
Epoch 7/1000 
	 loss: 581.5099, MinusLogProbMetric: 581.5099, val_loss: 529.2661, val_MinusLogProbMetric: 529.2661

Epoch 7: val_loss did not improve from 478.55280
196/196 - 48s - loss: 581.5099 - MinusLogProbMetric: 581.5099 - val_loss: 529.2661 - val_MinusLogProbMetric: 529.2661 - lr: 3.7037e-05 - 48s/epoch - 246ms/step
Epoch 8/1000
2023-10-30 15:01:31.662 
Epoch 8/1000 
	 loss: 538.0591, MinusLogProbMetric: 538.0591, val_loss: 505.7111, val_MinusLogProbMetric: 505.7111

Epoch 8: val_loss did not improve from 478.55280
196/196 - 48s - loss: 538.0591 - MinusLogProbMetric: 538.0591 - val_loss: 505.7111 - val_MinusLogProbMetric: 505.7111 - lr: 3.7037e-05 - 48s/epoch - 247ms/step
Epoch 9/1000
2023-10-30 15:02:20.134 
Epoch 9/1000 
	 loss: 482.0283, MinusLogProbMetric: 482.0283, val_loss: 497.8146, val_MinusLogProbMetric: 497.8146

Epoch 9: val_loss did not improve from 478.55280
196/196 - 48s - loss: 482.0283 - MinusLogProbMetric: 482.0283 - val_loss: 497.8146 - val_MinusLogProbMetric: 497.8146 - lr: 3.7037e-05 - 48s/epoch - 247ms/step
Epoch 10/1000
2023-10-30 15:03:08.822 
Epoch 10/1000 
	 loss: 450.8980, MinusLogProbMetric: 450.8980, val_loss: 420.1414, val_MinusLogProbMetric: 420.1414

Epoch 10: val_loss improved from 478.55280 to 420.14139, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 49s - loss: 450.8980 - MinusLogProbMetric: 450.8980 - val_loss: 420.1414 - val_MinusLogProbMetric: 420.1414 - lr: 3.7037e-05 - 49s/epoch - 252ms/step
Epoch 11/1000
2023-10-30 15:03:57.866 
Epoch 11/1000 
	 loss: 401.5731, MinusLogProbMetric: 401.5731, val_loss: 385.0327, val_MinusLogProbMetric: 385.0327

Epoch 11: val_loss improved from 420.14139 to 385.03275, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 49s - loss: 401.5731 - MinusLogProbMetric: 401.5731 - val_loss: 385.0327 - val_MinusLogProbMetric: 385.0327 - lr: 3.7037e-05 - 49s/epoch - 249ms/step
Epoch 12/1000
2023-10-30 15:04:47.708 
Epoch 12/1000 
	 loss: 375.9480, MinusLogProbMetric: 375.9480, val_loss: 366.0565, val_MinusLogProbMetric: 366.0565

Epoch 12: val_loss improved from 385.03275 to 366.05649, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 375.9480 - MinusLogProbMetric: 375.9480 - val_loss: 366.0565 - val_MinusLogProbMetric: 366.0565 - lr: 3.7037e-05 - 50s/epoch - 255ms/step
Epoch 13/1000
2023-10-30 15:05:37.409 
Epoch 13/1000 
	 loss: 353.8288, MinusLogProbMetric: 353.8288, val_loss: 344.4688, val_MinusLogProbMetric: 344.4688

Epoch 13: val_loss improved from 366.05649 to 344.46881, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 353.8288 - MinusLogProbMetric: 353.8288 - val_loss: 344.4688 - val_MinusLogProbMetric: 344.4688 - lr: 3.7037e-05 - 50s/epoch - 254ms/step
Epoch 14/1000
2023-10-30 15:06:26.726 
Epoch 14/1000 
	 loss: 335.3913, MinusLogProbMetric: 335.3913, val_loss: 324.6233, val_MinusLogProbMetric: 324.6233

Epoch 14: val_loss improved from 344.46881 to 324.62329, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 49s - loss: 335.3913 - MinusLogProbMetric: 335.3913 - val_loss: 324.6233 - val_MinusLogProbMetric: 324.6233 - lr: 3.7037e-05 - 49s/epoch - 252ms/step
Epoch 15/1000
2023-10-30 15:07:17.438 
Epoch 15/1000 
	 loss: 318.5058, MinusLogProbMetric: 318.5058, val_loss: 313.6682, val_MinusLogProbMetric: 313.6682

Epoch 15: val_loss improved from 324.62329 to 313.66824, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 51s - loss: 318.5058 - MinusLogProbMetric: 318.5058 - val_loss: 313.6682 - val_MinusLogProbMetric: 313.6682 - lr: 3.7037e-05 - 51s/epoch - 259ms/step
Epoch 16/1000
2023-10-30 15:08:07.305 
Epoch 16/1000 
	 loss: 305.6272, MinusLogProbMetric: 305.6272, val_loss: 298.0307, val_MinusLogProbMetric: 298.0307

Epoch 16: val_loss improved from 313.66824 to 298.03073, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 305.6272 - MinusLogProbMetric: 305.6272 - val_loss: 298.0307 - val_MinusLogProbMetric: 298.0307 - lr: 3.7037e-05 - 50s/epoch - 254ms/step
Epoch 17/1000
2023-10-30 15:08:56.878 
Epoch 17/1000 
	 loss: 293.2495, MinusLogProbMetric: 293.2495, val_loss: 286.1946, val_MinusLogProbMetric: 286.1946

Epoch 17: val_loss improved from 298.03073 to 286.19458, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 293.2495 - MinusLogProbMetric: 293.2495 - val_loss: 286.1946 - val_MinusLogProbMetric: 286.1946 - lr: 3.7037e-05 - 50s/epoch - 253ms/step
Epoch 18/1000
2023-10-30 15:09:47.043 
Epoch 18/1000 
	 loss: 284.1614, MinusLogProbMetric: 284.1614, val_loss: 274.7526, val_MinusLogProbMetric: 274.7526

Epoch 18: val_loss improved from 286.19458 to 274.75262, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 284.1614 - MinusLogProbMetric: 284.1614 - val_loss: 274.7526 - val_MinusLogProbMetric: 274.7526 - lr: 3.7037e-05 - 50s/epoch - 256ms/step
Epoch 19/1000
2023-10-30 15:10:36.331 
Epoch 19/1000 
	 loss: 282.2856, MinusLogProbMetric: 282.2856, val_loss: 289.8292, val_MinusLogProbMetric: 289.8292

Epoch 19: val_loss did not improve from 274.75262
196/196 - 49s - loss: 282.2856 - MinusLogProbMetric: 282.2856 - val_loss: 289.8292 - val_MinusLogProbMetric: 289.8292 - lr: 3.7037e-05 - 49s/epoch - 248ms/step
Epoch 20/1000
2023-10-30 15:11:25.510 
Epoch 20/1000 
	 loss: 323.5122, MinusLogProbMetric: 323.5122, val_loss: 290.6687, val_MinusLogProbMetric: 290.6687

Epoch 20: val_loss did not improve from 274.75262
196/196 - 49s - loss: 323.5122 - MinusLogProbMetric: 323.5122 - val_loss: 290.6687 - val_MinusLogProbMetric: 290.6687 - lr: 3.7037e-05 - 49s/epoch - 251ms/step
Epoch 21/1000
2023-10-30 15:12:14.657 
Epoch 21/1000 
	 loss: 281.1125, MinusLogProbMetric: 281.1125, val_loss: 275.4622, val_MinusLogProbMetric: 275.4622

Epoch 21: val_loss did not improve from 274.75262
196/196 - 49s - loss: 281.1125 - MinusLogProbMetric: 281.1125 - val_loss: 275.4622 - val_MinusLogProbMetric: 275.4622 - lr: 3.7037e-05 - 49s/epoch - 251ms/step
Epoch 22/1000
2023-10-30 15:13:11.360 
Epoch 22/1000 
	 loss: 326.8129, MinusLogProbMetric: 326.8129, val_loss: 379.1833, val_MinusLogProbMetric: 379.1833

Epoch 22: val_loss did not improve from 274.75262
196/196 - 57s - loss: 326.8129 - MinusLogProbMetric: 326.8129 - val_loss: 379.1833 - val_MinusLogProbMetric: 379.1833 - lr: 3.7037e-05 - 57s/epoch - 289ms/step
Epoch 23/1000
2023-10-30 15:14:00.673 
Epoch 23/1000 
	 loss: 334.6635, MinusLogProbMetric: 334.6635, val_loss: 306.5516, val_MinusLogProbMetric: 306.5516

Epoch 23: val_loss did not improve from 274.75262
196/196 - 49s - loss: 334.6635 - MinusLogProbMetric: 334.6635 - val_loss: 306.5516 - val_MinusLogProbMetric: 306.5516 - lr: 3.7037e-05 - 49s/epoch - 252ms/step
Epoch 24/1000
2023-10-30 15:14:49.535 
Epoch 24/1000 
	 loss: 294.9408, MinusLogProbMetric: 294.9408, val_loss: 283.8878, val_MinusLogProbMetric: 283.8878

Epoch 24: val_loss did not improve from 274.75262
196/196 - 49s - loss: 294.9408 - MinusLogProbMetric: 294.9408 - val_loss: 283.8878 - val_MinusLogProbMetric: 283.8878 - lr: 3.7037e-05 - 49s/epoch - 249ms/step
Epoch 25/1000
2023-10-30 15:15:39.231 
Epoch 25/1000 
	 loss: 279.8340, MinusLogProbMetric: 279.8340, val_loss: 273.3909, val_MinusLogProbMetric: 273.3909

Epoch 25: val_loss improved from 274.75262 to 273.39087, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 51s - loss: 279.8340 - MinusLogProbMetric: 279.8340 - val_loss: 273.3909 - val_MinusLogProbMetric: 273.3909 - lr: 3.7037e-05 - 51s/epoch - 258ms/step
Epoch 26/1000
2023-10-30 15:16:34.962 
Epoch 26/1000 
	 loss: 273.5545, MinusLogProbMetric: 273.5545, val_loss: 268.9713, val_MinusLogProbMetric: 268.9713

Epoch 26: val_loss improved from 273.39087 to 268.97134, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 273.5545 - MinusLogProbMetric: 273.5545 - val_loss: 268.9713 - val_MinusLogProbMetric: 268.9713 - lr: 3.7037e-05 - 56s/epoch - 284ms/step
Epoch 27/1000
2023-10-30 15:17:25.298 
Epoch 27/1000 
	 loss: 298.8906, MinusLogProbMetric: 298.8906, val_loss: 288.0360, val_MinusLogProbMetric: 288.0360

Epoch 27: val_loss did not improve from 268.97134
196/196 - 50s - loss: 298.8906 - MinusLogProbMetric: 298.8906 - val_loss: 288.0360 - val_MinusLogProbMetric: 288.0360 - lr: 3.7037e-05 - 50s/epoch - 253ms/step
Epoch 28/1000
2023-10-30 15:18:15.410 
Epoch 28/1000 
	 loss: 295.7615, MinusLogProbMetric: 295.7615, val_loss: 270.2545, val_MinusLogProbMetric: 270.2545

Epoch 28: val_loss did not improve from 268.97134
196/196 - 50s - loss: 295.7615 - MinusLogProbMetric: 295.7615 - val_loss: 270.2545 - val_MinusLogProbMetric: 270.2545 - lr: 3.7037e-05 - 50s/epoch - 256ms/step
Epoch 29/1000
2023-10-30 15:19:08.606 
Epoch 29/1000 
	 loss: 260.7477, MinusLogProbMetric: 260.7477, val_loss: 252.5737, val_MinusLogProbMetric: 252.5737

Epoch 29: val_loss improved from 268.97134 to 252.57375, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 260.7477 - MinusLogProbMetric: 260.7477 - val_loss: 252.5737 - val_MinusLogProbMetric: 252.5737 - lr: 3.7037e-05 - 54s/epoch - 277ms/step
Epoch 30/1000
2023-10-30 15:20:01.757 
Epoch 30/1000 
	 loss: 257.5502, MinusLogProbMetric: 257.5502, val_loss: 246.5669, val_MinusLogProbMetric: 246.5669

Epoch 30: val_loss improved from 252.57375 to 246.56686, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 53s - loss: 257.5502 - MinusLogProbMetric: 257.5502 - val_loss: 246.5669 - val_MinusLogProbMetric: 246.5669 - lr: 3.7037e-05 - 53s/epoch - 270ms/step
Epoch 31/1000
2023-10-30 15:20:52.127 
Epoch 31/1000 
	 loss: 239.5960, MinusLogProbMetric: 239.5960, val_loss: 233.6558, val_MinusLogProbMetric: 233.6558

Epoch 31: val_loss improved from 246.56686 to 233.65582, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 239.5960 - MinusLogProbMetric: 239.5960 - val_loss: 233.6558 - val_MinusLogProbMetric: 233.6558 - lr: 3.7037e-05 - 50s/epoch - 257ms/step
Epoch 32/1000
2023-10-30 15:21:45.559 
Epoch 32/1000 
	 loss: 231.1141, MinusLogProbMetric: 231.1141, val_loss: 228.6908, val_MinusLogProbMetric: 228.6908

Epoch 32: val_loss improved from 233.65582 to 228.69078, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 231.1141 - MinusLogProbMetric: 231.1141 - val_loss: 228.6908 - val_MinusLogProbMetric: 228.6908 - lr: 3.7037e-05 - 54s/epoch - 274ms/step
Epoch 33/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 146: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:22:27.608 
Epoch 33/1000 
	 loss: nan, MinusLogProbMetric: 649.8220, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 33: val_loss did not improve from 228.69078
196/196 - 41s - loss: nan - MinusLogProbMetric: 649.8220 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 41s/epoch - 209ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 1.2345679012345677e-05.
===========
Generating train data for run 366.
===========
Train data generated in 0.52 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_366
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_241"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_242 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7f3da9cf6ec0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f38ec629450>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f38ec629450>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f38705645b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3daa509c00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3daa50a140>, <keras.callbacks.ModelCheckpoint object at 0x7f3daa50a500>, <keras.callbacks.EarlyStopping object at 0x7f3daa509ba0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3daa509900>, <keras.callbacks.TerminateOnNaN object at 0x7f3daa509960>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 366/720 with hyperparameters:
timestamp = 2023-10-30 15:22:36.442564
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
2023-10-30 15:25:27.523 
Epoch 1/1000 
	 loss: 229.0867, MinusLogProbMetric: 229.0867, val_loss: 205.2265, val_MinusLogProbMetric: 205.2265

Epoch 1: val_loss improved from inf to 205.22652, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 171s - loss: 229.0867 - MinusLogProbMetric: 229.0867 - val_loss: 205.2265 - val_MinusLogProbMetric: 205.2265 - lr: 1.2346e-05 - 171s/epoch - 874ms/step
Epoch 2/1000
2023-10-30 15:26:18.123 
Epoch 2/1000 
	 loss: 212.9583, MinusLogProbMetric: 212.9583, val_loss: 244.8415, val_MinusLogProbMetric: 244.8415

Epoch 2: val_loss did not improve from 205.22652
196/196 - 50s - loss: 212.9583 - MinusLogProbMetric: 212.9583 - val_loss: 244.8415 - val_MinusLogProbMetric: 244.8415 - lr: 1.2346e-05 - 50s/epoch - 253ms/step
Epoch 3/1000
2023-10-30 15:27:08.377 
Epoch 3/1000 
	 loss: 199.2317, MinusLogProbMetric: 199.2317, val_loss: 184.9805, val_MinusLogProbMetric: 184.9805

Epoch 3: val_loss improved from 205.22652 to 184.98051, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 51s - loss: 199.2317 - MinusLogProbMetric: 199.2317 - val_loss: 184.9805 - val_MinusLogProbMetric: 184.9805 - lr: 1.2346e-05 - 51s/epoch - 261ms/step
Epoch 4/1000
2023-10-30 15:28:03.565 
Epoch 4/1000 
	 loss: 235.3341, MinusLogProbMetric: 235.3341, val_loss: 238.9650, val_MinusLogProbMetric: 238.9650

Epoch 4: val_loss did not improve from 184.98051
196/196 - 54s - loss: 235.3341 - MinusLogProbMetric: 235.3341 - val_loss: 238.9650 - val_MinusLogProbMetric: 238.9650 - lr: 1.2346e-05 - 54s/epoch - 277ms/step
Epoch 5/1000
2023-10-30 15:28:53.278 
Epoch 5/1000 
	 loss: 210.8249, MinusLogProbMetric: 210.8249, val_loss: 186.5409, val_MinusLogProbMetric: 186.5409

Epoch 5: val_loss did not improve from 184.98051
196/196 - 50s - loss: 210.8249 - MinusLogProbMetric: 210.8249 - val_loss: 186.5409 - val_MinusLogProbMetric: 186.5409 - lr: 1.2346e-05 - 50s/epoch - 254ms/step
Epoch 6/1000
2023-10-30 15:29:42.534 
Epoch 6/1000 
	 loss: 188.9358, MinusLogProbMetric: 188.9358, val_loss: 176.4977, val_MinusLogProbMetric: 176.4977

Epoch 6: val_loss improved from 184.98051 to 176.49768, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 188.9358 - MinusLogProbMetric: 188.9358 - val_loss: 176.4977 - val_MinusLogProbMetric: 176.4977 - lr: 1.2346e-05 - 50s/epoch - 255ms/step
Epoch 7/1000
2023-10-30 15:30:34.877 
Epoch 7/1000 
	 loss: 176.4490, MinusLogProbMetric: 176.4490, val_loss: 173.5370, val_MinusLogProbMetric: 173.5370

Epoch 7: val_loss improved from 176.49768 to 173.53703, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 53s - loss: 176.4490 - MinusLogProbMetric: 176.4490 - val_loss: 173.5370 - val_MinusLogProbMetric: 173.5370 - lr: 1.2346e-05 - 53s/epoch - 268ms/step
Epoch 8/1000
2023-10-30 15:31:28.946 
Epoch 8/1000 
	 loss: 165.5169, MinusLogProbMetric: 165.5169, val_loss: 161.2319, val_MinusLogProbMetric: 161.2319

Epoch 8: val_loss improved from 173.53703 to 161.23187, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 165.5169 - MinusLogProbMetric: 165.5169 - val_loss: 161.2319 - val_MinusLogProbMetric: 161.2319 - lr: 1.2346e-05 - 54s/epoch - 275ms/step
Epoch 9/1000
2023-10-30 15:32:18.252 
Epoch 9/1000 
	 loss: 157.6862, MinusLogProbMetric: 157.6862, val_loss: 154.0834, val_MinusLogProbMetric: 154.0834

Epoch 9: val_loss improved from 161.23187 to 154.08339, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 49s - loss: 157.6862 - MinusLogProbMetric: 157.6862 - val_loss: 154.0834 - val_MinusLogProbMetric: 154.0834 - lr: 1.2346e-05 - 49s/epoch - 251ms/step
Epoch 10/1000
2023-10-30 15:33:08.373 
Epoch 10/1000 
	 loss: 152.7052, MinusLogProbMetric: 152.7052, val_loss: 149.7980, val_MinusLogProbMetric: 149.7980

Epoch 10: val_loss improved from 154.08339 to 149.79797, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 152.7052 - MinusLogProbMetric: 152.7052 - val_loss: 149.7980 - val_MinusLogProbMetric: 149.7980 - lr: 1.2346e-05 - 50s/epoch - 256ms/step
Epoch 11/1000
2023-10-30 15:34:00.740 
Epoch 11/1000 
	 loss: 165.6621, MinusLogProbMetric: 165.6621, val_loss: 158.4662, val_MinusLogProbMetric: 158.4662

Epoch 11: val_loss did not improve from 149.79797
196/196 - 52s - loss: 165.6621 - MinusLogProbMetric: 165.6621 - val_loss: 158.4662 - val_MinusLogProbMetric: 158.4662 - lr: 1.2346e-05 - 52s/epoch - 263ms/step
Epoch 12/1000
2023-10-30 15:34:50.611 
Epoch 12/1000 
	 loss: 151.8277, MinusLogProbMetric: 151.8277, val_loss: 147.2752, val_MinusLogProbMetric: 147.2752

Epoch 12: val_loss improved from 149.79797 to 147.27518, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 51s - loss: 151.8277 - MinusLogProbMetric: 151.8277 - val_loss: 147.2752 - val_MinusLogProbMetric: 147.2752 - lr: 1.2346e-05 - 51s/epoch - 258ms/step
Epoch 13/1000
2023-10-30 15:35:40.787 
Epoch 13/1000 
	 loss: 151.1544, MinusLogProbMetric: 151.1544, val_loss: 147.2449, val_MinusLogProbMetric: 147.2449

Epoch 13: val_loss improved from 147.27518 to 147.24492, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 151.1544 - MinusLogProbMetric: 151.1544 - val_loss: 147.2449 - val_MinusLogProbMetric: 147.2449 - lr: 1.2346e-05 - 50s/epoch - 256ms/step
Epoch 14/1000
2023-10-30 15:36:32.320 
Epoch 14/1000 
	 loss: 145.8682, MinusLogProbMetric: 145.8682, val_loss: 142.0097, val_MinusLogProbMetric: 142.0097

Epoch 14: val_loss improved from 147.24492 to 142.00970, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 52s - loss: 145.8682 - MinusLogProbMetric: 145.8682 - val_loss: 142.0097 - val_MinusLogProbMetric: 142.0097 - lr: 1.2346e-05 - 52s/epoch - 263ms/step
Epoch 15/1000
2023-10-30 15:37:22.090 
Epoch 15/1000 
	 loss: 144.1142, MinusLogProbMetric: 144.1142, val_loss: 138.4273, val_MinusLogProbMetric: 138.4273

Epoch 15: val_loss improved from 142.00970 to 138.42732, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 144.1142 - MinusLogProbMetric: 144.1142 - val_loss: 138.4273 - val_MinusLogProbMetric: 138.4273 - lr: 1.2346e-05 - 50s/epoch - 253ms/step
Epoch 16/1000
2023-10-30 15:38:11.524 
Epoch 16/1000 
	 loss: 135.6691, MinusLogProbMetric: 135.6691, val_loss: 133.8942, val_MinusLogProbMetric: 133.8942

Epoch 16: val_loss improved from 138.42732 to 133.89423, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 49s - loss: 135.6691 - MinusLogProbMetric: 135.6691 - val_loss: 133.8942 - val_MinusLogProbMetric: 133.8942 - lr: 1.2346e-05 - 49s/epoch - 252ms/step
Epoch 17/1000
2023-10-30 15:39:02.527 
Epoch 17/1000 
	 loss: 130.8982, MinusLogProbMetric: 130.8982, val_loss: 128.5455, val_MinusLogProbMetric: 128.5455

Epoch 17: val_loss improved from 133.89423 to 128.54546, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 51s - loss: 130.8982 - MinusLogProbMetric: 130.8982 - val_loss: 128.5455 - val_MinusLogProbMetric: 128.5455 - lr: 1.2346e-05 - 51s/epoch - 261ms/step
Epoch 18/1000
2023-10-30 15:39:57.034 
Epoch 18/1000 
	 loss: 126.7377, MinusLogProbMetric: 126.7377, val_loss: 124.9554, val_MinusLogProbMetric: 124.9554

Epoch 18: val_loss improved from 128.54546 to 124.95536, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 126.7377 - MinusLogProbMetric: 126.7377 - val_loss: 124.9554 - val_MinusLogProbMetric: 124.9554 - lr: 1.2346e-05 - 54s/epoch - 277ms/step
Epoch 19/1000
2023-10-30 15:40:46.287 
Epoch 19/1000 
	 loss: 132.2185, MinusLogProbMetric: 132.2185, val_loss: 127.3664, val_MinusLogProbMetric: 127.3664

Epoch 19: val_loss did not improve from 124.95536
196/196 - 49s - loss: 132.2185 - MinusLogProbMetric: 132.2185 - val_loss: 127.3664 - val_MinusLogProbMetric: 127.3664 - lr: 1.2346e-05 - 49s/epoch - 248ms/step
Epoch 20/1000
2023-10-30 15:41:35.498 
Epoch 20/1000 
	 loss: 124.5124, MinusLogProbMetric: 124.5124, val_loss: 121.2550, val_MinusLogProbMetric: 121.2550

Epoch 20: val_loss improved from 124.95536 to 121.25503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 124.5124 - MinusLogProbMetric: 124.5124 - val_loss: 121.2550 - val_MinusLogProbMetric: 121.2550 - lr: 1.2346e-05 - 50s/epoch - 255ms/step
Epoch 21/1000
2023-10-30 15:42:29.983 
Epoch 21/1000 
	 loss: 119.7495, MinusLogProbMetric: 119.7495, val_loss: 117.8906, val_MinusLogProbMetric: 117.8906

Epoch 21: val_loss improved from 121.25503 to 117.89061, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 119.7495 - MinusLogProbMetric: 119.7495 - val_loss: 117.8906 - val_MinusLogProbMetric: 117.8906 - lr: 1.2346e-05 - 54s/epoch - 278ms/step
Epoch 22/1000
2023-10-30 15:43:21.028 
Epoch 22/1000 
	 loss: 116.2877, MinusLogProbMetric: 116.2877, val_loss: 115.2241, val_MinusLogProbMetric: 115.2241

Epoch 22: val_loss improved from 117.89061 to 115.22410, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 51s - loss: 116.2877 - MinusLogProbMetric: 116.2877 - val_loss: 115.2241 - val_MinusLogProbMetric: 115.2241 - lr: 1.2346e-05 - 51s/epoch - 260ms/step
Epoch 23/1000
2023-10-30 15:44:11.991 
Epoch 23/1000 
	 loss: 118.0418, MinusLogProbMetric: 118.0418, val_loss: 116.1994, val_MinusLogProbMetric: 116.1994

Epoch 23: val_loss did not improve from 115.22410
196/196 - 50s - loss: 118.0418 - MinusLogProbMetric: 118.0418 - val_loss: 116.1994 - val_MinusLogProbMetric: 116.1994 - lr: 1.2346e-05 - 50s/epoch - 256ms/step
Epoch 24/1000
2023-10-30 15:45:04.614 
Epoch 24/1000 
	 loss: 114.1679, MinusLogProbMetric: 114.1679, val_loss: 112.1971, val_MinusLogProbMetric: 112.1971

Epoch 24: val_loss improved from 115.22410 to 112.19714, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 53s - loss: 114.1679 - MinusLogProbMetric: 114.1679 - val_loss: 112.1971 - val_MinusLogProbMetric: 112.1971 - lr: 1.2346e-05 - 53s/epoch - 272ms/step
Epoch 25/1000
2023-10-30 15:45:54.823 
Epoch 25/1000 
	 loss: 159.9087, MinusLogProbMetric: 159.9087, val_loss: 166.1605, val_MinusLogProbMetric: 166.1605

Epoch 25: val_loss did not improve from 112.19714
196/196 - 49s - loss: 159.9087 - MinusLogProbMetric: 159.9087 - val_loss: 166.1605 - val_MinusLogProbMetric: 166.1605 - lr: 1.2346e-05 - 49s/epoch - 252ms/step
Epoch 26/1000
2023-10-30 15:46:44.579 
Epoch 26/1000 
	 loss: 151.8619, MinusLogProbMetric: 151.8619, val_loss: 143.0661, val_MinusLogProbMetric: 143.0661

Epoch 26: val_loss did not improve from 112.19714
196/196 - 50s - loss: 151.8619 - MinusLogProbMetric: 151.8619 - val_loss: 143.0661 - val_MinusLogProbMetric: 143.0661 - lr: 1.2346e-05 - 50s/epoch - 254ms/step
Epoch 27/1000
2023-10-30 15:47:33.189 
Epoch 27/1000 
	 loss: 136.9451, MinusLogProbMetric: 136.9451, val_loss: 132.9805, val_MinusLogProbMetric: 132.9805

Epoch 27: val_loss did not improve from 112.19714
196/196 - 49s - loss: 136.9451 - MinusLogProbMetric: 136.9451 - val_loss: 132.9805 - val_MinusLogProbMetric: 132.9805 - lr: 1.2346e-05 - 49s/epoch - 248ms/step
Epoch 28/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 118: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-30 15:48:03.833 
Epoch 28/1000 
	 loss: nan, MinusLogProbMetric: 132.7219, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 28: val_loss did not improve from 112.19714
196/196 - 31s - loss: nan - MinusLogProbMetric: 132.7219 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 31s/epoch - 156ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 4.115226337448558e-06.
===========
Generating train data for run 366.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_366/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_366
self.data_kwargs: {'seed': 541}
self.x_data: [[7.2128525 3.0379212 6.2367196 ... 3.0671625 3.9712758 2.9062366]
 [6.9294653 2.776415  6.1588674 ... 3.1774852 3.3745446 1.8558645]
 [5.9664783 2.8420978 6.1059666 ... 2.842883  4.150837  2.5282533]
 ...
 [1.7772449 4.049644  7.7634    ... 5.1831775 1.4676232 2.7587254]
 [5.071257  6.457654  5.797537  ... 1.1522104 7.100011  1.4003892]
 [5.5332866 8.634485  5.3762307 ... 1.4035112 6.4775186 1.2732773]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_252"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_253 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  3291840   
 ayer)                                                           
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7f3d432f1600>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f35e4c01690>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f35e4c01690>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f35e7faea70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3d42d08220>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3d42d08790>, <keras.callbacks.ModelCheckpoint object at 0x7f3d42d08850>, <keras.callbacks.EarlyStopping object at 0x7f3d42d08ac0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3d42d08af0>, <keras.callbacks.TerminateOnNaN object at 0x7f3d42d08730>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.672276  , 7.747077  , 5.353606  , ..., 1.9898596 , 6.4533057 ,
        1.3801477 ],
       [6.831665  , 2.8270175 , 6.123529  , ..., 3.7326455 , 4.034952  ,
        1.2204404 ],
       [6.945523  , 3.0432987 , 6.2105227 , ..., 3.0559149 , 4.230059  ,
        2.415763  ],
       ...,
       [5.3746367 , 7.366772  , 6.0980034 , ..., 1.4703689 , 7.145194  ,
        1.4068515 ],
       [1.7761964 , 3.7400303 , 9.268795  , ..., 5.5180016 , 0.19900042,
        4.052124  ],
       [5.3399296 , 8.263746  , 6.06707   , ..., 1.0115646 , 6.354734  ,
        1.5268791 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 366/720 with hyperparameters:
timestamp = 2023-10-30 15:48:10.660485
ndims = 64
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 7.2128525   3.0379212   6.2367196   4.3304725   1.1500694   2.2690165
  5.1556735   5.938522    5.8135405   6.610918    6.7030854   3.1799724
  8.919708    2.7416692   3.9000368   9.134979    8.039232    8.02363
  1.5622978   9.261075    6.5763392  10.023463    0.6410362   8.711104
  0.19289434  6.151961    1.8393004   7.966868    8.919258    6.2695036
  4.6195264   0.72402155  6.516158    4.5063744   6.651485    8.823901
 10.0223675   8.378911    0.25785983  3.0188398   7.1746483   2.8368304
  5.7315655   1.0176957   2.865625    0.34789518  8.293505    2.1613307
  3.3841972   9.323753    7.32612    -0.01674449  0.4303813   7.508686
  5.7885385   2.6548457   8.486073    6.1425443   5.8022866   6.5669823
  7.4040923   3.0671625   3.9712758   2.9062366 ]
Epoch 1/1000
2023-10-30 15:51:02.575 
Epoch 1/1000 
	 loss: 110.8186, MinusLogProbMetric: 110.8186, val_loss: 109.1193, val_MinusLogProbMetric: 109.1193

Epoch 1: val_loss improved from inf to 109.11929, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 172s - loss: 110.8186 - MinusLogProbMetric: 110.8186 - val_loss: 109.1193 - val_MinusLogProbMetric: 109.1193 - lr: 4.1152e-06 - 172s/epoch - 880ms/step
Epoch 2/1000
2023-10-30 15:51:54.811 
Epoch 2/1000 
	 loss: 108.4720, MinusLogProbMetric: 108.4720, val_loss: 108.5533, val_MinusLogProbMetric: 108.5533

Epoch 2: val_loss improved from 109.11929 to 108.55330, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 52s - loss: 108.4720 - MinusLogProbMetric: 108.4720 - val_loss: 108.5533 - val_MinusLogProbMetric: 108.5533 - lr: 4.1152e-06 - 52s/epoch - 265ms/step
Epoch 3/1000
2023-10-30 15:52:44.356 
Epoch 3/1000 
	 loss: 105.5087, MinusLogProbMetric: 105.5087, val_loss: 104.0109, val_MinusLogProbMetric: 104.0109

Epoch 3: val_loss improved from 108.55330 to 104.01090, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 105.5087 - MinusLogProbMetric: 105.5087 - val_loss: 104.0109 - val_MinusLogProbMetric: 104.0109 - lr: 4.1152e-06 - 50s/epoch - 253ms/step
Epoch 4/1000
2023-10-30 15:53:34.128 
Epoch 4/1000 
	 loss: 103.4987, MinusLogProbMetric: 103.4987, val_loss: 102.4123, val_MinusLogProbMetric: 102.4123

Epoch 4: val_loss improved from 104.01090 to 102.41232, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 103.4987 - MinusLogProbMetric: 103.4987 - val_loss: 102.4123 - val_MinusLogProbMetric: 102.4123 - lr: 4.1152e-06 - 50s/epoch - 254ms/step
Epoch 5/1000
2023-10-30 15:54:27.958 
Epoch 5/1000 
	 loss: 101.2450, MinusLogProbMetric: 101.2450, val_loss: 100.0692, val_MinusLogProbMetric: 100.0692

Epoch 5: val_loss improved from 102.41232 to 100.06920, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 101.2450 - MinusLogProbMetric: 101.2450 - val_loss: 100.0692 - val_MinusLogProbMetric: 100.0692 - lr: 4.1152e-06 - 54s/epoch - 275ms/step
Epoch 6/1000
2023-10-30 15:55:18.048 
Epoch 6/1000 
	 loss: 99.8778, MinusLogProbMetric: 99.8778, val_loss: 99.9037, val_MinusLogProbMetric: 99.9037

Epoch 6: val_loss improved from 100.06920 to 99.90370, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 99.8778 - MinusLogProbMetric: 99.8778 - val_loss: 99.9037 - val_MinusLogProbMetric: 99.9037 - lr: 4.1152e-06 - 50s/epoch - 255ms/step
Epoch 7/1000
2023-10-30 15:56:07.929 
Epoch 7/1000 
	 loss: 99.1544, MinusLogProbMetric: 99.1544, val_loss: 98.9860, val_MinusLogProbMetric: 98.9860

Epoch 7: val_loss improved from 99.90370 to 98.98603, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 99.1544 - MinusLogProbMetric: 99.1544 - val_loss: 98.9860 - val_MinusLogProbMetric: 98.9860 - lr: 4.1152e-06 - 50s/epoch - 255ms/step
Epoch 8/1000
2023-10-30 15:56:56.873 
Epoch 8/1000 
	 loss: 97.5615, MinusLogProbMetric: 97.5615, val_loss: 95.9270, val_MinusLogProbMetric: 95.9270

Epoch 8: val_loss improved from 98.98603 to 95.92704, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 49s - loss: 97.5615 - MinusLogProbMetric: 97.5615 - val_loss: 95.9270 - val_MinusLogProbMetric: 95.9270 - lr: 4.1152e-06 - 49s/epoch - 249ms/step
Epoch 9/1000
2023-10-30 15:57:45.811 
Epoch 9/1000 
	 loss: 95.8479, MinusLogProbMetric: 95.8479, val_loss: 94.6098, val_MinusLogProbMetric: 94.6098

Epoch 9: val_loss improved from 95.92704 to 94.60976, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 49s - loss: 95.8479 - MinusLogProbMetric: 95.8479 - val_loss: 94.6098 - val_MinusLogProbMetric: 94.6098 - lr: 4.1152e-06 - 49s/epoch - 250ms/step
Epoch 10/1000
2023-10-30 15:58:37.203 
Epoch 10/1000 
	 loss: 93.9179, MinusLogProbMetric: 93.9179, val_loss: 93.1838, val_MinusLogProbMetric: 93.1838

Epoch 10: val_loss improved from 94.60976 to 93.18377, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 51s - loss: 93.9179 - MinusLogProbMetric: 93.9179 - val_loss: 93.1838 - val_MinusLogProbMetric: 93.1838 - lr: 4.1152e-06 - 51s/epoch - 262ms/step
Epoch 11/1000
2023-10-30 15:59:25.939 
Epoch 11/1000 
	 loss: 99.0227, MinusLogProbMetric: 99.0227, val_loss: 95.3114, val_MinusLogProbMetric: 95.3114

Epoch 11: val_loss did not improve from 93.18377
196/196 - 48s - loss: 99.0227 - MinusLogProbMetric: 99.0227 - val_loss: 95.3114 - val_MinusLogProbMetric: 95.3114 - lr: 4.1152e-06 - 48s/epoch - 245ms/step
Epoch 12/1000
2023-10-30 16:00:13.610 
Epoch 12/1000 
	 loss: 93.8057, MinusLogProbMetric: 93.8057, val_loss: 92.4989, val_MinusLogProbMetric: 92.4989

Epoch 12: val_loss improved from 93.18377 to 92.49888, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 48s - loss: 93.8057 - MinusLogProbMetric: 93.8057 - val_loss: 92.4989 - val_MinusLogProbMetric: 92.4989 - lr: 4.1152e-06 - 48s/epoch - 247ms/step
Epoch 13/1000
2023-10-30 16:01:03.463 
Epoch 13/1000 
	 loss: 92.3220, MinusLogProbMetric: 92.3220, val_loss: 92.3719, val_MinusLogProbMetric: 92.3719

Epoch 13: val_loss improved from 92.49888 to 92.37193, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 92.3220 - MinusLogProbMetric: 92.3220 - val_loss: 92.3719 - val_MinusLogProbMetric: 92.3719 - lr: 4.1152e-06 - 50s/epoch - 255ms/step
Epoch 14/1000
2023-10-30 16:01:52.992 
Epoch 14/1000 
	 loss: 91.5261, MinusLogProbMetric: 91.5261, val_loss: 94.8071, val_MinusLogProbMetric: 94.8071

Epoch 14: val_loss did not improve from 92.37193
196/196 - 49s - loss: 91.5261 - MinusLogProbMetric: 91.5261 - val_loss: 94.8071 - val_MinusLogProbMetric: 94.8071 - lr: 4.1152e-06 - 49s/epoch - 249ms/step
Epoch 15/1000
2023-10-30 16:02:41.985 
Epoch 15/1000 
	 loss: 90.2080, MinusLogProbMetric: 90.2080, val_loss: 89.1728, val_MinusLogProbMetric: 89.1728

Epoch 15: val_loss improved from 92.37193 to 89.17277, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 90.2080 - MinusLogProbMetric: 90.2080 - val_loss: 89.1728 - val_MinusLogProbMetric: 89.1728 - lr: 4.1152e-06 - 50s/epoch - 254ms/step
Epoch 16/1000
2023-10-30 16:03:31.213 
Epoch 16/1000 
	 loss: 88.8929, MinusLogProbMetric: 88.8929, val_loss: 88.2204, val_MinusLogProbMetric: 88.2204

Epoch 16: val_loss improved from 89.17277 to 88.22038, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 49s - loss: 88.8929 - MinusLogProbMetric: 88.8929 - val_loss: 88.2204 - val_MinusLogProbMetric: 88.2204 - lr: 4.1152e-06 - 49s/epoch - 251ms/step
Epoch 17/1000
2023-10-30 16:04:20.265 
Epoch 17/1000 
	 loss: 89.1744, MinusLogProbMetric: 89.1744, val_loss: 89.4559, val_MinusLogProbMetric: 89.4559

Epoch 17: val_loss did not improve from 88.22038
196/196 - 48s - loss: 89.1744 - MinusLogProbMetric: 89.1744 - val_loss: 89.4559 - val_MinusLogProbMetric: 89.4559 - lr: 4.1152e-06 - 48s/epoch - 246ms/step
Epoch 18/1000
2023-10-30 16:05:09.121 
Epoch 18/1000 
	 loss: 89.2447, MinusLogProbMetric: 89.2447, val_loss: 87.6715, val_MinusLogProbMetric: 87.6715

Epoch 18: val_loss improved from 88.22038 to 87.67148, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 89.2447 - MinusLogProbMetric: 89.2447 - val_loss: 87.6715 - val_MinusLogProbMetric: 87.6715 - lr: 4.1152e-06 - 50s/epoch - 253ms/step
Epoch 19/1000
2023-10-30 16:05:57.927 
Epoch 19/1000 
	 loss: 86.8613, MinusLogProbMetric: 86.8613, val_loss: 86.2892, val_MinusLogProbMetric: 86.2892

Epoch 19: val_loss improved from 87.67148 to 86.28922, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 49s - loss: 86.8613 - MinusLogProbMetric: 86.8613 - val_loss: 86.2892 - val_MinusLogProbMetric: 86.2892 - lr: 4.1152e-06 - 49s/epoch - 249ms/step
Epoch 20/1000
2023-10-30 16:06:46.490 
Epoch 20/1000 
	 loss: 86.0056, MinusLogProbMetric: 86.0056, val_loss: 85.3000, val_MinusLogProbMetric: 85.3000

Epoch 20: val_loss improved from 86.28922 to 85.30001, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 49s - loss: 86.0056 - MinusLogProbMetric: 86.0056 - val_loss: 85.3000 - val_MinusLogProbMetric: 85.3000 - lr: 4.1152e-06 - 49s/epoch - 248ms/step
Epoch 21/1000
2023-10-30 16:07:37.053 
Epoch 21/1000 
	 loss: 85.1084, MinusLogProbMetric: 85.1084, val_loss: 84.8536, val_MinusLogProbMetric: 84.8536

Epoch 21: val_loss improved from 85.30001 to 84.85355, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 85.1084 - MinusLogProbMetric: 85.1084 - val_loss: 84.8536 - val_MinusLogProbMetric: 84.8536 - lr: 4.1152e-06 - 50s/epoch - 258ms/step
Epoch 22/1000
2023-10-30 16:08:24.923 
Epoch 22/1000 
	 loss: 84.6199, MinusLogProbMetric: 84.6199, val_loss: 84.4414, val_MinusLogProbMetric: 84.4414

Epoch 22: val_loss improved from 84.85355 to 84.44141, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 48s - loss: 84.6199 - MinusLogProbMetric: 84.6199 - val_loss: 84.4414 - val_MinusLogProbMetric: 84.4414 - lr: 4.1152e-06 - 48s/epoch - 244ms/step
Epoch 23/1000
2023-10-30 16:09:13.780 
Epoch 23/1000 
	 loss: 85.2228, MinusLogProbMetric: 85.2228, val_loss: 85.2729, val_MinusLogProbMetric: 85.2729

Epoch 23: val_loss did not improve from 84.44141
196/196 - 48s - loss: 85.2228 - MinusLogProbMetric: 85.2228 - val_loss: 85.2729 - val_MinusLogProbMetric: 85.2729 - lr: 4.1152e-06 - 48s/epoch - 246ms/step
Epoch 24/1000
2023-10-30 16:10:03.986 
Epoch 24/1000 
	 loss: 85.0355, MinusLogProbMetric: 85.0355, val_loss: 83.8352, val_MinusLogProbMetric: 83.8352

Epoch 24: val_loss improved from 84.44141 to 83.83517, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 51s - loss: 85.0355 - MinusLogProbMetric: 85.0355 - val_loss: 83.8352 - val_MinusLogProbMetric: 83.8352 - lr: 4.1152e-06 - 51s/epoch - 260ms/step
Epoch 25/1000
2023-10-30 16:10:53.375 
Epoch 25/1000 
	 loss: 85.5848, MinusLogProbMetric: 85.5848, val_loss: 86.4483, val_MinusLogProbMetric: 86.4483

Epoch 25: val_loss did not improve from 83.83517
196/196 - 49s - loss: 85.5848 - MinusLogProbMetric: 85.5848 - val_loss: 86.4483 - val_MinusLogProbMetric: 86.4483 - lr: 4.1152e-06 - 49s/epoch - 248ms/step
Epoch 26/1000
2023-10-30 16:11:42.135 
Epoch 26/1000 
	 loss: 83.8070, MinusLogProbMetric: 83.8070, val_loss: 83.1515, val_MinusLogProbMetric: 83.1515

Epoch 26: val_loss improved from 83.83517 to 83.15146, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 50s - loss: 83.8070 - MinusLogProbMetric: 83.8070 - val_loss: 83.1515 - val_MinusLogProbMetric: 83.1515 - lr: 4.1152e-06 - 50s/epoch - 253ms/step
Epoch 27/1000
2023-10-30 16:12:32.456 
Epoch 27/1000 
	 loss: 106.8931, MinusLogProbMetric: 106.8931, val_loss: 131.8102, val_MinusLogProbMetric: 131.8102

Epoch 27: val_loss did not improve from 83.15146
196/196 - 50s - loss: 106.8931 - MinusLogProbMetric: 106.8931 - val_loss: 131.8102 - val_MinusLogProbMetric: 131.8102 - lr: 4.1152e-06 - 50s/epoch - 253ms/step
Epoch 28/1000
2023-10-30 16:13:23.125 
Epoch 28/1000 
	 loss: 113.2182, MinusLogProbMetric: 113.2182, val_loss: 103.3864, val_MinusLogProbMetric: 103.3864

Epoch 28: val_loss did not improve from 83.15146
196/196 - 51s - loss: 113.2182 - MinusLogProbMetric: 113.2182 - val_loss: 103.3864 - val_MinusLogProbMetric: 103.3864 - lr: 4.1152e-06 - 51s/epoch - 258ms/step
Epoch 29/1000
2023-10-30 16:14:13.104 
Epoch 29/1000 
	 loss: 109.9927, MinusLogProbMetric: 109.9927, val_loss: 101.0596, val_MinusLogProbMetric: 101.0596

Epoch 29: val_loss did not improve from 83.15146
196/196 - 50s - loss: 109.9927 - MinusLogProbMetric: 109.9927 - val_loss: 101.0596 - val_MinusLogProbMetric: 101.0596 - lr: 4.1152e-06 - 50s/epoch - 255ms/step
Epoch 30/1000
2023-10-30 16:15:01.561 
Epoch 30/1000 
	 loss: 96.1598, MinusLogProbMetric: 96.1598, val_loss: 93.9889, val_MinusLogProbMetric: 93.9889

Epoch 30: val_loss did not improve from 83.15146
196/196 - 48s - loss: 96.1598 - MinusLogProbMetric: 96.1598 - val_loss: 93.9889 - val_MinusLogProbMetric: 93.9889 - lr: 4.1152e-06 - 48s/epoch - 247ms/step
Epoch 31/1000
2023-10-30 16:15:51.103 
Epoch 31/1000 
	 loss: 93.1481, MinusLogProbMetric: 93.1481, val_loss: 91.4696, val_MinusLogProbMetric: 91.4696

Epoch 31: val_loss did not improve from 83.15146
196/196 - 50s - loss: 93.1481 - MinusLogProbMetric: 93.1481 - val_loss: 91.4696 - val_MinusLogProbMetric: 91.4696 - lr: 4.1152e-06 - 50s/epoch - 253ms/step
Epoch 32/1000
2023-10-30 16:16:40.909 
Epoch 32/1000 
	 loss: 90.7166, MinusLogProbMetric: 90.7166, val_loss: 89.4652, val_MinusLogProbMetric: 89.4652

Epoch 32: val_loss did not improve from 83.15146
196/196 - 50s - loss: 90.7166 - MinusLogProbMetric: 90.7166 - val_loss: 89.4652 - val_MinusLogProbMetric: 89.4652 - lr: 4.1152e-06 - 50s/epoch - 254ms/step
Epoch 33/1000
2023-10-30 16:17:30.086 
Epoch 33/1000 
	 loss: 88.7767, MinusLogProbMetric: 88.7767, val_loss: 88.2582, val_MinusLogProbMetric: 88.2582

Epoch 33: val_loss did not improve from 83.15146
196/196 - 49s - loss: 88.7767 - MinusLogProbMetric: 88.7767 - val_loss: 88.2582 - val_MinusLogProbMetric: 88.2582 - lr: 4.1152e-06 - 49s/epoch - 251ms/step
Epoch 34/1000
2023-10-30 16:18:17.928 
Epoch 34/1000 
	 loss: 141.7654, MinusLogProbMetric: 141.7654, val_loss: 145.7466, val_MinusLogProbMetric: 145.7466

Epoch 34: val_loss did not improve from 83.15146
196/196 - 48s - loss: 141.7654 - MinusLogProbMetric: 141.7654 - val_loss: 145.7466 - val_MinusLogProbMetric: 145.7466 - lr: 4.1152e-06 - 48s/epoch - 244ms/step
Epoch 35/1000
2023-10-30 16:19:15.812 
Epoch 35/1000 
	 loss: 132.6423, MinusLogProbMetric: 132.6423, val_loss: 123.1874, val_MinusLogProbMetric: 123.1874

Epoch 35: val_loss did not improve from 83.15146
196/196 - 58s - loss: 132.6423 - MinusLogProbMetric: 132.6423 - val_loss: 123.1874 - val_MinusLogProbMetric: 123.1874 - lr: 4.1152e-06 - 58s/epoch - 295ms/step
Epoch 36/1000
2023-10-30 16:20:04.384 
Epoch 36/1000 
	 loss: 122.2808, MinusLogProbMetric: 122.2808, val_loss: 139.9194, val_MinusLogProbMetric: 139.9194

Epoch 36: val_loss did not improve from 83.15146
196/196 - 49s - loss: 122.2808 - MinusLogProbMetric: 122.2808 - val_loss: 139.9194 - val_MinusLogProbMetric: 139.9194 - lr: 4.1152e-06 - 49s/epoch - 248ms/step
Epoch 37/1000
2023-10-30 16:20:58.243 
Epoch 37/1000 
	 loss: 116.9466, MinusLogProbMetric: 116.9466, val_loss: 111.2351, val_MinusLogProbMetric: 111.2351

Epoch 37: val_loss did not improve from 83.15146
196/196 - 54s - loss: 116.9466 - MinusLogProbMetric: 116.9466 - val_loss: 111.2351 - val_MinusLogProbMetric: 111.2351 - lr: 4.1152e-06 - 54s/epoch - 275ms/step
Epoch 38/1000
2023-10-30 16:21:53.959 
Epoch 38/1000 
	 loss: 109.1655, MinusLogProbMetric: 109.1655, val_loss: 107.5075, val_MinusLogProbMetric: 107.5075

Epoch 38: val_loss did not improve from 83.15146
196/196 - 56s - loss: 109.1655 - MinusLogProbMetric: 109.1655 - val_loss: 107.5075 - val_MinusLogProbMetric: 107.5075 - lr: 4.1152e-06 - 56s/epoch - 284ms/step
Epoch 39/1000
2023-10-30 16:22:42.890 
Epoch 39/1000 
	 loss: 106.5230, MinusLogProbMetric: 106.5230, val_loss: 105.0279, val_MinusLogProbMetric: 105.0279

Epoch 39: val_loss did not improve from 83.15146
196/196 - 49s - loss: 106.5230 - MinusLogProbMetric: 106.5230 - val_loss: 105.0279 - val_MinusLogProbMetric: 105.0279 - lr: 4.1152e-06 - 49s/epoch - 250ms/step
Epoch 40/1000
2023-10-30 16:23:34.290 
Epoch 40/1000 
	 loss: 105.7889, MinusLogProbMetric: 105.7889, val_loss: 110.1284, val_MinusLogProbMetric: 110.1284

Epoch 40: val_loss did not improve from 83.15146
196/196 - 51s - loss: 105.7889 - MinusLogProbMetric: 105.7889 - val_loss: 110.1284 - val_MinusLogProbMetric: 110.1284 - lr: 4.1152e-06 - 51s/epoch - 262ms/step
Epoch 41/1000
2023-10-30 16:24:33.730 
Epoch 41/1000 
	 loss: 102.9670, MinusLogProbMetric: 102.9670, val_loss: 101.2435, val_MinusLogProbMetric: 101.2435

Epoch 41: val_loss did not improve from 83.15146
196/196 - 59s - loss: 102.9670 - MinusLogProbMetric: 102.9670 - val_loss: 101.2435 - val_MinusLogProbMetric: 101.2435 - lr: 4.1152e-06 - 59s/epoch - 303ms/step
Epoch 42/1000
2023-10-30 16:25:34.727 
Epoch 42/1000 
	 loss: 100.4161, MinusLogProbMetric: 100.4161, val_loss: 99.2871, val_MinusLogProbMetric: 99.2871

Epoch 42: val_loss did not improve from 83.15146
196/196 - 61s - loss: 100.4161 - MinusLogProbMetric: 100.4161 - val_loss: 99.2871 - val_MinusLogProbMetric: 99.2871 - lr: 4.1152e-06 - 61s/epoch - 311ms/step
Epoch 43/1000
2023-10-30 16:26:36.699 
Epoch 43/1000 
	 loss: 98.5908, MinusLogProbMetric: 98.5908, val_loss: 97.7718, val_MinusLogProbMetric: 97.7718

Epoch 43: val_loss did not improve from 83.15146
196/196 - 62s - loss: 98.5908 - MinusLogProbMetric: 98.5908 - val_loss: 97.7718 - val_MinusLogProbMetric: 97.7718 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 44/1000
2023-10-30 16:27:31.495 
Epoch 44/1000 
	 loss: 97.3503, MinusLogProbMetric: 97.3503, val_loss: 96.7931, val_MinusLogProbMetric: 96.7931

Epoch 44: val_loss did not improve from 83.15146
196/196 - 55s - loss: 97.3503 - MinusLogProbMetric: 97.3503 - val_loss: 96.7931 - val_MinusLogProbMetric: 96.7931 - lr: 4.1152e-06 - 55s/epoch - 280ms/step
Epoch 45/1000
2023-10-30 16:28:34.058 
Epoch 45/1000 
	 loss: 96.2255, MinusLogProbMetric: 96.2255, val_loss: 95.5099, val_MinusLogProbMetric: 95.5099

Epoch 45: val_loss did not improve from 83.15146
196/196 - 63s - loss: 96.2255 - MinusLogProbMetric: 96.2255 - val_loss: 95.5099 - val_MinusLogProbMetric: 95.5099 - lr: 4.1152e-06 - 63s/epoch - 319ms/step
Epoch 46/1000
2023-10-30 16:29:34.609 
Epoch 46/1000 
	 loss: 95.0585, MinusLogProbMetric: 95.0585, val_loss: 94.5067, val_MinusLogProbMetric: 94.5067

Epoch 46: val_loss did not improve from 83.15146
196/196 - 61s - loss: 95.0585 - MinusLogProbMetric: 95.0585 - val_loss: 94.5067 - val_MinusLogProbMetric: 94.5067 - lr: 4.1152e-06 - 61s/epoch - 309ms/step
Epoch 47/1000
2023-10-30 16:30:35.393 
Epoch 47/1000 
	 loss: 93.9525, MinusLogProbMetric: 93.9525, val_loss: 93.1558, val_MinusLogProbMetric: 93.1558

Epoch 47: val_loss did not improve from 83.15146
196/196 - 61s - loss: 93.9525 - MinusLogProbMetric: 93.9525 - val_loss: 93.1558 - val_MinusLogProbMetric: 93.1558 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 48/1000
2023-10-30 16:31:35.325 
Epoch 48/1000 
	 loss: 92.6285, MinusLogProbMetric: 92.6285, val_loss: 92.1431, val_MinusLogProbMetric: 92.1431

Epoch 48: val_loss did not improve from 83.15146
196/196 - 60s - loss: 92.6285 - MinusLogProbMetric: 92.6285 - val_loss: 92.1431 - val_MinusLogProbMetric: 92.1431 - lr: 4.1152e-06 - 60s/epoch - 306ms/step
Epoch 49/1000
2023-10-30 16:32:34.701 
Epoch 49/1000 
	 loss: 91.8621, MinusLogProbMetric: 91.8621, val_loss: 91.3938, val_MinusLogProbMetric: 91.3938

Epoch 49: val_loss did not improve from 83.15146
196/196 - 59s - loss: 91.8621 - MinusLogProbMetric: 91.8621 - val_loss: 91.3938 - val_MinusLogProbMetric: 91.3938 - lr: 4.1152e-06 - 59s/epoch - 303ms/step
Epoch 50/1000
2023-10-30 16:33:35.059 
Epoch 50/1000 
	 loss: 90.9863, MinusLogProbMetric: 90.9863, val_loss: 90.5371, val_MinusLogProbMetric: 90.5371

Epoch 50: val_loss did not improve from 83.15146
196/196 - 60s - loss: 90.9863 - MinusLogProbMetric: 90.9863 - val_loss: 90.5371 - val_MinusLogProbMetric: 90.5371 - lr: 4.1152e-06 - 60s/epoch - 308ms/step
Epoch 51/1000
2023-10-30 16:34:32.746 
Epoch 51/1000 
	 loss: 90.3108, MinusLogProbMetric: 90.3108, val_loss: 89.9075, val_MinusLogProbMetric: 89.9075

Epoch 51: val_loss did not improve from 83.15146
196/196 - 58s - loss: 90.3108 - MinusLogProbMetric: 90.3108 - val_loss: 89.9075 - val_MinusLogProbMetric: 89.9075 - lr: 4.1152e-06 - 58s/epoch - 294ms/step
Epoch 52/1000
2023-10-30 16:35:26.421 
Epoch 52/1000 
	 loss: 89.6259, MinusLogProbMetric: 89.6259, val_loss: 89.4059, val_MinusLogProbMetric: 89.4059

Epoch 52: val_loss did not improve from 83.15146
196/196 - 54s - loss: 89.6259 - MinusLogProbMetric: 89.6259 - val_loss: 89.4059 - val_MinusLogProbMetric: 89.4059 - lr: 4.1152e-06 - 54s/epoch - 274ms/step
Epoch 53/1000
2023-10-30 16:36:20.929 
Epoch 53/1000 
	 loss: 89.0281, MinusLogProbMetric: 89.0281, val_loss: 89.0010, val_MinusLogProbMetric: 89.0010

Epoch 53: val_loss did not improve from 83.15146
196/196 - 55s - loss: 89.0281 - MinusLogProbMetric: 89.0281 - val_loss: 89.0010 - val_MinusLogProbMetric: 89.0010 - lr: 4.1152e-06 - 55s/epoch - 278ms/step
Epoch 54/1000
2023-10-30 16:37:11.894 
Epoch 54/1000 
	 loss: 88.5117, MinusLogProbMetric: 88.5117, val_loss: 88.7117, val_MinusLogProbMetric: 88.7117

Epoch 54: val_loss did not improve from 83.15146
196/196 - 51s - loss: 88.5117 - MinusLogProbMetric: 88.5117 - val_loss: 88.7117 - val_MinusLogProbMetric: 88.7117 - lr: 4.1152e-06 - 51s/epoch - 260ms/step
Epoch 55/1000
2023-10-30 16:38:05.728 
Epoch 55/1000 
	 loss: 87.9093, MinusLogProbMetric: 87.9093, val_loss: 87.6137, val_MinusLogProbMetric: 87.6137

Epoch 55: val_loss did not improve from 83.15146
196/196 - 54s - loss: 87.9093 - MinusLogProbMetric: 87.9093 - val_loss: 87.6137 - val_MinusLogProbMetric: 87.6137 - lr: 4.1152e-06 - 54s/epoch - 275ms/step
Epoch 56/1000
2023-10-30 16:39:01.014 
Epoch 56/1000 
	 loss: 87.2670, MinusLogProbMetric: 87.2670, val_loss: 87.0488, val_MinusLogProbMetric: 87.0488

Epoch 56: val_loss did not improve from 83.15146
196/196 - 55s - loss: 87.2670 - MinusLogProbMetric: 87.2670 - val_loss: 87.0488 - val_MinusLogProbMetric: 87.0488 - lr: 4.1152e-06 - 55s/epoch - 282ms/step
Epoch 57/1000
2023-10-30 16:39:58.462 
Epoch 57/1000 
	 loss: 86.8167, MinusLogProbMetric: 86.8167, val_loss: 86.4452, val_MinusLogProbMetric: 86.4452

Epoch 57: val_loss did not improve from 83.15146
196/196 - 57s - loss: 86.8167 - MinusLogProbMetric: 86.8167 - val_loss: 86.4452 - val_MinusLogProbMetric: 86.4452 - lr: 4.1152e-06 - 57s/epoch - 293ms/step
Epoch 58/1000
2023-10-30 16:40:52.399 
Epoch 58/1000 
	 loss: 86.1557, MinusLogProbMetric: 86.1557, val_loss: 86.1204, val_MinusLogProbMetric: 86.1204

Epoch 58: val_loss did not improve from 83.15146
196/196 - 54s - loss: 86.1557 - MinusLogProbMetric: 86.1557 - val_loss: 86.1204 - val_MinusLogProbMetric: 86.1204 - lr: 4.1152e-06 - 54s/epoch - 275ms/step
Epoch 59/1000
2023-10-30 16:41:51.376 
Epoch 59/1000 
	 loss: 85.6400, MinusLogProbMetric: 85.6400, val_loss: 85.3461, val_MinusLogProbMetric: 85.3461

Epoch 59: val_loss did not improve from 83.15146
196/196 - 59s - loss: 85.6400 - MinusLogProbMetric: 85.6400 - val_loss: 85.3461 - val_MinusLogProbMetric: 85.3461 - lr: 4.1152e-06 - 59s/epoch - 301ms/step
Epoch 60/1000
2023-10-30 16:42:50.813 
Epoch 60/1000 
	 loss: 85.2134, MinusLogProbMetric: 85.2134, val_loss: 85.0491, val_MinusLogProbMetric: 85.0491

Epoch 60: val_loss did not improve from 83.15146
196/196 - 59s - loss: 85.2134 - MinusLogProbMetric: 85.2134 - val_loss: 85.0491 - val_MinusLogProbMetric: 85.0491 - lr: 4.1152e-06 - 59s/epoch - 303ms/step
Epoch 61/1000
2023-10-30 16:43:48.740 
Epoch 61/1000 
	 loss: 84.8378, MinusLogProbMetric: 84.8378, val_loss: 84.4893, val_MinusLogProbMetric: 84.4893

Epoch 61: val_loss did not improve from 83.15146
196/196 - 58s - loss: 84.8378 - MinusLogProbMetric: 84.8378 - val_loss: 84.4893 - val_MinusLogProbMetric: 84.4893 - lr: 4.1152e-06 - 58s/epoch - 296ms/step
Epoch 62/1000
2023-10-30 16:44:46.188 
Epoch 62/1000 
	 loss: 84.2899, MinusLogProbMetric: 84.2899, val_loss: 83.9158, val_MinusLogProbMetric: 83.9158

Epoch 62: val_loss did not improve from 83.15146
196/196 - 57s - loss: 84.2899 - MinusLogProbMetric: 84.2899 - val_loss: 83.9158 - val_MinusLogProbMetric: 83.9158 - lr: 4.1152e-06 - 57s/epoch - 293ms/step
Epoch 63/1000
2023-10-30 16:45:40.992 
Epoch 63/1000 
	 loss: 83.6017, MinusLogProbMetric: 83.6017, val_loss: 83.2924, val_MinusLogProbMetric: 83.2924

Epoch 63: val_loss did not improve from 83.15146
196/196 - 55s - loss: 83.6017 - MinusLogProbMetric: 83.6017 - val_loss: 83.2924 - val_MinusLogProbMetric: 83.2924 - lr: 4.1152e-06 - 55s/epoch - 280ms/step
Epoch 64/1000
2023-10-30 16:46:37.100 
Epoch 64/1000 
	 loss: 83.0607, MinusLogProbMetric: 83.0607, val_loss: 82.8369, val_MinusLogProbMetric: 82.8369

Epoch 64: val_loss improved from 83.15146 to 82.83692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 83.0607 - MinusLogProbMetric: 83.0607 - val_loss: 82.8369 - val_MinusLogProbMetric: 82.8369 - lr: 4.1152e-06 - 57s/epoch - 291ms/step
Epoch 65/1000
2023-10-30 16:47:38.546 
Epoch 65/1000 
	 loss: 82.5882, MinusLogProbMetric: 82.5882, val_loss: 82.5862, val_MinusLogProbMetric: 82.5862

Epoch 65: val_loss improved from 82.83692 to 82.58624, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 82.5882 - MinusLogProbMetric: 82.5882 - val_loss: 82.5862 - val_MinusLogProbMetric: 82.5862 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 66/1000
2023-10-30 16:48:40.753 
Epoch 66/1000 
	 loss: 83.3075, MinusLogProbMetric: 83.3075, val_loss: 82.9746, val_MinusLogProbMetric: 82.9746

Epoch 66: val_loss did not improve from 82.58624
196/196 - 61s - loss: 83.3075 - MinusLogProbMetric: 83.3075 - val_loss: 82.9746 - val_MinusLogProbMetric: 82.9746 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 67/1000
2023-10-30 16:49:41.565 
Epoch 67/1000 
	 loss: 82.1046, MinusLogProbMetric: 82.1046, val_loss: 81.8138, val_MinusLogProbMetric: 81.8138

Epoch 67: val_loss improved from 82.58624 to 81.81384, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 82.1046 - MinusLogProbMetric: 82.1046 - val_loss: 81.8138 - val_MinusLogProbMetric: 81.8138 - lr: 4.1152e-06 - 62s/epoch - 315ms/step
Epoch 68/1000
2023-10-30 16:50:42.567 
Epoch 68/1000 
	 loss: 81.5259, MinusLogProbMetric: 81.5259, val_loss: 81.3954, val_MinusLogProbMetric: 81.3954

Epoch 68: val_loss improved from 81.81384 to 81.39536, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 81.5259 - MinusLogProbMetric: 81.5259 - val_loss: 81.3954 - val_MinusLogProbMetric: 81.3954 - lr: 4.1152e-06 - 61s/epoch - 311ms/step
Epoch 69/1000
2023-10-30 16:51:44.286 
Epoch 69/1000 
	 loss: 81.9161, MinusLogProbMetric: 81.9161, val_loss: 81.0398, val_MinusLogProbMetric: 81.0398

Epoch 69: val_loss improved from 81.39536 to 81.03978, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 81.9161 - MinusLogProbMetric: 81.9161 - val_loss: 81.0398 - val_MinusLogProbMetric: 81.0398 - lr: 4.1152e-06 - 62s/epoch - 315ms/step
Epoch 70/1000
2023-10-30 16:52:43.065 
Epoch 70/1000 
	 loss: 80.6849, MinusLogProbMetric: 80.6849, val_loss: 80.6147, val_MinusLogProbMetric: 80.6147

Epoch 70: val_loss improved from 81.03978 to 80.61469, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 80.6849 - MinusLogProbMetric: 80.6849 - val_loss: 80.6147 - val_MinusLogProbMetric: 80.6147 - lr: 4.1152e-06 - 59s/epoch - 300ms/step
Epoch 71/1000
2023-10-30 16:53:37.780 
Epoch 71/1000 
	 loss: 80.4117, MinusLogProbMetric: 80.4117, val_loss: 80.3192, val_MinusLogProbMetric: 80.3192

Epoch 71: val_loss improved from 80.61469 to 80.31924, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 80.4117 - MinusLogProbMetric: 80.4117 - val_loss: 80.3192 - val_MinusLogProbMetric: 80.3192 - lr: 4.1152e-06 - 55s/epoch - 279ms/step
Epoch 72/1000
2023-10-30 16:54:34.818 
Epoch 72/1000 
	 loss: 79.9191, MinusLogProbMetric: 79.9191, val_loss: 79.9709, val_MinusLogProbMetric: 79.9709

Epoch 72: val_loss improved from 80.31924 to 79.97089, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 79.9191 - MinusLogProbMetric: 79.9191 - val_loss: 79.9709 - val_MinusLogProbMetric: 79.9709 - lr: 4.1152e-06 - 57s/epoch - 292ms/step
Epoch 73/1000
2023-10-30 16:55:35.950 
Epoch 73/1000 
	 loss: 79.6674, MinusLogProbMetric: 79.6674, val_loss: 79.6430, val_MinusLogProbMetric: 79.6430

Epoch 73: val_loss improved from 79.97089 to 79.64296, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 79.6674 - MinusLogProbMetric: 79.6674 - val_loss: 79.6430 - val_MinusLogProbMetric: 79.6430 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 74/1000
2023-10-30 16:56:38.486 
Epoch 74/1000 
	 loss: 79.2231, MinusLogProbMetric: 79.2231, val_loss: 79.2850, val_MinusLogProbMetric: 79.2850

Epoch 74: val_loss improved from 79.64296 to 79.28502, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 79.2231 - MinusLogProbMetric: 79.2231 - val_loss: 79.2850 - val_MinusLogProbMetric: 79.2850 - lr: 4.1152e-06 - 63s/epoch - 319ms/step
Epoch 75/1000
2023-10-30 16:57:39.195 
Epoch 75/1000 
	 loss: 78.8871, MinusLogProbMetric: 78.8871, val_loss: 78.9880, val_MinusLogProbMetric: 78.9880

Epoch 75: val_loss improved from 79.28502 to 78.98798, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 78.8871 - MinusLogProbMetric: 78.8871 - val_loss: 78.9880 - val_MinusLogProbMetric: 78.9880 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 76/1000
2023-10-30 16:58:40.518 
Epoch 76/1000 
	 loss: 78.5920, MinusLogProbMetric: 78.5920, val_loss: 78.6536, val_MinusLogProbMetric: 78.6536

Epoch 76: val_loss improved from 78.98798 to 78.65360, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 78.5920 - MinusLogProbMetric: 78.5920 - val_loss: 78.6536 - val_MinusLogProbMetric: 78.6536 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 77/1000
2023-10-30 16:59:42.694 
Epoch 77/1000 
	 loss: 78.4569, MinusLogProbMetric: 78.4569, val_loss: 78.5381, val_MinusLogProbMetric: 78.5381

Epoch 77: val_loss improved from 78.65360 to 78.53809, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 78.4569 - MinusLogProbMetric: 78.4569 - val_loss: 78.5381 - val_MinusLogProbMetric: 78.5381 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 78/1000
2023-10-30 17:00:44.924 
Epoch 78/1000 
	 loss: 78.0813, MinusLogProbMetric: 78.0813, val_loss: 78.2199, val_MinusLogProbMetric: 78.2199

Epoch 78: val_loss improved from 78.53809 to 78.21992, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 78.0813 - MinusLogProbMetric: 78.0813 - val_loss: 78.2199 - val_MinusLogProbMetric: 78.2199 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 79/1000
2023-10-30 17:01:47.002 
Epoch 79/1000 
	 loss: 77.8067, MinusLogProbMetric: 77.8067, val_loss: 77.8301, val_MinusLogProbMetric: 77.8301

Epoch 79: val_loss improved from 78.21992 to 77.83012, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 77.8067 - MinusLogProbMetric: 77.8067 - val_loss: 77.8301 - val_MinusLogProbMetric: 77.8301 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 80/1000
2023-10-30 17:02:49.072 
Epoch 80/1000 
	 loss: 78.4490, MinusLogProbMetric: 78.4490, val_loss: 78.1818, val_MinusLogProbMetric: 78.1818

Epoch 80: val_loss did not improve from 77.83012
196/196 - 61s - loss: 78.4490 - MinusLogProbMetric: 78.4490 - val_loss: 78.1818 - val_MinusLogProbMetric: 78.1818 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 81/1000
2023-10-30 17:03:50.167 
Epoch 81/1000 
	 loss: 78.2869, MinusLogProbMetric: 78.2869, val_loss: 78.3887, val_MinusLogProbMetric: 78.3887

Epoch 81: val_loss did not improve from 77.83012
196/196 - 61s - loss: 78.2869 - MinusLogProbMetric: 78.2869 - val_loss: 78.3887 - val_MinusLogProbMetric: 78.3887 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 82/1000
2023-10-30 17:04:50.455 
Epoch 82/1000 
	 loss: 78.4051, MinusLogProbMetric: 78.4051, val_loss: 81.4750, val_MinusLogProbMetric: 81.4750

Epoch 82: val_loss did not improve from 77.83012
196/196 - 60s - loss: 78.4051 - MinusLogProbMetric: 78.4051 - val_loss: 81.4750 - val_MinusLogProbMetric: 81.4750 - lr: 4.1152e-06 - 60s/epoch - 308ms/step
Epoch 83/1000
2023-10-30 17:05:53.125 
Epoch 83/1000 
	 loss: 78.7197, MinusLogProbMetric: 78.7197, val_loss: 78.3084, val_MinusLogProbMetric: 78.3084

Epoch 83: val_loss did not improve from 77.83012
196/196 - 63s - loss: 78.7197 - MinusLogProbMetric: 78.7197 - val_loss: 78.3084 - val_MinusLogProbMetric: 78.3084 - lr: 4.1152e-06 - 63s/epoch - 320ms/step
Epoch 84/1000
2023-10-30 17:06:53.949 
Epoch 84/1000 
	 loss: 77.9278, MinusLogProbMetric: 77.9278, val_loss: 77.8868, val_MinusLogProbMetric: 77.8868

Epoch 84: val_loss did not improve from 77.83012
196/196 - 61s - loss: 77.9278 - MinusLogProbMetric: 77.9278 - val_loss: 77.8868 - val_MinusLogProbMetric: 77.8868 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 85/1000
2023-10-30 17:07:54.932 
Epoch 85/1000 
	 loss: 77.4615, MinusLogProbMetric: 77.4615, val_loss: 77.4072, val_MinusLogProbMetric: 77.4072

Epoch 85: val_loss improved from 77.83012 to 77.40722, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 77.4615 - MinusLogProbMetric: 77.4615 - val_loss: 77.4072 - val_MinusLogProbMetric: 77.4072 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 86/1000
2023-10-30 17:08:58.190 
Epoch 86/1000 
	 loss: 77.0582, MinusLogProbMetric: 77.0582, val_loss: 77.1001, val_MinusLogProbMetric: 77.1001

Epoch 86: val_loss improved from 77.40722 to 77.10009, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 77.0582 - MinusLogProbMetric: 77.0582 - val_loss: 77.1001 - val_MinusLogProbMetric: 77.1001 - lr: 4.1152e-06 - 63s/epoch - 322ms/step
Epoch 87/1000
2023-10-30 17:09:59.463 
Epoch 87/1000 
	 loss: 77.6780, MinusLogProbMetric: 77.6780, val_loss: 76.8795, val_MinusLogProbMetric: 76.8795

Epoch 87: val_loss improved from 77.10009 to 76.87951, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 77.6780 - MinusLogProbMetric: 77.6780 - val_loss: 76.8795 - val_MinusLogProbMetric: 76.8795 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 88/1000
2023-10-30 17:11:02.341 
Epoch 88/1000 
	 loss: 76.4640, MinusLogProbMetric: 76.4640, val_loss: 76.5299, val_MinusLogProbMetric: 76.5299

Epoch 88: val_loss improved from 76.87951 to 76.52988, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 76.4640 - MinusLogProbMetric: 76.4640 - val_loss: 76.5299 - val_MinusLogProbMetric: 76.5299 - lr: 4.1152e-06 - 63s/epoch - 321ms/step
Epoch 89/1000
2023-10-30 17:11:59.281 
Epoch 89/1000 
	 loss: 77.2460, MinusLogProbMetric: 77.2460, val_loss: 77.5300, val_MinusLogProbMetric: 77.5300

Epoch 89: val_loss did not improve from 76.52988
196/196 - 56s - loss: 77.2460 - MinusLogProbMetric: 77.2460 - val_loss: 77.5300 - val_MinusLogProbMetric: 77.5300 - lr: 4.1152e-06 - 56s/epoch - 285ms/step
Epoch 90/1000
2023-10-30 17:12:57.389 
Epoch 90/1000 
	 loss: 76.7514, MinusLogProbMetric: 76.7514, val_loss: 76.7663, val_MinusLogProbMetric: 76.7663

Epoch 90: val_loss did not improve from 76.52988
196/196 - 58s - loss: 76.7514 - MinusLogProbMetric: 76.7514 - val_loss: 76.7663 - val_MinusLogProbMetric: 76.7663 - lr: 4.1152e-06 - 58s/epoch - 296ms/step
Epoch 91/1000
2023-10-30 17:13:58.135 
Epoch 91/1000 
	 loss: 76.1721, MinusLogProbMetric: 76.1721, val_loss: 76.2080, val_MinusLogProbMetric: 76.2080

Epoch 91: val_loss improved from 76.52988 to 76.20796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 76.1721 - MinusLogProbMetric: 76.1721 - val_loss: 76.2080 - val_MinusLogProbMetric: 76.2080 - lr: 4.1152e-06 - 62s/epoch - 314ms/step
Epoch 92/1000
2023-10-30 17:14:58.814 
Epoch 92/1000 
	 loss: 75.7694, MinusLogProbMetric: 75.7694, val_loss: 75.8438, val_MinusLogProbMetric: 75.8438

Epoch 92: val_loss improved from 76.20796 to 75.84377, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 75.7694 - MinusLogProbMetric: 75.7694 - val_loss: 75.8438 - val_MinusLogProbMetric: 75.8438 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 93/1000
2023-10-30 17:16:00.482 
Epoch 93/1000 
	 loss: 75.4763, MinusLogProbMetric: 75.4763, val_loss: 75.5221, val_MinusLogProbMetric: 75.5221

Epoch 93: val_loss improved from 75.84377 to 75.52211, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 75.4763 - MinusLogProbMetric: 75.4763 - val_loss: 75.5221 - val_MinusLogProbMetric: 75.5221 - lr: 4.1152e-06 - 62s/epoch - 315ms/step
Epoch 94/1000
2023-10-30 17:17:01.241 
Epoch 94/1000 
	 loss: 75.4542, MinusLogProbMetric: 75.4542, val_loss: 75.8343, val_MinusLogProbMetric: 75.8343

Epoch 94: val_loss did not improve from 75.52211
196/196 - 60s - loss: 75.4542 - MinusLogProbMetric: 75.4542 - val_loss: 75.8343 - val_MinusLogProbMetric: 75.8343 - lr: 4.1152e-06 - 60s/epoch - 305ms/step
Epoch 95/1000
2023-10-30 17:18:02.659 
Epoch 95/1000 
	 loss: 76.0878, MinusLogProbMetric: 76.0878, val_loss: 76.1070, val_MinusLogProbMetric: 76.1070

Epoch 95: val_loss did not improve from 75.52211
196/196 - 61s - loss: 76.0878 - MinusLogProbMetric: 76.0878 - val_loss: 76.1070 - val_MinusLogProbMetric: 76.1070 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 96/1000
2023-10-30 17:19:04.065 
Epoch 96/1000 
	 loss: 161.4371, MinusLogProbMetric: 161.4371, val_loss: 126.6909, val_MinusLogProbMetric: 126.6909

Epoch 96: val_loss did not improve from 75.52211
196/196 - 61s - loss: 161.4371 - MinusLogProbMetric: 161.4371 - val_loss: 126.6909 - val_MinusLogProbMetric: 126.6909 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 97/1000
2023-10-30 17:20:05.988 
Epoch 97/1000 
	 loss: 117.8059, MinusLogProbMetric: 117.8059, val_loss: 111.8164, val_MinusLogProbMetric: 111.8164

Epoch 97: val_loss did not improve from 75.52211
196/196 - 62s - loss: 117.8059 - MinusLogProbMetric: 117.8059 - val_loss: 111.8164 - val_MinusLogProbMetric: 111.8164 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 98/1000
2023-10-30 17:21:06.655 
Epoch 98/1000 
	 loss: 107.8623, MinusLogProbMetric: 107.8623, val_loss: 104.6208, val_MinusLogProbMetric: 104.6208

Epoch 98: val_loss did not improve from 75.52211
196/196 - 61s - loss: 107.8623 - MinusLogProbMetric: 107.8623 - val_loss: 104.6208 - val_MinusLogProbMetric: 104.6208 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 99/1000
2023-10-30 17:22:03.221 
Epoch 99/1000 
	 loss: 103.1109, MinusLogProbMetric: 103.1109, val_loss: 100.3936, val_MinusLogProbMetric: 100.3936

Epoch 99: val_loss did not improve from 75.52211
196/196 - 57s - loss: 103.1109 - MinusLogProbMetric: 103.1109 - val_loss: 100.3936 - val_MinusLogProbMetric: 100.3936 - lr: 4.1152e-06 - 57s/epoch - 289ms/step
Epoch 100/1000
2023-10-30 17:22:56.752 
Epoch 100/1000 
	 loss: 98.7535, MinusLogProbMetric: 98.7535, val_loss: 97.2407, val_MinusLogProbMetric: 97.2407

Epoch 100: val_loss did not improve from 75.52211
196/196 - 54s - loss: 98.7535 - MinusLogProbMetric: 98.7535 - val_loss: 97.2407 - val_MinusLogProbMetric: 97.2407 - lr: 4.1152e-06 - 54s/epoch - 273ms/step
Epoch 101/1000
2023-10-30 17:23:50.507 
Epoch 101/1000 
	 loss: 95.7075, MinusLogProbMetric: 95.7075, val_loss: 93.9379, val_MinusLogProbMetric: 93.9379

Epoch 101: val_loss did not improve from 75.52211
196/196 - 54s - loss: 95.7075 - MinusLogProbMetric: 95.7075 - val_loss: 93.9379 - val_MinusLogProbMetric: 93.9379 - lr: 4.1152e-06 - 54s/epoch - 274ms/step
Epoch 102/1000
2023-10-30 17:24:49.931 
Epoch 102/1000 
	 loss: 92.7361, MinusLogProbMetric: 92.7361, val_loss: 91.8037, val_MinusLogProbMetric: 91.8037

Epoch 102: val_loss did not improve from 75.52211
196/196 - 59s - loss: 92.7361 - MinusLogProbMetric: 92.7361 - val_loss: 91.8037 - val_MinusLogProbMetric: 91.8037 - lr: 4.1152e-06 - 59s/epoch - 303ms/step
Epoch 103/1000
2023-10-30 17:25:51.115 
Epoch 103/1000 
	 loss: 90.8981, MinusLogProbMetric: 90.8981, val_loss: 90.2955, val_MinusLogProbMetric: 90.2955

Epoch 103: val_loss did not improve from 75.52211
196/196 - 61s - loss: 90.8981 - MinusLogProbMetric: 90.8981 - val_loss: 90.2955 - val_MinusLogProbMetric: 90.2955 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 104/1000
2023-10-30 17:26:52.021 
Epoch 104/1000 
	 loss: 89.5150, MinusLogProbMetric: 89.5150, val_loss: 88.9755, val_MinusLogProbMetric: 88.9755

Epoch 104: val_loss did not improve from 75.52211
196/196 - 61s - loss: 89.5150 - MinusLogProbMetric: 89.5150 - val_loss: 88.9755 - val_MinusLogProbMetric: 88.9755 - lr: 4.1152e-06 - 61s/epoch - 311ms/step
Epoch 105/1000
2023-10-30 17:27:53.215 
Epoch 105/1000 
	 loss: 88.3906, MinusLogProbMetric: 88.3906, val_loss: 87.7581, val_MinusLogProbMetric: 87.7581

Epoch 105: val_loss did not improve from 75.52211
196/196 - 61s - loss: 88.3906 - MinusLogProbMetric: 88.3906 - val_loss: 87.7581 - val_MinusLogProbMetric: 87.7581 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 106/1000
2023-10-30 17:28:55.148 
Epoch 106/1000 
	 loss: 87.1153, MinusLogProbMetric: 87.1153, val_loss: 86.6828, val_MinusLogProbMetric: 86.6828

Epoch 106: val_loss did not improve from 75.52211
196/196 - 62s - loss: 87.1153 - MinusLogProbMetric: 87.1153 - val_loss: 86.6828 - val_MinusLogProbMetric: 86.6828 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 107/1000
2023-10-30 17:29:57.135 
Epoch 107/1000 
	 loss: 86.4019, MinusLogProbMetric: 86.4019, val_loss: 86.2564, val_MinusLogProbMetric: 86.2564

Epoch 107: val_loss did not improve from 75.52211
196/196 - 62s - loss: 86.4019 - MinusLogProbMetric: 86.4019 - val_loss: 86.2564 - val_MinusLogProbMetric: 86.2564 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 108/1000
2023-10-30 17:30:56.921 
Epoch 108/1000 
	 loss: 85.7996, MinusLogProbMetric: 85.7996, val_loss: 85.4106, val_MinusLogProbMetric: 85.4106

Epoch 108: val_loss did not improve from 75.52211
196/196 - 60s - loss: 85.7996 - MinusLogProbMetric: 85.7996 - val_loss: 85.4106 - val_MinusLogProbMetric: 85.4106 - lr: 4.1152e-06 - 60s/epoch - 305ms/step
Epoch 109/1000
2023-10-30 17:31:57.709 
Epoch 109/1000 
	 loss: 84.9189, MinusLogProbMetric: 84.9189, val_loss: 85.0786, val_MinusLogProbMetric: 85.0786

Epoch 109: val_loss did not improve from 75.52211
196/196 - 61s - loss: 84.9189 - MinusLogProbMetric: 84.9189 - val_loss: 85.0786 - val_MinusLogProbMetric: 85.0786 - lr: 4.1152e-06 - 61s/epoch - 310ms/step
Epoch 110/1000
2023-10-30 17:32:56.062 
Epoch 110/1000 
	 loss: 84.3122, MinusLogProbMetric: 84.3122, val_loss: 84.0116, val_MinusLogProbMetric: 84.0116

Epoch 110: val_loss did not improve from 75.52211
196/196 - 58s - loss: 84.3122 - MinusLogProbMetric: 84.3122 - val_loss: 84.0116 - val_MinusLogProbMetric: 84.0116 - lr: 4.1152e-06 - 58s/epoch - 298ms/step
Epoch 111/1000
2023-10-30 17:33:54.201 
Epoch 111/1000 
	 loss: 83.9908, MinusLogProbMetric: 83.9908, val_loss: 83.8516, val_MinusLogProbMetric: 83.8516

Epoch 111: val_loss did not improve from 75.52211
196/196 - 58s - loss: 83.9908 - MinusLogProbMetric: 83.9908 - val_loss: 83.8516 - val_MinusLogProbMetric: 83.8516 - lr: 4.1152e-06 - 58s/epoch - 297ms/step
Epoch 112/1000
2023-10-30 17:34:48.138 
Epoch 112/1000 
	 loss: 83.3008, MinusLogProbMetric: 83.3008, val_loss: 83.0489, val_MinusLogProbMetric: 83.0489

Epoch 112: val_loss did not improve from 75.52211
196/196 - 54s - loss: 83.3008 - MinusLogProbMetric: 83.3008 - val_loss: 83.0489 - val_MinusLogProbMetric: 83.0489 - lr: 4.1152e-06 - 54s/epoch - 275ms/step
Epoch 113/1000
2023-10-30 17:35:43.725 
Epoch 113/1000 
	 loss: 82.7174, MinusLogProbMetric: 82.7174, val_loss: 82.5431, val_MinusLogProbMetric: 82.5431

Epoch 113: val_loss did not improve from 75.52211
196/196 - 56s - loss: 82.7174 - MinusLogProbMetric: 82.7174 - val_loss: 82.5431 - val_MinusLogProbMetric: 82.5431 - lr: 4.1152e-06 - 56s/epoch - 284ms/step
Epoch 114/1000
2023-10-30 17:36:43.310 
Epoch 114/1000 
	 loss: 82.2483, MinusLogProbMetric: 82.2483, val_loss: 82.0586, val_MinusLogProbMetric: 82.0586

Epoch 114: val_loss did not improve from 75.52211
196/196 - 60s - loss: 82.2483 - MinusLogProbMetric: 82.2483 - val_loss: 82.0586 - val_MinusLogProbMetric: 82.0586 - lr: 4.1152e-06 - 60s/epoch - 304ms/step
Epoch 115/1000
2023-10-30 17:37:45.043 
Epoch 115/1000 
	 loss: 81.7811, MinusLogProbMetric: 81.7811, val_loss: 81.7218, val_MinusLogProbMetric: 81.7218

Epoch 115: val_loss did not improve from 75.52211
196/196 - 62s - loss: 81.7811 - MinusLogProbMetric: 81.7811 - val_loss: 81.7218 - val_MinusLogProbMetric: 81.7218 - lr: 4.1152e-06 - 62s/epoch - 315ms/step
Epoch 116/1000
2023-10-30 17:38:47.024 
Epoch 116/1000 
	 loss: 81.4279, MinusLogProbMetric: 81.4279, val_loss: 81.2402, val_MinusLogProbMetric: 81.2402

Epoch 116: val_loss did not improve from 75.52211
196/196 - 62s - loss: 81.4279 - MinusLogProbMetric: 81.4279 - val_loss: 81.2402 - val_MinusLogProbMetric: 81.2402 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 117/1000
2023-10-30 17:39:48.267 
Epoch 117/1000 
	 loss: 81.0504, MinusLogProbMetric: 81.0504, val_loss: 80.9040, val_MinusLogProbMetric: 80.9040

Epoch 117: val_loss did not improve from 75.52211
196/196 - 61s - loss: 81.0504 - MinusLogProbMetric: 81.0504 - val_loss: 80.9040 - val_MinusLogProbMetric: 80.9040 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 118/1000
2023-10-30 17:40:50.360 
Epoch 118/1000 
	 loss: 99.7037, MinusLogProbMetric: 99.7037, val_loss: 94.6981, val_MinusLogProbMetric: 94.6981

Epoch 118: val_loss did not improve from 75.52211
196/196 - 62s - loss: 99.7037 - MinusLogProbMetric: 99.7037 - val_loss: 94.6981 - val_MinusLogProbMetric: 94.6981 - lr: 4.1152e-06 - 62s/epoch - 317ms/step
Epoch 119/1000
2023-10-30 17:41:50.800 
Epoch 119/1000 
	 loss: 89.9021, MinusLogProbMetric: 89.9021, val_loss: 87.4181, val_MinusLogProbMetric: 87.4181

Epoch 119: val_loss did not improve from 75.52211
196/196 - 60s - loss: 89.9021 - MinusLogProbMetric: 89.9021 - val_loss: 87.4181 - val_MinusLogProbMetric: 87.4181 - lr: 4.1152e-06 - 60s/epoch - 308ms/step
Epoch 120/1000
2023-10-30 17:42:52.436 
Epoch 120/1000 
	 loss: 86.3464, MinusLogProbMetric: 86.3464, val_loss: 85.2196, val_MinusLogProbMetric: 85.2196

Epoch 120: val_loss did not improve from 75.52211
196/196 - 62s - loss: 86.3464 - MinusLogProbMetric: 86.3464 - val_loss: 85.2196 - val_MinusLogProbMetric: 85.2196 - lr: 4.1152e-06 - 62s/epoch - 314ms/step
Epoch 121/1000
2023-10-30 17:43:48.447 
Epoch 121/1000 
	 loss: 85.3343, MinusLogProbMetric: 85.3343, val_loss: 84.3609, val_MinusLogProbMetric: 84.3609

Epoch 121: val_loss did not improve from 75.52211
196/196 - 56s - loss: 85.3343 - MinusLogProbMetric: 85.3343 - val_loss: 84.3609 - val_MinusLogProbMetric: 84.3609 - lr: 4.1152e-06 - 56s/epoch - 286ms/step
Epoch 122/1000
2023-10-30 17:44:50.170 
Epoch 122/1000 
	 loss: 83.8733, MinusLogProbMetric: 83.8733, val_loss: 83.5265, val_MinusLogProbMetric: 83.5265

Epoch 122: val_loss did not improve from 75.52211
196/196 - 62s - loss: 83.8733 - MinusLogProbMetric: 83.8733 - val_loss: 83.5265 - val_MinusLogProbMetric: 83.5265 - lr: 4.1152e-06 - 62s/epoch - 315ms/step
Epoch 123/1000
2023-10-30 17:45:49.688 
Epoch 123/1000 
	 loss: 82.9768, MinusLogProbMetric: 82.9768, val_loss: 82.6719, val_MinusLogProbMetric: 82.6719

Epoch 123: val_loss did not improve from 75.52211
196/196 - 60s - loss: 82.9768 - MinusLogProbMetric: 82.9768 - val_loss: 82.6719 - val_MinusLogProbMetric: 82.6719 - lr: 4.1152e-06 - 60s/epoch - 304ms/step
Epoch 124/1000
2023-10-30 17:46:49.062 
Epoch 124/1000 
	 loss: 82.3439, MinusLogProbMetric: 82.3439, val_loss: 82.0967, val_MinusLogProbMetric: 82.0967

Epoch 124: val_loss did not improve from 75.52211
196/196 - 59s - loss: 82.3439 - MinusLogProbMetric: 82.3439 - val_loss: 82.0967 - val_MinusLogProbMetric: 82.0967 - lr: 4.1152e-06 - 59s/epoch - 303ms/step
Epoch 125/1000
2023-10-30 17:47:50.336 
Epoch 125/1000 
	 loss: 81.8432, MinusLogProbMetric: 81.8432, val_loss: 81.6579, val_MinusLogProbMetric: 81.6579

Epoch 125: val_loss did not improve from 75.52211
196/196 - 61s - loss: 81.8432 - MinusLogProbMetric: 81.8432 - val_loss: 81.6579 - val_MinusLogProbMetric: 81.6579 - lr: 4.1152e-06 - 61s/epoch - 313ms/step
Epoch 126/1000
2023-10-30 17:48:49.331 
Epoch 126/1000 
	 loss: 81.3896, MinusLogProbMetric: 81.3896, val_loss: 81.5395, val_MinusLogProbMetric: 81.5395

Epoch 126: val_loss did not improve from 75.52211
196/196 - 59s - loss: 81.3896 - MinusLogProbMetric: 81.3896 - val_loss: 81.5395 - val_MinusLogProbMetric: 81.5395 - lr: 4.1152e-06 - 59s/epoch - 301ms/step
Epoch 127/1000
2023-10-30 17:49:49.176 
Epoch 127/1000 
	 loss: 80.9879, MinusLogProbMetric: 80.9879, val_loss: 80.7903, val_MinusLogProbMetric: 80.7903

Epoch 127: val_loss did not improve from 75.52211
196/196 - 60s - loss: 80.9879 - MinusLogProbMetric: 80.9879 - val_loss: 80.7903 - val_MinusLogProbMetric: 80.7903 - lr: 4.1152e-06 - 60s/epoch - 305ms/step
Epoch 128/1000
2023-10-30 17:50:47.070 
Epoch 128/1000 
	 loss: 80.5136, MinusLogProbMetric: 80.5136, val_loss: 80.4450, val_MinusLogProbMetric: 80.4450

Epoch 128: val_loss did not improve from 75.52211
196/196 - 58s - loss: 80.5136 - MinusLogProbMetric: 80.5136 - val_loss: 80.4450 - val_MinusLogProbMetric: 80.4450 - lr: 4.1152e-06 - 58s/epoch - 295ms/step
Epoch 129/1000
2023-10-30 17:51:46.548 
Epoch 129/1000 
	 loss: 80.1900, MinusLogProbMetric: 80.1900, val_loss: 80.0945, val_MinusLogProbMetric: 80.0945

Epoch 129: val_loss did not improve from 75.52211
196/196 - 59s - loss: 80.1900 - MinusLogProbMetric: 80.1900 - val_loss: 80.0945 - val_MinusLogProbMetric: 80.0945 - lr: 4.1152e-06 - 59s/epoch - 303ms/step
Epoch 130/1000
2023-10-30 17:52:46.571 
Epoch 130/1000 
	 loss: 79.8910, MinusLogProbMetric: 79.8910, val_loss: 79.8219, val_MinusLogProbMetric: 79.8219

Epoch 130: val_loss did not improve from 75.52211
196/196 - 60s - loss: 79.8910 - MinusLogProbMetric: 79.8910 - val_loss: 79.8219 - val_MinusLogProbMetric: 79.8219 - lr: 4.1152e-06 - 60s/epoch - 306ms/step
Epoch 131/1000
2023-10-30 17:53:46.698 
Epoch 131/1000 
	 loss: 79.5208, MinusLogProbMetric: 79.5208, val_loss: 79.4372, val_MinusLogProbMetric: 79.4372

Epoch 131: val_loss did not improve from 75.52211
196/196 - 60s - loss: 79.5208 - MinusLogProbMetric: 79.5208 - val_loss: 79.4372 - val_MinusLogProbMetric: 79.4372 - lr: 4.1152e-06 - 60s/epoch - 307ms/step
Epoch 132/1000
2023-10-30 17:54:48.613 
Epoch 132/1000 
	 loss: 79.1805, MinusLogProbMetric: 79.1805, val_loss: 79.1727, val_MinusLogProbMetric: 79.1727

Epoch 132: val_loss did not improve from 75.52211
196/196 - 62s - loss: 79.1805 - MinusLogProbMetric: 79.1805 - val_loss: 79.1727 - val_MinusLogProbMetric: 79.1727 - lr: 4.1152e-06 - 62s/epoch - 316ms/step
Epoch 133/1000
2023-10-30 17:55:50.267 
Epoch 133/1000 
	 loss: 79.0482, MinusLogProbMetric: 79.0482, val_loss: 81.7922, val_MinusLogProbMetric: 81.7922

Epoch 133: val_loss did not improve from 75.52211
196/196 - 62s - loss: 79.0482 - MinusLogProbMetric: 79.0482 - val_loss: 81.7922 - val_MinusLogProbMetric: 81.7922 - lr: 4.1152e-06 - 62s/epoch - 315ms/step
Epoch 134/1000
2023-10-30 17:56:50.812 
Epoch 134/1000 
	 loss: 78.9253, MinusLogProbMetric: 78.9253, val_loss: 78.5965, val_MinusLogProbMetric: 78.5965

Epoch 134: val_loss did not improve from 75.52211
196/196 - 61s - loss: 78.9253 - MinusLogProbMetric: 78.9253 - val_loss: 78.5965 - val_MinusLogProbMetric: 78.5965 - lr: 4.1152e-06 - 61s/epoch - 309ms/step
Epoch 135/1000
2023-10-30 17:57:51.036 
Epoch 135/1000 
	 loss: 78.4699, MinusLogProbMetric: 78.4699, val_loss: 78.3743, val_MinusLogProbMetric: 78.3743

Epoch 135: val_loss did not improve from 75.52211
196/196 - 60s - loss: 78.4699 - MinusLogProbMetric: 78.4699 - val_loss: 78.3743 - val_MinusLogProbMetric: 78.3743 - lr: 4.1152e-06 - 60s/epoch - 307ms/step
Epoch 136/1000
2023-10-30 17:58:52.240 
Epoch 136/1000 
	 loss: 78.1592, MinusLogProbMetric: 78.1592, val_loss: 78.1342, val_MinusLogProbMetric: 78.1342

Epoch 136: val_loss did not improve from 75.52211
196/196 - 61s - loss: 78.1592 - MinusLogProbMetric: 78.1592 - val_loss: 78.1342 - val_MinusLogProbMetric: 78.1342 - lr: 4.1152e-06 - 61s/epoch - 312ms/step
Epoch 137/1000
2023-10-30 17:59:51.585 
Epoch 137/1000 
	 loss: 77.9579, MinusLogProbMetric: 77.9579, val_loss: 78.0644, val_MinusLogProbMetric: 78.0644

Epoch 137: val_loss did not improve from 75.52211
196/196 - 59s - loss: 77.9579 - MinusLogProbMetric: 77.9579 - val_loss: 78.0644 - val_MinusLogProbMetric: 78.0644 - lr: 4.1152e-06 - 59s/epoch - 303ms/step
Epoch 138/1000
2023-10-30 18:00:52.151 
Epoch 138/1000 
	 loss: 77.7284, MinusLogProbMetric: 77.7284, val_loss: 77.7391, val_MinusLogProbMetric: 77.7391

Epoch 138: val_loss did not improve from 75.52211
196/196 - 61s - loss: 77.7284 - MinusLogProbMetric: 77.7284 - val_loss: 77.7391 - val_MinusLogProbMetric: 77.7391 - lr: 4.1152e-06 - 61s/epoch - 309ms/step
Epoch 139/1000
2023-10-30 18:01:51.883 
Epoch 139/1000 
	 loss: 77.4926, MinusLogProbMetric: 77.4926, val_loss: 77.4574, val_MinusLogProbMetric: 77.4574

Epoch 139: val_loss did not improve from 75.52211
196/196 - 60s - loss: 77.4926 - MinusLogProbMetric: 77.4926 - val_loss: 77.4574 - val_MinusLogProbMetric: 77.4574 - lr: 4.1152e-06 - 60s/epoch - 305ms/step
Epoch 140/1000
2023-10-30 18:02:54.303 
Epoch 140/1000 
	 loss: 77.3109, MinusLogProbMetric: 77.3109, val_loss: 77.3433, val_MinusLogProbMetric: 77.3433

Epoch 140: val_loss did not improve from 75.52211
196/196 - 62s - loss: 77.3109 - MinusLogProbMetric: 77.3109 - val_loss: 77.3433 - val_MinusLogProbMetric: 77.3433 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 141/1000
2023-10-30 18:03:53.719 
Epoch 141/1000 
	 loss: 77.1600, MinusLogProbMetric: 77.1600, val_loss: 77.3692, val_MinusLogProbMetric: 77.3692

Epoch 141: val_loss did not improve from 75.52211
196/196 - 59s - loss: 77.1600 - MinusLogProbMetric: 77.1600 - val_loss: 77.3692 - val_MinusLogProbMetric: 77.3692 - lr: 4.1152e-06 - 59s/epoch - 303ms/step
Epoch 142/1000
2023-10-30 18:04:52.454 
Epoch 142/1000 
	 loss: 76.9240, MinusLogProbMetric: 76.9240, val_loss: 76.8354, val_MinusLogProbMetric: 76.8354

Epoch 142: val_loss did not improve from 75.52211
196/196 - 59s - loss: 76.9240 - MinusLogProbMetric: 76.9240 - val_loss: 76.8354 - val_MinusLogProbMetric: 76.8354 - lr: 4.1152e-06 - 59s/epoch - 300ms/step
Epoch 143/1000
2023-10-30 18:05:52.621 
Epoch 143/1000 
	 loss: 76.6699, MinusLogProbMetric: 76.6699, val_loss: 76.7686, val_MinusLogProbMetric: 76.7686

Epoch 143: val_loss did not improve from 75.52211
196/196 - 60s - loss: 76.6699 - MinusLogProbMetric: 76.6699 - val_loss: 76.7686 - val_MinusLogProbMetric: 76.7686 - lr: 4.1152e-06 - 60s/epoch - 307ms/step
Epoch 144/1000
2023-10-30 18:06:52.328 
Epoch 144/1000 
	 loss: 76.4836, MinusLogProbMetric: 76.4836, val_loss: 76.5279, val_MinusLogProbMetric: 76.5279

Epoch 144: val_loss did not improve from 75.52211
196/196 - 60s - loss: 76.4836 - MinusLogProbMetric: 76.4836 - val_loss: 76.5279 - val_MinusLogProbMetric: 76.5279 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 145/1000
2023-10-30 18:07:54.717 
Epoch 145/1000 
	 loss: 76.3642, MinusLogProbMetric: 76.3642, val_loss: 76.4456, val_MinusLogProbMetric: 76.4456

Epoch 145: val_loss did not improve from 75.52211
196/196 - 62s - loss: 76.3642 - MinusLogProbMetric: 76.3642 - val_loss: 76.4456 - val_MinusLogProbMetric: 76.4456 - lr: 2.0576e-06 - 62s/epoch - 318ms/step
Epoch 146/1000
2023-10-30 18:08:55.669 
Epoch 146/1000 
	 loss: 76.2841, MinusLogProbMetric: 76.2841, val_loss: 76.3290, val_MinusLogProbMetric: 76.3290

Epoch 146: val_loss did not improve from 75.52211
196/196 - 61s - loss: 76.2841 - MinusLogProbMetric: 76.2841 - val_loss: 76.3290 - val_MinusLogProbMetric: 76.3290 - lr: 2.0576e-06 - 61s/epoch - 311ms/step
Epoch 147/1000
2023-10-30 18:09:55.294 
Epoch 147/1000 
	 loss: 76.1902, MinusLogProbMetric: 76.1902, val_loss: 76.2670, val_MinusLogProbMetric: 76.2670

Epoch 147: val_loss did not improve from 75.52211
196/196 - 60s - loss: 76.1902 - MinusLogProbMetric: 76.1902 - val_loss: 76.2670 - val_MinusLogProbMetric: 76.2670 - lr: 2.0576e-06 - 60s/epoch - 304ms/step
Epoch 148/1000
2023-10-30 18:10:56.627 
Epoch 148/1000 
	 loss: 76.0807, MinusLogProbMetric: 76.0807, val_loss: 76.1952, val_MinusLogProbMetric: 76.1952

Epoch 148: val_loss did not improve from 75.52211
196/196 - 61s - loss: 76.0807 - MinusLogProbMetric: 76.0807 - val_loss: 76.1952 - val_MinusLogProbMetric: 76.1952 - lr: 2.0576e-06 - 61s/epoch - 313ms/step
Epoch 149/1000
2023-10-30 18:11:58.230 
Epoch 149/1000 
	 loss: 75.9871, MinusLogProbMetric: 75.9871, val_loss: 76.0982, val_MinusLogProbMetric: 76.0982

Epoch 149: val_loss did not improve from 75.52211
196/196 - 62s - loss: 75.9871 - MinusLogProbMetric: 75.9871 - val_loss: 76.0982 - val_MinusLogProbMetric: 76.0982 - lr: 2.0576e-06 - 62s/epoch - 314ms/step
Epoch 150/1000
2023-10-30 18:12:59.374 
Epoch 150/1000 
	 loss: 75.9530, MinusLogProbMetric: 75.9530, val_loss: 76.1058, val_MinusLogProbMetric: 76.1058

Epoch 150: val_loss did not improve from 75.52211
196/196 - 61s - loss: 75.9530 - MinusLogProbMetric: 75.9530 - val_loss: 76.1058 - val_MinusLogProbMetric: 76.1058 - lr: 2.0576e-06 - 61s/epoch - 312ms/step
Epoch 151/1000
2023-10-30 18:14:00.367 
Epoch 151/1000 
	 loss: 75.8564, MinusLogProbMetric: 75.8564, val_loss: 75.9699, val_MinusLogProbMetric: 75.9699

Epoch 151: val_loss did not improve from 75.52211
196/196 - 61s - loss: 75.8564 - MinusLogProbMetric: 75.8564 - val_loss: 75.9699 - val_MinusLogProbMetric: 75.9699 - lr: 2.0576e-06 - 61s/epoch - 311ms/step
Epoch 152/1000
2023-10-30 18:15:00.185 
Epoch 152/1000 
	 loss: 76.6108, MinusLogProbMetric: 76.6108, val_loss: 81.5820, val_MinusLogProbMetric: 81.5820

Epoch 152: val_loss did not improve from 75.52211
196/196 - 60s - loss: 76.6108 - MinusLogProbMetric: 76.6108 - val_loss: 81.5820 - val_MinusLogProbMetric: 81.5820 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 153/1000
2023-10-30 18:16:01.299 
Epoch 153/1000 
	 loss: 77.4047, MinusLogProbMetric: 77.4047, val_loss: 76.1964, val_MinusLogProbMetric: 76.1964

Epoch 153: val_loss did not improve from 75.52211
196/196 - 61s - loss: 77.4047 - MinusLogProbMetric: 77.4047 - val_loss: 76.1964 - val_MinusLogProbMetric: 76.1964 - lr: 2.0576e-06 - 61s/epoch - 312ms/step
Epoch 154/1000
2023-10-30 18:17:01.401 
Epoch 154/1000 
	 loss: 75.8638, MinusLogProbMetric: 75.8638, val_loss: 75.8969, val_MinusLogProbMetric: 75.8969

Epoch 154: val_loss did not improve from 75.52211
196/196 - 60s - loss: 75.8638 - MinusLogProbMetric: 75.8638 - val_loss: 75.8969 - val_MinusLogProbMetric: 75.8969 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 155/1000
2023-10-30 18:18:03.044 
Epoch 155/1000 
	 loss: 75.6722, MinusLogProbMetric: 75.6722, val_loss: 75.7081, val_MinusLogProbMetric: 75.7081

Epoch 155: val_loss did not improve from 75.52211
196/196 - 62s - loss: 75.6722 - MinusLogProbMetric: 75.6722 - val_loss: 75.7081 - val_MinusLogProbMetric: 75.7081 - lr: 2.0576e-06 - 62s/epoch - 314ms/step
Epoch 156/1000
2023-10-30 18:19:03.252 
Epoch 156/1000 
	 loss: 75.5315, MinusLogProbMetric: 75.5315, val_loss: 75.6194, val_MinusLogProbMetric: 75.6194

Epoch 156: val_loss did not improve from 75.52211
196/196 - 60s - loss: 75.5315 - MinusLogProbMetric: 75.5315 - val_loss: 75.6194 - val_MinusLogProbMetric: 75.6194 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 157/1000
2023-10-30 18:20:03.638 
Epoch 157/1000 
	 loss: 75.3617, MinusLogProbMetric: 75.3617, val_loss: 75.5022, val_MinusLogProbMetric: 75.5022

Epoch 157: val_loss improved from 75.52211 to 75.50216, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 75.3617 - MinusLogProbMetric: 75.3617 - val_loss: 75.5022 - val_MinusLogProbMetric: 75.5022 - lr: 2.0576e-06 - 61s/epoch - 313ms/step
Epoch 158/1000
2023-10-30 18:21:06.283 
Epoch 158/1000 
	 loss: 75.2685, MinusLogProbMetric: 75.2685, val_loss: 75.4023, val_MinusLogProbMetric: 75.4023

Epoch 158: val_loss improved from 75.50216 to 75.40225, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 75.2685 - MinusLogProbMetric: 75.2685 - val_loss: 75.4023 - val_MinusLogProbMetric: 75.4023 - lr: 2.0576e-06 - 63s/epoch - 320ms/step
Epoch 159/1000
2023-10-30 18:22:07.916 
Epoch 159/1000 
	 loss: 75.1769, MinusLogProbMetric: 75.1769, val_loss: 75.3005, val_MinusLogProbMetric: 75.3005

Epoch 159: val_loss improved from 75.40225 to 75.30049, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 75.1769 - MinusLogProbMetric: 75.1769 - val_loss: 75.3005 - val_MinusLogProbMetric: 75.3005 - lr: 2.0576e-06 - 62s/epoch - 314ms/step
Epoch 160/1000
2023-10-30 18:23:09.982 
Epoch 160/1000 
	 loss: 75.0887, MinusLogProbMetric: 75.0887, val_loss: 75.2211, val_MinusLogProbMetric: 75.2211

Epoch 160: val_loss improved from 75.30049 to 75.22114, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 75.0887 - MinusLogProbMetric: 75.0887 - val_loss: 75.2211 - val_MinusLogProbMetric: 75.2211 - lr: 2.0576e-06 - 62s/epoch - 316ms/step
Epoch 161/1000
2023-10-30 18:24:13.711 
Epoch 161/1000 
	 loss: 75.0020, MinusLogProbMetric: 75.0020, val_loss: 75.9578, val_MinusLogProbMetric: 75.9578

Epoch 161: val_loss did not improve from 75.22114
196/196 - 63s - loss: 75.0020 - MinusLogProbMetric: 75.0020 - val_loss: 75.9578 - val_MinusLogProbMetric: 75.9578 - lr: 2.0576e-06 - 63s/epoch - 321ms/step
Epoch 162/1000
2023-10-30 18:25:14.860 
Epoch 162/1000 
	 loss: 75.1599, MinusLogProbMetric: 75.1599, val_loss: 75.2003, val_MinusLogProbMetric: 75.2003

Epoch 162: val_loss improved from 75.22114 to 75.20031, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 75.1599 - MinusLogProbMetric: 75.1599 - val_loss: 75.2003 - val_MinusLogProbMetric: 75.2003 - lr: 2.0576e-06 - 62s/epoch - 316ms/step
Epoch 163/1000
2023-10-30 18:26:16.482 
Epoch 163/1000 
	 loss: 74.9515, MinusLogProbMetric: 74.9515, val_loss: 75.0757, val_MinusLogProbMetric: 75.0757

Epoch 163: val_loss improved from 75.20031 to 75.07571, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 74.9515 - MinusLogProbMetric: 74.9515 - val_loss: 75.0757 - val_MinusLogProbMetric: 75.0757 - lr: 2.0576e-06 - 62s/epoch - 315ms/step
Epoch 164/1000
2023-10-30 18:27:19.179 
Epoch 164/1000 
	 loss: 74.8304, MinusLogProbMetric: 74.8304, val_loss: 74.9905, val_MinusLogProbMetric: 74.9905

Epoch 164: val_loss improved from 75.07571 to 74.99051, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 74.8304 - MinusLogProbMetric: 74.8304 - val_loss: 74.9905 - val_MinusLogProbMetric: 74.9905 - lr: 2.0576e-06 - 63s/epoch - 320ms/step
Epoch 165/1000
2023-10-30 18:28:23.012 
Epoch 165/1000 
	 loss: 74.7485, MinusLogProbMetric: 74.7485, val_loss: 74.8972, val_MinusLogProbMetric: 74.8972

Epoch 165: val_loss improved from 74.99051 to 74.89716, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 64s - loss: 74.7485 - MinusLogProbMetric: 74.7485 - val_loss: 74.8972 - val_MinusLogProbMetric: 74.8972 - lr: 2.0576e-06 - 64s/epoch - 325ms/step
Epoch 166/1000
2023-10-30 18:29:25.229 
Epoch 166/1000 
	 loss: 74.6532, MinusLogProbMetric: 74.6532, val_loss: 74.7741, val_MinusLogProbMetric: 74.7741

Epoch 166: val_loss improved from 74.89716 to 74.77409, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 74.6532 - MinusLogProbMetric: 74.6532 - val_loss: 74.7741 - val_MinusLogProbMetric: 74.7741 - lr: 2.0576e-06 - 62s/epoch - 318ms/step
Epoch 167/1000
2023-10-30 18:30:24.997 
Epoch 167/1000 
	 loss: 74.5737, MinusLogProbMetric: 74.5737, val_loss: 74.6874, val_MinusLogProbMetric: 74.6874

Epoch 167: val_loss improved from 74.77409 to 74.68737, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 74.5737 - MinusLogProbMetric: 74.5737 - val_loss: 74.6874 - val_MinusLogProbMetric: 74.6874 - lr: 2.0576e-06 - 60s/epoch - 304ms/step
Epoch 168/1000
2023-10-30 18:31:27.875 
Epoch 168/1000 
	 loss: 74.4880, MinusLogProbMetric: 74.4880, val_loss: 74.6278, val_MinusLogProbMetric: 74.6278

Epoch 168: val_loss improved from 74.68737 to 74.62778, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 74.4880 - MinusLogProbMetric: 74.4880 - val_loss: 74.6278 - val_MinusLogProbMetric: 74.6278 - lr: 2.0576e-06 - 63s/epoch - 321ms/step
Epoch 169/1000
2023-10-30 18:32:24.183 
Epoch 169/1000 
	 loss: 74.4057, MinusLogProbMetric: 74.4057, val_loss: 74.5274, val_MinusLogProbMetric: 74.5274

Epoch 169: val_loss improved from 74.62778 to 74.52736, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 74.4057 - MinusLogProbMetric: 74.4057 - val_loss: 74.5274 - val_MinusLogProbMetric: 74.5274 - lr: 2.0576e-06 - 56s/epoch - 287ms/step
Epoch 170/1000
2023-10-30 18:33:19.697 
Epoch 170/1000 
	 loss: 74.3230, MinusLogProbMetric: 74.3230, val_loss: 74.4399, val_MinusLogProbMetric: 74.4399

Epoch 170: val_loss improved from 74.52736 to 74.43987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 74.3230 - MinusLogProbMetric: 74.3230 - val_loss: 74.4399 - val_MinusLogProbMetric: 74.4399 - lr: 2.0576e-06 - 56s/epoch - 283ms/step
Epoch 171/1000
2023-10-30 18:34:15.053 
Epoch 171/1000 
	 loss: 74.2513, MinusLogProbMetric: 74.2513, val_loss: 74.3831, val_MinusLogProbMetric: 74.3831

Epoch 171: val_loss improved from 74.43987 to 74.38307, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 74.2513 - MinusLogProbMetric: 74.2513 - val_loss: 74.3831 - val_MinusLogProbMetric: 74.3831 - lr: 2.0576e-06 - 55s/epoch - 282ms/step
Epoch 172/1000
2023-10-30 18:35:12.705 
Epoch 172/1000 
	 loss: 74.2256, MinusLogProbMetric: 74.2256, val_loss: 74.3114, val_MinusLogProbMetric: 74.3114

Epoch 172: val_loss improved from 74.38307 to 74.31144, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 74.2256 - MinusLogProbMetric: 74.2256 - val_loss: 74.3114 - val_MinusLogProbMetric: 74.3114 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 173/1000
2023-10-30 18:36:08.849 
Epoch 173/1000 
	 loss: 74.1006, MinusLogProbMetric: 74.1006, val_loss: 74.2223, val_MinusLogProbMetric: 74.2223

Epoch 173: val_loss improved from 74.31144 to 74.22229, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 74.1006 - MinusLogProbMetric: 74.1006 - val_loss: 74.2223 - val_MinusLogProbMetric: 74.2223 - lr: 2.0576e-06 - 56s/epoch - 286ms/step
Epoch 174/1000
2023-10-30 18:37:05.011 
Epoch 174/1000 
	 loss: 74.0205, MinusLogProbMetric: 74.0205, val_loss: 74.1746, val_MinusLogProbMetric: 74.1746

Epoch 174: val_loss improved from 74.22229 to 74.17457, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 74.0205 - MinusLogProbMetric: 74.0205 - val_loss: 74.1746 - val_MinusLogProbMetric: 74.1746 - lr: 2.0576e-06 - 56s/epoch - 287ms/step
Epoch 175/1000
2023-10-30 18:37:59.856 
Epoch 175/1000 
	 loss: 73.9382, MinusLogProbMetric: 73.9382, val_loss: 74.0776, val_MinusLogProbMetric: 74.0776

Epoch 175: val_loss improved from 74.17457 to 74.07761, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 73.9382 - MinusLogProbMetric: 73.9382 - val_loss: 74.0776 - val_MinusLogProbMetric: 74.0776 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 176/1000
2023-10-30 18:38:55.572 
Epoch 176/1000 
	 loss: 73.8721, MinusLogProbMetric: 73.8721, val_loss: 74.0262, val_MinusLogProbMetric: 74.0262

Epoch 176: val_loss improved from 74.07761 to 74.02616, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 73.8721 - MinusLogProbMetric: 73.8721 - val_loss: 74.0262 - val_MinusLogProbMetric: 74.0262 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 177/1000
2023-10-30 18:39:52.575 
Epoch 177/1000 
	 loss: 73.8396, MinusLogProbMetric: 73.8396, val_loss: 73.9524, val_MinusLogProbMetric: 73.9524

Epoch 177: val_loss improved from 74.02616 to 73.95242, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 73.8396 - MinusLogProbMetric: 73.8396 - val_loss: 73.9524 - val_MinusLogProbMetric: 73.9524 - lr: 2.0576e-06 - 57s/epoch - 290ms/step
Epoch 178/1000
2023-10-30 18:40:49.263 
Epoch 178/1000 
	 loss: 73.7439, MinusLogProbMetric: 73.7439, val_loss: 73.9249, val_MinusLogProbMetric: 73.9249

Epoch 178: val_loss improved from 73.95242 to 73.92487, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 73.7439 - MinusLogProbMetric: 73.7439 - val_loss: 73.9249 - val_MinusLogProbMetric: 73.9249 - lr: 2.0576e-06 - 57s/epoch - 289ms/step
Epoch 179/1000
2023-10-30 18:41:45.812 
Epoch 179/1000 
	 loss: 73.6810, MinusLogProbMetric: 73.6810, val_loss: 73.8151, val_MinusLogProbMetric: 73.8151

Epoch 179: val_loss improved from 73.92487 to 73.81514, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 73.6810 - MinusLogProbMetric: 73.6810 - val_loss: 73.8151 - val_MinusLogProbMetric: 73.8151 - lr: 2.0576e-06 - 57s/epoch - 289ms/step
Epoch 180/1000
2023-10-30 18:42:44.177 
Epoch 180/1000 
	 loss: 73.6074, MinusLogProbMetric: 73.6074, val_loss: 73.7378, val_MinusLogProbMetric: 73.7378

Epoch 180: val_loss improved from 73.81514 to 73.73779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 73.6074 - MinusLogProbMetric: 73.6074 - val_loss: 73.7378 - val_MinusLogProbMetric: 73.7378 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 181/1000
2023-10-30 18:43:40.317 
Epoch 181/1000 
	 loss: 73.5590, MinusLogProbMetric: 73.5590, val_loss: 73.6956, val_MinusLogProbMetric: 73.6956

Epoch 181: val_loss improved from 73.73779 to 73.69558, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 73.5590 - MinusLogProbMetric: 73.5590 - val_loss: 73.6956 - val_MinusLogProbMetric: 73.6956 - lr: 2.0576e-06 - 56s/epoch - 286ms/step
Epoch 182/1000
2023-10-30 18:44:35.833 
Epoch 182/1000 
	 loss: 73.4827, MinusLogProbMetric: 73.4827, val_loss: 73.6274, val_MinusLogProbMetric: 73.6274

Epoch 182: val_loss improved from 73.69558 to 73.62736, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 73.4827 - MinusLogProbMetric: 73.4827 - val_loss: 73.6274 - val_MinusLogProbMetric: 73.6274 - lr: 2.0576e-06 - 55s/epoch - 283ms/step
Epoch 183/1000
2023-10-30 18:45:30.616 
Epoch 183/1000 
	 loss: 73.4109, MinusLogProbMetric: 73.4109, val_loss: 73.5665, val_MinusLogProbMetric: 73.5665

Epoch 183: val_loss improved from 73.62736 to 73.56646, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 73.4109 - MinusLogProbMetric: 73.4109 - val_loss: 73.5665 - val_MinusLogProbMetric: 73.5665 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 184/1000
2023-10-30 18:46:25.254 
Epoch 184/1000 
	 loss: 73.3539, MinusLogProbMetric: 73.3539, val_loss: 73.4373, val_MinusLogProbMetric: 73.4373

Epoch 184: val_loss improved from 73.56646 to 73.43729, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 73.3539 - MinusLogProbMetric: 73.3539 - val_loss: 73.4373 - val_MinusLogProbMetric: 73.4373 - lr: 2.0576e-06 - 55s/epoch - 279ms/step
Epoch 185/1000
2023-10-30 18:47:22.708 
Epoch 185/1000 
	 loss: 73.2414, MinusLogProbMetric: 73.2414, val_loss: 73.3829, val_MinusLogProbMetric: 73.3829

Epoch 185: val_loss improved from 73.43729 to 73.38289, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 73.2414 - MinusLogProbMetric: 73.2414 - val_loss: 73.3829 - val_MinusLogProbMetric: 73.3829 - lr: 2.0576e-06 - 58s/epoch - 294ms/step
Epoch 186/1000
2023-10-30 18:48:19.098 
Epoch 186/1000 
	 loss: 73.1737, MinusLogProbMetric: 73.1737, val_loss: 73.3524, val_MinusLogProbMetric: 73.3524

Epoch 186: val_loss improved from 73.38289 to 73.35242, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 73.1737 - MinusLogProbMetric: 73.1737 - val_loss: 73.3524 - val_MinusLogProbMetric: 73.3524 - lr: 2.0576e-06 - 56s/epoch - 287ms/step
Epoch 187/1000
2023-10-30 18:49:15.883 
Epoch 187/1000 
	 loss: 73.1278, MinusLogProbMetric: 73.1278, val_loss: 73.3826, val_MinusLogProbMetric: 73.3826

Epoch 187: val_loss did not improve from 73.35242
196/196 - 56s - loss: 73.1278 - MinusLogProbMetric: 73.1278 - val_loss: 73.3826 - val_MinusLogProbMetric: 73.3826 - lr: 2.0576e-06 - 56s/epoch - 286ms/step
Epoch 188/1000
2023-10-30 18:50:13.201 
Epoch 188/1000 
	 loss: 73.1087, MinusLogProbMetric: 73.1087, val_loss: 73.2444, val_MinusLogProbMetric: 73.2444

Epoch 188: val_loss improved from 73.35242 to 73.24444, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 73.1087 - MinusLogProbMetric: 73.1087 - val_loss: 73.2444 - val_MinusLogProbMetric: 73.2444 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 189/1000
2023-10-30 18:51:12.879 
Epoch 189/1000 
	 loss: 73.0042, MinusLogProbMetric: 73.0042, val_loss: 73.1976, val_MinusLogProbMetric: 73.1976

Epoch 189: val_loss improved from 73.24444 to 73.19756, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 73.0042 - MinusLogProbMetric: 73.0042 - val_loss: 73.1976 - val_MinusLogProbMetric: 73.1976 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 190/1000
2023-10-30 18:52:14.745 
Epoch 190/1000 
	 loss: 72.9618, MinusLogProbMetric: 72.9618, val_loss: 73.0792, val_MinusLogProbMetric: 73.0792

Epoch 190: val_loss improved from 73.19756 to 73.07923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 72.9618 - MinusLogProbMetric: 72.9618 - val_loss: 73.0792 - val_MinusLogProbMetric: 73.0792 - lr: 2.0576e-06 - 61s/epoch - 314ms/step
Epoch 191/1000
2023-10-30 18:53:10.345 
Epoch 191/1000 
	 loss: 72.8680, MinusLogProbMetric: 72.8680, val_loss: 73.0208, val_MinusLogProbMetric: 73.0208

Epoch 191: val_loss improved from 73.07923 to 73.02078, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 72.8680 - MinusLogProbMetric: 72.8680 - val_loss: 73.0208 - val_MinusLogProbMetric: 73.0208 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 192/1000
2023-10-30 18:54:05.632 
Epoch 192/1000 
	 loss: 72.8079, MinusLogProbMetric: 72.8079, val_loss: 72.9380, val_MinusLogProbMetric: 72.9380

Epoch 192: val_loss improved from 73.02078 to 72.93800, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 72.8079 - MinusLogProbMetric: 72.8079 - val_loss: 72.9380 - val_MinusLogProbMetric: 72.9380 - lr: 2.0576e-06 - 55s/epoch - 282ms/step
Epoch 193/1000
2023-10-30 18:55:01.468 
Epoch 193/1000 
	 loss: 72.7085, MinusLogProbMetric: 72.7085, val_loss: 72.8491, val_MinusLogProbMetric: 72.8491

Epoch 193: val_loss improved from 72.93800 to 72.84909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 72.7085 - MinusLogProbMetric: 72.7085 - val_loss: 72.8491 - val_MinusLogProbMetric: 72.8491 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 194/1000
2023-10-30 18:55:57.047 
Epoch 194/1000 
	 loss: 72.6747, MinusLogProbMetric: 72.6747, val_loss: 72.8415, val_MinusLogProbMetric: 72.8415

Epoch 194: val_loss improved from 72.84909 to 72.84149, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 72.6747 - MinusLogProbMetric: 72.6747 - val_loss: 72.8415 - val_MinusLogProbMetric: 72.8415 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 195/1000
2023-10-30 18:56:52.023 
Epoch 195/1000 
	 loss: 72.5977, MinusLogProbMetric: 72.5977, val_loss: 72.7624, val_MinusLogProbMetric: 72.7624

Epoch 195: val_loss improved from 72.84149 to 72.76244, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 72.5977 - MinusLogProbMetric: 72.5977 - val_loss: 72.7624 - val_MinusLogProbMetric: 72.7624 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 196/1000
2023-10-30 18:57:47.696 
Epoch 196/1000 
	 loss: 72.5376, MinusLogProbMetric: 72.5376, val_loss: 72.6871, val_MinusLogProbMetric: 72.6871

Epoch 196: val_loss improved from 72.76244 to 72.68706, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 72.5376 - MinusLogProbMetric: 72.5376 - val_loss: 72.6871 - val_MinusLogProbMetric: 72.6871 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 197/1000
2023-10-30 18:58:43.984 
Epoch 197/1000 
	 loss: 72.4645, MinusLogProbMetric: 72.4645, val_loss: 72.6366, val_MinusLogProbMetric: 72.6366

Epoch 197: val_loss improved from 72.68706 to 72.63657, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 72.4645 - MinusLogProbMetric: 72.4645 - val_loss: 72.6366 - val_MinusLogProbMetric: 72.6366 - lr: 2.0576e-06 - 56s/epoch - 287ms/step
Epoch 198/1000
2023-10-30 18:59:38.782 
Epoch 198/1000 
	 loss: 72.4429, MinusLogProbMetric: 72.4429, val_loss: 72.5794, val_MinusLogProbMetric: 72.5794

Epoch 198: val_loss improved from 72.63657 to 72.57937, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 72.4429 - MinusLogProbMetric: 72.4429 - val_loss: 72.5794 - val_MinusLogProbMetric: 72.5794 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 199/1000
2023-10-30 19:00:37.378 
Epoch 199/1000 
	 loss: 72.3439, MinusLogProbMetric: 72.3439, val_loss: 72.4939, val_MinusLogProbMetric: 72.4939

Epoch 199: val_loss improved from 72.57937 to 72.49386, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 72.3439 - MinusLogProbMetric: 72.3439 - val_loss: 72.4939 - val_MinusLogProbMetric: 72.4939 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 200/1000
2023-10-30 19:01:32.967 
Epoch 200/1000 
	 loss: 72.2765, MinusLogProbMetric: 72.2765, val_loss: 72.4448, val_MinusLogProbMetric: 72.4448

Epoch 200: val_loss improved from 72.49386 to 72.44479, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 72.2765 - MinusLogProbMetric: 72.2765 - val_loss: 72.4448 - val_MinusLogProbMetric: 72.4448 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 201/1000
2023-10-30 19:02:34.112 
Epoch 201/1000 
	 loss: 72.2175, MinusLogProbMetric: 72.2175, val_loss: 72.3706, val_MinusLogProbMetric: 72.3706

Epoch 201: val_loss improved from 72.44479 to 72.37064, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 72.2175 - MinusLogProbMetric: 72.2175 - val_loss: 72.3706 - val_MinusLogProbMetric: 72.3706 - lr: 2.0576e-06 - 61s/epoch - 312ms/step
Epoch 202/1000
2023-10-30 19:03:29.673 
Epoch 202/1000 
	 loss: 72.1496, MinusLogProbMetric: 72.1496, val_loss: 72.3030, val_MinusLogProbMetric: 72.3030

Epoch 202: val_loss improved from 72.37064 to 72.30302, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 72.1496 - MinusLogProbMetric: 72.1496 - val_loss: 72.3030 - val_MinusLogProbMetric: 72.3030 - lr: 2.0576e-06 - 55s/epoch - 283ms/step
Epoch 203/1000
2023-10-30 19:04:25.508 
Epoch 203/1000 
	 loss: 72.0987, MinusLogProbMetric: 72.0987, val_loss: 72.2308, val_MinusLogProbMetric: 72.2308

Epoch 203: val_loss improved from 72.30302 to 72.23075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 72.0987 - MinusLogProbMetric: 72.0987 - val_loss: 72.2308 - val_MinusLogProbMetric: 72.2308 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 204/1000
2023-10-30 19:05:20.981 
Epoch 204/1000 
	 loss: 72.0240, MinusLogProbMetric: 72.0240, val_loss: 72.1941, val_MinusLogProbMetric: 72.1941

Epoch 204: val_loss improved from 72.23075 to 72.19411, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 72.0240 - MinusLogProbMetric: 72.0240 - val_loss: 72.1941 - val_MinusLogProbMetric: 72.1941 - lr: 2.0576e-06 - 55s/epoch - 283ms/step
Epoch 205/1000
2023-10-30 19:06:16.869 
Epoch 205/1000 
	 loss: 71.9868, MinusLogProbMetric: 71.9868, val_loss: 72.2043, val_MinusLogProbMetric: 72.2043

Epoch 205: val_loss did not improve from 72.19411
196/196 - 55s - loss: 71.9868 - MinusLogProbMetric: 71.9868 - val_loss: 72.2043 - val_MinusLogProbMetric: 72.2043 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 206/1000
2023-10-30 19:07:11.223 
Epoch 206/1000 
	 loss: 72.3305, MinusLogProbMetric: 72.3305, val_loss: 72.2292, val_MinusLogProbMetric: 72.2292

Epoch 206: val_loss did not improve from 72.19411
196/196 - 54s - loss: 72.3305 - MinusLogProbMetric: 72.3305 - val_loss: 72.2292 - val_MinusLogProbMetric: 72.2292 - lr: 2.0576e-06 - 54s/epoch - 277ms/step
Epoch 207/1000
2023-10-30 19:08:05.856 
Epoch 207/1000 
	 loss: 71.9991, MinusLogProbMetric: 71.9991, val_loss: 72.1313, val_MinusLogProbMetric: 72.1313

Epoch 207: val_loss improved from 72.19411 to 72.13126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 71.9991 - MinusLogProbMetric: 71.9991 - val_loss: 72.1313 - val_MinusLogProbMetric: 72.1313 - lr: 2.0576e-06 - 55s/epoch - 283ms/step
Epoch 208/1000
2023-10-30 19:09:01.030 
Epoch 208/1000 
	 loss: 71.9082, MinusLogProbMetric: 71.9082, val_loss: 72.0337, val_MinusLogProbMetric: 72.0337

Epoch 208: val_loss improved from 72.13126 to 72.03371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 71.9082 - MinusLogProbMetric: 71.9082 - val_loss: 72.0337 - val_MinusLogProbMetric: 72.0337 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 209/1000
2023-10-30 19:09:55.881 
Epoch 209/1000 
	 loss: 71.8207, MinusLogProbMetric: 71.8207, val_loss: 71.9632, val_MinusLogProbMetric: 71.9632

Epoch 209: val_loss improved from 72.03371 to 71.96318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 71.8207 - MinusLogProbMetric: 71.8207 - val_loss: 71.9632 - val_MinusLogProbMetric: 71.9632 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 210/1000
2023-10-30 19:10:50.917 
Epoch 210/1000 
	 loss: 71.7740, MinusLogProbMetric: 71.7740, val_loss: 71.9845, val_MinusLogProbMetric: 71.9845

Epoch 210: val_loss did not improve from 71.96318
196/196 - 54s - loss: 71.7740 - MinusLogProbMetric: 71.7740 - val_loss: 71.9845 - val_MinusLogProbMetric: 71.9845 - lr: 2.0576e-06 - 54s/epoch - 277ms/step
Epoch 211/1000
2023-10-30 19:11:48.085 
Epoch 211/1000 
	 loss: 71.6957, MinusLogProbMetric: 71.6957, val_loss: 71.8184, val_MinusLogProbMetric: 71.8184

Epoch 211: val_loss improved from 71.96318 to 71.81835, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 71.6957 - MinusLogProbMetric: 71.6957 - val_loss: 71.8184 - val_MinusLogProbMetric: 71.8184 - lr: 2.0576e-06 - 58s/epoch - 296ms/step
Epoch 212/1000
2023-10-30 19:12:44.264 
Epoch 212/1000 
	 loss: 71.6447, MinusLogProbMetric: 71.6447, val_loss: 71.7699, val_MinusLogProbMetric: 71.7699

Epoch 212: val_loss improved from 71.81835 to 71.76987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 71.6447 - MinusLogProbMetric: 71.6447 - val_loss: 71.7699 - val_MinusLogProbMetric: 71.7699 - lr: 2.0576e-06 - 56s/epoch - 286ms/step
Epoch 213/1000
2023-10-30 19:13:39.444 
Epoch 213/1000 
	 loss: 71.5749, MinusLogProbMetric: 71.5749, val_loss: 71.6995, val_MinusLogProbMetric: 71.6995

Epoch 213: val_loss improved from 71.76987 to 71.69952, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 71.5749 - MinusLogProbMetric: 71.5749 - val_loss: 71.6995 - val_MinusLogProbMetric: 71.6995 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 214/1000
2023-10-30 19:14:34.477 
Epoch 214/1000 
	 loss: 71.5252, MinusLogProbMetric: 71.5252, val_loss: 71.6318, val_MinusLogProbMetric: 71.6318

Epoch 214: val_loss improved from 71.69952 to 71.63180, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 71.5252 - MinusLogProbMetric: 71.5252 - val_loss: 71.6318 - val_MinusLogProbMetric: 71.6318 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 215/1000
2023-10-30 19:15:30.282 
Epoch 215/1000 
	 loss: 71.4629, MinusLogProbMetric: 71.4629, val_loss: 71.6154, val_MinusLogProbMetric: 71.6154

Epoch 215: val_loss improved from 71.63180 to 71.61536, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 71.4629 - MinusLogProbMetric: 71.4629 - val_loss: 71.6154 - val_MinusLogProbMetric: 71.6154 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 216/1000
2023-10-30 19:16:25.311 
Epoch 216/1000 
	 loss: 71.3996, MinusLogProbMetric: 71.3996, val_loss: 71.5365, val_MinusLogProbMetric: 71.5365

Epoch 216: val_loss improved from 71.61536 to 71.53646, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 71.3996 - MinusLogProbMetric: 71.3996 - val_loss: 71.5365 - val_MinusLogProbMetric: 71.5365 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 217/1000
2023-10-30 19:17:20.392 
Epoch 217/1000 
	 loss: 71.3266, MinusLogProbMetric: 71.3266, val_loss: 71.4574, val_MinusLogProbMetric: 71.4574

Epoch 217: val_loss improved from 71.53646 to 71.45743, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 71.3266 - MinusLogProbMetric: 71.3266 - val_loss: 71.4574 - val_MinusLogProbMetric: 71.4574 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 218/1000
2023-10-30 19:18:15.984 
Epoch 218/1000 
	 loss: 71.2714, MinusLogProbMetric: 71.2714, val_loss: 71.4095, val_MinusLogProbMetric: 71.4095

Epoch 218: val_loss improved from 71.45743 to 71.40950, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 71.2714 - MinusLogProbMetric: 71.2714 - val_loss: 71.4095 - val_MinusLogProbMetric: 71.4095 - lr: 2.0576e-06 - 56s/epoch - 283ms/step
Epoch 219/1000
2023-10-30 19:19:09.765 
Epoch 219/1000 
	 loss: 71.2754, MinusLogProbMetric: 71.2754, val_loss: 71.3454, val_MinusLogProbMetric: 71.3454

Epoch 219: val_loss improved from 71.40950 to 71.34544, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 71.2754 - MinusLogProbMetric: 71.2754 - val_loss: 71.3454 - val_MinusLogProbMetric: 71.3454 - lr: 2.0576e-06 - 54s/epoch - 274ms/step
Epoch 220/1000
2023-10-30 19:20:05.262 
Epoch 220/1000 
	 loss: 71.1400, MinusLogProbMetric: 71.1400, val_loss: 71.2615, val_MinusLogProbMetric: 71.2615

Epoch 220: val_loss improved from 71.34544 to 71.26153, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 71.1400 - MinusLogProbMetric: 71.1400 - val_loss: 71.2615 - val_MinusLogProbMetric: 71.2615 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 221/1000
2023-10-30 19:20:59.910 
Epoch 221/1000 
	 loss: 71.1086, MinusLogProbMetric: 71.1086, val_loss: 71.2080, val_MinusLogProbMetric: 71.2080

Epoch 221: val_loss improved from 71.26153 to 71.20796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 71.1086 - MinusLogProbMetric: 71.1086 - val_loss: 71.2080 - val_MinusLogProbMetric: 71.2080 - lr: 2.0576e-06 - 55s/epoch - 278ms/step
Epoch 222/1000
2023-10-30 19:21:57.988 
Epoch 222/1000 
	 loss: 71.0336, MinusLogProbMetric: 71.0336, val_loss: 71.1408, val_MinusLogProbMetric: 71.1408

Epoch 222: val_loss improved from 71.20796 to 71.14078, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 71.0336 - MinusLogProbMetric: 71.0336 - val_loss: 71.1408 - val_MinusLogProbMetric: 71.1408 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 223/1000
2023-10-30 19:22:51.369 
Epoch 223/1000 
	 loss: 71.1221, MinusLogProbMetric: 71.1221, val_loss: 71.2105, val_MinusLogProbMetric: 71.2105

Epoch 223: val_loss did not improve from 71.14078
196/196 - 52s - loss: 71.1221 - MinusLogProbMetric: 71.1221 - val_loss: 71.2105 - val_MinusLogProbMetric: 71.2105 - lr: 2.0576e-06 - 52s/epoch - 267ms/step
Epoch 224/1000
2023-10-30 19:23:46.417 
Epoch 224/1000 
	 loss: 70.9771, MinusLogProbMetric: 70.9771, val_loss: 71.1200, val_MinusLogProbMetric: 71.1200

Epoch 224: val_loss improved from 71.14078 to 71.12001, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 70.9771 - MinusLogProbMetric: 70.9771 - val_loss: 71.1200 - val_MinusLogProbMetric: 71.1200 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 225/1000
2023-10-30 19:24:46.079 
Epoch 225/1000 
	 loss: 70.9016, MinusLogProbMetric: 70.9016, val_loss: 71.0216, val_MinusLogProbMetric: 71.0216

Epoch 225: val_loss improved from 71.12001 to 71.02161, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 70.9016 - MinusLogProbMetric: 70.9016 - val_loss: 71.0216 - val_MinusLogProbMetric: 71.0216 - lr: 2.0576e-06 - 60s/epoch - 304ms/step
Epoch 226/1000
2023-10-30 19:25:46.195 
Epoch 226/1000 
	 loss: 70.8087, MinusLogProbMetric: 70.8087, val_loss: 70.9532, val_MinusLogProbMetric: 70.9532

Epoch 226: val_loss improved from 71.02161 to 70.95324, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 70.8087 - MinusLogProbMetric: 70.8087 - val_loss: 70.9532 - val_MinusLogProbMetric: 70.9532 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 227/1000
2023-10-30 19:26:41.114 
Epoch 227/1000 
	 loss: 70.7396, MinusLogProbMetric: 70.7396, val_loss: 70.9456, val_MinusLogProbMetric: 70.9456

Epoch 227: val_loss improved from 70.95324 to 70.94557, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 70.7396 - MinusLogProbMetric: 70.7396 - val_loss: 70.9456 - val_MinusLogProbMetric: 70.9456 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 228/1000
2023-10-30 19:27:37.832 
Epoch 228/1000 
	 loss: 70.7040, MinusLogProbMetric: 70.7040, val_loss: 71.0534, val_MinusLogProbMetric: 71.0534

Epoch 228: val_loss did not improve from 70.94557
196/196 - 56s - loss: 70.7040 - MinusLogProbMetric: 70.7040 - val_loss: 71.0534 - val_MinusLogProbMetric: 71.0534 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 229/1000
2023-10-30 19:28:32.445 
Epoch 229/1000 
	 loss: 70.6200, MinusLogProbMetric: 70.6200, val_loss: 70.7711, val_MinusLogProbMetric: 70.7711

Epoch 229: val_loss improved from 70.94557 to 70.77115, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 70.6200 - MinusLogProbMetric: 70.6200 - val_loss: 70.7711 - val_MinusLogProbMetric: 70.7711 - lr: 2.0576e-06 - 55s/epoch - 283ms/step
Epoch 230/1000
2023-10-30 19:29:28.608 
Epoch 230/1000 
	 loss: 70.5360, MinusLogProbMetric: 70.5360, val_loss: 70.6667, val_MinusLogProbMetric: 70.6667

Epoch 230: val_loss improved from 70.77115 to 70.66669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 70.5360 - MinusLogProbMetric: 70.5360 - val_loss: 70.6667 - val_MinusLogProbMetric: 70.6667 - lr: 2.0576e-06 - 56s/epoch - 287ms/step
Epoch 231/1000
2023-10-30 19:30:28.785 
Epoch 231/1000 
	 loss: 70.5952, MinusLogProbMetric: 70.5952, val_loss: 70.6606, val_MinusLogProbMetric: 70.6606

Epoch 231: val_loss improved from 70.66669 to 70.66056, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 70.5952 - MinusLogProbMetric: 70.5952 - val_loss: 70.6606 - val_MinusLogProbMetric: 70.6606 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 232/1000
2023-10-30 19:31:32.035 
Epoch 232/1000 
	 loss: 70.4216, MinusLogProbMetric: 70.4216, val_loss: 70.5457, val_MinusLogProbMetric: 70.5457

Epoch 232: val_loss improved from 70.66056 to 70.54572, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 70.4216 - MinusLogProbMetric: 70.4216 - val_loss: 70.5457 - val_MinusLogProbMetric: 70.5457 - lr: 2.0576e-06 - 63s/epoch - 322ms/step
Epoch 233/1000
2023-10-30 19:32:33.063 
Epoch 233/1000 
	 loss: 70.3683, MinusLogProbMetric: 70.3683, val_loss: 70.5026, val_MinusLogProbMetric: 70.5026

Epoch 233: val_loss improved from 70.54572 to 70.50265, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 70.3683 - MinusLogProbMetric: 70.3683 - val_loss: 70.5026 - val_MinusLogProbMetric: 70.5026 - lr: 2.0576e-06 - 61s/epoch - 312ms/step
Epoch 234/1000
2023-10-30 19:33:34.869 
Epoch 234/1000 
	 loss: 70.2889, MinusLogProbMetric: 70.2889, val_loss: 70.4459, val_MinusLogProbMetric: 70.4459

Epoch 234: val_loss improved from 70.50265 to 70.44591, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 70.2889 - MinusLogProbMetric: 70.2889 - val_loss: 70.4459 - val_MinusLogProbMetric: 70.4459 - lr: 2.0576e-06 - 62s/epoch - 315ms/step
Epoch 235/1000
2023-10-30 19:34:34.816 
Epoch 235/1000 
	 loss: 70.2367, MinusLogProbMetric: 70.2367, val_loss: 70.3712, val_MinusLogProbMetric: 70.3712

Epoch 235: val_loss improved from 70.44591 to 70.37122, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 70.2367 - MinusLogProbMetric: 70.2367 - val_loss: 70.3712 - val_MinusLogProbMetric: 70.3712 - lr: 2.0576e-06 - 60s/epoch - 306ms/step
Epoch 236/1000
2023-10-30 19:35:32.414 
Epoch 236/1000 
	 loss: 70.1906, MinusLogProbMetric: 70.1906, val_loss: 70.3006, val_MinusLogProbMetric: 70.3006

Epoch 236: val_loss improved from 70.37122 to 70.30055, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 70.1906 - MinusLogProbMetric: 70.1906 - val_loss: 70.3006 - val_MinusLogProbMetric: 70.3006 - lr: 2.0576e-06 - 58s/epoch - 294ms/step
Epoch 237/1000
2023-10-30 19:36:32.939 
Epoch 237/1000 
	 loss: 70.2365, MinusLogProbMetric: 70.2365, val_loss: 70.2551, val_MinusLogProbMetric: 70.2551

Epoch 237: val_loss improved from 70.30055 to 70.25511, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 70.2365 - MinusLogProbMetric: 70.2365 - val_loss: 70.2551 - val_MinusLogProbMetric: 70.2551 - lr: 2.0576e-06 - 61s/epoch - 309ms/step
Epoch 238/1000
2023-10-30 19:37:36.187 
Epoch 238/1000 
	 loss: 70.0723, MinusLogProbMetric: 70.0723, val_loss: 70.2119, val_MinusLogProbMetric: 70.2119

Epoch 238: val_loss improved from 70.25511 to 70.21194, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 70.0723 - MinusLogProbMetric: 70.0723 - val_loss: 70.2119 - val_MinusLogProbMetric: 70.2119 - lr: 2.0576e-06 - 63s/epoch - 322ms/step
Epoch 239/1000
2023-10-30 19:38:37.509 
Epoch 239/1000 
	 loss: 70.0431, MinusLogProbMetric: 70.0431, val_loss: 70.1425, val_MinusLogProbMetric: 70.1425

Epoch 239: val_loss improved from 70.21194 to 70.14252, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 70.0431 - MinusLogProbMetric: 70.0431 - val_loss: 70.1425 - val_MinusLogProbMetric: 70.1425 - lr: 2.0576e-06 - 61s/epoch - 313ms/step
Epoch 240/1000
2023-10-30 19:39:38.948 
Epoch 240/1000 
	 loss: 70.0076, MinusLogProbMetric: 70.0076, val_loss: 70.1141, val_MinusLogProbMetric: 70.1141

Epoch 240: val_loss improved from 70.14252 to 70.11407, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 70.0076 - MinusLogProbMetric: 70.0076 - val_loss: 70.1141 - val_MinusLogProbMetric: 70.1141 - lr: 2.0576e-06 - 61s/epoch - 314ms/step
Epoch 241/1000
2023-10-30 19:40:39.292 
Epoch 241/1000 
	 loss: 69.8991, MinusLogProbMetric: 69.8991, val_loss: 70.0568, val_MinusLogProbMetric: 70.0568

Epoch 241: val_loss improved from 70.11407 to 70.05679, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 69.8991 - MinusLogProbMetric: 69.8991 - val_loss: 70.0568 - val_MinusLogProbMetric: 70.0568 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 242/1000
2023-10-30 19:41:41.564 
Epoch 242/1000 
	 loss: 69.8494, MinusLogProbMetric: 69.8494, val_loss: 71.0594, val_MinusLogProbMetric: 71.0594

Epoch 242: val_loss did not improve from 70.05679
196/196 - 61s - loss: 69.8494 - MinusLogProbMetric: 69.8494 - val_loss: 71.0594 - val_MinusLogProbMetric: 71.0594 - lr: 2.0576e-06 - 61s/epoch - 314ms/step
Epoch 243/1000
2023-10-30 19:42:41.341 
Epoch 243/1000 
	 loss: 70.6684, MinusLogProbMetric: 70.6684, val_loss: 69.9394, val_MinusLogProbMetric: 69.9394

Epoch 243: val_loss improved from 70.05679 to 69.93938, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 70.6684 - MinusLogProbMetric: 70.6684 - val_loss: 69.9394 - val_MinusLogProbMetric: 69.9394 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 244/1000
2023-10-30 19:43:42.159 
Epoch 244/1000 
	 loss: 69.7634, MinusLogProbMetric: 69.7634, val_loss: 69.8748, val_MinusLogProbMetric: 69.8748

Epoch 244: val_loss improved from 69.93938 to 69.87485, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 69.7634 - MinusLogProbMetric: 69.7634 - val_loss: 69.8748 - val_MinusLogProbMetric: 69.8748 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 245/1000
2023-10-30 19:44:44.944 
Epoch 245/1000 
	 loss: 69.7365, MinusLogProbMetric: 69.7365, val_loss: 69.8418, val_MinusLogProbMetric: 69.8418

Epoch 245: val_loss improved from 69.87485 to 69.84178, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 69.7365 - MinusLogProbMetric: 69.7365 - val_loss: 69.8418 - val_MinusLogProbMetric: 69.8418 - lr: 2.0576e-06 - 63s/epoch - 320ms/step
Epoch 246/1000
2023-10-30 19:45:47.047 
Epoch 246/1000 
	 loss: 69.7314, MinusLogProbMetric: 69.7314, val_loss: 69.8169, val_MinusLogProbMetric: 69.8169

Epoch 246: val_loss improved from 69.84178 to 69.81689, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 69.7314 - MinusLogProbMetric: 69.7314 - val_loss: 69.8169 - val_MinusLogProbMetric: 69.8169 - lr: 2.0576e-06 - 62s/epoch - 317ms/step
Epoch 247/1000
2023-10-30 19:46:47.985 
Epoch 247/1000 
	 loss: 69.6340, MinusLogProbMetric: 69.6340, val_loss: 69.7684, val_MinusLogProbMetric: 69.7684

Epoch 247: val_loss improved from 69.81689 to 69.76836, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 69.6340 - MinusLogProbMetric: 69.6340 - val_loss: 69.7684 - val_MinusLogProbMetric: 69.7684 - lr: 2.0576e-06 - 61s/epoch - 311ms/step
Epoch 248/1000
2023-10-30 19:47:50.891 
Epoch 248/1000 
	 loss: 69.5512, MinusLogProbMetric: 69.5512, val_loss: 69.9172, val_MinusLogProbMetric: 69.9172

Epoch 248: val_loss did not improve from 69.76836
196/196 - 62s - loss: 69.5512 - MinusLogProbMetric: 69.5512 - val_loss: 69.9172 - val_MinusLogProbMetric: 69.9172 - lr: 2.0576e-06 - 62s/epoch - 316ms/step
Epoch 249/1000
2023-10-30 19:48:52.644 
Epoch 249/1000 
	 loss: 69.6332, MinusLogProbMetric: 69.6332, val_loss: 69.6926, val_MinusLogProbMetric: 69.6926

Epoch 249: val_loss improved from 69.76836 to 69.69261, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 69.6332 - MinusLogProbMetric: 69.6332 - val_loss: 69.6926 - val_MinusLogProbMetric: 69.6926 - lr: 2.0576e-06 - 63s/epoch - 320ms/step
Epoch 250/1000
2023-10-30 19:49:54.622 
Epoch 250/1000 
	 loss: 69.5067, MinusLogProbMetric: 69.5067, val_loss: 69.6264, val_MinusLogProbMetric: 69.6264

Epoch 250: val_loss improved from 69.69261 to 69.62635, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 69.5067 - MinusLogProbMetric: 69.5067 - val_loss: 69.6264 - val_MinusLogProbMetric: 69.6264 - lr: 2.0576e-06 - 62s/epoch - 316ms/step
Epoch 251/1000
2023-10-30 19:50:56.825 
Epoch 251/1000 
	 loss: 69.4399, MinusLogProbMetric: 69.4399, val_loss: 69.5634, val_MinusLogProbMetric: 69.5634

Epoch 251: val_loss improved from 69.62635 to 69.56343, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 69.4399 - MinusLogProbMetric: 69.4399 - val_loss: 69.5634 - val_MinusLogProbMetric: 69.5634 - lr: 2.0576e-06 - 62s/epoch - 317ms/step
Epoch 252/1000
2023-10-30 19:51:59.200 
Epoch 252/1000 
	 loss: 69.3760, MinusLogProbMetric: 69.3760, val_loss: 69.5474, val_MinusLogProbMetric: 69.5474

Epoch 252: val_loss improved from 69.56343 to 69.54741, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 69.3760 - MinusLogProbMetric: 69.3760 - val_loss: 69.5474 - val_MinusLogProbMetric: 69.5474 - lr: 2.0576e-06 - 62s/epoch - 319ms/step
Epoch 253/1000
2023-10-30 19:53:02.358 
Epoch 253/1000 
	 loss: 69.5607, MinusLogProbMetric: 69.5607, val_loss: 69.4989, val_MinusLogProbMetric: 69.4989

Epoch 253: val_loss improved from 69.54741 to 69.49893, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 69.5607 - MinusLogProbMetric: 69.5607 - val_loss: 69.4989 - val_MinusLogProbMetric: 69.4989 - lr: 2.0576e-06 - 63s/epoch - 322ms/step
Epoch 254/1000
2023-10-30 19:54:03.609 
Epoch 254/1000 
	 loss: 69.2531, MinusLogProbMetric: 69.2531, val_loss: 69.3778, val_MinusLogProbMetric: 69.3778

Epoch 254: val_loss improved from 69.49893 to 69.37779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 69.2531 - MinusLogProbMetric: 69.2531 - val_loss: 69.3778 - val_MinusLogProbMetric: 69.3778 - lr: 2.0576e-06 - 61s/epoch - 312ms/step
Epoch 255/1000
2023-10-30 19:55:05.782 
Epoch 255/1000 
	 loss: 69.1882, MinusLogProbMetric: 69.1882, val_loss: 69.3288, val_MinusLogProbMetric: 69.3288

Epoch 255: val_loss improved from 69.37779 to 69.32880, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 69.1882 - MinusLogProbMetric: 69.1882 - val_loss: 69.3288 - val_MinusLogProbMetric: 69.3288 - lr: 2.0576e-06 - 62s/epoch - 317ms/step
Epoch 256/1000
2023-10-30 19:56:06.426 
Epoch 256/1000 
	 loss: 69.1705, MinusLogProbMetric: 69.1705, val_loss: 69.2732, val_MinusLogProbMetric: 69.2732

Epoch 256: val_loss improved from 69.32880 to 69.27325, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 69.1705 - MinusLogProbMetric: 69.1705 - val_loss: 69.2732 - val_MinusLogProbMetric: 69.2732 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 257/1000
2023-10-30 19:57:06.707 
Epoch 257/1000 
	 loss: 69.0922, MinusLogProbMetric: 69.0922, val_loss: 69.2433, val_MinusLogProbMetric: 69.2433

Epoch 257: val_loss improved from 69.27325 to 69.24329, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 69.0922 - MinusLogProbMetric: 69.0922 - val_loss: 69.2433 - val_MinusLogProbMetric: 69.2433 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 258/1000
2023-10-30 19:58:08.163 
Epoch 258/1000 
	 loss: 69.0227, MinusLogProbMetric: 69.0227, val_loss: 69.1797, val_MinusLogProbMetric: 69.1797

Epoch 258: val_loss improved from 69.24329 to 69.17970, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 69.0227 - MinusLogProbMetric: 69.0227 - val_loss: 69.1797 - val_MinusLogProbMetric: 69.1797 - lr: 2.0576e-06 - 61s/epoch - 314ms/step
Epoch 259/1000
2023-10-30 19:59:06.862 
Epoch 259/1000 
	 loss: 69.0029, MinusLogProbMetric: 69.0029, val_loss: 69.1582, val_MinusLogProbMetric: 69.1582

Epoch 259: val_loss improved from 69.17970 to 69.15816, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 69.0029 - MinusLogProbMetric: 69.0029 - val_loss: 69.1582 - val_MinusLogProbMetric: 69.1582 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 260/1000
2023-10-30 20:00:02.731 
Epoch 260/1000 
	 loss: 68.9319, MinusLogProbMetric: 68.9319, val_loss: 69.0816, val_MinusLogProbMetric: 69.0816

Epoch 260: val_loss improved from 69.15816 to 69.08156, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 68.9319 - MinusLogProbMetric: 68.9319 - val_loss: 69.0816 - val_MinusLogProbMetric: 69.0816 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 261/1000
2023-10-30 20:00:59.150 
Epoch 261/1000 
	 loss: 68.8954, MinusLogProbMetric: 68.8954, val_loss: 69.0185, val_MinusLogProbMetric: 69.0185

Epoch 261: val_loss improved from 69.08156 to 69.01851, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 68.8954 - MinusLogProbMetric: 68.8954 - val_loss: 69.0185 - val_MinusLogProbMetric: 69.0185 - lr: 2.0576e-06 - 56s/epoch - 288ms/step
Epoch 262/1000
2023-10-30 20:01:57.191 
Epoch 262/1000 
	 loss: 68.8282, MinusLogProbMetric: 68.8282, val_loss: 68.9681, val_MinusLogProbMetric: 68.9681

Epoch 262: val_loss improved from 69.01851 to 68.96809, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 68.8282 - MinusLogProbMetric: 68.8282 - val_loss: 68.9681 - val_MinusLogProbMetric: 68.9681 - lr: 2.0576e-06 - 58s/epoch - 296ms/step
Epoch 263/1000
2023-10-30 20:02:52.033 
Epoch 263/1000 
	 loss: 68.7876, MinusLogProbMetric: 68.7876, val_loss: 68.9063, val_MinusLogProbMetric: 68.9063

Epoch 263: val_loss improved from 68.96809 to 68.90633, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 68.7876 - MinusLogProbMetric: 68.7876 - val_loss: 68.9063 - val_MinusLogProbMetric: 68.9063 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 264/1000
2023-10-30 20:03:49.343 
Epoch 264/1000 
	 loss: 68.7244, MinusLogProbMetric: 68.7244, val_loss: 68.8557, val_MinusLogProbMetric: 68.8557

Epoch 264: val_loss improved from 68.90633 to 68.85571, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 68.7244 - MinusLogProbMetric: 68.7244 - val_loss: 68.8557 - val_MinusLogProbMetric: 68.8557 - lr: 2.0576e-06 - 58s/epoch - 293ms/step
Epoch 265/1000
2023-10-30 20:04:43.270 
Epoch 265/1000 
	 loss: 68.7177, MinusLogProbMetric: 68.7177, val_loss: 68.8777, val_MinusLogProbMetric: 68.8777

Epoch 265: val_loss did not improve from 68.85571
196/196 - 53s - loss: 68.7177 - MinusLogProbMetric: 68.7177 - val_loss: 68.8777 - val_MinusLogProbMetric: 68.8777 - lr: 2.0576e-06 - 53s/epoch - 270ms/step
Epoch 266/1000
2023-10-30 20:05:37.663 
Epoch 266/1000 
	 loss: 68.6587, MinusLogProbMetric: 68.6587, val_loss: 68.9082, val_MinusLogProbMetric: 68.9082

Epoch 266: val_loss did not improve from 68.85571
196/196 - 54s - loss: 68.6587 - MinusLogProbMetric: 68.6587 - val_loss: 68.9082 - val_MinusLogProbMetric: 68.9082 - lr: 2.0576e-06 - 54s/epoch - 277ms/step
Epoch 267/1000
2023-10-30 20:06:33.064 
Epoch 267/1000 
	 loss: 68.6339, MinusLogProbMetric: 68.6339, val_loss: 68.8151, val_MinusLogProbMetric: 68.8151

Epoch 267: val_loss improved from 68.85571 to 68.81506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 68.6339 - MinusLogProbMetric: 68.6339 - val_loss: 68.8151 - val_MinusLogProbMetric: 68.8151 - lr: 2.0576e-06 - 56s/epoch - 287ms/step
Epoch 268/1000
2023-10-30 20:07:29.970 
Epoch 268/1000 
	 loss: 68.5371, MinusLogProbMetric: 68.5371, val_loss: 68.7567, val_MinusLogProbMetric: 68.7567

Epoch 268: val_loss improved from 68.81506 to 68.75670, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 68.5371 - MinusLogProbMetric: 68.5371 - val_loss: 68.7567 - val_MinusLogProbMetric: 68.7567 - lr: 2.0576e-06 - 57s/epoch - 290ms/step
Epoch 269/1000
2023-10-30 20:08:23.070 
Epoch 269/1000 
	 loss: 68.5269, MinusLogProbMetric: 68.5269, val_loss: 68.6763, val_MinusLogProbMetric: 68.6763

Epoch 269: val_loss improved from 68.75670 to 68.67627, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 53s - loss: 68.5269 - MinusLogProbMetric: 68.5269 - val_loss: 68.6763 - val_MinusLogProbMetric: 68.6763 - lr: 2.0576e-06 - 53s/epoch - 271ms/step
Epoch 270/1000
2023-10-30 20:09:21.037 
Epoch 270/1000 
	 loss: 68.4454, MinusLogProbMetric: 68.4454, val_loss: 68.6015, val_MinusLogProbMetric: 68.6015

Epoch 270: val_loss improved from 68.67627 to 68.60148, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 68.4454 - MinusLogProbMetric: 68.4454 - val_loss: 68.6015 - val_MinusLogProbMetric: 68.6015 - lr: 2.0576e-06 - 58s/epoch - 296ms/step
Epoch 271/1000
2023-10-30 20:10:15.811 
Epoch 271/1000 
	 loss: 68.3929, MinusLogProbMetric: 68.3929, val_loss: 68.5410, val_MinusLogProbMetric: 68.5410

Epoch 271: val_loss improved from 68.60148 to 68.54098, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 68.3929 - MinusLogProbMetric: 68.3929 - val_loss: 68.5410 - val_MinusLogProbMetric: 68.5410 - lr: 2.0576e-06 - 55s/epoch - 278ms/step
Epoch 272/1000
2023-10-30 20:11:10.227 
Epoch 272/1000 
	 loss: 68.3580, MinusLogProbMetric: 68.3580, val_loss: 68.6365, val_MinusLogProbMetric: 68.6365

Epoch 272: val_loss did not improve from 68.54098
196/196 - 54s - loss: 68.3580 - MinusLogProbMetric: 68.3580 - val_loss: 68.6365 - val_MinusLogProbMetric: 68.6365 - lr: 2.0576e-06 - 54s/epoch - 274ms/step
Epoch 273/1000
2023-10-30 20:12:05.107 
Epoch 273/1000 
	 loss: 68.2994, MinusLogProbMetric: 68.2994, val_loss: 68.4483, val_MinusLogProbMetric: 68.4483

Epoch 273: val_loss improved from 68.54098 to 68.44827, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 68.2994 - MinusLogProbMetric: 68.2994 - val_loss: 68.4483 - val_MinusLogProbMetric: 68.4483 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 274/1000
2023-10-30 20:13:03.219 
Epoch 274/1000 
	 loss: 68.3101, MinusLogProbMetric: 68.3101, val_loss: 68.4130, val_MinusLogProbMetric: 68.4130

Epoch 274: val_loss improved from 68.44827 to 68.41299, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 68.3101 - MinusLogProbMetric: 68.3101 - val_loss: 68.4130 - val_MinusLogProbMetric: 68.4130 - lr: 2.0576e-06 - 58s/epoch - 296ms/step
Epoch 275/1000
2023-10-30 20:13:58.356 
Epoch 275/1000 
	 loss: 68.2642, MinusLogProbMetric: 68.2642, val_loss: 68.4587, val_MinusLogProbMetric: 68.4587

Epoch 275: val_loss did not improve from 68.41299
196/196 - 54s - loss: 68.2642 - MinusLogProbMetric: 68.2642 - val_loss: 68.4587 - val_MinusLogProbMetric: 68.4587 - lr: 2.0576e-06 - 54s/epoch - 277ms/step
Epoch 276/1000
2023-10-30 20:14:54.232 
Epoch 276/1000 
	 loss: 68.1615, MinusLogProbMetric: 68.1615, val_loss: 68.3707, val_MinusLogProbMetric: 68.3707

Epoch 276: val_loss improved from 68.41299 to 68.37072, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 68.1615 - MinusLogProbMetric: 68.1615 - val_loss: 68.3707 - val_MinusLogProbMetric: 68.3707 - lr: 2.0576e-06 - 57s/epoch - 289ms/step
Epoch 277/1000
2023-10-30 20:15:52.564 
Epoch 277/1000 
	 loss: 68.1403, MinusLogProbMetric: 68.1403, val_loss: 68.2686, val_MinusLogProbMetric: 68.2686

Epoch 277: val_loss improved from 68.37072 to 68.26865, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 68.1403 - MinusLogProbMetric: 68.1403 - val_loss: 68.2686 - val_MinusLogProbMetric: 68.2686 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 278/1000
2023-10-30 20:16:46.443 
Epoch 278/1000 
	 loss: 68.0668, MinusLogProbMetric: 68.0668, val_loss: 68.2466, val_MinusLogProbMetric: 68.2466

Epoch 278: val_loss improved from 68.26865 to 68.24659, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 68.0668 - MinusLogProbMetric: 68.0668 - val_loss: 68.2466 - val_MinusLogProbMetric: 68.2466 - lr: 2.0576e-06 - 54s/epoch - 275ms/step
Epoch 279/1000
2023-10-30 20:17:41.993 
Epoch 279/1000 
	 loss: 67.9941, MinusLogProbMetric: 67.9941, val_loss: 68.1462, val_MinusLogProbMetric: 68.1462

Epoch 279: val_loss improved from 68.24659 to 68.14625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 67.9941 - MinusLogProbMetric: 67.9941 - val_loss: 68.1462 - val_MinusLogProbMetric: 68.1462 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 280/1000
2023-10-30 20:18:37.292 
Epoch 280/1000 
	 loss: 67.9650, MinusLogProbMetric: 67.9650, val_loss: 68.0836, val_MinusLogProbMetric: 68.0836

Epoch 280: val_loss improved from 68.14625 to 68.08357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 67.9650 - MinusLogProbMetric: 67.9650 - val_loss: 68.0836 - val_MinusLogProbMetric: 68.0836 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 281/1000
2023-10-30 20:19:30.167 
Epoch 281/1000 
	 loss: 67.8864, MinusLogProbMetric: 67.8864, val_loss: 68.1206, val_MinusLogProbMetric: 68.1206

Epoch 281: val_loss did not improve from 68.08357
196/196 - 52s - loss: 67.8864 - MinusLogProbMetric: 67.8864 - val_loss: 68.1206 - val_MinusLogProbMetric: 68.1206 - lr: 2.0576e-06 - 52s/epoch - 266ms/step
Epoch 282/1000
2023-10-30 20:20:26.170 
Epoch 282/1000 
	 loss: 67.8512, MinusLogProbMetric: 67.8512, val_loss: 68.2139, val_MinusLogProbMetric: 68.2139

Epoch 282: val_loss did not improve from 68.08357
196/196 - 56s - loss: 67.8512 - MinusLogProbMetric: 67.8512 - val_loss: 68.2139 - val_MinusLogProbMetric: 68.2139 - lr: 2.0576e-06 - 56s/epoch - 286ms/step
Epoch 283/1000
2023-10-30 20:21:23.534 
Epoch 283/1000 
	 loss: 67.9556, MinusLogProbMetric: 67.9556, val_loss: 68.0564, val_MinusLogProbMetric: 68.0564

Epoch 283: val_loss improved from 68.08357 to 68.05644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 67.9556 - MinusLogProbMetric: 67.9556 - val_loss: 68.0564 - val_MinusLogProbMetric: 68.0564 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 284/1000
2023-10-30 20:22:18.341 
Epoch 284/1000 
	 loss: 67.8817, MinusLogProbMetric: 67.8817, val_loss: 67.9742, val_MinusLogProbMetric: 67.9742

Epoch 284: val_loss improved from 68.05644 to 67.97415, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 67.8817 - MinusLogProbMetric: 67.8817 - val_loss: 67.9742 - val_MinusLogProbMetric: 67.9742 - lr: 2.0576e-06 - 55s/epoch - 279ms/step
Epoch 285/1000
2023-10-30 20:23:15.214 
Epoch 285/1000 
	 loss: 67.7887, MinusLogProbMetric: 67.7887, val_loss: 67.8971, val_MinusLogProbMetric: 67.8971

Epoch 285: val_loss improved from 67.97415 to 67.89706, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 67.7887 - MinusLogProbMetric: 67.7887 - val_loss: 67.8971 - val_MinusLogProbMetric: 67.8971 - lr: 2.0576e-06 - 57s/epoch - 291ms/step
Epoch 286/1000
2023-10-30 20:24:14.051 
Epoch 286/1000 
	 loss: 67.8997, MinusLogProbMetric: 67.8997, val_loss: 68.4663, val_MinusLogProbMetric: 68.4663

Epoch 286: val_loss did not improve from 67.89706
196/196 - 58s - loss: 67.8997 - MinusLogProbMetric: 67.8997 - val_loss: 68.4663 - val_MinusLogProbMetric: 68.4663 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 287/1000
2023-10-30 20:25:07.716 
Epoch 287/1000 
	 loss: 67.9465, MinusLogProbMetric: 67.9465, val_loss: 67.9969, val_MinusLogProbMetric: 67.9969

Epoch 287: val_loss did not improve from 67.89706
196/196 - 54s - loss: 67.9465 - MinusLogProbMetric: 67.9465 - val_loss: 67.9969 - val_MinusLogProbMetric: 67.9969 - lr: 2.0576e-06 - 54s/epoch - 274ms/step
Epoch 288/1000
2023-10-30 20:26:02.220 
Epoch 288/1000 
	 loss: 67.8415, MinusLogProbMetric: 67.8415, val_loss: 67.8775, val_MinusLogProbMetric: 67.8775

Epoch 288: val_loss improved from 67.89706 to 67.87750, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 67.8415 - MinusLogProbMetric: 67.8415 - val_loss: 67.8775 - val_MinusLogProbMetric: 67.8775 - lr: 2.0576e-06 - 55s/epoch - 282ms/step
Epoch 289/1000
2023-10-30 20:27:01.404 
Epoch 289/1000 
	 loss: 67.7190, MinusLogProbMetric: 67.7190, val_loss: 67.8255, val_MinusLogProbMetric: 67.8255

Epoch 289: val_loss improved from 67.87750 to 67.82553, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 67.7190 - MinusLogProbMetric: 67.7190 - val_loss: 67.8255 - val_MinusLogProbMetric: 67.8255 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 290/1000
2023-10-30 20:27:56.357 
Epoch 290/1000 
	 loss: 67.5989, MinusLogProbMetric: 67.5989, val_loss: 67.7495, val_MinusLogProbMetric: 67.7495

Epoch 290: val_loss improved from 67.82553 to 67.74945, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 67.5989 - MinusLogProbMetric: 67.5989 - val_loss: 67.7495 - val_MinusLogProbMetric: 67.7495 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 291/1000
2023-10-30 20:28:51.787 
Epoch 291/1000 
	 loss: 67.5352, MinusLogProbMetric: 67.5352, val_loss: 67.6860, val_MinusLogProbMetric: 67.6860

Epoch 291: val_loss improved from 67.74945 to 67.68602, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 67.5352 - MinusLogProbMetric: 67.5352 - val_loss: 67.6860 - val_MinusLogProbMetric: 67.6860 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 292/1000
2023-10-30 20:29:51.464 
Epoch 292/1000 
	 loss: 67.4905, MinusLogProbMetric: 67.4905, val_loss: 67.6801, val_MinusLogProbMetric: 67.6801

Epoch 292: val_loss improved from 67.68602 to 67.68005, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 67.4905 - MinusLogProbMetric: 67.4905 - val_loss: 67.6801 - val_MinusLogProbMetric: 67.6801 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 293/1000
2023-10-30 20:30:49.311 
Epoch 293/1000 
	 loss: 67.4485, MinusLogProbMetric: 67.4485, val_loss: 67.6420, val_MinusLogProbMetric: 67.6420

Epoch 293: val_loss improved from 67.68005 to 67.64203, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 67.4485 - MinusLogProbMetric: 67.4485 - val_loss: 67.6420 - val_MinusLogProbMetric: 67.6420 - lr: 2.0576e-06 - 58s/epoch - 296ms/step
Epoch 294/1000
2023-10-30 20:31:45.263 
Epoch 294/1000 
	 loss: 67.4262, MinusLogProbMetric: 67.4262, val_loss: 67.5523, val_MinusLogProbMetric: 67.5523

Epoch 294: val_loss improved from 67.64203 to 67.55231, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 67.4262 - MinusLogProbMetric: 67.4262 - val_loss: 67.5523 - val_MinusLogProbMetric: 67.5523 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 295/1000
2023-10-30 20:32:40.904 
Epoch 295/1000 
	 loss: 67.4069, MinusLogProbMetric: 67.4069, val_loss: 67.5299, val_MinusLogProbMetric: 67.5299

Epoch 295: val_loss improved from 67.55231 to 67.52991, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 67.4069 - MinusLogProbMetric: 67.4069 - val_loss: 67.5299 - val_MinusLogProbMetric: 67.5299 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 296/1000
2023-10-30 20:33:39.156 
Epoch 296/1000 
	 loss: 67.2978, MinusLogProbMetric: 67.2978, val_loss: 67.4650, val_MinusLogProbMetric: 67.4650

Epoch 296: val_loss improved from 67.52991 to 67.46503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 67.2978 - MinusLogProbMetric: 67.2978 - val_loss: 67.4650 - val_MinusLogProbMetric: 67.4650 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 297/1000
2023-10-30 20:34:35.771 
Epoch 297/1000 
	 loss: 67.3829, MinusLogProbMetric: 67.3829, val_loss: 67.4908, val_MinusLogProbMetric: 67.4908

Epoch 297: val_loss did not improve from 67.46503
196/196 - 56s - loss: 67.3829 - MinusLogProbMetric: 67.3829 - val_loss: 67.4908 - val_MinusLogProbMetric: 67.4908 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 298/1000
2023-10-30 20:35:33.138 
Epoch 298/1000 
	 loss: 67.2201, MinusLogProbMetric: 67.2201, val_loss: 67.4088, val_MinusLogProbMetric: 67.4088

Epoch 298: val_loss improved from 67.46503 to 67.40881, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 67.2201 - MinusLogProbMetric: 67.2201 - val_loss: 67.4088 - val_MinusLogProbMetric: 67.4088 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 299/1000
2023-10-30 20:36:31.276 
Epoch 299/1000 
	 loss: 67.2099, MinusLogProbMetric: 67.2099, val_loss: 67.3837, val_MinusLogProbMetric: 67.3837

Epoch 299: val_loss improved from 67.40881 to 67.38370, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 67.2099 - MinusLogProbMetric: 67.2099 - val_loss: 67.3837 - val_MinusLogProbMetric: 67.3837 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 300/1000
2023-10-30 20:37:27.625 
Epoch 300/1000 
	 loss: 67.1737, MinusLogProbMetric: 67.1737, val_loss: 67.2995, val_MinusLogProbMetric: 67.2995

Epoch 300: val_loss improved from 67.38370 to 67.29946, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 67.1737 - MinusLogProbMetric: 67.1737 - val_loss: 67.2995 - val_MinusLogProbMetric: 67.2995 - lr: 2.0576e-06 - 56s/epoch - 287ms/step
Epoch 301/1000
2023-10-30 20:38:24.674 
Epoch 301/1000 
	 loss: 67.1529, MinusLogProbMetric: 67.1529, val_loss: 67.2763, val_MinusLogProbMetric: 67.2763

Epoch 301: val_loss improved from 67.29946 to 67.27632, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 67.1529 - MinusLogProbMetric: 67.1529 - val_loss: 67.2763 - val_MinusLogProbMetric: 67.2763 - lr: 2.0576e-06 - 57s/epoch - 291ms/step
Epoch 302/1000
2023-10-30 20:39:20.783 
Epoch 302/1000 
	 loss: 67.1123, MinusLogProbMetric: 67.1123, val_loss: 67.2542, val_MinusLogProbMetric: 67.2542

Epoch 302: val_loss improved from 67.27632 to 67.25420, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 67.1123 - MinusLogProbMetric: 67.1123 - val_loss: 67.2542 - val_MinusLogProbMetric: 67.2542 - lr: 2.0576e-06 - 56s/epoch - 286ms/step
Epoch 303/1000
2023-10-30 20:40:16.124 
Epoch 303/1000 
	 loss: 67.1175, MinusLogProbMetric: 67.1175, val_loss: 67.4235, val_MinusLogProbMetric: 67.4235

Epoch 303: val_loss did not improve from 67.25420
196/196 - 55s - loss: 67.1175 - MinusLogProbMetric: 67.1175 - val_loss: 67.4235 - val_MinusLogProbMetric: 67.4235 - lr: 2.0576e-06 - 55s/epoch - 279ms/step
Epoch 304/1000
2023-10-30 20:41:13.184 
Epoch 304/1000 
	 loss: 70.2980, MinusLogProbMetric: 70.2980, val_loss: 69.7650, val_MinusLogProbMetric: 69.7650

Epoch 304: val_loss did not improve from 67.25420
196/196 - 57s - loss: 70.2980 - MinusLogProbMetric: 70.2980 - val_loss: 69.7650 - val_MinusLogProbMetric: 69.7650 - lr: 2.0576e-06 - 57s/epoch - 291ms/step
Epoch 305/1000
2023-10-30 20:42:09.280 
Epoch 305/1000 
	 loss: 68.8627, MinusLogProbMetric: 68.8627, val_loss: 68.4701, val_MinusLogProbMetric: 68.4701

Epoch 305: val_loss did not improve from 67.25420
196/196 - 56s - loss: 68.8627 - MinusLogProbMetric: 68.8627 - val_loss: 68.4701 - val_MinusLogProbMetric: 68.4701 - lr: 2.0576e-06 - 56s/epoch - 286ms/step
Epoch 306/1000
2023-10-30 20:43:06.849 
Epoch 306/1000 
	 loss: 68.1009, MinusLogProbMetric: 68.1009, val_loss: 68.0653, val_MinusLogProbMetric: 68.0653

Epoch 306: val_loss did not improve from 67.25420
196/196 - 58s - loss: 68.1009 - MinusLogProbMetric: 68.1009 - val_loss: 68.0653 - val_MinusLogProbMetric: 68.0653 - lr: 2.0576e-06 - 58s/epoch - 294ms/step
Epoch 307/1000
2023-10-30 20:44:02.170 
Epoch 307/1000 
	 loss: 67.8833, MinusLogProbMetric: 67.8833, val_loss: 67.8768, val_MinusLogProbMetric: 67.8768

Epoch 307: val_loss did not improve from 67.25420
196/196 - 55s - loss: 67.8833 - MinusLogProbMetric: 67.8833 - val_loss: 67.8768 - val_MinusLogProbMetric: 67.8768 - lr: 2.0576e-06 - 55s/epoch - 282ms/step
Epoch 308/1000
2023-10-30 20:44:56.646 
Epoch 308/1000 
	 loss: 67.6923, MinusLogProbMetric: 67.6923, val_loss: 67.7650, val_MinusLogProbMetric: 67.7650

Epoch 308: val_loss did not improve from 67.25420
196/196 - 54s - loss: 67.6923 - MinusLogProbMetric: 67.6923 - val_loss: 67.7650 - val_MinusLogProbMetric: 67.7650 - lr: 2.0576e-06 - 54s/epoch - 278ms/step
Epoch 309/1000
2023-10-30 20:45:53.051 
Epoch 309/1000 
	 loss: 67.5437, MinusLogProbMetric: 67.5437, val_loss: 67.6169, val_MinusLogProbMetric: 67.6169

Epoch 309: val_loss did not improve from 67.25420
196/196 - 56s - loss: 67.5437 - MinusLogProbMetric: 67.5437 - val_loss: 67.6169 - val_MinusLogProbMetric: 67.6169 - lr: 2.0576e-06 - 56s/epoch - 288ms/step
Epoch 310/1000
2023-10-30 20:46:48.009 
Epoch 310/1000 
	 loss: 67.3938, MinusLogProbMetric: 67.3938, val_loss: 67.4380, val_MinusLogProbMetric: 67.4380

Epoch 310: val_loss did not improve from 67.25420
196/196 - 55s - loss: 67.3938 - MinusLogProbMetric: 67.3938 - val_loss: 67.4380 - val_MinusLogProbMetric: 67.4380 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 311/1000
2023-10-30 20:47:42.780 
Epoch 311/1000 
	 loss: 67.0060, MinusLogProbMetric: 67.0060, val_loss: 66.9750, val_MinusLogProbMetric: 66.9750

Epoch 311: val_loss improved from 67.25420 to 66.97497, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 67.0060 - MinusLogProbMetric: 67.0060 - val_loss: 66.9750 - val_MinusLogProbMetric: 66.9750 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 312/1000
2023-10-30 20:48:39.858 
Epoch 312/1000 
	 loss: 66.7637, MinusLogProbMetric: 66.7637, val_loss: 66.8798, val_MinusLogProbMetric: 66.8798

Epoch 312: val_loss improved from 66.97497 to 66.87981, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 66.7637 - MinusLogProbMetric: 66.7637 - val_loss: 66.8798 - val_MinusLogProbMetric: 66.8798 - lr: 2.0576e-06 - 57s/epoch - 291ms/step
Epoch 313/1000
2023-10-30 20:49:36.508 
Epoch 313/1000 
	 loss: 66.6729, MinusLogProbMetric: 66.6729, val_loss: 66.8656, val_MinusLogProbMetric: 66.8656

Epoch 313: val_loss improved from 66.87981 to 66.86557, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 66.6729 - MinusLogProbMetric: 66.6729 - val_loss: 66.8656 - val_MinusLogProbMetric: 66.8656 - lr: 2.0576e-06 - 57s/epoch - 290ms/step
Epoch 314/1000
2023-10-30 20:50:33.181 
Epoch 314/1000 
	 loss: 66.6985, MinusLogProbMetric: 66.6985, val_loss: 66.8896, val_MinusLogProbMetric: 66.8896

Epoch 314: val_loss did not improve from 66.86557
196/196 - 56s - loss: 66.6985 - MinusLogProbMetric: 66.6985 - val_loss: 66.8896 - val_MinusLogProbMetric: 66.8896 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 315/1000
2023-10-30 20:51:32.579 
Epoch 315/1000 
	 loss: 66.6429, MinusLogProbMetric: 66.6429, val_loss: 66.8081, val_MinusLogProbMetric: 66.8081

Epoch 315: val_loss improved from 66.86557 to 66.80808, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 66.6429 - MinusLogProbMetric: 66.6429 - val_loss: 66.8081 - val_MinusLogProbMetric: 66.8081 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 316/1000
2023-10-30 20:52:31.806 
Epoch 316/1000 
	 loss: 69.9431, MinusLogProbMetric: 69.9431, val_loss: 67.2801, val_MinusLogProbMetric: 67.2801

Epoch 316: val_loss did not improve from 66.80808
196/196 - 58s - loss: 69.9431 - MinusLogProbMetric: 69.9431 - val_loss: 67.2801 - val_MinusLogProbMetric: 67.2801 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 317/1000
2023-10-30 20:53:26.791 
Epoch 317/1000 
	 loss: 66.7679, MinusLogProbMetric: 66.7679, val_loss: 66.8266, val_MinusLogProbMetric: 66.8266

Epoch 317: val_loss did not improve from 66.80808
196/196 - 55s - loss: 66.7679 - MinusLogProbMetric: 66.7679 - val_loss: 66.8266 - val_MinusLogProbMetric: 66.8266 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 318/1000
2023-10-30 20:54:22.903 
Epoch 318/1000 
	 loss: 66.5439, MinusLogProbMetric: 66.5439, val_loss: 66.6966, val_MinusLogProbMetric: 66.6966

Epoch 318: val_loss improved from 66.80808 to 66.69663, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 66.5439 - MinusLogProbMetric: 66.5439 - val_loss: 66.6966 - val_MinusLogProbMetric: 66.6966 - lr: 2.0576e-06 - 57s/epoch - 290ms/step
Epoch 319/1000
2023-10-30 20:55:19.238 
Epoch 319/1000 
	 loss: 66.4732, MinusLogProbMetric: 66.4732, val_loss: 66.6517, val_MinusLogProbMetric: 66.6517

Epoch 319: val_loss improved from 66.69663 to 66.65167, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 66.4732 - MinusLogProbMetric: 66.4732 - val_loss: 66.6517 - val_MinusLogProbMetric: 66.6517 - lr: 2.0576e-06 - 56s/epoch - 288ms/step
Epoch 320/1000
2023-10-30 20:56:14.178 
Epoch 320/1000 
	 loss: 66.3982, MinusLogProbMetric: 66.3982, val_loss: 66.5875, val_MinusLogProbMetric: 66.5875

Epoch 320: val_loss improved from 66.65167 to 66.58749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 66.3982 - MinusLogProbMetric: 66.3982 - val_loss: 66.5875 - val_MinusLogProbMetric: 66.5875 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 321/1000
2023-10-30 20:57:12.227 
Epoch 321/1000 
	 loss: 66.4015, MinusLogProbMetric: 66.4015, val_loss: 66.5897, val_MinusLogProbMetric: 66.5897

Epoch 321: val_loss did not improve from 66.58749
196/196 - 57s - loss: 66.4015 - MinusLogProbMetric: 66.4015 - val_loss: 66.5897 - val_MinusLogProbMetric: 66.5897 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 322/1000
2023-10-30 20:58:07.245 
Epoch 322/1000 
	 loss: 66.4266, MinusLogProbMetric: 66.4266, val_loss: 66.6644, val_MinusLogProbMetric: 66.6644

Epoch 322: val_loss did not improve from 66.58749
196/196 - 55s - loss: 66.4266 - MinusLogProbMetric: 66.4266 - val_loss: 66.6644 - val_MinusLogProbMetric: 66.6644 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 323/1000
2023-10-30 20:59:07.565 
Epoch 323/1000 
	 loss: 66.3428, MinusLogProbMetric: 66.3428, val_loss: 66.4934, val_MinusLogProbMetric: 66.4934

Epoch 323: val_loss improved from 66.58749 to 66.49339, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 66.3428 - MinusLogProbMetric: 66.3428 - val_loss: 66.4934 - val_MinusLogProbMetric: 66.4934 - lr: 2.0576e-06 - 61s/epoch - 312ms/step
Epoch 324/1000
2023-10-30 21:00:02.846 
Epoch 324/1000 
	 loss: 66.2782, MinusLogProbMetric: 66.2782, val_loss: 66.4204, val_MinusLogProbMetric: 66.4204

Epoch 324: val_loss improved from 66.49339 to 66.42036, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 66.2782 - MinusLogProbMetric: 66.2782 - val_loss: 66.4204 - val_MinusLogProbMetric: 66.4204 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 325/1000
2023-10-30 21:00:58.362 
Epoch 325/1000 
	 loss: 66.2405, MinusLogProbMetric: 66.2405, val_loss: 66.4109, val_MinusLogProbMetric: 66.4109

Epoch 325: val_loss improved from 66.42036 to 66.41086, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 66.2405 - MinusLogProbMetric: 66.2405 - val_loss: 66.4109 - val_MinusLogProbMetric: 66.4109 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 326/1000
2023-10-30 21:01:55.600 
Epoch 326/1000 
	 loss: 66.1930, MinusLogProbMetric: 66.1930, val_loss: 66.3475, val_MinusLogProbMetric: 66.3475

Epoch 326: val_loss improved from 66.41086 to 66.34752, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 66.1930 - MinusLogProbMetric: 66.1930 - val_loss: 66.3475 - val_MinusLogProbMetric: 66.3475 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 327/1000
2023-10-30 21:02:50.730 
Epoch 327/1000 
	 loss: 66.1505, MinusLogProbMetric: 66.1505, val_loss: 66.3129, val_MinusLogProbMetric: 66.3129

Epoch 327: val_loss improved from 66.34752 to 66.31289, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 66.1505 - MinusLogProbMetric: 66.1505 - val_loss: 66.3129 - val_MinusLogProbMetric: 66.3129 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 328/1000
2023-10-30 21:03:45.908 
Epoch 328/1000 
	 loss: 66.1110, MinusLogProbMetric: 66.1110, val_loss: 66.2663, val_MinusLogProbMetric: 66.2663

Epoch 328: val_loss improved from 66.31289 to 66.26633, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 66.1110 - MinusLogProbMetric: 66.1110 - val_loss: 66.2663 - val_MinusLogProbMetric: 66.2663 - lr: 2.0576e-06 - 55s/epoch - 282ms/step
Epoch 329/1000
2023-10-30 21:04:40.673 
Epoch 329/1000 
	 loss: 66.1222, MinusLogProbMetric: 66.1222, val_loss: 66.2297, val_MinusLogProbMetric: 66.2297

Epoch 329: val_loss improved from 66.26633 to 66.22965, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 66.1222 - MinusLogProbMetric: 66.1222 - val_loss: 66.2297 - val_MinusLogProbMetric: 66.2297 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 330/1000
2023-10-30 21:05:36.323 
Epoch 330/1000 
	 loss: 66.0302, MinusLogProbMetric: 66.0302, val_loss: 66.2640, val_MinusLogProbMetric: 66.2640

Epoch 330: val_loss did not improve from 66.22965
196/196 - 55s - loss: 66.0302 - MinusLogProbMetric: 66.0302 - val_loss: 66.2640 - val_MinusLogProbMetric: 66.2640 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 331/1000
2023-10-30 21:06:34.134 
Epoch 331/1000 
	 loss: 66.0028, MinusLogProbMetric: 66.0028, val_loss: 66.1068, val_MinusLogProbMetric: 66.1068

Epoch 331: val_loss improved from 66.22965 to 66.10683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 66.0028 - MinusLogProbMetric: 66.0028 - val_loss: 66.1068 - val_MinusLogProbMetric: 66.1068 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 332/1000
2023-10-30 21:07:29.895 
Epoch 332/1000 
	 loss: 66.2887, MinusLogProbMetric: 66.2887, val_loss: 66.1747, val_MinusLogProbMetric: 66.1747

Epoch 332: val_loss did not improve from 66.10683
196/196 - 55s - loss: 66.2887 - MinusLogProbMetric: 66.2887 - val_loss: 66.1747 - val_MinusLogProbMetric: 66.1747 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 333/1000
2023-10-30 21:08:27.858 
Epoch 333/1000 
	 loss: 66.1876, MinusLogProbMetric: 66.1876, val_loss: 66.1055, val_MinusLogProbMetric: 66.1055

Epoch 333: val_loss improved from 66.10683 to 66.10548, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 66.1876 - MinusLogProbMetric: 66.1876 - val_loss: 66.1055 - val_MinusLogProbMetric: 66.1055 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 334/1000
2023-10-30 21:09:21.939 
Epoch 334/1000 
	 loss: 65.8688, MinusLogProbMetric: 65.8688, val_loss: 66.0758, val_MinusLogProbMetric: 66.0758

Epoch 334: val_loss improved from 66.10548 to 66.07583, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 65.8688 - MinusLogProbMetric: 65.8688 - val_loss: 66.0758 - val_MinusLogProbMetric: 66.0758 - lr: 2.0576e-06 - 54s/epoch - 275ms/step
Epoch 335/1000
2023-10-30 21:10:16.686 
Epoch 335/1000 
	 loss: 65.8049, MinusLogProbMetric: 65.8049, val_loss: 65.9743, val_MinusLogProbMetric: 65.9743

Epoch 335: val_loss improved from 66.07583 to 65.97430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 65.8049 - MinusLogProbMetric: 65.8049 - val_loss: 65.9743 - val_MinusLogProbMetric: 65.9743 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 336/1000
2023-10-30 21:11:15.447 
Epoch 336/1000 
	 loss: 65.7576, MinusLogProbMetric: 65.7576, val_loss: 65.9478, val_MinusLogProbMetric: 65.9478

Epoch 336: val_loss improved from 65.97430 to 65.94780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 65.7576 - MinusLogProbMetric: 65.7576 - val_loss: 65.9478 - val_MinusLogProbMetric: 65.9478 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 337/1000
2023-10-30 21:12:10.475 
Epoch 337/1000 
	 loss: 65.7057, MinusLogProbMetric: 65.7057, val_loss: 65.8847, val_MinusLogProbMetric: 65.8847

Epoch 337: val_loss improved from 65.94780 to 65.88469, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 65.7057 - MinusLogProbMetric: 65.7057 - val_loss: 65.8847 - val_MinusLogProbMetric: 65.8847 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 338/1000
2023-10-30 21:13:07.993 
Epoch 338/1000 
	 loss: 65.6698, MinusLogProbMetric: 65.6698, val_loss: 65.8705, val_MinusLogProbMetric: 65.8705

Epoch 338: val_loss improved from 65.88469 to 65.87051, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 65.6698 - MinusLogProbMetric: 65.6698 - val_loss: 65.8705 - val_MinusLogProbMetric: 65.8705 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 339/1000
2023-10-30 21:14:02.499 
Epoch 339/1000 
	 loss: 65.6427, MinusLogProbMetric: 65.6427, val_loss: 65.8250, val_MinusLogProbMetric: 65.8250

Epoch 339: val_loss improved from 65.87051 to 65.82504, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 65.6427 - MinusLogProbMetric: 65.6427 - val_loss: 65.8250 - val_MinusLogProbMetric: 65.8250 - lr: 2.0576e-06 - 55s/epoch - 278ms/step
Epoch 340/1000
2023-10-30 21:14:59.714 
Epoch 340/1000 
	 loss: 65.6185, MinusLogProbMetric: 65.6185, val_loss: 65.8118, val_MinusLogProbMetric: 65.8118

Epoch 340: val_loss improved from 65.82504 to 65.81183, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 65.6185 - MinusLogProbMetric: 65.6185 - val_loss: 65.8118 - val_MinusLogProbMetric: 65.8118 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 341/1000
2023-10-30 21:15:55.246 
Epoch 341/1000 
	 loss: 65.5608, MinusLogProbMetric: 65.5608, val_loss: 65.7556, val_MinusLogProbMetric: 65.7556

Epoch 341: val_loss improved from 65.81183 to 65.75556, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 65.5608 - MinusLogProbMetric: 65.5608 - val_loss: 65.7556 - val_MinusLogProbMetric: 65.7556 - lr: 2.0576e-06 - 56s/epoch - 283ms/step
Epoch 342/1000
2023-10-30 21:16:49.303 
Epoch 342/1000 
	 loss: 65.5376, MinusLogProbMetric: 65.5376, val_loss: 65.7714, val_MinusLogProbMetric: 65.7714

Epoch 342: val_loss did not improve from 65.75556
196/196 - 53s - loss: 65.5376 - MinusLogProbMetric: 65.5376 - val_loss: 65.7714 - val_MinusLogProbMetric: 65.7714 - lr: 2.0576e-06 - 53s/epoch - 272ms/step
Epoch 343/1000
2023-10-30 21:17:46.409 
Epoch 343/1000 
	 loss: 65.4982, MinusLogProbMetric: 65.4982, val_loss: 65.6886, val_MinusLogProbMetric: 65.6886

Epoch 343: val_loss improved from 65.75556 to 65.68864, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 65.4982 - MinusLogProbMetric: 65.4982 - val_loss: 65.6886 - val_MinusLogProbMetric: 65.6886 - lr: 2.0576e-06 - 58s/epoch - 296ms/step
Epoch 344/1000
2023-10-30 21:18:40.640 
Epoch 344/1000 
	 loss: 65.5135, MinusLogProbMetric: 65.5135, val_loss: 65.6388, val_MinusLogProbMetric: 65.6388

Epoch 344: val_loss improved from 65.68864 to 65.63885, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 65.5135 - MinusLogProbMetric: 65.5135 - val_loss: 65.6388 - val_MinusLogProbMetric: 65.6388 - lr: 2.0576e-06 - 54s/epoch - 277ms/step
Epoch 345/1000
2023-10-30 21:19:37.749 
Epoch 345/1000 
	 loss: 65.4098, MinusLogProbMetric: 65.4098, val_loss: 65.5876, val_MinusLogProbMetric: 65.5876

Epoch 345: val_loss improved from 65.63885 to 65.58764, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 65.4098 - MinusLogProbMetric: 65.4098 - val_loss: 65.5876 - val_MinusLogProbMetric: 65.5876 - lr: 2.0576e-06 - 57s/epoch - 291ms/step
Epoch 346/1000
2023-10-30 21:20:32.925 
Epoch 346/1000 
	 loss: 65.3951, MinusLogProbMetric: 65.3951, val_loss: 65.6032, val_MinusLogProbMetric: 65.6032

Epoch 346: val_loss did not improve from 65.58764
196/196 - 54s - loss: 65.3951 - MinusLogProbMetric: 65.3951 - val_loss: 65.6032 - val_MinusLogProbMetric: 65.6032 - lr: 2.0576e-06 - 54s/epoch - 278ms/step
Epoch 347/1000
2023-10-30 21:21:27.744 
Epoch 347/1000 
	 loss: 66.2600, MinusLogProbMetric: 66.2600, val_loss: 68.3176, val_MinusLogProbMetric: 68.3176

Epoch 347: val_loss did not improve from 65.58764
196/196 - 55s - loss: 66.2600 - MinusLogProbMetric: 66.2600 - val_loss: 68.3176 - val_MinusLogProbMetric: 68.3176 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 348/1000
2023-10-30 21:22:25.243 
Epoch 348/1000 
	 loss: 65.9609, MinusLogProbMetric: 65.9609, val_loss: 65.8671, val_MinusLogProbMetric: 65.8671

Epoch 348: val_loss did not improve from 65.58764
196/196 - 57s - loss: 65.9609 - MinusLogProbMetric: 65.9609 - val_loss: 65.8671 - val_MinusLogProbMetric: 65.8671 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 349/1000
2023-10-30 21:23:18.994 
Epoch 349/1000 
	 loss: 65.5871, MinusLogProbMetric: 65.5871, val_loss: 65.7734, val_MinusLogProbMetric: 65.7734

Epoch 349: val_loss did not improve from 65.58764
196/196 - 54s - loss: 65.5871 - MinusLogProbMetric: 65.5871 - val_loss: 65.7734 - val_MinusLogProbMetric: 65.7734 - lr: 2.0576e-06 - 54s/epoch - 274ms/step
Epoch 350/1000
2023-10-30 21:24:14.497 
Epoch 350/1000 
	 loss: 65.4882, MinusLogProbMetric: 65.4882, val_loss: 65.7818, val_MinusLogProbMetric: 65.7818

Epoch 350: val_loss did not improve from 65.58764
196/196 - 55s - loss: 65.4882 - MinusLogProbMetric: 65.4882 - val_loss: 65.7818 - val_MinusLogProbMetric: 65.7818 - lr: 2.0576e-06 - 55s/epoch - 283ms/step
Epoch 351/1000
2023-10-30 21:25:10.540 
Epoch 351/1000 
	 loss: 65.4297, MinusLogProbMetric: 65.4297, val_loss: 65.7016, val_MinusLogProbMetric: 65.7016

Epoch 351: val_loss did not improve from 65.58764
196/196 - 56s - loss: 65.4297 - MinusLogProbMetric: 65.4297 - val_loss: 65.7016 - val_MinusLogProbMetric: 65.7016 - lr: 2.0576e-06 - 56s/epoch - 286ms/step
Epoch 352/1000
2023-10-30 21:26:05.462 
Epoch 352/1000 
	 loss: 65.3730, MinusLogProbMetric: 65.3730, val_loss: 65.5748, val_MinusLogProbMetric: 65.5748

Epoch 352: val_loss improved from 65.58764 to 65.57476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 65.3730 - MinusLogProbMetric: 65.3730 - val_loss: 65.5748 - val_MinusLogProbMetric: 65.5748 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 353/1000
2023-10-30 21:27:01.434 
Epoch 353/1000 
	 loss: 65.3172, MinusLogProbMetric: 65.3172, val_loss: 65.5266, val_MinusLogProbMetric: 65.5266

Epoch 353: val_loss improved from 65.57476 to 65.52655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 65.3172 - MinusLogProbMetric: 65.3172 - val_loss: 65.5266 - val_MinusLogProbMetric: 65.5266 - lr: 2.0576e-06 - 56s/epoch - 286ms/step
Epoch 354/1000
2023-10-30 21:28:00.978 
Epoch 354/1000 
	 loss: 65.2898, MinusLogProbMetric: 65.2898, val_loss: 65.4799, val_MinusLogProbMetric: 65.4799

Epoch 354: val_loss improved from 65.52655 to 65.47986, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 65.2898 - MinusLogProbMetric: 65.2898 - val_loss: 65.4799 - val_MinusLogProbMetric: 65.4799 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 355/1000
2023-10-30 21:28:55.993 
Epoch 355/1000 
	 loss: 66.3433, MinusLogProbMetric: 66.3433, val_loss: 77.2681, val_MinusLogProbMetric: 77.2681

Epoch 355: val_loss did not improve from 65.47986
196/196 - 54s - loss: 66.3433 - MinusLogProbMetric: 66.3433 - val_loss: 77.2681 - val_MinusLogProbMetric: 77.2681 - lr: 2.0576e-06 - 54s/epoch - 277ms/step
Epoch 356/1000
2023-10-30 21:29:48.434 
Epoch 356/1000 
	 loss: 66.7199, MinusLogProbMetric: 66.7199, val_loss: 65.6832, val_MinusLogProbMetric: 65.6832

Epoch 356: val_loss did not improve from 65.47986
196/196 - 52s - loss: 66.7199 - MinusLogProbMetric: 66.7199 - val_loss: 65.6832 - val_MinusLogProbMetric: 65.6832 - lr: 2.0576e-06 - 52s/epoch - 268ms/step
Epoch 357/1000
2023-10-30 21:30:47.511 
Epoch 357/1000 
	 loss: 65.3055, MinusLogProbMetric: 65.3055, val_loss: 65.4746, val_MinusLogProbMetric: 65.4746

Epoch 357: val_loss improved from 65.47986 to 65.47463, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 65.3055 - MinusLogProbMetric: 65.3055 - val_loss: 65.4746 - val_MinusLogProbMetric: 65.4746 - lr: 2.0576e-06 - 60s/epoch - 306ms/step
Epoch 358/1000
2023-10-30 21:31:46.970 
Epoch 358/1000 
	 loss: 65.2673, MinusLogProbMetric: 65.2673, val_loss: 65.3627, val_MinusLogProbMetric: 65.3627

Epoch 358: val_loss improved from 65.47463 to 65.36272, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 65.2673 - MinusLogProbMetric: 65.2673 - val_loss: 65.3627 - val_MinusLogProbMetric: 65.3627 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 359/1000
2023-10-30 21:32:47.473 
Epoch 359/1000 
	 loss: 65.1312, MinusLogProbMetric: 65.1312, val_loss: 65.3376, val_MinusLogProbMetric: 65.3376

Epoch 359: val_loss improved from 65.36272 to 65.33759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 65.1312 - MinusLogProbMetric: 65.1312 - val_loss: 65.3376 - val_MinusLogProbMetric: 65.3376 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 360/1000
2023-10-30 21:33:46.776 
Epoch 360/1000 
	 loss: 65.0737, MinusLogProbMetric: 65.0737, val_loss: 65.2690, val_MinusLogProbMetric: 65.2690

Epoch 360: val_loss improved from 65.33759 to 65.26897, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 65.0737 - MinusLogProbMetric: 65.0737 - val_loss: 65.2690 - val_MinusLogProbMetric: 65.2690 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 361/1000
2023-10-30 21:34:47.014 
Epoch 361/1000 
	 loss: 65.0333, MinusLogProbMetric: 65.0333, val_loss: 65.2332, val_MinusLogProbMetric: 65.2332

Epoch 361: val_loss improved from 65.26897 to 65.23315, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 65.0333 - MinusLogProbMetric: 65.0333 - val_loss: 65.2332 - val_MinusLogProbMetric: 65.2332 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 362/1000
2023-10-30 21:35:50.050 
Epoch 362/1000 
	 loss: 64.9942, MinusLogProbMetric: 64.9942, val_loss: 65.1793, val_MinusLogProbMetric: 65.1793

Epoch 362: val_loss improved from 65.23315 to 65.17930, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 64.9942 - MinusLogProbMetric: 64.9942 - val_loss: 65.1793 - val_MinusLogProbMetric: 65.1793 - lr: 2.0576e-06 - 63s/epoch - 322ms/step
Epoch 363/1000
2023-10-30 21:36:50.083 
Epoch 363/1000 
	 loss: 64.9573, MinusLogProbMetric: 64.9573, val_loss: 65.1409, val_MinusLogProbMetric: 65.1409

Epoch 363: val_loss improved from 65.17930 to 65.14087, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 64.9573 - MinusLogProbMetric: 64.9573 - val_loss: 65.1409 - val_MinusLogProbMetric: 65.1409 - lr: 2.0576e-06 - 60s/epoch - 306ms/step
Epoch 364/1000
2023-10-30 21:37:50.994 
Epoch 364/1000 
	 loss: 64.9029, MinusLogProbMetric: 64.9029, val_loss: 65.1114, val_MinusLogProbMetric: 65.1114

Epoch 364: val_loss improved from 65.14087 to 65.11138, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 64.9029 - MinusLogProbMetric: 64.9029 - val_loss: 65.1114 - val_MinusLogProbMetric: 65.1114 - lr: 2.0576e-06 - 61s/epoch - 311ms/step
Epoch 365/1000
2023-10-30 21:38:50.018 
Epoch 365/1000 
	 loss: 64.9039, MinusLogProbMetric: 64.9039, val_loss: 65.0875, val_MinusLogProbMetric: 65.0875

Epoch 365: val_loss improved from 65.11138 to 65.08749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 64.9039 - MinusLogProbMetric: 64.9039 - val_loss: 65.0875 - val_MinusLogProbMetric: 65.0875 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 366/1000
2023-10-30 21:39:51.009 
Epoch 366/1000 
	 loss: 64.8498, MinusLogProbMetric: 64.8498, val_loss: 65.0381, val_MinusLogProbMetric: 65.0381

Epoch 366: val_loss improved from 65.08749 to 65.03815, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 64.8498 - MinusLogProbMetric: 64.8498 - val_loss: 65.0381 - val_MinusLogProbMetric: 65.0381 - lr: 2.0576e-06 - 61s/epoch - 311ms/step
Epoch 367/1000
2023-10-30 21:40:50.322 
Epoch 367/1000 
	 loss: 64.8034, MinusLogProbMetric: 64.8034, val_loss: 65.0175, val_MinusLogProbMetric: 65.0175

Epoch 367: val_loss improved from 65.03815 to 65.01746, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 64.8034 - MinusLogProbMetric: 64.8034 - val_loss: 65.0175 - val_MinusLogProbMetric: 65.0175 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 368/1000
2023-10-30 21:41:49.118 
Epoch 368/1000 
	 loss: 64.8948, MinusLogProbMetric: 64.8948, val_loss: 65.0036, val_MinusLogProbMetric: 65.0036

Epoch 368: val_loss improved from 65.01746 to 65.00361, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 64.8948 - MinusLogProbMetric: 64.8948 - val_loss: 65.0036 - val_MinusLogProbMetric: 65.0036 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 369/1000
2023-10-30 21:42:47.410 
Epoch 369/1000 
	 loss: 64.7293, MinusLogProbMetric: 64.7293, val_loss: 65.0047, val_MinusLogProbMetric: 65.0047

Epoch 369: val_loss did not improve from 65.00361
196/196 - 57s - loss: 64.7293 - MinusLogProbMetric: 64.7293 - val_loss: 65.0047 - val_MinusLogProbMetric: 65.0047 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 370/1000
2023-10-30 21:43:45.502 
Epoch 370/1000 
	 loss: 64.7146, MinusLogProbMetric: 64.7146, val_loss: 64.9693, val_MinusLogProbMetric: 64.9693

Epoch 370: val_loss improved from 65.00361 to 64.96928, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 64.7146 - MinusLogProbMetric: 64.7146 - val_loss: 64.9693 - val_MinusLogProbMetric: 64.9693 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 371/1000
2023-10-30 21:44:43.699 
Epoch 371/1000 
	 loss: 64.6874, MinusLogProbMetric: 64.6874, val_loss: 64.8602, val_MinusLogProbMetric: 64.8602

Epoch 371: val_loss improved from 64.96928 to 64.86015, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 64.6874 - MinusLogProbMetric: 64.6874 - val_loss: 64.8602 - val_MinusLogProbMetric: 64.8602 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 372/1000
2023-10-30 21:45:42.071 
Epoch 372/1000 
	 loss: 64.6556, MinusLogProbMetric: 64.6556, val_loss: 64.9191, val_MinusLogProbMetric: 64.9191

Epoch 372: val_loss did not improve from 64.86015
196/196 - 57s - loss: 64.6556 - MinusLogProbMetric: 64.6556 - val_loss: 64.9191 - val_MinusLogProbMetric: 64.9191 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 373/1000
2023-10-30 21:46:38.585 
Epoch 373/1000 
	 loss: 64.5846, MinusLogProbMetric: 64.5846, val_loss: 64.7833, val_MinusLogProbMetric: 64.7833

Epoch 373: val_loss improved from 64.86015 to 64.78326, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 64.5846 - MinusLogProbMetric: 64.5846 - val_loss: 64.7833 - val_MinusLogProbMetric: 64.7833 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 374/1000
2023-10-30 21:47:39.271 
Epoch 374/1000 
	 loss: 64.5475, MinusLogProbMetric: 64.5475, val_loss: 64.7495, val_MinusLogProbMetric: 64.7495

Epoch 374: val_loss improved from 64.78326 to 64.74948, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 64.5475 - MinusLogProbMetric: 64.5475 - val_loss: 64.7495 - val_MinusLogProbMetric: 64.7495 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 375/1000
2023-10-30 21:48:36.570 
Epoch 375/1000 
	 loss: 64.5247, MinusLogProbMetric: 64.5247, val_loss: 64.7246, val_MinusLogProbMetric: 64.7246

Epoch 375: val_loss improved from 64.74948 to 64.72456, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 64.5247 - MinusLogProbMetric: 64.5247 - val_loss: 64.7246 - val_MinusLogProbMetric: 64.7246 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 376/1000
2023-10-30 21:49:35.990 
Epoch 376/1000 
	 loss: 64.4852, MinusLogProbMetric: 64.4852, val_loss: 64.6921, val_MinusLogProbMetric: 64.6921

Epoch 376: val_loss improved from 64.72456 to 64.69211, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 64.4852 - MinusLogProbMetric: 64.4852 - val_loss: 64.6921 - val_MinusLogProbMetric: 64.6921 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 377/1000
2023-10-30 21:50:35.467 
Epoch 377/1000 
	 loss: 64.4566, MinusLogProbMetric: 64.4566, val_loss: 64.7732, val_MinusLogProbMetric: 64.7732

Epoch 377: val_loss did not improve from 64.69211
196/196 - 59s - loss: 64.4566 - MinusLogProbMetric: 64.4566 - val_loss: 64.7732 - val_MinusLogProbMetric: 64.7732 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 378/1000
2023-10-30 21:51:33.351 
Epoch 378/1000 
	 loss: 64.4733, MinusLogProbMetric: 64.4733, val_loss: 64.9300, val_MinusLogProbMetric: 64.9300

Epoch 378: val_loss did not improve from 64.69211
196/196 - 58s - loss: 64.4733 - MinusLogProbMetric: 64.4733 - val_loss: 64.9300 - val_MinusLogProbMetric: 64.9300 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 379/1000
2023-10-30 21:52:28.763 
Epoch 379/1000 
	 loss: 65.4792, MinusLogProbMetric: 65.4792, val_loss: 65.6615, val_MinusLogProbMetric: 65.6615

Epoch 379: val_loss did not improve from 64.69211
196/196 - 55s - loss: 65.4792 - MinusLogProbMetric: 65.4792 - val_loss: 65.6615 - val_MinusLogProbMetric: 65.6615 - lr: 2.0576e-06 - 55s/epoch - 283ms/step
Epoch 380/1000
2023-10-30 21:53:24.314 
Epoch 380/1000 
	 loss: 64.8816, MinusLogProbMetric: 64.8816, val_loss: 64.6491, val_MinusLogProbMetric: 64.6491

Epoch 380: val_loss improved from 64.69211 to 64.64907, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 64.8816 - MinusLogProbMetric: 64.8816 - val_loss: 64.6491 - val_MinusLogProbMetric: 64.6491 - lr: 2.0576e-06 - 56s/epoch - 288ms/step
Epoch 381/1000
2023-10-30 21:54:22.419 
Epoch 381/1000 
	 loss: 64.3802, MinusLogProbMetric: 64.3802, val_loss: 64.5498, val_MinusLogProbMetric: 64.5498

Epoch 381: val_loss improved from 64.64907 to 64.54975, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 64.3802 - MinusLogProbMetric: 64.3802 - val_loss: 64.5498 - val_MinusLogProbMetric: 64.5498 - lr: 2.0576e-06 - 58s/epoch - 296ms/step
Epoch 382/1000
2023-10-30 21:55:20.062 
Epoch 382/1000 
	 loss: 64.3170, MinusLogProbMetric: 64.3170, val_loss: 64.5082, val_MinusLogProbMetric: 64.5082

Epoch 382: val_loss improved from 64.54975 to 64.50820, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 64.3170 - MinusLogProbMetric: 64.3170 - val_loss: 64.5082 - val_MinusLogProbMetric: 64.5082 - lr: 2.0576e-06 - 58s/epoch - 294ms/step
Epoch 383/1000
2023-10-30 21:56:21.897 
Epoch 383/1000 
	 loss: 64.2983, MinusLogProbMetric: 64.2983, val_loss: 64.4677, val_MinusLogProbMetric: 64.4677

Epoch 383: val_loss improved from 64.50820 to 64.46774, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 64.2983 - MinusLogProbMetric: 64.2983 - val_loss: 64.4677 - val_MinusLogProbMetric: 64.4677 - lr: 2.0576e-06 - 62s/epoch - 315ms/step
Epoch 384/1000
2023-10-30 21:57:24.373 
Epoch 384/1000 
	 loss: 64.2371, MinusLogProbMetric: 64.2371, val_loss: 64.4476, val_MinusLogProbMetric: 64.4476

Epoch 384: val_loss improved from 64.46774 to 64.44759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 64.2371 - MinusLogProbMetric: 64.2371 - val_loss: 64.4476 - val_MinusLogProbMetric: 64.4476 - lr: 2.0576e-06 - 62s/epoch - 319ms/step
Epoch 385/1000
2023-10-30 21:58:24.897 
Epoch 385/1000 
	 loss: 64.2030, MinusLogProbMetric: 64.2030, val_loss: 64.4650, val_MinusLogProbMetric: 64.4650

Epoch 385: val_loss did not improve from 64.44759
196/196 - 60s - loss: 64.2030 - MinusLogProbMetric: 64.2030 - val_loss: 64.4650 - val_MinusLogProbMetric: 64.4650 - lr: 2.0576e-06 - 60s/epoch - 304ms/step
Epoch 386/1000
2023-10-30 21:59:22.538 
Epoch 386/1000 
	 loss: 64.1885, MinusLogProbMetric: 64.1885, val_loss: 64.4137, val_MinusLogProbMetric: 64.4137

Epoch 386: val_loss improved from 64.44759 to 64.41366, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 64.1885 - MinusLogProbMetric: 64.1885 - val_loss: 64.4137 - val_MinusLogProbMetric: 64.4137 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 387/1000
2023-10-30 22:00:22.926 
Epoch 387/1000 
	 loss: 64.1401, MinusLogProbMetric: 64.1401, val_loss: 64.3505, val_MinusLogProbMetric: 64.3505

Epoch 387: val_loss improved from 64.41366 to 64.35052, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 64.1401 - MinusLogProbMetric: 64.1401 - val_loss: 64.3505 - val_MinusLogProbMetric: 64.3505 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 388/1000
2023-10-30 22:01:19.449 
Epoch 388/1000 
	 loss: 64.0800, MinusLogProbMetric: 64.0800, val_loss: 64.3003, val_MinusLogProbMetric: 64.3003

Epoch 388: val_loss improved from 64.35052 to 64.30025, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 64.0800 - MinusLogProbMetric: 64.0800 - val_loss: 64.3003 - val_MinusLogProbMetric: 64.3003 - lr: 2.0576e-06 - 57s/epoch - 289ms/step
Epoch 389/1000
2023-10-30 22:02:20.623 
Epoch 389/1000 
	 loss: 64.0499, MinusLogProbMetric: 64.0499, val_loss: 64.2402, val_MinusLogProbMetric: 64.2402

Epoch 389: val_loss improved from 64.30025 to 64.24021, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 64.0499 - MinusLogProbMetric: 64.0499 - val_loss: 64.2402 - val_MinusLogProbMetric: 64.2402 - lr: 2.0576e-06 - 61s/epoch - 313ms/step
Epoch 390/1000
2023-10-30 22:03:20.708 
Epoch 390/1000 
	 loss: 64.0351, MinusLogProbMetric: 64.0351, val_loss: 64.2627, val_MinusLogProbMetric: 64.2627

Epoch 390: val_loss did not improve from 64.24021
196/196 - 59s - loss: 64.0351 - MinusLogProbMetric: 64.0351 - val_loss: 64.2627 - val_MinusLogProbMetric: 64.2627 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 391/1000
2023-10-30 22:04:19.347 
Epoch 391/1000 
	 loss: 64.1686, MinusLogProbMetric: 64.1686, val_loss: 64.1898, val_MinusLogProbMetric: 64.1898

Epoch 391: val_loss improved from 64.24021 to 64.18975, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 64.1686 - MinusLogProbMetric: 64.1686 - val_loss: 64.1898 - val_MinusLogProbMetric: 64.1898 - lr: 2.0576e-06 - 60s/epoch - 304ms/step
Epoch 392/1000
2023-10-30 22:05:19.950 
Epoch 392/1000 
	 loss: 63.9771, MinusLogProbMetric: 63.9771, val_loss: 64.1516, val_MinusLogProbMetric: 64.1516

Epoch 392: val_loss improved from 64.18975 to 64.15158, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 63.9771 - MinusLogProbMetric: 63.9771 - val_loss: 64.1516 - val_MinusLogProbMetric: 64.1516 - lr: 2.0576e-06 - 61s/epoch - 309ms/step
Epoch 393/1000
2023-10-30 22:06:20.233 
Epoch 393/1000 
	 loss: 64.1396, MinusLogProbMetric: 64.1396, val_loss: 64.1587, val_MinusLogProbMetric: 64.1587

Epoch 393: val_loss did not improve from 64.15158
196/196 - 59s - loss: 64.1396 - MinusLogProbMetric: 64.1396 - val_loss: 64.1587 - val_MinusLogProbMetric: 64.1587 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 394/1000
2023-10-30 22:07:18.136 
Epoch 394/1000 
	 loss: 63.8984, MinusLogProbMetric: 63.8984, val_loss: 64.1173, val_MinusLogProbMetric: 64.1173

Epoch 394: val_loss improved from 64.15158 to 64.11732, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 63.8984 - MinusLogProbMetric: 63.8984 - val_loss: 64.1173 - val_MinusLogProbMetric: 64.1173 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 395/1000
2023-10-30 22:08:16.011 
Epoch 395/1000 
	 loss: 63.8575, MinusLogProbMetric: 63.8575, val_loss: 64.0606, val_MinusLogProbMetric: 64.0606

Epoch 395: val_loss improved from 64.11732 to 64.06059, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 63.8575 - MinusLogProbMetric: 63.8575 - val_loss: 64.0606 - val_MinusLogProbMetric: 64.0606 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 396/1000
2023-10-30 22:09:11.791 
Epoch 396/1000 
	 loss: 63.8366, MinusLogProbMetric: 63.8366, val_loss: 64.0947, val_MinusLogProbMetric: 64.0947

Epoch 396: val_loss did not improve from 64.06059
196/196 - 55s - loss: 63.8366 - MinusLogProbMetric: 63.8366 - val_loss: 64.0947 - val_MinusLogProbMetric: 64.0947 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 397/1000
2023-10-30 22:10:07.540 
Epoch 397/1000 
	 loss: 63.8199, MinusLogProbMetric: 63.8199, val_loss: 64.0189, val_MinusLogProbMetric: 64.0189

Epoch 397: val_loss improved from 64.06059 to 64.01894, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 63.8199 - MinusLogProbMetric: 63.8199 - val_loss: 64.0189 - val_MinusLogProbMetric: 64.0189 - lr: 2.0576e-06 - 57s/epoch - 289ms/step
Epoch 398/1000
2023-10-30 22:11:07.279 
Epoch 398/1000 
	 loss: 63.7705, MinusLogProbMetric: 63.7705, val_loss: 64.0288, val_MinusLogProbMetric: 64.0288

Epoch 398: val_loss did not improve from 64.01894
196/196 - 59s - loss: 63.7705 - MinusLogProbMetric: 63.7705 - val_loss: 64.0288 - val_MinusLogProbMetric: 64.0288 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 399/1000
2023-10-30 22:12:05.019 
Epoch 399/1000 
	 loss: 63.7464, MinusLogProbMetric: 63.7464, val_loss: 63.9416, val_MinusLogProbMetric: 63.9416

Epoch 399: val_loss improved from 64.01894 to 63.94156, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 63.7464 - MinusLogProbMetric: 63.7464 - val_loss: 63.9416 - val_MinusLogProbMetric: 63.9416 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 400/1000
2023-10-30 22:13:04.994 
Epoch 400/1000 
	 loss: 63.7142, MinusLogProbMetric: 63.7142, val_loss: 63.9370, val_MinusLogProbMetric: 63.9370

Epoch 400: val_loss improved from 63.94156 to 63.93701, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 63.7142 - MinusLogProbMetric: 63.7142 - val_loss: 63.9370 - val_MinusLogProbMetric: 63.9370 - lr: 2.0576e-06 - 60s/epoch - 306ms/step
Epoch 401/1000
2023-10-30 22:14:02.689 
Epoch 401/1000 
	 loss: 63.6807, MinusLogProbMetric: 63.6807, val_loss: 63.9352, val_MinusLogProbMetric: 63.9352

Epoch 401: val_loss improved from 63.93701 to 63.93523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 63.6807 - MinusLogProbMetric: 63.6807 - val_loss: 63.9352 - val_MinusLogProbMetric: 63.9352 - lr: 2.0576e-06 - 58s/epoch - 294ms/step
Epoch 402/1000
2023-10-30 22:14:58.476 
Epoch 402/1000 
	 loss: 63.6562, MinusLogProbMetric: 63.6562, val_loss: 63.8932, val_MinusLogProbMetric: 63.8932

Epoch 402: val_loss improved from 63.93523 to 63.89320, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 63.6562 - MinusLogProbMetric: 63.6562 - val_loss: 63.8932 - val_MinusLogProbMetric: 63.8932 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 403/1000
2023-10-30 22:15:54.645 
Epoch 403/1000 
	 loss: 63.6343, MinusLogProbMetric: 63.6343, val_loss: 63.8579, val_MinusLogProbMetric: 63.8579

Epoch 403: val_loss improved from 63.89320 to 63.85789, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 63.6343 - MinusLogProbMetric: 63.6343 - val_loss: 63.8579 - val_MinusLogProbMetric: 63.8579 - lr: 2.0576e-06 - 56s/epoch - 286ms/step
Epoch 404/1000
2023-10-30 22:16:52.357 
Epoch 404/1000 
	 loss: 63.5895, MinusLogProbMetric: 63.5895, val_loss: 63.8219, val_MinusLogProbMetric: 63.8219

Epoch 404: val_loss improved from 63.85789 to 63.82185, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 63.5895 - MinusLogProbMetric: 63.5895 - val_loss: 63.8219 - val_MinusLogProbMetric: 63.8219 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 405/1000
2023-10-30 22:17:53.191 
Epoch 405/1000 
	 loss: 63.5599, MinusLogProbMetric: 63.5599, val_loss: 63.7915, val_MinusLogProbMetric: 63.7915

Epoch 405: val_loss improved from 63.82185 to 63.79154, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 63.5599 - MinusLogProbMetric: 63.5599 - val_loss: 63.7915 - val_MinusLogProbMetric: 63.7915 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 406/1000
2023-10-30 22:18:51.118 
Epoch 406/1000 
	 loss: 63.5327, MinusLogProbMetric: 63.5327, val_loss: 63.8694, val_MinusLogProbMetric: 63.8694

Epoch 406: val_loss did not improve from 63.79154
196/196 - 57s - loss: 63.5327 - MinusLogProbMetric: 63.5327 - val_loss: 63.8694 - val_MinusLogProbMetric: 63.8694 - lr: 2.0576e-06 - 57s/epoch - 291ms/step
Epoch 407/1000
2023-10-30 22:19:48.638 
Epoch 407/1000 
	 loss: 63.5025, MinusLogProbMetric: 63.5025, val_loss: 63.7295, val_MinusLogProbMetric: 63.7295

Epoch 407: val_loss improved from 63.79154 to 63.72947, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 63.5025 - MinusLogProbMetric: 63.5025 - val_loss: 63.7295 - val_MinusLogProbMetric: 63.7295 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 408/1000
2023-10-30 22:20:48.526 
Epoch 408/1000 
	 loss: 63.4678, MinusLogProbMetric: 63.4678, val_loss: 63.6940, val_MinusLogProbMetric: 63.6940

Epoch 408: val_loss improved from 63.72947 to 63.69402, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 63.4678 - MinusLogProbMetric: 63.4678 - val_loss: 63.6940 - val_MinusLogProbMetric: 63.6940 - lr: 2.0576e-06 - 60s/epoch - 306ms/step
Epoch 409/1000
2023-10-30 22:21:47.860 
Epoch 409/1000 
	 loss: 63.4568, MinusLogProbMetric: 63.4568, val_loss: 63.6602, val_MinusLogProbMetric: 63.6602

Epoch 409: val_loss improved from 63.69402 to 63.66016, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 63.4568 - MinusLogProbMetric: 63.4568 - val_loss: 63.6602 - val_MinusLogProbMetric: 63.6602 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 410/1000
2023-10-30 22:22:47.880 
Epoch 410/1000 
	 loss: 63.3973, MinusLogProbMetric: 63.3973, val_loss: 63.6344, val_MinusLogProbMetric: 63.6344

Epoch 410: val_loss improved from 63.66016 to 63.63436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 63.3973 - MinusLogProbMetric: 63.3973 - val_loss: 63.6344 - val_MinusLogProbMetric: 63.6344 - lr: 2.0576e-06 - 60s/epoch - 306ms/step
Epoch 411/1000
2023-10-30 22:23:49.682 
Epoch 411/1000 
	 loss: 63.4165, MinusLogProbMetric: 63.4165, val_loss: 63.6159, val_MinusLogProbMetric: 63.6159

Epoch 411: val_loss improved from 63.63436 to 63.61588, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 63.4165 - MinusLogProbMetric: 63.4165 - val_loss: 63.6159 - val_MinusLogProbMetric: 63.6159 - lr: 2.0576e-06 - 62s/epoch - 315ms/step
Epoch 412/1000
2023-10-30 22:24:49.182 
Epoch 412/1000 
	 loss: 63.3918, MinusLogProbMetric: 63.3918, val_loss: 63.5512, val_MinusLogProbMetric: 63.5512

Epoch 412: val_loss improved from 63.61588 to 63.55115, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 63.3918 - MinusLogProbMetric: 63.3918 - val_loss: 63.5512 - val_MinusLogProbMetric: 63.5512 - lr: 2.0576e-06 - 59s/epoch - 304ms/step
Epoch 413/1000
2023-10-30 22:25:48.350 
Epoch 413/1000 
	 loss: 63.3911, MinusLogProbMetric: 63.3911, val_loss: 63.5670, val_MinusLogProbMetric: 63.5670

Epoch 413: val_loss did not improve from 63.55115
196/196 - 58s - loss: 63.3911 - MinusLogProbMetric: 63.3911 - val_loss: 63.5670 - val_MinusLogProbMetric: 63.5670 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 414/1000
2023-10-30 22:26:46.690 
Epoch 414/1000 
	 loss: 63.3241, MinusLogProbMetric: 63.3241, val_loss: 63.5547, val_MinusLogProbMetric: 63.5547

Epoch 414: val_loss did not improve from 63.55115
196/196 - 58s - loss: 63.3241 - MinusLogProbMetric: 63.3241 - val_loss: 63.5547 - val_MinusLogProbMetric: 63.5547 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 415/1000
2023-10-30 22:27:47.690 
Epoch 415/1000 
	 loss: 63.2799, MinusLogProbMetric: 63.2799, val_loss: 63.4844, val_MinusLogProbMetric: 63.4844

Epoch 415: val_loss improved from 63.55115 to 63.48439, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 63.2799 - MinusLogProbMetric: 63.2799 - val_loss: 63.4844 - val_MinusLogProbMetric: 63.4844 - lr: 2.0576e-06 - 62s/epoch - 316ms/step
Epoch 416/1000
2023-10-30 22:28:45.625 
Epoch 416/1000 
	 loss: 63.2456, MinusLogProbMetric: 63.2456, val_loss: 63.4640, val_MinusLogProbMetric: 63.4640

Epoch 416: val_loss improved from 63.48439 to 63.46402, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 63.2456 - MinusLogProbMetric: 63.2456 - val_loss: 63.4640 - val_MinusLogProbMetric: 63.4640 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 417/1000
2023-10-30 22:29:47.115 
Epoch 417/1000 
	 loss: 63.1905, MinusLogProbMetric: 63.1905, val_loss: 63.4639, val_MinusLogProbMetric: 63.4639

Epoch 417: val_loss improved from 63.46402 to 63.46391, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 63.1905 - MinusLogProbMetric: 63.1905 - val_loss: 63.4639 - val_MinusLogProbMetric: 63.4639 - lr: 2.0576e-06 - 61s/epoch - 313ms/step
Epoch 418/1000
2023-10-30 22:30:46.377 
Epoch 418/1000 
	 loss: 63.1953, MinusLogProbMetric: 63.1953, val_loss: 63.3859, val_MinusLogProbMetric: 63.3859

Epoch 418: val_loss improved from 63.46391 to 63.38585, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 63.1953 - MinusLogProbMetric: 63.1953 - val_loss: 63.3859 - val_MinusLogProbMetric: 63.3859 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 419/1000
2023-10-30 22:31:44.571 
Epoch 419/1000 
	 loss: 63.1408, MinusLogProbMetric: 63.1408, val_loss: 63.3726, val_MinusLogProbMetric: 63.3726

Epoch 419: val_loss improved from 63.38585 to 63.37263, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 63.1408 - MinusLogProbMetric: 63.1408 - val_loss: 63.3726 - val_MinusLogProbMetric: 63.3726 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 420/1000
2023-10-30 22:32:44.225 
Epoch 420/1000 
	 loss: 63.1160, MinusLogProbMetric: 63.1160, val_loss: 63.3753, val_MinusLogProbMetric: 63.3753

Epoch 420: val_loss did not improve from 63.37263
196/196 - 59s - loss: 63.1160 - MinusLogProbMetric: 63.1160 - val_loss: 63.3753 - val_MinusLogProbMetric: 63.3753 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 421/1000
2023-10-30 22:33:42.480 
Epoch 421/1000 
	 loss: 63.1127, MinusLogProbMetric: 63.1127, val_loss: 63.3382, val_MinusLogProbMetric: 63.3382

Epoch 421: val_loss improved from 63.37263 to 63.33825, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 63.1127 - MinusLogProbMetric: 63.1127 - val_loss: 63.3382 - val_MinusLogProbMetric: 63.3382 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 422/1000
2023-10-30 22:34:40.430 
Epoch 422/1000 
	 loss: 63.1310, MinusLogProbMetric: 63.1310, val_loss: 63.2680, val_MinusLogProbMetric: 63.2680

Epoch 422: val_loss improved from 63.33825 to 63.26798, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 63.1310 - MinusLogProbMetric: 63.1310 - val_loss: 63.2680 - val_MinusLogProbMetric: 63.2680 - lr: 2.0576e-06 - 58s/epoch - 296ms/step
Epoch 423/1000
2023-10-30 22:35:38.758 
Epoch 423/1000 
	 loss: 63.1858, MinusLogProbMetric: 63.1858, val_loss: 63.8624, val_MinusLogProbMetric: 63.8624

Epoch 423: val_loss did not improve from 63.26798
196/196 - 57s - loss: 63.1858 - MinusLogProbMetric: 63.1858 - val_loss: 63.8624 - val_MinusLogProbMetric: 63.8624 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 424/1000
2023-10-30 22:36:37.039 
Epoch 424/1000 
	 loss: 63.1373, MinusLogProbMetric: 63.1373, val_loss: 63.2315, val_MinusLogProbMetric: 63.2315

Epoch 424: val_loss improved from 63.26798 to 63.23149, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 63.1373 - MinusLogProbMetric: 63.1373 - val_loss: 63.2315 - val_MinusLogProbMetric: 63.2315 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 425/1000
2023-10-30 22:37:37.135 
Epoch 425/1000 
	 loss: 63.0520, MinusLogProbMetric: 63.0520, val_loss: 63.2001, val_MinusLogProbMetric: 63.2001

Epoch 425: val_loss improved from 63.23149 to 63.20012, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 63.0520 - MinusLogProbMetric: 63.0520 - val_loss: 63.2001 - val_MinusLogProbMetric: 63.2001 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 426/1000
2023-10-30 22:38:36.934 
Epoch 426/1000 
	 loss: 63.0150, MinusLogProbMetric: 63.0150, val_loss: 63.1968, val_MinusLogProbMetric: 63.1968

Epoch 426: val_loss improved from 63.20012 to 63.19682, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 63.0150 - MinusLogProbMetric: 63.0150 - val_loss: 63.1968 - val_MinusLogProbMetric: 63.1968 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 427/1000
2023-10-30 22:39:36.190 
Epoch 427/1000 
	 loss: 63.1408, MinusLogProbMetric: 63.1408, val_loss: 63.0839, val_MinusLogProbMetric: 63.0839

Epoch 427: val_loss improved from 63.19682 to 63.08389, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 63.1408 - MinusLogProbMetric: 63.1408 - val_loss: 63.0839 - val_MinusLogProbMetric: 63.0839 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 428/1000
2023-10-30 22:40:34.652 
Epoch 428/1000 
	 loss: 62.9296, MinusLogProbMetric: 62.9296, val_loss: 63.1493, val_MinusLogProbMetric: 63.1493

Epoch 428: val_loss did not improve from 63.08389
196/196 - 58s - loss: 62.9296 - MinusLogProbMetric: 62.9296 - val_loss: 63.1493 - val_MinusLogProbMetric: 63.1493 - lr: 2.0576e-06 - 58s/epoch - 294ms/step
Epoch 429/1000
2023-10-30 22:41:31.019 
Epoch 429/1000 
	 loss: 62.8541, MinusLogProbMetric: 62.8541, val_loss: 63.1462, val_MinusLogProbMetric: 63.1462

Epoch 429: val_loss did not improve from 63.08389
196/196 - 56s - loss: 62.8541 - MinusLogProbMetric: 62.8541 - val_loss: 63.1462 - val_MinusLogProbMetric: 63.1462 - lr: 2.0576e-06 - 56s/epoch - 288ms/step
Epoch 430/1000
2023-10-30 22:42:24.588 
Epoch 430/1000 
	 loss: 63.0619, MinusLogProbMetric: 63.0619, val_loss: 70.2045, val_MinusLogProbMetric: 70.2045

Epoch 430: val_loss did not improve from 63.08389
196/196 - 54s - loss: 63.0619 - MinusLogProbMetric: 63.0619 - val_loss: 70.2045 - val_MinusLogProbMetric: 70.2045 - lr: 2.0576e-06 - 54s/epoch - 273ms/step
Epoch 431/1000
2023-10-30 22:43:20.877 
Epoch 431/1000 
	 loss: 64.3359, MinusLogProbMetric: 64.3359, val_loss: 63.2608, val_MinusLogProbMetric: 63.2608

Epoch 431: val_loss did not improve from 63.08389
196/196 - 56s - loss: 64.3359 - MinusLogProbMetric: 64.3359 - val_loss: 63.2608 - val_MinusLogProbMetric: 63.2608 - lr: 2.0576e-06 - 56s/epoch - 287ms/step
Epoch 432/1000
2023-10-30 22:44:17.708 
Epoch 432/1000 
	 loss: 62.8819, MinusLogProbMetric: 62.8819, val_loss: 63.0818, val_MinusLogProbMetric: 63.0818

Epoch 432: val_loss improved from 63.08389 to 63.08180, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 62.8819 - MinusLogProbMetric: 62.8819 - val_loss: 63.0818 - val_MinusLogProbMetric: 63.0818 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 433/1000
2023-10-30 22:45:16.881 
Epoch 433/1000 
	 loss: 62.7937, MinusLogProbMetric: 62.7937, val_loss: 63.0286, val_MinusLogProbMetric: 63.0286

Epoch 433: val_loss improved from 63.08180 to 63.02862, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 62.7937 - MinusLogProbMetric: 62.7937 - val_loss: 63.0286 - val_MinusLogProbMetric: 63.0286 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 434/1000
2023-10-30 22:46:14.909 
Epoch 434/1000 
	 loss: 62.8508, MinusLogProbMetric: 62.8508, val_loss: 63.1147, val_MinusLogProbMetric: 63.1147

Epoch 434: val_loss did not improve from 63.02862
196/196 - 57s - loss: 62.8508 - MinusLogProbMetric: 62.8508 - val_loss: 63.1147 - val_MinusLogProbMetric: 63.1147 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 435/1000
2023-10-30 22:47:12.173 
Epoch 435/1000 
	 loss: 62.8128, MinusLogProbMetric: 62.8128, val_loss: 62.9629, val_MinusLogProbMetric: 62.9629

Epoch 435: val_loss improved from 63.02862 to 62.96294, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 62.8128 - MinusLogProbMetric: 62.8128 - val_loss: 62.9629 - val_MinusLogProbMetric: 62.9629 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 436/1000
2023-10-30 22:48:13.094 
Epoch 436/1000 
	 loss: 62.7867, MinusLogProbMetric: 62.7867, val_loss: 62.8857, val_MinusLogProbMetric: 62.8857

Epoch 436: val_loss improved from 62.96294 to 62.88573, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 62.7867 - MinusLogProbMetric: 62.7867 - val_loss: 62.8857 - val_MinusLogProbMetric: 62.8857 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 437/1000
2023-10-30 22:49:12.207 
Epoch 437/1000 
	 loss: 62.6852, MinusLogProbMetric: 62.6852, val_loss: 63.0380, val_MinusLogProbMetric: 63.0380

Epoch 437: val_loss did not improve from 62.88573
196/196 - 58s - loss: 62.6852 - MinusLogProbMetric: 62.6852 - val_loss: 63.0380 - val_MinusLogProbMetric: 63.0380 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 438/1000
2023-10-30 22:50:13.863 
Epoch 438/1000 
	 loss: 62.6865, MinusLogProbMetric: 62.6865, val_loss: 62.8780, val_MinusLogProbMetric: 62.8780

Epoch 438: val_loss improved from 62.88573 to 62.87804, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 62.6865 - MinusLogProbMetric: 62.6865 - val_loss: 62.8780 - val_MinusLogProbMetric: 62.8780 - lr: 2.0576e-06 - 63s/epoch - 319ms/step
Epoch 439/1000
2023-10-30 22:51:14.290 
Epoch 439/1000 
	 loss: 62.6179, MinusLogProbMetric: 62.6179, val_loss: 62.8222, val_MinusLogProbMetric: 62.8222

Epoch 439: val_loss improved from 62.87804 to 62.82217, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 62.6179 - MinusLogProbMetric: 62.6179 - val_loss: 62.8222 - val_MinusLogProbMetric: 62.8222 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 440/1000
2023-10-30 22:52:17.221 
Epoch 440/1000 
	 loss: 62.5758, MinusLogProbMetric: 62.5758, val_loss: 62.8115, val_MinusLogProbMetric: 62.8115

Epoch 440: val_loss improved from 62.82217 to 62.81146, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 62.5758 - MinusLogProbMetric: 62.5758 - val_loss: 62.8115 - val_MinusLogProbMetric: 62.8115 - lr: 2.0576e-06 - 63s/epoch - 321ms/step
Epoch 441/1000
2023-10-30 22:53:17.443 
Epoch 441/1000 
	 loss: 62.5462, MinusLogProbMetric: 62.5462, val_loss: 62.7487, val_MinusLogProbMetric: 62.7487

Epoch 441: val_loss improved from 62.81146 to 62.74866, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 62.5462 - MinusLogProbMetric: 62.5462 - val_loss: 62.7487 - val_MinusLogProbMetric: 62.7487 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 442/1000
2023-10-30 22:54:20.411 
Epoch 442/1000 
	 loss: 62.5345, MinusLogProbMetric: 62.5345, val_loss: 62.7366, val_MinusLogProbMetric: 62.7366

Epoch 442: val_loss improved from 62.74866 to 62.73658, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 62.5345 - MinusLogProbMetric: 62.5345 - val_loss: 62.7366 - val_MinusLogProbMetric: 62.7366 - lr: 2.0576e-06 - 63s/epoch - 321ms/step
Epoch 443/1000
2023-10-30 22:55:21.929 
Epoch 443/1000 
	 loss: 69.1094, MinusLogProbMetric: 69.1094, val_loss: 72.6667, val_MinusLogProbMetric: 72.6667

Epoch 443: val_loss did not improve from 62.73658
196/196 - 61s - loss: 69.1094 - MinusLogProbMetric: 69.1094 - val_loss: 72.6667 - val_MinusLogProbMetric: 72.6667 - lr: 2.0576e-06 - 61s/epoch - 309ms/step
Epoch 444/1000
2023-10-30 22:56:20.939 
Epoch 444/1000 
	 loss: 67.0436, MinusLogProbMetric: 67.0436, val_loss: 65.0462, val_MinusLogProbMetric: 65.0462

Epoch 444: val_loss did not improve from 62.73658
196/196 - 59s - loss: 67.0436 - MinusLogProbMetric: 67.0436 - val_loss: 65.0462 - val_MinusLogProbMetric: 65.0462 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 445/1000
2023-10-30 22:57:17.922 
Epoch 445/1000 
	 loss: 64.3795, MinusLogProbMetric: 64.3795, val_loss: 64.1604, val_MinusLogProbMetric: 64.1604

Epoch 445: val_loss did not improve from 62.73658
196/196 - 57s - loss: 64.3795 - MinusLogProbMetric: 64.3795 - val_loss: 64.1604 - val_MinusLogProbMetric: 64.1604 - lr: 2.0576e-06 - 57s/epoch - 291ms/step
Epoch 446/1000
2023-10-30 22:58:15.395 
Epoch 446/1000 
	 loss: 63.7659, MinusLogProbMetric: 63.7659, val_loss: 63.7246, val_MinusLogProbMetric: 63.7246

Epoch 446: val_loss did not improve from 62.73658
196/196 - 57s - loss: 63.7659 - MinusLogProbMetric: 63.7659 - val_loss: 63.7246 - val_MinusLogProbMetric: 63.7246 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 447/1000
2023-10-30 22:59:12.102 
Epoch 447/1000 
	 loss: 63.4479, MinusLogProbMetric: 63.4479, val_loss: 63.4894, val_MinusLogProbMetric: 63.4894

Epoch 447: val_loss did not improve from 62.73658
196/196 - 57s - loss: 63.4479 - MinusLogProbMetric: 63.4479 - val_loss: 63.4894 - val_MinusLogProbMetric: 63.4894 - lr: 2.0576e-06 - 57s/epoch - 289ms/step
Epoch 448/1000
2023-10-30 23:00:09.590 
Epoch 448/1000 
	 loss: 63.1722, MinusLogProbMetric: 63.1722, val_loss: 63.3847, val_MinusLogProbMetric: 63.3847

Epoch 448: val_loss did not improve from 62.73658
196/196 - 57s - loss: 63.1722 - MinusLogProbMetric: 63.1722 - val_loss: 63.3847 - val_MinusLogProbMetric: 63.3847 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 449/1000
2023-10-30 23:01:10.292 
Epoch 449/1000 
	 loss: 63.0652, MinusLogProbMetric: 63.0652, val_loss: 63.2688, val_MinusLogProbMetric: 63.2688

Epoch 449: val_loss did not improve from 62.73658
196/196 - 61s - loss: 63.0652 - MinusLogProbMetric: 63.0652 - val_loss: 63.2688 - val_MinusLogProbMetric: 63.2688 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 450/1000
2023-10-30 23:02:08.957 
Epoch 450/1000 
	 loss: 62.9489, MinusLogProbMetric: 62.9489, val_loss: 63.1350, val_MinusLogProbMetric: 63.1350

Epoch 450: val_loss did not improve from 62.73658
196/196 - 59s - loss: 62.9489 - MinusLogProbMetric: 62.9489 - val_loss: 63.1350 - val_MinusLogProbMetric: 63.1350 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 451/1000
2023-10-30 23:03:11.071 
Epoch 451/1000 
	 loss: 62.9024, MinusLogProbMetric: 62.9024, val_loss: 63.3512, val_MinusLogProbMetric: 63.3512

Epoch 451: val_loss did not improve from 62.73658
196/196 - 62s - loss: 62.9024 - MinusLogProbMetric: 62.9024 - val_loss: 63.3512 - val_MinusLogProbMetric: 63.3512 - lr: 2.0576e-06 - 62s/epoch - 317ms/step
Epoch 452/1000
2023-10-30 23:04:11.877 
Epoch 452/1000 
	 loss: 62.7989, MinusLogProbMetric: 62.7989, val_loss: 62.9906, val_MinusLogProbMetric: 62.9906

Epoch 452: val_loss did not improve from 62.73658
196/196 - 61s - loss: 62.7989 - MinusLogProbMetric: 62.7989 - val_loss: 62.9906 - val_MinusLogProbMetric: 62.9906 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 453/1000
2023-10-30 23:05:10.436 
Epoch 453/1000 
	 loss: 62.7062, MinusLogProbMetric: 62.7062, val_loss: 62.8703, val_MinusLogProbMetric: 62.8703

Epoch 453: val_loss did not improve from 62.73658
196/196 - 59s - loss: 62.7062 - MinusLogProbMetric: 62.7062 - val_loss: 62.8703 - val_MinusLogProbMetric: 62.8703 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 454/1000
2023-10-30 23:06:08.636 
Epoch 454/1000 
	 loss: 62.6067, MinusLogProbMetric: 62.6067, val_loss: 62.7937, val_MinusLogProbMetric: 62.7937

Epoch 454: val_loss did not improve from 62.73658
196/196 - 58s - loss: 62.6067 - MinusLogProbMetric: 62.6067 - val_loss: 62.7937 - val_MinusLogProbMetric: 62.7937 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 455/1000
2023-10-30 23:07:06.018 
Epoch 455/1000 
	 loss: 62.5283, MinusLogProbMetric: 62.5283, val_loss: 62.7628, val_MinusLogProbMetric: 62.7628

Epoch 455: val_loss did not improve from 62.73658
196/196 - 57s - loss: 62.5283 - MinusLogProbMetric: 62.5283 - val_loss: 62.7628 - val_MinusLogProbMetric: 62.7628 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 456/1000
2023-10-30 23:08:05.256 
Epoch 456/1000 
	 loss: 62.4869, MinusLogProbMetric: 62.4869, val_loss: 62.6691, val_MinusLogProbMetric: 62.6691

Epoch 456: val_loss improved from 62.73658 to 62.66915, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 62.4869 - MinusLogProbMetric: 62.4869 - val_loss: 62.6691 - val_MinusLogProbMetric: 62.6691 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 457/1000
2023-10-30 23:09:05.151 
Epoch 457/1000 
	 loss: 62.4568, MinusLogProbMetric: 62.4568, val_loss: 62.6637, val_MinusLogProbMetric: 62.6637

Epoch 457: val_loss improved from 62.66915 to 62.66367, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 62.4568 - MinusLogProbMetric: 62.4568 - val_loss: 62.6637 - val_MinusLogProbMetric: 62.6637 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 458/1000
2023-10-30 23:10:06.849 
Epoch 458/1000 
	 loss: 62.4170, MinusLogProbMetric: 62.4170, val_loss: 62.6114, val_MinusLogProbMetric: 62.6114

Epoch 458: val_loss improved from 62.66367 to 62.61144, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 62.4170 - MinusLogProbMetric: 62.4170 - val_loss: 62.6114 - val_MinusLogProbMetric: 62.6114 - lr: 2.0576e-06 - 62s/epoch - 314ms/step
Epoch 459/1000
2023-10-30 23:11:05.966 
Epoch 459/1000 
	 loss: 62.3460, MinusLogProbMetric: 62.3460, val_loss: 62.5427, val_MinusLogProbMetric: 62.5427

Epoch 459: val_loss improved from 62.61144 to 62.54272, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 62.3460 - MinusLogProbMetric: 62.3460 - val_loss: 62.5427 - val_MinusLogProbMetric: 62.5427 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 460/1000
2023-10-30 23:12:04.663 
Epoch 460/1000 
	 loss: 65.6057, MinusLogProbMetric: 65.6057, val_loss: 63.4537, val_MinusLogProbMetric: 63.4537

Epoch 460: val_loss did not improve from 62.54272
196/196 - 58s - loss: 65.6057 - MinusLogProbMetric: 65.6057 - val_loss: 63.4537 - val_MinusLogProbMetric: 63.4537 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 461/1000
2023-10-30 23:13:01.816 
Epoch 461/1000 
	 loss: 63.1829, MinusLogProbMetric: 63.1829, val_loss: 62.9729, val_MinusLogProbMetric: 62.9729

Epoch 461: val_loss did not improve from 62.54272
196/196 - 57s - loss: 63.1829 - MinusLogProbMetric: 63.1829 - val_loss: 62.9729 - val_MinusLogProbMetric: 62.9729 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 462/1000
2023-10-30 23:13:59.438 
Epoch 462/1000 
	 loss: 62.7256, MinusLogProbMetric: 62.7256, val_loss: 62.9222, val_MinusLogProbMetric: 62.9222

Epoch 462: val_loss did not improve from 62.54272
196/196 - 58s - loss: 62.7256 - MinusLogProbMetric: 62.7256 - val_loss: 62.9222 - val_MinusLogProbMetric: 62.9222 - lr: 2.0576e-06 - 58s/epoch - 294ms/step
Epoch 463/1000
2023-10-30 23:14:58.047 
Epoch 463/1000 
	 loss: 62.5260, MinusLogProbMetric: 62.5260, val_loss: 62.6491, val_MinusLogProbMetric: 62.6491

Epoch 463: val_loss did not improve from 62.54272
196/196 - 59s - loss: 62.5260 - MinusLogProbMetric: 62.5260 - val_loss: 62.6491 - val_MinusLogProbMetric: 62.6491 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 464/1000
2023-10-30 23:15:52.990 
Epoch 464/1000 
	 loss: 62.3698, MinusLogProbMetric: 62.3698, val_loss: 62.5482, val_MinusLogProbMetric: 62.5482

Epoch 464: val_loss did not improve from 62.54272
196/196 - 55s - loss: 62.3698 - MinusLogProbMetric: 62.3698 - val_loss: 62.5482 - val_MinusLogProbMetric: 62.5482 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 465/1000
2023-10-30 23:16:48.884 
Epoch 465/1000 
	 loss: 62.2775, MinusLogProbMetric: 62.2775, val_loss: 62.4443, val_MinusLogProbMetric: 62.4443

Epoch 465: val_loss improved from 62.54272 to 62.44426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 62.2775 - MinusLogProbMetric: 62.2775 - val_loss: 62.4443 - val_MinusLogProbMetric: 62.4443 - lr: 2.0576e-06 - 57s/epoch - 290ms/step
Epoch 466/1000
2023-10-30 23:17:47.425 
Epoch 466/1000 
	 loss: 62.2160, MinusLogProbMetric: 62.2160, val_loss: 62.4034, val_MinusLogProbMetric: 62.4034

Epoch 466: val_loss improved from 62.44426 to 62.40343, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 62.2160 - MinusLogProbMetric: 62.2160 - val_loss: 62.4034 - val_MinusLogProbMetric: 62.4034 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 467/1000
2023-10-30 23:18:48.760 
Epoch 467/1000 
	 loss: 62.2262, MinusLogProbMetric: 62.2262, val_loss: 62.3528, val_MinusLogProbMetric: 62.3528

Epoch 467: val_loss improved from 62.40343 to 62.35283, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 62.2262 - MinusLogProbMetric: 62.2262 - val_loss: 62.3528 - val_MinusLogProbMetric: 62.3528 - lr: 2.0576e-06 - 61s/epoch - 313ms/step
Epoch 468/1000
2023-10-30 23:19:48.881 
Epoch 468/1000 
	 loss: 62.1042, MinusLogProbMetric: 62.1042, val_loss: 62.2778, val_MinusLogProbMetric: 62.2778

Epoch 468: val_loss improved from 62.35283 to 62.27784, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 62.1042 - MinusLogProbMetric: 62.1042 - val_loss: 62.2778 - val_MinusLogProbMetric: 62.2778 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 469/1000
2023-10-30 23:20:49.128 
Epoch 469/1000 
	 loss: 62.0474, MinusLogProbMetric: 62.0474, val_loss: 62.2213, val_MinusLogProbMetric: 62.2213

Epoch 469: val_loss improved from 62.27784 to 62.22131, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 62.0474 - MinusLogProbMetric: 62.0474 - val_loss: 62.2213 - val_MinusLogProbMetric: 62.2213 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 470/1000
2023-10-30 23:21:48.521 
Epoch 470/1000 
	 loss: 61.9647, MinusLogProbMetric: 61.9647, val_loss: 62.1347, val_MinusLogProbMetric: 62.1347

Epoch 470: val_loss improved from 62.22131 to 62.13471, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 61.9647 - MinusLogProbMetric: 61.9647 - val_loss: 62.1347 - val_MinusLogProbMetric: 62.1347 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 471/1000
2023-10-30 23:22:46.227 
Epoch 471/1000 
	 loss: 61.9749, MinusLogProbMetric: 61.9749, val_loss: 62.0797, val_MinusLogProbMetric: 62.0797

Epoch 471: val_loss improved from 62.13471 to 62.07967, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 61.9749 - MinusLogProbMetric: 61.9749 - val_loss: 62.0797 - val_MinusLogProbMetric: 62.0797 - lr: 2.0576e-06 - 58s/epoch - 294ms/step
Epoch 472/1000
2023-10-30 23:23:45.095 
Epoch 472/1000 
	 loss: 61.8437, MinusLogProbMetric: 61.8437, val_loss: 61.9988, val_MinusLogProbMetric: 61.9988

Epoch 472: val_loss improved from 62.07967 to 61.99882, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 61.8437 - MinusLogProbMetric: 61.8437 - val_loss: 61.9988 - val_MinusLogProbMetric: 61.9988 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 473/1000
2023-10-30 23:24:40.073 
Epoch 473/1000 
	 loss: 61.8398, MinusLogProbMetric: 61.8398, val_loss: 61.9524, val_MinusLogProbMetric: 61.9524

Epoch 473: val_loss improved from 61.99882 to 61.95240, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 61.8398 - MinusLogProbMetric: 61.8398 - val_loss: 61.9524 - val_MinusLogProbMetric: 61.9524 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 474/1000
2023-10-30 23:25:42.064 
Epoch 474/1000 
	 loss: 61.7345, MinusLogProbMetric: 61.7345, val_loss: 61.9104, val_MinusLogProbMetric: 61.9104

Epoch 474: val_loss improved from 61.95240 to 61.91040, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 61.7345 - MinusLogProbMetric: 61.7345 - val_loss: 61.9104 - val_MinusLogProbMetric: 61.9104 - lr: 2.0576e-06 - 62s/epoch - 316ms/step
Epoch 475/1000
2023-10-30 23:26:41.638 
Epoch 475/1000 
	 loss: 61.6857, MinusLogProbMetric: 61.6857, val_loss: 61.8699, val_MinusLogProbMetric: 61.8699

Epoch 475: val_loss improved from 61.91040 to 61.86989, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 61.6857 - MinusLogProbMetric: 61.6857 - val_loss: 61.8699 - val_MinusLogProbMetric: 61.8699 - lr: 2.0576e-06 - 60s/epoch - 304ms/step
Epoch 476/1000
2023-10-30 23:27:40.638 
Epoch 476/1000 
	 loss: 61.6446, MinusLogProbMetric: 61.6446, val_loss: 61.8033, val_MinusLogProbMetric: 61.8033

Epoch 476: val_loss improved from 61.86989 to 61.80328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 61.6446 - MinusLogProbMetric: 61.6446 - val_loss: 61.8033 - val_MinusLogProbMetric: 61.8033 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 477/1000
2023-10-30 23:28:39.418 
Epoch 477/1000 
	 loss: 61.5833, MinusLogProbMetric: 61.5833, val_loss: 61.7444, val_MinusLogProbMetric: 61.7444

Epoch 477: val_loss improved from 61.80328 to 61.74437, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 61.5833 - MinusLogProbMetric: 61.5833 - val_loss: 61.7444 - val_MinusLogProbMetric: 61.7444 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 478/1000
2023-10-30 23:29:37.520 
Epoch 478/1000 
	 loss: 61.7161, MinusLogProbMetric: 61.7161, val_loss: 62.0263, val_MinusLogProbMetric: 62.0263

Epoch 478: val_loss did not improve from 61.74437
196/196 - 57s - loss: 61.7161 - MinusLogProbMetric: 61.7161 - val_loss: 62.0263 - val_MinusLogProbMetric: 62.0263 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 479/1000
2023-10-30 23:30:36.973 
Epoch 479/1000 
	 loss: 61.6160, MinusLogProbMetric: 61.6160, val_loss: 61.6884, val_MinusLogProbMetric: 61.6884

Epoch 479: val_loss improved from 61.74437 to 61.68842, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 61.6160 - MinusLogProbMetric: 61.6160 - val_loss: 61.6884 - val_MinusLogProbMetric: 61.6884 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 480/1000
2023-10-30 23:31:36.899 
Epoch 480/1000 
	 loss: 61.4593, MinusLogProbMetric: 61.4593, val_loss: 61.6071, val_MinusLogProbMetric: 61.6071

Epoch 480: val_loss improved from 61.68842 to 61.60706, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 61.4593 - MinusLogProbMetric: 61.4593 - val_loss: 61.6071 - val_MinusLogProbMetric: 61.6071 - lr: 2.0576e-06 - 60s/epoch - 306ms/step
Epoch 481/1000
2023-10-30 23:32:35.654 
Epoch 481/1000 
	 loss: 61.4231, MinusLogProbMetric: 61.4231, val_loss: 61.6018, val_MinusLogProbMetric: 61.6018

Epoch 481: val_loss improved from 61.60706 to 61.60178, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 61.4231 - MinusLogProbMetric: 61.4231 - val_loss: 61.6018 - val_MinusLogProbMetric: 61.6018 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 482/1000
2023-10-30 23:33:36.392 
Epoch 482/1000 
	 loss: 61.3734, MinusLogProbMetric: 61.3734, val_loss: 61.5273, val_MinusLogProbMetric: 61.5273

Epoch 482: val_loss improved from 61.60178 to 61.52731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 61.3734 - MinusLogProbMetric: 61.3734 - val_loss: 61.5273 - val_MinusLogProbMetric: 61.5273 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 483/1000
2023-10-30 23:34:37.897 
Epoch 483/1000 
	 loss: 61.3277, MinusLogProbMetric: 61.3277, val_loss: 61.4936, val_MinusLogProbMetric: 61.4936

Epoch 483: val_loss improved from 61.52731 to 61.49359, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 61.3277 - MinusLogProbMetric: 61.3277 - val_loss: 61.4936 - val_MinusLogProbMetric: 61.4936 - lr: 2.0576e-06 - 61s/epoch - 313ms/step
Epoch 484/1000
2023-10-30 23:35:38.018 
Epoch 484/1000 
	 loss: 61.2995, MinusLogProbMetric: 61.2995, val_loss: 61.4588, val_MinusLogProbMetric: 61.4588

Epoch 484: val_loss improved from 61.49359 to 61.45879, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 61.2995 - MinusLogProbMetric: 61.2995 - val_loss: 61.4588 - val_MinusLogProbMetric: 61.4588 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 485/1000
2023-10-30 23:36:38.998 
Epoch 485/1000 
	 loss: 61.2758, MinusLogProbMetric: 61.2758, val_loss: 61.4269, val_MinusLogProbMetric: 61.4269

Epoch 485: val_loss improved from 61.45879 to 61.42686, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 61.2758 - MinusLogProbMetric: 61.2758 - val_loss: 61.4269 - val_MinusLogProbMetric: 61.4269 - lr: 2.0576e-06 - 61s/epoch - 312ms/step
Epoch 486/1000
2023-10-30 23:37:41.482 
Epoch 486/1000 
	 loss: 61.2260, MinusLogProbMetric: 61.2260, val_loss: 61.3863, val_MinusLogProbMetric: 61.3863

Epoch 486: val_loss improved from 61.42686 to 61.38626, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 61.2260 - MinusLogProbMetric: 61.2260 - val_loss: 61.3863 - val_MinusLogProbMetric: 61.3863 - lr: 2.0576e-06 - 62s/epoch - 318ms/step
Epoch 487/1000
2023-10-30 23:38:39.523 
Epoch 487/1000 
	 loss: 61.1805, MinusLogProbMetric: 61.1805, val_loss: 61.3660, val_MinusLogProbMetric: 61.3660

Epoch 487: val_loss improved from 61.38626 to 61.36602, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 61.1805 - MinusLogProbMetric: 61.1805 - val_loss: 61.3660 - val_MinusLogProbMetric: 61.3660 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 488/1000
2023-10-30 23:39:38.320 
Epoch 488/1000 
	 loss: 61.1543, MinusLogProbMetric: 61.1543, val_loss: 61.3162, val_MinusLogProbMetric: 61.3162

Epoch 488: val_loss improved from 61.36602 to 61.31622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 61.1543 - MinusLogProbMetric: 61.1543 - val_loss: 61.3162 - val_MinusLogProbMetric: 61.3162 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 489/1000
2023-10-30 23:40:38.277 
Epoch 489/1000 
	 loss: 61.1093, MinusLogProbMetric: 61.1093, val_loss: 61.3003, val_MinusLogProbMetric: 61.3003

Epoch 489: val_loss improved from 61.31622 to 61.30025, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 61.1093 - MinusLogProbMetric: 61.1093 - val_loss: 61.3003 - val_MinusLogProbMetric: 61.3003 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 490/1000
2023-10-30 23:41:37.007 
Epoch 490/1000 
	 loss: 61.0758, MinusLogProbMetric: 61.0758, val_loss: 61.2829, val_MinusLogProbMetric: 61.2829

Epoch 490: val_loss improved from 61.30025 to 61.28294, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 61.0758 - MinusLogProbMetric: 61.0758 - val_loss: 61.2829 - val_MinusLogProbMetric: 61.2829 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 491/1000
2023-10-30 23:42:35.444 
Epoch 491/1000 
	 loss: 61.1799, MinusLogProbMetric: 61.1799, val_loss: 61.3855, val_MinusLogProbMetric: 61.3855

Epoch 491: val_loss did not improve from 61.28294
196/196 - 57s - loss: 61.1799 - MinusLogProbMetric: 61.1799 - val_loss: 61.3855 - val_MinusLogProbMetric: 61.3855 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 492/1000
2023-10-30 23:43:33.084 
Epoch 492/1000 
	 loss: 61.1220, MinusLogProbMetric: 61.1220, val_loss: 61.2466, val_MinusLogProbMetric: 61.2466

Epoch 492: val_loss improved from 61.28294 to 61.24663, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 61.1220 - MinusLogProbMetric: 61.1220 - val_loss: 61.2466 - val_MinusLogProbMetric: 61.2466 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 493/1000
2023-10-30 23:44:33.368 
Epoch 493/1000 
	 loss: 61.0219, MinusLogProbMetric: 61.0219, val_loss: 61.3077, val_MinusLogProbMetric: 61.3077

Epoch 493: val_loss did not improve from 61.24663
196/196 - 59s - loss: 61.0219 - MinusLogProbMetric: 61.0219 - val_loss: 61.3077 - val_MinusLogProbMetric: 61.3077 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 494/1000
2023-10-30 23:45:35.177 
Epoch 494/1000 
	 loss: 60.9899, MinusLogProbMetric: 60.9899, val_loss: 61.1836, val_MinusLogProbMetric: 61.1836

Epoch 494: val_loss improved from 61.24663 to 61.18360, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 60.9899 - MinusLogProbMetric: 60.9899 - val_loss: 61.1836 - val_MinusLogProbMetric: 61.1836 - lr: 2.0576e-06 - 63s/epoch - 320ms/step
Epoch 495/1000
2023-10-30 23:46:33.024 
Epoch 495/1000 
	 loss: 60.9423, MinusLogProbMetric: 60.9423, val_loss: 61.1789, val_MinusLogProbMetric: 61.1789

Epoch 495: val_loss improved from 61.18360 to 61.17891, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 60.9423 - MinusLogProbMetric: 60.9423 - val_loss: 61.1789 - val_MinusLogProbMetric: 61.1789 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 496/1000
2023-10-30 23:47:31.718 
Epoch 496/1000 
	 loss: 60.9056, MinusLogProbMetric: 60.9056, val_loss: 61.0938, val_MinusLogProbMetric: 61.0938

Epoch 496: val_loss improved from 61.17891 to 61.09380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 60.9056 - MinusLogProbMetric: 60.9056 - val_loss: 61.0938 - val_MinusLogProbMetric: 61.0938 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 497/1000
2023-10-30 23:48:29.864 
Epoch 497/1000 
	 loss: 60.8868, MinusLogProbMetric: 60.8868, val_loss: 61.0511, val_MinusLogProbMetric: 61.0511

Epoch 497: val_loss improved from 61.09380 to 61.05111, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 60.8868 - MinusLogProbMetric: 60.8868 - val_loss: 61.0511 - val_MinusLogProbMetric: 61.0511 - lr: 2.0576e-06 - 58s/epoch - 296ms/step
Epoch 498/1000
2023-10-30 23:49:30.577 
Epoch 498/1000 
	 loss: 60.8389, MinusLogProbMetric: 60.8389, val_loss: 61.0318, val_MinusLogProbMetric: 61.0318

Epoch 498: val_loss improved from 61.05111 to 61.03185, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 60.8389 - MinusLogProbMetric: 60.8389 - val_loss: 61.0318 - val_MinusLogProbMetric: 61.0318 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 499/1000
2023-10-30 23:50:29.352 
Epoch 499/1000 
	 loss: 60.8131, MinusLogProbMetric: 60.8131, val_loss: 60.9765, val_MinusLogProbMetric: 60.9765

Epoch 499: val_loss improved from 61.03185 to 60.97654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 60.8131 - MinusLogProbMetric: 60.8131 - val_loss: 60.9765 - val_MinusLogProbMetric: 60.9765 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 500/1000
2023-10-30 23:51:27.620 
Epoch 500/1000 
	 loss: 60.7807, MinusLogProbMetric: 60.7807, val_loss: 60.9711, val_MinusLogProbMetric: 60.9711

Epoch 500: val_loss improved from 60.97654 to 60.97105, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 60.7807 - MinusLogProbMetric: 60.7807 - val_loss: 60.9711 - val_MinusLogProbMetric: 60.9711 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 501/1000
2023-10-30 23:52:29.007 
Epoch 501/1000 
	 loss: 60.7680, MinusLogProbMetric: 60.7680, val_loss: 61.0243, val_MinusLogProbMetric: 61.0243

Epoch 501: val_loss did not improve from 60.97105
196/196 - 60s - loss: 60.7680 - MinusLogProbMetric: 60.7680 - val_loss: 61.0243 - val_MinusLogProbMetric: 61.0243 - lr: 2.0576e-06 - 60s/epoch - 309ms/step
Epoch 502/1000
2023-10-30 23:53:28.869 
Epoch 502/1000 
	 loss: 61.2014, MinusLogProbMetric: 61.2014, val_loss: 61.8518, val_MinusLogProbMetric: 61.8518

Epoch 502: val_loss did not improve from 60.97105
196/196 - 60s - loss: 61.2014 - MinusLogProbMetric: 61.2014 - val_loss: 61.8518 - val_MinusLogProbMetric: 61.8518 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 503/1000
2023-10-30 23:54:27.822 
Epoch 503/1000 
	 loss: 60.9827, MinusLogProbMetric: 60.9827, val_loss: 61.2512, val_MinusLogProbMetric: 61.2512

Epoch 503: val_loss did not improve from 60.97105
196/196 - 59s - loss: 60.9827 - MinusLogProbMetric: 60.9827 - val_loss: 61.2512 - val_MinusLogProbMetric: 61.2512 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 504/1000
2023-10-30 23:55:25.118 
Epoch 504/1000 
	 loss: 60.7585, MinusLogProbMetric: 60.7585, val_loss: 60.8776, val_MinusLogProbMetric: 60.8776

Epoch 504: val_loss improved from 60.97105 to 60.87762, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 60.7585 - MinusLogProbMetric: 60.7585 - val_loss: 60.8776 - val_MinusLogProbMetric: 60.8776 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 505/1000
2023-10-30 23:56:28.039 
Epoch 505/1000 
	 loss: 60.6689, MinusLogProbMetric: 60.6689, val_loss: 60.8132, val_MinusLogProbMetric: 60.8132

Epoch 505: val_loss improved from 60.87762 to 60.81320, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 60.6689 - MinusLogProbMetric: 60.6689 - val_loss: 60.8132 - val_MinusLogProbMetric: 60.8132 - lr: 2.0576e-06 - 63s/epoch - 321ms/step
Epoch 506/1000
2023-10-30 23:57:27.423 
Epoch 506/1000 
	 loss: 60.8767, MinusLogProbMetric: 60.8767, val_loss: 60.8641, val_MinusLogProbMetric: 60.8641

Epoch 506: val_loss did not improve from 60.81320
196/196 - 59s - loss: 60.8767 - MinusLogProbMetric: 60.8767 - val_loss: 60.8641 - val_MinusLogProbMetric: 60.8641 - lr: 2.0576e-06 - 59s/epoch - 298ms/step
Epoch 507/1000
2023-10-30 23:58:26.044 
Epoch 507/1000 
	 loss: 60.6204, MinusLogProbMetric: 60.6204, val_loss: 60.7930, val_MinusLogProbMetric: 60.7930

Epoch 507: val_loss improved from 60.81320 to 60.79298, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 60.6204 - MinusLogProbMetric: 60.6204 - val_loss: 60.7930 - val_MinusLogProbMetric: 60.7930 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 508/1000
2023-10-30 23:59:20.307 
Epoch 508/1000 
	 loss: 60.5718, MinusLogProbMetric: 60.5718, val_loss: 60.7596, val_MinusLogProbMetric: 60.7596

Epoch 508: val_loss improved from 60.79298 to 60.75960, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 60.5718 - MinusLogProbMetric: 60.5718 - val_loss: 60.7596 - val_MinusLogProbMetric: 60.7596 - lr: 2.0576e-06 - 54s/epoch - 278ms/step
Epoch 509/1000
2023-10-31 00:00:12.276 
Epoch 509/1000 
	 loss: 60.5583, MinusLogProbMetric: 60.5583, val_loss: 60.7177, val_MinusLogProbMetric: 60.7177

Epoch 509: val_loss improved from 60.75960 to 60.71769, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 52s - loss: 60.5583 - MinusLogProbMetric: 60.5583 - val_loss: 60.7177 - val_MinusLogProbMetric: 60.7177 - lr: 2.0576e-06 - 52s/epoch - 264ms/step
Epoch 510/1000
2023-10-31 00:01:08.819 
Epoch 510/1000 
	 loss: 60.5002, MinusLogProbMetric: 60.5002, val_loss: 60.7129, val_MinusLogProbMetric: 60.7129

Epoch 510: val_loss improved from 60.71769 to 60.71286, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 60.5002 - MinusLogProbMetric: 60.5002 - val_loss: 60.7129 - val_MinusLogProbMetric: 60.7129 - lr: 2.0576e-06 - 57s/epoch - 288ms/step
Epoch 511/1000
2023-10-31 00:02:02.786 
Epoch 511/1000 
	 loss: 60.4991, MinusLogProbMetric: 60.4991, val_loss: 60.7335, val_MinusLogProbMetric: 60.7335

Epoch 511: val_loss did not improve from 60.71286
196/196 - 53s - loss: 60.4991 - MinusLogProbMetric: 60.4991 - val_loss: 60.7335 - val_MinusLogProbMetric: 60.7335 - lr: 2.0576e-06 - 53s/epoch - 272ms/step
Epoch 512/1000
2023-10-31 00:02:56.366 
Epoch 512/1000 
	 loss: 60.4650, MinusLogProbMetric: 60.4650, val_loss: 60.7249, val_MinusLogProbMetric: 60.7249

Epoch 512: val_loss did not improve from 60.71286
196/196 - 54s - loss: 60.4650 - MinusLogProbMetric: 60.4650 - val_loss: 60.7249 - val_MinusLogProbMetric: 60.7249 - lr: 2.0576e-06 - 54s/epoch - 273ms/step
Epoch 513/1000
2023-10-31 00:03:50.773 
Epoch 513/1000 
	 loss: 60.4221, MinusLogProbMetric: 60.4221, val_loss: 60.6638, val_MinusLogProbMetric: 60.6638

Epoch 513: val_loss improved from 60.71286 to 60.66380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 60.4221 - MinusLogProbMetric: 60.4221 - val_loss: 60.6638 - val_MinusLogProbMetric: 60.6638 - lr: 2.0576e-06 - 55s/epoch - 282ms/step
Epoch 514/1000
2023-10-31 00:04:47.577 
Epoch 514/1000 
	 loss: 60.4126, MinusLogProbMetric: 60.4126, val_loss: 60.6555, val_MinusLogProbMetric: 60.6555

Epoch 514: val_loss improved from 60.66380 to 60.65550, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 60.4126 - MinusLogProbMetric: 60.4126 - val_loss: 60.6555 - val_MinusLogProbMetric: 60.6555 - lr: 2.0576e-06 - 57s/epoch - 290ms/step
Epoch 515/1000
2023-10-31 00:05:41.376 
Epoch 515/1000 
	 loss: 60.4226, MinusLogProbMetric: 60.4226, val_loss: 60.5846, val_MinusLogProbMetric: 60.5846

Epoch 515: val_loss improved from 60.65550 to 60.58458, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 60.4226 - MinusLogProbMetric: 60.4226 - val_loss: 60.5846 - val_MinusLogProbMetric: 60.5846 - lr: 2.0576e-06 - 54s/epoch - 274ms/step
Epoch 516/1000
2023-10-31 00:06:36.030 
Epoch 516/1000 
	 loss: 60.3666, MinusLogProbMetric: 60.3666, val_loss: 60.5928, val_MinusLogProbMetric: 60.5928

Epoch 516: val_loss did not improve from 60.58458
196/196 - 54s - loss: 60.3666 - MinusLogProbMetric: 60.3666 - val_loss: 60.5928 - val_MinusLogProbMetric: 60.5928 - lr: 2.0576e-06 - 54s/epoch - 275ms/step
Epoch 517/1000
2023-10-31 00:07:30.348 
Epoch 517/1000 
	 loss: 60.3303, MinusLogProbMetric: 60.3303, val_loss: 60.5416, val_MinusLogProbMetric: 60.5416

Epoch 517: val_loss improved from 60.58458 to 60.54160, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 60.3303 - MinusLogProbMetric: 60.3303 - val_loss: 60.5416 - val_MinusLogProbMetric: 60.5416 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 518/1000
2023-10-31 00:08:24.530 
Epoch 518/1000 
	 loss: 60.3097, MinusLogProbMetric: 60.3097, val_loss: 60.5785, val_MinusLogProbMetric: 60.5785

Epoch 518: val_loss did not improve from 60.54160
196/196 - 53s - loss: 60.3097 - MinusLogProbMetric: 60.3097 - val_loss: 60.5785 - val_MinusLogProbMetric: 60.5785 - lr: 2.0576e-06 - 53s/epoch - 273ms/step
Epoch 519/1000
2023-10-31 00:09:19.363 
Epoch 519/1000 
	 loss: 60.2960, MinusLogProbMetric: 60.2960, val_loss: 60.5116, val_MinusLogProbMetric: 60.5116

Epoch 519: val_loss improved from 60.54160 to 60.51157, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 60.2960 - MinusLogProbMetric: 60.2960 - val_loss: 60.5116 - val_MinusLogProbMetric: 60.5116 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 520/1000
2023-10-31 00:10:16.937 
Epoch 520/1000 
	 loss: 60.3163, MinusLogProbMetric: 60.3163, val_loss: 60.5384, val_MinusLogProbMetric: 60.5384

Epoch 520: val_loss did not improve from 60.51157
196/196 - 57s - loss: 60.3163 - MinusLogProbMetric: 60.3163 - val_loss: 60.5384 - val_MinusLogProbMetric: 60.5384 - lr: 2.0576e-06 - 57s/epoch - 289ms/step
Epoch 521/1000
2023-10-31 00:11:07.745 
Epoch 521/1000 
	 loss: 60.2807, MinusLogProbMetric: 60.2807, val_loss: 60.4431, val_MinusLogProbMetric: 60.4431

Epoch 521: val_loss improved from 60.51157 to 60.44313, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 52s - loss: 60.2807 - MinusLogProbMetric: 60.2807 - val_loss: 60.4431 - val_MinusLogProbMetric: 60.4431 - lr: 2.0576e-06 - 52s/epoch - 263ms/step
Epoch 522/1000
2023-10-31 00:12:00.987 
Epoch 522/1000 
	 loss: 60.2255, MinusLogProbMetric: 60.2255, val_loss: 60.3959, val_MinusLogProbMetric: 60.3959

Epoch 522: val_loss improved from 60.44313 to 60.39594, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 53s - loss: 60.2255 - MinusLogProbMetric: 60.2255 - val_loss: 60.3959 - val_MinusLogProbMetric: 60.3959 - lr: 2.0576e-06 - 53s/epoch - 272ms/step
Epoch 523/1000
2023-10-31 00:12:57.815 
Epoch 523/1000 
	 loss: 60.1790, MinusLogProbMetric: 60.1790, val_loss: 60.4083, val_MinusLogProbMetric: 60.4083

Epoch 523: val_loss did not improve from 60.39594
196/196 - 56s - loss: 60.1790 - MinusLogProbMetric: 60.1790 - val_loss: 60.4083 - val_MinusLogProbMetric: 60.4083 - lr: 2.0576e-06 - 56s/epoch - 286ms/step
Epoch 524/1000
2023-10-31 00:13:52.650 
Epoch 524/1000 
	 loss: 60.1887, MinusLogProbMetric: 60.1887, val_loss: 60.4121, val_MinusLogProbMetric: 60.4121

Epoch 524: val_loss did not improve from 60.39594
196/196 - 55s - loss: 60.1887 - MinusLogProbMetric: 60.1887 - val_loss: 60.4121 - val_MinusLogProbMetric: 60.4121 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 525/1000
2023-10-31 00:14:46.297 
Epoch 525/1000 
	 loss: 60.1589, MinusLogProbMetric: 60.1589, val_loss: 60.3399, val_MinusLogProbMetric: 60.3399

Epoch 525: val_loss improved from 60.39594 to 60.33986, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 60.1589 - MinusLogProbMetric: 60.1589 - val_loss: 60.3399 - val_MinusLogProbMetric: 60.3399 - lr: 2.0576e-06 - 54s/epoch - 277ms/step
Epoch 526/1000
2023-10-31 00:15:44.919 
Epoch 526/1000 
	 loss: 60.1263, MinusLogProbMetric: 60.1263, val_loss: 60.3877, val_MinusLogProbMetric: 60.3877

Epoch 526: val_loss did not improve from 60.33986
196/196 - 58s - loss: 60.1263 - MinusLogProbMetric: 60.1263 - val_loss: 60.3877 - val_MinusLogProbMetric: 60.3877 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 527/1000
2023-10-31 00:16:40.680 
Epoch 527/1000 
	 loss: 60.0873, MinusLogProbMetric: 60.0873, val_loss: 60.3221, val_MinusLogProbMetric: 60.3221

Epoch 527: val_loss improved from 60.33986 to 60.32211, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 60.0873 - MinusLogProbMetric: 60.0873 - val_loss: 60.3221 - val_MinusLogProbMetric: 60.3221 - lr: 2.0576e-06 - 57s/epoch - 288ms/step
Epoch 528/1000
2023-10-31 00:17:34.836 
Epoch 528/1000 
	 loss: 60.0550, MinusLogProbMetric: 60.0550, val_loss: 60.3102, val_MinusLogProbMetric: 60.3102

Epoch 528: val_loss improved from 60.32211 to 60.31019, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 60.0550 - MinusLogProbMetric: 60.0550 - val_loss: 60.3102 - val_MinusLogProbMetric: 60.3102 - lr: 2.0576e-06 - 54s/epoch - 276ms/step
Epoch 529/1000
2023-10-31 00:18:31.887 
Epoch 529/1000 
	 loss: 60.0314, MinusLogProbMetric: 60.0314, val_loss: 60.2682, val_MinusLogProbMetric: 60.2682

Epoch 529: val_loss improved from 60.31019 to 60.26820, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 60.0314 - MinusLogProbMetric: 60.0314 - val_loss: 60.2682 - val_MinusLogProbMetric: 60.2682 - lr: 2.0576e-06 - 57s/epoch - 291ms/step
Epoch 530/1000
2023-10-31 00:19:23.656 
Epoch 530/1000 
	 loss: 60.0188, MinusLogProbMetric: 60.0188, val_loss: 60.2145, val_MinusLogProbMetric: 60.2145

Epoch 530: val_loss improved from 60.26820 to 60.21449, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 52s - loss: 60.0188 - MinusLogProbMetric: 60.0188 - val_loss: 60.2145 - val_MinusLogProbMetric: 60.2145 - lr: 2.0576e-06 - 52s/epoch - 265ms/step
Epoch 531/1000
2023-10-31 00:20:14.113 
Epoch 531/1000 
	 loss: 60.1592, MinusLogProbMetric: 60.1592, val_loss: 60.3428, val_MinusLogProbMetric: 60.3428

Epoch 531: val_loss did not improve from 60.21449
196/196 - 50s - loss: 60.1592 - MinusLogProbMetric: 60.1592 - val_loss: 60.3428 - val_MinusLogProbMetric: 60.3428 - lr: 2.0576e-06 - 50s/epoch - 253ms/step
Epoch 532/1000
2023-10-31 00:21:08.042 
Epoch 532/1000 
	 loss: 60.1454, MinusLogProbMetric: 60.1454, val_loss: 60.2335, val_MinusLogProbMetric: 60.2335

Epoch 532: val_loss did not improve from 60.21449
196/196 - 54s - loss: 60.1454 - MinusLogProbMetric: 60.1454 - val_loss: 60.2335 - val_MinusLogProbMetric: 60.2335 - lr: 2.0576e-06 - 54s/epoch - 275ms/step
Epoch 533/1000
2023-10-31 00:22:05.569 
Epoch 533/1000 
	 loss: 59.9741, MinusLogProbMetric: 59.9741, val_loss: 60.2032, val_MinusLogProbMetric: 60.2032

Epoch 533: val_loss improved from 60.21449 to 60.20317, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 59.9741 - MinusLogProbMetric: 59.9741 - val_loss: 60.2032 - val_MinusLogProbMetric: 60.2032 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 534/1000
2023-10-31 00:22:59.114 
Epoch 534/1000 
	 loss: 59.9335, MinusLogProbMetric: 59.9335, val_loss: 60.2100, val_MinusLogProbMetric: 60.2100

Epoch 534: val_loss did not improve from 60.20317
196/196 - 53s - loss: 59.9335 - MinusLogProbMetric: 59.9335 - val_loss: 60.2100 - val_MinusLogProbMetric: 60.2100 - lr: 2.0576e-06 - 53s/epoch - 269ms/step
Epoch 535/1000
2023-10-31 00:23:50.029 
Epoch 535/1000 
	 loss: 59.9270, MinusLogProbMetric: 59.9270, val_loss: 60.1169, val_MinusLogProbMetric: 60.1169

Epoch 535: val_loss improved from 60.20317 to 60.11690, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 52s - loss: 59.9270 - MinusLogProbMetric: 59.9270 - val_loss: 60.1169 - val_MinusLogProbMetric: 60.1169 - lr: 2.0576e-06 - 52s/epoch - 264ms/step
Epoch 536/1000
2023-10-31 00:24:45.906 
Epoch 536/1000 
	 loss: 59.8979, MinusLogProbMetric: 59.8979, val_loss: 60.1195, val_MinusLogProbMetric: 60.1195

Epoch 536: val_loss did not improve from 60.11690
196/196 - 55s - loss: 59.8979 - MinusLogProbMetric: 59.8979 - val_loss: 60.1195 - val_MinusLogProbMetric: 60.1195 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 537/1000
2023-10-31 00:25:39.213 
Epoch 537/1000 
	 loss: 59.8766, MinusLogProbMetric: 59.8766, val_loss: 60.1037, val_MinusLogProbMetric: 60.1037

Epoch 537: val_loss improved from 60.11690 to 60.10369, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 59.8766 - MinusLogProbMetric: 59.8766 - val_loss: 60.1037 - val_MinusLogProbMetric: 60.1037 - lr: 2.0576e-06 - 54s/epoch - 276ms/step
Epoch 538/1000
2023-10-31 00:26:33.832 
Epoch 538/1000 
	 loss: 59.8338, MinusLogProbMetric: 59.8338, val_loss: 60.0032, val_MinusLogProbMetric: 60.0032

Epoch 538: val_loss improved from 60.10369 to 60.00317, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 59.8338 - MinusLogProbMetric: 59.8338 - val_loss: 60.0032 - val_MinusLogProbMetric: 60.0032 - lr: 2.0576e-06 - 55s/epoch - 279ms/step
Epoch 539/1000
2023-10-31 00:27:31.254 
Epoch 539/1000 
	 loss: 59.8247, MinusLogProbMetric: 59.8247, val_loss: 60.1091, val_MinusLogProbMetric: 60.1091

Epoch 539: val_loss did not improve from 60.00317
196/196 - 57s - loss: 59.8247 - MinusLogProbMetric: 59.8247 - val_loss: 60.1091 - val_MinusLogProbMetric: 60.1091 - lr: 2.0576e-06 - 57s/epoch - 289ms/step
Epoch 540/1000
2023-10-31 00:28:24.135 
Epoch 540/1000 
	 loss: 59.8116, MinusLogProbMetric: 59.8116, val_loss: 59.9829, val_MinusLogProbMetric: 59.9829

Epoch 540: val_loss improved from 60.00317 to 59.98287, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 59.8116 - MinusLogProbMetric: 59.8116 - val_loss: 59.9829 - val_MinusLogProbMetric: 59.9829 - lr: 2.0576e-06 - 54s/epoch - 274ms/step
Epoch 541/1000
2023-10-31 00:29:18.375 
Epoch 541/1000 
	 loss: 59.7798, MinusLogProbMetric: 59.7798, val_loss: 59.9807, val_MinusLogProbMetric: 59.9807

Epoch 541: val_loss improved from 59.98287 to 59.98066, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 59.7798 - MinusLogProbMetric: 59.7798 - val_loss: 59.9807 - val_MinusLogProbMetric: 59.9807 - lr: 2.0576e-06 - 55s/epoch - 278ms/step
Epoch 542/1000
2023-10-31 00:30:14.711 
Epoch 542/1000 
	 loss: 59.7614, MinusLogProbMetric: 59.7614, val_loss: 59.9469, val_MinusLogProbMetric: 59.9469

Epoch 542: val_loss improved from 59.98066 to 59.94689, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 59.7614 - MinusLogProbMetric: 59.7614 - val_loss: 59.9469 - val_MinusLogProbMetric: 59.9469 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 543/1000
2023-10-31 00:31:08.862 
Epoch 543/1000 
	 loss: 59.7537, MinusLogProbMetric: 59.7537, val_loss: 59.9558, val_MinusLogProbMetric: 59.9558

Epoch 543: val_loss did not improve from 59.94689
196/196 - 53s - loss: 59.7537 - MinusLogProbMetric: 59.7537 - val_loss: 59.9558 - val_MinusLogProbMetric: 59.9558 - lr: 2.0576e-06 - 53s/epoch - 272ms/step
Epoch 544/1000
2023-10-31 00:32:02.683 
Epoch 544/1000 
	 loss: 59.7806, MinusLogProbMetric: 59.7806, val_loss: 59.9081, val_MinusLogProbMetric: 59.9081

Epoch 544: val_loss improved from 59.94689 to 59.90809, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 59.7806 - MinusLogProbMetric: 59.7806 - val_loss: 59.9081 - val_MinusLogProbMetric: 59.9081 - lr: 2.0576e-06 - 55s/epoch - 278ms/step
Epoch 545/1000
2023-10-31 00:32:57.493 
Epoch 545/1000 
	 loss: 59.7329, MinusLogProbMetric: 59.7329, val_loss: 59.9224, val_MinusLogProbMetric: 59.9224

Epoch 545: val_loss did not improve from 59.90809
196/196 - 54s - loss: 59.7329 - MinusLogProbMetric: 59.7329 - val_loss: 59.9224 - val_MinusLogProbMetric: 59.9224 - lr: 2.0576e-06 - 54s/epoch - 276ms/step
Epoch 546/1000
2023-10-31 00:33:53.846 
Epoch 546/1000 
	 loss: 59.6617, MinusLogProbMetric: 59.6617, val_loss: 59.8998, val_MinusLogProbMetric: 59.8998

Epoch 546: val_loss improved from 59.90809 to 59.89980, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 59.6617 - MinusLogProbMetric: 59.6617 - val_loss: 59.8998 - val_MinusLogProbMetric: 59.8998 - lr: 2.0576e-06 - 57s/epoch - 291ms/step
Epoch 547/1000
2023-10-31 00:34:50.000 
Epoch 547/1000 
	 loss: 59.7513, MinusLogProbMetric: 59.7513, val_loss: 59.8643, val_MinusLogProbMetric: 59.8643

Epoch 547: val_loss improved from 59.89980 to 59.86428, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 59.7513 - MinusLogProbMetric: 59.7513 - val_loss: 59.8643 - val_MinusLogProbMetric: 59.8643 - lr: 2.0576e-06 - 56s/epoch - 287ms/step
Epoch 548/1000
2023-10-31 00:35:46.961 
Epoch 548/1000 
	 loss: 59.8264, MinusLogProbMetric: 59.8264, val_loss: 59.8517, val_MinusLogProbMetric: 59.8517

Epoch 548: val_loss improved from 59.86428 to 59.85165, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 59.8264 - MinusLogProbMetric: 59.8264 - val_loss: 59.8517 - val_MinusLogProbMetric: 59.8517 - lr: 2.0576e-06 - 57s/epoch - 291ms/step
Epoch 549/1000
2023-10-31 00:36:42.951 
Epoch 549/1000 
	 loss: 59.6499, MinusLogProbMetric: 59.6499, val_loss: 59.8924, val_MinusLogProbMetric: 59.8924

Epoch 549: val_loss did not improve from 59.85165
196/196 - 55s - loss: 59.6499 - MinusLogProbMetric: 59.6499 - val_loss: 59.8924 - val_MinusLogProbMetric: 59.8924 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 550/1000
2023-10-31 00:37:37.391 
Epoch 550/1000 
	 loss: 59.7250, MinusLogProbMetric: 59.7250, val_loss: 59.7962, val_MinusLogProbMetric: 59.7962

Epoch 550: val_loss improved from 59.85165 to 59.79616, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 59.7250 - MinusLogProbMetric: 59.7250 - val_loss: 59.7962 - val_MinusLogProbMetric: 59.7962 - lr: 2.0576e-06 - 55s/epoch - 283ms/step
Epoch 551/1000
2023-10-31 00:38:31.012 
Epoch 551/1000 
	 loss: 59.8389, MinusLogProbMetric: 59.8389, val_loss: 59.7291, val_MinusLogProbMetric: 59.7291

Epoch 551: val_loss improved from 59.79616 to 59.72906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 53s - loss: 59.8389 - MinusLogProbMetric: 59.8389 - val_loss: 59.7291 - val_MinusLogProbMetric: 59.7291 - lr: 2.0576e-06 - 53s/epoch - 273ms/step
Epoch 552/1000
2023-10-31 00:39:28.644 
Epoch 552/1000 
	 loss: 59.5368, MinusLogProbMetric: 59.5368, val_loss: 59.7400, val_MinusLogProbMetric: 59.7400

Epoch 552: val_loss did not improve from 59.72906
196/196 - 57s - loss: 59.5368 - MinusLogProbMetric: 59.5368 - val_loss: 59.7400 - val_MinusLogProbMetric: 59.7400 - lr: 2.0576e-06 - 57s/epoch - 290ms/step
Epoch 553/1000
2023-10-31 00:40:22.353 
Epoch 553/1000 
	 loss: 59.5732, MinusLogProbMetric: 59.5732, val_loss: 59.6934, val_MinusLogProbMetric: 59.6934

Epoch 553: val_loss improved from 59.72906 to 59.69337, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 59.5732 - MinusLogProbMetric: 59.5732 - val_loss: 59.6934 - val_MinusLogProbMetric: 59.6934 - lr: 2.0576e-06 - 54s/epoch - 278ms/step
Epoch 554/1000
2023-10-31 00:41:17.506 
Epoch 554/1000 
	 loss: 59.5518, MinusLogProbMetric: 59.5518, val_loss: 59.6784, val_MinusLogProbMetric: 59.6784

Epoch 554: val_loss improved from 59.69337 to 59.67842, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 59.5518 - MinusLogProbMetric: 59.5518 - val_loss: 59.6784 - val_MinusLogProbMetric: 59.6784 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 555/1000
2023-10-31 00:42:13.423 
Epoch 555/1000 
	 loss: 59.6853, MinusLogProbMetric: 59.6853, val_loss: 59.9292, val_MinusLogProbMetric: 59.9292

Epoch 555: val_loss did not improve from 59.67842
196/196 - 55s - loss: 59.6853 - MinusLogProbMetric: 59.6853 - val_loss: 59.9292 - val_MinusLogProbMetric: 59.9292 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 556/1000
2023-10-31 00:43:08.817 
Epoch 556/1000 
	 loss: 73.1818, MinusLogProbMetric: 73.1818, val_loss: 72.7979, val_MinusLogProbMetric: 72.7979

Epoch 556: val_loss did not improve from 59.67842
196/196 - 55s - loss: 73.1818 - MinusLogProbMetric: 73.1818 - val_loss: 72.7979 - val_MinusLogProbMetric: 72.7979 - lr: 2.0576e-06 - 55s/epoch - 283ms/step
Epoch 557/1000
2023-10-31 00:44:04.352 
Epoch 557/1000 
	 loss: 68.5291, MinusLogProbMetric: 68.5291, val_loss: 82.3942, val_MinusLogProbMetric: 82.3942

Epoch 557: val_loss did not improve from 59.67842
196/196 - 56s - loss: 68.5291 - MinusLogProbMetric: 68.5291 - val_loss: 82.3942 - val_MinusLogProbMetric: 82.3942 - lr: 2.0576e-06 - 56s/epoch - 283ms/step
Epoch 558/1000
2023-10-31 00:44:59.122 
Epoch 558/1000 
	 loss: 65.7191, MinusLogProbMetric: 65.7191, val_loss: 61.6128, val_MinusLogProbMetric: 61.6128

Epoch 558: val_loss did not improve from 59.67842
196/196 - 55s - loss: 65.7191 - MinusLogProbMetric: 65.7191 - val_loss: 61.6128 - val_MinusLogProbMetric: 61.6128 - lr: 2.0576e-06 - 55s/epoch - 279ms/step
Epoch 559/1000
2023-10-31 00:45:53.089 
Epoch 559/1000 
	 loss: 60.8094, MinusLogProbMetric: 60.8094, val_loss: 60.6218, val_MinusLogProbMetric: 60.6218

Epoch 559: val_loss did not improve from 59.67842
196/196 - 54s - loss: 60.8094 - MinusLogProbMetric: 60.8094 - val_loss: 60.6218 - val_MinusLogProbMetric: 60.6218 - lr: 2.0576e-06 - 54s/epoch - 275ms/step
Epoch 560/1000
2023-10-31 00:46:49.351 
Epoch 560/1000 
	 loss: 60.1811, MinusLogProbMetric: 60.1811, val_loss: 60.2910, val_MinusLogProbMetric: 60.2910

Epoch 560: val_loss did not improve from 59.67842
196/196 - 56s - loss: 60.1811 - MinusLogProbMetric: 60.1811 - val_loss: 60.2910 - val_MinusLogProbMetric: 60.2910 - lr: 2.0576e-06 - 56s/epoch - 287ms/step
Epoch 561/1000
2023-10-31 00:47:44.872 
Epoch 561/1000 
	 loss: 62.2211, MinusLogProbMetric: 62.2211, val_loss: 60.4936, val_MinusLogProbMetric: 60.4936

Epoch 561: val_loss did not improve from 59.67842
196/196 - 56s - loss: 62.2211 - MinusLogProbMetric: 62.2211 - val_loss: 60.4936 - val_MinusLogProbMetric: 60.4936 - lr: 2.0576e-06 - 56s/epoch - 283ms/step
Epoch 562/1000
2023-10-31 00:48:39.853 
Epoch 562/1000 
	 loss: 60.0256, MinusLogProbMetric: 60.0256, val_loss: 60.0512, val_MinusLogProbMetric: 60.0512

Epoch 562: val_loss did not improve from 59.67842
196/196 - 55s - loss: 60.0256 - MinusLogProbMetric: 60.0256 - val_loss: 60.0512 - val_MinusLogProbMetric: 60.0512 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 563/1000
2023-10-31 00:49:33.302 
Epoch 563/1000 
	 loss: 59.7894, MinusLogProbMetric: 59.7894, val_loss: 59.8884, val_MinusLogProbMetric: 59.8884

Epoch 563: val_loss did not improve from 59.67842
196/196 - 53s - loss: 59.7894 - MinusLogProbMetric: 59.7894 - val_loss: 59.8884 - val_MinusLogProbMetric: 59.8884 - lr: 2.0576e-06 - 53s/epoch - 273ms/step
Epoch 564/1000
2023-10-31 00:50:25.697 
Epoch 564/1000 
	 loss: 59.6618, MinusLogProbMetric: 59.6618, val_loss: 59.7882, val_MinusLogProbMetric: 59.7882

Epoch 564: val_loss did not improve from 59.67842
196/196 - 52s - loss: 59.6618 - MinusLogProbMetric: 59.6618 - val_loss: 59.7882 - val_MinusLogProbMetric: 59.7882 - lr: 2.0576e-06 - 52s/epoch - 267ms/step
Epoch 565/1000
2023-10-31 00:51:19.297 
Epoch 565/1000 
	 loss: 59.5818, MinusLogProbMetric: 59.5818, val_loss: 59.7303, val_MinusLogProbMetric: 59.7303

Epoch 565: val_loss did not improve from 59.67842
196/196 - 54s - loss: 59.5818 - MinusLogProbMetric: 59.5818 - val_loss: 59.7303 - val_MinusLogProbMetric: 59.7303 - lr: 2.0576e-06 - 54s/epoch - 273ms/step
Epoch 566/1000
2023-10-31 00:52:14.486 
Epoch 566/1000 
	 loss: 59.5209, MinusLogProbMetric: 59.5209, val_loss: 59.7385, val_MinusLogProbMetric: 59.7385

Epoch 566: val_loss did not improve from 59.67842
196/196 - 55s - loss: 59.5209 - MinusLogProbMetric: 59.5209 - val_loss: 59.7385 - val_MinusLogProbMetric: 59.7385 - lr: 2.0576e-06 - 55s/epoch - 282ms/step
Epoch 567/1000
2023-10-31 00:53:06.783 
Epoch 567/1000 
	 loss: 59.4517, MinusLogProbMetric: 59.4517, val_loss: 59.6212, val_MinusLogProbMetric: 59.6212

Epoch 567: val_loss improved from 59.67842 to 59.62123, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 53s - loss: 59.4517 - MinusLogProbMetric: 59.4517 - val_loss: 59.6212 - val_MinusLogProbMetric: 59.6212 - lr: 2.0576e-06 - 53s/epoch - 271ms/step
Epoch 568/1000
2023-10-31 00:54:02.108 
Epoch 568/1000 
	 loss: 59.3993, MinusLogProbMetric: 59.3993, val_loss: 59.5651, val_MinusLogProbMetric: 59.5651

Epoch 568: val_loss improved from 59.62123 to 59.56513, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 59.3993 - MinusLogProbMetric: 59.3993 - val_loss: 59.5651 - val_MinusLogProbMetric: 59.5651 - lr: 2.0576e-06 - 55s/epoch - 282ms/step
Epoch 569/1000
2023-10-31 00:54:57.950 
Epoch 569/1000 
	 loss: 59.3512, MinusLogProbMetric: 59.3512, val_loss: 59.5327, val_MinusLogProbMetric: 59.5327

Epoch 569: val_loss improved from 59.56513 to 59.53271, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 59.3512 - MinusLogProbMetric: 59.3512 - val_loss: 59.5327 - val_MinusLogProbMetric: 59.5327 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 570/1000
2023-10-31 00:55:53.604 
Epoch 570/1000 
	 loss: 59.3138, MinusLogProbMetric: 59.3138, val_loss: 59.4801, val_MinusLogProbMetric: 59.4801

Epoch 570: val_loss improved from 59.53271 to 59.48009, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 59.3138 - MinusLogProbMetric: 59.3138 - val_loss: 59.4801 - val_MinusLogProbMetric: 59.4801 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 571/1000
2023-10-31 00:56:48.101 
Epoch 571/1000 
	 loss: 59.2783, MinusLogProbMetric: 59.2783, val_loss: 59.4171, val_MinusLogProbMetric: 59.4171

Epoch 571: val_loss improved from 59.48009 to 59.41714, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 59.2783 - MinusLogProbMetric: 59.2783 - val_loss: 59.4171 - val_MinusLogProbMetric: 59.4171 - lr: 2.0576e-06 - 54s/epoch - 278ms/step
Epoch 572/1000
2023-10-31 00:57:41.038 
Epoch 572/1000 
	 loss: 59.2361, MinusLogProbMetric: 59.2361, val_loss: 59.3847, val_MinusLogProbMetric: 59.3847

Epoch 572: val_loss improved from 59.41714 to 59.38468, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 53s - loss: 59.2361 - MinusLogProbMetric: 59.2361 - val_loss: 59.3847 - val_MinusLogProbMetric: 59.3847 - lr: 2.0576e-06 - 53s/epoch - 270ms/step
Epoch 573/1000
2023-10-31 00:58:34.636 
Epoch 573/1000 
	 loss: 59.1871, MinusLogProbMetric: 59.1871, val_loss: 59.4035, val_MinusLogProbMetric: 59.4035

Epoch 573: val_loss did not improve from 59.38468
196/196 - 53s - loss: 59.1871 - MinusLogProbMetric: 59.1871 - val_loss: 59.4035 - val_MinusLogProbMetric: 59.4035 - lr: 2.0576e-06 - 53s/epoch - 270ms/step
Epoch 574/1000
2023-10-31 00:59:28.841 
Epoch 574/1000 
	 loss: 59.1544, MinusLogProbMetric: 59.1544, val_loss: 59.3353, val_MinusLogProbMetric: 59.3353

Epoch 574: val_loss improved from 59.38468 to 59.33528, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 59.1544 - MinusLogProbMetric: 59.1544 - val_loss: 59.3353 - val_MinusLogProbMetric: 59.3353 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 575/1000
2023-10-31 01:00:22.724 
Epoch 575/1000 
	 loss: 59.1190, MinusLogProbMetric: 59.1190, val_loss: 59.2824, val_MinusLogProbMetric: 59.2824

Epoch 575: val_loss improved from 59.33528 to 59.28238, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 59.1190 - MinusLogProbMetric: 59.1190 - val_loss: 59.2824 - val_MinusLogProbMetric: 59.2824 - lr: 2.0576e-06 - 54s/epoch - 274ms/step
Epoch 576/1000
2023-10-31 01:01:15.394 
Epoch 576/1000 
	 loss: 59.0929, MinusLogProbMetric: 59.0929, val_loss: 59.2903, val_MinusLogProbMetric: 59.2903

Epoch 576: val_loss did not improve from 59.28238
196/196 - 52s - loss: 59.0929 - MinusLogProbMetric: 59.0929 - val_loss: 59.2903 - val_MinusLogProbMetric: 59.2903 - lr: 2.0576e-06 - 52s/epoch - 265ms/step
Epoch 577/1000
2023-10-31 01:02:12.443 
Epoch 577/1000 
	 loss: 59.0702, MinusLogProbMetric: 59.0702, val_loss: 59.9493, val_MinusLogProbMetric: 59.9493

Epoch 577: val_loss did not improve from 59.28238
196/196 - 57s - loss: 59.0702 - MinusLogProbMetric: 59.0702 - val_loss: 59.9493 - val_MinusLogProbMetric: 59.9493 - lr: 2.0576e-06 - 57s/epoch - 291ms/step
Epoch 578/1000
2023-10-31 01:03:06.644 
Epoch 578/1000 
	 loss: 59.1852, MinusLogProbMetric: 59.1852, val_loss: 59.2638, val_MinusLogProbMetric: 59.2638

Epoch 578: val_loss improved from 59.28238 to 59.26377, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 59.1852 - MinusLogProbMetric: 59.1852 - val_loss: 59.2638 - val_MinusLogProbMetric: 59.2638 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 579/1000
2023-10-31 01:04:09.793 
Epoch 579/1000 
	 loss: 59.0602, MinusLogProbMetric: 59.0602, val_loss: 59.2121, val_MinusLogProbMetric: 59.2121

Epoch 579: val_loss improved from 59.26377 to 59.21211, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 59.0602 - MinusLogProbMetric: 59.0602 - val_loss: 59.2121 - val_MinusLogProbMetric: 59.2121 - lr: 2.0576e-06 - 63s/epoch - 322ms/step
Epoch 580/1000
2023-10-31 01:05:12.614 
Epoch 580/1000 
	 loss: 59.0151, MinusLogProbMetric: 59.0151, val_loss: 59.1927, val_MinusLogProbMetric: 59.1927

Epoch 580: val_loss improved from 59.21211 to 59.19270, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 59.0151 - MinusLogProbMetric: 59.0151 - val_loss: 59.1927 - val_MinusLogProbMetric: 59.1927 - lr: 2.0576e-06 - 63s/epoch - 320ms/step
Epoch 581/1000
2023-10-31 01:06:13.001 
Epoch 581/1000 
	 loss: 58.9991, MinusLogProbMetric: 58.9991, val_loss: 59.1454, val_MinusLogProbMetric: 59.1454

Epoch 581: val_loss improved from 59.19270 to 59.14535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 58.9991 - MinusLogProbMetric: 58.9991 - val_loss: 59.1454 - val_MinusLogProbMetric: 59.1454 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 582/1000
2023-10-31 01:07:11.896 
Epoch 582/1000 
	 loss: 58.9598, MinusLogProbMetric: 58.9598, val_loss: 59.1664, val_MinusLogProbMetric: 59.1664

Epoch 582: val_loss did not improve from 59.14535
196/196 - 58s - loss: 58.9598 - MinusLogProbMetric: 58.9598 - val_loss: 59.1664 - val_MinusLogProbMetric: 59.1664 - lr: 2.0576e-06 - 58s/epoch - 296ms/step
Epoch 583/1000
2023-10-31 01:08:07.025 
Epoch 583/1000 
	 loss: 58.9199, MinusLogProbMetric: 58.9199, val_loss: 59.1058, val_MinusLogProbMetric: 59.1058

Epoch 583: val_loss improved from 59.14535 to 59.10578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 58.9199 - MinusLogProbMetric: 58.9199 - val_loss: 59.1058 - val_MinusLogProbMetric: 59.1058 - lr: 2.0576e-06 - 56s/epoch - 287ms/step
Epoch 584/1000
2023-10-31 01:09:08.971 
Epoch 584/1000 
	 loss: 58.8895, MinusLogProbMetric: 58.8895, val_loss: 59.0756, val_MinusLogProbMetric: 59.0756

Epoch 584: val_loss improved from 59.10578 to 59.07558, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 58.8895 - MinusLogProbMetric: 58.8895 - val_loss: 59.0756 - val_MinusLogProbMetric: 59.0756 - lr: 2.0576e-06 - 62s/epoch - 315ms/step
Epoch 585/1000
2023-10-31 01:10:07.279 
Epoch 585/1000 
	 loss: 58.8553, MinusLogProbMetric: 58.8553, val_loss: 59.0149, val_MinusLogProbMetric: 59.0149

Epoch 585: val_loss improved from 59.07558 to 59.01492, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 58.8553 - MinusLogProbMetric: 58.8553 - val_loss: 59.0149 - val_MinusLogProbMetric: 59.0149 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 586/1000
2023-10-31 01:11:08.679 
Epoch 586/1000 
	 loss: 58.8148, MinusLogProbMetric: 58.8148, val_loss: 59.0345, val_MinusLogProbMetric: 59.0345

Epoch 586: val_loss did not improve from 59.01492
196/196 - 60s - loss: 58.8148 - MinusLogProbMetric: 58.8148 - val_loss: 59.0345 - val_MinusLogProbMetric: 59.0345 - lr: 2.0576e-06 - 60s/epoch - 309ms/step
Epoch 587/1000
2023-10-31 01:12:08.362 
Epoch 587/1000 
	 loss: 58.7890, MinusLogProbMetric: 58.7890, val_loss: 58.9856, val_MinusLogProbMetric: 58.9856

Epoch 587: val_loss improved from 59.01492 to 58.98558, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 58.7890 - MinusLogProbMetric: 58.7890 - val_loss: 58.9856 - val_MinusLogProbMetric: 58.9856 - lr: 2.0576e-06 - 61s/epoch - 309ms/step
Epoch 588/1000
2023-10-31 01:13:07.570 
Epoch 588/1000 
	 loss: 58.7674, MinusLogProbMetric: 58.7674, val_loss: 58.9168, val_MinusLogProbMetric: 58.9168

Epoch 588: val_loss improved from 58.98558 to 58.91678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 58.7674 - MinusLogProbMetric: 58.7674 - val_loss: 58.9168 - val_MinusLogProbMetric: 58.9168 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 589/1000
2023-10-31 01:14:06.299 
Epoch 589/1000 
	 loss: 58.8042, MinusLogProbMetric: 58.8042, val_loss: 58.8994, val_MinusLogProbMetric: 58.8994

Epoch 589: val_loss improved from 58.91678 to 58.89937, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 58.8042 - MinusLogProbMetric: 58.8042 - val_loss: 58.8994 - val_MinusLogProbMetric: 58.8994 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 590/1000
2023-10-31 01:15:02.983 
Epoch 590/1000 
	 loss: 58.7062, MinusLogProbMetric: 58.7062, val_loss: 58.8872, val_MinusLogProbMetric: 58.8872

Epoch 590: val_loss improved from 58.89937 to 58.88725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 58.7062 - MinusLogProbMetric: 58.7062 - val_loss: 58.8872 - val_MinusLogProbMetric: 58.8872 - lr: 2.0576e-06 - 57s/epoch - 290ms/step
Epoch 591/1000
2023-10-31 01:16:01.790 
Epoch 591/1000 
	 loss: 58.6854, MinusLogProbMetric: 58.6854, val_loss: 58.9688, val_MinusLogProbMetric: 58.9688

Epoch 591: val_loss did not improve from 58.88725
196/196 - 58s - loss: 58.6854 - MinusLogProbMetric: 58.6854 - val_loss: 58.9688 - val_MinusLogProbMetric: 58.9688 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 592/1000
2023-10-31 01:16:55.304 
Epoch 592/1000 
	 loss: 58.6660, MinusLogProbMetric: 58.6660, val_loss: 58.9164, val_MinusLogProbMetric: 58.9164

Epoch 592: val_loss did not improve from 58.88725
196/196 - 54s - loss: 58.6660 - MinusLogProbMetric: 58.6660 - val_loss: 58.9164 - val_MinusLogProbMetric: 58.9164 - lr: 2.0576e-06 - 54s/epoch - 273ms/step
Epoch 593/1000
2023-10-31 01:17:52.906 
Epoch 593/1000 
	 loss: 58.6382, MinusLogProbMetric: 58.6382, val_loss: 58.7991, val_MinusLogProbMetric: 58.7991

Epoch 593: val_loss improved from 58.88725 to 58.79910, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 58.6382 - MinusLogProbMetric: 58.6382 - val_loss: 58.7991 - val_MinusLogProbMetric: 58.7991 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 594/1000
2023-10-31 01:18:54.032 
Epoch 594/1000 
	 loss: 58.7046, MinusLogProbMetric: 58.7046, val_loss: 58.9261, val_MinusLogProbMetric: 58.9261

Epoch 594: val_loss did not improve from 58.79910
196/196 - 60s - loss: 58.7046 - MinusLogProbMetric: 58.7046 - val_loss: 58.9261 - val_MinusLogProbMetric: 58.9261 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 595/1000
2023-10-31 01:19:55.133 
Epoch 595/1000 
	 loss: 58.6171, MinusLogProbMetric: 58.6171, val_loss: 58.7666, val_MinusLogProbMetric: 58.7666

Epoch 595: val_loss improved from 58.79910 to 58.76658, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 58.6171 - MinusLogProbMetric: 58.6171 - val_loss: 58.7666 - val_MinusLogProbMetric: 58.7666 - lr: 2.0576e-06 - 62s/epoch - 317ms/step
Epoch 596/1000
2023-10-31 01:20:52.441 
Epoch 596/1000 
	 loss: 58.6030, MinusLogProbMetric: 58.6030, val_loss: 58.7165, val_MinusLogProbMetric: 58.7165

Epoch 596: val_loss improved from 58.76658 to 58.71646, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 58.6030 - MinusLogProbMetric: 58.6030 - val_loss: 58.7165 - val_MinusLogProbMetric: 58.7165 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 597/1000
2023-10-31 01:21:51.931 
Epoch 597/1000 
	 loss: 58.5574, MinusLogProbMetric: 58.5574, val_loss: 58.7131, val_MinusLogProbMetric: 58.7131

Epoch 597: val_loss improved from 58.71646 to 58.71314, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 58.5574 - MinusLogProbMetric: 58.5574 - val_loss: 58.7131 - val_MinusLogProbMetric: 58.7131 - lr: 2.0576e-06 - 59s/epoch - 304ms/step
Epoch 598/1000
2023-10-31 01:22:49.118 
Epoch 598/1000 
	 loss: 58.5237, MinusLogProbMetric: 58.5237, val_loss: 58.7078, val_MinusLogProbMetric: 58.7078

Epoch 598: val_loss improved from 58.71314 to 58.70782, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 58.5237 - MinusLogProbMetric: 58.5237 - val_loss: 58.7078 - val_MinusLogProbMetric: 58.7078 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 599/1000
2023-10-31 01:23:43.679 
Epoch 599/1000 
	 loss: 58.5172, MinusLogProbMetric: 58.5172, val_loss: 58.6560, val_MinusLogProbMetric: 58.6560

Epoch 599: val_loss improved from 58.70782 to 58.65602, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 58.5172 - MinusLogProbMetric: 58.5172 - val_loss: 58.6560 - val_MinusLogProbMetric: 58.6560 - lr: 2.0576e-06 - 55s/epoch - 279ms/step
Epoch 600/1000
2023-10-31 01:24:37.835 
Epoch 600/1000 
	 loss: 58.4923, MinusLogProbMetric: 58.4923, val_loss: 58.7032, val_MinusLogProbMetric: 58.7032

Epoch 600: val_loss did not improve from 58.65602
196/196 - 53s - loss: 58.4923 - MinusLogProbMetric: 58.4923 - val_loss: 58.7032 - val_MinusLogProbMetric: 58.7032 - lr: 2.0576e-06 - 53s/epoch - 272ms/step
Epoch 601/1000
2023-10-31 01:25:34.832 
Epoch 601/1000 
	 loss: 58.4716, MinusLogProbMetric: 58.4716, val_loss: 58.6313, val_MinusLogProbMetric: 58.6313

Epoch 601: val_loss improved from 58.65602 to 58.63132, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 58.4716 - MinusLogProbMetric: 58.4716 - val_loss: 58.6313 - val_MinusLogProbMetric: 58.6313 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 602/1000
2023-10-31 01:26:34.220 
Epoch 602/1000 
	 loss: 58.4511, MinusLogProbMetric: 58.4511, val_loss: 58.6065, val_MinusLogProbMetric: 58.6065

Epoch 602: val_loss improved from 58.63132 to 58.60650, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 58.4511 - MinusLogProbMetric: 58.4511 - val_loss: 58.6065 - val_MinusLogProbMetric: 58.6065 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 603/1000
2023-10-31 01:27:33.123 
Epoch 603/1000 
	 loss: 58.4478, MinusLogProbMetric: 58.4478, val_loss: 58.6399, val_MinusLogProbMetric: 58.6399

Epoch 603: val_loss did not improve from 58.60650
196/196 - 58s - loss: 58.4478 - MinusLogProbMetric: 58.4478 - val_loss: 58.6399 - val_MinusLogProbMetric: 58.6399 - lr: 2.0576e-06 - 58s/epoch - 296ms/step
Epoch 604/1000
2023-10-31 01:28:32.117 
Epoch 604/1000 
	 loss: 58.4089, MinusLogProbMetric: 58.4089, val_loss: 58.5796, val_MinusLogProbMetric: 58.5796

Epoch 604: val_loss improved from 58.60650 to 58.57957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 58.4089 - MinusLogProbMetric: 58.4089 - val_loss: 58.5796 - val_MinusLogProbMetric: 58.5796 - lr: 2.0576e-06 - 60s/epoch - 306ms/step
Epoch 605/1000
2023-10-31 01:29:27.596 
Epoch 605/1000 
	 loss: 58.4157, MinusLogProbMetric: 58.4157, val_loss: 58.5566, val_MinusLogProbMetric: 58.5566

Epoch 605: val_loss improved from 58.57957 to 58.55662, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 58.4157 - MinusLogProbMetric: 58.4157 - val_loss: 58.5566 - val_MinusLogProbMetric: 58.5566 - lr: 2.0576e-06 - 55s/epoch - 283ms/step
Epoch 606/1000
2023-10-31 01:30:24.956 
Epoch 606/1000 
	 loss: 58.3519, MinusLogProbMetric: 58.3519, val_loss: 58.5371, val_MinusLogProbMetric: 58.5371

Epoch 606: val_loss improved from 58.55662 to 58.53713, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 58.3519 - MinusLogProbMetric: 58.3519 - val_loss: 58.5371 - val_MinusLogProbMetric: 58.5371 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 607/1000
2023-10-31 01:31:22.053 
Epoch 607/1000 
	 loss: 58.3704, MinusLogProbMetric: 58.3704, val_loss: 58.5213, val_MinusLogProbMetric: 58.5213

Epoch 607: val_loss improved from 58.53713 to 58.52128, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 58.3704 - MinusLogProbMetric: 58.3704 - val_loss: 58.5213 - val_MinusLogProbMetric: 58.5213 - lr: 2.0576e-06 - 57s/epoch - 291ms/step
Epoch 608/1000
2023-10-31 01:32:21.497 
Epoch 608/1000 
	 loss: 58.3253, MinusLogProbMetric: 58.3253, val_loss: 58.5319, val_MinusLogProbMetric: 58.5319

Epoch 608: val_loss did not improve from 58.52128
196/196 - 59s - loss: 58.3253 - MinusLogProbMetric: 58.3253 - val_loss: 58.5319 - val_MinusLogProbMetric: 58.5319 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 609/1000
2023-10-31 01:33:18.040 
Epoch 609/1000 
	 loss: 58.3112, MinusLogProbMetric: 58.3112, val_loss: 58.4632, val_MinusLogProbMetric: 58.4632

Epoch 609: val_loss improved from 58.52128 to 58.46323, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 58.3112 - MinusLogProbMetric: 58.3112 - val_loss: 58.4632 - val_MinusLogProbMetric: 58.4632 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 610/1000
2023-10-31 01:34:19.272 
Epoch 610/1000 
	 loss: 58.4530, MinusLogProbMetric: 58.4530, val_loss: 58.4942, val_MinusLogProbMetric: 58.4942

Epoch 610: val_loss did not improve from 58.46323
196/196 - 60s - loss: 58.4530 - MinusLogProbMetric: 58.4530 - val_loss: 58.4942 - val_MinusLogProbMetric: 58.4942 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 611/1000
2023-10-31 01:35:16.222 
Epoch 611/1000 
	 loss: 58.2879, MinusLogProbMetric: 58.2879, val_loss: 58.4767, val_MinusLogProbMetric: 58.4767

Epoch 611: val_loss did not improve from 58.46323
196/196 - 57s - loss: 58.2879 - MinusLogProbMetric: 58.2879 - val_loss: 58.4767 - val_MinusLogProbMetric: 58.4767 - lr: 2.0576e-06 - 57s/epoch - 291ms/step
Epoch 612/1000
2023-10-31 01:36:16.193 
Epoch 612/1000 
	 loss: 58.2467, MinusLogProbMetric: 58.2467, val_loss: 58.4313, val_MinusLogProbMetric: 58.4313

Epoch 612: val_loss improved from 58.46323 to 58.43132, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 58.2467 - MinusLogProbMetric: 58.2467 - val_loss: 58.4313 - val_MinusLogProbMetric: 58.4313 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 613/1000
2023-10-31 01:37:14.382 
Epoch 613/1000 
	 loss: 58.2289, MinusLogProbMetric: 58.2289, val_loss: 58.3978, val_MinusLogProbMetric: 58.3978

Epoch 613: val_loss improved from 58.43132 to 58.39785, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 58.2289 - MinusLogProbMetric: 58.2289 - val_loss: 58.3978 - val_MinusLogProbMetric: 58.3978 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 614/1000
2023-10-31 01:38:14.174 
Epoch 614/1000 
	 loss: 58.1996, MinusLogProbMetric: 58.1996, val_loss: 58.4137, val_MinusLogProbMetric: 58.4137

Epoch 614: val_loss did not improve from 58.39785
196/196 - 59s - loss: 58.1996 - MinusLogProbMetric: 58.1996 - val_loss: 58.4137 - val_MinusLogProbMetric: 58.4137 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 615/1000
2023-10-31 01:39:14.148 
Epoch 615/1000 
	 loss: 58.2567, MinusLogProbMetric: 58.2567, val_loss: 58.3710, val_MinusLogProbMetric: 58.3710

Epoch 615: val_loss improved from 58.39785 to 58.37102, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 58.2567 - MinusLogProbMetric: 58.2567 - val_loss: 58.3710 - val_MinusLogProbMetric: 58.3710 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 616/1000
2023-10-31 01:40:14.075 
Epoch 616/1000 
	 loss: 58.3763, MinusLogProbMetric: 58.3763, val_loss: 58.3774, val_MinusLogProbMetric: 58.3774

Epoch 616: val_loss did not improve from 58.37102
196/196 - 59s - loss: 58.3763 - MinusLogProbMetric: 58.3763 - val_loss: 58.3774 - val_MinusLogProbMetric: 58.3774 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 617/1000
2023-10-31 01:41:09.855 
Epoch 617/1000 
	 loss: 58.1633, MinusLogProbMetric: 58.1633, val_loss: 58.3442, val_MinusLogProbMetric: 58.3442

Epoch 617: val_loss improved from 58.37102 to 58.34425, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 58.1633 - MinusLogProbMetric: 58.1633 - val_loss: 58.3442 - val_MinusLogProbMetric: 58.3442 - lr: 2.0576e-06 - 57s/epoch - 289ms/step
Epoch 618/1000
2023-10-31 01:42:10.700 
Epoch 618/1000 
	 loss: 58.1539, MinusLogProbMetric: 58.1539, val_loss: 58.3216, val_MinusLogProbMetric: 58.3216

Epoch 618: val_loss improved from 58.34425 to 58.32160, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 58.1539 - MinusLogProbMetric: 58.1539 - val_loss: 58.3216 - val_MinusLogProbMetric: 58.3216 - lr: 2.0576e-06 - 61s/epoch - 311ms/step
Epoch 619/1000
2023-10-31 01:43:12.717 
Epoch 619/1000 
	 loss: 58.1142, MinusLogProbMetric: 58.1142, val_loss: 58.2928, val_MinusLogProbMetric: 58.2928

Epoch 619: val_loss improved from 58.32160 to 58.29279, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 58.1142 - MinusLogProbMetric: 58.1142 - val_loss: 58.2928 - val_MinusLogProbMetric: 58.2928 - lr: 2.0576e-06 - 62s/epoch - 316ms/step
Epoch 620/1000
2023-10-31 01:44:09.910 
Epoch 620/1000 
	 loss: 58.1020, MinusLogProbMetric: 58.1020, val_loss: 58.3309, val_MinusLogProbMetric: 58.3309

Epoch 620: val_loss did not improve from 58.29279
196/196 - 56s - loss: 58.1020 - MinusLogProbMetric: 58.1020 - val_loss: 58.3309 - val_MinusLogProbMetric: 58.3309 - lr: 2.0576e-06 - 56s/epoch - 287ms/step
Epoch 621/1000
2023-10-31 01:45:07.630 
Epoch 621/1000 
	 loss: 58.0863, MinusLogProbMetric: 58.0863, val_loss: 58.2532, val_MinusLogProbMetric: 58.2532

Epoch 621: val_loss improved from 58.29279 to 58.25320, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 58.0863 - MinusLogProbMetric: 58.0863 - val_loss: 58.2532 - val_MinusLogProbMetric: 58.2532 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 622/1000
2023-10-31 01:46:06.716 
Epoch 622/1000 
	 loss: 58.1594, MinusLogProbMetric: 58.1594, val_loss: 58.2431, val_MinusLogProbMetric: 58.2431

Epoch 622: val_loss improved from 58.25320 to 58.24310, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 58.1594 - MinusLogProbMetric: 58.1594 - val_loss: 58.2431 - val_MinusLogProbMetric: 58.2431 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 623/1000
2023-10-31 01:47:06.211 
Epoch 623/1000 
	 loss: 58.0467, MinusLogProbMetric: 58.0467, val_loss: 58.2405, val_MinusLogProbMetric: 58.2405

Epoch 623: val_loss improved from 58.24310 to 58.24049, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 58.0467 - MinusLogProbMetric: 58.0467 - val_loss: 58.2405 - val_MinusLogProbMetric: 58.2405 - lr: 2.0576e-06 - 60s/epoch - 304ms/step
Epoch 624/1000
2023-10-31 01:48:06.659 
Epoch 624/1000 
	 loss: 58.2280, MinusLogProbMetric: 58.2280, val_loss: 58.2245, val_MinusLogProbMetric: 58.2245

Epoch 624: val_loss improved from 58.24049 to 58.22449, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 58.2280 - MinusLogProbMetric: 58.2280 - val_loss: 58.2245 - val_MinusLogProbMetric: 58.2245 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 625/1000
2023-10-31 01:49:03.965 
Epoch 625/1000 
	 loss: 58.0030, MinusLogProbMetric: 58.0030, val_loss: 58.1914, val_MinusLogProbMetric: 58.1914

Epoch 625: val_loss improved from 58.22449 to 58.19138, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 58.0030 - MinusLogProbMetric: 58.0030 - val_loss: 58.1914 - val_MinusLogProbMetric: 58.1914 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 626/1000
2023-10-31 01:50:02.317 
Epoch 626/1000 
	 loss: 57.9833, MinusLogProbMetric: 57.9833, val_loss: 58.1703, val_MinusLogProbMetric: 58.1703

Epoch 626: val_loss improved from 58.19138 to 58.17027, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 57.9833 - MinusLogProbMetric: 57.9833 - val_loss: 58.1703 - val_MinusLogProbMetric: 58.1703 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 627/1000
2023-10-31 01:51:03.823 
Epoch 627/1000 
	 loss: 57.9607, MinusLogProbMetric: 57.9607, val_loss: 58.1688, val_MinusLogProbMetric: 58.1688

Epoch 627: val_loss improved from 58.17027 to 58.16877, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 57.9607 - MinusLogProbMetric: 57.9607 - val_loss: 58.1688 - val_MinusLogProbMetric: 58.1688 - lr: 2.0576e-06 - 62s/epoch - 314ms/step
Epoch 628/1000
2023-10-31 01:52:02.423 
Epoch 628/1000 
	 loss: 57.9633, MinusLogProbMetric: 57.9633, val_loss: 58.1316, val_MinusLogProbMetric: 58.1316

Epoch 628: val_loss improved from 58.16877 to 58.13156, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 57.9633 - MinusLogProbMetric: 57.9633 - val_loss: 58.1316 - val_MinusLogProbMetric: 58.1316 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 629/1000
2023-10-31 01:53:00.218 
Epoch 629/1000 
	 loss: 57.9512, MinusLogProbMetric: 57.9512, val_loss: 58.1025, val_MinusLogProbMetric: 58.1025

Epoch 629: val_loss improved from 58.13156 to 58.10253, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 57.9512 - MinusLogProbMetric: 57.9512 - val_loss: 58.1025 - val_MinusLogProbMetric: 58.1025 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 630/1000
2023-10-31 01:54:00.977 
Epoch 630/1000 
	 loss: 57.9185, MinusLogProbMetric: 57.9185, val_loss: 58.1089, val_MinusLogProbMetric: 58.1089

Epoch 630: val_loss did not improve from 58.10253
196/196 - 60s - loss: 57.9185 - MinusLogProbMetric: 57.9185 - val_loss: 58.1089 - val_MinusLogProbMetric: 58.1089 - lr: 2.0576e-06 - 60s/epoch - 306ms/step
Epoch 631/1000
2023-10-31 01:55:00.600 
Epoch 631/1000 
	 loss: 57.8962, MinusLogProbMetric: 57.8962, val_loss: 58.0885, val_MinusLogProbMetric: 58.0885

Epoch 631: val_loss improved from 58.10253 to 58.08849, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 57.8962 - MinusLogProbMetric: 57.8962 - val_loss: 58.0885 - val_MinusLogProbMetric: 58.0885 - lr: 2.0576e-06 - 60s/epoch - 309ms/step
Epoch 632/1000
2023-10-31 01:56:01.157 
Epoch 632/1000 
	 loss: 57.8783, MinusLogProbMetric: 57.8783, val_loss: 58.0593, val_MinusLogProbMetric: 58.0593

Epoch 632: val_loss improved from 58.08849 to 58.05933, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 57.8783 - MinusLogProbMetric: 57.8783 - val_loss: 58.0593 - val_MinusLogProbMetric: 58.0593 - lr: 2.0576e-06 - 60s/epoch - 309ms/step
Epoch 633/1000
2023-10-31 01:57:00.360 
Epoch 633/1000 
	 loss: 58.1808, MinusLogProbMetric: 58.1808, val_loss: 58.0854, val_MinusLogProbMetric: 58.0854

Epoch 633: val_loss did not improve from 58.05933
196/196 - 58s - loss: 58.1808 - MinusLogProbMetric: 58.1808 - val_loss: 58.0854 - val_MinusLogProbMetric: 58.0854 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 634/1000
2023-10-31 01:57:58.914 
Epoch 634/1000 
	 loss: 57.8556, MinusLogProbMetric: 57.8556, val_loss: 58.0649, val_MinusLogProbMetric: 58.0649

Epoch 634: val_loss did not improve from 58.05933
196/196 - 59s - loss: 57.8556 - MinusLogProbMetric: 57.8556 - val_loss: 58.0649 - val_MinusLogProbMetric: 58.0649 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 635/1000
2023-10-31 01:58:56.708 
Epoch 635/1000 
	 loss: 57.8481, MinusLogProbMetric: 57.8481, val_loss: 58.0646, val_MinusLogProbMetric: 58.0646

Epoch 635: val_loss did not improve from 58.05933
196/196 - 58s - loss: 57.8481 - MinusLogProbMetric: 57.8481 - val_loss: 58.0646 - val_MinusLogProbMetric: 58.0646 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 636/1000
2023-10-31 01:59:55.531 
Epoch 636/1000 
	 loss: 57.8074, MinusLogProbMetric: 57.8074, val_loss: 58.0109, val_MinusLogProbMetric: 58.0109

Epoch 636: val_loss improved from 58.05933 to 58.01091, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 57.8074 - MinusLogProbMetric: 57.8074 - val_loss: 58.0109 - val_MinusLogProbMetric: 58.0109 - lr: 2.0576e-06 - 60s/epoch - 304ms/step
Epoch 637/1000
2023-10-31 02:00:58.449 
Epoch 637/1000 
	 loss: 57.7919, MinusLogProbMetric: 57.7919, val_loss: 58.0031, val_MinusLogProbMetric: 58.0031

Epoch 637: val_loss improved from 58.01091 to 58.00313, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 57.7919 - MinusLogProbMetric: 57.7919 - val_loss: 58.0031 - val_MinusLogProbMetric: 58.0031 - lr: 2.0576e-06 - 63s/epoch - 321ms/step
Epoch 638/1000
2023-10-31 02:01:57.944 
Epoch 638/1000 
	 loss: 57.7813, MinusLogProbMetric: 57.7813, val_loss: 57.9648, val_MinusLogProbMetric: 57.9648

Epoch 638: val_loss improved from 58.00313 to 57.96478, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 57.7813 - MinusLogProbMetric: 57.7813 - val_loss: 57.9648 - val_MinusLogProbMetric: 57.9648 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 639/1000
2023-10-31 02:02:58.072 
Epoch 639/1000 
	 loss: 57.7665, MinusLogProbMetric: 57.7665, val_loss: 57.9284, val_MinusLogProbMetric: 57.9284

Epoch 639: val_loss improved from 57.96478 to 57.92842, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 57.7665 - MinusLogProbMetric: 57.7665 - val_loss: 57.9284 - val_MinusLogProbMetric: 57.9284 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 640/1000
2023-10-31 02:03:53.494 
Epoch 640/1000 
	 loss: 57.7899, MinusLogProbMetric: 57.7899, val_loss: 57.9150, val_MinusLogProbMetric: 57.9150

Epoch 640: val_loss improved from 57.92842 to 57.91498, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 57.7899 - MinusLogProbMetric: 57.7899 - val_loss: 57.9150 - val_MinusLogProbMetric: 57.9150 - lr: 2.0576e-06 - 55s/epoch - 283ms/step
Epoch 641/1000
2023-10-31 02:04:52.736 
Epoch 641/1000 
	 loss: 57.7218, MinusLogProbMetric: 57.7218, val_loss: 58.3276, val_MinusLogProbMetric: 58.3276

Epoch 641: val_loss did not improve from 57.91498
196/196 - 58s - loss: 57.7218 - MinusLogProbMetric: 57.7218 - val_loss: 58.3276 - val_MinusLogProbMetric: 58.3276 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 642/1000
2023-10-31 02:05:50.575 
Epoch 642/1000 
	 loss: 57.8189, MinusLogProbMetric: 57.8189, val_loss: 57.9047, val_MinusLogProbMetric: 57.9047

Epoch 642: val_loss improved from 57.91498 to 57.90470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 57.8189 - MinusLogProbMetric: 57.8189 - val_loss: 57.9047 - val_MinusLogProbMetric: 57.9047 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 643/1000
2023-10-31 02:06:51.560 
Epoch 643/1000 
	 loss: 57.7158, MinusLogProbMetric: 57.7158, val_loss: 57.8910, val_MinusLogProbMetric: 57.8910

Epoch 643: val_loss improved from 57.90470 to 57.89100, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 57.7158 - MinusLogProbMetric: 57.7158 - val_loss: 57.8910 - val_MinusLogProbMetric: 57.8910 - lr: 2.0576e-06 - 61s/epoch - 311ms/step
Epoch 644/1000
2023-10-31 02:07:51.067 
Epoch 644/1000 
	 loss: 57.8679, MinusLogProbMetric: 57.8679, val_loss: 57.9060, val_MinusLogProbMetric: 57.9060

Epoch 644: val_loss did not improve from 57.89100
196/196 - 59s - loss: 57.8679 - MinusLogProbMetric: 57.8679 - val_loss: 57.9060 - val_MinusLogProbMetric: 57.9060 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 645/1000
2023-10-31 02:08:48.824 
Epoch 645/1000 
	 loss: 57.6686, MinusLogProbMetric: 57.6686, val_loss: 57.8501, val_MinusLogProbMetric: 57.8501

Epoch 645: val_loss improved from 57.89100 to 57.85009, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 57.6686 - MinusLogProbMetric: 57.6686 - val_loss: 57.8501 - val_MinusLogProbMetric: 57.8501 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 646/1000
2023-10-31 02:09:51.884 
Epoch 646/1000 
	 loss: 57.8896, MinusLogProbMetric: 57.8896, val_loss: 57.9207, val_MinusLogProbMetric: 57.9207

Epoch 646: val_loss did not improve from 57.85009
196/196 - 62s - loss: 57.8896 - MinusLogProbMetric: 57.8896 - val_loss: 57.9207 - val_MinusLogProbMetric: 57.9207 - lr: 2.0576e-06 - 62s/epoch - 317ms/step
Epoch 647/1000
2023-10-31 02:10:52.115 
Epoch 647/1000 
	 loss: 57.7198, MinusLogProbMetric: 57.7198, val_loss: 57.8870, val_MinusLogProbMetric: 57.8870

Epoch 647: val_loss did not improve from 57.85009
196/196 - 60s - loss: 57.7198 - MinusLogProbMetric: 57.7198 - val_loss: 57.8870 - val_MinusLogProbMetric: 57.8870 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 648/1000
2023-10-31 02:11:54.353 
Epoch 648/1000 
	 loss: 57.7008, MinusLogProbMetric: 57.7008, val_loss: 57.8629, val_MinusLogProbMetric: 57.8629

Epoch 648: val_loss did not improve from 57.85009
196/196 - 62s - loss: 57.7008 - MinusLogProbMetric: 57.7008 - val_loss: 57.8629 - val_MinusLogProbMetric: 57.8629 - lr: 2.0576e-06 - 62s/epoch - 318ms/step
Epoch 649/1000
2023-10-31 02:12:53.536 
Epoch 649/1000 
	 loss: 57.6732, MinusLogProbMetric: 57.6732, val_loss: 57.8839, val_MinusLogProbMetric: 57.8839

Epoch 649: val_loss did not improve from 57.85009
196/196 - 59s - loss: 57.6732 - MinusLogProbMetric: 57.6732 - val_loss: 57.8839 - val_MinusLogProbMetric: 57.8839 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 650/1000
2023-10-31 02:13:51.396 
Epoch 650/1000 
	 loss: 57.6261, MinusLogProbMetric: 57.6261, val_loss: 57.8193, val_MinusLogProbMetric: 57.8193

Epoch 650: val_loss improved from 57.85009 to 57.81928, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 57.6261 - MinusLogProbMetric: 57.6261 - val_loss: 57.8193 - val_MinusLogProbMetric: 57.8193 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 651/1000
2023-10-31 02:14:51.908 
Epoch 651/1000 
	 loss: 57.6509, MinusLogProbMetric: 57.6509, val_loss: 57.8081, val_MinusLogProbMetric: 57.8081

Epoch 651: val_loss improved from 57.81928 to 57.80812, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 57.6509 - MinusLogProbMetric: 57.6509 - val_loss: 57.8081 - val_MinusLogProbMetric: 57.8081 - lr: 2.0576e-06 - 61s/epoch - 309ms/step
Epoch 652/1000
2023-10-31 02:15:51.409 
Epoch 652/1000 
	 loss: 57.5835, MinusLogProbMetric: 57.5835, val_loss: 57.7645, val_MinusLogProbMetric: 57.7645

Epoch 652: val_loss improved from 57.80812 to 57.76447, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 57.5835 - MinusLogProbMetric: 57.5835 - val_loss: 57.7645 - val_MinusLogProbMetric: 57.7645 - lr: 2.0576e-06 - 60s/epoch - 304ms/step
Epoch 653/1000
2023-10-31 02:16:53.030 
Epoch 653/1000 
	 loss: 57.5658, MinusLogProbMetric: 57.5658, val_loss: 57.7936, val_MinusLogProbMetric: 57.7936

Epoch 653: val_loss did not improve from 57.76447
196/196 - 61s - loss: 57.5658 - MinusLogProbMetric: 57.5658 - val_loss: 57.7936 - val_MinusLogProbMetric: 57.7936 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 654/1000
2023-10-31 02:17:50.033 
Epoch 654/1000 
	 loss: 57.5276, MinusLogProbMetric: 57.5276, val_loss: 57.7377, val_MinusLogProbMetric: 57.7377

Epoch 654: val_loss improved from 57.76447 to 57.73768, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 57.5276 - MinusLogProbMetric: 57.5276 - val_loss: 57.7377 - val_MinusLogProbMetric: 57.7377 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 655/1000
2023-10-31 02:18:46.208 
Epoch 655/1000 
	 loss: 57.5037, MinusLogProbMetric: 57.5037, val_loss: 57.6805, val_MinusLogProbMetric: 57.6805

Epoch 655: val_loss improved from 57.73768 to 57.68045, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 57.5037 - MinusLogProbMetric: 57.5037 - val_loss: 57.6805 - val_MinusLogProbMetric: 57.6805 - lr: 2.0576e-06 - 56s/epoch - 287ms/step
Epoch 656/1000
2023-10-31 02:19:44.435 
Epoch 656/1000 
	 loss: 57.4887, MinusLogProbMetric: 57.4887, val_loss: 57.7167, val_MinusLogProbMetric: 57.7167

Epoch 656: val_loss did not improve from 57.68045
196/196 - 57s - loss: 57.4887 - MinusLogProbMetric: 57.4887 - val_loss: 57.7167 - val_MinusLogProbMetric: 57.7167 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 657/1000
2023-10-31 02:20:46.888 
Epoch 657/1000 
	 loss: 57.4698, MinusLogProbMetric: 57.4698, val_loss: 57.6865, val_MinusLogProbMetric: 57.6865

Epoch 657: val_loss did not improve from 57.68045
196/196 - 62s - loss: 57.4698 - MinusLogProbMetric: 57.4698 - val_loss: 57.6865 - val_MinusLogProbMetric: 57.6865 - lr: 2.0576e-06 - 62s/epoch - 319ms/step
Epoch 658/1000
2023-10-31 02:21:44.731 
Epoch 658/1000 
	 loss: 57.4425, MinusLogProbMetric: 57.4425, val_loss: 57.6567, val_MinusLogProbMetric: 57.6567

Epoch 658: val_loss improved from 57.68045 to 57.65671, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 57.4425 - MinusLogProbMetric: 57.4425 - val_loss: 57.6567 - val_MinusLogProbMetric: 57.6567 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 659/1000
2023-10-31 02:22:45.868 
Epoch 659/1000 
	 loss: 57.4338, MinusLogProbMetric: 57.4338, val_loss: 57.6134, val_MinusLogProbMetric: 57.6134

Epoch 659: val_loss improved from 57.65671 to 57.61344, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 57.4338 - MinusLogProbMetric: 57.4338 - val_loss: 57.6134 - val_MinusLogProbMetric: 57.6134 - lr: 2.0576e-06 - 61s/epoch - 312ms/step
Epoch 660/1000
2023-10-31 02:23:48.605 
Epoch 660/1000 
	 loss: 57.4088, MinusLogProbMetric: 57.4088, val_loss: 57.6072, val_MinusLogProbMetric: 57.6072

Epoch 660: val_loss improved from 57.61344 to 57.60722, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 57.4088 - MinusLogProbMetric: 57.4088 - val_loss: 57.6072 - val_MinusLogProbMetric: 57.6072 - lr: 2.0576e-06 - 63s/epoch - 320ms/step
Epoch 661/1000
2023-10-31 02:24:51.531 
Epoch 661/1000 
	 loss: 57.3787, MinusLogProbMetric: 57.3787, val_loss: 57.5703, val_MinusLogProbMetric: 57.5703

Epoch 661: val_loss improved from 57.60722 to 57.57029, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 57.3787 - MinusLogProbMetric: 57.3787 - val_loss: 57.5703 - val_MinusLogProbMetric: 57.5703 - lr: 2.0576e-06 - 63s/epoch - 321ms/step
Epoch 662/1000
2023-10-31 02:25:49.227 
Epoch 662/1000 
	 loss: 57.3700, MinusLogProbMetric: 57.3700, val_loss: 57.5586, val_MinusLogProbMetric: 57.5586

Epoch 662: val_loss improved from 57.57029 to 57.55857, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 57.3700 - MinusLogProbMetric: 57.3700 - val_loss: 57.5586 - val_MinusLogProbMetric: 57.5586 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 663/1000
2023-10-31 02:26:47.128 
Epoch 663/1000 
	 loss: 57.3520, MinusLogProbMetric: 57.3520, val_loss: 57.5332, val_MinusLogProbMetric: 57.5332

Epoch 663: val_loss improved from 57.55857 to 57.53318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 57.3520 - MinusLogProbMetric: 57.3520 - val_loss: 57.5332 - val_MinusLogProbMetric: 57.5332 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 664/1000
2023-10-31 02:27:49.966 
Epoch 664/1000 
	 loss: 57.3218, MinusLogProbMetric: 57.3218, val_loss: 57.5326, val_MinusLogProbMetric: 57.5326

Epoch 664: val_loss improved from 57.53318 to 57.53259, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 57.3218 - MinusLogProbMetric: 57.3218 - val_loss: 57.5326 - val_MinusLogProbMetric: 57.5326 - lr: 2.0576e-06 - 63s/epoch - 320ms/step
Epoch 665/1000
2023-10-31 02:28:48.576 
Epoch 665/1000 
	 loss: 57.3144, MinusLogProbMetric: 57.3144, val_loss: 57.5264, val_MinusLogProbMetric: 57.5264

Epoch 665: val_loss improved from 57.53259 to 57.52642, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 57.3144 - MinusLogProbMetric: 57.3144 - val_loss: 57.5264 - val_MinusLogProbMetric: 57.5264 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 666/1000
2023-10-31 02:29:52.004 
Epoch 666/1000 
	 loss: 57.2990, MinusLogProbMetric: 57.2990, val_loss: 57.4795, val_MinusLogProbMetric: 57.4795

Epoch 666: val_loss improved from 57.52642 to 57.47949, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 57.2990 - MinusLogProbMetric: 57.2990 - val_loss: 57.4795 - val_MinusLogProbMetric: 57.4795 - lr: 2.0576e-06 - 63s/epoch - 324ms/step
Epoch 667/1000
2023-10-31 02:30:51.976 
Epoch 667/1000 
	 loss: 57.2820, MinusLogProbMetric: 57.2820, val_loss: 57.4870, val_MinusLogProbMetric: 57.4870

Epoch 667: val_loss did not improve from 57.47949
196/196 - 59s - loss: 57.2820 - MinusLogProbMetric: 57.2820 - val_loss: 57.4870 - val_MinusLogProbMetric: 57.4870 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 668/1000
2023-10-31 02:31:54.373 
Epoch 668/1000 
	 loss: 57.2628, MinusLogProbMetric: 57.2628, val_loss: 57.4471, val_MinusLogProbMetric: 57.4471

Epoch 668: val_loss improved from 57.47949 to 57.44714, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 57.2628 - MinusLogProbMetric: 57.2628 - val_loss: 57.4471 - val_MinusLogProbMetric: 57.4471 - lr: 2.0576e-06 - 63s/epoch - 323ms/step
Epoch 669/1000
2023-10-31 02:32:55.739 
Epoch 669/1000 
	 loss: 57.4708, MinusLogProbMetric: 57.4708, val_loss: 57.4733, val_MinusLogProbMetric: 57.4733

Epoch 669: val_loss did not improve from 57.44714
196/196 - 60s - loss: 57.4708 - MinusLogProbMetric: 57.4708 - val_loss: 57.4733 - val_MinusLogProbMetric: 57.4733 - lr: 2.0576e-06 - 60s/epoch - 309ms/step
Epoch 670/1000
2023-10-31 02:33:50.912 
Epoch 670/1000 
	 loss: 57.3065, MinusLogProbMetric: 57.3065, val_loss: 57.4546, val_MinusLogProbMetric: 57.4546

Epoch 670: val_loss did not improve from 57.44714
196/196 - 55s - loss: 57.3065 - MinusLogProbMetric: 57.3065 - val_loss: 57.4546 - val_MinusLogProbMetric: 57.4546 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 671/1000
2023-10-31 02:34:51.575 
Epoch 671/1000 
	 loss: 57.2238, MinusLogProbMetric: 57.2238, val_loss: 57.4361, val_MinusLogProbMetric: 57.4361

Epoch 671: val_loss improved from 57.44714 to 57.43610, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 57.2238 - MinusLogProbMetric: 57.2238 - val_loss: 57.4361 - val_MinusLogProbMetric: 57.4361 - lr: 2.0576e-06 - 62s/epoch - 315ms/step
Epoch 672/1000
2023-10-31 02:35:49.612 
Epoch 672/1000 
	 loss: 57.2631, MinusLogProbMetric: 57.2631, val_loss: 57.4117, val_MinusLogProbMetric: 57.4117

Epoch 672: val_loss improved from 57.43610 to 57.41173, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 57.2631 - MinusLogProbMetric: 57.2631 - val_loss: 57.4117 - val_MinusLogProbMetric: 57.4117 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 673/1000
2023-10-31 02:36:46.720 
Epoch 673/1000 
	 loss: 57.3110, MinusLogProbMetric: 57.3110, val_loss: 57.3897, val_MinusLogProbMetric: 57.3897

Epoch 673: val_loss improved from 57.41173 to 57.38967, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 57.3110 - MinusLogProbMetric: 57.3110 - val_loss: 57.3897 - val_MinusLogProbMetric: 57.3897 - lr: 2.0576e-06 - 57s/epoch - 291ms/step
Epoch 674/1000
2023-10-31 02:37:43.023 
Epoch 674/1000 
	 loss: 57.1648, MinusLogProbMetric: 57.1648, val_loss: 57.3962, val_MinusLogProbMetric: 57.3962

Epoch 674: val_loss did not improve from 57.38967
196/196 - 56s - loss: 57.1648 - MinusLogProbMetric: 57.1648 - val_loss: 57.3962 - val_MinusLogProbMetric: 57.3962 - lr: 2.0576e-06 - 56s/epoch - 283ms/step
Epoch 675/1000
2023-10-31 02:38:41.462 
Epoch 675/1000 
	 loss: 57.1480, MinusLogProbMetric: 57.1480, val_loss: 57.3399, val_MinusLogProbMetric: 57.3399

Epoch 675: val_loss improved from 57.38967 to 57.33990, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 57.1480 - MinusLogProbMetric: 57.1480 - val_loss: 57.3399 - val_MinusLogProbMetric: 57.3399 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 676/1000
2023-10-31 02:39:43.083 
Epoch 676/1000 
	 loss: 58.0794, MinusLogProbMetric: 58.0794, val_loss: 57.5097, val_MinusLogProbMetric: 57.5097

Epoch 676: val_loss did not improve from 57.33990
196/196 - 61s - loss: 58.0794 - MinusLogProbMetric: 58.0794 - val_loss: 57.5097 - val_MinusLogProbMetric: 57.5097 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 677/1000
2023-10-31 02:40:41.503 
Epoch 677/1000 
	 loss: 57.1929, MinusLogProbMetric: 57.1929, val_loss: 57.3475, val_MinusLogProbMetric: 57.3475

Epoch 677: val_loss did not improve from 57.33990
196/196 - 58s - loss: 57.1929 - MinusLogProbMetric: 57.1929 - val_loss: 57.3475 - val_MinusLogProbMetric: 57.3475 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 678/1000
2023-10-31 02:41:39.341 
Epoch 678/1000 
	 loss: 57.1305, MinusLogProbMetric: 57.1305, val_loss: 57.3058, val_MinusLogProbMetric: 57.3058

Epoch 678: val_loss improved from 57.33990 to 57.30584, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 57.1305 - MinusLogProbMetric: 57.1305 - val_loss: 57.3058 - val_MinusLogProbMetric: 57.3058 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 679/1000
2023-10-31 02:42:40.402 
Epoch 679/1000 
	 loss: 57.0946, MinusLogProbMetric: 57.0946, val_loss: 57.3023, val_MinusLogProbMetric: 57.3023

Epoch 679: val_loss improved from 57.30584 to 57.30233, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 57.0946 - MinusLogProbMetric: 57.0946 - val_loss: 57.3023 - val_MinusLogProbMetric: 57.3023 - lr: 2.0576e-06 - 61s/epoch - 312ms/step
Epoch 680/1000
2023-10-31 02:43:43.239 
Epoch 680/1000 
	 loss: 57.0725, MinusLogProbMetric: 57.0725, val_loss: 57.2653, val_MinusLogProbMetric: 57.2653

Epoch 680: val_loss improved from 57.30233 to 57.26530, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 57.0725 - MinusLogProbMetric: 57.0725 - val_loss: 57.2653 - val_MinusLogProbMetric: 57.2653 - lr: 2.0576e-06 - 63s/epoch - 321ms/step
Epoch 681/1000
2023-10-31 02:44:44.873 
Epoch 681/1000 
	 loss: 57.0619, MinusLogProbMetric: 57.0619, val_loss: 57.2575, val_MinusLogProbMetric: 57.2575

Epoch 681: val_loss improved from 57.26530 to 57.25750, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 57.0619 - MinusLogProbMetric: 57.0619 - val_loss: 57.2575 - val_MinusLogProbMetric: 57.2575 - lr: 2.0576e-06 - 62s/epoch - 315ms/step
Epoch 682/1000
2023-10-31 02:45:46.305 
Epoch 682/1000 
	 loss: 57.0410, MinusLogProbMetric: 57.0410, val_loss: 57.2947, val_MinusLogProbMetric: 57.2947

Epoch 682: val_loss did not improve from 57.25750
196/196 - 60s - loss: 57.0410 - MinusLogProbMetric: 57.0410 - val_loss: 57.2947 - val_MinusLogProbMetric: 57.2947 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 683/1000
2023-10-31 02:46:43.048 
Epoch 683/1000 
	 loss: 57.0198, MinusLogProbMetric: 57.0198, val_loss: 57.2268, val_MinusLogProbMetric: 57.2268

Epoch 683: val_loss improved from 57.25750 to 57.22684, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 57.0198 - MinusLogProbMetric: 57.0198 - val_loss: 57.2268 - val_MinusLogProbMetric: 57.2268 - lr: 2.0576e-06 - 58s/epoch - 294ms/step
Epoch 684/1000
2023-10-31 02:47:42.686 
Epoch 684/1000 
	 loss: 57.0142, MinusLogProbMetric: 57.0142, val_loss: 57.1855, val_MinusLogProbMetric: 57.1855

Epoch 684: val_loss improved from 57.22684 to 57.18550, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 57.0142 - MinusLogProbMetric: 57.0142 - val_loss: 57.1855 - val_MinusLogProbMetric: 57.1855 - lr: 2.0576e-06 - 60s/epoch - 304ms/step
Epoch 685/1000
2023-10-31 02:48:45.075 
Epoch 685/1000 
	 loss: 56.9817, MinusLogProbMetric: 56.9817, val_loss: 57.2214, val_MinusLogProbMetric: 57.2214

Epoch 685: val_loss did not improve from 57.18550
196/196 - 62s - loss: 56.9817 - MinusLogProbMetric: 56.9817 - val_loss: 57.2214 - val_MinusLogProbMetric: 57.2214 - lr: 2.0576e-06 - 62s/epoch - 314ms/step
Epoch 686/1000
2023-10-31 02:49:48.003 
Epoch 686/1000 
	 loss: 56.9701, MinusLogProbMetric: 56.9701, val_loss: 57.2100, val_MinusLogProbMetric: 57.2100

Epoch 686: val_loss did not improve from 57.18550
196/196 - 63s - loss: 56.9701 - MinusLogProbMetric: 56.9701 - val_loss: 57.2100 - val_MinusLogProbMetric: 57.2100 - lr: 2.0576e-06 - 63s/epoch - 321ms/step
Epoch 687/1000
2023-10-31 02:50:51.659 
Epoch 687/1000 
	 loss: 56.9548, MinusLogProbMetric: 56.9548, val_loss: 57.1778, val_MinusLogProbMetric: 57.1778

Epoch 687: val_loss improved from 57.18550 to 57.17779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 65s - loss: 56.9548 - MinusLogProbMetric: 56.9548 - val_loss: 57.1778 - val_MinusLogProbMetric: 57.1778 - lr: 2.0576e-06 - 65s/epoch - 329ms/step
Epoch 688/1000
2023-10-31 02:51:54.196 
Epoch 688/1000 
	 loss: 56.9586, MinusLogProbMetric: 56.9586, val_loss: 57.1704, val_MinusLogProbMetric: 57.1704

Epoch 688: val_loss improved from 57.17779 to 57.17039, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 56.9586 - MinusLogProbMetric: 56.9586 - val_loss: 57.1704 - val_MinusLogProbMetric: 57.1704 - lr: 2.0576e-06 - 63s/epoch - 319ms/step
Epoch 689/1000
2023-10-31 02:52:55.495 
Epoch 689/1000 
	 loss: 56.9228, MinusLogProbMetric: 56.9228, val_loss: 57.1344, val_MinusLogProbMetric: 57.1344

Epoch 689: val_loss improved from 57.17039 to 57.13436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 56.9228 - MinusLogProbMetric: 56.9228 - val_loss: 57.1344 - val_MinusLogProbMetric: 57.1344 - lr: 2.0576e-06 - 61s/epoch - 313ms/step
Epoch 690/1000
2023-10-31 02:53:57.948 
Epoch 690/1000 
	 loss: 56.9073, MinusLogProbMetric: 56.9073, val_loss: 57.1378, val_MinusLogProbMetric: 57.1378

Epoch 690: val_loss did not improve from 57.13436
196/196 - 62s - loss: 56.9073 - MinusLogProbMetric: 56.9073 - val_loss: 57.1378 - val_MinusLogProbMetric: 57.1378 - lr: 2.0576e-06 - 62s/epoch - 314ms/step
Epoch 691/1000
2023-10-31 02:55:00.776 
Epoch 691/1000 
	 loss: 56.8873, MinusLogProbMetric: 56.8873, val_loss: 57.1509, val_MinusLogProbMetric: 57.1509

Epoch 691: val_loss did not improve from 57.13436
196/196 - 63s - loss: 56.8873 - MinusLogProbMetric: 56.8873 - val_loss: 57.1509 - val_MinusLogProbMetric: 57.1509 - lr: 2.0576e-06 - 63s/epoch - 321ms/step
Epoch 692/1000
2023-10-31 02:56:02.714 
Epoch 692/1000 
	 loss: 56.8880, MinusLogProbMetric: 56.8880, val_loss: 57.1090, val_MinusLogProbMetric: 57.1090

Epoch 692: val_loss improved from 57.13436 to 57.10898, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 56.8880 - MinusLogProbMetric: 56.8880 - val_loss: 57.1090 - val_MinusLogProbMetric: 57.1090 - lr: 2.0576e-06 - 63s/epoch - 319ms/step
Epoch 693/1000
2023-10-31 02:57:01.070 
Epoch 693/1000 
	 loss: 56.8553, MinusLogProbMetric: 56.8553, val_loss: 57.1004, val_MinusLogProbMetric: 57.1004

Epoch 693: val_loss improved from 57.10898 to 57.10039, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 56.8553 - MinusLogProbMetric: 56.8553 - val_loss: 57.1004 - val_MinusLogProbMetric: 57.1004 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 694/1000
2023-10-31 02:58:01.498 
Epoch 694/1000 
	 loss: 56.8715, MinusLogProbMetric: 56.8715, val_loss: 57.0659, val_MinusLogProbMetric: 57.0659

Epoch 694: val_loss improved from 57.10039 to 57.06586, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 56.8715 - MinusLogProbMetric: 56.8715 - val_loss: 57.0659 - val_MinusLogProbMetric: 57.0659 - lr: 2.0576e-06 - 61s/epoch - 309ms/step
Epoch 695/1000
2023-10-31 02:59:00.922 
Epoch 695/1000 
	 loss: 56.8316, MinusLogProbMetric: 56.8316, val_loss: 57.0456, val_MinusLogProbMetric: 57.0456

Epoch 695: val_loss improved from 57.06586 to 57.04556, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 56.8316 - MinusLogProbMetric: 56.8316 - val_loss: 57.0456 - val_MinusLogProbMetric: 57.0456 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 696/1000
2023-10-31 02:59:58.188 
Epoch 696/1000 
	 loss: 56.8060, MinusLogProbMetric: 56.8060, val_loss: 57.0348, val_MinusLogProbMetric: 57.0348

Epoch 696: val_loss improved from 57.04556 to 57.03476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 56.8060 - MinusLogProbMetric: 56.8060 - val_loss: 57.0348 - val_MinusLogProbMetric: 57.0348 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 697/1000
2023-10-31 03:00:55.425 
Epoch 697/1000 
	 loss: 56.7907, MinusLogProbMetric: 56.7907, val_loss: 57.0116, val_MinusLogProbMetric: 57.0116

Epoch 697: val_loss improved from 57.03476 to 57.01162, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 56.7907 - MinusLogProbMetric: 56.7907 - val_loss: 57.0116 - val_MinusLogProbMetric: 57.0116 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 698/1000
2023-10-31 03:01:54.709 
Epoch 698/1000 
	 loss: 56.7907, MinusLogProbMetric: 56.7907, val_loss: 57.0013, val_MinusLogProbMetric: 57.0013

Epoch 698: val_loss improved from 57.01162 to 57.00129, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 56.7907 - MinusLogProbMetric: 56.7907 - val_loss: 57.0013 - val_MinusLogProbMetric: 57.0013 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 699/1000
2023-10-31 03:02:53.747 
Epoch 699/1000 
	 loss: 56.7587, MinusLogProbMetric: 56.7587, val_loss: 56.9837, val_MinusLogProbMetric: 56.9837

Epoch 699: val_loss improved from 57.00129 to 56.98370, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 56.7587 - MinusLogProbMetric: 56.7587 - val_loss: 56.9837 - val_MinusLogProbMetric: 56.9837 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 700/1000
2023-10-31 03:03:52.714 
Epoch 700/1000 
	 loss: 56.7366, MinusLogProbMetric: 56.7366, val_loss: 56.9778, val_MinusLogProbMetric: 56.9778

Epoch 700: val_loss improved from 56.98370 to 56.97776, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 56.7366 - MinusLogProbMetric: 56.7366 - val_loss: 56.9778 - val_MinusLogProbMetric: 56.9778 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 701/1000
2023-10-31 03:04:50.048 
Epoch 701/1000 
	 loss: 56.7270, MinusLogProbMetric: 56.7270, val_loss: 56.9442, val_MinusLogProbMetric: 56.9442

Epoch 701: val_loss improved from 56.97776 to 56.94424, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 56.7270 - MinusLogProbMetric: 56.7270 - val_loss: 56.9442 - val_MinusLogProbMetric: 56.9442 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 702/1000
2023-10-31 03:05:48.612 
Epoch 702/1000 
	 loss: 56.7042, MinusLogProbMetric: 56.7042, val_loss: 56.9781, val_MinusLogProbMetric: 56.9781

Epoch 702: val_loss did not improve from 56.94424
196/196 - 58s - loss: 56.7042 - MinusLogProbMetric: 56.7042 - val_loss: 56.9781 - val_MinusLogProbMetric: 56.9781 - lr: 2.0576e-06 - 58s/epoch - 294ms/step
Epoch 703/1000
2023-10-31 03:06:44.671 
Epoch 703/1000 
	 loss: 56.7020, MinusLogProbMetric: 56.7020, val_loss: 56.9226, val_MinusLogProbMetric: 56.9226

Epoch 703: val_loss improved from 56.94424 to 56.92261, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 56.7020 - MinusLogProbMetric: 56.7020 - val_loss: 56.9226 - val_MinusLogProbMetric: 56.9226 - lr: 2.0576e-06 - 57s/epoch - 291ms/step
Epoch 704/1000
2023-10-31 03:07:44.518 
Epoch 704/1000 
	 loss: 56.6815, MinusLogProbMetric: 56.6815, val_loss: 56.9074, val_MinusLogProbMetric: 56.9074

Epoch 704: val_loss improved from 56.92261 to 56.90738, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 56.6815 - MinusLogProbMetric: 56.6815 - val_loss: 56.9074 - val_MinusLogProbMetric: 56.9074 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 705/1000
2023-10-31 03:08:45.035 
Epoch 705/1000 
	 loss: 56.7703, MinusLogProbMetric: 56.7703, val_loss: 56.8990, val_MinusLogProbMetric: 56.8990

Epoch 705: val_loss improved from 56.90738 to 56.89896, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 56.7703 - MinusLogProbMetric: 56.7703 - val_loss: 56.8990 - val_MinusLogProbMetric: 56.8990 - lr: 2.0576e-06 - 60s/epoch - 309ms/step
Epoch 706/1000
2023-10-31 03:09:43.110 
Epoch 706/1000 
	 loss: 56.6569, MinusLogProbMetric: 56.6569, val_loss: 56.9000, val_MinusLogProbMetric: 56.9000

Epoch 706: val_loss did not improve from 56.89896
196/196 - 57s - loss: 56.6569 - MinusLogProbMetric: 56.6569 - val_loss: 56.9000 - val_MinusLogProbMetric: 56.9000 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 707/1000
2023-10-31 03:10:42.675 
Epoch 707/1000 
	 loss: 56.6435, MinusLogProbMetric: 56.6435, val_loss: 56.8973, val_MinusLogProbMetric: 56.8973

Epoch 707: val_loss improved from 56.89896 to 56.89727, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 56.6435 - MinusLogProbMetric: 56.6435 - val_loss: 56.8973 - val_MinusLogProbMetric: 56.8973 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 708/1000
2023-10-31 03:11:38.470 
Epoch 708/1000 
	 loss: 56.9034, MinusLogProbMetric: 56.9034, val_loss: 56.8554, val_MinusLogProbMetric: 56.8554

Epoch 708: val_loss improved from 56.89727 to 56.85545, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 56.9034 - MinusLogProbMetric: 56.9034 - val_loss: 56.8554 - val_MinusLogProbMetric: 56.8554 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 709/1000
2023-10-31 03:12:33.406 
Epoch 709/1000 
	 loss: 56.6418, MinusLogProbMetric: 56.6418, val_loss: 56.8569, val_MinusLogProbMetric: 56.8569

Epoch 709: val_loss did not improve from 56.85545
196/196 - 54s - loss: 56.6418 - MinusLogProbMetric: 56.6418 - val_loss: 56.8569 - val_MinusLogProbMetric: 56.8569 - lr: 2.0576e-06 - 54s/epoch - 276ms/step
Epoch 710/1000
2023-10-31 03:13:28.894 
Epoch 710/1000 
	 loss: 56.5968, MinusLogProbMetric: 56.5968, val_loss: 56.8161, val_MinusLogProbMetric: 56.8161

Epoch 710: val_loss improved from 56.85545 to 56.81611, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 56.5968 - MinusLogProbMetric: 56.5968 - val_loss: 56.8161 - val_MinusLogProbMetric: 56.8161 - lr: 2.0576e-06 - 56s/epoch - 288ms/step
Epoch 711/1000
2023-10-31 03:14:25.240 
Epoch 711/1000 
	 loss: 56.5937, MinusLogProbMetric: 56.5937, val_loss: 56.8600, val_MinusLogProbMetric: 56.8600

Epoch 711: val_loss did not improve from 56.81611
196/196 - 55s - loss: 56.5937 - MinusLogProbMetric: 56.5937 - val_loss: 56.8600 - val_MinusLogProbMetric: 56.8600 - lr: 2.0576e-06 - 55s/epoch - 283ms/step
Epoch 712/1000
2023-10-31 03:15:24.156 
Epoch 712/1000 
	 loss: 56.5645, MinusLogProbMetric: 56.5645, val_loss: 56.7793, val_MinusLogProbMetric: 56.7793

Epoch 712: val_loss improved from 56.81611 to 56.77933, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 56.5645 - MinusLogProbMetric: 56.5645 - val_loss: 56.7793 - val_MinusLogProbMetric: 56.7793 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 713/1000
2023-10-31 03:16:21.686 
Epoch 713/1000 
	 loss: 56.5463, MinusLogProbMetric: 56.5463, val_loss: 56.7714, val_MinusLogProbMetric: 56.7714

Epoch 713: val_loss improved from 56.77933 to 56.77144, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 56.5463 - MinusLogProbMetric: 56.5463 - val_loss: 56.7714 - val_MinusLogProbMetric: 56.7714 - lr: 2.0576e-06 - 58s/epoch - 294ms/step
Epoch 714/1000
2023-10-31 03:17:18.216 
Epoch 714/1000 
	 loss: 56.5407, MinusLogProbMetric: 56.5407, val_loss: 56.7719, val_MinusLogProbMetric: 56.7719

Epoch 714: val_loss did not improve from 56.77144
196/196 - 56s - loss: 56.5407 - MinusLogProbMetric: 56.5407 - val_loss: 56.7719 - val_MinusLogProbMetric: 56.7719 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 715/1000
2023-10-31 03:18:14.872 
Epoch 715/1000 
	 loss: 56.5598, MinusLogProbMetric: 56.5598, val_loss: 56.7717, val_MinusLogProbMetric: 56.7717

Epoch 715: val_loss did not improve from 56.77144
196/196 - 57s - loss: 56.5598 - MinusLogProbMetric: 56.5598 - val_loss: 56.7717 - val_MinusLogProbMetric: 56.7717 - lr: 2.0576e-06 - 57s/epoch - 289ms/step
Epoch 716/1000
2023-10-31 03:19:09.733 
Epoch 716/1000 
	 loss: 56.4920, MinusLogProbMetric: 56.4920, val_loss: 56.7236, val_MinusLogProbMetric: 56.7236

Epoch 716: val_loss improved from 56.77144 to 56.72358, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 56.4920 - MinusLogProbMetric: 56.4920 - val_loss: 56.7236 - val_MinusLogProbMetric: 56.7236 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 717/1000
2023-10-31 03:20:05.463 
Epoch 717/1000 
	 loss: 56.4729, MinusLogProbMetric: 56.4729, val_loss: 56.6912, val_MinusLogProbMetric: 56.6912

Epoch 717: val_loss improved from 56.72358 to 56.69123, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 56.4729 - MinusLogProbMetric: 56.4729 - val_loss: 56.6912 - val_MinusLogProbMetric: 56.6912 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 718/1000
2023-10-31 03:21:07.528 
Epoch 718/1000 
	 loss: 56.4681, MinusLogProbMetric: 56.4681, val_loss: 56.6688, val_MinusLogProbMetric: 56.6688

Epoch 718: val_loss improved from 56.69123 to 56.66883, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 56.4681 - MinusLogProbMetric: 56.4681 - val_loss: 56.6688 - val_MinusLogProbMetric: 56.6688 - lr: 2.0576e-06 - 62s/epoch - 317ms/step
Epoch 719/1000
2023-10-31 03:22:11.102 
Epoch 719/1000 
	 loss: 56.4532, MinusLogProbMetric: 56.4532, val_loss: 56.6757, val_MinusLogProbMetric: 56.6757

Epoch 719: val_loss did not improve from 56.66883
196/196 - 63s - loss: 56.4532 - MinusLogProbMetric: 56.4532 - val_loss: 56.6757 - val_MinusLogProbMetric: 56.6757 - lr: 2.0576e-06 - 63s/epoch - 320ms/step
Epoch 720/1000
2023-10-31 03:23:10.383 
Epoch 720/1000 
	 loss: 56.4373, MinusLogProbMetric: 56.4373, val_loss: 56.6605, val_MinusLogProbMetric: 56.6605

Epoch 720: val_loss improved from 56.66883 to 56.66048, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 56.4373 - MinusLogProbMetric: 56.4373 - val_loss: 56.6605 - val_MinusLogProbMetric: 56.6605 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 721/1000
2023-10-31 03:24:08.343 
Epoch 721/1000 
	 loss: 56.4150, MinusLogProbMetric: 56.4150, val_loss: 56.6854, val_MinusLogProbMetric: 56.6854

Epoch 721: val_loss did not improve from 56.66048
196/196 - 57s - loss: 56.4150 - MinusLogProbMetric: 56.4150 - val_loss: 56.6854 - val_MinusLogProbMetric: 56.6854 - lr: 2.0576e-06 - 57s/epoch - 291ms/step
Epoch 722/1000
2023-10-31 03:25:09.003 
Epoch 722/1000 
	 loss: 56.4100, MinusLogProbMetric: 56.4100, val_loss: 56.6387, val_MinusLogProbMetric: 56.6387

Epoch 722: val_loss improved from 56.66048 to 56.63865, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 56.4100 - MinusLogProbMetric: 56.4100 - val_loss: 56.6387 - val_MinusLogProbMetric: 56.6387 - lr: 2.0576e-06 - 62s/epoch - 315ms/step
Epoch 723/1000
2023-10-31 03:26:10.444 
Epoch 723/1000 
	 loss: 56.4135, MinusLogProbMetric: 56.4135, val_loss: 56.5850, val_MinusLogProbMetric: 56.5850

Epoch 723: val_loss improved from 56.63865 to 56.58498, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 56.4135 - MinusLogProbMetric: 56.4135 - val_loss: 56.5850 - val_MinusLogProbMetric: 56.5850 - lr: 2.0576e-06 - 61s/epoch - 312ms/step
Epoch 724/1000
2023-10-31 03:27:15.174 
Epoch 724/1000 
	 loss: 56.3624, MinusLogProbMetric: 56.3624, val_loss: 56.5944, val_MinusLogProbMetric: 56.5944

Epoch 724: val_loss did not improve from 56.58498
196/196 - 64s - loss: 56.3624 - MinusLogProbMetric: 56.3624 - val_loss: 56.5944 - val_MinusLogProbMetric: 56.5944 - lr: 2.0576e-06 - 64s/epoch - 326ms/step
Epoch 725/1000
2023-10-31 03:28:14.791 
Epoch 725/1000 
	 loss: 56.3642, MinusLogProbMetric: 56.3642, val_loss: 56.5607, val_MinusLogProbMetric: 56.5607

Epoch 725: val_loss improved from 56.58498 to 56.56075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 56.3642 - MinusLogProbMetric: 56.3642 - val_loss: 56.5607 - val_MinusLogProbMetric: 56.5607 - lr: 2.0576e-06 - 60s/epoch - 309ms/step
Epoch 726/1000
2023-10-31 03:29:14.537 
Epoch 726/1000 
	 loss: 56.3360, MinusLogProbMetric: 56.3360, val_loss: 56.5676, val_MinusLogProbMetric: 56.5676

Epoch 726: val_loss did not improve from 56.56075
196/196 - 59s - loss: 56.3360 - MinusLogProbMetric: 56.3360 - val_loss: 56.5676 - val_MinusLogProbMetric: 56.5676 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 727/1000
2023-10-31 03:30:15.797 
Epoch 727/1000 
	 loss: 56.3269, MinusLogProbMetric: 56.3269, val_loss: 56.5485, val_MinusLogProbMetric: 56.5485

Epoch 727: val_loss improved from 56.56075 to 56.54845, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 56.3269 - MinusLogProbMetric: 56.3269 - val_loss: 56.5485 - val_MinusLogProbMetric: 56.5485 - lr: 2.0576e-06 - 62s/epoch - 317ms/step
Epoch 728/1000
2023-10-31 03:31:11.258 
Epoch 728/1000 
	 loss: 56.3095, MinusLogProbMetric: 56.3095, val_loss: 56.5332, val_MinusLogProbMetric: 56.5332

Epoch 728: val_loss improved from 56.54845 to 56.53324, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 56.3095 - MinusLogProbMetric: 56.3095 - val_loss: 56.5332 - val_MinusLogProbMetric: 56.5332 - lr: 2.0576e-06 - 56s/epoch - 283ms/step
Epoch 729/1000
2023-10-31 03:32:13.043 
Epoch 729/1000 
	 loss: 56.2830, MinusLogProbMetric: 56.2830, val_loss: 56.5703, val_MinusLogProbMetric: 56.5703

Epoch 729: val_loss did not improve from 56.53324
196/196 - 61s - loss: 56.2830 - MinusLogProbMetric: 56.2830 - val_loss: 56.5703 - val_MinusLogProbMetric: 56.5703 - lr: 2.0576e-06 - 61s/epoch - 311ms/step
Epoch 730/1000
2023-10-31 03:33:11.017 
Epoch 730/1000 
	 loss: 56.2717, MinusLogProbMetric: 56.2717, val_loss: 56.4945, val_MinusLogProbMetric: 56.4945

Epoch 730: val_loss improved from 56.53324 to 56.49447, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 56.2717 - MinusLogProbMetric: 56.2717 - val_loss: 56.4945 - val_MinusLogProbMetric: 56.4945 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 731/1000
2023-10-31 03:34:07.302 
Epoch 731/1000 
	 loss: 56.2663, MinusLogProbMetric: 56.2663, val_loss: 56.4899, val_MinusLogProbMetric: 56.4899

Epoch 731: val_loss improved from 56.49447 to 56.48986, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 56.2663 - MinusLogProbMetric: 56.2663 - val_loss: 56.4899 - val_MinusLogProbMetric: 56.4899 - lr: 2.0576e-06 - 56s/epoch - 287ms/step
Epoch 732/1000
2023-10-31 03:35:07.109 
Epoch 732/1000 
	 loss: 56.2457, MinusLogProbMetric: 56.2457, val_loss: 56.4867, val_MinusLogProbMetric: 56.4867

Epoch 732: val_loss improved from 56.48986 to 56.48670, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 56.2457 - MinusLogProbMetric: 56.2457 - val_loss: 56.4867 - val_MinusLogProbMetric: 56.4867 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 733/1000
2023-10-31 03:36:05.645 
Epoch 733/1000 
	 loss: 56.2248, MinusLogProbMetric: 56.2248, val_loss: 56.4541, val_MinusLogProbMetric: 56.4541

Epoch 733: val_loss improved from 56.48670 to 56.45407, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 56.2248 - MinusLogProbMetric: 56.2248 - val_loss: 56.4541 - val_MinusLogProbMetric: 56.4541 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 734/1000
2023-10-31 03:37:06.566 
Epoch 734/1000 
	 loss: 56.2159, MinusLogProbMetric: 56.2159, val_loss: 56.4490, val_MinusLogProbMetric: 56.4490

Epoch 734: val_loss improved from 56.45407 to 56.44897, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 56.2159 - MinusLogProbMetric: 56.2159 - val_loss: 56.4490 - val_MinusLogProbMetric: 56.4490 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 735/1000
2023-10-31 03:38:01.583 
Epoch 735/1000 
	 loss: 56.2645, MinusLogProbMetric: 56.2645, val_loss: 56.4239, val_MinusLogProbMetric: 56.4239

Epoch 735: val_loss improved from 56.44897 to 56.42389, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 56.2645 - MinusLogProbMetric: 56.2645 - val_loss: 56.4239 - val_MinusLogProbMetric: 56.4239 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 736/1000
2023-10-31 03:39:00.463 
Epoch 736/1000 
	 loss: 56.1958, MinusLogProbMetric: 56.1958, val_loss: 56.4172, val_MinusLogProbMetric: 56.4172

Epoch 736: val_loss improved from 56.42389 to 56.41719, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 56.1958 - MinusLogProbMetric: 56.1958 - val_loss: 56.4172 - val_MinusLogProbMetric: 56.4172 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 737/1000
2023-10-31 03:40:01.212 
Epoch 737/1000 
	 loss: 56.1690, MinusLogProbMetric: 56.1690, val_loss: 56.3894, val_MinusLogProbMetric: 56.3894

Epoch 737: val_loss improved from 56.41719 to 56.38941, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 56.1690 - MinusLogProbMetric: 56.1690 - val_loss: 56.3894 - val_MinusLogProbMetric: 56.3894 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 738/1000
2023-10-31 03:41:01.448 
Epoch 738/1000 
	 loss: 56.1463, MinusLogProbMetric: 56.1463, val_loss: 56.3666, val_MinusLogProbMetric: 56.3666

Epoch 738: val_loss improved from 56.38941 to 56.36661, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 56.1463 - MinusLogProbMetric: 56.1463 - val_loss: 56.3666 - val_MinusLogProbMetric: 56.3666 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 739/1000
2023-10-31 03:41:59.492 
Epoch 739/1000 
	 loss: 56.1433, MinusLogProbMetric: 56.1433, val_loss: 56.3481, val_MinusLogProbMetric: 56.3481

Epoch 739: val_loss improved from 56.36661 to 56.34805, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 56.1433 - MinusLogProbMetric: 56.1433 - val_loss: 56.3481 - val_MinusLogProbMetric: 56.3481 - lr: 2.0576e-06 - 58s/epoch - 296ms/step
Epoch 740/1000
2023-10-31 03:42:59.170 
Epoch 740/1000 
	 loss: 56.1213, MinusLogProbMetric: 56.1213, val_loss: 56.3691, val_MinusLogProbMetric: 56.3691

Epoch 740: val_loss did not improve from 56.34805
196/196 - 59s - loss: 56.1213 - MinusLogProbMetric: 56.1213 - val_loss: 56.3691 - val_MinusLogProbMetric: 56.3691 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 741/1000
2023-10-31 03:43:56.572 
Epoch 741/1000 
	 loss: 56.1513, MinusLogProbMetric: 56.1513, val_loss: 56.3460, val_MinusLogProbMetric: 56.3460

Epoch 741: val_loss improved from 56.34805 to 56.34603, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 56.1513 - MinusLogProbMetric: 56.1513 - val_loss: 56.3460 - val_MinusLogProbMetric: 56.3460 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 742/1000
2023-10-31 03:44:57.358 
Epoch 742/1000 
	 loss: 56.0915, MinusLogProbMetric: 56.0915, val_loss: 56.3038, val_MinusLogProbMetric: 56.3038

Epoch 742: val_loss improved from 56.34603 to 56.30383, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 56.0915 - MinusLogProbMetric: 56.0915 - val_loss: 56.3038 - val_MinusLogProbMetric: 56.3038 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 743/1000
2023-10-31 03:45:52.615 
Epoch 743/1000 
	 loss: 56.0695, MinusLogProbMetric: 56.0695, val_loss: 56.3193, val_MinusLogProbMetric: 56.3193

Epoch 743: val_loss did not improve from 56.30383
196/196 - 54s - loss: 56.0695 - MinusLogProbMetric: 56.0695 - val_loss: 56.3193 - val_MinusLogProbMetric: 56.3193 - lr: 2.0576e-06 - 54s/epoch - 277ms/step
Epoch 744/1000
2023-10-31 03:46:47.436 
Epoch 744/1000 
	 loss: 56.0838, MinusLogProbMetric: 56.0838, val_loss: 56.3182, val_MinusLogProbMetric: 56.3182

Epoch 744: val_loss did not improve from 56.30383
196/196 - 55s - loss: 56.0838 - MinusLogProbMetric: 56.0838 - val_loss: 56.3182 - val_MinusLogProbMetric: 56.3182 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 745/1000
2023-10-31 03:47:41.056 
Epoch 745/1000 
	 loss: 56.0495, MinusLogProbMetric: 56.0495, val_loss: 56.3033, val_MinusLogProbMetric: 56.3033

Epoch 745: val_loss improved from 56.30383 to 56.30330, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 56.0495 - MinusLogProbMetric: 56.0495 - val_loss: 56.3033 - val_MinusLogProbMetric: 56.3033 - lr: 2.0576e-06 - 54s/epoch - 278ms/step
Epoch 746/1000
2023-10-31 03:48:34.462 
Epoch 746/1000 
	 loss: 56.0243, MinusLogProbMetric: 56.0243, val_loss: 56.3813, val_MinusLogProbMetric: 56.3813

Epoch 746: val_loss did not improve from 56.30330
196/196 - 53s - loss: 56.0243 - MinusLogProbMetric: 56.0243 - val_loss: 56.3813 - val_MinusLogProbMetric: 56.3813 - lr: 2.0576e-06 - 53s/epoch - 268ms/step
Epoch 747/1000
2023-10-31 03:49:32.615 
Epoch 747/1000 
	 loss: 56.0283, MinusLogProbMetric: 56.0283, val_loss: 56.2730, val_MinusLogProbMetric: 56.2730

Epoch 747: val_loss improved from 56.30330 to 56.27304, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 56.0283 - MinusLogProbMetric: 56.0283 - val_loss: 56.2730 - val_MinusLogProbMetric: 56.2730 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 748/1000
2023-10-31 03:50:32.355 
Epoch 748/1000 
	 loss: 56.0020, MinusLogProbMetric: 56.0020, val_loss: 56.2303, val_MinusLogProbMetric: 56.2303

Epoch 748: val_loss improved from 56.27304 to 56.23030, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 56.0020 - MinusLogProbMetric: 56.0020 - val_loss: 56.2303 - val_MinusLogProbMetric: 56.2303 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 749/1000
2023-10-31 03:51:33.222 
Epoch 749/1000 
	 loss: 55.9877, MinusLogProbMetric: 55.9877, val_loss: 56.2256, val_MinusLogProbMetric: 56.2256

Epoch 749: val_loss improved from 56.23030 to 56.22561, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 55.9877 - MinusLogProbMetric: 55.9877 - val_loss: 56.2256 - val_MinusLogProbMetric: 56.2256 - lr: 2.0576e-06 - 61s/epoch - 311ms/step
Epoch 750/1000
2023-10-31 03:52:33.315 
Epoch 750/1000 
	 loss: 55.9990, MinusLogProbMetric: 55.9990, val_loss: 56.1985, val_MinusLogProbMetric: 56.1985

Epoch 750: val_loss improved from 56.22561 to 56.19853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 55.9990 - MinusLogProbMetric: 55.9990 - val_loss: 56.1985 - val_MinusLogProbMetric: 56.1985 - lr: 2.0576e-06 - 60s/epoch - 306ms/step
Epoch 751/1000
2023-10-31 03:53:35.675 
Epoch 751/1000 
	 loss: 56.0787, MinusLogProbMetric: 56.0787, val_loss: 56.1901, val_MinusLogProbMetric: 56.1901

Epoch 751: val_loss improved from 56.19853 to 56.19013, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 56.0787 - MinusLogProbMetric: 56.0787 - val_loss: 56.1901 - val_MinusLogProbMetric: 56.1901 - lr: 2.0576e-06 - 63s/epoch - 319ms/step
Epoch 752/1000
2023-10-31 03:54:36.149 
Epoch 752/1000 
	 loss: 55.9497, MinusLogProbMetric: 55.9497, val_loss: 56.1642, val_MinusLogProbMetric: 56.1642

Epoch 752: val_loss improved from 56.19013 to 56.16422, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 55.9497 - MinusLogProbMetric: 55.9497 - val_loss: 56.1642 - val_MinusLogProbMetric: 56.1642 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 753/1000
2023-10-31 03:55:35.055 
Epoch 753/1000 
	 loss: 55.9451, MinusLogProbMetric: 55.9451, val_loss: 56.1451, val_MinusLogProbMetric: 56.1451

Epoch 753: val_loss improved from 56.16422 to 56.14510, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 55.9451 - MinusLogProbMetric: 55.9451 - val_loss: 56.1451 - val_MinusLogProbMetric: 56.1451 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 754/1000
2023-10-31 03:56:30.015 
Epoch 754/1000 
	 loss: 55.9346, MinusLogProbMetric: 55.9346, val_loss: 56.1480, val_MinusLogProbMetric: 56.1480

Epoch 754: val_loss did not improve from 56.14510
196/196 - 54s - loss: 55.9346 - MinusLogProbMetric: 55.9346 - val_loss: 56.1480 - val_MinusLogProbMetric: 56.1480 - lr: 2.0576e-06 - 54s/epoch - 276ms/step
Epoch 755/1000
2023-10-31 03:57:28.129 
Epoch 755/1000 
	 loss: 55.9297, MinusLogProbMetric: 55.9297, val_loss: 56.1085, val_MinusLogProbMetric: 56.1085

Epoch 755: val_loss improved from 56.14510 to 56.10850, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 55.9297 - MinusLogProbMetric: 55.9297 - val_loss: 56.1085 - val_MinusLogProbMetric: 56.1085 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 756/1000
2023-10-31 03:58:29.113 
Epoch 756/1000 
	 loss: 55.9879, MinusLogProbMetric: 55.9879, val_loss: 56.1500, val_MinusLogProbMetric: 56.1500

Epoch 756: val_loss did not improve from 56.10850
196/196 - 60s - loss: 55.9879 - MinusLogProbMetric: 55.9879 - val_loss: 56.1500 - val_MinusLogProbMetric: 56.1500 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 757/1000
2023-10-31 03:59:27.699 
Epoch 757/1000 
	 loss: 55.8871, MinusLogProbMetric: 55.8871, val_loss: 56.0838, val_MinusLogProbMetric: 56.0838

Epoch 757: val_loss improved from 56.10850 to 56.08375, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 55.8871 - MinusLogProbMetric: 55.8871 - val_loss: 56.0838 - val_MinusLogProbMetric: 56.0838 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 758/1000
2023-10-31 04:00:27.853 
Epoch 758/1000 
	 loss: 55.8611, MinusLogProbMetric: 55.8611, val_loss: 56.0603, val_MinusLogProbMetric: 56.0603

Epoch 758: val_loss improved from 56.08375 to 56.06034, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 55.8611 - MinusLogProbMetric: 55.8611 - val_loss: 56.0603 - val_MinusLogProbMetric: 56.0603 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 759/1000
2023-10-31 04:01:30.308 
Epoch 759/1000 
	 loss: 55.8653, MinusLogProbMetric: 55.8653, val_loss: 56.0505, val_MinusLogProbMetric: 56.0505

Epoch 759: val_loss improved from 56.06034 to 56.05053, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 55.8653 - MinusLogProbMetric: 55.8653 - val_loss: 56.0505 - val_MinusLogProbMetric: 56.0505 - lr: 2.0576e-06 - 62s/epoch - 319ms/step
Epoch 760/1000
2023-10-31 04:02:29.565 
Epoch 760/1000 
	 loss: 55.8254, MinusLogProbMetric: 55.8254, val_loss: 56.0221, val_MinusLogProbMetric: 56.0221

Epoch 760: val_loss improved from 56.05053 to 56.02211, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 55.8254 - MinusLogProbMetric: 55.8254 - val_loss: 56.0221 - val_MinusLogProbMetric: 56.0221 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 761/1000
2023-10-31 04:03:26.541 
Epoch 761/1000 
	 loss: 55.8224, MinusLogProbMetric: 55.8224, val_loss: 56.0317, val_MinusLogProbMetric: 56.0317

Epoch 761: val_loss did not improve from 56.02211
196/196 - 56s - loss: 55.8224 - MinusLogProbMetric: 55.8224 - val_loss: 56.0317 - val_MinusLogProbMetric: 56.0317 - lr: 2.0576e-06 - 56s/epoch - 286ms/step
Epoch 762/1000
2023-10-31 04:04:27.462 
Epoch 762/1000 
	 loss: 55.8055, MinusLogProbMetric: 55.8055, val_loss: 55.9852, val_MinusLogProbMetric: 55.9852

Epoch 762: val_loss improved from 56.02211 to 55.98517, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 55.8055 - MinusLogProbMetric: 55.8055 - val_loss: 55.9852 - val_MinusLogProbMetric: 55.9852 - lr: 2.0576e-06 - 62s/epoch - 315ms/step
Epoch 763/1000
2023-10-31 04:05:27.426 
Epoch 763/1000 
	 loss: 55.7880, MinusLogProbMetric: 55.7880, val_loss: 55.9701, val_MinusLogProbMetric: 55.9701

Epoch 763: val_loss improved from 55.98517 to 55.97011, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 55.7880 - MinusLogProbMetric: 55.7880 - val_loss: 55.9701 - val_MinusLogProbMetric: 55.9701 - lr: 2.0576e-06 - 60s/epoch - 306ms/step
Epoch 764/1000
2023-10-31 04:06:28.163 
Epoch 764/1000 
	 loss: 55.7761, MinusLogProbMetric: 55.7761, val_loss: 55.9706, val_MinusLogProbMetric: 55.9706

Epoch 764: val_loss did not improve from 55.97011
196/196 - 60s - loss: 55.7761 - MinusLogProbMetric: 55.7761 - val_loss: 55.9706 - val_MinusLogProbMetric: 55.9706 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 765/1000
2023-10-31 04:07:25.987 
Epoch 765/1000 
	 loss: 55.7503, MinusLogProbMetric: 55.7503, val_loss: 56.0025, val_MinusLogProbMetric: 56.0025

Epoch 765: val_loss did not improve from 55.97011
196/196 - 58s - loss: 55.7503 - MinusLogProbMetric: 55.7503 - val_loss: 56.0025 - val_MinusLogProbMetric: 56.0025 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 766/1000
2023-10-31 04:08:24.541 
Epoch 766/1000 
	 loss: 55.7860, MinusLogProbMetric: 55.7860, val_loss: 56.9801, val_MinusLogProbMetric: 56.9801

Epoch 766: val_loss did not improve from 55.97011
196/196 - 59s - loss: 55.7860 - MinusLogProbMetric: 55.7860 - val_loss: 56.9801 - val_MinusLogProbMetric: 56.9801 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 767/1000
2023-10-31 04:09:21.160 
Epoch 767/1000 
	 loss: 55.7735, MinusLogProbMetric: 55.7735, val_loss: 55.9606, val_MinusLogProbMetric: 55.9606

Epoch 767: val_loss improved from 55.97011 to 55.96059, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 55.7735 - MinusLogProbMetric: 55.7735 - val_loss: 55.9606 - val_MinusLogProbMetric: 55.9606 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 768/1000
2023-10-31 04:10:18.105 
Epoch 768/1000 
	 loss: 55.7060, MinusLogProbMetric: 55.7060, val_loss: 55.9371, val_MinusLogProbMetric: 55.9371

Epoch 768: val_loss improved from 55.96059 to 55.93711, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 55.7060 - MinusLogProbMetric: 55.7060 - val_loss: 55.9371 - val_MinusLogProbMetric: 55.9371 - lr: 2.0576e-06 - 57s/epoch - 289ms/step
Epoch 769/1000
2023-10-31 04:11:12.602 
Epoch 769/1000 
	 loss: 55.6878, MinusLogProbMetric: 55.6878, val_loss: 55.9251, val_MinusLogProbMetric: 55.9251

Epoch 769: val_loss improved from 55.93711 to 55.92506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 55.6878 - MinusLogProbMetric: 55.6878 - val_loss: 55.9251 - val_MinusLogProbMetric: 55.9251 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 770/1000
2023-10-31 04:12:14.104 
Epoch 770/1000 
	 loss: 55.7261, MinusLogProbMetric: 55.7261, val_loss: 55.9250, val_MinusLogProbMetric: 55.9250

Epoch 770: val_loss improved from 55.92506 to 55.92498, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 55.7261 - MinusLogProbMetric: 55.7261 - val_loss: 55.9250 - val_MinusLogProbMetric: 55.9250 - lr: 2.0576e-06 - 61s/epoch - 312ms/step
Epoch 771/1000
2023-10-31 04:13:15.755 
Epoch 771/1000 
	 loss: 55.6602, MinusLogProbMetric: 55.6602, val_loss: 55.8835, val_MinusLogProbMetric: 55.8835

Epoch 771: val_loss improved from 55.92498 to 55.88353, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 55.6602 - MinusLogProbMetric: 55.6602 - val_loss: 55.8835 - val_MinusLogProbMetric: 55.8835 - lr: 2.0576e-06 - 62s/epoch - 317ms/step
Epoch 772/1000
2023-10-31 04:14:17.900 
Epoch 772/1000 
	 loss: 55.6446, MinusLogProbMetric: 55.6446, val_loss: 55.8843, val_MinusLogProbMetric: 55.8843

Epoch 772: val_loss did not improve from 55.88353
196/196 - 61s - loss: 55.6446 - MinusLogProbMetric: 55.6446 - val_loss: 55.8843 - val_MinusLogProbMetric: 55.8843 - lr: 2.0576e-06 - 61s/epoch - 312ms/step
Epoch 773/1000
2023-10-31 04:15:16.489 
Epoch 773/1000 
	 loss: 55.6273, MinusLogProbMetric: 55.6273, val_loss: 55.8580, val_MinusLogProbMetric: 55.8580

Epoch 773: val_loss improved from 55.88353 to 55.85803, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 55.6273 - MinusLogProbMetric: 55.6273 - val_loss: 55.8580 - val_MinusLogProbMetric: 55.8580 - lr: 2.0576e-06 - 60s/epoch - 304ms/step
Epoch 774/1000
2023-10-31 04:16:11.383 
Epoch 774/1000 
	 loss: 55.6752, MinusLogProbMetric: 55.6752, val_loss: 55.8662, val_MinusLogProbMetric: 55.8662

Epoch 774: val_loss did not improve from 55.85803
196/196 - 54s - loss: 55.6752 - MinusLogProbMetric: 55.6752 - val_loss: 55.8662 - val_MinusLogProbMetric: 55.8662 - lr: 2.0576e-06 - 54s/epoch - 275ms/step
Epoch 775/1000
2023-10-31 04:17:06.568 
Epoch 775/1000 
	 loss: 55.6012, MinusLogProbMetric: 55.6012, val_loss: 55.8373, val_MinusLogProbMetric: 55.8373

Epoch 775: val_loss improved from 55.85803 to 55.83735, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 55.6012 - MinusLogProbMetric: 55.6012 - val_loss: 55.8373 - val_MinusLogProbMetric: 55.8373 - lr: 2.0576e-06 - 56s/epoch - 286ms/step
Epoch 776/1000
2023-10-31 04:18:04.399 
Epoch 776/1000 
	 loss: 55.5968, MinusLogProbMetric: 55.5968, val_loss: 55.8176, val_MinusLogProbMetric: 55.8176

Epoch 776: val_loss improved from 55.83735 to 55.81758, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 55.5968 - MinusLogProbMetric: 55.5968 - val_loss: 55.8176 - val_MinusLogProbMetric: 55.8176 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 777/1000
2023-10-31 04:19:00.535 
Epoch 777/1000 
	 loss: 55.5919, MinusLogProbMetric: 55.5919, val_loss: 55.8083, val_MinusLogProbMetric: 55.8083

Epoch 777: val_loss improved from 55.81758 to 55.80833, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 55.5919 - MinusLogProbMetric: 55.5919 - val_loss: 55.8083 - val_MinusLogProbMetric: 55.8083 - lr: 2.0576e-06 - 56s/epoch - 286ms/step
Epoch 778/1000
2023-10-31 04:19:59.198 
Epoch 778/1000 
	 loss: 55.5622, MinusLogProbMetric: 55.5622, val_loss: 55.8554, val_MinusLogProbMetric: 55.8554

Epoch 778: val_loss did not improve from 55.80833
196/196 - 58s - loss: 55.5622 - MinusLogProbMetric: 55.5622 - val_loss: 55.8554 - val_MinusLogProbMetric: 55.8554 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 779/1000
2023-10-31 04:20:59.761 
Epoch 779/1000 
	 loss: 55.5418, MinusLogProbMetric: 55.5418, val_loss: 55.7853, val_MinusLogProbMetric: 55.7853

Epoch 779: val_loss improved from 55.80833 to 55.78532, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 55.5418 - MinusLogProbMetric: 55.5418 - val_loss: 55.7853 - val_MinusLogProbMetric: 55.7853 - lr: 2.0576e-06 - 61s/epoch - 314ms/step
Epoch 780/1000
2023-10-31 04:21:57.549 
Epoch 780/1000 
	 loss: 55.5344, MinusLogProbMetric: 55.5344, val_loss: 55.7694, val_MinusLogProbMetric: 55.7694

Epoch 780: val_loss improved from 55.78532 to 55.76944, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 55.5344 - MinusLogProbMetric: 55.5344 - val_loss: 55.7694 - val_MinusLogProbMetric: 55.7694 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 781/1000
2023-10-31 04:22:53.608 
Epoch 781/1000 
	 loss: 56.0979, MinusLogProbMetric: 56.0979, val_loss: 55.7876, val_MinusLogProbMetric: 55.7876

Epoch 781: val_loss did not improve from 55.76944
196/196 - 55s - loss: 56.0979 - MinusLogProbMetric: 56.0979 - val_loss: 55.7876 - val_MinusLogProbMetric: 55.7876 - lr: 2.0576e-06 - 55s/epoch - 282ms/step
Epoch 782/1000
2023-10-31 04:23:48.853 
Epoch 782/1000 
	 loss: 55.5252, MinusLogProbMetric: 55.5252, val_loss: 55.7442, val_MinusLogProbMetric: 55.7442

Epoch 782: val_loss improved from 55.76944 to 55.74418, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 55.5252 - MinusLogProbMetric: 55.5252 - val_loss: 55.7442 - val_MinusLogProbMetric: 55.7442 - lr: 2.0576e-06 - 56s/epoch - 286ms/step
Epoch 783/1000
2023-10-31 04:24:44.377 
Epoch 783/1000 
	 loss: 55.4964, MinusLogProbMetric: 55.4964, val_loss: 55.7380, val_MinusLogProbMetric: 55.7380

Epoch 783: val_loss improved from 55.74418 to 55.73803, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 55.4964 - MinusLogProbMetric: 55.4964 - val_loss: 55.7380 - val_MinusLogProbMetric: 55.7380 - lr: 2.0576e-06 - 56s/epoch - 283ms/step
Epoch 784/1000
2023-10-31 04:25:41.372 
Epoch 784/1000 
	 loss: 55.4731, MinusLogProbMetric: 55.4731, val_loss: 55.7107, val_MinusLogProbMetric: 55.7107

Epoch 784: val_loss improved from 55.73803 to 55.71072, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 55.4731 - MinusLogProbMetric: 55.4731 - val_loss: 55.7107 - val_MinusLogProbMetric: 55.7107 - lr: 2.0576e-06 - 57s/epoch - 291ms/step
Epoch 785/1000
2023-10-31 04:26:44.034 
Epoch 785/1000 
	 loss: 55.4863, MinusLogProbMetric: 55.4863, val_loss: 55.7081, val_MinusLogProbMetric: 55.7081

Epoch 785: val_loss improved from 55.71072 to 55.70812, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 55.4863 - MinusLogProbMetric: 55.4863 - val_loss: 55.7081 - val_MinusLogProbMetric: 55.7081 - lr: 2.0576e-06 - 63s/epoch - 319ms/step
Epoch 786/1000
2023-10-31 04:27:43.896 
Epoch 786/1000 
	 loss: 55.4473, MinusLogProbMetric: 55.4473, val_loss: 55.6739, val_MinusLogProbMetric: 55.6739

Epoch 786: val_loss improved from 55.70812 to 55.67387, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 55.4473 - MinusLogProbMetric: 55.4473 - val_loss: 55.6739 - val_MinusLogProbMetric: 55.6739 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 787/1000
2023-10-31 04:28:42.159 
Epoch 787/1000 
	 loss: 55.4482, MinusLogProbMetric: 55.4482, val_loss: 55.6739, val_MinusLogProbMetric: 55.6739

Epoch 787: val_loss did not improve from 55.67387
196/196 - 57s - loss: 55.4482 - MinusLogProbMetric: 55.4482 - val_loss: 55.6739 - val_MinusLogProbMetric: 55.6739 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 788/1000
2023-10-31 04:29:40.034 
Epoch 788/1000 
	 loss: 55.4412, MinusLogProbMetric: 55.4412, val_loss: 55.6400, val_MinusLogProbMetric: 55.6400

Epoch 788: val_loss improved from 55.67387 to 55.64002, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 55.4412 - MinusLogProbMetric: 55.4412 - val_loss: 55.6400 - val_MinusLogProbMetric: 55.6400 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 789/1000
2023-10-31 04:30:39.512 
Epoch 789/1000 
	 loss: 55.6717, MinusLogProbMetric: 55.6717, val_loss: 55.6436, val_MinusLogProbMetric: 55.6436

Epoch 789: val_loss did not improve from 55.64002
196/196 - 59s - loss: 55.6717 - MinusLogProbMetric: 55.6717 - val_loss: 55.6436 - val_MinusLogProbMetric: 55.6436 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 790/1000
2023-10-31 04:31:39.590 
Epoch 790/1000 
	 loss: 55.4003, MinusLogProbMetric: 55.4003, val_loss: 55.6235, val_MinusLogProbMetric: 55.6235

Epoch 790: val_loss improved from 55.64002 to 55.62350, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 55.4003 - MinusLogProbMetric: 55.4003 - val_loss: 55.6235 - val_MinusLogProbMetric: 55.6235 - lr: 2.0576e-06 - 61s/epoch - 311ms/step
Epoch 791/1000
2023-10-31 04:32:38.037 
Epoch 791/1000 
	 loss: 55.3681, MinusLogProbMetric: 55.3681, val_loss: 55.6239, val_MinusLogProbMetric: 55.6239

Epoch 791: val_loss did not improve from 55.62350
196/196 - 58s - loss: 55.3681 - MinusLogProbMetric: 55.3681 - val_loss: 55.6239 - val_MinusLogProbMetric: 55.6239 - lr: 2.0576e-06 - 58s/epoch - 294ms/step
Epoch 792/1000
2023-10-31 04:33:37.150 
Epoch 792/1000 
	 loss: 55.3504, MinusLogProbMetric: 55.3504, val_loss: 55.5806, val_MinusLogProbMetric: 55.5806

Epoch 792: val_loss improved from 55.62350 to 55.58057, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 55.3504 - MinusLogProbMetric: 55.3504 - val_loss: 55.5806 - val_MinusLogProbMetric: 55.5806 - lr: 2.0576e-06 - 60s/epoch - 306ms/step
Epoch 793/1000
2023-10-31 04:34:37.396 
Epoch 793/1000 
	 loss: 55.3437, MinusLogProbMetric: 55.3437, val_loss: 55.5678, val_MinusLogProbMetric: 55.5678

Epoch 793: val_loss improved from 55.58057 to 55.56779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 55.3437 - MinusLogProbMetric: 55.3437 - val_loss: 55.5678 - val_MinusLogProbMetric: 55.5678 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 794/1000
2023-10-31 04:35:37.460 
Epoch 794/1000 
	 loss: 55.3371, MinusLogProbMetric: 55.3371, val_loss: 55.5555, val_MinusLogProbMetric: 55.5555

Epoch 794: val_loss improved from 55.56779 to 55.55550, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 55.3371 - MinusLogProbMetric: 55.3371 - val_loss: 55.5555 - val_MinusLogProbMetric: 55.5555 - lr: 2.0576e-06 - 60s/epoch - 306ms/step
Epoch 795/1000
2023-10-31 04:36:41.136 
Epoch 795/1000 
	 loss: 55.3124, MinusLogProbMetric: 55.3124, val_loss: 55.5742, val_MinusLogProbMetric: 55.5742

Epoch 795: val_loss did not improve from 55.55550
196/196 - 63s - loss: 55.3124 - MinusLogProbMetric: 55.3124 - val_loss: 55.5742 - val_MinusLogProbMetric: 55.5742 - lr: 2.0576e-06 - 63s/epoch - 321ms/step
Epoch 796/1000
2023-10-31 04:37:40.171 
Epoch 796/1000 
	 loss: 55.2959, MinusLogProbMetric: 55.2959, val_loss: 55.5364, val_MinusLogProbMetric: 55.5364

Epoch 796: val_loss improved from 55.55550 to 55.53643, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 55.2959 - MinusLogProbMetric: 55.2959 - val_loss: 55.5364 - val_MinusLogProbMetric: 55.5364 - lr: 2.0576e-06 - 60s/epoch - 306ms/step
Epoch 797/1000
2023-10-31 04:38:38.488 
Epoch 797/1000 
	 loss: 55.2827, MinusLogProbMetric: 55.2827, val_loss: 55.5002, val_MinusLogProbMetric: 55.5002

Epoch 797: val_loss improved from 55.53643 to 55.50021, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 55.2827 - MinusLogProbMetric: 55.2827 - val_loss: 55.5002 - val_MinusLogProbMetric: 55.5002 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 798/1000
2023-10-31 04:39:35.964 
Epoch 798/1000 
	 loss: 55.2724, MinusLogProbMetric: 55.2724, val_loss: 55.5340, val_MinusLogProbMetric: 55.5340

Epoch 798: val_loss did not improve from 55.50021
196/196 - 57s - loss: 55.2724 - MinusLogProbMetric: 55.2724 - val_loss: 55.5340 - val_MinusLogProbMetric: 55.5340 - lr: 2.0576e-06 - 57s/epoch - 289ms/step
Epoch 799/1000
2023-10-31 04:40:35.269 
Epoch 799/1000 
	 loss: 55.2750, MinusLogProbMetric: 55.2750, val_loss: 55.7915, val_MinusLogProbMetric: 55.7915

Epoch 799: val_loss did not improve from 55.50021
196/196 - 59s - loss: 55.2750 - MinusLogProbMetric: 55.2750 - val_loss: 55.7915 - val_MinusLogProbMetric: 55.7915 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 800/1000
2023-10-31 04:41:29.466 
Epoch 800/1000 
	 loss: 55.2700, MinusLogProbMetric: 55.2700, val_loss: 55.5525, val_MinusLogProbMetric: 55.5525

Epoch 800: val_loss did not improve from 55.50021
196/196 - 54s - loss: 55.2700 - MinusLogProbMetric: 55.2700 - val_loss: 55.5525 - val_MinusLogProbMetric: 55.5525 - lr: 2.0576e-06 - 54s/epoch - 276ms/step
Epoch 801/1000
2023-10-31 04:42:25.726 
Epoch 801/1000 
	 loss: 55.2413, MinusLogProbMetric: 55.2413, val_loss: 55.4454, val_MinusLogProbMetric: 55.4454

Epoch 801: val_loss improved from 55.50021 to 55.44541, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 55.2413 - MinusLogProbMetric: 55.2413 - val_loss: 55.4454 - val_MinusLogProbMetric: 55.4454 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 802/1000
2023-10-31 04:43:25.305 
Epoch 802/1000 
	 loss: 55.2271, MinusLogProbMetric: 55.2271, val_loss: 55.4582, val_MinusLogProbMetric: 55.4582

Epoch 802: val_loss did not improve from 55.44541
196/196 - 59s - loss: 55.2271 - MinusLogProbMetric: 55.2271 - val_loss: 55.4582 - val_MinusLogProbMetric: 55.4582 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 803/1000
2023-10-31 04:44:21.850 
Epoch 803/1000 
	 loss: 55.2102, MinusLogProbMetric: 55.2102, val_loss: 55.4612, val_MinusLogProbMetric: 55.4612

Epoch 803: val_loss did not improve from 55.44541
196/196 - 57s - loss: 55.2102 - MinusLogProbMetric: 55.2102 - val_loss: 55.4612 - val_MinusLogProbMetric: 55.4612 - lr: 2.0576e-06 - 57s/epoch - 288ms/step
Epoch 804/1000
2023-10-31 04:45:20.879 
Epoch 804/1000 
	 loss: 55.1954, MinusLogProbMetric: 55.1954, val_loss: 55.4433, val_MinusLogProbMetric: 55.4433

Epoch 804: val_loss improved from 55.44541 to 55.44333, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 55.1954 - MinusLogProbMetric: 55.1954 - val_loss: 55.4433 - val_MinusLogProbMetric: 55.4433 - lr: 2.0576e-06 - 60s/epoch - 306ms/step
Epoch 805/1000
2023-10-31 04:46:19.519 
Epoch 805/1000 
	 loss: 55.1786, MinusLogProbMetric: 55.1786, val_loss: 55.4047, val_MinusLogProbMetric: 55.4047

Epoch 805: val_loss improved from 55.44333 to 55.40466, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 55.1786 - MinusLogProbMetric: 55.1786 - val_loss: 55.4047 - val_MinusLogProbMetric: 55.4047 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 806/1000
2023-10-31 04:47:16.226 
Epoch 806/1000 
	 loss: 55.1593, MinusLogProbMetric: 55.1593, val_loss: 55.3847, val_MinusLogProbMetric: 55.3847

Epoch 806: val_loss improved from 55.40466 to 55.38474, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 55.1593 - MinusLogProbMetric: 55.1593 - val_loss: 55.3847 - val_MinusLogProbMetric: 55.3847 - lr: 2.0576e-06 - 57s/epoch - 289ms/step
Epoch 807/1000
2023-10-31 04:48:17.764 
Epoch 807/1000 
	 loss: 55.1451, MinusLogProbMetric: 55.1451, val_loss: 55.3957, val_MinusLogProbMetric: 55.3957

Epoch 807: val_loss did not improve from 55.38474
196/196 - 61s - loss: 55.1451 - MinusLogProbMetric: 55.1451 - val_loss: 55.3957 - val_MinusLogProbMetric: 55.3957 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 808/1000
2023-10-31 04:49:13.311 
Epoch 808/1000 
	 loss: 55.1290, MinusLogProbMetric: 55.1290, val_loss: 55.3835, val_MinusLogProbMetric: 55.3835

Epoch 808: val_loss improved from 55.38474 to 55.38351, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 55.1290 - MinusLogProbMetric: 55.1290 - val_loss: 55.3835 - val_MinusLogProbMetric: 55.3835 - lr: 2.0576e-06 - 56s/epoch - 288ms/step
Epoch 809/1000
2023-10-31 04:50:11.851 
Epoch 809/1000 
	 loss: 55.1248, MinusLogProbMetric: 55.1248, val_loss: 55.3737, val_MinusLogProbMetric: 55.3737

Epoch 809: val_loss improved from 55.38351 to 55.37372, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 55.1248 - MinusLogProbMetric: 55.1248 - val_loss: 55.3737 - val_MinusLogProbMetric: 55.3737 - lr: 2.0576e-06 - 59s/epoch - 298ms/step
Epoch 810/1000
2023-10-31 04:51:12.118 
Epoch 810/1000 
	 loss: 55.1609, MinusLogProbMetric: 55.1609, val_loss: 55.3723, val_MinusLogProbMetric: 55.3723

Epoch 810: val_loss improved from 55.37372 to 55.37235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 55.1609 - MinusLogProbMetric: 55.1609 - val_loss: 55.3723 - val_MinusLogProbMetric: 55.3723 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 811/1000
2023-10-31 04:52:09.719 
Epoch 811/1000 
	 loss: 55.0898, MinusLogProbMetric: 55.0898, val_loss: 55.3536, val_MinusLogProbMetric: 55.3536

Epoch 811: val_loss improved from 55.37235 to 55.35360, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 55.0898 - MinusLogProbMetric: 55.0898 - val_loss: 55.3536 - val_MinusLogProbMetric: 55.3536 - lr: 2.0576e-06 - 58s/epoch - 294ms/step
Epoch 812/1000
2023-10-31 04:53:10.341 
Epoch 812/1000 
	 loss: 55.1380, MinusLogProbMetric: 55.1380, val_loss: 55.3061, val_MinusLogProbMetric: 55.3061

Epoch 812: val_loss improved from 55.35360 to 55.30609, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 55.1380 - MinusLogProbMetric: 55.1380 - val_loss: 55.3061 - val_MinusLogProbMetric: 55.3061 - lr: 2.0576e-06 - 60s/epoch - 309ms/step
Epoch 813/1000
2023-10-31 04:54:08.365 
Epoch 813/1000 
	 loss: 55.0941, MinusLogProbMetric: 55.0941, val_loss: 55.2987, val_MinusLogProbMetric: 55.2987

Epoch 813: val_loss improved from 55.30609 to 55.29873, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 55.0941 - MinusLogProbMetric: 55.0941 - val_loss: 55.2987 - val_MinusLogProbMetric: 55.2987 - lr: 2.0576e-06 - 58s/epoch - 296ms/step
Epoch 814/1000
2023-10-31 04:55:06.072 
Epoch 814/1000 
	 loss: 55.0491, MinusLogProbMetric: 55.0491, val_loss: 55.2852, val_MinusLogProbMetric: 55.2852

Epoch 814: val_loss improved from 55.29873 to 55.28516, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 55.0491 - MinusLogProbMetric: 55.0491 - val_loss: 55.2852 - val_MinusLogProbMetric: 55.2852 - lr: 2.0576e-06 - 58s/epoch - 294ms/step
Epoch 815/1000
2023-10-31 04:56:00.491 
Epoch 815/1000 
	 loss: 55.0533, MinusLogProbMetric: 55.0533, val_loss: 55.2732, val_MinusLogProbMetric: 55.2732

Epoch 815: val_loss improved from 55.28516 to 55.27317, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 55.0533 - MinusLogProbMetric: 55.0533 - val_loss: 55.2732 - val_MinusLogProbMetric: 55.2732 - lr: 2.0576e-06 - 54s/epoch - 278ms/step
Epoch 816/1000
2023-10-31 04:56:57.314 
Epoch 816/1000 
	 loss: 55.0180, MinusLogProbMetric: 55.0180, val_loss: 55.2862, val_MinusLogProbMetric: 55.2862

Epoch 816: val_loss did not improve from 55.27317
196/196 - 56s - loss: 55.0180 - MinusLogProbMetric: 55.0180 - val_loss: 55.2862 - val_MinusLogProbMetric: 55.2862 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 817/1000
2023-10-31 04:57:57.631 
Epoch 817/1000 
	 loss: 55.0069, MinusLogProbMetric: 55.0069, val_loss: 55.2631, val_MinusLogProbMetric: 55.2631

Epoch 817: val_loss improved from 55.27317 to 55.26312, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 55.0069 - MinusLogProbMetric: 55.0069 - val_loss: 55.2631 - val_MinusLogProbMetric: 55.2631 - lr: 2.0576e-06 - 61s/epoch - 312ms/step
Epoch 818/1000
2023-10-31 04:58:56.156 
Epoch 818/1000 
	 loss: 55.0044, MinusLogProbMetric: 55.0044, val_loss: 55.2295, val_MinusLogProbMetric: 55.2295

Epoch 818: val_loss improved from 55.26312 to 55.22953, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 55.0044 - MinusLogProbMetric: 55.0044 - val_loss: 55.2295 - val_MinusLogProbMetric: 55.2295 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 819/1000
2023-10-31 04:59:55.527 
Epoch 819/1000 
	 loss: 54.9825, MinusLogProbMetric: 54.9825, val_loss: 55.2114, val_MinusLogProbMetric: 55.2114

Epoch 819: val_loss improved from 55.22953 to 55.21142, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 54.9825 - MinusLogProbMetric: 54.9825 - val_loss: 55.2114 - val_MinusLogProbMetric: 55.2114 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 820/1000
2023-10-31 05:00:54.519 
Epoch 820/1000 
	 loss: 54.9680, MinusLogProbMetric: 54.9680, val_loss: 55.1974, val_MinusLogProbMetric: 55.1974

Epoch 820: val_loss improved from 55.21142 to 55.19740, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 54.9680 - MinusLogProbMetric: 54.9680 - val_loss: 55.1974 - val_MinusLogProbMetric: 55.1974 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 821/1000
2023-10-31 05:01:52.154 
Epoch 821/1000 
	 loss: 54.9613, MinusLogProbMetric: 54.9613, val_loss: 55.2190, val_MinusLogProbMetric: 55.2190

Epoch 821: val_loss did not improve from 55.19740
196/196 - 57s - loss: 54.9613 - MinusLogProbMetric: 54.9613 - val_loss: 55.2190 - val_MinusLogProbMetric: 55.2190 - lr: 2.0576e-06 - 57s/epoch - 289ms/step
Epoch 822/1000
2023-10-31 05:02:44.414 
Epoch 822/1000 
	 loss: 54.9439, MinusLogProbMetric: 54.9439, val_loss: 55.1850, val_MinusLogProbMetric: 55.1850

Epoch 822: val_loss improved from 55.19740 to 55.18502, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 53s - loss: 54.9439 - MinusLogProbMetric: 54.9439 - val_loss: 55.1850 - val_MinusLogProbMetric: 55.1850 - lr: 2.0576e-06 - 53s/epoch - 271ms/step
Epoch 823/1000
2023-10-31 05:03:40.021 
Epoch 823/1000 
	 loss: 54.9193, MinusLogProbMetric: 54.9193, val_loss: 55.1507, val_MinusLogProbMetric: 55.1507

Epoch 823: val_loss improved from 55.18502 to 55.15075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 54.9193 - MinusLogProbMetric: 54.9193 - val_loss: 55.1507 - val_MinusLogProbMetric: 55.1507 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 824/1000
2023-10-31 05:04:39.484 
Epoch 824/1000 
	 loss: 54.9541, MinusLogProbMetric: 54.9541, val_loss: 55.2133, val_MinusLogProbMetric: 55.2133

Epoch 824: val_loss did not improve from 55.15075
196/196 - 59s - loss: 54.9541 - MinusLogProbMetric: 54.9541 - val_loss: 55.2133 - val_MinusLogProbMetric: 55.2133 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 825/1000
2023-10-31 05:05:36.893 
Epoch 825/1000 
	 loss: 54.9023, MinusLogProbMetric: 54.9023, val_loss: 55.1269, val_MinusLogProbMetric: 55.1269

Epoch 825: val_loss improved from 55.15075 to 55.12693, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 54.9023 - MinusLogProbMetric: 54.9023 - val_loss: 55.1269 - val_MinusLogProbMetric: 55.1269 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 826/1000
2023-10-31 05:06:36.364 
Epoch 826/1000 
	 loss: 54.8974, MinusLogProbMetric: 54.8974, val_loss: 55.1087, val_MinusLogProbMetric: 55.1087

Epoch 826: val_loss improved from 55.12693 to 55.10874, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 54.8974 - MinusLogProbMetric: 54.8974 - val_loss: 55.1087 - val_MinusLogProbMetric: 55.1087 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 827/1000
2023-10-31 05:07:34.665 
Epoch 827/1000 
	 loss: 54.8949, MinusLogProbMetric: 54.8949, val_loss: 55.1121, val_MinusLogProbMetric: 55.1121

Epoch 827: val_loss did not improve from 55.10874
196/196 - 57s - loss: 54.8949 - MinusLogProbMetric: 54.8949 - val_loss: 55.1121 - val_MinusLogProbMetric: 55.1121 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 828/1000
2023-10-31 05:08:31.141 
Epoch 828/1000 
	 loss: 54.8571, MinusLogProbMetric: 54.8571, val_loss: 55.1004, val_MinusLogProbMetric: 55.1004

Epoch 828: val_loss improved from 55.10874 to 55.10038, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 54.8571 - MinusLogProbMetric: 54.8571 - val_loss: 55.1004 - val_MinusLogProbMetric: 55.1004 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 829/1000
2023-10-31 05:09:30.975 
Epoch 829/1000 
	 loss: 54.8453, MinusLogProbMetric: 54.8453, val_loss: 55.0722, val_MinusLogProbMetric: 55.0722

Epoch 829: val_loss improved from 55.10038 to 55.07222, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 54.8453 - MinusLogProbMetric: 54.8453 - val_loss: 55.0722 - val_MinusLogProbMetric: 55.0722 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 830/1000
2023-10-31 05:10:30.097 
Epoch 830/1000 
	 loss: 54.8294, MinusLogProbMetric: 54.8294, val_loss: 55.0520, val_MinusLogProbMetric: 55.0520

Epoch 830: val_loss improved from 55.07222 to 55.05204, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 54.8294 - MinusLogProbMetric: 54.8294 - val_loss: 55.0520 - val_MinusLogProbMetric: 55.0520 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 831/1000
2023-10-31 05:11:27.915 
Epoch 831/1000 
	 loss: 54.8328, MinusLogProbMetric: 54.8328, val_loss: 55.0508, val_MinusLogProbMetric: 55.0508

Epoch 831: val_loss improved from 55.05204 to 55.05081, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 54.8328 - MinusLogProbMetric: 54.8328 - val_loss: 55.0508 - val_MinusLogProbMetric: 55.0508 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 832/1000
2023-10-31 05:12:26.884 
Epoch 832/1000 
	 loss: 54.8252, MinusLogProbMetric: 54.8252, val_loss: 55.0285, val_MinusLogProbMetric: 55.0285

Epoch 832: val_loss improved from 55.05081 to 55.02852, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 54.8252 - MinusLogProbMetric: 54.8252 - val_loss: 55.0285 - val_MinusLogProbMetric: 55.0285 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 833/1000
2023-10-31 05:13:25.169 
Epoch 833/1000 
	 loss: 54.7882, MinusLogProbMetric: 54.7882, val_loss: 55.0044, val_MinusLogProbMetric: 55.0044

Epoch 833: val_loss improved from 55.02852 to 55.00438, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 54.7882 - MinusLogProbMetric: 54.7882 - val_loss: 55.0044 - val_MinusLogProbMetric: 55.0044 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 834/1000
2023-10-31 05:14:26.746 
Epoch 834/1000 
	 loss: 54.7708, MinusLogProbMetric: 54.7708, val_loss: 55.0260, val_MinusLogProbMetric: 55.0260

Epoch 834: val_loss did not improve from 55.00438
196/196 - 61s - loss: 54.7708 - MinusLogProbMetric: 54.7708 - val_loss: 55.0260 - val_MinusLogProbMetric: 55.0260 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 835/1000
2023-10-31 05:15:26.598 
Epoch 835/1000 
	 loss: 54.7590, MinusLogProbMetric: 54.7590, val_loss: 54.9957, val_MinusLogProbMetric: 54.9957

Epoch 835: val_loss improved from 55.00438 to 54.99573, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 54.7590 - MinusLogProbMetric: 54.7590 - val_loss: 54.9957 - val_MinusLogProbMetric: 54.9957 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 836/1000
2023-10-31 05:16:25.954 
Epoch 836/1000 
	 loss: 54.7465, MinusLogProbMetric: 54.7465, val_loss: 54.9772, val_MinusLogProbMetric: 54.9772

Epoch 836: val_loss improved from 54.99573 to 54.97725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 54.7465 - MinusLogProbMetric: 54.7465 - val_loss: 54.9772 - val_MinusLogProbMetric: 54.9772 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 837/1000
2023-10-31 05:17:24.659 
Epoch 837/1000 
	 loss: 54.7257, MinusLogProbMetric: 54.7257, val_loss: 54.9576, val_MinusLogProbMetric: 54.9576

Epoch 837: val_loss improved from 54.97725 to 54.95760, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 54.7257 - MinusLogProbMetric: 54.7257 - val_loss: 54.9576 - val_MinusLogProbMetric: 54.9576 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 838/1000
2023-10-31 05:18:23.991 
Epoch 838/1000 
	 loss: 54.7500, MinusLogProbMetric: 54.7500, val_loss: 54.9560, val_MinusLogProbMetric: 54.9560

Epoch 838: val_loss improved from 54.95760 to 54.95597, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 54.7500 - MinusLogProbMetric: 54.7500 - val_loss: 54.9560 - val_MinusLogProbMetric: 54.9560 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 839/1000
2023-10-31 05:19:24.968 
Epoch 839/1000 
	 loss: 54.7337, MinusLogProbMetric: 54.7337, val_loss: 54.9429, val_MinusLogProbMetric: 54.9429

Epoch 839: val_loss improved from 54.95597 to 54.94291, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 54.7337 - MinusLogProbMetric: 54.7337 - val_loss: 54.9429 - val_MinusLogProbMetric: 54.9429 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 840/1000
2023-10-31 05:20:23.922 
Epoch 840/1000 
	 loss: 54.6984, MinusLogProbMetric: 54.6984, val_loss: 54.9599, val_MinusLogProbMetric: 54.9599

Epoch 840: val_loss did not improve from 54.94291
196/196 - 58s - loss: 54.6984 - MinusLogProbMetric: 54.6984 - val_loss: 54.9599 - val_MinusLogProbMetric: 54.9599 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 841/1000
2023-10-31 05:21:24.036 
Epoch 841/1000 
	 loss: 54.6809, MinusLogProbMetric: 54.6809, val_loss: 54.9065, val_MinusLogProbMetric: 54.9065

Epoch 841: val_loss improved from 54.94291 to 54.90646, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 54.6809 - MinusLogProbMetric: 54.6809 - val_loss: 54.9065 - val_MinusLogProbMetric: 54.9065 - lr: 2.0576e-06 - 61s/epoch - 311ms/step
Epoch 842/1000
2023-10-31 05:22:22.146 
Epoch 842/1000 
	 loss: 54.6927, MinusLogProbMetric: 54.6927, val_loss: 54.9153, val_MinusLogProbMetric: 54.9153

Epoch 842: val_loss did not improve from 54.90646
196/196 - 57s - loss: 54.6927 - MinusLogProbMetric: 54.6927 - val_loss: 54.9153 - val_MinusLogProbMetric: 54.9153 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 843/1000
2023-10-31 05:23:19.632 
Epoch 843/1000 
	 loss: 56.7778, MinusLogProbMetric: 56.7778, val_loss: 57.6732, val_MinusLogProbMetric: 57.6732

Epoch 843: val_loss did not improve from 54.90646
196/196 - 57s - loss: 56.7778 - MinusLogProbMetric: 56.7778 - val_loss: 57.6732 - val_MinusLogProbMetric: 57.6732 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 844/1000
2023-10-31 05:24:15.956 
Epoch 844/1000 
	 loss: 55.4258, MinusLogProbMetric: 55.4258, val_loss: 55.3412, val_MinusLogProbMetric: 55.3412

Epoch 844: val_loss did not improve from 54.90646
196/196 - 56s - loss: 55.4258 - MinusLogProbMetric: 55.4258 - val_loss: 55.3412 - val_MinusLogProbMetric: 55.3412 - lr: 2.0576e-06 - 56s/epoch - 287ms/step
Epoch 845/1000
2023-10-31 05:25:17.218 
Epoch 845/1000 
	 loss: 55.0137, MinusLogProbMetric: 55.0137, val_loss: 55.3002, val_MinusLogProbMetric: 55.3002

Epoch 845: val_loss did not improve from 54.90646
196/196 - 61s - loss: 55.0137 - MinusLogProbMetric: 55.0137 - val_loss: 55.3002 - val_MinusLogProbMetric: 55.3002 - lr: 2.0576e-06 - 61s/epoch - 313ms/step
Epoch 846/1000
2023-10-31 05:26:15.617 
Epoch 846/1000 
	 loss: 54.9275, MinusLogProbMetric: 54.9275, val_loss: 55.1201, val_MinusLogProbMetric: 55.1201

Epoch 846: val_loss did not improve from 54.90646
196/196 - 58s - loss: 54.9275 - MinusLogProbMetric: 54.9275 - val_loss: 55.1201 - val_MinusLogProbMetric: 55.1201 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 847/1000
2023-10-31 05:27:14.316 
Epoch 847/1000 
	 loss: 54.8662, MinusLogProbMetric: 54.8662, val_loss: 55.0669, val_MinusLogProbMetric: 55.0669

Epoch 847: val_loss did not improve from 54.90646
196/196 - 59s - loss: 54.8662 - MinusLogProbMetric: 54.8662 - val_loss: 55.0669 - val_MinusLogProbMetric: 55.0669 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 848/1000
2023-10-31 05:28:13.464 
Epoch 848/1000 
	 loss: 54.8229, MinusLogProbMetric: 54.8229, val_loss: 55.0703, val_MinusLogProbMetric: 55.0703

Epoch 848: val_loss did not improve from 54.90646
196/196 - 59s - loss: 54.8229 - MinusLogProbMetric: 54.8229 - val_loss: 55.0703 - val_MinusLogProbMetric: 55.0703 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 849/1000
2023-10-31 05:29:12.853 
Epoch 849/1000 
	 loss: 54.7845, MinusLogProbMetric: 54.7845, val_loss: 55.0244, val_MinusLogProbMetric: 55.0244

Epoch 849: val_loss did not improve from 54.90646
196/196 - 59s - loss: 54.7845 - MinusLogProbMetric: 54.7845 - val_loss: 55.0244 - val_MinusLogProbMetric: 55.0244 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 850/1000
2023-10-31 05:30:12.436 
Epoch 850/1000 
	 loss: 54.7561, MinusLogProbMetric: 54.7561, val_loss: 55.0270, val_MinusLogProbMetric: 55.0270

Epoch 850: val_loss did not improve from 54.90646
196/196 - 60s - loss: 54.7561 - MinusLogProbMetric: 54.7561 - val_loss: 55.0270 - val_MinusLogProbMetric: 55.0270 - lr: 2.0576e-06 - 60s/epoch - 304ms/step
Epoch 851/1000
2023-10-31 05:31:12.742 
Epoch 851/1000 
	 loss: 54.7505, MinusLogProbMetric: 54.7505, val_loss: 54.9615, val_MinusLogProbMetric: 54.9615

Epoch 851: val_loss did not improve from 54.90646
196/196 - 60s - loss: 54.7505 - MinusLogProbMetric: 54.7505 - val_loss: 54.9615 - val_MinusLogProbMetric: 54.9615 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 852/1000
2023-10-31 05:32:13.219 
Epoch 852/1000 
	 loss: 54.7090, MinusLogProbMetric: 54.7090, val_loss: 55.0315, val_MinusLogProbMetric: 55.0315

Epoch 852: val_loss did not improve from 54.90646
196/196 - 60s - loss: 54.7090 - MinusLogProbMetric: 54.7090 - val_loss: 55.0315 - val_MinusLogProbMetric: 55.0315 - lr: 2.0576e-06 - 60s/epoch - 309ms/step
Epoch 853/1000
2023-10-31 05:33:10.039 
Epoch 853/1000 
	 loss: 54.6938, MinusLogProbMetric: 54.6938, val_loss: 54.9452, val_MinusLogProbMetric: 54.9452

Epoch 853: val_loss did not improve from 54.90646
196/196 - 57s - loss: 54.6938 - MinusLogProbMetric: 54.6938 - val_loss: 54.9452 - val_MinusLogProbMetric: 54.9452 - lr: 2.0576e-06 - 57s/epoch - 290ms/step
Epoch 854/1000
2023-10-31 05:34:09.400 
Epoch 854/1000 
	 loss: 54.7405, MinusLogProbMetric: 54.7405, val_loss: 54.9347, val_MinusLogProbMetric: 54.9347

Epoch 854: val_loss did not improve from 54.90646
196/196 - 59s - loss: 54.7405 - MinusLogProbMetric: 54.7405 - val_loss: 54.9347 - val_MinusLogProbMetric: 54.9347 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 855/1000
2023-10-31 05:35:06.004 
Epoch 855/1000 
	 loss: 54.6690, MinusLogProbMetric: 54.6690, val_loss: 54.8868, val_MinusLogProbMetric: 54.8868

Epoch 855: val_loss improved from 54.90646 to 54.88680, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 54.6690 - MinusLogProbMetric: 54.6690 - val_loss: 54.8868 - val_MinusLogProbMetric: 54.8868 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 856/1000
2023-10-31 05:36:03.622 
Epoch 856/1000 
	 loss: 54.6775, MinusLogProbMetric: 54.6775, val_loss: 54.9052, val_MinusLogProbMetric: 54.9052

Epoch 856: val_loss did not improve from 54.88680
196/196 - 57s - loss: 54.6775 - MinusLogProbMetric: 54.6775 - val_loss: 54.9052 - val_MinusLogProbMetric: 54.9052 - lr: 2.0576e-06 - 57s/epoch - 289ms/step
Epoch 857/1000
2023-10-31 05:37:02.482 
Epoch 857/1000 
	 loss: 54.6187, MinusLogProbMetric: 54.6187, val_loss: 54.8572, val_MinusLogProbMetric: 54.8572

Epoch 857: val_loss improved from 54.88680 to 54.85715, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 54.6187 - MinusLogProbMetric: 54.6187 - val_loss: 54.8572 - val_MinusLogProbMetric: 54.8572 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 858/1000
2023-10-31 05:38:00.217 
Epoch 858/1000 
	 loss: 54.6953, MinusLogProbMetric: 54.6953, val_loss: 55.0528, val_MinusLogProbMetric: 55.0528

Epoch 858: val_loss did not improve from 54.85715
196/196 - 57s - loss: 54.6953 - MinusLogProbMetric: 54.6953 - val_loss: 55.0528 - val_MinusLogProbMetric: 55.0528 - lr: 2.0576e-06 - 57s/epoch - 290ms/step
Epoch 859/1000
2023-10-31 05:38:54.847 
Epoch 859/1000 
	 loss: 54.6139, MinusLogProbMetric: 54.6139, val_loss: 54.8369, val_MinusLogProbMetric: 54.8369

Epoch 859: val_loss improved from 54.85715 to 54.83691, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 54.6139 - MinusLogProbMetric: 54.6139 - val_loss: 54.8369 - val_MinusLogProbMetric: 54.8369 - lr: 2.0576e-06 - 55s/epoch - 283ms/step
Epoch 860/1000
2023-10-31 05:39:52.101 
Epoch 860/1000 
	 loss: 54.5632, MinusLogProbMetric: 54.5632, val_loss: 54.8351, val_MinusLogProbMetric: 54.8351

Epoch 860: val_loss improved from 54.83691 to 54.83514, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 54.5632 - MinusLogProbMetric: 54.5632 - val_loss: 54.8351 - val_MinusLogProbMetric: 54.8351 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 861/1000
2023-10-31 05:40:50.282 
Epoch 861/1000 
	 loss: 54.5504, MinusLogProbMetric: 54.5504, val_loss: 54.8093, val_MinusLogProbMetric: 54.8093

Epoch 861: val_loss improved from 54.83514 to 54.80926, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 54.5504 - MinusLogProbMetric: 54.5504 - val_loss: 54.8093 - val_MinusLogProbMetric: 54.8093 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 862/1000
2023-10-31 05:41:50.954 
Epoch 862/1000 
	 loss: 54.5334, MinusLogProbMetric: 54.5334, val_loss: 54.7949, val_MinusLogProbMetric: 54.7949

Epoch 862: val_loss improved from 54.80926 to 54.79490, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 54.5334 - MinusLogProbMetric: 54.5334 - val_loss: 54.7949 - val_MinusLogProbMetric: 54.7949 - lr: 2.0576e-06 - 61s/epoch - 309ms/step
Epoch 863/1000
2023-10-31 05:42:49.166 
Epoch 863/1000 
	 loss: 54.5238, MinusLogProbMetric: 54.5238, val_loss: 54.7594, val_MinusLogProbMetric: 54.7594

Epoch 863: val_loss improved from 54.79490 to 54.75938, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 54.5238 - MinusLogProbMetric: 54.5238 - val_loss: 54.7594 - val_MinusLogProbMetric: 54.7594 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 864/1000
2023-10-31 05:43:48.941 
Epoch 864/1000 
	 loss: 54.4988, MinusLogProbMetric: 54.4988, val_loss: 54.7631, val_MinusLogProbMetric: 54.7631

Epoch 864: val_loss did not improve from 54.75938
196/196 - 59s - loss: 54.4988 - MinusLogProbMetric: 54.4988 - val_loss: 54.7631 - val_MinusLogProbMetric: 54.7631 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 865/1000
2023-10-31 05:44:47.309 
Epoch 865/1000 
	 loss: 54.5139, MinusLogProbMetric: 54.5139, val_loss: 54.7104, val_MinusLogProbMetric: 54.7104

Epoch 865: val_loss improved from 54.75938 to 54.71040, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 54.5139 - MinusLogProbMetric: 54.5139 - val_loss: 54.7104 - val_MinusLogProbMetric: 54.7104 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 866/1000
2023-10-31 05:45:45.074 
Epoch 866/1000 
	 loss: 54.7646, MinusLogProbMetric: 54.7646, val_loss: 54.8430, val_MinusLogProbMetric: 54.8430

Epoch 866: val_loss did not improve from 54.71040
196/196 - 57s - loss: 54.7646 - MinusLogProbMetric: 54.7646 - val_loss: 54.8430 - val_MinusLogProbMetric: 54.8430 - lr: 2.0576e-06 - 57s/epoch - 290ms/step
Epoch 867/1000
2023-10-31 05:46:44.241 
Epoch 867/1000 
	 loss: 54.4742, MinusLogProbMetric: 54.4742, val_loss: 54.6897, val_MinusLogProbMetric: 54.6897

Epoch 867: val_loss improved from 54.71040 to 54.68974, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 54.4742 - MinusLogProbMetric: 54.4742 - val_loss: 54.6897 - val_MinusLogProbMetric: 54.6897 - lr: 2.0576e-06 - 60s/epoch - 306ms/step
Epoch 868/1000
2023-10-31 05:47:45.057 
Epoch 868/1000 
	 loss: 54.4291, MinusLogProbMetric: 54.4291, val_loss: 54.6831, val_MinusLogProbMetric: 54.6831

Epoch 868: val_loss improved from 54.68974 to 54.68306, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 54.4291 - MinusLogProbMetric: 54.4291 - val_loss: 54.6831 - val_MinusLogProbMetric: 54.6831 - lr: 2.0576e-06 - 61s/epoch - 311ms/step
Epoch 869/1000
2023-10-31 05:48:46.106 
Epoch 869/1000 
	 loss: 54.4077, MinusLogProbMetric: 54.4077, val_loss: 54.6456, val_MinusLogProbMetric: 54.6456

Epoch 869: val_loss improved from 54.68306 to 54.64558, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 54.4077 - MinusLogProbMetric: 54.4077 - val_loss: 54.6456 - val_MinusLogProbMetric: 54.6456 - lr: 2.0576e-06 - 61s/epoch - 311ms/step
Epoch 870/1000
2023-10-31 05:49:42.519 
Epoch 870/1000 
	 loss: 54.3931, MinusLogProbMetric: 54.3931, val_loss: 54.6237, val_MinusLogProbMetric: 54.6237

Epoch 870: val_loss improved from 54.64558 to 54.62371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 54.3931 - MinusLogProbMetric: 54.3931 - val_loss: 54.6237 - val_MinusLogProbMetric: 54.6237 - lr: 2.0576e-06 - 56s/epoch - 288ms/step
Epoch 871/1000
2023-10-31 05:50:41.333 
Epoch 871/1000 
	 loss: 54.3715, MinusLogProbMetric: 54.3715, val_loss: 54.6184, val_MinusLogProbMetric: 54.6184

Epoch 871: val_loss improved from 54.62371 to 54.61843, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 54.3715 - MinusLogProbMetric: 54.3715 - val_loss: 54.6184 - val_MinusLogProbMetric: 54.6184 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 872/1000
2023-10-31 05:51:37.559 
Epoch 872/1000 
	 loss: 54.3616, MinusLogProbMetric: 54.3616, val_loss: 54.6093, val_MinusLogProbMetric: 54.6093

Epoch 872: val_loss improved from 54.61843 to 54.60925, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 54.3616 - MinusLogProbMetric: 54.3616 - val_loss: 54.6093 - val_MinusLogProbMetric: 54.6093 - lr: 2.0576e-06 - 56s/epoch - 287ms/step
Epoch 873/1000
2023-10-31 05:52:35.605 
Epoch 873/1000 
	 loss: 54.3456, MinusLogProbMetric: 54.3456, val_loss: 54.5880, val_MinusLogProbMetric: 54.5880

Epoch 873: val_loss improved from 54.60925 to 54.58799, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 54.3456 - MinusLogProbMetric: 54.3456 - val_loss: 54.5880 - val_MinusLogProbMetric: 54.5880 - lr: 2.0576e-06 - 58s/epoch - 296ms/step
Epoch 874/1000
2023-10-31 05:53:34.664 
Epoch 874/1000 
	 loss: 54.3842, MinusLogProbMetric: 54.3842, val_loss: 54.5747, val_MinusLogProbMetric: 54.5747

Epoch 874: val_loss improved from 54.58799 to 54.57473, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 54.3842 - MinusLogProbMetric: 54.3842 - val_loss: 54.5747 - val_MinusLogProbMetric: 54.5747 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 875/1000
2023-10-31 05:54:34.121 
Epoch 875/1000 
	 loss: 54.3236, MinusLogProbMetric: 54.3236, val_loss: 54.5881, val_MinusLogProbMetric: 54.5881

Epoch 875: val_loss did not improve from 54.57473
196/196 - 59s - loss: 54.3236 - MinusLogProbMetric: 54.3236 - val_loss: 54.5881 - val_MinusLogProbMetric: 54.5881 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 876/1000
2023-10-31 05:55:28.886 
Epoch 876/1000 
	 loss: 54.3152, MinusLogProbMetric: 54.3152, val_loss: 54.5313, val_MinusLogProbMetric: 54.5313

Epoch 876: val_loss improved from 54.57473 to 54.53131, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 54.3152 - MinusLogProbMetric: 54.3152 - val_loss: 54.5313 - val_MinusLogProbMetric: 54.5313 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 877/1000
2023-10-31 05:56:28.371 
Epoch 877/1000 
	 loss: 54.2896, MinusLogProbMetric: 54.2896, val_loss: 54.5094, val_MinusLogProbMetric: 54.5094

Epoch 877: val_loss improved from 54.53131 to 54.50936, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 54.2896 - MinusLogProbMetric: 54.2896 - val_loss: 54.5094 - val_MinusLogProbMetric: 54.5094 - lr: 2.0576e-06 - 59s/epoch - 304ms/step
Epoch 878/1000
2023-10-31 05:57:31.767 
Epoch 878/1000 
	 loss: 54.2686, MinusLogProbMetric: 54.2686, val_loss: 54.5153, val_MinusLogProbMetric: 54.5153

Epoch 878: val_loss did not improve from 54.50936
196/196 - 63s - loss: 54.2686 - MinusLogProbMetric: 54.2686 - val_loss: 54.5153 - val_MinusLogProbMetric: 54.5153 - lr: 2.0576e-06 - 63s/epoch - 319ms/step
Epoch 879/1000
2023-10-31 05:58:30.218 
Epoch 879/1000 
	 loss: 54.2453, MinusLogProbMetric: 54.2453, val_loss: 54.5185, val_MinusLogProbMetric: 54.5185

Epoch 879: val_loss did not improve from 54.50936
196/196 - 58s - loss: 54.2453 - MinusLogProbMetric: 54.2453 - val_loss: 54.5185 - val_MinusLogProbMetric: 54.5185 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 880/1000
2023-10-31 05:59:25.021 
Epoch 880/1000 
	 loss: 54.2650, MinusLogProbMetric: 54.2650, val_loss: 54.4861, val_MinusLogProbMetric: 54.4861

Epoch 880: val_loss improved from 54.50936 to 54.48612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 54.2650 - MinusLogProbMetric: 54.2650 - val_loss: 54.4861 - val_MinusLogProbMetric: 54.4861 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 881/1000
2023-10-31 06:00:20.768 
Epoch 881/1000 
	 loss: 54.2189, MinusLogProbMetric: 54.2189, val_loss: 54.4465, val_MinusLogProbMetric: 54.4465

Epoch 881: val_loss improved from 54.48612 to 54.44648, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 54.2189 - MinusLogProbMetric: 54.2189 - val_loss: 54.4465 - val_MinusLogProbMetric: 54.4465 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 882/1000
2023-10-31 06:01:24.656 
Epoch 882/1000 
	 loss: 54.2026, MinusLogProbMetric: 54.2026, val_loss: 54.4990, val_MinusLogProbMetric: 54.4990

Epoch 882: val_loss did not improve from 54.44648
196/196 - 63s - loss: 54.2026 - MinusLogProbMetric: 54.2026 - val_loss: 54.4990 - val_MinusLogProbMetric: 54.4990 - lr: 2.0576e-06 - 63s/epoch - 321ms/step
Epoch 883/1000
2023-10-31 06:02:26.675 
Epoch 883/1000 
	 loss: 54.1817, MinusLogProbMetric: 54.1817, val_loss: 54.4369, val_MinusLogProbMetric: 54.4369

Epoch 883: val_loss improved from 54.44648 to 54.43689, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 63s - loss: 54.1817 - MinusLogProbMetric: 54.1817 - val_loss: 54.4369 - val_MinusLogProbMetric: 54.4369 - lr: 2.0576e-06 - 63s/epoch - 321ms/step
Epoch 884/1000
2023-10-31 06:03:28.023 
Epoch 884/1000 
	 loss: 54.1698, MinusLogProbMetric: 54.1698, val_loss: 54.4356, val_MinusLogProbMetric: 54.4356

Epoch 884: val_loss improved from 54.43689 to 54.43562, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 54.1698 - MinusLogProbMetric: 54.1698 - val_loss: 54.4356 - val_MinusLogProbMetric: 54.4356 - lr: 2.0576e-06 - 61s/epoch - 313ms/step
Epoch 885/1000
2023-10-31 06:04:26.811 
Epoch 885/1000 
	 loss: 54.1501, MinusLogProbMetric: 54.1501, val_loss: 54.3826, val_MinusLogProbMetric: 54.3826

Epoch 885: val_loss improved from 54.43562 to 54.38264, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 54.1501 - MinusLogProbMetric: 54.1501 - val_loss: 54.3826 - val_MinusLogProbMetric: 54.3826 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 886/1000
2023-10-31 06:05:26.395 
Epoch 886/1000 
	 loss: 54.3420, MinusLogProbMetric: 54.3420, val_loss: 54.3827, val_MinusLogProbMetric: 54.3827

Epoch 886: val_loss did not improve from 54.38264
196/196 - 59s - loss: 54.3420 - MinusLogProbMetric: 54.3420 - val_loss: 54.3827 - val_MinusLogProbMetric: 54.3827 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 887/1000
2023-10-31 06:06:26.004 
Epoch 887/1000 
	 loss: 54.1998, MinusLogProbMetric: 54.1998, val_loss: 54.3774, val_MinusLogProbMetric: 54.3774

Epoch 887: val_loss improved from 54.38264 to 54.37737, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 54.1998 - MinusLogProbMetric: 54.1998 - val_loss: 54.3774 - val_MinusLogProbMetric: 54.3774 - lr: 2.0576e-06 - 61s/epoch - 309ms/step
Epoch 888/1000
2023-10-31 06:07:25.091 
Epoch 888/1000 
	 loss: 54.1392, MinusLogProbMetric: 54.1392, val_loss: 54.3499, val_MinusLogProbMetric: 54.3499

Epoch 888: val_loss improved from 54.37737 to 54.34986, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 54.1392 - MinusLogProbMetric: 54.1392 - val_loss: 54.3499 - val_MinusLogProbMetric: 54.3499 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 889/1000
2023-10-31 06:08:26.395 
Epoch 889/1000 
	 loss: 54.0930, MinusLogProbMetric: 54.0930, val_loss: 54.3295, val_MinusLogProbMetric: 54.3295

Epoch 889: val_loss improved from 54.34986 to 54.32949, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 54.0930 - MinusLogProbMetric: 54.0930 - val_loss: 54.3295 - val_MinusLogProbMetric: 54.3295 - lr: 2.0576e-06 - 61s/epoch - 313ms/step
Epoch 890/1000
2023-10-31 06:09:26.558 
Epoch 890/1000 
	 loss: 54.0710, MinusLogProbMetric: 54.0710, val_loss: 54.3242, val_MinusLogProbMetric: 54.3242

Epoch 890: val_loss improved from 54.32949 to 54.32425, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 54.0710 - MinusLogProbMetric: 54.0710 - val_loss: 54.3242 - val_MinusLogProbMetric: 54.3242 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 891/1000
2023-10-31 06:10:26.751 
Epoch 891/1000 
	 loss: 54.1723, MinusLogProbMetric: 54.1723, val_loss: 54.3235, val_MinusLogProbMetric: 54.3235

Epoch 891: val_loss improved from 54.32425 to 54.32349, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 54.1723 - MinusLogProbMetric: 54.1723 - val_loss: 54.3235 - val_MinusLogProbMetric: 54.3235 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 892/1000
2023-10-31 06:11:25.262 
Epoch 892/1000 
	 loss: 54.0499, MinusLogProbMetric: 54.0499, val_loss: 54.3169, val_MinusLogProbMetric: 54.3169

Epoch 892: val_loss improved from 54.32349 to 54.31692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 54.0499 - MinusLogProbMetric: 54.0499 - val_loss: 54.3169 - val_MinusLogProbMetric: 54.3169 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 893/1000
2023-10-31 06:12:20.890 
Epoch 893/1000 
	 loss: 54.0355, MinusLogProbMetric: 54.0355, val_loss: 54.2924, val_MinusLogProbMetric: 54.2924

Epoch 893: val_loss improved from 54.31692 to 54.29244, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 54.0355 - MinusLogProbMetric: 54.0355 - val_loss: 54.2924 - val_MinusLogProbMetric: 54.2924 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 894/1000
2023-10-31 06:13:21.833 
Epoch 894/1000 
	 loss: 54.0227, MinusLogProbMetric: 54.0227, val_loss: 54.2553, val_MinusLogProbMetric: 54.2553

Epoch 894: val_loss improved from 54.29244 to 54.25532, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 54.0227 - MinusLogProbMetric: 54.0227 - val_loss: 54.2553 - val_MinusLogProbMetric: 54.2553 - lr: 2.0576e-06 - 61s/epoch - 311ms/step
Epoch 895/1000
2023-10-31 06:14:23.896 
Epoch 895/1000 
	 loss: 53.9994, MinusLogProbMetric: 53.9994, val_loss: 54.2334, val_MinusLogProbMetric: 54.2334

Epoch 895: val_loss improved from 54.25532 to 54.23336, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 53.9994 - MinusLogProbMetric: 53.9994 - val_loss: 54.2334 - val_MinusLogProbMetric: 54.2334 - lr: 2.0576e-06 - 62s/epoch - 317ms/step
Epoch 896/1000
2023-10-31 06:15:21.500 
Epoch 896/1000 
	 loss: 53.9807, MinusLogProbMetric: 53.9807, val_loss: 54.2212, val_MinusLogProbMetric: 54.2212

Epoch 896: val_loss improved from 54.23336 to 54.22123, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 53.9807 - MinusLogProbMetric: 53.9807 - val_loss: 54.2212 - val_MinusLogProbMetric: 54.2212 - lr: 2.0576e-06 - 58s/epoch - 294ms/step
Epoch 897/1000
2023-10-31 06:16:20.232 
Epoch 897/1000 
	 loss: 53.9731, MinusLogProbMetric: 53.9731, val_loss: 54.2082, val_MinusLogProbMetric: 54.2082

Epoch 897: val_loss improved from 54.22123 to 54.20823, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 53.9731 - MinusLogProbMetric: 53.9731 - val_loss: 54.2082 - val_MinusLogProbMetric: 54.2082 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 898/1000
2023-10-31 06:17:20.704 
Epoch 898/1000 
	 loss: 53.9450, MinusLogProbMetric: 53.9450, val_loss: 54.1872, val_MinusLogProbMetric: 54.1872

Epoch 898: val_loss improved from 54.20823 to 54.18715, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 53.9450 - MinusLogProbMetric: 53.9450 - val_loss: 54.1872 - val_MinusLogProbMetric: 54.1872 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 899/1000
2023-10-31 06:18:18.020 
Epoch 899/1000 
	 loss: 54.0897, MinusLogProbMetric: 54.0897, val_loss: 54.2185, val_MinusLogProbMetric: 54.2185

Epoch 899: val_loss did not improve from 54.18715
196/196 - 56s - loss: 54.0897 - MinusLogProbMetric: 54.0897 - val_loss: 54.2185 - val_MinusLogProbMetric: 54.2185 - lr: 2.0576e-06 - 56s/epoch - 288ms/step
Epoch 900/1000
2023-10-31 06:19:17.488 
Epoch 900/1000 
	 loss: 53.9180, MinusLogProbMetric: 53.9180, val_loss: 54.1562, val_MinusLogProbMetric: 54.1562

Epoch 900: val_loss improved from 54.18715 to 54.15618, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 53.9180 - MinusLogProbMetric: 53.9180 - val_loss: 54.1562 - val_MinusLogProbMetric: 54.1562 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 901/1000
2023-10-31 06:20:18.145 
Epoch 901/1000 
	 loss: 53.8941, MinusLogProbMetric: 53.8941, val_loss: 54.1548, val_MinusLogProbMetric: 54.1548

Epoch 901: val_loss improved from 54.15618 to 54.15484, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 53.8941 - MinusLogProbMetric: 53.8941 - val_loss: 54.1548 - val_MinusLogProbMetric: 54.1548 - lr: 2.0576e-06 - 61s/epoch - 309ms/step
Epoch 902/1000
2023-10-31 06:21:20.258 
Epoch 902/1000 
	 loss: 53.8830, MinusLogProbMetric: 53.8830, val_loss: 54.1288, val_MinusLogProbMetric: 54.1288

Epoch 902: val_loss improved from 54.15484 to 54.12877, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 53.8830 - MinusLogProbMetric: 53.8830 - val_loss: 54.1288 - val_MinusLogProbMetric: 54.1288 - lr: 2.0576e-06 - 62s/epoch - 317ms/step
Epoch 903/1000
2023-10-31 06:22:20.096 
Epoch 903/1000 
	 loss: 53.8532, MinusLogProbMetric: 53.8532, val_loss: 54.1322, val_MinusLogProbMetric: 54.1322

Epoch 903: val_loss did not improve from 54.12877
196/196 - 59s - loss: 53.8532 - MinusLogProbMetric: 53.8532 - val_loss: 54.1322 - val_MinusLogProbMetric: 54.1322 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 904/1000
2023-10-31 06:23:20.263 
Epoch 904/1000 
	 loss: 53.8538, MinusLogProbMetric: 53.8538, val_loss: 54.1129, val_MinusLogProbMetric: 54.1129

Epoch 904: val_loss improved from 54.12877 to 54.11295, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 53.8538 - MinusLogProbMetric: 53.8538 - val_loss: 54.1129 - val_MinusLogProbMetric: 54.1129 - lr: 2.0576e-06 - 61s/epoch - 311ms/step
Epoch 905/1000
2023-10-31 06:24:21.778 
Epoch 905/1000 
	 loss: 54.2526, MinusLogProbMetric: 54.2526, val_loss: 54.6229, val_MinusLogProbMetric: 54.6229

Epoch 905: val_loss did not improve from 54.11295
196/196 - 61s - loss: 54.2526 - MinusLogProbMetric: 54.2526 - val_loss: 54.6229 - val_MinusLogProbMetric: 54.6229 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 906/1000
2023-10-31 06:25:22.231 
Epoch 906/1000 
	 loss: 53.8834, MinusLogProbMetric: 53.8834, val_loss: 54.0564, val_MinusLogProbMetric: 54.0564

Epoch 906: val_loss improved from 54.11295 to 54.05636, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 53.8834 - MinusLogProbMetric: 53.8834 - val_loss: 54.0564 - val_MinusLogProbMetric: 54.0564 - lr: 2.0576e-06 - 61s/epoch - 313ms/step
Epoch 907/1000
2023-10-31 06:26:23.763 
Epoch 907/1000 
	 loss: 53.8078, MinusLogProbMetric: 53.8078, val_loss: 54.0453, val_MinusLogProbMetric: 54.0453

Epoch 907: val_loss improved from 54.05636 to 54.04535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 53.8078 - MinusLogProbMetric: 53.8078 - val_loss: 54.0453 - val_MinusLogProbMetric: 54.0453 - lr: 2.0576e-06 - 61s/epoch - 314ms/step
Epoch 908/1000
2023-10-31 06:27:20.402 
Epoch 908/1000 
	 loss: 53.7899, MinusLogProbMetric: 53.7899, val_loss: 54.0416, val_MinusLogProbMetric: 54.0416

Epoch 908: val_loss improved from 54.04535 to 54.04156, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 53.7899 - MinusLogProbMetric: 53.7899 - val_loss: 54.0416 - val_MinusLogProbMetric: 54.0416 - lr: 2.0576e-06 - 56s/epoch - 288ms/step
Epoch 909/1000
2023-10-31 06:28:20.217 
Epoch 909/1000 
	 loss: 53.9015, MinusLogProbMetric: 53.9015, val_loss: 54.1153, val_MinusLogProbMetric: 54.1153

Epoch 909: val_loss did not improve from 54.04156
196/196 - 59s - loss: 53.9015 - MinusLogProbMetric: 53.9015 - val_loss: 54.1153 - val_MinusLogProbMetric: 54.1153 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 910/1000
2023-10-31 06:29:19.309 
Epoch 910/1000 
	 loss: 53.8118, MinusLogProbMetric: 53.8118, val_loss: 53.9823, val_MinusLogProbMetric: 53.9823

Epoch 910: val_loss improved from 54.04156 to 53.98229, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 53.8118 - MinusLogProbMetric: 53.8118 - val_loss: 53.9823 - val_MinusLogProbMetric: 53.9823 - lr: 2.0576e-06 - 60s/epoch - 306ms/step
Epoch 911/1000
2023-10-31 06:30:19.266 
Epoch 911/1000 
	 loss: 53.8693, MinusLogProbMetric: 53.8693, val_loss: 54.0432, val_MinusLogProbMetric: 54.0432

Epoch 911: val_loss did not improve from 53.98229
196/196 - 59s - loss: 53.8693 - MinusLogProbMetric: 53.8693 - val_loss: 54.0432 - val_MinusLogProbMetric: 54.0432 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 912/1000
2023-10-31 06:31:16.060 
Epoch 912/1000 
	 loss: 53.7792, MinusLogProbMetric: 53.7792, val_loss: 54.0059, val_MinusLogProbMetric: 54.0059

Epoch 912: val_loss did not improve from 53.98229
196/196 - 57s - loss: 53.7792 - MinusLogProbMetric: 53.7792 - val_loss: 54.0059 - val_MinusLogProbMetric: 54.0059 - lr: 2.0576e-06 - 57s/epoch - 290ms/step
Epoch 913/1000
2023-10-31 06:32:14.397 
Epoch 913/1000 
	 loss: 53.7419, MinusLogProbMetric: 53.7419, val_loss: 53.9671, val_MinusLogProbMetric: 53.9671

Epoch 913: val_loss improved from 53.98229 to 53.96714, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 53.7419 - MinusLogProbMetric: 53.7419 - val_loss: 53.9671 - val_MinusLogProbMetric: 53.9671 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 914/1000
2023-10-31 06:33:13.926 
Epoch 914/1000 
	 loss: 53.7155, MinusLogProbMetric: 53.7155, val_loss: 53.9540, val_MinusLogProbMetric: 53.9540

Epoch 914: val_loss improved from 53.96714 to 53.95397, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 53.7155 - MinusLogProbMetric: 53.7155 - val_loss: 53.9540 - val_MinusLogProbMetric: 53.9540 - lr: 2.0576e-06 - 60s/epoch - 304ms/step
Epoch 915/1000
2023-10-31 06:34:09.858 
Epoch 915/1000 
	 loss: 53.6935, MinusLogProbMetric: 53.6935, val_loss: 53.9494, val_MinusLogProbMetric: 53.9494

Epoch 915: val_loss improved from 53.95397 to 53.94943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 53.6935 - MinusLogProbMetric: 53.6935 - val_loss: 53.9494 - val_MinusLogProbMetric: 53.9494 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 916/1000
2023-10-31 06:35:07.924 
Epoch 916/1000 
	 loss: 53.6746, MinusLogProbMetric: 53.6746, val_loss: 53.9104, val_MinusLogProbMetric: 53.9104

Epoch 916: val_loss improved from 53.94943 to 53.91040, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 53.6746 - MinusLogProbMetric: 53.6746 - val_loss: 53.9104 - val_MinusLogProbMetric: 53.9104 - lr: 2.0576e-06 - 58s/epoch - 296ms/step
Epoch 917/1000
2023-10-31 06:36:08.845 
Epoch 917/1000 
	 loss: 53.6398, MinusLogProbMetric: 53.6398, val_loss: 53.8847, val_MinusLogProbMetric: 53.8847

Epoch 917: val_loss improved from 53.91040 to 53.88473, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 53.6398 - MinusLogProbMetric: 53.6398 - val_loss: 53.8847 - val_MinusLogProbMetric: 53.8847 - lr: 2.0576e-06 - 61s/epoch - 311ms/step
Epoch 918/1000
2023-10-31 06:37:08.040 
Epoch 918/1000 
	 loss: 53.8307, MinusLogProbMetric: 53.8307, val_loss: 53.8990, val_MinusLogProbMetric: 53.8990

Epoch 918: val_loss did not improve from 53.88473
196/196 - 58s - loss: 53.8307 - MinusLogProbMetric: 53.8307 - val_loss: 53.8990 - val_MinusLogProbMetric: 53.8990 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 919/1000
2023-10-31 06:38:02.717 
Epoch 919/1000 
	 loss: 53.6325, MinusLogProbMetric: 53.6325, val_loss: 53.8890, val_MinusLogProbMetric: 53.8890

Epoch 919: val_loss did not improve from 53.88473
196/196 - 55s - loss: 53.6325 - MinusLogProbMetric: 53.6325 - val_loss: 53.8890 - val_MinusLogProbMetric: 53.8890 - lr: 2.0576e-06 - 55s/epoch - 279ms/step
Epoch 920/1000
2023-10-31 06:38:59.176 
Epoch 920/1000 
	 loss: 53.5995, MinusLogProbMetric: 53.5995, val_loss: 53.8528, val_MinusLogProbMetric: 53.8528

Epoch 920: val_loss improved from 53.88473 to 53.85282, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 53.5995 - MinusLogProbMetric: 53.5995 - val_loss: 53.8528 - val_MinusLogProbMetric: 53.8528 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 921/1000
2023-10-31 06:39:58.045 
Epoch 921/1000 
	 loss: 53.5891, MinusLogProbMetric: 53.5891, val_loss: 53.8344, val_MinusLogProbMetric: 53.8344

Epoch 921: val_loss improved from 53.85282 to 53.83439, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 53.5891 - MinusLogProbMetric: 53.5891 - val_loss: 53.8344 - val_MinusLogProbMetric: 53.8344 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 922/1000
2023-10-31 06:40:53.546 
Epoch 922/1000 
	 loss: 53.5675, MinusLogProbMetric: 53.5675, val_loss: 53.8082, val_MinusLogProbMetric: 53.8082

Epoch 922: val_loss improved from 53.83439 to 53.80819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 53.5675 - MinusLogProbMetric: 53.5675 - val_loss: 53.8082 - val_MinusLogProbMetric: 53.8082 - lr: 2.0576e-06 - 55s/epoch - 283ms/step
Epoch 923/1000
2023-10-31 06:41:52.438 
Epoch 923/1000 
	 loss: 53.5640, MinusLogProbMetric: 53.5640, val_loss: 53.8076, val_MinusLogProbMetric: 53.8076

Epoch 923: val_loss improved from 53.80819 to 53.80757, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 53.5640 - MinusLogProbMetric: 53.5640 - val_loss: 53.8076 - val_MinusLogProbMetric: 53.8076 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 924/1000
2023-10-31 06:42:49.742 
Epoch 924/1000 
	 loss: 53.5549, MinusLogProbMetric: 53.5549, val_loss: 53.7919, val_MinusLogProbMetric: 53.7919

Epoch 924: val_loss improved from 53.80757 to 53.79186, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 53.5549 - MinusLogProbMetric: 53.5549 - val_loss: 53.7919 - val_MinusLogProbMetric: 53.7919 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 925/1000
2023-10-31 06:43:50.636 
Epoch 925/1000 
	 loss: 53.5347, MinusLogProbMetric: 53.5347, val_loss: 53.7986, val_MinusLogProbMetric: 53.7986

Epoch 925: val_loss did not improve from 53.79186
196/196 - 60s - loss: 53.5347 - MinusLogProbMetric: 53.5347 - val_loss: 53.7986 - val_MinusLogProbMetric: 53.7986 - lr: 2.0576e-06 - 60s/epoch - 306ms/step
Epoch 926/1000
2023-10-31 06:44:45.824 
Epoch 926/1000 
	 loss: 53.5165, MinusLogProbMetric: 53.5165, val_loss: 53.7563, val_MinusLogProbMetric: 53.7563

Epoch 926: val_loss improved from 53.79186 to 53.75627, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 53.5165 - MinusLogProbMetric: 53.5165 - val_loss: 53.7563 - val_MinusLogProbMetric: 53.7563 - lr: 2.0576e-06 - 56s/epoch - 286ms/step
Epoch 927/1000
2023-10-31 06:45:39.376 
Epoch 927/1000 
	 loss: 53.5032, MinusLogProbMetric: 53.5032, val_loss: 53.7442, val_MinusLogProbMetric: 53.7442

Epoch 927: val_loss improved from 53.75627 to 53.74422, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 53.5032 - MinusLogProbMetric: 53.5032 - val_loss: 53.7442 - val_MinusLogProbMetric: 53.7442 - lr: 2.0576e-06 - 54s/epoch - 273ms/step
Epoch 928/1000
2023-10-31 06:46:34.372 
Epoch 928/1000 
	 loss: 53.4975, MinusLogProbMetric: 53.4975, val_loss: 53.7214, val_MinusLogProbMetric: 53.7214

Epoch 928: val_loss improved from 53.74422 to 53.72145, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 53.4975 - MinusLogProbMetric: 53.4975 - val_loss: 53.7214 - val_MinusLogProbMetric: 53.7214 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 929/1000
2023-10-31 06:47:32.222 
Epoch 929/1000 
	 loss: 53.4745, MinusLogProbMetric: 53.4745, val_loss: 53.7346, val_MinusLogProbMetric: 53.7346

Epoch 929: val_loss did not improve from 53.72145
196/196 - 57s - loss: 53.4745 - MinusLogProbMetric: 53.4745 - val_loss: 53.7346 - val_MinusLogProbMetric: 53.7346 - lr: 2.0576e-06 - 57s/epoch - 291ms/step
Epoch 930/1000
2023-10-31 06:48:26.559 
Epoch 930/1000 
	 loss: 53.4587, MinusLogProbMetric: 53.4587, val_loss: 53.6829, val_MinusLogProbMetric: 53.6829

Epoch 930: val_loss improved from 53.72145 to 53.68293, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 53.4587 - MinusLogProbMetric: 53.4587 - val_loss: 53.6829 - val_MinusLogProbMetric: 53.6829 - lr: 2.0576e-06 - 55s/epoch - 282ms/step
Epoch 931/1000
2023-10-31 06:49:22.256 
Epoch 931/1000 
	 loss: 53.4567, MinusLogProbMetric: 53.4567, val_loss: 53.6943, val_MinusLogProbMetric: 53.6943

Epoch 931: val_loss did not improve from 53.68293
196/196 - 55s - loss: 53.4567 - MinusLogProbMetric: 53.4567 - val_loss: 53.6943 - val_MinusLogProbMetric: 53.6943 - lr: 2.0576e-06 - 55s/epoch - 280ms/step
Epoch 932/1000
2023-10-31 06:50:16.922 
Epoch 932/1000 
	 loss: 53.4289, MinusLogProbMetric: 53.4289, val_loss: 53.6974, val_MinusLogProbMetric: 53.6974

Epoch 932: val_loss did not improve from 53.68293
196/196 - 55s - loss: 53.4289 - MinusLogProbMetric: 53.4289 - val_loss: 53.6974 - val_MinusLogProbMetric: 53.6974 - lr: 2.0576e-06 - 55s/epoch - 279ms/step
Epoch 933/1000
2023-10-31 06:51:14.335 
Epoch 933/1000 
	 loss: 53.4209, MinusLogProbMetric: 53.4209, val_loss: 53.6891, val_MinusLogProbMetric: 53.6891

Epoch 933: val_loss did not improve from 53.68293
196/196 - 57s - loss: 53.4209 - MinusLogProbMetric: 53.4209 - val_loss: 53.6891 - val_MinusLogProbMetric: 53.6891 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 934/1000
2023-10-31 06:52:12.044 
Epoch 934/1000 
	 loss: 53.4046, MinusLogProbMetric: 53.4046, val_loss: 53.6782, val_MinusLogProbMetric: 53.6782

Epoch 934: val_loss improved from 53.68293 to 53.67822, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 53.4046 - MinusLogProbMetric: 53.4046 - val_loss: 53.6782 - val_MinusLogProbMetric: 53.6782 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 935/1000
2023-10-31 06:53:07.733 
Epoch 935/1000 
	 loss: 53.4485, MinusLogProbMetric: 53.4485, val_loss: 53.6383, val_MinusLogProbMetric: 53.6383

Epoch 935: val_loss improved from 53.67822 to 53.63832, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 56s - loss: 53.4485 - MinusLogProbMetric: 53.4485 - val_loss: 53.6383 - val_MinusLogProbMetric: 53.6383 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 936/1000
2023-10-31 06:54:07.232 
Epoch 936/1000 
	 loss: 53.3790, MinusLogProbMetric: 53.3790, val_loss: 53.6557, val_MinusLogProbMetric: 53.6557

Epoch 936: val_loss did not improve from 53.63832
196/196 - 59s - loss: 53.3790 - MinusLogProbMetric: 53.3790 - val_loss: 53.6557 - val_MinusLogProbMetric: 53.6557 - lr: 2.0576e-06 - 59s/epoch - 299ms/step
Epoch 937/1000
2023-10-31 06:55:08.666 
Epoch 937/1000 
	 loss: 53.3749, MinusLogProbMetric: 53.3749, val_loss: 53.6590, val_MinusLogProbMetric: 53.6590

Epoch 937: val_loss did not improve from 53.63832
196/196 - 61s - loss: 53.3749 - MinusLogProbMetric: 53.3749 - val_loss: 53.6590 - val_MinusLogProbMetric: 53.6590 - lr: 2.0576e-06 - 61s/epoch - 313ms/step
Epoch 938/1000
2023-10-31 06:56:08.760 
Epoch 938/1000 
	 loss: 53.3929, MinusLogProbMetric: 53.3929, val_loss: 53.6152, val_MinusLogProbMetric: 53.6152

Epoch 938: val_loss improved from 53.63832 to 53.61525, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 53.3929 - MinusLogProbMetric: 53.3929 - val_loss: 53.6152 - val_MinusLogProbMetric: 53.6152 - lr: 2.0576e-06 - 61s/epoch - 311ms/step
Epoch 939/1000
2023-10-31 06:57:07.848 
Epoch 939/1000 
	 loss: 53.3469, MinusLogProbMetric: 53.3469, val_loss: 53.6049, val_MinusLogProbMetric: 53.6049

Epoch 939: val_loss improved from 53.61525 to 53.60487, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 53.3469 - MinusLogProbMetric: 53.3469 - val_loss: 53.6049 - val_MinusLogProbMetric: 53.6049 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 940/1000
2023-10-31 06:58:07.487 
Epoch 940/1000 
	 loss: 53.3215, MinusLogProbMetric: 53.3215, val_loss: 53.6070, val_MinusLogProbMetric: 53.6070

Epoch 940: val_loss did not improve from 53.60487
196/196 - 59s - loss: 53.3215 - MinusLogProbMetric: 53.3215 - val_loss: 53.6070 - val_MinusLogProbMetric: 53.6070 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 941/1000
2023-10-31 06:59:08.807 
Epoch 941/1000 
	 loss: 53.3136, MinusLogProbMetric: 53.3136, val_loss: 53.5788, val_MinusLogProbMetric: 53.5788

Epoch 941: val_loss improved from 53.60487 to 53.57883, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 53.3136 - MinusLogProbMetric: 53.3136 - val_loss: 53.5788 - val_MinusLogProbMetric: 53.5788 - lr: 2.0576e-06 - 62s/epoch - 317ms/step
Epoch 942/1000
2023-10-31 07:00:09.606 
Epoch 942/1000 
	 loss: 53.2982, MinusLogProbMetric: 53.2982, val_loss: 53.5614, val_MinusLogProbMetric: 53.5614

Epoch 942: val_loss improved from 53.57883 to 53.56136, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 53.2982 - MinusLogProbMetric: 53.2982 - val_loss: 53.5614 - val_MinusLogProbMetric: 53.5614 - lr: 2.0576e-06 - 61s/epoch - 310ms/step
Epoch 943/1000
2023-10-31 07:01:04.035 
Epoch 943/1000 
	 loss: 53.2857, MinusLogProbMetric: 53.2857, val_loss: 53.5425, val_MinusLogProbMetric: 53.5425

Epoch 943: val_loss improved from 53.56136 to 53.54252, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 54s - loss: 53.2857 - MinusLogProbMetric: 53.2857 - val_loss: 53.5425 - val_MinusLogProbMetric: 53.5425 - lr: 2.0576e-06 - 54s/epoch - 278ms/step
Epoch 944/1000
2023-10-31 07:02:03.055 
Epoch 944/1000 
	 loss: 53.2783, MinusLogProbMetric: 53.2783, val_loss: 53.5379, val_MinusLogProbMetric: 53.5379

Epoch 944: val_loss improved from 53.54252 to 53.53791, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 53.2783 - MinusLogProbMetric: 53.2783 - val_loss: 53.5379 - val_MinusLogProbMetric: 53.5379 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 945/1000
2023-10-31 07:03:04.731 
Epoch 945/1000 
	 loss: 53.2754, MinusLogProbMetric: 53.2754, val_loss: 53.5188, val_MinusLogProbMetric: 53.5188

Epoch 945: val_loss improved from 53.53791 to 53.51879, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 53.2754 - MinusLogProbMetric: 53.2754 - val_loss: 53.5188 - val_MinusLogProbMetric: 53.5188 - lr: 2.0576e-06 - 62s/epoch - 315ms/step
Epoch 946/1000
2023-10-31 07:04:03.591 
Epoch 946/1000 
	 loss: 53.2586, MinusLogProbMetric: 53.2586, val_loss: 53.5060, val_MinusLogProbMetric: 53.5060

Epoch 946: val_loss improved from 53.51879 to 53.50599, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 53.2586 - MinusLogProbMetric: 53.2586 - val_loss: 53.5060 - val_MinusLogProbMetric: 53.5060 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 947/1000
2023-10-31 07:05:01.981 
Epoch 947/1000 
	 loss: 53.2371, MinusLogProbMetric: 53.2371, val_loss: 53.5138, val_MinusLogProbMetric: 53.5138

Epoch 947: val_loss did not improve from 53.50599
196/196 - 57s - loss: 53.2371 - MinusLogProbMetric: 53.2371 - val_loss: 53.5138 - val_MinusLogProbMetric: 53.5138 - lr: 2.0576e-06 - 57s/epoch - 293ms/step
Epoch 948/1000
2023-10-31 07:06:00.765 
Epoch 948/1000 
	 loss: 53.2244, MinusLogProbMetric: 53.2244, val_loss: 53.4773, val_MinusLogProbMetric: 53.4773

Epoch 948: val_loss improved from 53.50599 to 53.47727, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 53.2244 - MinusLogProbMetric: 53.2244 - val_loss: 53.4773 - val_MinusLogProbMetric: 53.4773 - lr: 2.0576e-06 - 60s/epoch - 304ms/step
Epoch 949/1000
2023-10-31 07:07:01.256 
Epoch 949/1000 
	 loss: 53.2134, MinusLogProbMetric: 53.2134, val_loss: 53.4846, val_MinusLogProbMetric: 53.4846

Epoch 949: val_loss did not improve from 53.47727
196/196 - 60s - loss: 53.2134 - MinusLogProbMetric: 53.2134 - val_loss: 53.4846 - val_MinusLogProbMetric: 53.4846 - lr: 2.0576e-06 - 60s/epoch - 304ms/step
Epoch 950/1000
2023-10-31 07:08:02.488 
Epoch 950/1000 
	 loss: 53.1954, MinusLogProbMetric: 53.1954, val_loss: 53.4800, val_MinusLogProbMetric: 53.4800

Epoch 950: val_loss did not improve from 53.47727
196/196 - 61s - loss: 53.1954 - MinusLogProbMetric: 53.1954 - val_loss: 53.4800 - val_MinusLogProbMetric: 53.4800 - lr: 2.0576e-06 - 61s/epoch - 312ms/step
Epoch 951/1000
2023-10-31 07:09:03.993 
Epoch 951/1000 
	 loss: 53.1841, MinusLogProbMetric: 53.1841, val_loss: 53.4567, val_MinusLogProbMetric: 53.4567

Epoch 951: val_loss improved from 53.47727 to 53.45669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 62s - loss: 53.1841 - MinusLogProbMetric: 53.1841 - val_loss: 53.4567 - val_MinusLogProbMetric: 53.4567 - lr: 2.0576e-06 - 62s/epoch - 318ms/step
Epoch 952/1000
2023-10-31 07:10:03.801 
Epoch 952/1000 
	 loss: 53.1715, MinusLogProbMetric: 53.1715, val_loss: 53.4295, val_MinusLogProbMetric: 53.4295

Epoch 952: val_loss improved from 53.45669 to 53.42951, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 53.1715 - MinusLogProbMetric: 53.1715 - val_loss: 53.4295 - val_MinusLogProbMetric: 53.4295 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 953/1000
2023-10-31 07:11:04.004 
Epoch 953/1000 
	 loss: 53.1538, MinusLogProbMetric: 53.1538, val_loss: 53.4284, val_MinusLogProbMetric: 53.4284

Epoch 953: val_loss improved from 53.42951 to 53.42844, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 53.1538 - MinusLogProbMetric: 53.1538 - val_loss: 53.4284 - val_MinusLogProbMetric: 53.4284 - lr: 2.0576e-06 - 60s/epoch - 307ms/step
Epoch 954/1000
2023-10-31 07:12:03.248 
Epoch 954/1000 
	 loss: 53.2058, MinusLogProbMetric: 53.2058, val_loss: 53.4215, val_MinusLogProbMetric: 53.4215

Epoch 954: val_loss improved from 53.42844 to 53.42146, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 53.2058 - MinusLogProbMetric: 53.2058 - val_loss: 53.4215 - val_MinusLogProbMetric: 53.4215 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 955/1000
2023-10-31 07:13:01.170 
Epoch 955/1000 
	 loss: 53.1335, MinusLogProbMetric: 53.1335, val_loss: 53.4127, val_MinusLogProbMetric: 53.4127

Epoch 955: val_loss improved from 53.42146 to 53.41272, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 53.1335 - MinusLogProbMetric: 53.1335 - val_loss: 53.4127 - val_MinusLogProbMetric: 53.4127 - lr: 2.0576e-06 - 58s/epoch - 295ms/step
Epoch 956/1000
2023-10-31 07:13:59.497 
Epoch 956/1000 
	 loss: 53.1314, MinusLogProbMetric: 53.1314, val_loss: 53.4020, val_MinusLogProbMetric: 53.4020

Epoch 956: val_loss improved from 53.41272 to 53.40197, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 53.1314 - MinusLogProbMetric: 53.1314 - val_loss: 53.4020 - val_MinusLogProbMetric: 53.4020 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 957/1000
2023-10-31 07:14:56.724 
Epoch 957/1000 
	 loss: 53.1162, MinusLogProbMetric: 53.1162, val_loss: 53.3781, val_MinusLogProbMetric: 53.3781

Epoch 957: val_loss improved from 53.40197 to 53.37811, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 53.1162 - MinusLogProbMetric: 53.1162 - val_loss: 53.3781 - val_MinusLogProbMetric: 53.3781 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 958/1000
2023-10-31 07:15:51.504 
Epoch 958/1000 
	 loss: 53.0973, MinusLogProbMetric: 53.0973, val_loss: 53.4213, val_MinusLogProbMetric: 53.4213

Epoch 958: val_loss did not improve from 53.37811
196/196 - 54s - loss: 53.0973 - MinusLogProbMetric: 53.0973 - val_loss: 53.4213 - val_MinusLogProbMetric: 53.4213 - lr: 2.0576e-06 - 54s/epoch - 276ms/step
Epoch 959/1000
2023-10-31 07:16:46.617 
Epoch 959/1000 
	 loss: 53.1429, MinusLogProbMetric: 53.1429, val_loss: 53.3838, val_MinusLogProbMetric: 53.3838

Epoch 959: val_loss did not improve from 53.37811
196/196 - 55s - loss: 53.1429 - MinusLogProbMetric: 53.1429 - val_loss: 53.3838 - val_MinusLogProbMetric: 53.3838 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 960/1000
2023-10-31 07:17:45.101 
Epoch 960/1000 
	 loss: 53.0836, MinusLogProbMetric: 53.0836, val_loss: 53.3639, val_MinusLogProbMetric: 53.3639

Epoch 960: val_loss improved from 53.37811 to 53.36394, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 53.0836 - MinusLogProbMetric: 53.0836 - val_loss: 53.3639 - val_MinusLogProbMetric: 53.3639 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 961/1000
2023-10-31 07:18:44.933 
Epoch 961/1000 
	 loss: 53.0725, MinusLogProbMetric: 53.0725, val_loss: 53.3361, val_MinusLogProbMetric: 53.3361

Epoch 961: val_loss improved from 53.36394 to 53.33610, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 53.0725 - MinusLogProbMetric: 53.0725 - val_loss: 53.3361 - val_MinusLogProbMetric: 53.3361 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 962/1000
2023-10-31 07:19:43.153 
Epoch 962/1000 
	 loss: 53.0465, MinusLogProbMetric: 53.0465, val_loss: 53.3312, val_MinusLogProbMetric: 53.3312

Epoch 962: val_loss improved from 53.33610 to 53.33115, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 53.0465 - MinusLogProbMetric: 53.0465 - val_loss: 53.3312 - val_MinusLogProbMetric: 53.3312 - lr: 2.0576e-06 - 58s/epoch - 297ms/step
Epoch 963/1000
2023-10-31 07:20:43.688 
Epoch 963/1000 
	 loss: 53.0491, MinusLogProbMetric: 53.0491, val_loss: 53.2995, val_MinusLogProbMetric: 53.2995

Epoch 963: val_loss improved from 53.33115 to 53.29952, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 53.0491 - MinusLogProbMetric: 53.0491 - val_loss: 53.2995 - val_MinusLogProbMetric: 53.2995 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 964/1000
2023-10-31 07:21:40.522 
Epoch 964/1000 
	 loss: 53.0236, MinusLogProbMetric: 53.0236, val_loss: 53.2870, val_MinusLogProbMetric: 53.2870

Epoch 964: val_loss improved from 53.29952 to 53.28704, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 53.0236 - MinusLogProbMetric: 53.0236 - val_loss: 53.2870 - val_MinusLogProbMetric: 53.2870 - lr: 2.0576e-06 - 57s/epoch - 290ms/step
Epoch 965/1000
2023-10-31 07:22:42.682 
Epoch 965/1000 
	 loss: 53.0207, MinusLogProbMetric: 53.0207, val_loss: 53.2893, val_MinusLogProbMetric: 53.2893

Epoch 965: val_loss did not improve from 53.28704
196/196 - 61s - loss: 53.0207 - MinusLogProbMetric: 53.0207 - val_loss: 53.2893 - val_MinusLogProbMetric: 53.2893 - lr: 2.0576e-06 - 61s/epoch - 313ms/step
Epoch 966/1000
2023-10-31 07:23:41.779 
Epoch 966/1000 
	 loss: 53.0169, MinusLogProbMetric: 53.0169, val_loss: 53.3651, val_MinusLogProbMetric: 53.3651

Epoch 966: val_loss did not improve from 53.28704
196/196 - 59s - loss: 53.0169 - MinusLogProbMetric: 53.0169 - val_loss: 53.3651 - val_MinusLogProbMetric: 53.3651 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 967/1000
2023-10-31 07:24:41.429 
Epoch 967/1000 
	 loss: 52.9910, MinusLogProbMetric: 52.9910, val_loss: 53.2551, val_MinusLogProbMetric: 53.2551

Epoch 967: val_loss improved from 53.28704 to 53.25511, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 52.9910 - MinusLogProbMetric: 52.9910 - val_loss: 53.2551 - val_MinusLogProbMetric: 53.2551 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 968/1000
2023-10-31 07:25:42.294 
Epoch 968/1000 
	 loss: 53.0681, MinusLogProbMetric: 53.0681, val_loss: 53.2411, val_MinusLogProbMetric: 53.2411

Epoch 968: val_loss improved from 53.25511 to 53.24110, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 53.0681 - MinusLogProbMetric: 53.0681 - val_loss: 53.2411 - val_MinusLogProbMetric: 53.2411 - lr: 2.0576e-06 - 61s/epoch - 311ms/step
Epoch 969/1000
2023-10-31 07:26:42.604 
Epoch 969/1000 
	 loss: 52.9833, MinusLogProbMetric: 52.9833, val_loss: 53.2239, val_MinusLogProbMetric: 53.2239

Epoch 969: val_loss improved from 53.24110 to 53.22390, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 52.9833 - MinusLogProbMetric: 52.9833 - val_loss: 53.2239 - val_MinusLogProbMetric: 53.2239 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 970/1000
2023-10-31 07:27:43.165 
Epoch 970/1000 
	 loss: 52.9495, MinusLogProbMetric: 52.9495, val_loss: 53.2273, val_MinusLogProbMetric: 53.2273

Epoch 970: val_loss did not improve from 53.22390
196/196 - 60s - loss: 52.9495 - MinusLogProbMetric: 52.9495 - val_loss: 53.2273 - val_MinusLogProbMetric: 53.2273 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 971/1000
2023-10-31 07:28:39.007 
Epoch 971/1000 
	 loss: 52.9383, MinusLogProbMetric: 52.9383, val_loss: 53.1991, val_MinusLogProbMetric: 53.1991

Epoch 971: val_loss improved from 53.22390 to 53.19914, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 57s - loss: 52.9383 - MinusLogProbMetric: 52.9383 - val_loss: 53.1991 - val_MinusLogProbMetric: 53.1991 - lr: 2.0576e-06 - 57s/epoch - 289ms/step
Epoch 972/1000
2023-10-31 07:29:38.396 
Epoch 972/1000 
	 loss: 52.9146, MinusLogProbMetric: 52.9146, val_loss: 53.1868, val_MinusLogProbMetric: 53.1868

Epoch 972: val_loss improved from 53.19914 to 53.18683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 52.9146 - MinusLogProbMetric: 52.9146 - val_loss: 53.1868 - val_MinusLogProbMetric: 53.1868 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 973/1000
2023-10-31 07:30:38.121 
Epoch 973/1000 
	 loss: 53.0628, MinusLogProbMetric: 53.0628, val_loss: 53.1956, val_MinusLogProbMetric: 53.1956

Epoch 973: val_loss did not improve from 53.18683
196/196 - 59s - loss: 53.0628 - MinusLogProbMetric: 53.0628 - val_loss: 53.1956 - val_MinusLogProbMetric: 53.1956 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 974/1000
2023-10-31 07:31:38.612 
Epoch 974/1000 
	 loss: 52.9234, MinusLogProbMetric: 52.9234, val_loss: 53.1757, val_MinusLogProbMetric: 53.1757

Epoch 974: val_loss improved from 53.18683 to 53.17568, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 52.9234 - MinusLogProbMetric: 52.9234 - val_loss: 53.1757 - val_MinusLogProbMetric: 53.1757 - lr: 2.0576e-06 - 61s/epoch - 313ms/step
Epoch 975/1000
2023-10-31 07:32:34.930 
Epoch 975/1000 
	 loss: 52.9429, MinusLogProbMetric: 52.9429, val_loss: 53.2304, val_MinusLogProbMetric: 53.2304

Epoch 975: val_loss did not improve from 53.17568
196/196 - 55s - loss: 52.9429 - MinusLogProbMetric: 52.9429 - val_loss: 53.2304 - val_MinusLogProbMetric: 53.2304 - lr: 2.0576e-06 - 55s/epoch - 283ms/step
Epoch 976/1000
2023-10-31 07:33:33.302 
Epoch 976/1000 
	 loss: 52.8796, MinusLogProbMetric: 52.8796, val_loss: 53.1361, val_MinusLogProbMetric: 53.1361

Epoch 976: val_loss improved from 53.17568 to 53.13612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 59s - loss: 52.8796 - MinusLogProbMetric: 52.8796 - val_loss: 53.1361 - val_MinusLogProbMetric: 53.1361 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 977/1000
2023-10-31 07:34:31.721 
Epoch 977/1000 
	 loss: 52.8636, MinusLogProbMetric: 52.8636, val_loss: 53.1223, val_MinusLogProbMetric: 53.1223

Epoch 977: val_loss improved from 53.13612 to 53.12235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 58s - loss: 52.8636 - MinusLogProbMetric: 52.8636 - val_loss: 53.1223 - val_MinusLogProbMetric: 53.1223 - lr: 2.0576e-06 - 58s/epoch - 298ms/step
Epoch 978/1000
2023-10-31 07:35:28.386 
Epoch 978/1000 
	 loss: 52.8474, MinusLogProbMetric: 52.8474, val_loss: 53.1276, val_MinusLogProbMetric: 53.1276

Epoch 978: val_loss did not improve from 53.12235
196/196 - 56s - loss: 52.8474 - MinusLogProbMetric: 52.8474 - val_loss: 53.1276 - val_MinusLogProbMetric: 53.1276 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 979/1000
2023-10-31 07:36:28.632 
Epoch 979/1000 
	 loss: 52.8469, MinusLogProbMetric: 52.8469, val_loss: 53.1177, val_MinusLogProbMetric: 53.1177

Epoch 979: val_loss improved from 53.12235 to 53.11765, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 61s - loss: 52.8469 - MinusLogProbMetric: 52.8469 - val_loss: 53.1177 - val_MinusLogProbMetric: 53.1177 - lr: 2.0576e-06 - 61s/epoch - 312ms/step
Epoch 980/1000
2023-10-31 07:37:28.267 
Epoch 980/1000 
	 loss: 52.8462, MinusLogProbMetric: 52.8462, val_loss: 53.1322, val_MinusLogProbMetric: 53.1322

Epoch 980: val_loss did not improve from 53.11765
196/196 - 59s - loss: 52.8462 - MinusLogProbMetric: 52.8462 - val_loss: 53.1322 - val_MinusLogProbMetric: 53.1322 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 981/1000
2023-10-31 07:38:26.992 
Epoch 981/1000 
	 loss: 52.8150, MinusLogProbMetric: 52.8150, val_loss: 53.0818, val_MinusLogProbMetric: 53.0818

Epoch 981: val_loss improved from 53.11765 to 53.08182, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 60s - loss: 52.8150 - MinusLogProbMetric: 52.8150 - val_loss: 53.0818 - val_MinusLogProbMetric: 53.0818 - lr: 2.0576e-06 - 60s/epoch - 304ms/step
Epoch 982/1000
2023-10-31 07:39:22.048 
Epoch 982/1000 
	 loss: 52.7946, MinusLogProbMetric: 52.7946, val_loss: 53.0625, val_MinusLogProbMetric: 53.0625

Epoch 982: val_loss improved from 53.08182 to 53.06246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_366/weights/best_weights.h5
196/196 - 55s - loss: 52.7946 - MinusLogProbMetric: 52.7946 - val_loss: 53.0625 - val_MinusLogProbMetric: 53.0625 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 983/1000
2023-10-31 07:40:18.667 
Epoch 983/1000 
	 loss: 52.8115, MinusLogProbMetric: 52.8115, val_loss: 53.0766, val_MinusLogProbMetric: 53.0766

Epoch 983: val_loss did not improve from 53.06246
196/196 - 56s - loss: 52.8115 - MinusLogProbMetric: 52.8115 - val_loss: 53.0766 - val_MinusLogProbMetric: 53.0766 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 984/1000
2023-10-31 07:41:14.989 
Epoch 984/1000 
	 loss: 70.6695, MinusLogProbMetric: 70.6695, val_loss: 58.1939, val_MinusLogProbMetric: 58.1939

Epoch 984: val_loss did not improve from 53.06246
196/196 - 56s - loss: 70.6695 - MinusLogProbMetric: 70.6695 - val_loss: 58.1939 - val_MinusLogProbMetric: 58.1939 - lr: 2.0576e-06 - 56s/epoch - 287ms/step
Epoch 985/1000
2023-10-31 07:42:12.232 
Epoch 985/1000 
	 loss: 56.5595, MinusLogProbMetric: 56.5595, val_loss: 56.7579, val_MinusLogProbMetric: 56.7579

Epoch 985: val_loss did not improve from 53.06246
196/196 - 57s - loss: 56.5595 - MinusLogProbMetric: 56.5595 - val_loss: 56.7579 - val_MinusLogProbMetric: 56.7579 - lr: 2.0576e-06 - 57s/epoch - 292ms/step
Epoch 986/1000
2023-10-31 07:43:11.312 
Epoch 986/1000 
	 loss: 55.7688, MinusLogProbMetric: 55.7688, val_loss: 55.9539, val_MinusLogProbMetric: 55.9539

Epoch 986: val_loss did not improve from 53.06246
196/196 - 59s - loss: 55.7688 - MinusLogProbMetric: 55.7688 - val_loss: 55.9539 - val_MinusLogProbMetric: 55.9539 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 987/1000
2023-10-31 07:44:11.683 
Epoch 987/1000 
	 loss: 55.5369, MinusLogProbMetric: 55.5369, val_loss: 55.6290, val_MinusLogProbMetric: 55.6290

Epoch 987: val_loss did not improve from 53.06246
196/196 - 60s - loss: 55.5369 - MinusLogProbMetric: 55.5369 - val_loss: 55.6290 - val_MinusLogProbMetric: 55.6290 - lr: 2.0576e-06 - 60s/epoch - 308ms/step
Epoch 988/1000
2023-10-31 07:45:11.542 
Epoch 988/1000 
	 loss: 55.3741, MinusLogProbMetric: 55.3741, val_loss: 55.4748, val_MinusLogProbMetric: 55.4748

Epoch 988: val_loss did not improve from 53.06246
196/196 - 60s - loss: 55.3741 - MinusLogProbMetric: 55.3741 - val_loss: 55.4748 - val_MinusLogProbMetric: 55.4748 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 989/1000
2023-10-31 07:46:10.968 
Epoch 989/1000 
	 loss: 55.2940, MinusLogProbMetric: 55.2940, val_loss: 55.4160, val_MinusLogProbMetric: 55.4160

Epoch 989: val_loss did not improve from 53.06246
196/196 - 59s - loss: 55.2940 - MinusLogProbMetric: 55.2940 - val_loss: 55.4160 - val_MinusLogProbMetric: 55.4160 - lr: 2.0576e-06 - 59s/epoch - 303ms/step
Epoch 990/1000
2023-10-31 07:47:06.921 
Epoch 990/1000 
	 loss: 55.1887, MinusLogProbMetric: 55.1887, val_loss: 55.3816, val_MinusLogProbMetric: 55.3816

Epoch 990: val_loss did not improve from 53.06246
196/196 - 56s - loss: 55.1887 - MinusLogProbMetric: 55.1887 - val_loss: 55.3816 - val_MinusLogProbMetric: 55.3816 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Epoch 991/1000
2023-10-31 07:48:03.954 
Epoch 991/1000 
	 loss: 55.1996, MinusLogProbMetric: 55.1996, val_loss: 55.2525, val_MinusLogProbMetric: 55.2525

Epoch 991: val_loss did not improve from 53.06246
196/196 - 57s - loss: 55.1996 - MinusLogProbMetric: 55.1996 - val_loss: 55.2525 - val_MinusLogProbMetric: 55.2525 - lr: 2.0576e-06 - 57s/epoch - 291ms/step
Epoch 992/1000
2023-10-31 07:48:59.600 
Epoch 992/1000 
	 loss: 55.0496, MinusLogProbMetric: 55.0496, val_loss: 55.2459, val_MinusLogProbMetric: 55.2459

Epoch 992: val_loss did not improve from 53.06246
196/196 - 56s - loss: 55.0496 - MinusLogProbMetric: 55.0496 - val_loss: 55.2459 - val_MinusLogProbMetric: 55.2459 - lr: 2.0576e-06 - 56s/epoch - 284ms/step
Epoch 993/1000
2023-10-31 07:49:59.318 
Epoch 993/1000 
	 loss: 55.0117, MinusLogProbMetric: 55.0117, val_loss: 55.1553, val_MinusLogProbMetric: 55.1553

Epoch 993: val_loss did not improve from 53.06246
196/196 - 60s - loss: 55.0117 - MinusLogProbMetric: 55.0117 - val_loss: 55.1553 - val_MinusLogProbMetric: 55.1553 - lr: 2.0576e-06 - 60s/epoch - 305ms/step
Epoch 994/1000
2023-10-31 07:50:58.063 
Epoch 994/1000 
	 loss: 54.9303, MinusLogProbMetric: 54.9303, val_loss: 55.1193, val_MinusLogProbMetric: 55.1193

Epoch 994: val_loss did not improve from 53.06246
196/196 - 59s - loss: 54.9303 - MinusLogProbMetric: 54.9303 - val_loss: 55.1193 - val_MinusLogProbMetric: 55.1193 - lr: 2.0576e-06 - 59s/epoch - 300ms/step
Epoch 995/1000
2023-10-31 07:51:57.164 
Epoch 995/1000 
	 loss: 54.8969, MinusLogProbMetric: 54.8969, val_loss: 55.1020, val_MinusLogProbMetric: 55.1020

Epoch 995: val_loss did not improve from 53.06246
196/196 - 59s - loss: 54.8969 - MinusLogProbMetric: 54.8969 - val_loss: 55.1020 - val_MinusLogProbMetric: 55.1020 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 996/1000
2023-10-31 07:52:56.101 
Epoch 996/1000 
	 loss: 54.8378, MinusLogProbMetric: 54.8378, val_loss: 54.9931, val_MinusLogProbMetric: 54.9931

Epoch 996: val_loss did not improve from 53.06246
196/196 - 59s - loss: 54.8378 - MinusLogProbMetric: 54.8378 - val_loss: 54.9931 - val_MinusLogProbMetric: 54.9931 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 997/1000
2023-10-31 07:53:55.011 
Epoch 997/1000 
	 loss: 54.8006, MinusLogProbMetric: 54.8006, val_loss: 54.9621, val_MinusLogProbMetric: 54.9621

Epoch 997: val_loss did not improve from 53.06246
196/196 - 59s - loss: 54.8006 - MinusLogProbMetric: 54.8006 - val_loss: 54.9621 - val_MinusLogProbMetric: 54.9621 - lr: 2.0576e-06 - 59s/epoch - 301ms/step
Epoch 998/1000
2023-10-31 07:54:50.180 
Epoch 998/1000 
	 loss: 54.7450, MinusLogProbMetric: 54.7450, val_loss: 54.9481, val_MinusLogProbMetric: 54.9481

Epoch 998: val_loss did not improve from 53.06246
196/196 - 55s - loss: 54.7450 - MinusLogProbMetric: 54.7450 - val_loss: 54.9481 - val_MinusLogProbMetric: 54.9481 - lr: 2.0576e-06 - 55s/epoch - 281ms/step
Epoch 999/1000
2023-10-31 07:55:49.457 
Epoch 999/1000 
	 loss: 54.7172, MinusLogProbMetric: 54.7172, val_loss: 54.8852, val_MinusLogProbMetric: 54.8852

Epoch 999: val_loss did not improve from 53.06246
196/196 - 59s - loss: 54.7172 - MinusLogProbMetric: 54.7172 - val_loss: 54.8852 - val_MinusLogProbMetric: 54.8852 - lr: 2.0576e-06 - 59s/epoch - 302ms/step
Epoch 1000/1000
2023-10-31 07:56:45.343 
Epoch 1000/1000 
	 loss: 54.6586, MinusLogProbMetric: 54.6586, val_loss: 54.8551, val_MinusLogProbMetric: 54.8551

Epoch 1000: val_loss did not improve from 53.06246
196/196 - 56s - loss: 54.6586 - MinusLogProbMetric: 54.6586 - val_loss: 54.8551 - val_MinusLogProbMetric: 54.8551 - lr: 2.0576e-06 - 56s/epoch - 285ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 541.
Model trained in 58114.82 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.07 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.57 s.
===========
Run 366/720 done in 62125.61 s.
===========

Directory ../../results/CsplineN_new/run_367/ already exists.
Skipping it.
===========
Run 367/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_368/ already exists.
Skipping it.
===========
Run 368/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_369/ already exists.
Skipping it.
===========
Run 369/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_370/ already exists.
Skipping it.
===========
Run 370/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_371/ already exists.
Skipping it.
===========
Run 371/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_372/ already exists.
Skipping it.
===========
Run 372/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_373/ already exists.
Skipping it.
===========
Run 373/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_374/ already exists.
Skipping it.
===========
Run 374/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_375/ already exists.
Skipping it.
===========
Run 375/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_376/ already exists.
Skipping it.
===========
Run 376/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_377/ already exists.
Skipping it.
===========
Run 377/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_378/ already exists.
Skipping it.
===========
Run 378/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_379/ already exists.
Skipping it.
===========
Run 379/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_380/ already exists.
Skipping it.
===========
Run 380/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_381/ already exists.
Skipping it.
===========
Run 381/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_382/ already exists.
Skipping it.
===========
Run 382/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_383/ already exists.
Skipping it.
===========
Run 383/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_384/ already exists.
Skipping it.
===========
Run 384/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_385/ already exists.
Skipping it.
===========
Run 385/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_386/ already exists.
Skipping it.
===========
Run 386/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_387/ already exists.
Skipping it.
===========
Run 387/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_388/ already exists.
Skipping it.
===========
Run 388/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_389/ already exists.
Skipping it.
===========
Run 389/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_390/ already exists.
Skipping it.
===========
Run 390/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_391/ already exists.
Skipping it.
===========
Run 391/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_392/ already exists.
Skipping it.
===========
Run 392/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_393/ already exists.
Skipping it.
===========
Run 393/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_394/ already exists.
Skipping it.
===========
Run 394/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_395/ already exists.
Skipping it.
===========
Run 395/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_396/ already exists.
Skipping it.
===========
Run 396/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_397/ already exists.
Skipping it.
===========
Run 397/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_398/ already exists.
Skipping it.
===========
Run 398/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_399/ already exists.
Skipping it.
===========
Run 399/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_400/ already exists.
Skipping it.
===========
Run 400/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_401/ already exists.
Skipping it.
===========
Run 401/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_402/ already exists.
Skipping it.
===========
Run 402/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_403/ already exists.
Skipping it.
===========
Run 403/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_404/ already exists.
Skipping it.
===========
Run 404/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_405/ already exists.
Skipping it.
===========
Run 405/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_406/ already exists.
Skipping it.
===========
Run 406/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_407/ already exists.
Skipping it.
===========
Run 407/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_408/ already exists.
Skipping it.
===========
Run 408/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_409/ already exists.
Skipping it.
===========
Run 409/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_410/ already exists.
Skipping it.
===========
Run 410/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_411/ already exists.
Skipping it.
===========
Run 411/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_412/ already exists.
Skipping it.
===========
Run 412/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_413/ already exists.
Skipping it.
===========
Run 413/720 already exists. Skipping it.
===========

===========
Generating train data for run 414.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_263"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_264 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7f3d75b37be0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36b6219e70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36b6219e70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f35e4f71ba0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3d54911810>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3d54911d80>, <keras.callbacks.ModelCheckpoint object at 0x7f3d54911e40>, <keras.callbacks.EarlyStopping object at 0x7f3d549120b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3d549120e0>, <keras.callbacks.TerminateOnNaN object at 0x7f3d54911d20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-31 07:56:53.227161
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 3: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 07:58:37.795 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11057.2744, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 105s - loss: nan - MinusLogProbMetric: 11057.2744 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 105s/epoch - 533ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0003333333333333333.
===========
Generating train data for run 414.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_274"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_275 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7f3d186b0400>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3589a3f0a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3589a3f0a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3d001c3b80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3d00227e20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3d0025c3d0>, <keras.callbacks.ModelCheckpoint object at 0x7f3d0025c490>, <keras.callbacks.EarlyStopping object at 0x7f3d0025c700>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3d0025c730>, <keras.callbacks.TerminateOnNaN object at 0x7f3d0025c370>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-31 07:58:44.619825
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 4: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:00:30.407 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10881.3057, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 106s - loss: nan - MinusLogProbMetric: 10881.3057 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 106s/epoch - 540ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0001111111111111111.
===========
Generating train data for run 414.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_285"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_286 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7f366e4b3280>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3646556620>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3646556620>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f36c50dbdc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36c5034700>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36c5034c70>, <keras.callbacks.ModelCheckpoint object at 0x7f36c5034d30>, <keras.callbacks.EarlyStopping object at 0x7f36c5034fa0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36c5034fd0>, <keras.callbacks.TerminateOnNaN object at 0x7f36c5034c10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-31 08:00:37.943447
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:02:25.049 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10977.7812, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 107s - loss: nan - MinusLogProbMetric: 10977.7812 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 107s/epoch - 546ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 3.703703703703703e-05.
===========
Generating train data for run 414.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_296"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_297 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7f35e4edafe0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f35dd8d73d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f35dd8d73d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f35a5117fd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f35a51385b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f35a5138b20>, <keras.callbacks.ModelCheckpoint object at 0x7f35a5138be0>, <keras.callbacks.EarlyStopping object at 0x7f35a5138e50>, <keras.callbacks.ReduceLROnPlateau object at 0x7f35a5138e80>, <keras.callbacks.TerminateOnNaN object at 0x7f35a5138ac0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-31 08:02:32.075113
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:04:21.887 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11015.6104, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 110s - loss: nan - MinusLogProbMetric: 11015.6104 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 110s/epoch - 560ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.2345679012345677e-05.
===========
Generating train data for run 414.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_307"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_308 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7f3588efde70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f35e4f71ba0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f35e4f71ba0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3d75a1ca60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36b5fffaf0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36b5ffffd0>, <keras.callbacks.ModelCheckpoint object at 0x7f36b5f7c160>, <keras.callbacks.EarlyStopping object at 0x7f36b5f7c3d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36b5f7c400>, <keras.callbacks.TerminateOnNaN object at 0x7f36b5f7c040>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-31 08:04:28.917210
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:06:14.438 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11030.7109, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 105s - loss: nan - MinusLogProbMetric: 11030.7109 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 105s/epoch - 538ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.115226337448558e-06.
===========
Generating train data for run 414.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_318"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_319 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7f3cf75b8280>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3cf72f83a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3cf72f83a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f350c6a2170>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3cf722b9a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3cf722bf10>, <keras.callbacks.ModelCheckpoint object at 0x7f3cf722bfd0>, <keras.callbacks.EarlyStopping object at 0x7f3cf722bee0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3cf722beb0>, <keras.callbacks.TerminateOnNaN object at 0x7f3cf7258280>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-31 08:06:21.223222
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:08:11.359 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11035.6309, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 110s - loss: nan - MinusLogProbMetric: 11035.6309 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 110s/epoch - 562ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.3717421124828526e-06.
===========
Generating train data for run 414.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_329"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_330 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7f366e15fdf0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3645732a10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3645732a10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f35e511eec0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36c7ec75e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36c7ec7b50>, <keras.callbacks.ModelCheckpoint object at 0x7f36c7ec7c10>, <keras.callbacks.EarlyStopping object at 0x7f36c7ec7e80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36c7ec7eb0>, <keras.callbacks.TerminateOnNaN object at 0x7f36c7ec7af0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-31 08:08:18.291020
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:10:05.570 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11043.9355, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 107s - loss: nan - MinusLogProbMetric: 11043.9355 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 107s/epoch - 547ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.572473708276175e-07.
===========
Generating train data for run 414.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_340"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_341 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7f36c4f1c400>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3605e6f3a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3605e6f3a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f35e7ec9540>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f35e7e4da50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f35e7e4dfc0>, <keras.callbacks.ModelCheckpoint object at 0x7f35e7e4e080>, <keras.callbacks.EarlyStopping object at 0x7f35e7e4e2f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f35e7e4e320>, <keras.callbacks.TerminateOnNaN object at 0x7f35e7e4df60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-31 08:10:12.697251
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:11:53.620 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11033.8535, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 101s - loss: nan - MinusLogProbMetric: 11033.8535 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 101s/epoch - 515ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.524157902758725e-07.
===========
Generating train data for run 414.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_351"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_352 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7f3cf6a673a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f366ecc1630>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f366ecc1630>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3cf6a382b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3cf63d4760>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3cf63d4cd0>, <keras.callbacks.ModelCheckpoint object at 0x7f3cf63d4d90>, <keras.callbacks.EarlyStopping object at 0x7f3cf63d5000>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3cf63d5030>, <keras.callbacks.TerminateOnNaN object at 0x7f3cf63d4c70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-31 08:12:00.767016
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:13:53.159 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11043.0674, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 112s - loss: nan - MinusLogProbMetric: 11043.0674 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 112s/epoch - 573ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.0805263425290834e-08.
===========
Generating train data for run 414.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_362"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_363 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7f36b573eb00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3495b6a1d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3495b6a1d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f36cfdc1480>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36059f8280>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36059faa10>, <keras.callbacks.ModelCheckpoint object at 0x7f36059fa830>, <keras.callbacks.EarlyStopping object at 0x7f36059fa950>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36059fb250>, <keras.callbacks.TerminateOnNaN object at 0x7f36059fb5e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-31 08:14:00.356552
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:15:44.579 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11039.0693, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 104s - loss: nan - MinusLogProbMetric: 11039.0693 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 104s/epoch - 532ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.6935087808430278e-08.
===========
Generating train data for run 414.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_414/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_414
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_373"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_374 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7f35043c4370>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f34e46d8820>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f34e46d8820>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f34e46b7d60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f34e46b64a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_414/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f34e46b5db0>, <keras.callbacks.ModelCheckpoint object at 0x7f34e46b5e70>, <keras.callbacks.EarlyStopping object at 0x7f34e46b57e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f34e46b59c0>, <keras.callbacks.TerminateOnNaN object at 0x7f34e46b62c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_414/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 414/720 with hyperparameters:
timestamp = 2023-10-31 08:15:57.048027
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:17:42.771 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11041.0166, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 106s - loss: nan - MinusLogProbMetric: 11041.0166 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 106s/epoch - 539ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.645029269476759e-09.
===========
Run 414/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 415.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_415
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_384"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_385 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7f34819a7610>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f355d3eff40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f355d3eff40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3481eeaf80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f34908937f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3490893d60>, <keras.callbacks.ModelCheckpoint object at 0x7f3490893e20>, <keras.callbacks.EarlyStopping object at 0x7f3490893fd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3490893d30>, <keras.callbacks.TerminateOnNaN object at 0x7f3490893f10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_415/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 415/720 with hyperparameters:
timestamp = 2023-10-31 08:17:51.509277
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:20:11.469 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12402.1191, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 140s - loss: nan - MinusLogProbMetric: 12402.1191 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 140s/epoch - 714ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0003333333333333333.
===========
Generating train data for run 415.
===========
Train data generated in 0.13 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_415
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_395"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_396 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7f35242ae5c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f35247d4040>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f35247d4040>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f358a8f68c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3524274d00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3524275270>, <keras.callbacks.ModelCheckpoint object at 0x7f3524275330>, <keras.callbacks.EarlyStopping object at 0x7f35242755a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f35242755d0>, <keras.callbacks.TerminateOnNaN object at 0x7f3524275210>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_415/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 415/720 with hyperparameters:
timestamp = 2023-10-31 08:20:17.913795
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:22:39.538 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12402.1191, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 142s - loss: nan - MinusLogProbMetric: 12402.1191 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 142s/epoch - 723ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0001111111111111111.
===========
Generating train data for run 415.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_415
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_406"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_407 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7f3d76d18e50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3d44534b80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3d44534b80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f35a54bd000>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3605f95840>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3605f94490>, <keras.callbacks.ModelCheckpoint object at 0x7f3605f946a0>, <keras.callbacks.EarlyStopping object at 0x7f3605f94310>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3605f940d0>, <keras.callbacks.TerminateOnNaN object at 0x7f3605f95f90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_415/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 415/720 with hyperparameters:
timestamp = 2023-10-31 08:22:49.510855
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:25:04.146 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12402.1191, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 135s - loss: nan - MinusLogProbMetric: 12402.1191 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 135s/epoch - 687ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 3.703703703703703e-05.
===========
Generating train data for run 415.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_415
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_417"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_418 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_42 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_42/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_42'")
self.model: <keras.engine.functional.Functional object at 0x7f3cf5e63d60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3ccce2a860>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3ccce2a860>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3ccce63460>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3ccccf4c40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3ccccf51b0>, <keras.callbacks.ModelCheckpoint object at 0x7f3ccccf5270>, <keras.callbacks.EarlyStopping object at 0x7f3ccccf54e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3ccccf5510>, <keras.callbacks.TerminateOnNaN object at 0x7f3ccccf5150>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_415/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 415/720 with hyperparameters:
timestamp = 2023-10-31 08:25:13.723875
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:27:38.372 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12402.1191, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 144s - loss: nan - MinusLogProbMetric: 12402.1191 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 144s/epoch - 736ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.2345679012345677e-05.
===========
Generating train data for run 415.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_415
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_428"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_429 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_43 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_43/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_43'")
self.model: <keras.engine.functional.Functional object at 0x7f35dd4270a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f36b7235630>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f36b7235630>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f34414a7eb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f35dd4db280>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f35dd4db7f0>, <keras.callbacks.ModelCheckpoint object at 0x7f35dd4db8b0>, <keras.callbacks.EarlyStopping object at 0x7f35dd4dbb20>, <keras.callbacks.ReduceLROnPlateau object at 0x7f35dd4dbb50>, <keras.callbacks.TerminateOnNaN object at 0x7f35dd4db790>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_415/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 415/720 with hyperparameters:
timestamp = 2023-10-31 08:27:47.211522
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:29:55.493 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12402.1191, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 128s - loss: nan - MinusLogProbMetric: 12402.1191 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 128s/epoch - 654ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.115226337448558e-06.
===========
Generating train data for run 415.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_415
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_439"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_440 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_44 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_44/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_44'")
self.model: <keras.engine.functional.Functional object at 0x7f3439af9ab0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f35dd4bc790>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f35dd4bc790>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3430361f60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3565b17940>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3565b17eb0>, <keras.callbacks.ModelCheckpoint object at 0x7f3565b17f70>, <keras.callbacks.EarlyStopping object at 0x7f3565b17e80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3565b17e50>, <keras.callbacks.TerminateOnNaN object at 0x7f3565ba0220>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_415/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 415/720 with hyperparameters:
timestamp = 2023-10-31 08:30:03.747644
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:32:24.318 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12402.1191, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 141s - loss: nan - MinusLogProbMetric: 12402.1191 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 141s/epoch - 717ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.3717421124828526e-06.
===========
Generating train data for run 415.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_415
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_450"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_451 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_45 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_45/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_45'")
self.model: <keras.engine.functional.Functional object at 0x7f36b71aad70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f34e5e26440>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f34e5e26440>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3589691a20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f351856b400>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f351856b970>, <keras.callbacks.ModelCheckpoint object at 0x7f351856ba30>, <keras.callbacks.EarlyStopping object at 0x7f351856bca0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f351856bcd0>, <keras.callbacks.TerminateOnNaN object at 0x7f351856b910>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_415/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 415/720 with hyperparameters:
timestamp = 2023-10-31 08:32:34.268719
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:34:56.506 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12402.1191, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 142s - loss: nan - MinusLogProbMetric: 12402.1191 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 142s/epoch - 725ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.572473708276175e-07.
===========
Generating train data for run 415.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_415
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_461"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_462 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_46 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_46/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_46'")
self.model: <keras.engine.functional.Functional object at 0x7f3606c045b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f34398f9870>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f34398f9870>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f36bddedea0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f36463cc6a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f36463ccc10>, <keras.callbacks.ModelCheckpoint object at 0x7f36463cccd0>, <keras.callbacks.EarlyStopping object at 0x7f36463ccf40>, <keras.callbacks.ReduceLROnPlateau object at 0x7f36463ccf70>, <keras.callbacks.TerminateOnNaN object at 0x7f36463ccbb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_415/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 415/720 with hyperparameters:
timestamp = 2023-10-31 08:35:04.592412
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:37:31.318 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12402.1191, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 147s - loss: nan - MinusLogProbMetric: 12402.1191 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 147s/epoch - 748ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.524157902758725e-07.
===========
Generating train data for run 415.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_415
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_472"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_473 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_47 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_47/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_47'")
self.model: <keras.engine.functional.Functional object at 0x7f3ccc6afd90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3cbb6ce5f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3cbb6ce5f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f343353bb20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3cab5ff4f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3cab5ffa60>, <keras.callbacks.ModelCheckpoint object at 0x7f3cab5ffb20>, <keras.callbacks.EarlyStopping object at 0x7f3cab5ffd90>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3cab5ffdc0>, <keras.callbacks.TerminateOnNaN object at 0x7f3cab5ffa00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_415/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 415/720 with hyperparameters:
timestamp = 2023-10-31 08:37:41.206483
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:40:26.093 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12402.1191, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 165s - loss: nan - MinusLogProbMetric: 12402.1191 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 165s/epoch - 840ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.0805263425290834e-08.
===========
Generating train data for run 415.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_415
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_483"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_484 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_48 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_48/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_48'")
self.model: <keras.engine.functional.Functional object at 0x7f366ffe01f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3565e5f100>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3565e5f100>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f34f8cecdf0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f350c4a5810>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f350c4a5d80>, <keras.callbacks.ModelCheckpoint object at 0x7f350c4a5e40>, <keras.callbacks.EarlyStopping object at 0x7f350c4a60b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f350c4a60e0>, <keras.callbacks.TerminateOnNaN object at 0x7f350c4a5d20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_415/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 415/720 with hyperparameters:
timestamp = 2023-10-31 08:40:36.351498
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:43:25.047 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12402.1191, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 169s - loss: nan - MinusLogProbMetric: 12402.1191 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 169s/epoch - 860ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.6935087808430278e-08.
===========
Generating train data for run 415.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_415/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_415
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_494"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_495 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_49 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_49/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_49'")
self.model: <keras.engine.functional.Functional object at 0x7f364492e020>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f366ef64b50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f366ef64b50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3429421a80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f35665823e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_415/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3566580fd0>, <keras.callbacks.ModelCheckpoint object at 0x7f35665821a0>, <keras.callbacks.EarlyStopping object at 0x7f3566581bd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3566583dc0>, <keras.callbacks.TerminateOnNaN object at 0x7f35665802b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_415/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 415/720 with hyperparameters:
timestamp = 2023-10-31 08:43:36.520999
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:45:52.486 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12402.1191, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 136s - loss: nan - MinusLogProbMetric: 12402.1191 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 136s/epoch - 692ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.645029269476759e-09.
===========
Run 415/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 416.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_416/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_416/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_416/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_416
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_505"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_506 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_50 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_50/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_50'")
self.model: <keras.engine.functional.Functional object at 0x7f3ca28ce2c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f342aba1db0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f342aba1db0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3ca1db3ac0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3ca1cd8310>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_416/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3ca1cd8880>, <keras.callbacks.ModelCheckpoint object at 0x7f3ca1cd8940>, <keras.callbacks.EarlyStopping object at 0x7f3ca1cd8bb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3ca1cd8be0>, <keras.callbacks.TerminateOnNaN object at 0x7f3ca1cd8820>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_416/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 416/720 with hyperparameters:
timestamp = 2023-10-31 08:46:03.365427
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:48:45.175 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11848.9121, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 162s - loss: nan - MinusLogProbMetric: 11848.9121 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 162s/epoch - 825ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0003333333333333333.
===========
Generating train data for run 416.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_416/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_416/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_416/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_416
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_516"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_517 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_51 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_51/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_51'")
self.model: <keras.engine.functional.Functional object at 0x7f350d1c1510>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3d28fe6560>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3d28fe6560>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f34302a7730>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3d75c7f3d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_416/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3d75c7f940>, <keras.callbacks.ModelCheckpoint object at 0x7f3d75c7fa00>, <keras.callbacks.EarlyStopping object at 0x7f3d75c7fc70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3d75c7fca0>, <keras.callbacks.TerminateOnNaN object at 0x7f3d75c7f8e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_416/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 416/720 with hyperparameters:
timestamp = 2023-10-31 08:48:57.366405
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:51:35.629 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11848.9121, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 158s - loss: nan - MinusLogProbMetric: 11848.9121 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 158s/epoch - 806ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0001111111111111111.
===========
Generating train data for run 416.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_416/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_416/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_416/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_416
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_527"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_528 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_52 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_52/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_52'")
self.model: <keras.engine.functional.Functional object at 0x7f342a64eef0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f34481dfd30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f34481dfd30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f342a3f3d90>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f34913ae4d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_416/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f34913aea40>, <keras.callbacks.ModelCheckpoint object at 0x7f34913aeb00>, <keras.callbacks.EarlyStopping object at 0x7f34913aed70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f34913aeda0>, <keras.callbacks.TerminateOnNaN object at 0x7f34913ae9e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_416/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 416/720 with hyperparameters:
timestamp = 2023-10-31 08:51:45.404816
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:54:24.399 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11848.9121, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 159s - loss: nan - MinusLogProbMetric: 11848.9121 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 159s/epoch - 812ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 3.703703703703703e-05.
===========
Generating train data for run 416.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_416/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_416/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_416/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_416
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_538"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_539 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_53 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_53/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_53'")
self.model: <keras.engine.functional.Functional object at 0x7f3607aebd90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f34f9429570>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f34f9429570>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3607afaa70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f35a56d9300>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_416/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f35a56d8790>, <keras.callbacks.ModelCheckpoint object at 0x7f35a56d84f0>, <keras.callbacks.EarlyStopping object at 0x7f35a56d82e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f35a56d8370>, <keras.callbacks.TerminateOnNaN object at 0x7f35a56d8c70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_416/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 416/720 with hyperparameters:
timestamp = 2023-10-31 08:54:34.105922
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:56:55.337 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11848.9121, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 141s - loss: nan - MinusLogProbMetric: 11848.9121 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 141s/epoch - 720ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.2345679012345677e-05.
===========
Generating train data for run 416.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_416/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_416/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_416/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_416
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_549"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_550 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_54 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_54/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_54'")
self.model: <keras.engine.functional.Functional object at 0x7f3c98b147f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f34918d1c30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f34918d1c30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f3ca17b5240>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3ca17b4a90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_416/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3ca17b79a0>, <keras.callbacks.ModelCheckpoint object at 0x7f3ca17b66e0>, <keras.callbacks.EarlyStopping object at 0x7f3ca17b5ae0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3ca17b5630>, <keras.callbacks.TerminateOnNaN object at 0x7f3ca17b4220>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_416/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 416/720 with hyperparameters:
timestamp = 2023-10-31 08:57:19.376007
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-31 08:59:52.772 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11848.9121, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 153s - loss: nan - MinusLogProbMetric: 11848.9121 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 153s/epoch - 783ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.115226337448558e-06.
===========
Generating train data for run 416.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_416/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_416/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_416/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_416
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_560"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_561 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_55 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_55/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_55'")
self.model: <keras.engine.functional.Functional object at 0x7f3c6f91da80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f3c670e6950>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f3c670e6950>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f32e3f7dff0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3c66fdf9a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_416/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3c66fdff10>, <keras.callbacks.ModelCheckpoint object at 0x7f3c66fdffd0>, <keras.callbacks.EarlyStopping object at 0x7f3c66fdfee0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3c66fdfeb0>, <keras.callbacks.TerminateOnNaN object at 0x7f3c67030280>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_416/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 416/720 with hyperparameters:
timestamp = 2023-10-31 09:00:02.469779
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
LLVM ERROR: pthread_create failed: Resource temporarily unavailable
LLVM ERROR: pthread_create failed: Resource temporarily unavailable
